{
  "https://www.isca-speech.org/archive/interspeech_2019/tokuda19_interspeech.html": {
    "title": "Statistical Approach to Speech Synthesis: Past, Present and Future",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "fa87441f84e6fe211195648b103707bbe9bd2367",
    "semantic_title": "statistical approach to speech synthesis: past, present and future",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wu19_interspeech.html": {
    "title": "Advances in Automatic Speech Recognition for Child Speech Using Factored Time Delay Neural Network",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) has shown huge advances in adult speech; however, when the models are tested on child speech, the performance does not achieve satisfactory word error rates (WER). This is mainly due to the high variance in acoustic features of child speech and the lack of clean, labeled corpora. We apply the factored time delay neural network (TDNN-F) to the child speech domain, finding that it yields better performance. To enable our models to handle the different noise conditions and extremely small corpora, we augment the original training data by adding noise and reverberation. Compared with conventional GMM-HMM and TDNN systems, TDNN-F does better on two widely accessible corpora: CMU Kids and CSLU Kids, and on the combination of these two. Our system achieves a 26% relative improvement in WER",
    "checked": true,
    "id": "beeaa7417e7818f737c2958550757735982fc49b",
    "semantic_title": "advances in automatic speech recognition for child speech using factored time delay neural network",
    "citation_count": 32
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yeung19_interspeech.html": {
    "title": "A Frequency Normalization Technique for Kindergarten Speech Recognition Inspired by the Role of fo in Vowel Perception",
    "volume": "main",
    "abstract": "Accurate automatic speech recognition (ASR) of kindergarten speech is particularly important as this age group may benefit the most from voice-based educational tools. Due to the lack of young child speech data, kindergarten ASR systems often are trained using older child or adult speech. This study proposes a fundamental frequency (f )-based normalization technique to reduce the spectral mismatch between kindergarten and older child speech. The technique is based on the tonotopic distances between formants and f developed to model vowel perception. This proposed procedure only relies on the computation of median f across an utterance. Tonotopic distances for vowel perception were reformulated as a linear relationship between formants and f to provide an effective approach for frequency normalization. This reformulation was verified by examining the formants and f of child vowel productions. A 208-word ASR experiment using older child speech for training and kindergarten speech for testing was performed to examine the effectiveness of the proposed technique against piecewise vocal tract length, F3-based, and subglottal resonance normalization techniques. Results suggest that the proposed technique either has performance advantages or requires the computation of fewer parameters",
    "checked": true,
    "id": "b521d1c626053e5252bc0fcd3e16d63efde5ba3c",
    "semantic_title": "a frequency normalization technique for kindergarten speech recognition inspired by the role of fo in vowel perception",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gale19_interspeech.html": {
    "title": "Improving ASR Systems for Children with Autism and Language Impairment Using Domain-Focused DNN Transfer Techniques",
    "volume": "main",
    "abstract": "This study explores building and improving an automatic speech recognition (ASR) system for children aged 6–9 years and diagnosed with autism spectrum disorder (ASD), language impairment (LI), or both. Working with only 1.5 hours of target data in which children perform the Clinical Evaluation of Language Fundamentals Recalling Sentences task, we apply deep neural network (DNN) weight transfer techniques to adapt a large DNN model trained on the LibriSpeech corpus of adult speech. To begin, we aim to find the best proportional training rates of the DNN layers. Our best configuration yields a 29.38% word error rate (WER). Using this configuration, we explore the effects of quantity and similarity of data augmentation in transfer learning. We augment our training with portions of the OGI Kids' Corpus, adding 4.6 hours of typically developing speakers aged kindergarten through 3 grade. We find that 2 grade data alone — approximately the mean age of the target data — outperforms other grades and all the sets combined. Doubling the data for 1 , 2 , and 3 grade, we again compare each grade as well as pairs of grades. We find the combination of 1 and 2 grade performs best at a 26.21% WER",
    "checked": true,
    "id": "9826fb6c64b76d5d0809a7c6ae5914342c782930",
    "semantic_title": "improving asr systems for children with autism and language impairment using domain-focused dnn transfer techniques",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ribeiro19_interspeech.html": {
    "title": "Ultrasound Tongue Imaging for Diarization and Alignment of Child Speech Therapy Sessions",
    "volume": "main",
    "abstract": "We investigate the automatic processing of child speech therapy sessions using ultrasound visual biofeedback, with a specific focus on complementing acoustic features with ultrasound images of the tongue for the tasks of speaker diarization and time-alignment of target words. For speaker diarization, we propose an ultrasound-based time-domain signal which we call estimated tongue activity. For word-alignment, we augment an acoustic model with low-dimensional representations of ultrasound images of the tongue, learned by a convolutional neural network. We conduct our experiments using the Ultrasuite repository of ultrasound and speech recordings for child speech therapy sessions. For both tasks, we observe that systems augmented with ultrasound data outperform corresponding systems using only the audio signal",
    "checked": true,
    "id": "e940bdd66421ab3d0d26434798344f846553c35f",
    "semantic_title": "ultrasound tongue imaging for diarization and alignment of child speech therapy sessions",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/loukina19_interspeech.html": {
    "title": "Automated Estimation of Oral Reading Fluency During Summer Camp e-Book Reading with MyTurnToRead",
    "volume": "main",
    "abstract": "Use of speech technologies in the classroom is often limited by the inferior acoustic conditions as well as other factors that might affect the quality of the recordings. We describe MyTurnToRead, an e-book-based app designed to support an interleaved listening and reading experience, where the child takes turns reading aloud with a virtual partner. The child's reading turns are recorded, and processed by an automated speech analysis system in order to provide feedback or track improvement in reading skill. We describe the architecture of the speech processing back-end and evaluate system performance on the data collected in several summer camps where children used the app on consumer-grade devices as part of the camp programming. We show that while the quality of the audio recordings varies greatly, our estimates of student oral reading fluency are very good: for example, the correlation between ASR-based and transcription-based estimates of reading fluency at the speaker level is r=0.93. These are also highly correlated with an external measure of reading comprehension",
    "checked": true,
    "id": "4b6866be7c111007cf4cb949b36c2123cc7f285e",
    "semantic_title": "automated estimation of oral reading fluency during summer camp e-book reading with myturntoread",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lopes19_interspeech.html": {
    "title": "Sustained Vowel Game: A Computer Therapy Game for Children with Dysphonia",
    "volume": "main",
    "abstract": "Problems in vocal quality are common in 4 to 12-year-old children, which may affect their health as well as their social interactions and development process. The sustained vowel exercise is widely used by speech and language pathologists for the child's voice recovery and vocal re-education. Nonetheless, despite being an important voice exercise, it can be a monotonous and tedious activity for children. Here, we propose a computer therapy game that uses the sustained vowel exercise to motivate children on doing this exercise often. In addition, the game gives visual feedback on the child's performance, which helps the child understand how to improve the voice production. The game uses a vowel classification model learned with a support vector machine and Mel frequency cepstral coefficients. A user test with 14 children showed that when using the game, children achieve longer phonation times than without the game. Also, it shows that the visual feedback helps and motivates children on improving their sustained vowel productions",
    "checked": true,
    "id": "594c6449ddd016ae5ac10079546d39b518cee779",
    "semantic_title": "sustained vowel game: a computer therapy game for children with dysphonia",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/esposito19_interspeech.html": {
    "title": "The Dependability of Voice on Elders' Acceptance of Humanoid Agents",
    "volume": "main",
    "abstract": "The research on ambient assistive technology is concerned with features humanoid agents should show in order to gain user acceptance. However, differently aged groups may have different requirements. This paper is particularly focused on agent's voice preferences among elders, young adults, and adolescents To this aim 316 users organized in groups of 45/46 subjects of which 3 groups of elders (65+ years old), 2 of young adults (aged between 22–35 years), and 2 of adolescents (aged between 14–16 years) were recruited and administered the Virtual Agent Acceptance Questionnaire (VAAQ), after watching video-clips of mute and speaking agents, in order to test their preferences in terms of willingness to interact, pragmatic and hedonic qualities, and attractiveness, of proposed speaking and mute agents. In addition, the elders were also tested on listening only the agent's. The results suggest that voice is primary for getting elder's acceptance of virtual humanoid agents in contrast to young adults and adolescents which accept equally well either mute or speaking agents",
    "checked": true,
    "id": "7e956e27098fe50c55e709ed2ba4247ac24f8f5e",
    "semantic_title": "the dependability of voice on elders' acceptance of humanoid agents",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/niebuhr19_interspeech.html": {
    "title": "God as Interlocutor — Real or Imaginary? Prosodic Markers of Dialogue Speech and Expected Efficacy in Spoken Prayer",
    "volume": "main",
    "abstract": "We analyze the phonetic correlates of petitionary prayer in 22 Christian practitioners. Our aim is to examine if praying is characterized by prosodic markers of dialogue speech and expected efficacy. Three similar conditions are compared; 1) requests to God, 2) requests to a human recipient, 3) requests to an imaginary person. We find that making requests to God is clearly distinguishable from making requests to both human and imaginary interlocutors. Requests to God are, unlike requests to an imaginary person, characterized by markers of dialogue speech (as opposed to monologue speech), including, a higher f0 level, a larger f0 range, and a slower speaking rate. In addition, requests to God differ from those made to both human and imaginary persons in markers of expected efficacy on the part of the speaker. These markers are related to a more careful speech production, including almost complete lack of hesitations, more pauses, and a much longer speaking time",
    "checked": true,
    "id": "8608531f9219f0f0b8ed895c26a9dfa3bd95cb17",
    "semantic_title": "god as interlocutor - real or imaginary? prosodic markers of dialogue speech and expected efficacy in spoken prayer",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cohn19_interspeech.html": {
    "title": "Expressiveness Influences Human Vocal Alignment Toward voice-AI",
    "volume": "main",
    "abstract": "This study explores whether people align to expressive speech spoken by a voice-activated artificially intelligent device (voice-AI), specifically Amazon's Alexa. Participants shadowed words produced by the Alexa voice in two acoustically distinct conditions: \"regular\" and \"expressive\", containing more exaggerated pitch contours and longer word durations. Another group of participants rated the shadowed items, in an AXB perceptual similarity task, as an assessment of overall degree of vocal alignment. Results show greater vocal alignment toward expressive speech produced by the Alexa voice and, furthermore, systematic variation based on speaker gender. Overall, these findings have applications to the field of affective computing in understanding human responses to synthesized emotional expressiveness",
    "checked": true,
    "id": "7499570fd0ddc4fe1da060aeb8a5fcec15f33755",
    "semantic_title": "expressiveness influences human vocal alignment toward voice-ai",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lai19_interspeech.html": {
    "title": "Detecting Topic-Oriented Speaker Stance in Conversational Speech",
    "volume": "main",
    "abstract": "Being able to detect topics and speaker stances in conversations is a key requirement for developing spoken language understanding systems that are personalized and adaptive. In this work, we explore how topic-oriented speaker stance is expressed in conversational speech. To do this, we present a new set of topic and stance annotations of the CallHome corpus of spontaneous dialogues. Specifically, we focus on six stances — positivity, certainty, surprise, amusement, interest, and comfort — which are useful for characterizing important aspects of a conversation, such as whether a conversation is going well or not. Based on this, we investigate the use of neural network models for automatically detecting speaker stance from speech in multi-turn, multi-speaker contexts. In particular, we examine how performance changes depending on how input feature representations are constructed and how this is related to dialogue structure. Our experiments show that incorporating both lexical and acoustic features is beneficial for stance detection. However, we observe variation in whether using hierarchical models for encoding lexical and acoustic information improves performance, suggesting that some aspects of speaker stance are expressed more locally than others. Overall, our findings highlight the importance of modelling interaction dynamics and non-lexical content for stance detection",
    "checked": true,
    "id": "ab76b65badae8872ce191ea6173d0fdf1f1c3536",
    "semantic_title": "detecting topic-oriented speaker stance in conversational speech",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sebastian19_interspeech.html": {
    "title": "Fusion Techniques for Utterance-Level Emotion Recognition Combining Speech and Transcripts",
    "volume": "main",
    "abstract": "In human perception and understanding, a number of different and complementary cues are adopted according to different modalities. Various emotional states in communication between humans reflect this variety of cues across modalities. Recent developments in multi-modal emotion recognition utilize deep-learning techniques to achieve remarkable performances, with models based on different features suitable for text, audio and vision. This work focuses on cross-modal fusion techniques over deep learning models for emotion detection from spoken audio and corresponding transcripts We investigate the use of long short-term memory (LSTM) recurrent neural network (RNN) with pre-trained word embedding for text-based emotion recognition and convolutional neural network (CNN) with utterance-level descriptors for emotion recognition from speech. Various fusion strategies are adopted on these models to yield an overall score for each of the emotional categories. Intra-modality dynamics for each emotion is captured in the neural network designed for the specific modality. Fusion techniques are employed to obtain the inter-modality dynamics. Speaker and session-independent experiments on IEMOCAP multi-modal emotion detection dataset show the effectiveness of the proposed approaches. This method yields state-of-the-art results for utterance-level emotion recognition based on speech and text",
    "checked": true,
    "id": "7b43738839277ba7123e8df056df983b79c14530",
    "semantic_title": "fusion techniques for utterance-level emotion recognition combining speech and transcripts",
    "citation_count": 37
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rajwadi19_interspeech.html": {
    "title": "Explaining Sentiment Classification",
    "volume": "main",
    "abstract": "This paper presents a novel 1-D sentiment classifier trained on the benchmark IMDB dataset. The classifier is a 1-D convolutional neural network with repeated convolution and max pooling layers. The main contribution of this work is the demonstration of a deconvolution technique for 1-D convolutional neural networks that is agnostic to specific architecture types. This deconvolution technique enables text classification to be explained, a feature that is important for NLP-based decision support systems, as well as being an invaluable diagnostic tool",
    "checked": true,
    "id": "03849bdecfc720717d3953c8fe9f4ded437f1d1b",
    "semantic_title": "explaining sentiment classification",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kleinlein19_interspeech.html": {
    "title": "Predicting Group-Level Skin Attention to Short Movies from Audio-Based LSTM-Mixture of Experts Models",
    "volume": "main",
    "abstract": "Electrodermal activity (EDA) is a psychophysiological indicator that can be considered a somatic marker of the emotional and attentional reaction of subjects towards stimuli like audiovisual content. EDA measurements are not biased by the cognitive process of giving an opinion or a score to characterize the subjective perception, and group-level EDA recordings integrate the reaction of an audience, thus reducing the signal noise. This paper contributes to the field of audience's attention prediction to video content, extending previous novel work on the use of EDA as ground truth for prediction algorithms. Videos are segmented into shorter clips attending to the audience's increasing or decreasing attention, and we process videos' audio waveform to extract meaningful aural embeddings from a VGGish model pretrained on the Audioset database. While previous similar work on attention level prediction using only audio accomplished 69.83% accuracy, we propose a Mixture of Experts approach to train a binary classifier that outperforms the main existing state-of-the-art approaches predicting increasing and decreasing attention levels with 81.76% accuracy. These results confirm the usefulness of providing acoustic features with a semantic significance, and the convenience of considering experts over partitions of the dataset in order to predict group-level attention from audio",
    "checked": true,
    "id": "13c30a3e0888f4d1af79c05fddaae06895380a4f",
    "semantic_title": "predicting group-level skin attention to short movies from audio-based lstm-mixture of experts models",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schluter19_interspeech.html": {
    "title": "Survey Talk: Modeling in Automatic Speech Recognition: Beyond Hidden Markov Models",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "17e5123e8954a077ab2e2e933b812eec5878a0ce",
    "semantic_title": "survey talk: modeling in automatic speech recognition: beyond hidden markov models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pham19_interspeech.html": {
    "title": "Very Deep Self-Attention Networks for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "Recently, end-to-end sequence-to-sequence models for speech recognition have gained significant interest in the research community. While previous architecture choices revolve around time-delay neural networks (TDNN) and long short-term memory (LSTM) recurrent neural networks, we propose to use self-attention via the Transformer architecture as an alternative. Our analysis shows that deep Transformer networks with high learning capacity are able to exceed performance from previous end-to-end approaches and even match the conventional hybrid systems. Moreover, we trained very deep models with up to 48 Transformer layers for both encoder and decoders combined with stochastic residual connections, which greatly improve generalizability and training efficiency. The resulting models outperform all previous end-to-end ASR approaches on the Switchboard benchmark. An ensemble of these models achieve 9.9% and 17.7% WER on Switchboard and CallHome test sets respectively. This finding brings our end-to-end models to competitive levels with previous hybrid systems. Further, with model ensembling the Transformers can outperform certain hybrid systems, which are more complicated in terms of both structure and training procedure",
    "checked": true,
    "id": "f2bb7e2f5a1afad5370159c15760c44df93c0438",
    "semantic_title": "very deep self-attention networks for end-to-end speech recognition",
    "citation_count": 143
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19_interspeech.html": {
    "title": "Jasper: An End-to-End Convolutional Neural Acoustic Model",
    "volume": "main",
    "abstract": "In this paper we report state-of-the-art results on LibriSpeech among end-to-end speech recognition models without any external training data. Our model, Jasper, uses only 1D convolutions, batch normalization, ReLU, dropout, and residual connections. To improve training, we further introduce a new layer-wise optimizer called NovoGrad. Through experiments, we demonstrate that the proposed deep architecture performs as well or better than more complex choices. Our deepest Jasper variant uses 54 convolutional layers. With this architecture, we achieve 2.95% WER using a beam-search decoder with an external neural language model and 3.86% WER with a greedy decoder on LibriSpeech test-clean. We also report competitive results on Wall Street Journal and the Hub5'00 conversational evaluation datasets",
    "checked": true,
    "id": "d85b2af4f163383bbfa62b73d5f0b179868cc9a8",
    "semantic_title": "jasper: an end-to-end convolutional neural acoustic model",
    "citation_count": 203
  },
  "https://www.isca-speech.org/archive/interspeech_2019/moritz19_interspeech.html": {
    "title": "Unidirectional Neural Network Architectures for End-to-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "In hybrid automatic speech recognition (ASR) systems, neural networks are used as acoustic models (AMs) to recognize phonemes that are composed to words and sentences using pronunciation dictionaries, hidden Markov models, and language models, which can be jointly represented by a weighted finite state transducer (WFST). The importance of capturing temporal context by an AM has been studied and discussed in prior work. In an end-to-end ASR system, however, all components are merged into a single neural network, i.e., the breakdown into an AM and the different parts of the WFST model is no longer possible. This implies that end-to-end neural network architectures have even stronger requirements for processing long contextual information. Bidirectional long short-term memory (BLSTM) neural networks have demonstrated state-of-the-art results in end-to-end ASR but are unsuitable for streaming applications. Latency-controlled BLSTMs account for this by limiting the future context seen by the backward directed recurrence using chunk-wise processing. In this paper, we propose two new unidirectional neural network architectures, the time-delay LSTM (TDLSTM) and the parallel time-delayed LSTM (PTDLSTM) streams, which both limit the processing latency to a fixed size and demonstrate significant improvements compared to prior art on a variety of ASR tasks",
    "checked": true,
    "id": "6aed9eedd5cb712c3d902152cb94f2f18ba4c729",
    "semantic_title": "unidirectional neural network architectures for end-to-end automatic speech recognition",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/belinkov19_interspeech.html": {
    "title": "Analyzing Phonetic and Graphemic Representations in End-to-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end neural network systems for automatic speech recognition (ASR) are trained from acoustic features to text transcriptions. In contrast to modular ASR systems, which contain separately-trained components for acoustic modeling, pronunciation lexicon, and language modeling, the end-to-end paradigm is both conceptually simpler and has the potential benefit of training the entire system on the end task. However, such neural network models are more opaque: it is not clear how to interpret the role of different parts of the network and what information it learns during training. In this paper, we analyze the learned internal representations in an end-to-end ASR model. We evaluate the representation quality in terms of several classification tasks, comparing phonemes and graphemes, as well as different articulatory features. We study two languages (English and Arabic) and three datasets, finding remarkable consistency in how different properties are represented in different layers of the deep neural network",
    "checked": true,
    "id": "facccce4f8d059d6156cf3ce536786eb43016939",
    "semantic_title": "analyzing phonetic and graphemic representations in end-to-end automatic speech recognition",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tawara19_interspeech.html": {
    "title": "Multi-Channel Speech Enhancement Using Time-Domain Convolutional Denoising Autoencoder",
    "volume": "main",
    "abstract": "This paper investigates the use of time-domain convolutional denoising autoencoders (TCDAEs) with multiple channels as a method of speech enhancement. In general, denoising autoencoders (DAEs), deep learning systems that map noise-corrupted into clean waveforms, have been shown to generate high-quality signals while working in the time domain without the intermediate stage of phase modeling. Convolutional DAEs are one of the popular structures which learns a mapping between noise-corrupted and clean waveforms with convolutional denoising autoencoder. Multi-channel signals for TCDAEs are promising because the different times of arrival of a signal can be directly processed with their convolutional structure, Up to this time, TCDAEs have only been applied to single-channel signals. This paper explorers the effectiveness of TCDAEs in a multi-channel configuration. A multi-channel TCDAEs are evaluated on multi-channel speech enhancement experiments, yielding significant improvement over single-channel DAEs in terms of signal-to-distortion ratio, perceptual evaluation of speech quality (PESQ), and word error rate",
    "checked": true,
    "id": "69161fc1b301054f7b4423085530e2dc6083255d",
    "semantic_title": "multi-channel speech enhancement using time-domain convolutional denoising autoencoder",
    "citation_count": 29
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tesch19_interspeech.html": {
    "title": "On Nonlinear Spatial Filtering in Multichannel Speech Enhancement",
    "volume": "main",
    "abstract": "Using multiple microphones for speech enhancement allows for exploiting spatial information for improved performance. In most cases, the spatial filter is selected to be a linear function of the input as, for example, the minimum variance distortionless response (MVDR) beamformer. For non-Gaussian distributed noise, however, the minimum mean square error (MMSE) optimal spatial filter may be nonlinear Potentially, such nonlinear functional relationships could be learned by deep neural networks. However, the performance would depend on many parameters and the architecture of the neural network. Therefore, in this paper, we more generally analyze the potential benefit of nonlinear spatial filters as a function of the multivariate kurtosis of the noise distribution The results imply that using a nonlinear spatial filter is only worth the effort if the noise data follows a distribution with a multivariate kurtosis that is considerably higher than for a Gaussian. In this case, we report a performance difference of up to 2.6 dB segmental signal-to-noise ratio (SNR) improvement for artificial stationary noise. We observe an advantage of 1.2dB for the nonlinear spatial filter over the linear one even for real-world noise data from the CHiME-3 dataset given oracle data for parameter estimation",
    "checked": true,
    "id": "8945e5c963f77d663cf9832c31dcd2da0469ada9",
    "semantic_title": "nonlinear spatial filtering in multichannel speech enhancement",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/martindonas19_interspeech.html": {
    "title": "Multi-Channel Block-Online Source Extraction Based on Utterance Adaptation",
    "volume": "main",
    "abstract": "This paper deals with multi-channel speech recognition in scenarios with multiple speakers. Recently, the spectral characteristics of a target speaker, extracted from an adaptation utterance, have been used to guide a neural network mask estimator to focus on that speaker. In this work we present two variants of speaker-aware neural networks, which exploit both spectral and spatial information to allow better discrimination between target and interfering speakers. Thus, we introduce either a spatial pre-processing prior to the mask estimation or a spatial plus spectral speaker characterization block whose output is directly fed into the neural mask estimator. The target speaker's spectral and spatial signature is extracted from an adaptation utterance recorded at the beginning of a session. We further adapt the architecture for low-latency processing by means of block-online beamforming that recursively updates the signal statistics. Experimental results show that the additional spatial information clearly improves source extraction, in particular in the same-gender case, and that our proposal achieves state-of-the-art performance in terms of distortion reduction and recognition accuracy",
    "checked": true,
    "id": "396c65bbf4098d767586ea0210a8bfa0b829405b",
    "semantic_title": "multi-channel block-online source extraction based on utterance adaptation",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bagheri19_interspeech.html": {
    "title": "Exploiting Multi-Channel Speech Presence Probability in Parametric Multi-Channel Wiener Filter",
    "volume": "main",
    "abstract": "In this paper, we present a practical implementation of the parametric multi-channel Wiener filter (PMWF) noise reduction algorithm. In particular, we extend on methods that incorporate the multi-channel speech presence probability (MC-SPP) in the PMWF derivation and its output. The use of the MC-SPP brings several advantages. Firstly, the MC-SPP allows for better estimates of noise and speech statistics, for which we derive a direct update of the inverse of the noise power spectral density (PSD). Secondly, the MC-SPP is used to control the trade-off parameter in PMWF which, with proper tuning, outperforms the traditional approach with a fixed trade-off parameter. Thirdly, the MC-SPP for each frequency-band is used to obtain the MMSE estimate of the desired speech signal at the output, where we control the maximum amount of noise reduction based on our application. Experimental results on a large number of simulated scenarios show significant benefits of employing MC-SPP in terms of SNR improvements and speech distortion",
    "checked": true,
    "id": "7eb423a18481d37e7d766e9b083a190effecc6a9",
    "semantic_title": "exploiting multi-channel speech presence probability in parametric multi-channel wiener filter",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/togami19_interspeech.html": {
    "title": "Variational Bayesian Multi-Channel Speech Dereverberation Under Noisy Environments with Probabilistic Convolutive Transfer Function",
    "volume": "main",
    "abstract": "In this paper, we propose a multi-channel speech dereverberation method which can reduce reverberation even when acoustic transfer functions (ATFs) are time varying under noisy environments. The microphone input signal is modeled as a convolutive mixture in a time-frequency domain so as to incorporate late reverberation whose tap length is longer than frame size of short term Fourier transform. To reduce reverberation effectively under the time-varying ATF conditions, the proposed method extends the deterministic convolutive transfer function (D-CTF) into a probabilistic convolutive transfer function (P-CTF). A variational Bayesian framework was applied to approximation of a joint posterior probability density functions of a speech source signal and the ATFs. Variational posterior probability density functions and the other parameters are iteratively updated so as to maximize an evidence lower bound (ELBO). Experimental results when the ATFs are time-varying and there is background noise showed that the proposed method can reduce reverberation more accurately than the Weighted Prediction error (WPE) and the Kalman-EM for dereverberation (KEMD)",
    "checked": true,
    "id": "96273401679e1d2da953e3367712c11fd17ee7f1",
    "semantic_title": "variational bayesian multi-channel speech dereverberation under noisy environments with probabilistic convolutive transfer function",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nakatani19_interspeech.html": {
    "title": "Simultaneous Denoising and Dereverberation for Low-Latency Applications Using Frame-by-Frame Online Unified Convolutional Beamformer",
    "volume": "main",
    "abstract": "This article presents frame-by-frame online processing algorithms for a Weighted Power minimization Distortionless response convolutional beamformer (WPD). The WPD unifies widely-used multichannel dereverberation and denoising methods, namely a weighted prediction error based dereverberation method (WPE) and a minimum power distortionless response beamformer (MPDR) into a single convolutional beamformer, and achieves simultaneous dereverberation and denoising based on maximum likelihood estimation. We derive two different online algorithms, one based on frame-by-frame recursive updating of the spatio-temporal covariance matrix of the captured signal, and the other on recursive least square estimation of the convolutional beamformer. In addition, for both algorithms, the desired signal's relative transfer function (RTF) is estimated by online processing using a neural network based online mask estimation. Experiments using the REVERB challenge dataset show the effectiveness of both algorithms in terms of objective speech enhancement measures and automatic speech recognition (ASR) performance",
    "checked": true,
    "id": "460ecb228570a9fbbb12d0cdfa4670a0110e3870",
    "semantic_title": "simultaneous denoising and dereverberation for low-latency applications using frame-by-frame online unified convolutional beamformer",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2019/snyder19_interspeech.html": {
    "title": "Individual Variation in Cognitive Processing Style Predicts Differences in Phonetic Imitation of Device and Human Voices",
    "volume": "main",
    "abstract": "Phonetic imitation, or implicitly matching the acoustic-phonetic patterns of another speaker, has been empirically associated with natural tendencies to promote successful social communication, as well as individual differences in personality and cognitive processing style. The present study explores whether individual differences in cognitive processing style, as indexed by self-reported scored from the Autism-Spectrum Quotient (AQ) questionnaire, are linked to the way people imitate the vocal productions by two digital device voices (i.e., Apple's Siri) and two human voices. Subjects first performed a word shadowing task of human and device voices and then completed the self-administered AQ. We assessed imitation of two acoustic properties: f0 and vowel duration. We find that the attention to detail and the imagination subscale scores on the AQ mediated degree of imitation of f0 and vowel duration, respectively. The findings yield new insight to speech production and perception mechanisms and how it interacts with individual cognitive processing style differences",
    "checked": true,
    "id": "b3516010239a9e4fd3061581793a335fe4615682",
    "semantic_title": "individual variation in cognitive processing style predicts differences in phonetic imitation of device and human voices",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2019/illa19_interspeech.html": {
    "title": "An Investigation on Speaker Specific Articulatory Synthesis with Speaker Independent Articulatory Inversion",
    "volume": "main",
    "abstract": "Estimating speech representations from articulatory movements is known as articulatory-to-acoustic forward (AAF) mapping. Typically this mapping is learned using directly measured articulatory movement in a subject-specific manner. Such AAF mapping has been shown to benefit the speech synthesis applications. In this work, we investigate the speaker similarity and naturalness of utterances generated by AAF which is driven by the articulatory movements from a subject (referred to as cross speaker) different from the speaker (target speaker) used for training AAF mapping. Experiments are performed with directly measured articulatory data from 9 speakers (8 target speakers and 1 cross speaker), which are recorded using Electromagnetic articulograph AG501. Experiments are also performed with articulatory features estimated using speaker independent acoustic-to-articulatory inversion (SI-AAI) model trained on 26 reference speakers. Objective evaluation on target speakers reveal that the articulatory features estimated from SI-AAI result in a lower Mel-cepstrum distortion compared to that using directly measured articulatory features. Further, listening tests reveal that the directly measured articulatory movements preserve the speaker similarity better than estimated ones. Although, for naturalness, articulatory movements predicted by SI-AAI perform better than the direct measurements",
    "checked": true,
    "id": "9e3b686c1443d8e9d263de95cd46fcd9c825edd8",
    "semantic_title": "an investigation on speaker specific articulatory synthesis with speaker independent articulatory inversion",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19_interspeech.html": {
    "title": "Individual Difference of Relative Tongue Size and its Acoustic Effects",
    "volume": "main",
    "abstract": "This study examines how the speaker's tongue size contributes to generating dynamic characteristics of speaker individuality. The relative tongue size (RTS) has been proposed as an index for the tongue area within the oropharyngeal cavity on the midsagittal magnetic resonance imaging (MRI). Our earlier studies have shown that the smaller the RTS, the faster the tongue movement. In this study, acoustic consequences of individual RTS values were analyzed by comparing tongue movement velocity and formant transition rate. The materials used were cine-MRI data and acoustic signals during production of a sentence and two words produced by two female speakers with contrasting RTS values. The results indicate that the speaker with the small RTS value exhibited the faster changes of tongue positions and formant transitions than the speakers with the large RTS values. Since the tongue size is uncontrollable by a speaker's intention, the RTS can be regarded as one of the causal factors of dynamic individual characteristics in the lower frequency region of speech signals",
    "checked": true,
    "id": "68d0c775c0c976117da4b914321e89e5cfa9a717",
    "semantic_title": "individual difference of relative tongue size and its acoustic effects",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yoshinaga19_interspeech.html": {
    "title": "Individual Differences of Airflow and Sound Generation in the Vocal Tract of Sibilant /s/",
    "volume": "main",
    "abstract": "To clarify the individual differences of flow and sound characteristics of sibilant /s/, the large eddy simulation of compressible flow was applied to vocal tract geometries of five subjects pronouncing /s/. The vocal tract geometry was extracted by separately collecting images of digital dental casts and the vocal tract of /s/. The computational grids were constructed for each geometry, and flow and acoustic fields were predicted by the simulation. Results of the simulation showed that jet flow in the vocal tract was disturbed and fluctuated, and the sound source of /s/ was generated in different place for each subject. With an increment of the jet velocity, not only the overall sound amplitude but also the spectral mean was increased, indicating that the increment of the jet velocity contributes to the increase of amplitudes in a higher frequency range among different vocal tract geometries",
    "checked": true,
    "id": "7b273a1c00e585e0677ed99968cf1c18ecbe706f",
    "semantic_title": "individual differences of airflow and sound generation in the vocal tract of sibilant /s/",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/uttam19_interspeech.html": {
    "title": "Hush-Hush Speak: Speech Reconstruction Using Silent Videos",
    "volume": "main",
    "abstract": "Speech Reconstruction is the task of recreation of speech using silent videos as input. In the literature, it is also referred to as lipreading. In this paper, we design an encoder-decoder architecture which takes silent videos as input and outputs an audio spectrogram of the reconstructed speech. The model, despite being a speaker-independent model, achieves comparable results on speech reconstruction to the current state-of-the-art speaker-dependent model. We also perform user studies to infer speech intelligibility. Additionally, we test the usability of the trained model using bilingual speech",
    "checked": true,
    "id": "ec9fda5db1ced6058824ce0c8fe9cb6e1d20c24e",
    "semantic_title": "hush-hush speak: speech reconstruction using silent videos",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/saha19_interspeech.html": {
    "title": "SPEAK YOUR MIND! Towards Imagined Speech Recognition with Hierarchical Deep Learning",
    "volume": "main",
    "abstract": "Speech-related Brain Computer Interface (BCI) technologies provide effective vocal communication strategies for controlling devices through speech commands interpreted from brain signals. In order to infer imagined speech from active thoughts, we propose a novel hierarchical deep learning BCI system for subject-independent classification of 11 speech tokens including phonemes and words. Our novel approach exploits predicted articulatory information of six phonological categories (e.g., nasal, bilabial) as an intermediate step for classifying the phonemes and words, thereby finding discriminative signal responsible for natural speech synthesis. The proposed network is composed of hierarchical combination of spatial and temporal CNN cascaded with a deep autoencoder. Our best models on the KARA database achieve an average accuracy of 83.42% across the six different binary phonological classification tasks, and 53.36% for the individual token identification task, significantly outperforming our baselines. Ultimately, our work suggests the possible existence of a brain imagery footprint for the underlying articulatory movement related to different sounds that can be used to aid imagined speech decoding",
    "checked": true,
    "id": "fb2029d6587f3099b9b62b3abc601c64bd48fefc",
    "semantic_title": "speak your mind! towards imagined speech recognition with hierarchical deep learning",
    "citation_count": 31
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chung19_interspeech.html": {
    "title": "An Unsupervised Autoregressive Model for Speech Representation Learning",
    "volume": "main",
    "abstract": "This paper proposes a novel unsupervised autoregressive neural model for learning generic speech representations. In contrast to other speech representation learning methods that aim to remove noise or speaker variabilities, ours is designed to preserve information for a wide range of downstream tasks. In addition, the proposed model does not require any phonetic or word boundary labels, allowing the model to benefit from large quantities of unlabeled data. Speech representations learned by our model significantly improve performance on both phone classification and speaker verification over the surface features and other supervised and unsupervised approaches. Further analysis shows that different levels of speech information are captured by our model at different layers. In particular, the lower layers tend to be more discriminative for speakers, while the upper layers provide more phonetic content",
    "checked": true,
    "id": "2f803165d054ee89bec2401368ceb9e75bad8b60",
    "semantic_title": "an unsupervised autoregressive model for speech representation learning",
    "citation_count": 322
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19_interspeech.html": {
    "title": "Harmonic-Aligned Frame Mask Based on Non-Stationary Gabor Transform with Application to Content-Dependent Speaker Comparison",
    "volume": "main",
    "abstract": "We propose harmonic-aligned frame mask for speech signals using non-stationary Gabor transform (NSGT). A frame mask operates on the transfer coefficients of a signal and consequently converts the signal into a counterpart signal. It depicts the difference between the two signals. In preceding studies, frame masks based on regular Gabor transform were applied to single-note instrumental sound analysis. This study extends the frame mask approach to speech signals. For voiced speech, the fundamental frequency is usually changing consecutively over time. We employ NSGT with pitch-dependent and therefore time-varying frequency resolution to attain harmonic alignment in the transform domain and hence yield harmonic-aligned frame masks for speech signals. We propose to apply the harmonic-aligned frame mask to content-dependent speaker comparison. Frame masks, computed from voiced signals of a same vowel but from different speakers, were utilized as similarity measures to compare and distinguish the speaker identities (SID). Results obtained with deep neural networks demonstrate that the proposed frame mask is valid in representing speaker characteristics and shows a potential for SID applications in limited data scenarios",
    "checked": true,
    "id": "7cdcd85e2e9e44c5b1040079313d48886ce27553",
    "semantic_title": "harmonic-aligned frame mask based on non-stationary gabor transform with application to content-dependent speaker comparison",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/m19_interspeech.html": {
    "title": "Glottal Closure Instants Detection from Speech Signal by Deep Features Extracted from Raw Speech and Linear Prediction Residual",
    "volume": "main",
    "abstract": "Glottal closure instants (GCI) also called as instants of significant excitation occur during abrupt closure of vocal folds is a well-studied problem for its many potential applications in speech processing. Speech signal or its transformed linear prediction residual (LPR) is the most popular signal representations for GCI detection. In this paper, we propose a supervised classification based GCI detection method, in which, we train multiple convolution neural networks to determine the suitable feature representation for efficient GCI detection. Also, we show that the combined model trained with joint acoustic-residual deep features and the model trained with low pass filtered speech significantly increases the detection accuracy. We have manually annotated the speech signal for ground truth GCI using electroglottograph (EGG) as a reference signal. The evaluation results showed that the proposed model trained with very small and less diverse data performs significantly better than the traditional signal processing and most recent data-driven approaches",
    "checked": true,
    "id": "bf70044aefbf7c8433e93116f1ed9a4e85ca669c",
    "semantic_title": "glottal closure instants detection from speech signal by deep features extracted from raw speech and linear prediction residual",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pascual19_interspeech.html": {
    "title": "Learning Problem-Agnostic Speech Representations from Multiple Self-Supervised Tasks",
    "volume": "main",
    "abstract": "Learning good representations without supervision is still an open issue in machine learning, and is particularly challenging for speech signals, which are often characterized by long sequences with a complex hierarchical structure. Some recent works, however, have shown that it is possible to derive useful speech representations by employing a self-supervised encoder-discriminator approach. This paper proposes an improved self-supervised method, where a single neural encoder is followed by multiple workers that jointly solve different self-supervised tasks. The needed consensus across different tasks naturally imposes meaningful constraints to the encoder, contributing to discover general representations and to minimize the risk of learning superficial ones. Experiments show that the proposed approach can learn transferable, robust, and problem-agnostic features that carry on relevant information from the speech signal, such as speaker identity, phonemes, and even higher-level features such as emotional cues. In addition, a number of design choices make the encoder easily exportable, facilitating its direct usage or adaptation to different problems",
    "checked": true,
    "id": "b3beb9bc7395a8a489b9c64c46329a84d45968bd",
    "semantic_title": "learning problem-agnostic speech representations from multiple self-supervised tasks",
    "citation_count": 197
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nellore19_interspeech.html": {
    "title": "Excitation Source and Vocal Tract System Based Acoustic Features for Detection of Nasals in Continuous Speech",
    "volume": "main",
    "abstract": "The aim of the current study is to propose acoustic features for detection of nasals in continuous speech. Acoustic features that represent certain characteristics of speech production are extracted. Features representing excitation source characteristics are extracted using zero frequency filtering method. Features representing vocal tract system characteristics are extracted using zero time windowing method Feature sets are formed by combining certain subsets of the features mentioned above. These feature sets are evaluated for their representativeness of nasals in continuous speech in three different languages, namely, English, Hindi and Telugu. Results show that nasal detection is reliable and consistent across all the languages mentioned above",
    "checked": true,
    "id": "997642aa05d29b542130ef90653e306fae7c8260",
    "semantic_title": "excitation source and vocal tract system based acoustic features for detection of nasals in continuous speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chatziagapi19_interspeech.html": {
    "title": "Data Augmentation Using GANs for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "In this work, we address the problem of data imbalance for the task of Speech Emotion Recognition (SER). We investigate conditioned data augmentation using Generative Adversarial Networks (GANs), in order to generate samples for underrepresented emotions. We adapt and improve a conditional GAN architecture to generate synthetic spectrograms for the minority class. For comparison purposes, we implement a series of signal-based data augmentation methods. The proposed GAN-based approach is evaluated on two datasets, namely IEMOCAP and FEEL-25k, a large multi-domain dataset. Results demonstrate a 10% relative performance improvement in IEMOCAP and 5% in FEEL-25k, when augmenting the minority classes",
    "checked": true,
    "id": "f3c25b8aa1f7e2a00b947c38d52dcaa6b3da31bd",
    "semantic_title": "data augmentation using gans for speech emotion recognition",
    "citation_count": 82
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kons19_interspeech.html": {
    "title": "High Quality, Lightweight and Adaptable TTS Using LPCNet",
    "volume": "main",
    "abstract": "We present a lightweight adaptable neural TTS system with high quality output. The system is composed of three separate neural network blocks: prosody prediction, acoustic feature prediction and Linear Prediction Coding Net as a neural vocoder. This system can synthesize speech with close to natural quality while running 3 times faster than real-time on a standard CPU The modular setup of the system allows for simple adaptation to new voices with a small amount of data We first demonstrate the ability of the system to produce high quality speech when trained on large, high quality datasets. Following that, we demonstrate its adaptability by mimicking unseen voices using 5 to 20 minutes long datasets with lower recording quality. Large scale Mean Opinion Score quality and similarity tests are presented, showing that the system can adapt to unseen voices with quality gap of 0.12 and similarity gap of 3% compared to natural speech for male voices and quality gap of 0.35 and similarity of gap of 9% for female voices",
    "checked": true,
    "id": "52daae0ff7d09d4b11ab447fa0cb57e2ba1d12b6",
    "semantic_title": "high quality, lightweight and adaptable tts using lpcnet",
    "citation_count": 46
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lorenzotrueba19_interspeech.html": {
    "title": "Towards Achieving Robust Universal Neural Vocoding",
    "volume": "main",
    "abstract": "This paper explores the potential universality of neural vocoders. We train a WaveRNN-based vocoder on 74 speakers coming from 17 languages. This vocoder is shown to be capable of generating speech of consistently good quality (98% relative mean MUSHRA when compared to natural speech) regardless of whether the input spectrogram comes from a speaker or style seen during training or from an out-of-domain scenario when the recording conditions are studio-quality. When the recordings show significant changes in quality, or when moving towards non-speech vocalizations or singing, the vocoder still significantly outperforms speaker-dependent vocoders, but operates at a lower average relative MUSHRA of 75%. These results are shown to be consistent across languages, regardless of them being seen during training (e.g. English or Japanese) or unseen (e.g. Wolof, Swahili, Ahmaric)",
    "checked": true,
    "id": "93d857da76fdeeece7ed641f4d48e1e9770e8315",
    "semantic_title": "towards achieving robust universal neural vocoding",
    "citation_count": 78
  },
  "https://www.isca-speech.org/archive/interspeech_2019/neekhara19_interspeech.html": {
    "title": "Expediting TTS Synthesis with Adversarial Vocoding",
    "volume": "main",
    "abstract": "Recent approaches in text-to-speech (TTS) synthesis employ neural network strategies to vocode perceptually-informed spectrogram representations directly into listenable waveforms. Such vocoding procedures create a computational bottleneck in modern TTS pipelines. We propose an alternative approach which utilizes generative adversarial networks (GANs) to learn mappings from perceptually-informed spectrograms to simple magnitude spectrograms which can be heuristically vocoded. Through a user study, we show that our approach significantly outperforms naïve vocoding strategies while being hundreds of times faster than neural network vocoders used in state-of-the-art TTS systems. We also show that our method can be used to achieve state-of-the-art results in unsupervised synthesis of individual words of speech",
    "checked": true,
    "id": "3b978703968c2e3f8a41b0d34f870bfc2228677f",
    "semantic_title": "expediting tts synthesis with adversarial vocoding",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mustafa19_interspeech.html": {
    "title": "Analysis by Adversarial Synthesis — A Novel Approach for Speech Vocoding",
    "volume": "main",
    "abstract": "Classical parametric speech coding techniques provide a compact representation for speech signals. This affords a very low transmission rate but with a reduced perceptual quality of the reconstructed signals. Recently, autoregressive deep generative models such as WaveNet and SampleRNN have been used as speech vocoders to scale up the perceptual quality of the reconstructed signals without increasing the coding rate. However, such models suffer from a very slow signal generation mechanism due to their sample-by-sample modelling approach. In this work, we introduce a new methodology for neural speech vocoding based on generative adversarial networks (GANs). A fake speech signal is generated from a very compressed representation of the glottal excitation using conditional GANs as a deep generative model. This fake speech is then refined using the LPC parameters of the original speech signal to obtain a natural reconstruction. The reconstructed speech waveforms based on this approach show a higher perceptual quality than the classical vocoder counterparts according to subjective and objective evaluation scores for a dataset of 30 male and female speakers. Moreover, the usage of GANs enables to generate signals in one-shot compared to autoregressive generative models. This makes GANs promising for exploration to implement high-quality neural vocoders",
    "checked": true,
    "id": "07a9dc45559f8c4cc2a8740e36e18f29db8e7a3d",
    "semantic_title": "analysis by adversarial synthesis - a novel approach for speech vocoding",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wu19b_interspeech.html": {
    "title": "Quasi-Periodic WaveNet Vocoder: A Pitch Dependent Dilated Convolution Model for Parametric Speech Generation",
    "volume": "main",
    "abstract": "In this paper, we propose a quasi-periodic neural network (QPNet) vocoder with a novel network architecture named pitch-dependent dilated convolution (PDCNN) to improve the pitch controllability of WaveNet (WN) vocoder. The effectiveness of the WN vocoder to generate high-fidelity speech samples from given acoustic features has been proved recently. However, because of the fixed dilated convolution and generic network architecture, the WN vocoder hardly generates speech with given F values which are outside the range observed in training data. Consequently, the WN vocoder lacks the pitch controllability which is one of the essential capabilities of conventional vocoders. To address this limitation, we propose the PDCNN component which has the time-variant adaptive dilation size related to the given F values and a cascade network structure of the QPNet vocoder to generate quasi-periodic signals such as speech. Both objective and subjective tests are conducted, and the experimental results demonstrate the better pitch controllability of the QPNet vocoder compared to the same and double sized WN vocoders while attaining comparable speech qualities",
    "checked": true,
    "id": "ff01789535aa1535f610739176083f17c6239d2f",
    "semantic_title": "quasi-periodic wavenet vocoder: a pitch dependent dilated convolution model for parametric speech generation",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tian19_interspeech.html": {
    "title": "A Speaker-Dependent WaveNet for Voice Conversion with Non-Parallel Data",
    "volume": "main",
    "abstract": "In a typical voice conversion system, vocoder is commonly used for speech-to-features analysis and features-to-speech synthesis. However, vocoder can be a source of speech quality degradation. This paper presents a novel approach to voice conversion using WaveNet for non-parallel training data. Instead of reconstructing speech with intermediate features, the proposed approach utilizes the WaveNet to map the Phonetic PosteriorGrams (PPGs) to the waveform samples directly. In this way, we avoid the estimation errors arising from vocoding and feature conversion. Additionally, as PPG is assumed to be speaker independent, the proposed approach also reduces the feature mismatch problem in WaveNet vocoder based solutions. Experimental results conducted on the CMU-ARCTIC database show that the proposed approach significantly outperforms the traditional vocoder and WaveNet Vocoder baselines in terms of speech quality",
    "checked": true,
    "id": "b8360a4ec8d54f5bbc4b8081730a47b9c7fe026e",
    "semantic_title": "a speaker-dependent wavenet for voice conversion with non-parallel data",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2019/han19_interspeech.html": {
    "title": "Survey Talk: When Attention Meets Speech Applications: Speech & Speaker Recognition Perspective",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "984a38f397b3211963b36e2049b945afaadaebbf",
    "semantic_title": "survey talk: when attention meets speech applications: speech & speaker recognition perspective",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhao19_interspeech.html": {
    "title": "Attention-Enhanced Connectionist Temporal Classification for Discrete Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Discrete speech emotion recognition (SER), the assignment of a single emotion label to an entire speech utterance, is typically performed as a sequence-to-label task. This approach, however, is limited, in that it can result in models that do not capture temporal changes in the speech signal, including those indicative of a particular emotion. One potential solution to overcome this limitation is to model SER as a sequence-to-sequence task instead. In this regard, we have developed an attention-based bidirectional long short-term memory (BLSTM) neural network in combination with a connectionist temporal classification (CTC) objective function (Attention-BLSTM-CTC) for SER. We also assessed the benefits of incorporating two contemporary attention mechanisms, namely component attention and quantum attention, into the CTC framework. To the best of the authors' knowledge, this is the first time that such a hybrid architecture has been employed for SER.We demonstrated the effectiveness of our approach on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) and FAU-Aibo Emotion corpora. The experimental results demonstrate that our proposed model outperforms current state-of-the-art approaches",
    "checked": true,
    "id": "920f779bef257922d7685244f1a7afc1e4d6ad86",
    "semantic_title": "attention-enhanced connectionist temporal classification for discrete speech emotion recognition",
    "citation_count": 45
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19b_interspeech.html": {
    "title": "Attentive to Individual: A Multimodal Emotion Recognition Network with Personalized Attention Profile",
    "volume": "main",
    "abstract": "A growing number of human-centered applications benefit from continuous advancements in the emotion recognition technology. Many emotion recognition algorithms have been designed to model multimodal behavior cues to achieve high performances. However, most of them do not consider the modulating factors of an individual's personal attributes in his/her expressive behaviors. In this work, we propose a Personalized Attributes-Aware Attention Network (PAaAN) with a novel personalized attention mechanism to perform emotion recognition using speech and language cues. The attention profile is learned from embeddings of an individual's profile, acoustic, and lexical behavior data. The profile embedding is derived using linguistics inquiry word count computed between the target speaker and a large set of movie scripts. Our method achieves the state-of-the-art 70.3% unweighted accuracy in a four class emotion recognition task on the IEMOCAP. Further analysis reveals that affect-related semantic categories are emphasized differently for each speaker in the corpus showing the effectiveness of our attention mechanism for personalization",
    "checked": true,
    "id": "1515d8597ef03ff52cda5bc3551fbdee6350c2bd",
    "semantic_title": "attentive to individual: a multimodal emotion recognition network with personalized attention profile",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gallardoantolin19_interspeech.html": {
    "title": "A Saliency-Based Attention LSTM Model for Cognitive Load Classification from Speech",
    "volume": "main",
    "abstract": "Cognitive Load (CL) refers to the amount of mental demand that a given task imposes on an individual's cognitive system and it can affect his/her productivity in very high load situations. In this paper, we propose an automatic system capable of classifying the CL level of a speaker by analyzing his/her voice. Our research on this topic goes into two main directions. In the first one, we focus on the use of Long Short-Term Memory (LSTM) networks with different weighted pooling strategies for CL level classification. In the second contribution, for overcoming the need of a large amount of training data, we propose a novel attention mechanism that uses the Kalinli's auditory saliency model. Experiments show that our proposal outperforms significantly both, a baseline system based on Support Vector Machines (SVM) and a LSTM-based system with logistic regression attention model",
    "checked": true,
    "id": "af4f0d9acf3841703855891c12ce24f6b41da608",
    "semantic_title": "a saliency-based attention lstm model for cognitive load classification from speech",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mallolragolta19_interspeech.html": {
    "title": "A Hierarchical Attention Network-Based Approach for Depression Detection from Transcribed Clinical Interviews",
    "volume": "main",
    "abstract": "The high prevalence of depression in society has given rise to a need for new digital tools that can aid its early detection. Among other effects, depression impacts the use of language. Seeking to exploit this, this work focuses on the detection of depressed and non-depressed individuals through the analysis of linguistic information extracted from transcripts of clinical interviews with a virtual agent. Specifically, we investigated the advantages of employing hierarchical attention-based networks for this task. Using Global Vectors (GloVe) pretrained word embedding models to extract low-level representations of the words, we compared hierarchical local-global attention networks and hierarchical contextual attention networks. We performed our experiments on the Distress Analysis Interview Corpus - Wizard of Oz (DAIC-WoZ) dataset, which contains audio, visual, and linguistic information acquired from participants during a clinical session. Our results using the DAIC-WoZ test set indicate that hierarchical contextual attention networks are the most suitable configuration to detect depression from transcripts. The configuration achieves an Unweighted Average Recall (UAR) of .66 using the test set, surpassing our baseline, a Recurrent Neural Network that does not use attention",
    "checked": true,
    "id": "8db761dc173e30b0882390892fe92af7acd11208",
    "semantic_title": "a hierarchical attention network-based approach for depression detection from transcribed clinical interviews",
    "citation_count": 30
  },
  "https://www.isca-speech.org/archive/interspeech_2019/carmantini19_interspeech.html": {
    "title": "Untranscribed Web Audio for Low Resource Speech Recognition",
    "volume": "main",
    "abstract": "Speech recognition models are highly susceptible to mismatch in the acoustic and language domains between the training and the evaluation data. For low resource languages, it is difficult to obtain transcribed speech for target domains, while untranscribed data can be collected with minimal effort. Recently, a method applying lattice-free maximum mutual information (LF-MMI) to untranscribed data has been found to be effective for semi-supervised training. However, weaker initial models and domain mismatch can result in high deletion rates for the semi-supervised model. Therefore, we propose a method to force the base model to overgenerate possible transcriptions, relying on the ability of LF-MMI to deal with uncertainty. On data from the IARPA MATERIAL programme, our new semi-supervised method outperforms the standard semi-supervised method, yielding significant gains when adapting for mismatched bandwidth and domain",
    "checked": true,
    "id": "07a873b8a6c00dc72978ef7e98160b3e245c4bca",
    "semantic_title": "untranscribed web audio for low resource speech recognition",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/luscher19_interspeech.html": {
    "title": "RWTH ASR Systems for LibriSpeech: Hybrid vs Attention",
    "volume": "main",
    "abstract": "We present state-of-the-art automatic speech recognition (ASR) systems employing a standard hybrid DNN/HMM architecture compared to an attention-based encoder-decoder design for the LibriSpeech task. Detailed descriptions of the system development, including model design, pretraining schemes, training schedules, and optimization approaches are provided for both system architectures. Both hybrid DNN/HMM and attention-based systems employ bi-directional LSTMs for acoustic modeling/encoding. For language modeling, we employ both LSTM and Transformer based architectures. All our systems are built using RWTH's open-source toolkits RASR and RETURNN. To the best knowledge of the authors, the results obtained when training on the full LibriSpeech training set, are the best published currently, both for the hybrid DNN/HMM and the attention-based systems. Our single hybrid system even outperforms previous results obtained from combining eight single systems. Our comparison shows that on the LibriSpeech 960h task, the hybrid DNN/HMM system outperforms the attention-based system by 15% relative on the clean and 40% relative on the other test sets in terms of word error rate. Moreover, experiments on a reduced 100h-subset of the LibriSpeech training corpus even show a more pronounced margin between the hybrid DNN/HMM and attention-based architectures",
    "checked": true,
    "id": "744196b6cb5091c0760d05ef068a92a6cd531587",
    "semantic_title": "rwth asr systems for librispeech: hybrid vs attention - w/o data augmentation",
    "citation_count": 211
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kanda19_interspeech.html": {
    "title": "Auxiliary Interference Speaker Loss for Target-Speaker Speech Recognition",
    "volume": "main",
    "abstract": "In this paper, we propose a novel auxiliary loss function for target-speaker automatic speech recognition (ASR). Our method automatically extracts and transcribes target speaker's utterances from a monaural mixture of multiple speakers speech given a short sample of the target speaker. The proposed auxiliary loss function attempts to additionally maximize interference speaker ASR accuracy during training. This will regularize the network to achieve a better representation for speaker separation, thus achieving better accuracy on the target-speaker ASR. We evaluated our proposed method using two-speaker-mixed speech in various signal-to-interference-ratio conditions. We first built a strong target-speaker ASR baseline based on the state-of-the-art lattice-free maximum mutual information. This baseline achieved a word error rate (WER) of 18.06% on the test set while a normal ASR trained with clean data produced a completely corrupted result (WER of 84.71%). Then, our proposed loss further reduced the WER by 6.6% relative to this strong baseline, achieving a WER of 16.87%. In addition to the accuracy improvement, we also showed that the auxiliary output branch for the proposed loss can even be used for a secondary ASR for interference speakers' speech",
    "checked": true,
    "id": "388d41b99c9c0867301f345c65877a2796225ead",
    "semantic_title": "auxiliary interference speaker loss for target-speaker speech recognition",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2019/meng19_interspeech.html": {
    "title": "Speaker Adaptation for Attention-Based End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "We propose three regularization-based speaker adaptation approaches to adapt the attention-based encoder-decoder (AED) model with very limited adaptation data from target speakers for end-to-end automatic speech recognition. The first method is Kullback-Leibler divergence (KLD) regularization, in which the output distribution of a speaker-dependent (SD) AED is forced to be close to that of the speaker-independent (SI) model by adding a KLD regularization to the adaptation criterion. To compensate for the asymmetric deficiency in KLD regularization, an adversarial speaker adaptation (ASA) method is proposed to regularize the deep-feature distribution of the SD AED through the adversarial learning of an auxiliary discriminator and the SD AED. The third approach is the multi-task learning, in which an SD AED is trained to jointly perform the primary task of predicting a large number of output units and an auxiliary task of predicting a small number of output units to alleviate the target sparsity issue. Evaluated on a Microsoft short message dictation task, all three methods are highly effective in adapting the AED model, achieving up to 12.2% and 3.0% word error rate improvement over an SI AED trained from 3400 hours data for supervised and unsupervised adaptation, respectively",
    "checked": true,
    "id": "ce77b9212751e1afd10c7fccd5271a806dfbb445",
    "semantic_title": "speaker adaptation for attention-based end-to-end speech recognition",
    "citation_count": 29
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19_interspeech.html": {
    "title": "Large Margin Training for Attention Based End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end speech recognition systems are typically evaluated using the maximum a posterior criterion. Since only one hypothesis is involved during evaluation, the ideal number of hypotheses for training should also be one. In this study, we propose a large margin training scheme for attention based end-to-end speech recognition. Using only one training hypothesis, the large margin training strategy achieves the same performance as the minimum word error rate criterion using four hypotheses. The theoretical derivation in this study is widely applicable to other sequence discriminative criteria such as maximum mutual information. In addition, this paper provides a more succinct formulation of the large margin concept, paving the road towards a better combination of support vector machine and deep neural network",
    "checked": true,
    "id": "72abad6cd58731dafa8e7e35b2ce7192f8f6fc72",
    "semantic_title": "large margin training for attention based end-to-end speech recognition",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mac19_interspeech.html": {
    "title": "Large-Scale Mixed-Bandwidth Deep Neural Network Acoustic Modeling for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "In automatic speech recognition (ASR), wideband (WB) and narrowband (NB) speech signals with different sampling rates typically use separate acoustic models. Therefore mixed-bandwidth (MB) acoustic modeling has important practical values for ASR system deployment. In this paper, we extensively investigate large-scale MB deep neural network acoustic modeling for ASR using 1,150 hours of WB data and 2,300 hours of NB data. We study various MB strategies including downsampling, upsampling and bandwidth extension for MB acoustic modeling and evaluate their performance on 8 diverse WB and NB test sets from various application domains. To deal with the large amounts of training data, distributed training is carried out on multiple GPUs using synchronous data parallelism",
    "checked": true,
    "id": "2804c86dc045206d9759f4f9813e5b66cdbb2771",
    "semantic_title": "large-scale mixed-bandwidth deep neural network acoustic modeling for automatic speech recognition",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/milde19_interspeech.html": {
    "title": "SparseSpeech: Unsupervised Acoustic Unit Discovery with Memory-Augmented Sequence Autoencoders",
    "volume": "main",
    "abstract": "We propose a sparse sequence autoencoder model for unsupervised acoustic unit discovery, based on bidirectional LSTM encoders/decoders with a sparsity-inducing bottleneck. The sparsity layer is based on memory-augmented neural networks, with a differentiable embedding memory bank addressed from the encoder. The decoder reconstructs the encoded input feature sequence from an utterance-level context embedding and the bottleneck representation. At some time steps, the input to the decoder is randomly omitted by applying sequence dropout, forcing the decoder to learn about the temporal structure of the sequence. We propose a bootstrapping training procedure, after which the network can be trained end-to-end with standard back-propagation. Sparsity of the generated representation can be controlled with a parameter in the proposed loss function. We evaluate the units with the ABX discriminability on minimal triphone pairs and also on entire words. Forcing the network to favor highly sparse memory addressings in the memory component yields symbolic-like representations of speech that are very compact and still offer better ABX discriminability than MFCC",
    "checked": true,
    "id": "a98badb3e7503d7be09c01a43f85429505d4c907",
    "semantic_title": "sparsespeech: unsupervised acoustic unit discovery with memory-augmented sequence autoencoders",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ondel19_interspeech.html": {
    "title": "Bayesian Subspace Hidden Markov Model for Acoustic Unit Discovery",
    "volume": "main",
    "abstract": "This work tackles the problem of learning a set of language specific acoustic units from unlabeled speech recordings given a set of labeled recordings from other languages. Our approach may be described by the following two steps procedure: first the model learns the notion of acoustic units from the labelled data and then the model uses its knowledge to find new acoustic units on the target language. We implement this process with the Bayesian Subspace Hidden Markov Model (SHMM), a model akin to the Subspace Gaussian Mixture Model (SGMM) where each low dimensional embedding represents an acoustic unit rather than just a HMM's state. The subspace is trained on 3 languages from the GlobalPhone corpus (German, Polish and Spanish) and the AUs are discovered on the TIMIT corpus. Results, measured in equivalent Phone Error Rate, show that this approach significantly outperforms previous HMM based acoustic units discovery systems and compares favorably with the Variational Auto Encoder-HMM",
    "checked": true,
    "id": "57d1734db27c6ce1aae420d56182a4dab9f4fa5c",
    "semantic_title": "bayesian subspace hidden markov model for acoustic unit discovery",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2019/higuchi19_interspeech.html": {
    "title": "Speaker Adversarial Training of DPGMM-Based Feature Extractor for Zero-Resource Languages",
    "volume": "main",
    "abstract": "We propose a novel framework for extracting speaker-invariant features for zero-resource languages. A deep neural network (DNN)-based acoustic model is normalized against speakers via adversarial training: a multi-task learning process trains a shared bottleneck feature to be discriminative to phonemes and independent of speakers. However, owing to the absence of phoneme labels, zero-resource languages cannot employ adversarial multi-task (AMT) learning for speaker normalization. In this work, we obtain a posteriorgram from a Dirichlet process Gaussian mixture model (DPGMM) and utilize the posterior vector for supervision of the phoneme estimation in the AMT training. The AMT network is designed so that the DPGMM posteriorgram itself is embedded in a speaker-invariant feature space. The proposed network is expected to resolve the potential problem that the posteriorgram may lack reliability as a phoneme representation if the DPGMM components are intermingled with phoneme and speaker information. Based on the Zero Resource Speech Challenges, we conduct phoneme discriminant experiments on the extracted features. The results of the experiments show that the proposed framework extracts discriminative features, suppressing the variety in speakers",
    "checked": true,
    "id": "a62a5322c25a9074fd2499c88a46690afd176755",
    "semantic_title": "speaker adversarial training of dpgmm-based feature extractor for zero-resource languages",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/prasad19_interspeech.html": {
    "title": "Building Large-Vocabulary ASR Systems for Languages Without Any Audio Training Data",
    "volume": "main",
    "abstract": "When building automatic speech recognition (ASR) systems, typically some amount of audio and text data in the target language is needed. While text data can be obtained relatively easily across many languages, transcribed audio data is challenging to obtain. This presents a barrier to making voice technologies available in more languages of the world. In this paper, we present a way to build an ASR system system for a language even in the absence of any audio training data in that language at all. We do this by simply re-using an existing acoustic model from a phonologically similar language, without any kind of modification or adaptation towards the target language. The basic insight is that, if two languages are sufficiently similar in terms of their phonological system, an acoustic model should hold up relatively well when used for another language. We describe how we tailor our pronunciation models to enable such re-use, and show experimental results across a number of languages from various language families. We also provide a theoretical analysis of situations in which this approach is likely to work. Our results show that it is possible to achieve less than 20% word error rate (WER) using this method",
    "checked": true,
    "id": "6a3ea42e8cd381e3ed95a0d4d965409172728aa1",
    "semantic_title": "building large-vocabulary asr systems for languages without any audio training data",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2019/azuh19_interspeech.html": {
    "title": "Towards Bilingual Lexicon Discovery From Visually Grounded Speech Audio",
    "volume": "main",
    "abstract": "In this paper, we present a method for the discovery of word-like units and their approximate translations from visually grounded speech across multiple languages. We first train a neural network model to map images and their spoken audio captions in both English and Hindi to a shared, multimodal embedding space. Next, we use this model to segment and cluster regions of the spoken captions which approximately correspond to words. Finally, we exploit between-cluster similarities in the embedding space to associate English pseudo-word clusters with Hindi pseudo-word clusters, and show that many of these cluster pairings capture semantic translations between English and Hindi words. We present quantitative cross-lingual clustering results, as well as qualitative results in the form of a bilingual picture dictionary",
    "checked": true,
    "id": "7ab9392167bdbaa272c95178d50fb08bcfba7148",
    "semantic_title": "towards bilingual lexicon discovery from visually grounded speech audio",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/feng19_interspeech.html": {
    "title": "Improving Unsupervised Subword Modeling via Disentangled Speech Representation Learning and Transformation",
    "volume": "main",
    "abstract": "This study tackles unsupervised subword modeling in the zero-resource scenario, learning frame-level speech representation that is phonetically discriminative and speaker-invariant, using only untranscribed speech for target languages. Frame label acquisition is an essential step in solving this problem. High quality frame labels should be in good consistency with golden transcriptions and robust to speaker variation. We propose to improve frame label acquisition in our previously adopted deep neural network-bottleneck feature (DNN-BNF) architecture by applying the factorized hierarchical variational autoencoder (FHVAE). FHVAEs learn to disentangle linguistic content and speaker identity information encoded in speech. By discarding or unifying speaker information, speaker-invariant features are learned and fed as inputs to DPGMM frame clustering and DNN-BNF training. Experiments conducted on ZeroSpeech 2017 show that our proposed approaches achieve 2.4% and 0.6% absolute ABX error rate reductions in across- and within-speaker conditions, comparing to the baseline DNN-BNF system without applying FHVAEs. Our proposed approaches significantly outperform vocal tract length normalization in improving frame labeling and subword modeling",
    "checked": true,
    "id": "3bad6b7a7d17eaa0027bee97d6b92cbcb40a33d1",
    "semantic_title": "improving unsupervised subword modeling via disentangled speech representation learning and transformation",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nissen19_interspeech.html": {
    "title": "Listeners' Ability to Identify the Gender of Preadolescent Children in Different Linguistic Contexts",
    "volume": "main",
    "abstract": "This study evaluated listeners' ability to identify the gender of preadolescent children from speech samples of varying length and linguistic context. The listeners were presented with a total of 190 speech samples in four different categories of linguistic context: segments, words, sentences, and discourse. The listeners were instructed to evaluate each speech sample and decide whether the speaker was a male or female and rate their level of confidence in their decision. Results showed listeners identified the gender of the speakers with a high degree of accuracy, ranging from 86% to 95%. Significant differences in listener judgments were found across the four levels of linguistic context, with segments having the lowest accuracy (83%) and discourse the highest accuracy (99%). At the segmental level, the listeners' identification of each speaker's gender was greater for vowels than for fricatives, with both types of phoneme being identified at a rate well above chance. Significant differences in identification were found between the /s/ and /ʃ/ fricatives, but not between the four corner vowels. The perception of gender is likely multifactorial, with listeners possibly using phonetic, prosodic, or stylistic speech cues to determine a speaker's gender",
    "checked": true,
    "id": "f32d178eaffa86fefa8ea014fb67e5ca1b521184",
    "semantic_title": "listeners' ability to identify the gender of preadolescent children in different linguistic contexts",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ahlers19_interspeech.html": {
    "title": "Sibilant Variation in New Englishes: A Comparative Sociophonetic Study of Trinidadian and American English /s(tr)/-Retraction",
    "volume": "main",
    "abstract": "The retraction of /s/, particularly in /str/ clusters, toward [ʃ] has been investigated in British, Australian, and American English and shown to be conditioned phonetically and sociolinguistically. To date, however, no research exists on the retraction of /s/ in New Englishes, the nativized Englishes spoken in postcolonial territories like the Caribbean. We take up this research gap and present the results of a large-scale comparative acoustic analysis of /s/-retraction in Trinidadian English (TrinE) and American English (AmE), using Center of Gravity measurements of more than 23,500 sibilants produced by 181 speakers from two speech corpora The results show that, in TrinE, /str/ is considerably retracted toward [ʃtɹ], while all other /sC(r)/ clusters are non-retracted and acoustically close to singleton /s/; less retracted realizations of /str/ occur across word boundaries. Although a statistically significant contrast is overall maintained between /ʃ/ and the sibilant in /str/, there is considerable overlap across many speakers. The comparison between TrinE and AmE indicates that, while sibilants in TrinE overall show acoustically lower values, both varieties have in common that retraction is limited to /str/ contexts and significantly larger in younger speakers. The degree of /str/-retraction, however, is overall larger in TrinE than AmE",
    "checked": true,
    "id": "2ff5f37edd3cec2c2ec4405cb345ee89bf34027a",
    "semantic_title": "sibilant variation in new englishes: a comparative sociophonetic study of trinidadian and american english /s(tr)/-retraction",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gubian19_interspeech.html": {
    "title": "Tracking the New Zealand English NEAR/SQUARE Merger Using Functional Principal Components Analysis",
    "volume": "main",
    "abstract": "The focus of the study is the application of functional principal components analysis (FPCA) to a sound change in progress in which the square and near falling diphthongs are merging in New Zealand English. FPCA approximated the trajectory shapes of the first two formant frequencies (F1/F2) in a large acoustic database of read New Zealand English speech spanning three different age groups and two regions. The derived FPCA parameters showed a greater degree of centralisation and monophthongisation in square than in near. Compatibly with the evidence of an ongoing sound change in which square is shifting towards near, these shape differences were more marked for older than for younger/mid-age speakers. There was no effect of region nor of place of articulation of the preceding consonant; there was a trend for the merger to be more advanced in low frequency words. The study underlines the benefits of FPCA for quantifying the many types of sound changes involving subtle shifts in speech dynamics. In particular, multi-dimensional trajectory shape differences can be quantified without the need for vowel targets nor for determining the influence of the parameters — in this case of the first two formant frequencies — independently of each other",
    "checked": true,
    "id": "731f20b2a2efc4f4ea684a387f972a33390237d4",
    "semantic_title": "tracking the new zealand english near/square merger using functional principal components analysis",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gessinger19_interspeech.html": {
    "title": "Phonetic Accommodation in a Wizard-of-Oz Experiment: Intonation and Segments",
    "volume": "main",
    "abstract": "This paper discusses phonetic accommodation of 20 native German speakers interacting with the simulated spoken dialogue system Mirabella in a Wizard-of-Oz experiment. The study examines intonation of wh-questions and pronunciation of allophonic contrasts in German. In a question-and-answer exchange with the system, the users produce predominantly falling intonation patterns for wh-questions when the system does so as well. The number of rising patterns on the part of the users increases significantly when Mirabella produces questions with rising intonation. In a map task, Mirabella provides information about hidden items while producing variants of two allophonic contrasts which are dispreferred by the users. For the [ɪç] vs. [ɪk] contrast in the suffix ⟨-ig⟩, the number of dispreferred variants on the part of the users increases significantly during the map task. For the [εː] vs. [eː] contrast as a realization of stressed ⟨-ä-⟩, such a convergence effect is not found on the group level, yet still occurs for some individual users. Almost every user converges to the system to a substantial degree for a subset of the examined features, but we also find maintenance of preferred variants and even occasional divergence. This individual variation is in line with previous findings in accommodation research",
    "checked": true,
    "id": "3cc2acd1c36c2a4a4482cfb215f63a3cbfd374cc",
    "semantic_title": "phonetic accommodation in a wizard-of-oz experiment: intonation and segments",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/niebuhr19b_interspeech.html": {
    "title": "PASCAL and DPA: A Pilot Study on Using Prosodic Competence Scores to Predict Communicative Skills for Team Working and Public Speaking",
    "volume": "main",
    "abstract": "Strong communication skills in public-speaking and team-working exercises are associated with specific acoustic-prosodic profiles and strategies. We hypothesize that analyzing and assessing these profiles and strategies allows us to predict communicative skills. To that end, we used two analysis methods, one for charismatic and persuasive public speaking (PASCAL), and one for cooperative communication (DPA). PASCAL and DPA competency scores are determined on an acoustic basis for speech recordings of 21 students whose task was to co-create, in 7 teams of 3 students, a fully functioning weather station over 14 weeks in an Electrical Engineering project course — and to jointly write a development report about it afterwards. Results show that the students' PASCAL scores are significantly correlated with both the grade in their final oral project presentation and the grade of their written report as assessed by an independent lecturer group. The DPA scores correlate with better time-management and team working as well as with the quality and functionality of the designed product. Explanations for the links between student performance and acoustic competence scores are discussed",
    "checked": true,
    "id": "b6db8d80d41454d6a26e0ab861ea960fe73c8f51",
    "semantic_title": "pascal and dpa: a pilot study on using prosodic competence scores to predict communicative skills for team working and public speaking",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/michalsky19_interspeech.html": {
    "title": "Towards the Prosody of Persuasion in Competitive Negotiation. The Relationship Between f0 and Negotiation Success in Same Sex Sales Tasks",
    "volume": "main",
    "abstract": "Prosodic features play a key role in a speaker's persuasive power. However, previous studies on persuasion have been focused on public speaking and the signaling of leadership, while acoustic studies on negotiation have been primarily concerned with cooperative interactions. In this study we are taking a first step into investigating the role of acoustic-prosodic cues in competitive negotiation, focusing on f0 in same-sex negotiations. Specifically, we ask whether the prosodic correlates of persuasive speech are comparable for public speaking and negotiation. Sixty-two speakers (44f/18m) in 31 same-sex pairs participated in a competitive task to bargain over the selling price of a fictional company. We find a significant correlation between a speaker's f0 features and his/her interlocutor's concession range. In line with findings from public speaking, greater f0 excursions and higher f0 minima correlate with negotiation success. However, while the female speakers also show an expected elevated f0 mean, the opposite is the case for male speakers. We propose that in competitive negotiation, displaying dominance may overrule showing passion in contrast to public speaking, but only for male speakers",
    "checked": true,
    "id": "2a75a198f1b396effd78e8bcccd78c75dc62c5c4",
    "semantic_title": "towards the prosody of persuasion in competitive negotiation. the relationship between f0 and negotiation success in same sex sales tasks",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sager19_interspeech.html": {
    "title": "VESUS: A Crowd-Annotated Database to Study Emotion Production and Perception in Spoken English",
    "volume": "main",
    "abstract": "We introduce the Varied Emotion in Syntactically Uniform Speech (VESUS) repository as a new resource for the speech community. VESUS is a lexically controlled database, in which a semantically neutral script is portrayed with different emotional inflections. In total, VESUS contains over 250 distinct phrases, each read by ten actors in five emotional states. We use crowd sourcing to obtain ten human ratings for the perceived emotional content of each utterance. Our unique database construction enables a multitude of scientific and technical explorations. To jumpstart this effort, we provide benchmark performance on three distinct emotion recognition tasks using VESUS: longitudinal speaker analysis, extrapolating across syntactical complexity, and generalization to a new speaker",
    "checked": true,
    "id": "88fbd781267892f6c50d9bcdb5cfdfe7558fac9f",
    "semantic_title": "vesus: a crowd-annotated database to study emotion production and perception in spoken english",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2019/koh19_interspeech.html": {
    "title": "Building the Singapore English National Speech Corpus",
    "volume": "main",
    "abstract": "The National Speech Corpus (NSC) is the first large-scale Singapore English corpus spearheaded by the Info-communications and Media Development Authority of Singapore. It aims to become an important source of open speech data for automatic speech recognition (ASR) research and speech-related applications. The first release of the corpus features more than 2000 hours of orthographically transcribed read speech data designed with the inclusion of locally relevant words. It is available for public and commercial use upon request at \"www.imda.gov.sg/nationalspeechcorpus\", under the Singapore Open Data License. An accompanying lexicon is currently in the works and will be published soon. In addition, another 1000 hours of conversational speech data will be made available in the near future under the second release of NSC. This paper reports on the development and collection process of the read speech and conversational speech corpora",
    "checked": true,
    "id": "5d86886822404c7398edfad18b01b922eabccec2",
    "semantic_title": "building the singapore english national speech corpus",
    "citation_count": 28
  },
  "https://www.isca-speech.org/archive/interspeech_2019/picheny19_interspeech.html": {
    "title": "Challenging the Boundaries of Speech Recognition: The MALACH Corpus",
    "volume": "main",
    "abstract": "There has been huge progress in speech recognition over the last several years. Tasks once thought extremely difficult, such as SWITCHBOARD, now approach levels of human performance. The MALACH corpus (LDC catalog LDC2012S05), a 375-Hour subset of a large archive of Holocaust testimonies collected by the Survivors of the Shoah Visual History Foundation, presents significant challenges to the speech community. The collection consists of unconstrained, natural speech filled with disfluencies, heavy accents, age-related coarticulations, un-cued speaker and language switching, and emotional speech - all still open problems for speech recognition systems. Transcription is challenging even for skilled human annotators. This paper proposes that the community place focus on the MALACH corpus to develop speech recognition systems that are more robust with respect to accents, disfluencies and emotional speech. To reduce the barrier for entry, a lexicon and training and testing setups have been created and baseline results using current deep learning technologies are presented. The metadata has just been released by LDC (LDC2019S11). It is hoped that this resource will enable the community to build on top of these baselines so that the extremely important information in these and related oral histories becomes accessible to a wider audience",
    "checked": true,
    "id": "8bcbcaa3f507d1d3c26f9a378a7c1d488caacf11",
    "semantic_title": "challenging the boundaries of speech recognition: the malach corpus",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ramteke19_interspeech.html": {
    "title": "NITK Kids' Speech Corpus",
    "volume": "main",
    "abstract": "This paper introduces speech database for analyzing children's speech. The proposed database of children is recorded in Kannada language (one of the South Indian languages) from children between age 2.5 to 6.5 years. The database is named as National Institute of Technology Karnataka Kids' Speech Corpus (NITK Kids' Speech Corpus). The relevant design considerations for the database collection are discussed in detail. It is divided into four age groups with an interval of 1 year between each age group. The speech corpus includes nearly 10 hours of speech recordings from 160 children. For each age range, the data is recorded from 40 children (20 male and 20 female). Further, the effect of developmental changes on the speech from 2.5 to 6.5 years are analyzed using pitch and formant analysis. Some of the potential applications, of the NITK Kids' Speech Corpus, such as, systematic study on the language learning ability of children, phonological process analysis and children speech recognition are discussed",
    "checked": true,
    "id": "057e9df4772bc756051edaad4bcfa1068908e493",
    "semantic_title": "nitk kids' speech corpus",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ali19_interspeech.html": {
    "title": "Towards Variability Resistant Dialectal Speech Evaluation",
    "volume": "main",
    "abstract": "We study the problem of evaluating automatic speech recognition (ASR) systems that target dialectal speech input. A major challenge in this case is that the orthography of dialects is typically not standardized. From an ASR evaluation perspective, this means that there is no clear gold standard for the expected output, and several possible outputs could be considered correct according to different human annotators, which makes standard word error rate (WER) inadequate as an evaluation metric. Specifically targeting the case of Arabic dialects, which are also morphologically rich and complex, we propose a number of alternative WER-based metrics that vary in terms of text representation, including different degrees of morphological abstraction and spelling normalization.We evaluate the efficacy of these metrics by comparing their correlation with human judgments on a validation set of 1,000 utterances. Our results show that the use of morphological abstractions and spelling normalization produces systems with higher correlation with human judgment. We released the code and the datasets to the research community",
    "checked": true,
    "id": "438a0502b893be669eb80e4043bbed52835a0843",
    "semantic_title": "towards variability resistant dialectal speech evaluation",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fallgren19_interspeech.html": {
    "title": "How to Annotate 100 Hours in 45 Minutes",
    "volume": "main",
    "abstract": "Speech data found in the wild hold many advantages over artificially constructed speech corpora in terms of ecological validity and cultural worth. Perhaps most importantly, there is a lot of it. However, the combination of great quantity, noisiness and variation poses a challenge for its access and processing. Generally speaking, automatic approaches to tackle the problem require good labels for training, while manual approaches require time. In this study, we provide further evidence for a semi-supervised, human-in-the-loop framework that previously has shown promising results for browsing and annotating large quantities of found audio data quickly. The findings of this study show that a 100-hour long subset of the Fearless Steps corpus can be annotated for speech activity in less than 45 minutes, a fraction of the time it would take traditional annotation methods, without a loss in performance",
    "checked": true,
    "id": "7eae889eed59ac7813412a96b00e6a9990c8c47d",
    "semantic_title": "how to annotate 100 hours in 45 minutes",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/diez19_interspeech.html": {
    "title": "Bayesian HMM Based x-Vector Clustering for Speaker Diarization",
    "volume": "main",
    "abstract": "This paper presents a simplified version of the previously proposed diarization algorithm based on Bayesian Hidden Markov Models, which uses Variational Bayesian inference for very fast and robust clustering of x-vector (neural network based speaker embeddings). The presented results show that this clustering algorithm provides significant improvements in diarization performance as compared to the previously used Agglomerative Hierarchical Clustering. The output of this system can be further employed as an initialization for a second stage VB diarization system, using frame-wise MFCC features as input, to obtain optimal results",
    "checked": true,
    "id": "55277df8e04cc75d46470318d9ffbffe365527ee",
    "semantic_title": "bayesian hmm based x-vector clustering for speaker diarization",
    "citation_count": 60
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vestman19_interspeech.html": {
    "title": "Unleashing the Unused Potential of i-Vectors Enabled by GPU Acceleration",
    "volume": "main",
    "abstract": "Speaker embeddings are continuous-value vector representations that allow easy comparison between voices of speakers with simple geometric operations. Among others, i-vector and x-vector have emerged as the mainstream methods for speaker embedding. In this paper, we illustrate the use of modern computation platform to harness the benefit of GPU acceleration for i-vector extraction. In particular, we achieve an acceleration of 3000 times in frame posterior computation compared to real time and 25 times in training the i-vector extractor compared to the CPU baseline from Kaldi toolkit. This significant speed-up allows the exploration of ideas that were hitherto impossible. In particular, we show that it is beneficial to update the universal background model (UBM) and re-compute frame alignments while training the i-vector extractor. Additionally, we are able to study different variations of i-vector extractors more rigorously than before. In this process, we reveal some undocumented details of Kaldi's i-vector extractor and show that it outperforms the standard formulation by a margin of 1 to 2% when tested with VoxCeleb speaker verification protocol. All of our findings are asserted by ensemble averaging the results from multiple runs with random start",
    "checked": true,
    "id": "cccc0dc0167dc5a919922d5fb44c431c545e9e1f",
    "semantic_title": "unleashing the unused potential of i-vectors enabled by gpu acceleration",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shon19_interspeech.html": {
    "title": "MCE 2018: The 1st Multi-Target Speaker Detection and Identification Challenge Evaluation",
    "volume": "main",
    "abstract": "The Multi-target Challenge aims to assess how well current speech technology is able to determine whether or not a recorded utterance was spoken by one of a large number of blacklisted speakers. It is a form of multi-target speaker detection based on real-world telephone conversations. Data recordings are generated from call center customer-agent conversations. The task is to measure how accurately one can detect 1) whether a test recording is spoken by a blacklisted speaker, and 2) which specific blacklisted speaker was talking. This paper outlines the challenge and provides its baselines, results, and discussions",
    "checked": true,
    "id": "b3918fab36f106e83e016a3e33d260ad656191c4",
    "semantic_title": "mce 2018: the 1st multi-target speaker detection and identification challenge evaluation",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gao19_interspeech.html": {
    "title": "Improving Aggregation and Loss Function for Better Embedding Learning in End-to-End Speaker Verification System",
    "volume": "main",
    "abstract": "Deep embedding learning based speaker verification (SV) methods have recently achieved significant performance improvement over traditional i-vector systems, especially for short duration utterances. Embedding learning commonly consists of three components: frame-level feature processing, utterance-level embedding learning, and loss function to discriminate between speakers. For the learned embeddings, a back-end model (i.e., Linear Discriminant Analysis followed by Probabilistic Linear Discriminant Analysis (LDA-PLDA)) is generally applied as a similarity measure. In this paper, we propose to further improve the effectiveness of deep embedding learning methods in the following components: (1) A multi-stage aggregation strategy, exploited to hierarchically fuse time-frequency context information for effective frame-level feature processing. (2) A discriminant analysis loss is designed for end-to-end training, which aims to explicitly learn the discriminative embeddings, i.e. with small intra-speaker and large inter-speaker variances. To evaluate the effectiveness of the proposed improvements, we conduct extensive experiments on the VoxCeleb1 dataset. The results outperform state-of-the-art systems by a significant margin. It is also worth noting that the results are obtained using a simple cosine metric instead of the more complex LDA-PLDA backend scoring",
    "checked": true,
    "id": "547e7c3a786e4880f50f4650f1c9b08c69b08253",
    "semantic_title": "improving aggregation and loss function for better embedding learning in end-to-end speaker verification system",
    "citation_count": 54
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lin19_interspeech.html": {
    "title": "LSTM Based Similarity Measurement with Spectral Clustering for Speaker Diarization",
    "volume": "main",
    "abstract": "More and more neural network approaches have achieved considerable improvement upon submodules of speaker diarization system, including speaker change detection and segment-wise speaker embedding extraction. Still, in the clustering stage, traditional algorithms like probabilistic linear discriminant analysis (PLDA) are widely used for scoring the similarity between two speech segments. In this paper, we propose a supervised method to measure the similarity matrix between all segments of an audio recording with sequential bidirectional long short-term memory networks (Bi-LSTM). Spectral clustering is applied on top of the similarity matrix to further improve the performance. Experimental results show that our system significantly outperforms the state-of-the-art methods and achieves a diarization error rate of 6.63% on the NIST SRE 2000 CALLHOME database",
    "checked": true,
    "id": "d900a0d828cef4ea7da7082e970a7fd4119f86aa",
    "semantic_title": "lstm based similarity measurement with spectral clustering for speaker diarization",
    "citation_count": 68
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chung19b_interspeech.html": {
    "title": "Who Said That?: Audio-Visual Speaker Diarisation of Real-World Meetings",
    "volume": "main",
    "abstract": "The goal of this work is to determine ‘who spoke when' in real-world meetings. The method takes surround-view video and single or multi-channel audio as inputs, and generates robust diarisation outputs To achieve this, we propose a novel iterative approach that first enrolls speaker models using audio-visual correspondence, then uses the enrolled models together with the visual information to determine the active speaker We show strong quantitative and qualitative performance on a dataset of real-world meetings. The method is also evaluated on the public AMI meeting corpus, on which we demonstrate results that exceed all comparable methods. We also show that beamforming can be used together with the video to further improve the performance when multi-channel audio is available",
    "checked": true,
    "id": "0b04f4b0a6caeca85282c5e3baa5f24706c0cbe3",
    "semantic_title": "who said that?: audio-visual speaker diarisation of real-world meetings",
    "citation_count": 27
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xie19_interspeech.html": {
    "title": "Multi-PLDA Diarization on Children's Speech",
    "volume": "main",
    "abstract": "Children's speech and other vocalizations pose challenges for speaker diarization. The spontaneity of kids causes rapid or delayed phonetic variations in an utterance, which makes speaker's information difficult to extract. Fast speaker turns and long overlap in conversations between children and their guardians makes correct segmentation even harder compared to, say a business meeting. In this work, we explore diarization of child-guardian interactions. We investigate the effectiveness of adding children's speech to adult data in Probabilistic Linear Discriminant Analysis (PLDA) training. We also train each of two PLDAs with separate objective to a coarse or fine classification of speakers. A fusion of the two PLDAs is examined. By performing this fusion, we expect to improve on children's speech while preserving adult segmentations. Our experimental results show that including children's speech helps reduce DER by 2.7%, achieving a best overall DER of 33.1% with the x-vector system. A fusion system yields a reasonable 33.3% DER that validates our concept",
    "checked": true,
    "id": "c04f91ed76aa8f6410312229520383db361b2a4f",
    "semantic_title": "multi-plda diarization on children's speech",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mccree19_interspeech.html": {
    "title": "Speaker Diarization Using Leave-One-Out Gaussian PLDA Clustering of DNN Embeddings",
    "volume": "main",
    "abstract": "Many modern systems for speaker diarization, such as the top-performing JHU system in the DIHARD 2018 challenge, rely on clustering of DNN speaker embeddings followed by HMM resegmentation. Two problems with this approach are that parameters need significant retuning for different applications, and that the DNN contributes only to the clustering task and not the resegmentation. This paper presents two contributions: an improved HMM segment assignment algorithm using leave-one-out Gaussian PLDA scoring, and an approach to training the DNN such that embeddings directly optimize performance of this scoring method with generatively updated PLDA parameters. Initial experiments with this new system are very promising, achieving state-of-the-art performance for two separate tasks (Callhome and DIHARD18) without any task-dependent parameter tuning",
    "checked": true,
    "id": "224d6aef7d6522f9c97bd8cea764704653df0192",
    "semantic_title": "speaker diarization using leave-one-out gaussian plda clustering of dnn embeddings",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ghahabi19_interspeech.html": {
    "title": "Speaker-Corrupted Embeddings for Online Speaker Diarization",
    "volume": "main",
    "abstract": "Speaker diarization is more challenging in presence of background noise or music, frequent speaker changes, and cross talks. In an online scenario, the decision should be made at time, given only the current short segment and the speakers detected in the past, which makes the task even harder. In this work, an online robust speaker diarization algorithm is proposed in which speech segments are represented by low dimensional vectors referred to as speaker-corrupted embeddings. The proposed speaker embedding network is a deep neural network which takes speaker-corrupted supervectors as input, uses variable ReLU (VReLU) as an activation function, and tries to discriminate the background speakers. Speaker corruption is performed by adding supervectors built by 20 speech frames from other speakers to the supervectors of a given speaker. It is shown that speaker corruption, VReLU, and input dropout increase the generalization power of the proposed network. To increase the robustness, the proposed embeddings are concatenated with LDA transformed supervectors. Experimental results on the Albayzin 2018 evaluation set show a competitive accuracy, more robustness, and much lower computational cost compared to typical offline algorithms",
    "checked": true,
    "id": "f110ea60287aaa90a86c7a256361bad209b44416",
    "semantic_title": "speaker-corrupted embeddings for online speaker diarization",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/park19_interspeech.html": {
    "title": "Speaker Diarization with Lexical Information",
    "volume": "main",
    "abstract": "This work presents a novel approach for speaker diarization to leverage lexical information provided by automatic speech recognition. We propose a speaker diarization system that can incorporate word-level speaker turn probabilities with speaker embeddings into a speaker clustering process to improve the overall diarization accuracy. To integrate lexical and acoustic information in a comprehensive way during clustering, we introduce an adjacency matrix integration for spectral clustering. Since words and word boundary information for word-level speaker turn probability estimation are provided by a speech recognition system, our proposed method works without any human intervention for manual transcriptions. We show that the proposed method improves diarization performance on various evaluation datasets compared to the baseline diarization system using acoustic information only in speaker embeddings",
    "checked": true,
    "id": "828444dfe221ac7ce5c0d1a5d7a7db7d1d78b7ce",
    "semantic_title": "speaker diarization with lexical information",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shafey19_interspeech.html": {
    "title": "Joint Speech Recognition and Speaker Diarization via Sequence Transduction",
    "volume": "main",
    "abstract": "Speech applications dealing with conversations require not only recognizing the spoken words, but also determining who spoke when. The task of assigning words to speakers is typically addressed by merging the outputs of two separate systems, namely, an automatic speech recognition (ASR) system and a speaker diarization (SD) system. The two systems are trained independently with different objective functions. Often the SD systems operate directly on the acoustics and are not constrained to respect word boundaries and this deficiency is overcome in an ad hoc manner. Motivated by recent advances in sequence to sequence learning, we propose a novel approach to tackle the two tasks by a joint ASR and SD system using a recurrent neural network transducer. Our approach utilizes both linguistic and acoustic cues to infer speaker roles, as opposed to typical SD systems, which only use acoustic cues. We evaluated the performance of our approach on a large corpus of medical conversations between physicians and patients. Compared to a competitive conventional baseline, our approach improves word-level diarization error rate from 15.8% to 2.2%",
    "checked": true,
    "id": "6632853535bd7f7f9c438d19467341f6e46a63e5",
    "semantic_title": "joint speech recognition and speaker diarization via sequence transduction",
    "citation_count": 71
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cumani19_interspeech.html": {
    "title": "Normal Variance-Mean Mixtures for Unsupervised Score Calibration",
    "volume": "main",
    "abstract": "Generative calibration models have shown to be an effective alternative to traditional discriminative score calibration techniques, such as Logistic Regression (LogReg). Provided that the score distribution assumptions are sufficiently accurate, generative approaches not only have similar or better performance with respect to LogReg, but also allow for unsupervised or semi-supervised training Recently, we have proposed non-Gaussian linear calibration models able to overcome the limitations of Gaussian approaches. Although these models allow for better characterization of score distributions, they still require the target and non-target distributions to be reciprocally symmetric In this work we further extend these models to cover asymmetric score distributions, as to improve calibration for both supervised and unsupervised scenarios. The improvements have been assessed on NIST SRE 2010 telephone data",
    "checked": true,
    "id": "364ce471c1cddcc930c6e1a88696851ae05db1c0",
    "semantic_title": "normal variance-mean mixtures for unsupervised score calibration",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yamamoto19_interspeech.html": {
    "title": "Speaker Augmentation and Bandwidth Extension for Deep Speaker Embedding",
    "volume": "main",
    "abstract": "This paper investigates a novel data augmentation approach to train deep neural networks (DNNs) used for speaker embedding, i.e. to extract representation that allows easy comparison between speaker voices with a simple geometric operation. Data augmentation is used to create new examples from an existing training set, thereby increasing the quantity of training data improves the robustness of the model. We attempt to increase the number of speakers in the training set by generating new speakers via voice conversion. This speaker augmentation expands the coverage of speakers in the embedding space in contrast to conventional audio augmentation methods which focus on within-speaker variability. With an increased number of speakers in the training set, the DNN is trained to produce a better speaker-discriminative embedding. We also advocate using bandwidth extension to augment narrowband speech for a wideband application. Text-independent speaker recognition experiments in Speakers in the Wild (SITW) demonstrate a 17.9% reduction in minimum detection cost with speaker augmentation. The combined use of the two techniques provides further improvement",
    "checked": true,
    "id": "2bda55920cdef57fa4cd3829a98d178c611a8871",
    "semantic_title": "speaker augmentation and bandwidth extension for deep speaker embedding",
    "citation_count": 49
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ylmaz19_interspeech.html": {
    "title": "Large-Scale Speaker Diarization of Radio Broadcast Archives",
    "volume": "main",
    "abstract": "This paper describes our initial efforts to build a large-scale speaker diarization (SD) and identification system on a recently digitized radio broadcast archive from the Netherlands which has more than 6500 audio tapes with 3000 hours of Frisian-Dutch speech recorded between 1950–2016. The employed large-scale diarization scheme involves two stages: (1) tape-level speaker diarization providing pseudo-speaker identities and (2) speaker linking to relate pseudo-speakers appearing in multiple tapes. Having access to the speaker models of several frequently appearing speakers from the previously collected FAME! speech corpus, we further perform speaker identification by linking these known speakers to the pseudo-speakers identified at the first stage. In this work, we present a recently created longitudinal and multilingual SD corpus designed for large-scale SD research and evaluate the performance of a new speaker linking system using x-vectors with PLDA to quantify cross-tape speaker similarity on this corpus. The performance of this speaker linking system is evaluated on a small subset of the archive which is manually annotated with speaker information. The speaker linking performance reported on this subset (53 hours) and the whole archive (3000 hours) is compared to quantify the impact of scaling up in the amount of speech data",
    "checked": true,
    "id": "8518d62a10a49cb180a1586d13c3973a36bdf81b",
    "semantic_title": "large-scale speaker diarization of radio broadcast archives",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dubey19_interspeech.html": {
    "title": "Toeplitz Inverse Covariance Based Robust Speaker Clustering for Naturalistic Audio Streams",
    "volume": "main",
    "abstract": "Speaker diarization determines who spoke and when? in an audio stream. In this study, we propose a model-based approach for robust speaker clustering using i-vectors. The i-vectors extracted from different segments of same speaker are correlated. We model this correlation with a Markov Random Field (MRF) network. Leveraging the advancements in MRF modeling, we used Toeplitz Inverse Covariance (TIC) matrix to represent the MRF correlation network for each speaker. This approaches captures the sequential structure of i-vectors (or equivalent speaker turns) belonging to same speaker in an audio stream. A variant of standard Expectation Maximization (EM) algorithm is adopted for deriving closed-form solution using dynamic programming (DP) and the alternating direction method of multiplier (ADMM). Our diarization system has four steps: (1) ground-truth segmentation; (2) i-vector extraction; (3) post-processing (mean subtraction, principal component analysis, and length-normalization) ; and (4) proposed speaker clustering. We employ cosine K-means and movMF speaker clustering as baseline approaches. Our evaluation data is derived from: (i) CRSS-PLTL corpus, and (ii) two meetings subset of the AMI corpus. Relative reduction in diarization error rate (DER) for CRSS-PLTL corpus is 43.22% using the proposed advancements as compared to baseline. For AMI meetings IS1000a and IS1003b, relative DER reduction is 29.37% and 9.21%, respectively",
    "checked": true,
    "id": "7342cbe19f130ca0de319dca36d75648cec2ad70",
    "semantic_title": "toeplitz inverse covariance based robust speaker clustering for naturalistic audio streams",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kovacs19_interspeech.html": {
    "title": "Examining the Combination of Multi-Band Processing and Channel Dropout for Robust Speech Recognition",
    "volume": "main",
    "abstract": "A pivotal question in Automatic Speech Recognition (ASR) is the robustness of the trained models. In this study, we investigate the combination of two methods commonly applied to increase the robustness of ASR systems. On the one hand, inspired by auditory experiments and signal processing considerations, multi-band band processing has been used for decades to improve the noise robustness of speech recognition. On the other hand, dropout is a commonly used regularization technique to prevent overfitting by keeping the model from becoming over-reliant on a small set of neurons. We hypothesize that the careful combination of the two approaches would lead to increased robustness, by preventing the resulting model from over-rely on any given band To verify our hypothesis, we investigate various approaches for the combination of the two methods using the Aurora-4 corpus. The results obtained corroborate our initial assumption, and show that the proper combination of the two techniques leads to increased robustness, and to significantly lower word error rates (WERs). Furthermore, we find that the accuracy scores attained here compare favourably to those reported recently on the clean training scenario of the Aurora-4 corpus",
    "checked": true,
    "id": "9f5b8831cd40e6987a70016d107c928ba508dc07",
    "semantic_title": "examining the combination of multi-band processing and channel dropout for robust speech recognition",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/soni19_interspeech.html": {
    "title": "Label Driven Time-Frequency Masking for Robust Continuous Speech Recognition",
    "volume": "main",
    "abstract": "The application of Time-Frequency (T-F) masking based approaches for Automatic Speech Recognition has been shown to provide significant gains in system performance in the presence of additive noise. Such approaches give performance improvement when the T-F masking front-end is trained jointly with the acoustic model. However, such systems still rely on a pre-trained T-F masking enhancement block, trained using pairs of clean and noisy speech signals. Pre-training is necessary due to large number of parameters associated with the enhancement network. In this paper, we propose a flat-start joint training of a network that has both a T-F masking based enhancement block and a phoneme classification block. In particular, we use fully convolutional network as an enhancement front-end to reduce the number of parameters. We train the network by jointly updating the parameters of both these blocks using tied Context-Dependent phoneme states as targets. We observe that pretraining of the proposed enhancement block is not necessary for the convergence. In fact, the proposed flat-start joint training converges faster than the baseline multi-condition trained model. The experiments performed on Aurora-4 database show 7.06% relative improvement over multi-conditioned baseline. We get similar improvements for unseen test conditions as well",
    "checked": true,
    "id": "8bfe5bb970fc99148f1c8c6b666312fd05a12f21",
    "semantic_title": "label driven time-frequency masking for robust continuous speech recognition",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wu19c_interspeech.html": {
    "title": "Speaker-Invariant Feature-Mapping for Distant Speech Recognition via Adversarial Teacher-Student Learning",
    "volume": "main",
    "abstract": "Feature mapping (FM) jointly trained with acoustic model (AFM) is commonly used for single-channel speech enhancement. However, the performance is affected by the inter-speaker variability. In this paper, we propose speaker-invariant AFM (SIAFM) aiming at curtailing the inter-talker variability while achieving speech enhancement. In SIAFM, a feature-mapping network, an acoustic model and a speaker classifier network are jointly optimized to minimize the feature-mapping loss and the senone classification loss, and simultaneously min-maximize the speaker classification loss. Evaluated on AMI dataset, the proposed SIAFM achieves 4.8% and 7.0% relative word error rate (WER) reduction on the overlapped and non-overlapped condition over the baseline acoustic model trained with single distant microphone (SDM) data. Additionally, the SIAFM obtains 3.0% relative overlapped WER and 4.2% relative non-overlapped WER decrease over the multi-conditional (MCT) acoustic model. To further promote the performance of SIAFM, we employ teacher-student learning (TS), in which the posterior probabilities generated by the individual headset microphone (IHM) data can be used in lieu of labels to train the SIAFM model. The experiments show that compared with MCT model, SIAFM with TS (SIAFM-TS) can reach 4.2% relative overlapped WER and 6.3% relative non-overlapped WER decrease respectively",
    "checked": true,
    "id": "a84b392aeb4bd31da086baf4a88484d550e1023c",
    "semantic_title": "speaker-invariant feature-mapping for distant speech recognition via adversarial teacher-student learning",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ming19_interspeech.html": {
    "title": "Full-Sentence Correlation: A Method to Handle Unpredictable Noise for Robust Speech Recognition",
    "volume": "main",
    "abstract": "We describe the theory and implementation of full-sentence speech correlation for speech recognition, and demonstrate its superior robustness to unseen/untrained noise. For the Aurora 2 data, trained with only clean speech, the new method performs competitively against the state-of-the-art with multicondition training and adaptation, and achieves the lowest word error rate in very low SNR (-5 dB). Further experiments with highly nonstationary noise (pop song, broadcast news, etc.) show the surprising ability of the new method to handle unpredictable noise. The new method adds several novel developments to our previous research, including the modeling of the speaker characteristics along with other acoustic and semantic features of speech for separating speech from noise, and a novel Viterbi algorithm to implement full-sentence correlation for speech recognition",
    "checked": true,
    "id": "a34eae29fb8a199ba961d39fdb6b64c154df27bb",
    "semantic_title": "full-sentence correlation: a method to handle unpredictable noise for robust speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/soni19b_interspeech.html": {
    "title": "Generative Noise Modeling and Channel Simulation for Robust Speech Recognition in Unseen Conditions",
    "volume": "main",
    "abstract": "Multi-conditioned training is a state-of-the-art approach to achieve robustness in Automatic Speech Recognition (ASR) systems. This approach works well in practice for seen degradation conditions. However, the performance of such system is still an issue for unseen degradation conditions. In this work we consider distortions due to additive noise and channel mismatch. To achieve the robustness to additive noise, we propose a parametric generative model for noise signals. By changing the parameters of the proposed generative model, various noise signals can be generated and used to develop a multi-conditioned dataset for ASR system training. The generative model is designed to span the feature space of Mel Filterbank Energies by using band-limited white noise signals as basis. To simulate channel distortions, we propose to shift the mean of log spectral magnitude using utterances with estimated channel distortions. Experiments performed on the Aurora 4 noisy speech database show that using noise types generated from the proposed generative model for multi-conditioned training provides significant performance gain for additive noise in unseen conditions. We compare our results with those from multi-conditioning by various real noise databases including environmental and other real life noises",
    "checked": true,
    "id": "4ca5ede1da48cf0649a8cef7dc2269812177cb3e",
    "semantic_title": "generative noise modeling and channel simulation for robust speech recognition in unseen conditions",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kumar19_interspeech.html": {
    "title": "Far-Field Speech Enhancement Using Heteroscedastic Autoencoder for Improved Speech Recognition",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) systems trained on clean speech do not perform well in far-field scenario. Degradation in word error rate (WER) can be as large as 40% in this mismatched scenario. Typically, speech enhancement is applied to map speech from far-field condition to clean condition using a neural network, commonly known as denoising autoencoder (DA). Such speech enhancement technique has shown significant improvement in ASR accuracy. It is a common practice to use mean-square error (MSE) loss to train DA which is based on regression model with residual noise modeled by zero-mean and constant co-variance Gaussian distribution. However, both these assumptions are not optimal, especially in highly non-stationary noisy and far-field scenario. Here, we propose a more generalized loss based on non-zero mean and heteroscedastic co-variance distribution for the residual variables. On the top, we present several novel DA architectures that are more suitable for the heteroscedastic loss. It is shown that the proposed methods outperform the conventional DA and MSE loss by a large margin. We observe relative improvement of 7.31% in WER compared to conventional DA and overall, a relative improvement of 14.4% compared to mismatched train and test scenario",
    "checked": true,
    "id": "8827899043786dee7563d4f93005c2b519fee75e",
    "semantic_title": "far-field speech enhancement using heteroscedastic autoencoder for improved speech recognition",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/delcroix19_interspeech.html": {
    "title": "End-to-End SpeakerBeam for Single Channel Target Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end (E2E) automatic speech recognition (ASR) that directly maps a sequence of speech features into a sequence of characters using a single neural network has received a lot of attention as it greatly simplifies the training and decoding pipelines and enables optimizing the whole system E2E. Recently, such systems have been extended to recognize speech mixtures by inserting a speech separation mechanism into the neural network, allowing to output recognition results for each speaker in the mixture. However, speech separation suffers from a global permutation ambiguity issue, i.e. arbitrary mapping between source speakers and outputs. We argue that this ambiguity would seriously limit the practical use of E2E separation systems. SpeakerBeam has been proposed as an alternative to speech separation to mitigate the global permutation ambiguity. SpeakerBeam aims at extracting only a target speaker in a mixture based on his/her speech characteristics, thus avoiding the global permutation problem. In this paper, we combine SpeakerBeam and an E2E ASR system to allow E2E training of a target speech recognition system. We show promising target speech recognition results in mixtures of two speakers, and discuss interesting properties of the proposed system in terms of speech enhancement and diarization ability",
    "checked": true,
    "id": "931cbd9d689e9fd6bd91f4e8e1dbdd7fbb6df9de",
    "semantic_title": "end-to-end speakerbeam for single channel target speech recognition",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hsu19_interspeech.html": {
    "title": "NIESR: Nuisance Invariant End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "Deep neural network models for speech recognition have achieved great success recently, but they can learn incorrect associations between the target and nuisance factors of speech (e.g., speaker identities, background noise, etc.), which can lead to overfitting. While several methods have been proposed to tackle this problem, existing methods incorporate additional information about nuisance factors during training to develop invariant models. However, enumeration of all possible nuisance factors in speech data and the collection of their annotations is difficult and expensive. We present a robust training scheme for end-to-end speech recognition that adopts an unsupervised adversarial invariance induction framework to separate out essential factors for speech-recognition from nuisances without using any supplementary labels besides the transcriptions. Experiments show that the speech recognition model trained with the proposed training scheme achieves relative improvements of 5.48% on WSJ0, 6.16% on CHiME3, and 6.61% on TIMIT dataset over the base model. Additionally, the proposed method achieves a relative improvement of 14.44% on the combined WSJ0+CHiME3 dataset",
    "checked": true,
    "id": "b510695b9c324eec07576346ceeefcc4cf35387f",
    "semantic_title": "niesr: nuisance invariant end-to-end speech recognition",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/suzuki19_interspeech.html": {
    "title": "Knowledge Distillation for Throat Microphone Speech Recognition",
    "volume": "main",
    "abstract": "Throat microphones are robust against external noise because they receive vibrations directly from the skin, however, their available speech data is limited. This work aims to improve the speech recognition accuracy of throat microphones, and we propose a knowledge distillation method of hybrid DNN-HMM acoustic model. This method distills the knowledge from acoustic model trained with a large amount of close-talk microphone speech data (teacher model) to acoustic model for throat microphones (student model) using a small amount of parallel data of throat and close-talk microphones. The frontend network of the student model contains a feature mapping network from throat microphone acoustic features to close-talk microphone bottleneck features, and the back-end network is a phonetic discrimination network from close-talk microphone bottleneck features. We attempted to improve recognition accuracy further by initializing student model parameters using pretrained front-end and back-end networks. Experimental results using Japanese read speech data showed that the proposed approach achieved 9.8% relative improvement of character error rate (14.3% → 12.9%) compared to the hybrid acoustic model trained only with throat microphone speech data. Furthermore, under noise environments of approximately 70 dBA or higher, the throat microphone system with our approach outperformed the close-talk microphone system",
    "checked": true,
    "id": "fe6fc80fa0aa98ae83fbbc1acc085c774abdc6f5",
    "semantic_title": "knowledge distillation for throat microphone speech recognition",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wu19d_interspeech.html": {
    "title": "Improved Speaker-Dependent Separation for CHiME-5 Challenge",
    "volume": "main",
    "abstract": "This paper summarizes several contributions for improving the speaker-dependent separation system for CHiME-5 challenge, which aims to solve the problem of multi-channel, highly-overlapped conversational speech recognition in a dinner party scenario with reverberations and non-stationary noises. Specifically, we adopt a speaker-aware training method by using i-vector as the target speaker information for multi-talker speech separation. With only one unified separation model for all speakers, we achieve a 10% absolute improvement in terms of word error rate (WER) over the previous baseline of 80.28% on the development set by leveraging our newly proposed data processing techniques and beamforming approach. With our improved back-end acoustic model, we further reduce WER to 60.15% which surpasses the result of our submitted CHiME-5 challenge system without applying any fusion techniques",
    "checked": true,
    "id": "a89322e687af8bf64bcd1c0ee22482b4070c12b2",
    "semantic_title": "improved speaker-dependent separation for chime-5 challenge",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19b_interspeech.html": {
    "title": "Bridging the Gap Between Monaural Speech Enhancement and Recognition with Distortion-Independent Acoustic Modeling",
    "volume": "main",
    "abstract": "Monaural speech enhancement has made dramatic advances in recent years. Although enhanced speech has been demonstrated to have better intelligibility and quality for human listeners, feeding it directly to automatic speech recognition (ASR) systems trained with noisy speech has not produced expected improvements in ASR performance. The lack of an enhancement benefit on recognition, or the gap between monaural speech enhancement and recognition, is often attributed to speech distortions introduced in the enhancement process. In this study, we analyze the distortion problem and propose a distortion-independent acoustic modeling scheme. Experimental results show that the distortion-independent acoustic model is able to overcome the distortion problem. Moreover, it can be used with various speech enhancement models. Both the distortion-independent and a noise-dependent acoustic model perform better than the previous best system on the CHiME-2 corpus. The noise-dependent acoustic model achieves a word error rate of 8.7%, outperforming the previous best result by 6.5% relatively",
    "checked": true,
    "id": "1e7b0f5d6745cd3e36638b5adeabcd42becbebcd",
    "semantic_title": "bridging the gap between monaural speech enhancement and recognition with distortion-independent acoustic modeling",
    "citation_count": 33
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19c_interspeech.html": {
    "title": "Enhanced Spectral Features for Distortion-Independent Acoustic Modeling",
    "volume": "main",
    "abstract": "It has recently been shown that a distortion-independent acoustic modeling method is able to overcome the distortion problem caused by speech enhancement. In this study, we improve the distortion-independent acoustic model by feeding it with enhanced spectral features. Using enhanced magnitude spectra, the automatic speech recognition (ASR) system achieves a word error rate of 7.8% on the CHiME-2 corpus, outperforming our previous best system by more than 10% relatively. Compared with the corresponding enhanced waveform signal based system, systems using enhanced spectral features obtain up to 24% relative improvement. These comparisons show that speech enhancement is helpful for robust ASR and that enhanced spectral features are more suitable for ASR tasks than enhanced waveform signals",
    "checked": true,
    "id": "1a18b768e870d8dc9ab81dc17f6732b664172736",
    "semantic_title": "enhanced spectral features for distortion-independent acoustic modeling",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/neekhara19b_interspeech.html": {
    "title": "Universal Adversarial Perturbations for Speech Recognition Systems",
    "volume": "main",
    "abstract": "In this work, we demonstrate the existence of universal adversarial audio perturbations that cause mis-transcription of audio signals by automatic speech recognition (ASR) systems. We propose an algorithm to find a single quasi-imperceptible perturbation, which when added to any arbitrary speech signal, will most likely fool the victim speech recognition model. Our experiments demonstrate the application of our proposed technique by crafting audio-agnostic universal perturbations for the state-of-the-art ASR system — Mozilla DeepSpeech. Additionally, we show that such perturbations generalize to a significant extent across models that are not available during training, by performing a transferability test on a WaveNet based ASR system",
    "checked": true,
    "id": "1a6446a451472b1c5815ce3e00cf84eb7e641a2a",
    "semantic_title": "universal adversarial perturbations for speech recognition systems",
    "citation_count": 82
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fujimoto19_interspeech.html": {
    "title": "One-Pass Single-Channel Noisy Speech Recognition Using a Combination of Noisy and Enhanced Features",
    "volume": "main",
    "abstract": "This paper introduces a method of noise-robust automatic speech recognition (ASR) that remains effective under one-pass single-channel processing. Under these constraints, the use of single-channel speech enhancement seems to be a reasonable noise-robust approach to ASR, because complicated techniques requiring multi-pass processing cannot be used. However, in many cases, single-channel speech enhancement seriously deteriorates the accuracy of ASR because of speech distortion. In addition, the advanced acoustic modeling framework (joint training) is relatively ineffective in the case of single-channel processing. To overcome these problems, we propose a noise-robust acoustic modeling framework based on a feature-level combination of noisy speech and enhanced speech. To obtain further improvements, we also adopt a sub-network-level combination of noisy and enhanced speech, and a gating mechanism that can dynamically select appropriate speech features. Through comparative evaluations, we confirm that the proposed method successfully improves the accuracy of ASR in noisy environments under strong constraints",
    "checked": true,
    "id": "b1ae7cda95d5ee2da1f33521954b742d4a68e4db",
    "semantic_title": "one-pass single-channel noisy speech recognition using a combination of noisy and enhanced features",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19_interspeech.html": {
    "title": "Jointly Adversarial Enhancement Training for Robust End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "Recently, the end-to-end system has made significant breakthroughs in the field of speech recognition. However, this single end-to-end architecture is not especially robust to the input variations interfered of noises and reverberations, resulting in performance degradation dramatically in reality. To alleviate this issue, the mainstream approach is to use a well-designed speech enhancement module as the front-end of ASR. However, enhancement modules would result in speech distortions and mismatches to training, which sometimes degrades the ASR performance. In this paper, we propose a jointly adversarial enhancement training to boost robustness of end-to-end systems. Specifically, we use a jointly compositional scheme of mask-based enhancement network, attention-based encoder-decoder network and discriminant network during training. The discriminator is used to distinguish between the enhanced features from enhancement network and clean features, which could guide enhancement network to output towards the realistic distribution. With the joint optimization of the recognition, enhancement and adversarial loss, the compositional scheme is expected to learn more robust representations for the recognition task automatically. Systematic experiments on AISHELL-1 show that the proposed method improves the noise robustness of end-to-end systems and achieves the relative error rate reduction of 4.6% over the multi-condition training",
    "checked": true,
    "id": "e564f49e872dab8a030f705434d9c5c195900d78",
    "semantic_title": "jointly adversarial enhancement training for robust end-to-end speech recognition",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yang19_interspeech.html": {
    "title": "Predicting Humor by Learning from Time-Aligned Comments",
    "volume": "main",
    "abstract": "In this paper, we describe a novel approach for generating unsupervised humor labels using time-aligned user comments, and predicting humor using audio information alone. We collected 241 videos of comedy movies and gameplay videos from one of the largest Chinese video-sharing websites. We generate unsupervised humor labels from laughing comments, and find high agreement between these labels and human annotations. From these unsupervised labels, we build deep learning models using speech and text features, which obtain an AUC of 0.751 in predicting humor on a manually annotated test set. To our knowledge, this is the first study predicting perceived humor in large-scale audio data",
    "checked": true,
    "id": "5a414cd30caa50f26136f2fc351cb0abf7c2897f",
    "semantic_title": "predicting humor by learning from time-aligned comments",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dinkov19_interspeech.html": {
    "title": "Predicting the Leading Political Ideology of YouTube Channels Using Acoustic, Textual, and Metadata Information",
    "volume": "main",
    "abstract": "We address the problem of predicting the leading political ideology, i.e., left-center-right bias, for YouTube channels of news media. Previous work on the problem has focused exclusively on text and on analysis of the language used, topics discussed, sentiment, and the like. In contrast, here we study videos, which yields an interesting multimodal setup. Starting with gold annotations about the leading political ideology of major world news media from Media Bias/Fact Check, we searched on YouTube to find their corresponding channels, and we downloaded a recent sample of videos from each channel. We crawled more than 1,000 YouTube hours along with the corresponding subtitles and metadata, thus producing a new multimodal dataset. We further developed a multimodal deep-learning architecture for the task. Our analysis shows that the use of acoustic signal helped to improve bias detection by more than 6% absolute over using text and metadata only. We release the dataset to the research community, hoping to help advance the field of multi-modal political bias detection",
    "checked": true,
    "id": "afa0f48dab3884c0a6e309699bf67efa2f8d5b61",
    "semantic_title": "predicting the leading political ideology of youtube channels using acoustic, textual, and metadata information",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2019/an19_interspeech.html": {
    "title": "Mitigating Gender and L1 Differences to Improve State and Trait Recognition",
    "volume": "main",
    "abstract": "Automatic detection of speaker states and traits is made more difficult by intergroup differences in how they are distributed and expressed in speech and language. In this study, we explore various deep learning architectures for incorporating demographic information into the classification task. We find that early and late fusion of demographic information both improve performance on the task of personality recognition, and a multitask learning model, which performs best, also significantly improves deception detection accuracy. Our findings establish a new state-of-the-art for personality recognition and deception detection on the CXD corpus, and suggest new best practices for mitigating intergroup differences to improve speaker state and trait recognition",
    "checked": true,
    "id": "ea64d4e06812956961f34ce8c47a5e084cf57712",
    "semantic_title": "mitigating gender and l1 differences to improve state and trait recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/weninger19_interspeech.html": {
    "title": "Deep Learning Based Mandarin Accent Identification for Accent Robust ASR",
    "volume": "main",
    "abstract": "In this paper, we present an in-depth study on the classification of regional accents in Mandarin speech. Experiments are carried out on Mandarin speech data systematically collected from 15 different geographical regions in China for broad coverage. We explore bidirectional Long Short-Term Memory (bLSTM) networks and i-vectors to model longer-term acoustic context. Starting from the classification of the collected data into the 15 regional accents, we derive a three-class grouping via non-metric dimensional scaling (NMDS), for which 68.4% average recall can be obtained. Furthermore, we evaluate a state-of-the-art ASR system on the accented data and demonstrate that the character error rate (CER) strongly varies among these accent groups, even if i-vector speaker adaptation is used. Finally, we show that model selection based on the prediction of our bLSTM accent classifier can yield up to 7.6% CER reduction for accented speech",
    "checked": true,
    "id": "862ecbbcd9f57145fc49b01ca4240d7559c063df",
    "semantic_title": "deep learning based mandarin accent identification for accent robust asr",
    "citation_count": 25
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gosztolya19_interspeech.html": {
    "title": "Calibrating DNN Posterior Probability Estimates of HMM/DNN Models to Improve Social Signal Detection from Audio Data",
    "volume": "main",
    "abstract": "To detect social signals such as laughter or filler events from audio data, a straightforward choice is to apply a Hidden Markov Model (HMM) in combination with a Deep Neural Network (DNN) that supplies the local class posterior estimates ( HMM/DNN hybrid model). However, the posterior estimates of the DNN may be suboptimal due to a mismatch between the cost function used during training (e.g. frame-level cross-entropy) and the actual evaluation metric (e.g. segment-level F score). In this study, we show experimentally that by employing a simple posterior probability calibration technique on the DNN outputs, the performance of the HMM/DNN workflow can be significantly improved. Specifically, we apply a linear transformation on the activations of the output layer right before using the softmax function, and fine-tune the parameters of this transformation. Out of the calibration approaches tested, we got the best F scores when the posterior calibration process was adjusted so as to maximize the actual HMM-based evaluation metric",
    "checked": true,
    "id": "ef3071f4e011c444fd49968d6ff7bc8129dbc9e7",
    "semantic_title": "calibrating dnn posterior probability estimates of hmm/dnn models to improve social signal detection from audio data",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mori19_interspeech.html": {
    "title": "Conversational and Social Laughter Synthesis with WaveNet",
    "volume": "main",
    "abstract": "The studies of laughter synthesis are relatively few, and they are still in a preliminary stage. We explored the possibility of applying WaveNet to laughter synthesis. WaveNet is potentially more suitable to model laughter waveforms that do not have a well-established theory of production like speech signals. Conversational laughter was modelled with a spontaneous dialogue speech corpus based on WaveNet. To obtain more stable laughter generation, conditioning WaveNet by power contour was proposed. Experimental results showed that the synthesized laughter by WaveNet was perceived as closer to natural laughter than HMM-based synthesized laughter",
    "checked": true,
    "id": "d764e4687e398b33a32f9da6fff391703f34cd22",
    "semantic_title": "conversational and social laughter synthesis with wavenet",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ludusan19_interspeech.html": {
    "title": "Laughter Dynamics in Dyadic Conversations",
    "volume": "main",
    "abstract": "Human verbal communication is a complex phenomenon involving dynamics that normally result in the alignment of participants on several modalities, and across various linguistic domains. We examined here whether such dynamics occur also for paralinguistic events, in particular, in the case of laughter. Using a conversational corpus containing dyadic interactions in three languages (French, German and Mandarin Chinese), we investigated three measures of alignment: convergence, synchrony and agreement. Support for convergence and synchrony was found in all three languages, although the level of support varied with the language, while the agreement in laughter type was found to be significant for the German data. The implications of these findings towards a better understanding of the role of laughter in human communication are discussed",
    "checked": true,
    "id": "3393da047f281383f283258bbc5ba6240bbd607c",
    "semantic_title": "laughter dynamics in dyadic conversations",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/truong19_interspeech.html": {
    "title": "Towards an Annotation Scheme for Complex Laughter in Speech Corpora",
    "volume": "main",
    "abstract": "Although laughter research has gained quite some interest over the past few years, a shared description of how to annotate laughter and its sub-units is still missing. We present a first attempt towards an annotation scheme that contributes to improving the homogeneity and transparency with which laughter is annotated. This includes the integration of respiratory noises as well as stretches of speech-laughs, and to a limited extend to smiled speech and short silent intervals. Inter-annotator agreement is assessed while applying the scheme to different corpora where laughter is evoked through different methods and varying settings. Annotating laughter becomes more complex when the situation in which laughter occurs becomes more spontaneous and social. There is a substantial disagreement among the annotators with respect to temporal alignment (when does a unit start and when does it end) and unit classification, particularly the determination of starts/ends of laughter episodes. In summary, this detailed laughter annotation study reflects the need for better investigations of the various components of laughter",
    "checked": true,
    "id": "3a3d936f196c010dcbdfa0858e63f2cf08a5e15e",
    "semantic_title": "towards an annotation scheme for complex laughter in speech corpora",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/baird19_interspeech.html": {
    "title": "Using Speech to Predict Sequentially Measured Cortisol Levels During a Trier Social Stress Test",
    "volume": "main",
    "abstract": "The effect of stress on the human body is substantial, potentially resulting in serious health implications. Furthermore, with modern stressors seemingly on the increase, there is an abundance of contributing factors which lead to a diagnosis of acute stress. However, observing biological stress reactions usually includes costly and time consuming sequential fluid-based samples to determine the degree of biological stress. On the contrary, a speech monitoring approach would allow for a non-invasive indication of stress. To evaluate the efficacy of the speech signal as a marker of stress, we explored, for the first time, the relationship between sequential cortisol samples and speech-based features. Utilising a novel corpus of 43 individuals undergoing a standardised Trier Social Stress Test (TSST), we extract a variety of feature sets and observe a correlation between speech and sequential cortisol measurements. For prediction of mean cortisol levels from speech, results show that for the entire TSST oral presentation, handcrafted COMPARE features achieve best results of 0.244 root mean square error [0 ;1] for the sample 20 minutes after the TSST. Correlation also increases at minute 20, with a Spearman's correlation coefficient of 0.421, and Cohen's d of 0.883 between the baseline and minute 20 cortisol predictions",
    "checked": true,
    "id": "7b5204d925ea5b5b88c257f75088145a47dc749d",
    "semantic_title": "using speech to predict sequentially measured cortisol levels during a trier social stress test",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2019/baird19b_interspeech.html": {
    "title": "Sincerity in Acted Speech: Presenting the Sincere Apology Corpus and Results",
    "volume": "main",
    "abstract": "The ability to discern an individual's level of sincerity varies from person to person and across cultures. Sincerity is typically a key indication of personality traits such as trustworthiness, and portraying sincerity can be integral to an abundance of scenarios, e. g. , when apologising. Speech signals are one important factor when discerning sincerity and, with more modern interactions occurring remotely, automatic approaches for the recognition of sincerity from speech are beneficial during both interpersonal and professional scenarios. In this study we present details of the Sincere Apology Corpus ( Sina-C). Annotated by 22 individuals for their perception of sincerity, Sina-C is an English acted-speech corpus of 32 speakers, apologising in multiple ways. To provide an updated baseline for the corpus, various machine learning experiments are conducted. Finding that extracting deep data-representations (utilising the Deep Spectrum toolkit) from the speech signals is best suited. Classification results on the binary (sincere / not sincere) task are at best 79.2% Unweighted Average Recall and for regression, in regards to the degree of sincerity, a Root Mean Square Error of 0.395 from the standardised range [-1.51; 1.72] is obtained",
    "checked": true,
    "id": "821ebbd0dceada9f7182d7ba416b7a0fcca365b4",
    "semantic_title": "sincerity in acted speech: presenting the sincere apology corpus and results",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/niebuhr19c_interspeech.html": {
    "title": "Do not Hesitate! — Unless You Do it Shortly or Nasally: How the Phonetics of Filled Pauses Determine Their Subjective Frequency and Perceived Speaker Performance",
    "volume": "main",
    "abstract": "In this paper, we test whether the perception of filled-pause (FP) frequency and public-speaking performance are mediated by the phonetic characteristics of FPs. In particular, total duration, vowel-formant pattern (if present), and nasal-segment proportion of FPs were correlated with perceptual data of 29 German listeners who rated excerpts of business presentations given by 68 German-speaking managers. Results show strong inter-speaker differences in how and how often FPs are realized. Moreover, differences in FP duration and nasal proportion are significantly correlated with estimated (i.e. subjective) FP frequency and perceived speaker performance. The shorter and more nasal a speaker's FPs are, the more do listeners underestimate the speaker's actual FP frequency and the higher they rate the speaker's public-speaking performance. The results are discussed in terms of their implications for FP saliency and rhetorical training",
    "checked": true,
    "id": "5efda0c06a3679af74366a1f43738c56c1ee0649",
    "semantic_title": "do not hesitate! - unless you do it shortly or nasally: how the phonetics of filled pauses determine their subjective frequency and perceived speaker performance",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vasquezcorrea19_interspeech.html": {
    "title": "Phonet: A Tool Based on Gated Recurrent Neural Networks to Extract Phonological Posteriors from Speech",
    "volume": "main",
    "abstract": "There are a lot of features that can be extracted from speech signals for different applications such as automatic speech recognition or speaker verification. However, for pathological speech processing there is a need to extract features about the presence of the disease or the state of the patients that are comprehensible for clinical experts. Phonological posteriors are a group of features that can be interpretable by the clinicians and at the same time carry suitable information about the patient's speech. This paper presents a tool to extract phonological posteriors directly from speech signals. The proposed method consists of a bank of parallel bidirectional recurrent neural networks to estimate the posterior probabilities of the occurrence of different phonological classes. The proposed models are able to detect the phonological classes with accuracies over 90%. In addition, the trained models are available to be used by the research community interested in the topic",
    "checked": true,
    "id": "5ea63993284d8efbaf063294b72955b41d1a73a1",
    "semantic_title": "phonet: a tool based on gated recurrent neural networks to extract phonological posteriors from speech",
    "citation_count": 26
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chang19_interspeech.html": {
    "title": "Code-Switching Sentence Generation by Generative Adversarial Networks and its Application to Data Augmentation",
    "volume": "main",
    "abstract": "Code-switching is about dealing with alternative languages in speech or text. It is partially speaker-dependent and domain-related, so completely explaining the phenomenon by linguistic rules is challenging. Compared to most monolingual tasks, insufficient data is an issue for code-switching. To mitigate the issue without expensive human annotation, we proposed an unsupervised method for code-switching data augmentation. By utilizing a generative adversarial network, we can generate intra-sentential code-switching sentences from monolingual sentences. We applied the proposed method on two corpora, and the result shows that the generated code-switching sentences improve the performance of code-switching language models",
    "checked": true,
    "id": "cbfac953843d16a6447c7547ab7048eccfa142c9",
    "semantic_title": "code-switching sentence generation by generative adversarial networks and its application to data augmentation",
    "citation_count": 54
  },
  "https://www.isca-speech.org/archive/interspeech_2019/meier19_interspeech.html": {
    "title": "Comparative Analysis of Think-Aloud Methods for Everyday Activities in the Context of Cognitive Robotics",
    "volume": "main",
    "abstract": "We describe our efforts to compare data collection methods using two think-aloud protocols in preparation to be used as a basis for automatic structuring and labeling of a large database of high-dimensional human activities data into a valuable resource for research in cognitive robotics. The envisioned dataset, currently in development, will contain synchronously recorded multimodal data, including audio, video, and biosignals (eye-tracking, motion-tracking, muscle and brain activity) from about 100 participants performing everyday activities while describing their task through use of think-aloud protocols. This paper provides details of our pilot recordings in the well-established and scalable \"table setting scenario,\" describes the concurrent and retrospective think-aloud protocols used, the methods used to analyze them, and compares their potential impact on the data collected as well as the automatic data segmentation and structuring process",
    "checked": true,
    "id": "5122e8acc32d7089f508fb9c11e33712c033e85d",
    "semantic_title": "comparative analysis of think-aloud methods for everyday activities in the context of cognitive robotics",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/beeferman19_interspeech.html": {
    "title": "RadioTalk: A Large-Scale Corpus of Talk Radio Transcripts",
    "volume": "main",
    "abstract": "We introduce RadioTalk, a corpus of speech recognition transcripts sampled from talk radio broadcasts in the United States between October of 2018 and March of 2019. The corpus is intended for use by researchers in the fields of natural language processing, conversational analysis, and the social sciences. The corpus encompasses approximately 2.8 billion words of automatically transcribed speech from 284,000 hours of radio, together with metadata about the speech, such as geographical location, speaker turn boundaries, gender, and radio program information. In this paper we summarize why and how we prepared the corpus, give some descriptive statistics on stations, shows and speakers, and carry out a few high-level analyses",
    "checked": true,
    "id": "455c31d401981a49b08d7f6fefd62101734b925d",
    "semantic_title": "radiotalk: a large-scale corpus of talk radio transcripts",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mdhaffar19_interspeech.html": {
    "title": "Qualitative Evaluation of ASR Adaptation in a Lecture Context: Application to the PASTEL Corpus",
    "volume": "main",
    "abstract": "Lectures are usually known to be highly specialised in that they deal with multiple and domain specific topics. This context is challenging for Automatic Speech Recognition (ASR) systems since they are sensitive to topic variability. Language Model (LM) adaptation is a commonly used technique to address the mismatch problem between training and test data. In this paper, we are interested in a qualitative analysis in order to relevantly compare the accuracy of the LM adaptation. While word error rate is the most common metric used to evaluate ASR systems, we consider that this metric cannot provide accurate information. Consequently, we explore the use of other metrics based on individual word error rate, indexability, and capability of building relevant requests for information retrieval from the ASR outputs. Experiments are carried out on the PASTEL corpus, a new dataset in French language, composed of lecture recordings, manual chaptering, manual transcriptions, and slides. While an adapted LM allows us to reduce the global classical word error rate by 15.62% in relative, we show that this reduction reaches 44.2% when computed on relevant words only. These observations are confirmed with the high LM adaptation gains obtained with indexability and information retrieval metrics",
    "checked": true,
    "id": "e2d6614b1fc90fce21d47f2ceced68690222d042",
    "semantic_title": "qualitative evaluation of asr adaptation in a lecture context: application to the pastel corpus",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/marinelli19_interspeech.html": {
    "title": "Active Annotation: Bootstrapping Annotation Lexicon and Guidelines for Supervised NLU Learning",
    "volume": "main",
    "abstract": "Natural Language Understanding (NLU) models are typically trained in a supervised learning framework. In the case of intent classification, the predicted labels are predefined and based on the designed annotation schema while the labeling process is based on a laborious task where annotators manually inspect each utterance and assign the corresponding label. We propose an Active Annotation (AA) approach where we combine an unsupervised learning method in the embedding space, a human-in-the-loop verification process, and linguistic insights to create lexicons that can be open categories and adapted over time. In particular, annotators define the y-label space on-the-fly during the annotation using an iterative process and without the need for prior knowledge about the input data. We evaluate the proposed annotation paradigm in a real use-case NLU scenario. Results show that our Active Annotation paradigm achieves accurate and higher quality training data, with an annotation speed of an order of magnitude higher with respect to the traditional human-only driven baseline annotation methodology",
    "checked": true,
    "id": "6eca4a8bada8c59373366241e0f474e18ab28194",
    "semantic_title": "active annotation: bootstrapping annotation lexicon and guidelines for supervised nlu learning",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dabike19_interspeech.html": {
    "title": "Automatic Lyric Transcription from Karaoke Vocal Tracks: Resources and a Baseline System",
    "volume": "main",
    "abstract": "Automatic sung speech recognition is a relatively understudied topic that has been held back by a lack of large and freely available datasets. This has recently changed thanks to the release of the DAMP Sing! dataset, a 1100 hour karaoke dataset originating from the social music-making company, Smule. This paper presents work undertaken to define an easily replicable, automatic speech recognition benchmark for this data. In particular, we describe how transcripts and alignments have been recovered from Karaoke prompts and timings; how suitable training, development and test sets have been defined with varying degrees of accent variability; and how language models have been developed using lyric data from the LyricWikia website. Initial recognition experiments have been performed using factored-layer TDNN acoustic models with lattice-free MMI training using Kaldi. The best WER is 19.60% — a new state-of-the-art for this type of data. The paper concludes with a discussion of the many challenging problems that remain to be solved. Dataset definitions and Kaldi scripts have been made available so that the benchmark is easily replicable",
    "checked": true,
    "id": "97c3344232933d29bc4f9e5f548a3cbebc3ac72d",
    "semantic_title": "automatic lyric transcription from karaoke vocal tracks: resources and a baseline system",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19b_interspeech.html": {
    "title": "Detecting Mismatch Between Speech and Transcription Using Cross-Modal Attention",
    "volume": "main",
    "abstract": "In this paper, we propose to detect mismatches between speech and transcriptions using deep neural networks. Although it is generally assumed there are no mismatches in some speech related applications, it is hard to avoid the errors due to one reason or another. Moreover, the use of mismatched data probably leads to performance reduction when training a model. In our work, instead of detecting the errors by computing the distance between manual transcriptions and text strings obtained using a speech recogniser, we view mismatch detection as a classification task and merge speech and transcription features using deep neural networks. To enhance detection ability, we use cross-modal attention mechanism in our approach by learning the relevance between the features obtained from the two modalities. To evaluate the effectiveness of our approach, we test it on Factored WSJCAM0 by randomly setting three kinds of mismatch, word deletion, insertion or substitution. To test its robustness, we train our models using a small number of samples and detect mismatch with different number of words being removed, inserted, and substituted. In our experiments, the results show the use of our approach for mismatch detection is close to 80% on insertion and deletion and outperforms the baseline",
    "checked": true,
    "id": "663e927200e8fe8f3e2c443bc19e108e5c7388dd",
    "semantic_title": "detecting mismatch between speech and transcription using cross-modal attention",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vidal19_interspeech.html": {
    "title": "EpaDB: A Database for Development of Pronunciation Assessment Systems",
    "volume": "main",
    "abstract": "In this paper, we describe the methodology for collecting and annotating a new database designed for conducting research and development on pronunciation assessment. While a significant amount of research has been done in the area of pronunciation assessment, to our knowledge, no database is available for public use for research in the field. Considering this need, we created EpaDB (English Pronunciation by Argentinians Database), which is composed of English phrases read by native Spanish speakers with different levels of English proficiency. The recordings are annotated with ratings of pronunciation quality at phrase-level and detailed phonetic alignments and transcriptions indicating which phones were actually pronounced by the speakers. We present inter-rater agreement, the effect of each phone on overall perceived non-nativeness, and the frequency of specific pronunciation errors",
    "checked": true,
    "id": "13b584e57b52a93af5352a6ac0480dabd15a5813",
    "semantic_title": "epadb: a database for development of pronunciation assessment systems",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/angerbauer19_interspeech.html": {
    "title": "Automatic Compression of Subtitles with Neural Networks and its Effect on User Experience",
    "volume": "main",
    "abstract": "Understanding spoken language can be impeded through factors like noisy environments, hearing impairments or lack of proficiency. Subtitles can help in those cases. However, for fast speech or limited screen size, it might be advantageous to compress the subtitles to their most relevant content. Therefore, we address automatic sentence compression in this paper. We propose a neural network model based on an encoder-decoder approach with the possibility of integrating the desired compression ratio. Using this model, we conduct a user study to investigate the effects of compressed subtitles on user experience. Our results show that compressed subtitles can suffice for comprehension but may pose additional cognitive load",
    "checked": true,
    "id": "297bac44f284a0b62add3223158cef33e8452d98",
    "semantic_title": "automatic compression of subtitles with neural networks and its effect on user experience",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/luo19_interspeech.html": {
    "title": "Integrating Video Retrieval and Moment Detection in a Unified Corpus for Video Question Answering",
    "volume": "main",
    "abstract": "Traditional video question answering models have been designed to retrieve videos to answer input questions. A drawback of this scenario is that users have to watch the entire video to find their desired answer. Recent work presented unsupervised neural models with attention mechanisms to find moments or segments from retrieved videos to provide accurate answers to input questions. Although these two tasks look similar, the latter is more challenging because the former task only needs to judge whether the question is answered in a video and returns the entire video, while the latter is expected to judge which moment within a video matches the question and accurately returns a segment of the video. Moreover, there is a lack of labeled data for training moment detection models. In this paper, we focus on integrating video retrieval and moment detection in a unified corpus. We further develop two models — a self-attention convolutional network and a memory network — for the tasks. Experimental results on our corpus show that the neural models can accurately detect and retrieve moments in supervised settings",
    "checked": true,
    "id": "a58cd5975fe958e11642edfc63e479c9c066b57d",
    "semantic_title": "integrating video retrieval and moment detection in a unified corpus for video question answering",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gutz19_interspeech.html": {
    "title": "Early Identification of Speech Changes Due to Amyotrophic Lateral Sclerosis Using Machine Classification",
    "volume": "main",
    "abstract": "We used a machine learning (ML) approach to detect bulbar amyotrophic lateral sclerosis (ALS) prior to the onset of overt speech symptoms. The dataset included speech samples from 123 participants who were stratified by sex and into three groups: healthy controls, ALS symptomatic, and ALS presymptomatic. We compared models trained on three group pairs (symptomatic-control, presymptomatic-control, and all ALS-control participants). Using acoustic features obtained with the OpenSMILE ComParE13 configuration, we tested several feature filtering techniques. ML classification was achieved using an SVM model and leave-one-out cross-validation. The most successful model, which was trained on symptomatic-control data, yielded an AUC=0.99 for females and AUC=0.91 for males. Models trained on all ALS-control participants had high diagnostic accuracy for classifying symptomatic and presymptomatic ALS participants (females: AUC=0.85; males: AUC=0.91). Additionally, probabilities from these models correlated with speaking rate (females: Spearman coefficient=-0.60, p<0.001; males: Spearman coefficient=-0.43, p<0.001) and intelligible speaking rate (females: Spearman coefficient=-0.65, p<0.001; males: Spearman coefficient=-0.40, p<0.01), indicating their possible use as a severity index of bulbar motor involvement in ALS. These results highlight the importance of stratifying patients by speech severity when testing diagnostic models and demonstrate the potential of ML classification in early detection and progress monitoring of ALS",
    "checked": true,
    "id": "b90d5faeaf7a610b722b66abe5a3ca9ded010d2b",
    "semantic_title": "early identification of speech changes due to amyotrophic lateral sclerosis using machine classification",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/k19_interspeech.html": {
    "title": "Automatic Detection of Breath Using Voice Activity Detection and SVM Classifier with Application on News Reports",
    "volume": "main",
    "abstract": "Breath detection during speech has broad applications ranging from emotion recognition to detection of diseases. Most of the breath detection equipment are contact based. In the proposed method, we use a voice activity detector (VAD) to find the non-speech region and searches the breath only in this region since breath is a non-speech activity. This reduces the execution time. A support vector machine (SVM) classifier is used with radial basis function (RBF) kernel trained on the cepstrogram feature to detect the breaths in the non-speech regions. The classifier output is post-processed to join breathing segments which are closely spaced and remove small duration breaths. Speech breathing rate is calculated as the ratio of the number of breaths to the time between the first and last breath. The algorithm is tested on a student evaluation database. The algorithm yields an F1 Score of 94% and root mean square error (RMSE) of 7.08 breaths/min for the speech-breathing rate. The output has been validated using thermal videos. The breaths have been classified as full and partial detection based on the Intersection over Union (IOU). The algorithm is also tested on some news channel reports which gave a minimum F1 Score of 73%",
    "checked": true,
    "id": "470da741e8afd6e2ab4eabcb7791f92de754da78",
    "semantic_title": "automatic detection of breath using voice activity detection and svm classifier with application on news reports",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/heo19_interspeech.html": {
    "title": "Acoustic Scene Classification Using Teacher-Student Learning with Soft-Labels",
    "volume": "main",
    "abstract": "Acoustic scene classification identifies an input segment into one of the pre-defined classes using spectral information. The spectral information of acoustic scenes may not be mutually exclusive due to common acoustic properties across different classes, such as babble noises included in both airports and shopping malls. However, conventional training procedure based on one-hot labels does not consider the similarities between different acoustic scenes. We exploit teacher-student learning with the purpose to derive soft-labels that consider common acoustic properties among different acoustic scenes. In teacher-student learning, the teacher network produces soft-labels, based on which the student network is trained. We investigate various methods to extract soft-labels that better represent similarities across different scenes. Such attempts include extracting soft-labels from multiple audio segments that are defined as an identical acoustic scene. Experimental results demonstrate the potential of our approach, showing a classification accuracy of 77.36% on the DCASE 2018 task 1 validation set",
    "checked": true,
    "id": "0f7016fca9cf4f3c89683d94530f0eb6a66af8de",
    "semantic_title": "acoustic scene classification using teacher-student learning with soft-labels",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19_interspeech.html": {
    "title": "Rare Sound Event Detection Using Deep Learning and Data Augmentation",
    "volume": "main",
    "abstract": "There is an increasing interest in smart environment and a growing adoption of smart devices. Smart assistants such as Google Home and Amazon Alexa, although focus on speech, could be extended to identify domestic events in real-time to provide more and better smart functions. Sound event detection aims to detect multiple target sound events that may happen simultaneously. The task is challenging due to the overlapping of sound events, the highly imbalanced nature of target and non-target data, and the complicated real-world background noise. In this paper, we proposed a unified approach that takes advantages of both the deep learning and data augmentation. A convolutional neural network (CNN) was combined with a feed-forward neural network (FNN) to improve the detection performance, and a dynamic time warping based data augmentation (DA) method was proposed to address the data imbalance problem. Experiments on several datasets showed a more than 7% increase in accuracy compared to the state-of-the-art approaches",
    "checked": true,
    "id": "4c9e18ce3877a4d90bc0848b57a42b4e8809bba7",
    "semantic_title": "rare sound event detection using deep learning and data augmentation",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sharma19_interspeech.html": {
    "title": "A Combination of Model-Based and Feature-Based Strategy for Speech-to-Singing Alignment",
    "volume": "main",
    "abstract": "Speech and singing are different in many ways. In this work, we propose a novel method to align phonetically identical spoken lyric with a singing vocal in a speech-singing parallel corpus, that is needed in speech-to-singing conversion. We attempt to align speech to singing vocal using a combination of model-based forced alignment and feature-based dynamic time warping (DTW). We first obtain the word boundaries of speech and singing vocals with forced alignment using speech and singing adapted acoustic models, respectively. We consider that speech acoustic models are more accurate than singing acoustic models, therefore, boundaries of spoken words are more accurate than sung words. By searching in the neighborhood of the sung word boundaries in the singing vocal, we hope to improve the alignment between spoken words and sung words. Considering the word boundaries as landmark, we perform speech-to-singing alignment at frame-level using DTW. The proposed method is able to achieve a 47.5% reduction in terms of word boundary error over the baseline, and subsequent improvement of singing quality in a speech-to-singing conversion system",
    "checked": true,
    "id": "07b79f26a8a84f7ddbd81d8222ffd1e55e9ea4bd",
    "semantic_title": "a combination of model-based and feature-based strategy for speech-to-singing alignment",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shrem19_interspeech.html": {
    "title": "Dr.VOT: Measuring Positive and Negative Voice Onset Time in the Wild",
    "volume": "main",
    "abstract": "Voice Onset Time (VOT), a key measurement of speech for basic research and applied medical studies, is the time between the onset of a stop burst and the onset of voicing. When the voicing onset precedes burst onset the VOT is negative; if voicing onset follows the burst, it is positive. In this work, we present a deep-learning model for accurate and reliable measurement of VOT in naturalistic speech. The proposed system addresses two critical issues: it can measure positive and negative VOT equally well, and it is trained to be robust to variation across annotations. Our approach is based on the structured prediction framework, where the feature functions are defined to be RNNs. These learn to capture segmental variation in the signal. Results suggest that our method substantially improves over the current state-of-the-art. In contrast to previous work, our Deep and Robust VOT annotator, Dr.VOT, can successfully estimate negative VOTs while maintaining state-of-the-art performance on positive VOTs. This high level of performance generalizes to new corpora without further retraining",
    "checked": true,
    "id": "90d7175eb18eab5bb969e6e35bd8e2cc6881e43c",
    "semantic_title": "dr.vot : measuring positive and negative voice onset time in the wild",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hui19_interspeech.html": {
    "title": "Effects of Base-Frequency and Spectral Envelope on Deep-Learning Speech Separation and Recognition Models",
    "volume": "main",
    "abstract": "Base-frequencies (F0) and spectral envelopes play an important role in speech separation and recognition by humans. Two experiments were conducted to study how trained networks for multi-speaker speech separation/recognition are affected by difference of F0 and spectral envelopes between source signals. The first experiment examined the effects of natural F0/envelope on the performance of speech separation. Results showed that when the two target signals differed in F0 by ±3 semitones or more or differed in the envelope by a scaling factor larger than 1.08 or less than 0.92, separation performance improved significantly. This is consistent with human listeners and is the first finding for deep learning-network (DNN) models. The second experiment tested the effect of F0/envelope difference on multi-speaker automatic speech recognition(ASR) system's performance. Results showed that multi-speaker recognition result also significantly rely on F0/envelope differences. The overall results indicated that the dependency of the existing automatic systems on monaural cues is similar to that of human, while automatic systems still perform inferior than human on same tasks",
    "checked": true,
    "id": "342580d32d493b25fbc0c52f87a58091f54e4e5f",
    "semantic_title": "effects of base-frequency and spectral envelope on deep-learning speech separation and recognition models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shah19_interspeech.html": {
    "title": "Phone Aware Nearest Neighbor Technique Using Spectral Transition Measure for Non-Parallel Voice Conversion",
    "volume": "main",
    "abstract": "Nearest Neighbor (NN)-based alignment techniques are popular in non-parallel Voice Conversion (VC). The performance of NN-based alignment improves with the information about phone boundary. However, estimating the exact phone boundary is a challenging task. If text corresponding to the utterance is available, the Hidden Markov Model (HMM) can be used to identify the phone boundaries. However, it requires a large amount of training data that is difficult to collect in realistic VC scenarios. Hence, we propose to exploit a Spectral Transition Measure (STM)-based alignment technique that does not require apriori training data. The idea behind STM is that neurons in the auditory or visual cortex respond strongly to the transitional stimuli compared to the steady-state stimuli. The phone boundaries estimated using the STM algorithm are then applied to the NN technique to obtain the aligned spectral features of the source and target speakers. Proposed STM+NN alignment technique is giving on an average 13.67% relative improvement in phonetic accuracy (PA) compared to the NN-based alignment technique. The improvement in %PA after alignment has positively reflected in the better performance in terms of speech quality and speaker similarity (in particular, a relative improvement of 13.63% and 13.26% , respectively) of the converted voice",
    "checked": true,
    "id": "13f8cd2bc61dfdf514172e27441f075285a8ee91",
    "semantic_title": "phone aware nearest neighbor technique using spectral transition measure for non-parallel voice conversion",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shankar19_interspeech.html": {
    "title": "Weakly Supervised Syllable Segmentation by Vowel-Consonant Peak Classification",
    "volume": "main",
    "abstract": "We present a novel approach for blind syllable segmentation that combines model-based feature selection with data-driven classification. In particular, we learn a function that maps short-term energy peaks of a speech utterance onto either the vowel or consonant class. The features used for classification capture spectral and energy signatures which are characteristic of the phonetic properties of the English language. The identified vowel peaks subsequently act as the nucleus of our syllable segments. We demonstrate the effectiveness of our proposed method using nested cross validation on 400 unique test utterances taken randomly from the TIMIT dataset containing over 5000 syllables in total. Our hybrid approach achieves lower insertion rate than the state-of-the-art segmentation methods and a lower deletion rate than all the baseline comparisons",
    "checked": true,
    "id": "3faa3dcb4fc2ac82e54cd021251743b35419bf15",
    "semantic_title": "weakly supervised syllable segmentation by vowel-consonant peak classification",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mateju19_interspeech.html": {
    "title": "An Approach to Online Speaker Change Point Detection Using DNNs and WFSTs",
    "volume": "main",
    "abstract": "In this paper, a new approach to speaker change point (SCP) detection is presented. This method is suitable for online applications (e.g., real-time broadcast monitoring). It is designed in a series of consecutive experiments, aiming at quality of detection as well as low latency. The resulting scheme utilizes a convolution neural network (CNN), whose output is smoothed by a decoder. The CNN is trained using data complemented by artificial examples to reduce different types of errors, and the decoder is based on a weighted finite state transducer (WFST) with the forced length of the transition model. Results obtained on data taken from the COST278 database show that our online approach yields results comparable with an offline multi-pass LIUM toolkit while operating online with a low latency",
    "checked": true,
    "id": "d6f8ef95caef1ebcf2e12fea23a5e76e39f5f5a0",
    "semantic_title": "an approach to online speaker change point detection using dnns and wfsts",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tang19_interspeech.html": {
    "title": "Regression and Classification for Direction-of-Arrival Estimation with Convolutional Recurrent Neural Networks",
    "volume": "main",
    "abstract": "We present a novel learning-based approach to estimate the direction-of-arrival (DOA) of a sound source using a convolutional recurrent neural network (CRNN) trained via regression on synthetic data and Cartesian labels. We also describe an improved method to generate synthetic data to train the neural network using state-of-the-art sound propagation algorithms that model specular as well as diffuse reflections of sound. We compare our model against three other CRNNs trained using different formulations of the same problem: classification on categorical labels, and regression on spherical coordinate labels. In practice, our model achieves up to 43% decrease in angular error over prior methods. The use of diffuse reflection results in 34% and 41% reduction in angular prediction errors on LOCATA and SOFA datasets, respectively, over prior methods based on image-source methods. Our method results in an additional 3% error reduction over prior schemes that use classification networks, and we use 36% fewer network parameters",
    "checked": true,
    "id": "f282a6e0ca8f3f6bdc32fd471094145bd847f319",
    "semantic_title": "regression and classification for direction-of-arrival estimation with convolutional recurrent neural networks",
    "citation_count": 28
  },
  "https://www.isca-speech.org/archive/interspeech_2019/paul19_interspeech.html": {
    "title": "Non-Parallel Voice Conversion Using Weighted Generative Adversarial Networks",
    "volume": "main",
    "abstract": "In this paper, we suggest a novel way to train Generative Adversarial Network (GAN) for the purpose of non-parallel, many-to-many voice conversion. The goal of voice conversion (VC) is to transform speech from a source speaker to that of a target speaker without changing the phonetic contents. Based on ideas from Game Theory, we suggest to multiply the gradient of the Generator with suitable weights. Weights are calculated so that they increase the power of fake samples that fool the Discriminator resulting in a stronger Generator. Motivated by a recently presented GAN based approach for VC, StarGAN-VC, we suggest a variation to StarGAN, referred to as Weighted StarGAN (WeStarGAN). The experiments are conducted on standard CMU ARCTIC database. WeStarGAN-VC approach achieves significantly better relative performance and is clearly preferred over recently proposed StarGAN-VC method in terms of speech subjective quality and speaker similarity with 75% and 65% preference scores, respectively",
    "checked": true,
    "id": "f87e009ed2c0010b40894bee954d2fae8ad81c5a",
    "semantic_title": "non-parallel voice conversion using weighted generative adversarial networks",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chou19_interspeech.html": {
    "title": "One-Shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization",
    "volume": "main",
    "abstract": "Recently, voice conversion (VC) without parallel data has been successfully adapted to multi-target scenario in which a single model is trained to convert the input voice to many different speakers. However, such model suffers from the limitation that it can only convert the voice to the speakers in the training data, which narrows down the applicable scenario of VC. In this paper, we proposed a novel one-shot VC approach which is able to perform VC by only an example utterance from source and target speaker respectively, and the source and target speaker do not even need to be seen during training. This is achieved by disentangling speaker and content representations with instance normalization (IN). Objective and subjective evaluation shows that our model is able to generate the voice similar to target speaker. In addition to the performance measurement, we also demonstrate that this model is able to learn meaningful speaker representations without any supervision",
    "checked": true,
    "id": "c77fa76a857051a6c7deb135a45af8d4a5f32f0f",
    "semantic_title": "one-shot voice conversion by separating speaker and content representations with instance normalization",
    "citation_count": 162
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lu19_interspeech.html": {
    "title": "One-Shot Voice Conversion with Global Speaker Embeddings",
    "volume": "main",
    "abstract": "Building a voice conversion (VC) system for a new target speaker typically requires a large amount of speech data from the target speaker. This paper investigates a method to build a VC system for arbitrary target speaker using one given utterance without any adaptation training process. Inspired by global style tokens (GSTs), which recently has been shown to be effective in controlling the style of synthetic speech, we propose the use of global speaker embeddings (GSEs) to control the conversion target of the VC system. Speaker-independent phonetic posteriorgrams (PPGs) are employed as the local condition input to a conditional WaveNet synthesizer for waveform generation of the target speaker. Meanwhile, spectrograms are extracted from the given utterance and fed into a reference encoder, the generated reference embedding is then employed as attention query to the GSEs to produce the speaker embedding, which is employed as the global condition input to the WaveNet synthesizer to control the generated waveform's speaker identity. In experiments, when compared with an adaptation training based any-to-any VC system, the proposed GSEs based VC approach performs equally well or better in both speech naturalness and speaker similarity, with apparently higher flexibility to the comparison",
    "checked": true,
    "id": "f411384d0fc902e6492ea0863aa6f7d910b4abeb",
    "semantic_title": "one-shot voice conversion with global speaker embeddings",
    "citation_count": 32
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tobing19_interspeech.html": {
    "title": "Non-Parallel Voice Conversion with Cyclic Variational Autoencoder",
    "volume": "main",
    "abstract": "In this paper, we present a novel technique for a non-parallel voice conversion (VC) with the use of cyclic variational auto-encoder (CycleVAE)-based spectral modeling. In a variational autoencoder (VAE) framework, a latent space, usually with a Gaussian prior, is used to encode a set of input features. In a VAE-based VC, the encoded latent features are fed into a decoder, along with speaker-coding features, to generate estimated spectra with either the original speaker identity (reconstructed) or another speaker identity (converted). Due to the non-parallel modeling condition, the converted spectra can not be directly optimized, which heavily degrades the performance of a VAE-based VC. In this work, to overcome this problem, we propose to use CycleVAE-based spectral model that indirectly optimizes the conversion flow by recycling the converted features back into the system to obtain corresponding cyclic reconstructed spectra that can be directly optimized. The cyclic flow can be continued by using the cyclic reconstructed features as input for the next cycle. The experimental results demonstrate the effectiveness of the proposed CycleVAE-based VC, which yields higher accuracy of converted spectra, generates latent features with higher correlation degree, and significantly improves the quality and conversion accuracy of the converted speech",
    "checked": true,
    "id": "a63ad584efd71d9ab0700f083b6fa7b516c8e1d8",
    "semantic_title": "non-parallel voice conversion with cyclic variational autoencoder",
    "citation_count": 54
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kaneko19_interspeech.html": {
    "title": "StarGAN-VC2: Rethinking Conditional Methods for StarGAN-Based Voice Conversion",
    "volume": "main",
    "abstract": "Non-parallel multi-domain voice conversion (VC) is a technique for learning mappings among multiple domains without relying on parallel data. This is important but challenging owing to the requirement of learning multiple mappings and the non-availability of explicit supervision. Recently, StarGAN-VC has garnered attention owing to its ability to solve this problem only using a single generator. However, there is still a gap between real and converted speech. To bridge this gap, we rethink conditional methods of StarGAN-VC, which are key components for achieving non-parallel multi-domain VC in a single model, and propose an improved variant called StarGAN-VC2. Particularly, we rethink conditional methods in two aspects: training objectives and network architectures. For the former, we propose a source-and-target conditional adversarial loss that allows all source domain data to be convertible to the target domain data. For the latter, we introduce a modulation-based conditional method that can transform the modulation of the acoustic feature in a domain-specific manner. We evaluated our methods on non-parallel multi-speaker VC. An objective evaluation demonstrates that our proposed methods improve speech quality in terms of both global and local structure measures. Furthermore, a subjective evaluation shows that StarGAN-VC2 outperforms StarGAN-VC in terms of naturalness and speaker similarity",
    "checked": true,
    "id": "3830bb029e2abd6240ecacf74ec95f584aa2c167",
    "semantic_title": "stargan-vc2: rethinking conditional methods for stargan-based voice conversion",
    "citation_count": 103
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kurita19_interspeech.html": {
    "title": "Robustness of Statistical Voice Conversion Based on Direct Waveform Modification Against Background Sounds",
    "volume": "main",
    "abstract": "This paper presents an investigation of the robustness of statistical voice conversion (VC) under noisy environments. To develop various VC applications, such as augmented vocal production and augmented speech production, it is necessary to handle noisy input speech because some background sounds, such as external noise and an accompanying sound, usually exist in a real environment. In this paper, we investigate an impact of the background sounds on the conversion performance in singing voice conversion focusing on two main VC frameworks, 1) vocoder-based VC and 2) vocoder-free VC based on direct waveform modification. We conduct a subjective evaluation on the converted singing voice quality under noisy conditions and reveal that the vocoder-free VC is more robust against background sounds compared with the vocoder-based VC. We also analyze the robustness of statistical VC and show that a kurtosis ratio of power spectral components before and after conversion is useful as an objective metric to evaluate it without using any target reference signals",
    "checked": true,
    "id": "97e142eee1bc2e8bad7b3a1b11b595750bacbdff",
    "semantic_title": "robustness of statistical voice conversion based on direct waveform modification against background sounds",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhao19b_interspeech.html": {
    "title": "Fast Learning for Non-Parallel Many-to-Many Voice Conversion with Residual Star Generative Adversarial Networks",
    "volume": "main",
    "abstract": "This paper proposes a fast learning framework for non-parallel many-to-many voice conversion with residual Star Generative Adversarial Networks (StarGAN). In addition to the state-of-the-art StarGAN-VC approach that learns an unreferenced mapping between a group of speakers' acoustic features for nonparallel many-to-many voice conversion, our method, which we call Res-StarGAN-VC, presents an enhancement by incorporating a residual mapping. The idea is to leverage on the shared linguistic content between source and target features during conversion. The residual mapping is realized by using identity shortcut connections from the input to the output of the generator in Res-StarGAN-VC. Such shortcut connections accelerate the learning process of the network with no increase of parameters and computational complexity. They also help generate high-quality fake samples at the very beginning of the adversarial training. Experiments and subjective evaluations show that the proposed method offers (1) significantly faster convergence in adversarial training and (2) clearer pronunciations and better speaker similarity of converted speech, compared to the StarGAN-VC baseline on both mono-lingual and cross-lingual many-to-many voice conversion tasks",
    "checked": true,
    "id": "03a401ff5d20cf8b3cbc29ebcd517662e403106e",
    "semantic_title": "fast learning for non-parallel many-to-many voice conversion with residual star generative adversarial networks",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/juvela19_interspeech.html": {
    "title": "GELP: GAN-Excited Linear Prediction for Speech Synthesis from Mel-Spectrogram",
    "volume": "main",
    "abstract": "Recent advances in neural network -based text-to-speech have reached human level naturalness in synthetic speech. The present sequence-to-sequence models can directly map text to mel-spectrogram acoustic features, which are convenient for modeling, but present additional challenges for vocoding (i.e., waveform generation from the acoustic features). High-quality synthesis can be achieved with neural vocoders, such as WaveNet, but such autoregressive models suffer from slow sequential inference. Meanwhile, their existing parallel inference counterparts are difficult to train and require increasingly large model sizes. In this paper, we propose an alternative training strategy for a parallel neural vocoder utilizing generative adversarial networks, and integrate a linear predictive synthesis filter into the model. Results show that the proposed model achieves significant improvement in inference speed, while outperforming a WaveNet in copy-synthesis quality",
    "checked": true,
    "id": "77634fb70549ebff71e4a39936ae351f630812b5",
    "semantic_title": "gelp: gan-excited liner prediction for speech synthesis from mel-spectrogram",
    "citation_count": 26
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yamamoto19b_interspeech.html": {
    "title": "Probability Density Distillation with Generative Adversarial Networks for High-Quality Parallel Waveform Generation",
    "volume": "main",
    "abstract": "This paper proposes an effective probability density distillation (PDD) algorithm for WaveNet-based parallel waveform generation (PWG) systems. Recently proposed teacher-student frameworks in the PWG system have successfully achieved a real-time generation of speech signals. However, the difficulties optimizing the PDD criteria without auxiliary losses result in quality degradation of synthesized speech. To generate more natural speech signals within the teacher-student framework, we propose a novel optimization criterion based on generative adversarial networks (GANs). In the proposed method, the inverse autoregressive flow-based student model is incorporated as a generator in the GAN framework, and jointly optimized by the PDD mechanism with the proposed adversarial learning method. As this process encourages the student to model the distribution of realistic speech waveform, the perceptual quality of the synthesized speech becomes much more natural. Our experimental results verify that the PWG systems with the proposed method outperform both those using conventional approaches, and also autoregressive generation systems with a well-trained teacher WaveNet",
    "checked": true,
    "id": "2f6e03d60bcd38c76811463eb653e0d5012b9480",
    "semantic_title": "probability density distillation with generative adversarial networks for high-quality parallel waveform generation",
    "citation_count": 39
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mohammadi19_interspeech.html": {
    "title": "One-Shot Voice Conversion with Disentangled Representations by Leveraging Phonetic Posteriorgrams",
    "volume": "main",
    "abstract": "We propose voice conversion model from arbitrary source speaker to arbitrary target speaker with disentangled representations. Voice conversion is a task to convert the voice of spoken utterance of source speaker to that of target speaker. Most prior work require to know either source speaker or target speaker or both in training, with either parallel or non-parallel corpus. Instead, we study the problem of voice conversion in nonparallel speech corpora and one-shot learning setting. We convert an arbitrary sentences of an arbitrary source speaker to target speakers given only one or few target speaker training utterances. To achieve this, we propose to use disentangled representations of speaker identity and linguistic context. We use a recurrent neural network (RNN) encoder for speaker embedding and phonetic posteriorgram as linguistic context encoding, along with a RNN decoder to generate converted utterances. Ours is a simpler model without adversarial training or hierarchical model design and thus more efficient. In the subjective tests, our approach achieved significantly better results compared to baseline regarding similarity",
    "checked": true,
    "id": "420b0c4773068b4507684cc361b4ccb9ea811bc1",
    "semantic_title": "one-shot voice conversion with disentangled representations by leveraging phonetic posteriorgrams",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19c_interspeech.html": {
    "title": "Investigation of F0 Conditioning and Fully Convolutional Networks in Variational Autoencoder Based Voice Conversion",
    "volume": "main",
    "abstract": "In this work, we investigate the effectiveness of two techniques for improving variational autoencoder (VAE) based voice conversion (VC). First, we reconsider the relationship between vocoder features extracted using the high quality vocoders adopted in conventional VC systems, and hypothesize that the spectral features are in fact F0 dependent. Such hypothesis implies that during the conversion phase, the latent codes and the converted features in VAE based VC are in fact source F0 dependent. To this end, we propose to utilize the F0 as an additional input of the decoder. The model can learn to disentangle the latent code from the F0 and thus generates converted F0 dependent converted features. Second, to better capture temporal dependencies of the spectral features and the F0 pattern, we replace the frame wise conversion structure in the original VAE based VC framework with a fully convolutional network structure. Our experiments demonstrate that the degree of disentanglement as well as the naturalness of the converted speech are indeed improved",
    "checked": true,
    "id": "b4dd9991a937cf7a7a542f527fc38397c2752dd9",
    "semantic_title": "investigation of f0 conditioning and fully convolutional networks in variational autoencoder based voice conversion",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19b_interspeech.html": {
    "title": "Jointly Trained Conversion Model and WaveNet Vocoder for Non-Parallel Voice Conversion Using Mel-Spectrograms and Phonetic Posteriorgrams",
    "volume": "main",
    "abstract": "The N10 system in the Voice Conversion Challenge 2018 (VCC 2018) has achieved high voice conversion (VC) performance in terms of speech naturalness and speaker similarity. We believe that further improvements can be gained from joint optimization (instead of separate optimization) of the conversion model and WaveNet vocoder, as well as leveraging information from the acoustic representation of the speech waveform, e.g. from Mel-spectrograms. In this paper, we propose a VC architecture to jointly train a conversion model that maps phonetic posteriorgrams (PPGs) to Mel-spectrograms and a WaveNet vocoder. The conversion model has a bottle-neck layer, whose outputs are concatenated with PPGs before being fed into the WaveNet vocoder as local conditioning. A weighted sum of a Mel-spectrogram prediction loss and a WaveNet loss is used as the objective function to jointly optimize parameters of the conversion model and the WaveNet vocoder. Objective and subjective evaluation results show that the proposed approach is capable of achieving significantly improved quality in voice conversion in terms of speech naturalness and speaker similarity of the converted speech for both cross-gender and intra-gender conversions",
    "checked": true,
    "id": "4012133ae119d3f3b556c0b1c66da9eda961b7a7",
    "semantic_title": "jointly trained conversion model and wavenet vocoder for non-parallel voice conversion using mel-spectrograms and phonetic posteriorgrams",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19b_interspeech.html": {
    "title": "Generative Adversarial Networks for Unpaired Voice Transformation on Impaired Speech",
    "volume": "main",
    "abstract": "This paper focuses on using voice conversion (VC) to improve the speech intelligibility of surgical patients who have had parts of their articulators removed. Due to the difficulty of data collection, VC without parallel data is highly desired. Although techniques for unparallel VC — for example, CycleGAN — have been developed, they usually focus on transforming the speaker identity, and directly transforming the speech of one speaker to that of another speaker and as such do not address the task here. In this paper, we propose a new approach for unparallel VC. The proposed approach transforms impaired speech to normal speech while preserving the linguistic content and speaker characteristics. To our knowledge, this is the first end-to-end GAN-based unsupervised VC model applied to impaired speech. The experimental results show that the proposed approach outperforms CycleGAN",
    "checked": true,
    "id": "7a9dd5562ddd3943925a7b06aecb1268d266dfb1",
    "semantic_title": "generative adversarial networks for unpaired voice transformation on impaired speech",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ding19_interspeech.html": {
    "title": "Group Latent Embedding for Vector Quantized Variational Autoencoder in Non-Parallel Voice Conversion",
    "volume": "main",
    "abstract": "This paper proposes a Group Latent Embedding for Vector Quantized Variational Autoencoders (VQ-VAE) used in nonparallel Voice Conversion (VC). Previous studies have shown that VQ-VAE can generate high-quality VC syntheses when it is paired with a powerful decoder. However, in a conventional VQ-VAE, adjacent atoms in the embedding dictionary can represent entirely different phonetic content. Therefore, the VC syntheses can have mispronunciations and distortions whenever the output of the encoder is quantized to an atom representing entirely different phonetic content. To address this issue, we propose an approach that divides the embedding dictionary into groups and uses the weighted average of atoms in the nearest group as the latent embedding. We conducted both objective and subjective experiments on the non-parallel CSTR VCTK corpus. Results show that the proposed approach significantly improves the acoustic quality of the VC syntheses compared to the traditional VQ-VAE (13.7% relative improvement) while retaining the voice identity of the target speaker",
    "checked": true,
    "id": "d2f7e74336baddc6e5e1ac607ad3e33cb24068bd",
    "semantic_title": "group latent embedding for vector quantized variational autoencoder in non-parallel voice conversion",
    "citation_count": 41
  },
  "https://www.isca-speech.org/archive/interspeech_2019/stephenson19_interspeech.html": {
    "title": "Semi-Supervised Voice Conversion with Amortized Variational Inference",
    "volume": "main",
    "abstract": "In this work we introduce a semi-supervised approach to the voice conversion problem, in which speech from a source speaker is converted into speech of a target speaker. The proposed method makes use of both parallel and non-parallel utterances from the source and target simultaneously during training. This approach can be used to extend existing parallel data voice conversion systems such that they can be trained with semi-supervision. We show that incorporating semi-supervision improves the voice conversion performance compared to fully supervised training when the number of parallel utterances is limited as in many practical applications. Additionally, we find that increasing the number non-parallel utterances used in training continues to improve performance when the amount of parallel training data is held constant",
    "checked": true,
    "id": "1f356df1333122c4e90b13681e57fb4d1dd71715",
    "semantic_title": "semi-supervised voice conversion with amortized variational inference",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dey19_interspeech.html": {
    "title": "Exploiting Semi-Supervised Training Through a Dropout Regularization in End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "In this paper, we explore various approaches for semi-supervised learning in an end-to-end automatic speech recognition (ASR) framework. The first step in our approach involves training a seed model on the limited amount of labelled data. Additional unlabelled speech data is employed through a data-selection mechanism to obtain the best hypothesized output, further used to retrain the seed model. However, uncertainties of the model may not be well captured with a single hypothesis. As opposed to this technique, we apply a dropout mechanism to capture the uncertainty by obtaining multiple hypothesized text transcripts of an speech recording. We assume that the diversity of automatically generated transcripts for an utterance will implicitly increase the reliability of the model. Finally, the data-selection process is also applied on these hypothesized transcripts to reduce the uncertainty. Experiments on freely-available TEDLIUM corpus and proprietary Adobe's internal dataset show that the proposed approach significantly reduces ASR errors, compared to the baseline model",
    "checked": true,
    "id": "8cfbaed6f1edecd51226f5a9e3c4645588e24746",
    "semantic_title": "exploiting semi-supervised training through a dropout regularization in end-to-end speech recognition",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kim19_interspeech.html": {
    "title": "Improved Vocal Tract Length Perturbation for a State-of-the-Art End-to-End Speech Recognition System",
    "volume": "main",
    "abstract": "In this paper, we present an improved vocal tract length perturbation (VTLP) algorithm as a data augmentation technique. VTLP is usually accomplished by adjusting the center frequencies of mel filterbank in [1]. Compared to the conventional approach, we re-synthesize waveforms from the frequency-warped spectra using overlap and addition (OLA). This approach had two advantages: First, we can apply an \"acoustic simulator\" [2, 3] after performing the VTLP-based frequency warping. Second, we may use a different window length for frequency warping from that used in feature processing. We observe that the best performance was obtained when the warping coefficient distribution is between 0.8 and 1.2, and the window length is 50 ms. We obtained 3.66% WER and 12.39% WER on the Librispeech test-clean and test-other using an attention-based end-to-end speech recognition system without using any Language Models (LMs). Using the shallow-fusion technique with a Transformer LM, we achieved 2.44% WER and 8.29% WER on the Librispeech test-clean and test-other sets. To the best of our knowledge, the 2.44% WER on the test-clean is the best result ever reported on this test set",
    "checked": true,
    "id": "82a51742f7693a3a0df7096560f0534037ebbccf",
    "semantic_title": "improved vocal tract length perturbation for a state-of-the-art end-to-end speech recognition system",
    "citation_count": 28
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhu19_interspeech.html": {
    "title": "Multi-Accent Adaptation Based on Gate Mechanism",
    "volume": "main",
    "abstract": "When only a limited amount of accented speech data is available, to promote multi-accent speech recognition performance, the conventional approach is accent-specific adaptation, which adapts the baseline model to multiple target accents independently. To simplify the adaptation procedure, we explore adapting the baseline model to multiple target accents simultaneously with multi-accent mixed data. Thus, we propose using accent-specific top layer with gate mechanism (AST-G) to realize multi-accent adaptation. Compared with the baseline model and accent-specific adaptation, AST-G achieves 9.8% and 1.9% average relative WER reduction respectively. However, in real-world applications, we can't obtain the accent category label for inference in advance. Therefore, we apply using an accent classifier to predict the accent label. To jointly train the acoustic model and the accent classifier, we propose the multi-task learning with gate mechanism (MTL-G). As the accent label prediction could be inaccurate, it performs worse than the accent-specific adaptation. Yet, in comparison with the baseline model, MTL-G achieves 5.1% average relative WER reduction",
    "checked": true,
    "id": "b194db16b1fdddd5b1b087429a754a625f7215c6",
    "semantic_title": "multi-accent adaptation based on gate mechanism",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2019/guo19_interspeech.html": {
    "title": "Unsupervised Adaptation with Adversarial Dropout Regularization for Robust Speech Recognition",
    "volume": "main",
    "abstract": "Recent adversarial methods proposed for unsupervised domain adaptation of acoustic models try to fool a specific domain discriminator and learn both senone-discriminative and domain-invariant hidden feature representations. However, a drawback of these approaches is that the feature generator simply aligns different features into the same distribution without considering the class boundaries of the target domain data. Thus, ambiguous target domain features can be generated near the decision boundaries, decreasing speech recognition performance. In this study, we propose to use Adversarial Dropout Regularization (ADR) in acoustic modeling to overcome the foregoing issue. Specifically, we optimize the senone classifier to make its decision boundaries lie in the class boundaries of unlabeled target data. Then, the feature generator learns to create features far away from the decision boundaries, which are more discriminative. We apply the ADR approach on the CHiME-3 corpus and the proposed method yields up to 12.9% relative WER reductions compared with the baseline trained on source domain data only and further improvement over the widely used gradient reversal layer method",
    "checked": true,
    "id": "bbe07b9c03cb3b11521876b2216faf151f6002db",
    "semantic_title": "unsupervised adaptation with adversarial dropout regularization for robust speech recognition",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kitza19_interspeech.html": {
    "title": "Cumulative Adaptation for BLSTM Acoustic Models",
    "volume": "main",
    "abstract": "This paper addresses the robust speech recognition problem as an adaptation task. Specifically, we investigate the cumulative application of adaptation methods. A bidirectional Long Short-Term Memory (BLSTM) based neural network, capable of learning temporal relationships and translation invariant representations, is used for robust acoustic modeling. Further, i-vectors were used as an input to the neural network to perform instantaneous speaker and environment adaptation, providing 8% relative improvement in word error rate on the NIST Hub5 2000 evaluation testset. By enhancing the first-pass i-vector based adaptation with a second-pass adaptation using speaker and environment dependent transformations within the network, a further relative improvement of 5% in word error rate was achieved. We have reevaluated the features used to estimate i-vectors and their normalization to achieve the best performance in a modern large scale automatic speech recognition system",
    "checked": true,
    "id": "8a568abeba0c2668cbbd58ccc094b8257c9ad4d1",
    "semantic_title": "cumulative adaptation for blstm acoustic models",
    "citation_count": 31
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xie19b_interspeech.html": {
    "title": "Fast DNN Acoustic Model Speaker Adaptation by Learning Hidden Unit Contribution Features",
    "volume": "main",
    "abstract": "Speaker adaptation techniques play a key role in reducing the mismatch between automatic speech recognition (ASR) systems and target users. Deep neural network (DNN) acoustic model adaptation by learning speaker-dependent hidden unit contributions (LHUC) scaling vectors has been widely used. The standard LHUC method not only requires multiple decoding passes in test time but also a substantial amount of adaptation data for robust parameter estimation. In order to address the issues, an efficient method of predicting and compressing the LHUC scaling vectors directly from acoustic features using a time-delay DNN (TDNN) and an online averaging layer is proposed in this paper. The resulting LHUC vectors are then used as auxiliary features to adapt DNN acoustic models. Experiments conducted on a 300-hour Switchboard corpus showed that the DNN and TDNN systems using the proposed predicted LHUC features consistently outperformed the corresponding baseline systems by up to about 9% relative reductions of word error rate. Being combined with i-Vector based adaptation, the LHUC feature adapted TDNN systems demonstrated consistent improvement over comparable i-Vector adapted TDNN system",
    "checked": true,
    "id": "1d79786bd857768a2dad047836d54b067807a719",
    "semantic_title": "fast dnn acoustic model speaker adaptation by learning hidden unit contribution features",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tsunoo19_interspeech.html": {
    "title": "End-to-End Adaptation with Backpropagation Through WFST for On-Device Speech Recognition System",
    "volume": "main",
    "abstract": "An on-device DNN-HMM speech recognition system efficiently works with a limited vocabulary in the presence of a variety of predictable noise. In such a case, vocabulary and environment adaptation is highly effective. In this paper, we propose a novel method of end-to-end (E2E) adaptation, which adjusts not only an acoustic model (AM) but also a weighted finite-state transducer (WFST). We convert a pretrained WFST to a trainable neural network and adapt the system to target environments/vocabulary by E2E joint training with an AM. We replicate Viterbi decoding with forward-backward neural network computation, which is similar to recurrent neural networks (RNNs). By pooling output score sequences, a vocabulary posterior for each utterance is obtained and used for discriminative loss computation. Experiments using 2–10 hours of English/Japanese adaptation datasets indicate that the fine-tuning of only WFSTs and that of only AMs are both comparable to a state-of-the-art adaptation method, and E2E joint training of the two components achieves the best recognition performance. We also adapt each language system to the other language using the adaptation data, and the results show that the proposed method also works well for language adaptations",
    "checked": true,
    "id": "e4b741df848e29236139abb6d9f033b496eb619c",
    "semantic_title": "end-to-end adaptation with backpropagation through wfst for on-device speech recognition system",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sar19_interspeech.html": {
    "title": "Learning Speaker Aware Offsets for Speaker Adaptation of Neural Networks",
    "volume": "main",
    "abstract": "In this work, we present an unsupervised long short-term memory (LSTM) layer normalization technique that we call adaptation by speaker aware offsets (ASAO). These offsets are learned using an auxiliary network attached to the main senone classifier. The auxiliary network takes main network LSTM activations as input and tries to reconstruct speaker, (speaker,phone) and (speaker,senone)-level averages of the activations by minimizing the mean-squared error. Once the auxiliary network is jointly trained with the main network, during test time we do not need additional information for the test data as the network will generate the offset itself. Unlike many speaker adaptation studies which only adapt fully connected layers, our method is applicable to LSTM layers in addition to fully-connected layers. In our experiments, we investigate the effect of ASAO of LSTM layers at different depths. We also show its performance when the inputs are already speaker adapted by feature space maximum likelihood linear regression (fMLLR). In addition, we compare ASAO with a speaker adversarial training framework. ASAO achieves higher senone classification accuracy and lower word error rate (WER) than both the unadapted models and the adversarial model on the HUB4 dataset, with an absolute WER reduction of up to 2%",
    "checked": true,
    "id": "7640a918f798fbbf8034a72a13bf30029954d1f8",
    "semantic_title": "learning speaker aware offsets for speaker adaptation of neural networks",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sim19_interspeech.html": {
    "title": "An Investigation into On-Device Personalization of End-to-End Automatic Speech Recognition Models",
    "volume": "main",
    "abstract": "Speaker-independent speech recognition systems trained with data from many users are generally robust against speaker variability and work well for a large population of speakers. However, these systems do not always generalize well for users with very different speech characteristics. This issue can be addressed by building personalized systems that are designed to work well for each specific user. In this paper, we investigate the idea of securely training personalized end-to-end speech recognition models on mobile devices so that user data and models never leave the device and are never stored on a server. We study how the mobile training environment impacts performance by simulating on-device data consumption. We conduct experiments using data collected from speech impaired users for personalization. Our results show that personalization achieved 63.7% relative word error rate reduction when trained in a server environment and 58.1% in a mobile environment. Moving to on-device personalization resulted in 18.7% performance degradation, in exchange for improved scalability and data privacy. To train the model on device, we split the gradient computation into two and achieved 45% memory reduction at the expense of 42% increase in training time",
    "checked": true,
    "id": "aec9df8489c58cb822f911f184ce3b0c61363dda",
    "semantic_title": "an investigation into on-device personalization of end-to-end automatic speech recognition models",
    "citation_count": 50
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jain19_interspeech.html": {
    "title": "A Multi-Accent Acoustic Model Using Mixture of Experts for Speech Recognition",
    "volume": "main",
    "abstract": "A major challenge in Automatic Speech Recognition(ASR) systems is to handle speech from a diverse set of accents. A model trained using a single accent performs rather poorly when confronted with different accents. One of the solutions is a multi-condition model trained on all the accents. However the performance improvement in this approach might be rather limited. Otherwise, accent-specific models might be trained but they become impractical as number of accents increases. In this paper, we propose a novel acoustic model architecture based on Mixture of Experts (MoE) which works well on multiple accents without having the overhead of training separate models for separate accents. The work is based on our earlier work, termed as MixNet, where we showed performance improvement by separation of phonetic class distributions in the feature space. In this paper, we propose an architecture that helps to compensate phonetic and accent variabilities which helps in even better discrimination among the classes. These variabilities are learned in a joint frame-work, and produce consistent improvements over all the individual accents, amounting to an overall 18% relative improvement in accuracy compared to baseline trained in multi-condition style",
    "checked": true,
    "id": "834c5744ccaabbf1930c96f781ebcd4ee0ef76bc",
    "semantic_title": "a multi-accent acoustic model using mixture of experts for speech recognition",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shor19_interspeech.html": {
    "title": "Personalizing ASR for Dysarthric and Accented Speech with Limited Data",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) systems have dramatically improved over the last few years. ASR systems are most often trained from ‘typical' speech, which means that underrepresented groups don't experience the same level of improvement. In this paper, we present and evaluate finetuning techniques to improve ASR for users with non-standard speech. We focus on two types of non-standard speech: speech from people with amyotrophic lateral sclerosis (ALS) and accented speech. We train personalized models that achieve 62% and 35% relative WER improvement on these two groups, bringing the absolute WER for ALS speakers, on a test set of message bank phrases, down to 10% for mild dysarthria and 20% for more serious dysarthria. We show that 71% of the improvement comes from only 5 minutes of training data. Finetuning a particular subset of layers (with many fewer parameters) often gives better results than finetuning the entire model. This is the first step towards building state of the art ASR models for dysarthric speech",
    "checked": true,
    "id": "df3f3c908be233e8aee9872c28258741f4b5ba0a",
    "semantic_title": "personalizing asr for dysarthric and accented speech with limited data",
    "citation_count": 76
  },
  "https://www.isca-speech.org/archive/interspeech_2019/peskov19_interspeech.html": {
    "title": "Mitigating Noisy Inputs for Question Answering",
    "volume": "main",
    "abstract": "Natural language processing systems are often downstream of unreliable inputs: machine translation, optical character recognition, or speech recognition. For instance, virtual assistants can only answer your questions after understanding your speech. We investigate and mitigate the effects of noise from Automatic Speech Recognition systems on two factoid Question Answering ( qa) tasks. Integrating confidences into the model and forced decoding of unknown words are empirically shown to improve the accuracy of downstream neural qa systems. We create and train models on a synthetic corpus of over 500,000 noisy sentences and evaluate on two human corpora from Quizbowl and Jeopardy! competitions",
    "checked": true,
    "id": "970383c0a41d7ae1ec4b8abaa3033778203377b9",
    "semantic_title": "mitigating noisy inputs for question answering",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gupta19_interspeech.html": {
    "title": "One-vs-All Models for Asynchronous Training: An Empirical Analysis",
    "volume": "main",
    "abstract": "Any given classification problem can be modeled using multiclass or One-vs-All (OVA) architecture. An OVA system consists of as many OVA models as the number of classes, providing the advantage of asynchrony, where each OVA model can be re-trained independent of other models. This is particularly advantageous in settings where scalable model training is a consideration (for instance in an industrial environment where multiple and frequent updates need to be made to the classification system). In this paper, we conduct empirical analysis on realizing independent updates to OVA models and its impact on the accuracy of the overall OVA system. Given that asynchronous updates lead to differences in training datasets for OVA models, we first define a metric to quantify the differences in datasets. Thereafter, using Natural Language Understanding as a task of interest, we estimate the impact of three factors: (i) number of classes, (ii) number of data points and, (iii) divergences in training datasets across OVA models; on the OVA system accuracy. Finally, we observe the accuracy impact of increased asynchrony in a Spoken Language Understanding system. We analyze the results and establish that the proposed metric correlates strongly with the model performances in both the experimental settings",
    "checked": true,
    "id": "63e190652d91b23b60a9b6bcc5c706f56eebbf88",
    "semantic_title": "one-vs-all models for asynchronous training: an empirical analysis",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/marzinotto19_interspeech.html": {
    "title": "Adapting a FrameNet Semantic Parser for Spoken Language Understanding Using Adversarial Learning",
    "volume": "main",
    "abstract": "This paper presents a new semantic frame parsing model, based on Berkeley FrameNet, adapted to process spoken documents in order to perform information extraction from broadcast contents. Building upon previous work that had shown the effectiveness of adversarial learning for domain generalization in the context of semantic parsing of encyclopedic written documents, we propose to extend this approach to elocutionary style generalization. The underlying question throughout this study is whether adversarial learning can be used to combine data from different sources and train models on a higher level of abstraction in order to increase their robustness to lexical and stylistic variations as well as automatic speech recognition errors. The proposed strategy is evaluated on a French corpus of encyclopedic written documents and a smaller corpus of radio podcast transcriptions, both annotated with a FrameNet paradigm. We show that adversarial learning increases all models generalization capabilities both on manual and automatic speech transcription as well as on encyclopedic data",
    "checked": true,
    "id": "f6a520994035941a2a2e301003643fefbe0e0914",
    "semantic_title": "adapting a framenet semantic parser for spoken language understanding using adversarial learning",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/parcollet19_interspeech.html": {
    "title": "M2H-GAN: A GAN-Based Mapping from Machine to Human Transcripts for Speech Understanding",
    "volume": "main",
    "abstract": "Deep learning is at the core of recent spoken language understanding (SLU) related tasks. More precisely, deep neural networks (DNNs) drastically increased the performances of SLU systems, and numerous architectures have been proposed. In the real-life context of theme identification of telephone conversations, it is common to hold both a human, manual (TRS) and an automatically transcribed (ASR) versions of the conversations. Nonetheless, and due to production constraints, only the ASR transcripts are considered to build automatic classifiers. TRS transcripts are only used to measure the performances of ASR systems. Moreover, the recent performances in term of classification accuracy, obtained by DNN related systems are close to the performances reached by humans, and it becomes difficult to further increase the performances by only considering the ASR transcripts. This paper proposes to distillates the TRS knowledge available during the training phase within the ASR representation, by using a new generative adversarial network called M2H-GAN to generate a TRS-like version of an ASR document, to improve the theme identification performances",
    "checked": true,
    "id": "4f26b0e04985a7014e8e374d4832c9e556ec8cf8",
    "semantic_title": "m2h-gan: a gan-based mapping from machine to human transcripts for speech understanding",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/georges19_interspeech.html": {
    "title": "Ultra-Compact NLU: Neuronal Network Binarization as Regularization",
    "volume": "main",
    "abstract": "This paper describes an approach for intent classification and tagging on embedded devices, such as smart watches. We describe a technique to train neuronal networks where the final neuronal network weights are binary. This enables memory bandwidth optimized inference and efficient computation even on constrained/embedded platforms The flow of the approach is as follows: tf-idf word selection method reduces the number of overall weights. Bag-of-Words features are used with a feedforward and recurrent neuronal network for intent classification and tagging, respectively. A novel double Gaussian based regularization term is used to train the network. Finally, the weights are almost clipped lossless to -1 or 1 which results in a tiny binary neuronal network for intent classification and tagging Our technique is evaluated using a text corpus of transcribed and annotated voice queries. The test domain is \"lights control\". We compare the intent and tagging accuracy of the ultra-compact binary neuronal network with our baseline system. The novel approach yields comparable accuracy but reduces the model size by a factor of 16: from 160kB to 10kB",
    "checked": true,
    "id": "8f967d0d3f43ecfdfa847775e24e8a5dab6b239c",
    "semantic_title": "ultra-compact nlu: neuronal network binarization as regularization",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lugosch19_interspeech.html": {
    "title": "Speech Model Pre-Training for End-to-End Spoken Language Understanding",
    "volume": "main",
    "abstract": "Whereas conventional spoken language understanding (SLU) systems map speech to text, and then text to intent, end-to-end SLU systems map speech directly to intent through a single trainable model. Achieving high accuracy with these end-to-end models without a large amount of training data is difficult. We propose a method to reduce the data requirements of end-to-end SLU in which the model is first pre-trained to predict words and phonemes, thus learning good features for SLU. We introduce a new SLU dataset, Fluent Speech Commands, and show that our method improves performance both when the full dataset is used for training and when only a small subset is used. We also describe preliminary experiments to gauge the model's ability to generalize to new phrases not heard during training",
    "checked": true,
    "id": "24d7b1487202e3aaf329df3d8135ae6eabefaa45",
    "semantic_title": "speech model pre-training for end-to-end spoken language understanding",
    "citation_count": 244
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shivakumar19_interspeech.html": {
    "title": "Spoken Language Intent Detection Using Confusion2Vec",
    "volume": "main",
    "abstract": "Decoding speaker's intent is a crucial part of spoken language understanding (SLU). The presence of noise or errors in the text transcriptions, in real life scenarios make the task more challenging. In this paper, we address the spoken language intent detection under noisy conditions imposed by automatic speech recognition (ASR) systems. We propose to employ confusion2vec word feature representation to compensate for the errors made by ASR and to increase the robustness of the SLU system. The confusion2vec, motivated from human speech production and perception, models acoustic relationships between words in addition to the semantic and syntactic relations of words in human language. We hypothesize that ASR often makes errors relating to acoustically similar words, and the confusion2vec with inherent model of acoustic relationships between words is able to compensate for the errors. We demonstrate through experiments on the ATIS benchmark dataset, the robustness of the proposed model to achieve state-of-the-art results under noisy ASR conditions. Our system reduces classification error rate (CER) by 20.84% and improves robustness by 37.48% (lower CER degradation) relative to the previous state-of-the-art going from clean to noisy transcripts. Improvements are also demonstrated when training the intent detection models on noisy transcripts",
    "checked": true,
    "id": "944c90a271afad85d4443f8a63284b708e38913a",
    "semantic_title": "spoken language intent detection using confusion2vec",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tomashenko19_interspeech.html": {
    "title": "Investigating Adaptation and Transfer Learning for End-to-End Spoken Language Understanding from Speech",
    "volume": "main",
    "abstract": "This work investigates speaker adaptation and transfer learning for spoken language understanding (SLU). We focus on the direct extraction of semantic tags from the audio signal using an end-to-end neural network approach. We demonstrate that the learning performance of the target predictive function for the semantic slot filling task can be substantially improved by speaker adaptation and by various knowledge transfer approaches. First, we explore speaker adaptive training (SAT) for end-to-end SLU models and propose to use zero pseudo i-vectors for more efficient model initialization and pretraining in SAT. Second, in order to improve the learning convergence for the target semantic slot filling (SF) task, models trained for different tasks, such as automatic speech recognition and named entity extraction are used to initialize neural end-to-end models trained for the target task. In addition, we explore the impact of the knowledge transfer for SLU from a speech recognition task trained in a different language. These approaches allow to develop end-to-end SLU systems in low-resource data scenarios when there is no enough in-domain semantically labeled data, but other resources, such as word transcriptions for the same or another language or named entity annotation, are available",
    "checked": true,
    "id": "60db06f7a29fa9281cabc15f3d1ef8036abe96f5",
    "semantic_title": "investigating adaptation and transfer learning for end-to-end spoken language understanding from speech",
    "citation_count": 25
  },
  "https://www.isca-speech.org/archive/interspeech_2019/song19_interspeech.html": {
    "title": "Topic-Aware Dialogue Speech Recognition with Transfer Learning",
    "volume": "main",
    "abstract": "Dialogue speech widely exists in scenarios such as chitchat, meeting and customer service. General-purpose speech recognition systems usually neglect the topic information in the context of dialogue speech, which has great potential for improving the performance of speech recognition. In this paper, we propose a transfer learning mechanism to conduct topic-aware recognition for dialogue speech. We first propose a new probabilistic topic model named Dialogue Speech Topic Model (DSTM) that is specialized for modeling the context of dialogue speech. We further propose a novel transfer learning mechanism for DSTM to significantly reduce its training cost while preserving its effectiveness for accurate topic inference. The experiment results demonstrate that proposed techniques in language model adaptation effectively improve the performance of the state-of-the-art Automatic Speech Recognition (ASR) system",
    "checked": true,
    "id": "1e564639cd2a37c97afc9959c14aec529adc7de7",
    "semantic_title": "topic-aware dialogue speech recognition with transfer learning",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/masumura19_interspeech.html": {
    "title": "Improving Conversation-Context Language Models with Multiple Spoken Language Understanding Models",
    "volume": "main",
    "abstract": "In this paper, we integrate fully neural network based conversation-context language models (CCLMs) that are suitable for handling multi-turn conversational automatic speech recognition (ASR) tasks, with multiple neural spoken language understanding (SLU) models. A main strength of CCLMs is their capacity to take long-range interactive contexts beyond utterance boundaries into consideration. However, it is hard to optimize the CCLMs so as to fully exploit the long-range interactive contexts because conversation-level training datasets are often limited. In order to mitigate this problem, our key idea is to introduce various SLU models that are developed for spoken dialogue systems into the CCLMs. In our proposed method (which we call \"SLU-assisted CCLM\"), hierarchical recurrent encoder-decoder based language modeling is extended so as to handle various utterance-level SLU results of preceding utterances in a continuous space. We expect that the SLU models will help the CCLMs to properly understand semantic meanings of long-range interactive contexts and to fully leverage them for estimating a next utterance. Our experiments on contact center dialogue ASR tasks demonstrate that SLU-assisted CCLMs combined with three types of SLU models can yield ASR performance improvements",
    "checked": true,
    "id": "3e08eb1fbc4617976173f97db03b49ea88014241",
    "semantic_title": "improving conversation-context language models with multiple spoken language understanding models",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chien19_interspeech.html": {
    "title": "Meta Learning for Hyperparameter Optimization in Dialogue System",
    "volume": "main",
    "abstract": "The performance of dialogue system based on deep reinforcement learning (DRL) highly depends on the selected hyperparameters in DRL algorithms. Traditionally, Gaussian process (GP) provides a probabilistic approach to Bayesian optimization for sequential search which is beneficial to select optimal hyperparameter. However, GP suffers from the expanding computation when the dimension of hyperparameters and the number of search points are increased. This paper presents a meta learning approach to carry out multifidelity Bayesian optimization where a two-level recurrent neural network (RNN) is developed for sequential learning and optimization. The search space is explored via the first-level RNN with cheap and low fidelity over a global region of hyperparameters. The optimization is then exploited and leveraged by the second-level RNN with a high fidelity on the successively small regions. The experiments on the hyperparameter optimization for dialogue system based on the deep Q network show the effectiveness and efficiency by using the proposed multifidelity Bayesian optimization",
    "checked": true,
    "id": "93e4b6acda7e725ffa4104e270273d96ba97e816",
    "semantic_title": "meta learning for hyperparameter optimization in dialogue system",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2019/williams19_interspeech.html": {
    "title": "Zero Shot Intent Classification Using Long-Short Term Memory Networks",
    "volume": "main",
    "abstract": "We describe a zero shot approach to intent classification that allows for the identification of intents that were not present during training. Our approach makes use of a Long-short Term Memory neural network to encode user queries and intents and uses these encodings to score previously unseen intents based on their semantic similarity to the queries. We test our model on intent classification in a personal digital assistant and show an improvement of 15% over a strong baseline. We also investigate the effect of adding a few training samples for the previously unseen intents in a few shot learning setting and show improvements of up to 16% over the baseline method",
    "checked": true,
    "id": "a40b648c13158b6d87744dcf94d40834653f56c8",
    "semantic_title": "zero shot intent classification using long-short term memory networks",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/korpusik19_interspeech.html": {
    "title": "A Comparison of Deep Learning Methods for Language Understanding",
    "volume": "main",
    "abstract": "In this paper, we compare a suite of neural networks (recurrent, convolutional, and the recently proposed BERT model) to a CRF with hand-crafted features on three semantic tagging corpora: the Air Travel Information System (ATIS) benchmark, restaurant queries, and written and spoken meal descriptions. Our motivation is to investigate pre-trained BERT's transferability to the domains we are interested in. We demonstrate that neural networks without feature engineering outperform state-of-the-art statistical and deep learning approaches on all three tasks (except written meal descriptions, where the CRF is slightly better) and that deep, attention-based BERT, in particular, surpasses state-of-the-art results on these tasks. Error analysis shows the models are less confident when making errors, enabling the system to follow up with the user when uncertain",
    "checked": true,
    "id": "684597268c6ac614439d4f0d5de1756a668fdb8a",
    "semantic_title": "a comparison of deep learning methods for language understanding",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kobayashi19_interspeech.html": {
    "title": "Slot Filling with Weighted Multi-Encoders for Out-of-Domain Values",
    "volume": "main",
    "abstract": "This paper proposes a new method for slot filling of out-of-domain (OOD) slot values, which are not included in the training data, in spoken dialogue systems. Word embeddings have been proposed to estimate the OOD slot values included in the word embedding model from keyword information. At the same time, context information is an important clue for estimation because the values in a given slot tend to appear in similar contexts. The proper use of either or both keyword and context information depends on the sentence. Conventional methods input a whole sentence into an encoder and extract important clues by the attention mechanism. However, it is difficult to properly distinguish context and keyword information from the encoder outputs because these two features are already mixed. Our proposed method uses two encoders, which distinctly encode contexts and keywords, respectively. The model calculates weights for the two encoders based on a user utterance and estimates a slot with weighted outputs from the two encoders. Experimental results show that the proposed method achieves a 50% relative improvement in F1 score compared with a baseline model, which detects slot values from user utterances and estimates slots at once with a single encoder",
    "checked": true,
    "id": "179d8f0960980e48baa50e9094878eccbf2908e4",
    "semantic_title": "slot filling with weighted multi-encoders for out-of-domain values",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/seneviratne19_interspeech.html": {
    "title": "Multi-Corpus Acoustic-to-Articulatory Speech Inversion",
    "volume": "main",
    "abstract": "There are several technologies like Electromagnetic articulometry (EMA), ultrasound, real-time Magnetic Resonance Imaging (MRI), and X-ray microbeam that are used to measure speech articulatory movements. Each of these techniques provides a different view of the vocal tract. The measurements performed using the similar techniques also differ greatly due to differences in the placement of sensors, and the anatomy of speakers. This limits most articulatory studies to single datasets. However to yield better results in its applications, the speech inversion systems should be more generalized, which requires the combination of data from multiple sources. This paper proposes a multi-task learning based deep neural network architecture for acoustic-to-articulatory speech inversion trained using three different articulatory datasets — two of them were measured using EMA, and one using X-ray microbeam. Experiments show improved accuracy of the proposed acoustic-to-articulatory mapping compared to the systems trained using single datasets",
    "checked": true,
    "id": "cc79683297c657277b5b82084e0e36134d03bac8",
    "semantic_title": "multi-corpus acoustic-to-articulatory speech inversion",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dash19_interspeech.html": {
    "title": "Towards a Speaker Independent Speech-BCI Using Speaker Adaptation",
    "volume": "main",
    "abstract": "Neurodegenerative diseases such as amyotrophic lateral sclerosis (ALS) can cause locked-in-syndrome (fully paralyzed but aware). Brain-computer interface (BCI) may be the only option to restore their communication. Current BCIs typically use visual or attention correlates in neural activities to select letters randomly displayed on a screen, which are extremely slow (a few words per minute). Speech-BCIs, which aim to convert the brain activity patterns to speech (neural speech decoding), hold the potential to enable faster communication. Although a few recent studies have shown the potential of neural speech decoding, those are focused on speaker-dependent models. In this study, we investigated speaker-independent neural speech decoding of five continuous phrases from Magnetoencephalography (MEG) signals while 8 subjects produced speech covertly (imagination) or overtly (articulation). We have used both supervised and unsupervised speaker adaptation strategies for implementing a speaker independent model. Experimental results demonstrated that the proposed adaptation-based speaker-independent model has significantly improved decoding performance. To our knowledge, this is the first demonstration of the possibility of speaker-independent neural speech decoding",
    "checked": true,
    "id": "f6e52681fb672dfc7f7ce422395d5b67d2687651",
    "semantic_title": "towards a speaker independent speech-bci using speaker adaptation",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sheth19_interspeech.html": {
    "title": "Identifying Input Features for Development of Real-Time Translation of Neural Signals to Text",
    "volume": "main",
    "abstract": "One of the main goals in Brain-Computer Interface (BCI) research is to help patients with faltering communication abilities due to neurodegenerative diseases produce text or speech output using their neural recordings. However, practical implementation of such a system has proven difficult due to limitations in the speed, accuracy, and training time of existing interfaces. In this paper, we contribute to this endeavour by isolating appropriate input features from speech-producing neural signals that will feed into a machine learning classifier to identify target phonemes. Analysing data from six subjects, we discern frequency bands that encapsulate differential information regarding production of vowels and consonants broadly, and more specifically nasals and semivowels. Subsequent spatial localization analysis reveals the underlying cortical regions responsible for different phoneme categories. Anatomical locations along with their respective frequency bands act as prospective feature sets for machine learning classifiers. We demonstrate this classification ability in a preliminary language reconstruction task and show an average word classification accuracy of 30.6% (p<0.001)",
    "checked": true,
    "id": "2e9dab07ec25a3b5bf032c170c99eccfa432dd74",
    "semantic_title": "identifying input features for development of real-time translation of neural signals to text",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/silva19_interspeech.html": {
    "title": "Exploring Critical Articulator Identification from 50Hz RT-MRI Data of the Vocal Tract",
    "volume": "main",
    "abstract": "The study of the static and dynamic aspects of speech production can profit from technologies such as electromagnetic midsagittal articulography (EMA) and real-time magnetic resonance (RTMRI). These can improve our knowledge on which articulators and gestures are involved in producing specific sounds and foster improved speech production models, paramount to advance, e.g., articulatory speech synthesis. Previous work, by the authors, has shown that critical articulator identification could be performed from RTMRI data of the vocal tract, with encouraging results, by extending the applicability of an unsupervised statistical identification method previously proposed for EMA data. Nevertheless, the slower time resolution of the considered RT-MRI corpus (14 Hz), when compared to EMA, potentially influencing the ability to select the most suitable representative configuration for each phone — paramount for strongly dynamic phones, e.g., nasal vowels —, and the lack of a richer set of contexts — relevant for observing coarticulation effects —, were identified as limitations. This article addresses these limitations by exploring critical articulator identification from a faster RTMRI corpus (50 Hz), for European Portuguese, providing a richer set of contexts, and testing how fusing the articulatory data of two speakers might influence critical articulator determination",
    "checked": true,
    "id": "a0b5fbc2de84d0050a8bbea9b120e37c131a4cb0",
    "semantic_title": "exploring critical articulator identification from 50hz rt-mri data of the vocal tract",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/douros19_interspeech.html": {
    "title": "Towards a Method of Dynamic Vocal Tract Shapes Generation by Combining Static 3D and Dynamic 2D MRI Speech Data",
    "volume": "main",
    "abstract": "We present an algorithm for augmenting the shape of the vocal tract using 3D static and 2D dynamic speech MRI data. While static 3D images have better resolution and provide spatial information, 2D dynamic images capture the transitions. The aim of this work is to combine strong points of these two types of data to obtain better image quality of 2D dynamic images and extend the 2D dynamic images to the 3D domain To produce a 3D dynamic consonant-vowel (CV) sequence, our algorithm takes as input the 2D CV transition and the static 3D targets for C and V. To obtain the enhanced sequence of images, the first step is to find a transformation between the 2D images and the mid-sagittal slice of the acoustically corresponding 3D image stack, and then find a transformation between neighbouring sagittal slices in the 3D static image stack. Combination of these transformations allows producing the final set of images. In the present study we first examined the transformation from the 3D mid-sagittal frame to the 2D video in order to improve image quality and then we examined the extension of the 2D video to the 3rd dimension with the aim to enrich spatial information",
    "checked": true,
    "id": "d67862fc963561190e929e778132e0e62c311903",
    "semantic_title": "towards a method of dynamic vocal tract shapes generation by combining static 3d and dynamic 2d mri speech data",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rasskazova19_interspeech.html": {
    "title": "Temporal Coordination of Articulatory and Respiratory Events Prior to Speech Initiation",
    "volume": "main",
    "abstract": "The investigation of the speech planning processes, in particular the timing between acoustic and articulatory onset, has recently received a lot of attention. Respiration has not been considered in this process so far, although it is involved and may be well coordinated with the oral articulators prior and at the onset of the utterance. In light of these considerations, we investigated the temporal coordination between acoustic, respiratory and articulatory events prior to utterance onset. For this purpose 12 native speakers of German have been recorded with Electromagnetic Articulography and Inductance Plethysmography reading sentences that were controlled for length and stress of the first word. The initial segment of the utterance was either /t/ or /n/. The results for six speakers so far indicate that early speech preparation consists of mouth opening during the inhalation phase. The onset of expiration seems to be tightly coupled with the acoustic and the articulatory onset, particularly with the constriction interval of the tongue tip gesture in the first segment. Manner of articulation of the initial segment seems to affect the temporal fine-tuning of preparatory events",
    "checked": true,
    "id": "bb5dbb9b1f2bd931dead024c171b014771e4f542",
    "semantic_title": "temporal coordination of articulatory and respiratory events prior to speech initiation",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gubian19b_interspeech.html": {
    "title": "Zooming in on Spatiotemporal V-to-C Coarticulation with Functional PCA",
    "volume": "main",
    "abstract": "It has long been proposed in speech production research that in CV sequences, the movement for consonant and vowel are initiated synchronously. However, mostly due to limitations on the statistical analysis of articulator motion over time, this could only be shown in a limited fashion, based on positional differences at a single time point during consonantal constriction formation. It is unknown to which extent this observation generalizes to earlier timepoints. In this paper, we illustrate the use of functional principal component analysis (FPCA) for the statistical analysis of articulator motion over time. Using articulography data, we quantify CV coarticulation during constriction formation of [k] in two vowel contexts. We show how FPCA enables us to analyse both horizontal and vertical movement components over time in a single model while preserving information on temporal variability. We combine FPCA with linear mixed modelling to obtain estimated mean trajectories and confidence bands for [k] in the two vowel contexts. Results show that well before the timepoint of peak velocity the vowel causes a substantial spatial separation of the consonantal trajectories, estimated to be at least 3 mm at peak velocity. This lends support to the hypothesis that vowel and consonant are initiated synchronously",
    "checked": true,
    "id": "80d9da86e9215c288cc900043bb1e61aee4e2bff",
    "semantic_title": "zooming in on spatiotemporal v-to-c coarticulation with functional pca",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/csapo19_interspeech.html": {
    "title": "Ultrasound-Based Silent Speech Interface Built on a Continuous Vocoder",
    "volume": "main",
    "abstract": "Recently it was shown that within the Silent Speech Interface (SSI) field, the prediction of F0 is possible from Ultrasound Tongue Images (UTI) as the articulatory input, using Deep Neural Networks for articulatory-to-acoustic mapping. Moreover, text-to-speech synthesizers were shown to produce higher quality speech when using a continuous pitch estimate, which takes non-zero pitch values even when voicing is not present. Therefore, in this paper on UTI-based SSI, we use a simple continuous F0 tracker which does not apply a strict voiced /unvoiced decision. Continuous vocoder parameters (ContF0, Maximum Voiced Frequency and Mel-Generalized Cepstrum) are predicted using a convolutional neural network, with UTI as input. The results demonstrate that during the articulatory-to-acoustic mapping experiments, the continuous F0 is predicted with lower error, and the continuous vocoder produces slightly more natural synthesized speech than the baseline vocoder using standard discontinuous F0",
    "checked": true,
    "id": "586d74f48dc077b9b4a5c538a6efb269fc153d99",
    "semantic_title": "ultrasound-based silent speech interface built on a continuous vocoder",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2019/klein19_interspeech.html": {
    "title": "Assessing Acoustic and Articulatory Dimensions of Speech Motor Adaptation with Random Forests",
    "volume": "main",
    "abstract": "Although most modern theories of speech production assume that representations of speech sounds are multidimensional encompassing acoustic and articulatory information, speech motor learning studies which assess the degree of adaptation in both dimensions are few and far between. In the current paper, we present an auditory perturbation study of German sibilant [s] in which speakers' audio and articulatory movements were recorded by means of electromagnetic articulography. Random Forest, a supervised learning algorithm, was employed to classify speakers' responses produced under unaltered or perturbed feedback based either on acoustic or articulatory parameters. Preliminary results demonstrate that while classification accuracy increases in the acoustic dimension as the perturbation session goes on, the classification accuracy in the articulatory dimension, although overall higher, remains approximately at the same level. This suggests that the adaptation process is characterized by active exploration of the articulatory space which is guided by speakers' auditory feedback",
    "checked": true,
    "id": "5a453c06c86ceb6cc71da04846ac5472f0a8f22f",
    "semantic_title": "assessing acoustic and articulatory dimensions of speech motor adaptation with random forests",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/takemoto19_interspeech.html": {
    "title": "Speech Organ Contour Extraction Using Real-Time MRI and Machine Learning Method",
    "volume": "main",
    "abstract": "Real-time MRI can be used to obtain videos that describe articulatory movements during running speech. For detailed analysis based on a large number of video frames, it is necessary to extract the contours of speech organs, such as the tongue, semi-automatically. The present study attempted to extract the contours of speech organs from videos using a machine learning method. First, an expert operator manually extracted the contours from the frames of a video to build training data sets. The learning operators, or learners, then extracted the contours from each frame of the video. Finally, the errors representing the geometrical distance between the extracted contours and the ground truth, which were the contours excluded from the training data sets, were examined. The results showed that the contours extracted using machine learning were closer to the ground truth than the contours traced by other expert and non-expert operators. In addition, using the same learners, the contours were extracted from other naive videos obtained during different speech tasks of the same subject. As a result, the errors in those videos were similar to those in the video in which the learners were trained",
    "checked": true,
    "id": "d473df3cdaab94de1083256f4d764e77439b05e8",
    "semantic_title": "speech organ contour extraction using real-time mri and machine learning method",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/leeuwen19_interspeech.html": {
    "title": "CNN-Based Phoneme Classifier from Vocal Tract MRI Learns Embedding Consistent with Articulatory Topology",
    "volume": "main",
    "abstract": "Recent advances in real-time magnetic resonance imaging (rtMRI) of the vocal tract provides opportunities for studying human speech. This modality together with acquired speech may enable the mapping of articulatory configurations to acoustic features. In this study, we take the first step by training a deep learning model to classify 27 different phonemes from midsagittal MR images of the vocal tract An American English database was used to train a convolutional neural network for classifying vowels (13 classes), consonants (14 classes) and all phonemes (27 classes) of 17 subjects. Classification top-1 accuracy of the test set for all phonemes was 57%. Error analysis showed voiced and unvoiced sounds often being confused. Moreover, we performed principal component analysis on the network's embedding and observed topological similarities between the network learned representation and the vowel diagram. Saliency maps gave insight into the anatomical regions most important for classification and show congruence with known regions of articulatory importance We demonstrate the feasibility for deep learning to distinguish between phonemes from MRI. Network analysis can be used to improve understanding of normal articulation and speech and, in the future, impaired speech. This study brings us a step closer to the articulatory-to-acoustic mapping from rtMRI",
    "checked": true,
    "id": "fe8c88ce388e17d9dd2bc3485437464dc7f631ea",
    "semantic_title": "cnn-based phoneme classifier from vocal tract mri learns embedding consistent with articulatory topology",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mucke19_interspeech.html": {
    "title": "Strength and Structure: Coupling Tones with Oral Constriction Gestures",
    "volume": "main",
    "abstract": "According to the segmental anchor hypothesis within the Autosegmental-Metrical approach, tones are aligned with segmental boundaries of consonant and vowels in the acoustic domain. In prenuclear rising pitch accents (LH*), the rise is assumed to occur in the vicinity of the accented syllable it is phonologically associated with. However, there are differences in the alignment patterns within and across languages that cannot be captured within the AM approach. In the present study, we investigate the coordination of tonal and oral constriction gestures within Articulatory Phonology. Therefore, we model the coordination of prenuclear LH* pitch accents in Catalan, Northern and Southern German with respect to syllable production on the basis of recordings with a 2D electromagnetic articulography. We provide an extended coupled oscillators model that allows for balanced and imbalanced coupling strengths. Based on examples, we show that the observed differences in alignment patterns for prenuclear rising pitch accents can be modelled with the same underlying coordinative structures/coupling modes for vocalic and tonal gestures and that surface differences arise from gradient variation in coupling strengths",
    "checked": true,
    "id": "3ebf743f94ad9537ec8d7d8db1474bed5a797b33",
    "semantic_title": "strength and structure: coupling tones with oral constriction gestures",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kleijn19_interspeech.html": {
    "title": "Salient Speech Representations Based on Cloned Networks",
    "volume": "main",
    "abstract": "We define salient features as features that are shared by signals that are defined as being equivalent by a system designer. The definition allows the designer to contribute qualitative information. We aim to find salient features that are useful as conditioning for generative networks. We extract salient features by jointly training a set of clones of an encoder network. Each network clone receives as input a different signal from a set of equivalent signals. The objective function encourages the network clones to map their input into a set of features that is identical across the clones. It additionally encourages feature independence and, optionally, reconstruction of a desired target signal by a decoder. As an application, we train a system that extracts a time-sequence of feature vectors of speech and uses it as a conditioning of a WaveNet generative system, facilitating both coding and enhancement",
    "checked": true,
    "id": "fa5c2be217b6592aebbc7726014692434d0d58dc",
    "semantic_title": "salient speech representations based on cloned networks",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ramanathi19_interspeech.html": {
    "title": "ASR Inspired Syllable Stress Detection for Pronunciation Evaluation Without Using a Supervised Classifier and Syllable Level Features",
    "volume": "main",
    "abstract": "Automatic syllable stress detection is typically performed with a supervised classifier considering manually annotated stress markings and features computed within the syllable segments derived from phoneme transcriptions and their time-aligned boundaries. However, the manual annotation is tedious and the errors in estimating segmental information could degrade stress detection accuracy. In order to circumvent these, we propose to estimate stress markings in automatic speech recognition (ASR) framework involving finite-state-transducer (FST) without using annotated stress markings and segmental information. For this, we train the ASR system with native English data along with pronunciation lexicon containing canonical stress markings and decode non-native utterances as pronunciations embedded with stress markings. In the decoding, we use an FST encoded with the pronunciations derived using phoneme transcriptions and the instructions involved in a typical manual annotation. Experiments are conducted on polysyllabic words taken from ISLE corpus containing utterances spoken by Italian and German speakers and using the ASR models trained with three corpora. Among all the three models, the highest stress detection accuracies with the proposed approach respectively on Italian & German speakers are found to be 2.07% & 1.19% higher than and comparable to those with the two supervised classification approaches used as baselines",
    "checked": true,
    "id": "ead6031f9b40ec5d259b5ec7cfac8ee6f9ea0a29",
    "semantic_title": "asr inspired syllable stress detection for pronunciation evaluation without using a supervised classifier and syllable level features",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mannem19_interspeech.html": {
    "title": "Acoustic and Articulatory Feature Based Speech Rate Estimation Using a Convolutional Dense Neural Network",
    "volume": "main",
    "abstract": "In this paper, we propose a speech rate estimation approach using a convolutional dense neural network (CDNN). The CDNN based approach uses the acoustic and articulatory features for speech rate estimation. The Mel Frequency Cepstral Coefficients (MFCCs) are used as acoustic features and the articulograms representing time-varying vocal tract profile are used as articulatory features. The articulogram is computed from a real-time magnetic resonance imaging (rtMRI) video in the midsagittal plane of a subject while speaking. However, in practice, the articulogram features are not directly available, unlike acoustic features from speech recording. Thus, we use an Acoustic-to-Articulatory Inversion method using a bidirectional long-short-term memory network which estimates the articulogram features from the acoustics. The proposed CDNN based approach using estimated articulatory features requires both acoustic and articulatory features during training but it requires only acoustic data during testing. Experiments are conducted using rtMRI videos from four subjects each speaking 460 sentences. The Pearson correlation coefficient is used to evaluate the speech rate estimation. It is found that the CDNN based approach gives a better correlation coefficient than the temporal and selected sub-band correlation (TCSSBC) based baseline scheme by 81.58% and 73.68% (relative) in seen and unseen subject conditions respectively",
    "checked": true,
    "id": "d78b4ebd047a0d38dfbcfadb53d9666e08dd6a47",
    "semantic_title": "acoustic and articulatory feature based speech rate estimation using a convolutional dense neural network",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/springenberg19_interspeech.html": {
    "title": "Predictive Auxiliary Variational Autoencoder for Representation Learning of Global Speech Characteristics",
    "volume": "main",
    "abstract": "Unsupervised learning represents an important opportunity for obtaining useful speech representations. Recently, variational autoencoders (VAEs) have been shown to extract useful representations in an unsupervised manner. These models are usually not designed to explicitly disentangle specific sources of information. When processing data of sequential nature which involves multi-timescale information, disentanglement can however be beneficial. In this paper we address this issue by developing a predictive auxiliary variational autoencoder to obtain speech representations at different timescales. We will present an auxiliary lower bound which is used to develop a model that we call the Predictive Aux-VAE. The model is designed to disentangle global from local information into a dedicated auxiliary variable. Learned representations are analysed with respect to their ability to capture global speech characteristics. We observe that representations of individual speakers are separated well in the latent space and can successfully be used in a subsequent speaker identification task where they achieve high classification accuracy, comparable to a fully supervised model. Moreover, manipulating the global variable allows to change global characteristics while retaining the local content during generation which demonstrates the success of our model to disentangle global from local information",
    "checked": true,
    "id": "bc70d4c7d375986e79875cae2d714ea0521fbd5a",
    "semantic_title": "predictive auxiliary variational autoencoder for representation learning of global speech characteristics",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/paraskevopoulos19_interspeech.html": {
    "title": "Unsupervised Low-Rank Representations for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "We examine the use of linear and non-linear dimensionality reduction algorithms for extracting low-rank feature representations for speech emotion recognition. Two feature sets are used, one based on low-level descriptors and their aggregations (IS10) and one modeling recurrence dynamics of speech (RQA), as well as their fusion. We report speech emotion recognition (SER) results for learned representations on two databases using different classification methods. Classification with low-dimensional representations yields performance improvement in a variety of settings. This indicates that dimensionality reduction is an effective way to combat the curse of dimensionality for SER. Visualization of features in two dimensions provides insight into discriminatory abilities of reduced feature sets",
    "checked": true,
    "id": "eebd7e974f84c49edbee62b1c1c8b2557d254b9e",
    "semantic_title": "unsupervised low-rank representations for speech emotion recognition",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dhiman19_interspeech.html": {
    "title": "On the Suitability of the Riesz Spectro-Temporal Envelope for WaveNet Based Speech Synthesis",
    "volume": "main",
    "abstract": "We address the problem of estimating the time-varying spectral envelope of a speech signal using a spectro-temporal demodulation technique. Unlike the conventional spectrogram, we consider a pitch-adaptive spectrogram and model a spectro-temporal patch using an amplitude- and frequency-modulated two-dimensional (2-D) cosine signal. We employ a demodulation technique based on the Riesz transform that we proposed recently to estimate the amplitude and frequency modulations. The amplitude modulation (AM) corresponds to the vocal-tract filter magnitude response (or envelope) and the frequency modulation (FM) corresponds to the excitation. We consider the AM and demonstrate its effectiveness by incorporating it as an acoustic feature for local conditioning in the statistical WaveNet vocoder for the task of speech synthesis. The quality of the synthesized speech obtained with the Riesz envelope is compared with that obtained using the envelope estimated by the WORLD vocoder. Objective measures and subjective listening tests on the CMU-Arctic database show that the quality of synthesis is superior to that obtained using the WORLD envelope. This study thus establishes the Riesz envelope as an efficient alternative to the WORLD envelope",
    "checked": true,
    "id": "543da25d1d63ac5d607181021d2b2d5ba35b660e",
    "semantic_title": "on the suitability of the riesz spectro-temporal envelope for wavenet based speech synthesis",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xu19_interspeech.html": {
    "title": "Autonomous Emotion Learning in Speech: A View of Zero-Shot Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Conventionally, speech emotion recognition is achieved using passive learning approaches. Differing from such approaches, we herein propose and develop a dynamic method of autonomous emotion learning based on zero-shot learning. The proposed methodology employs emotional dimensions as the attributes in the zero-shot learning paradigm, resulting in two phases of learning, namely attribute learning and label learning. Attribute learning connects the paralinguistic features and attributes utilising speech with known emotional labels, while label learning aims at defining unseen emotions through the attributes. The experimental results achieved on the CINEMO corpus indicate that zero-shot learning is a useful technique for autonomous speech-based emotion learning, achieving accuracies considerably better than chance level and an attribute-based gold-standard setup. Furthermore, different emotion recognition tasks, emotional attributes, and employed approaches strongly influence system performance",
    "checked": true,
    "id": "98960684fc96c0f9c2594bec3b69e59bda567713",
    "semantic_title": "autonomous emotion learning in speech: a view of zero-shot speech emotion recognition",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sudhakara19_interspeech.html": {
    "title": "An Improved Goodness of Pronunciation (GoP) Measure for Pronunciation Evaluation with DNN-HMM System Considering HMM Transition Probabilities",
    "volume": "main",
    "abstract": "Goodness of pronunciation (GoP) is typically formulated with Gaussian mixture model-hidden Markov model (GMM-HMM) based acoustic models considering HMM state transition probabilities (STPs) and GMM likelihoods of context dependent phonemes. On the other hand, deep neural network (DNN)-HMM based acoustic models employed sub-phonemic (senone) posteriors instead of GMM likelihoods along with STPs. However, each senone is shared across many states; thus, there is no one-to-one correspondence between them. In order to circumvent this, most of the existing works have proposed modifications to the GoP formulation considering only posteriors neglecting the STPs. In this work, we derive a formulation for the GoP and it results in the formulation involving both senone posteriors and STPs. Further, we illustrate the steps to implement the proposed GoP formulation in Kaldi, a state-of-the-art automatic speech recognition toolkit. Experiments are conducted on English data collected from Indian speakers using acoustic models trained with native English data from LibriSpeech and Fisher-English corpora. The highest improvement in the correlation coefficient between the scores from the formulations and the expert ratings is found to be 14.89% (relative) better with the proposed approach compared to the best of the existing formulations that don't include STPs",
    "checked": true,
    "id": "aa07cb8a956ba9f7bf7a425997bfcb37a0e4b6cc",
    "semantic_title": "an improved goodness of pronunciation (gop) measure for pronunciation evaluation with dnn-hmm system considering hmm transition probabilities",
    "citation_count": 41
  },
  "https://www.isca-speech.org/archive/interspeech_2019/saha19b_interspeech.html": {
    "title": "Low Resource Automatic Intonation Classification Using Gated Recurrent Unit (GRU) Networks Pre-Trained with Synthesized Pitch Patterns",
    "volume": "main",
    "abstract": "Second language learners of British English (BE) are typically trained to learn four intonation classes — Glide-up, Glide-down, Dive and Take-off. We predict the intonation class in a learner's utterance by modeling the temporal dependencies in the pitch patterns with gated recurrent unit (GRU) networks. For these, we pre-train the GRU network using a set of synthesized pitch patterns representing each intonation class. For the synthesis, we propose to obtain pitch patterns from the tone sequences representing each intonation class obtained from domain knowledge. Experiments are conducted on speech data collected from experts in a spoken English training material for teaching BE intonation. The absolute improvements in the unweighted average recall (UAR) using the proposed scheme with pre-training are found to be 4.14% and 6.01% respectively over the proposed approach without pre-training and the baseline scheme that uses hidden Markov models (HMMs)",
    "checked": true,
    "id": "acee10c113dacb578d603a69f091dcb688886482",
    "semantic_title": "low resource automatic intonation classification using gated recurrent unit (gru) networks pre-trained with synthesized pitch patterns",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vasquezcorrea19b_interspeech.html": {
    "title": "Apkinson: A Mobile Solution for Multimodal Assessment of Patients with Parkinson's Disease",
    "volume": "main",
    "abstract": "Parkinson's disease is a neurological disorder that produces different motor impairments in the patients. The longitudinal assessment of the neurological state of patients is important to improve their quality of life. We introduced Apkinson, a smartphone application to evaluate continuously the speech and movement deficits of Parkinson's patients, who receive feedback about their current state after performing different exercises. The speech assessment considers phonation, articulation, and prosody capabilities of the patients. Movement exercises captured with the inertial sensors of the smartphone evaluated symptoms in the upper and lower limbs",
    "checked": true,
    "id": "dea4695e88ce239222a76605e6db34cd790b5459",
    "semantic_title": "apkinson: a mobile solution for multimodal assessment of patients with parkinson's disease",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kiss19_interspeech.html": {
    "title": "Depression State Assessment: Application for Detection of Depression by Speech",
    "volume": "main",
    "abstract": "We present an application that detects depression by speech based on a speech feature extraction engine. The input of the application is a read speech sample and the output is predicted depression severity level (Beck Depression Inventory). The application analyses the speech sample and evaluates it using support vector regression (SVR). The developed system could assist general medical staff if no specialist is present to aid the diagnosis. If there is a suspicion that the speaker is suffering from depression, it is inevitable to seek special medical assistance. The application supports five native languages: English, French, German, Hungarian and Italian",
    "checked": true,
    "id": "899c31418f8ba698e97bd97a2a113300327942e7",
    "semantic_title": "depression state assessment: application for detection of depression by speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yarra19_interspeech.html": {
    "title": "SPIRE-fluent: A Self-Learning App for Tutoring Oral Fluency to Second Language English Learners",
    "volume": "main",
    "abstract": "Second language (L2) learners often achieve oral fluency by correct pronunciation of words with appropriate pauses. It has been shown that the L2 learners improve their language skills using mobile apps in a self-learning manner. Effective learning is possible with apps that provide detailed feedback. However, apps that train oral fluency in an automatic way are not available. In this work, we present SPIRE-fluent app, which provides an automatic feedback with scores representing learner's pronunciation quality, for each word in a sentence and for the entire sentence. The word specific scores are computed based on the correctness of pronunciation with respect to the expert's audio. Further, the app displays the syllables uttered and a set of two types of pauses produced by the learners and the expert while speaking the sentence. Considering this as a feedback, the learner can correct their mistakes based on the mismatches between those utterances. In addition, it also estimates any pause made by the learners within a word and highlights the syllable containing the phoneme preceding the pause",
    "checked": true,
    "id": "f8798fa1886254d9a549968c9eb0f3b6d0a0ca79",
    "semantic_title": "spire-fluent: a self-learning app for tutoring oral fluency to second language english learners",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nissen19b_interspeech.html": {
    "title": "Using Real-Time Visual Biofeedback for Second Language Instruction",
    "volume": "main",
    "abstract": "This demonstration will illustrate how using real-time visual biofeedback, through a relatively new type of electropalatographic (EPG) sensor, might facilitate improved pronunciation for learners of a second language (L2). The manner in which the EPG sensor is created and its use to track lingua-palatal articulation patterns will be described to individuals. This presentation will also include an explanation of how a student can visualize the contact patterns of their speech using the associated instructional software. A brief tutorial on the features of the instructional software will also be explained during the \"show and tell\" presentation",
    "checked": true,
    "id": "3016e98fff247bcba150ffa3aa326a39ea1dd0f8",
    "semantic_title": "using real-time visual biofeedback for second language instruction",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/miwardelli19_interspeech.html": {
    "title": "Splash: Speech and Language Assessment in Schools and Homes",
    "volume": "main",
    "abstract": "This paper presents a tablet-based app for Speech and Language Assessment in Schools and Homes ( Splash) to provide a first screening for young children aged 4–6 years to assess their speech and language skills. The app aims to be easy-to-administer with an adult, such as a teacher or parent, directing the child through the tasks. Three fun games have been developed to assess receptive language, expressive language and connected speech, respectively. Currently in proof-of-concept mode, when complete Splash will use automatic spoken language processing to give an instant estimate of a child's communication ability and provide guidance on whether to speak specialist support. While not a diagnostic tool, the aim is for Splash to be used to provide immediate reassurance or direction to concerned parents, guardians or teachers as it can be administered by anyone, anywhere",
    "checked": true,
    "id": "f45b3de380ebedd00262246e3c53b136e6891fae",
    "semantic_title": "splash: speech and language assessment in schools and homes",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/annand19_interspeech.html": {
    "title": "Using Ultrasound Imaging to Create Augmented Visual Biofeedback for Articulatory Practice",
    "volume": "main",
    "abstract": "Ultrasound images of the tongue surface can be used to provide real-time visual feedback for clinical practitioners and speakers adjusting pronunciation patterns. However, rapid and complex movements of the tongue can be difficult to interpret and directly relate to desired changes. We are developing a method for simplified visual feedback controlled by efficient, real-time tracking of tongue contours in ultrasound images. Our feedback and control paradigm are briefly discussed, and video of a potential game-like biofeedback stimulus is demonstrated",
    "checked": true,
    "id": "829cfd92b8745b3a69467a6a1327a77fd774e46e",
    "semantic_title": "using ultrasound imaging to create augmented visual biofeedback for articulatory practice",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/radostev19_interspeech.html": {
    "title": "Speech-Based Web Navigation for Limited Mobility Users",
    "volume": "main",
    "abstract": "We present a novel approach that introduces the strengths of voice assistants into a web browser that makes the task of web navigation a lot more accessible to all users, especially under limited mobility circumstances. Voice assistants have now been widely adopted and is providing great user experience for getting simple actions done quickly or getting a quick answer to a question. On the other hand, the benefits of voice assistants have not yet penetrated to the scenarios such as web navigation, which has so far been driven by mouse, keyboard and touch-based input only. In this paper, we demonstrate our speech-based web navigation system, and show that our system improves the completion of the web navigation task on both PC and mobile phone significantly as compared with an out-of-the-box voice assistants on this task",
    "checked": true,
    "id": "6d00006d18a1a9348b08434440e90a9b26c577c8",
    "semantic_title": "speech-based web navigation for limited mobility users",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schultz19_interspeech.html": {
    "title": "Biosignal Processing for Human-Machine Interaction",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ed97347c1be5df907a2f6a950fead0776830b9d5",
    "semantic_title": "biosignal processing for human-machine interaction",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ryant19_interspeech.html": {
    "title": "The Second DIHARD Diarization Challenge: Dataset, Task, and Baselines",
    "volume": "main",
    "abstract": "This paper introduces the second DIHARD challenge, the second in a series of speaker diarization challenges intended to improve the robustness of diarization systems to variation in recording equipment, noise conditions, and conversational domain. The challenge comprises four tracks evaluating diarization performance under two input conditions (single channel vs. multi-channel) and two segmentation conditions (diarization from a reference speech segmentation vs. diarization from scratch). In order to prevent participants from overtuning to a particular combination of recording conditions and conversational domain, recordings are drawn from a variety of sources ranging from read audiobooks to meeting speech, to child language acquisition recordings, to dinner parties, to web video. We describe the task and metrics, challenge design, datasets, and baseline systems for speech enhancement, speech activity detection, and diarization",
    "checked": true,
    "id": "ff88699c6bac1b289272c445581541ad66848044",
    "semantic_title": "the second dihard diarization challenge: dataset, task, and baselines",
    "citation_count": 147
  },
  "https://www.isca-speech.org/archive/interspeech_2019/singh19_interspeech.html": {
    "title": "LEAP Diarization System for the Second DIHARD Challenge",
    "volume": "main",
    "abstract": "This paper presents the LEAP System, developed for the Second DIHARD diarization Challenge. The evaluation data in the challenge is composed of multi-talker speech in restaurants, doctor-patient conversations, child language acquisition recordings in home environments and audio extracted YouTube videos. The LEAP system is developed using two types of embeddings, one based on i-vector representations and the other one based on x-vector representations. The initial diarization output obtained using agglomerative hierarchical clustering (AHC) done on the probabilistic linear discriminant analysis (PLDA) scores is refined using the Variational-Bayes hidden Markov model (VB-HMM) model. We propose a modified VB-HMM model with posterior scaling which provides significant improvements in the final diarization error rate (DER). We also use a domain compensation on the i-vector features to reduce the mis-match between training and evaluation conditions. N(s)TN(s)TN(s)T Using the proposed approaches, we obtain relative improvements in DER of about 7.1% relative for the best individual system over the DIHARD baseline system and about 13.7% relative for the final system combination on evaluation set. An analysis performed using the proposed posterior scaling method shows that scaling results in improved discrimination among the HMM states in the VB-HMM",
    "checked": true,
    "id": "5bd8938366fcc2cfa6faaf99291873a3a912b650",
    "semantic_title": "leap diarization system for the second dihard challenge",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vinals19_interspeech.html": {
    "title": "ViVoLAB Speaker Diarization System for the DIHARD 2019 Challenge",
    "volume": "main",
    "abstract": "This paper presents the latest improvements in Speaker Diarization obtained by ViVoLAB research group for the 2019 DIHARD Diarization Challenge. This evaluation seeks the improvement of the diarization task in adverse conditions. For this purpose, the audio recordings involve multiple scenarios with no restrictions in terms of speakers, overlapped speech nor quality of the audio. Our submission follows the traditional segmentation-clustering-resegmentation pipeline: Speaker embeddings are extracted from acoustic segments with a single speaker on them, later clustered by means of a PLDA. Our contribution in this work is focused on the clustering step. We present results with our Variational Bayes PLDA clustering and our tree-based clustering strategy, which sequentially assigns the different embeddings to its corresponding speaker according to a PLDA model. Both strategies compare multiple diarization hypotheses and choose their candidate one according to a generative criterion. We also analyze the impact of the different available embeddings in the state-of-the-art with both clustering approaches",
    "checked": true,
    "id": "81e0efe29fe884b81e9d3e956dd31b97ba258d34",
    "semantic_title": "vivolab speaker diarization system for the dihard 2019 challenge",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zajic19_interspeech.html": {
    "title": "UWB-NTIS Speaker Diarization System for the DIHARD II 2019 Challenge",
    "volume": "main",
    "abstract": "In this paper, we present our system developed by the team from the New Technologies for the Information Society (NTIS) research center of the University of West Bohemia in Pilsen, for the Second DIHARD Speech Diarization Challenge. The base of our system follows the currently-standard approach of segmentation, i/x-vector extraction, clustering, and resegmentation. The hyperparameters for each of the subsystems were selected according to the domain classifier trained on the development set of DIHARD II. We compared our system with results from the Kaldi diarization (with i/x-vectors) and combined these systems. At the time of writing of this abstract, our best submission achieved a DER of 23.47% and a JER of 48.99% on the evaluation set (in Track 1 using reference SAD)",
    "checked": true,
    "id": "5b56e4cbb248c94177b01ad993f6742ee7c72ab4",
    "semantic_title": "uwb-ntis speaker diarization system for the dihard ii 2019 challenge",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/park19b_interspeech.html": {
    "title": "The Second DIHARD Challenge: System Description for USC-SAIL Team",
    "volume": "main",
    "abstract": "In this paper, we describe components that form a part of USC-SAIL team's submissions to Track 1 and Track 2 of the second DIHARD speaker diarization challenge. We describe each module in our speaker diarization pipeline and explain the rationale behind our choice of algorithms for each module, while comparing the Diarization Error Rate (DER) against different module combinations. We propose a clustering scheme based on spectral clustering that yields competitive performance. Moreover, we introduce an overlap detection scheme and a re-segmentation system for speaker diarization and investigate their performances using controlled and in-the-wild conditions. In addition, we describe the additional components that will be integrated to our speaker diarization system. To pursue the best performance, we compare our system with the state-of-the-art methods that are presented in the previous challenge and literature. We include preliminary results of our speaker diarization system on the evaluation data from the second DIHARD challenge",
    "checked": true,
    "id": "9a81280f8e8982f39fffdd090d04e3ca5138e6ce",
    "semantic_title": "the second dihard challenge: system description for usc-sail team",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/novoselov19_interspeech.html": {
    "title": "Speaker Diarization with Deep Speaker Embeddings for DIHARD Challenge II",
    "volume": "main",
    "abstract": "This paper describes the ITMO University (DI-IT team) speaker diarization systems submitted to DIHARD Challenge II. As with DIHARD I, this challenge is focused on diarization task for microphone recordings in varying difficult conditions. According to the results of the previous DIHARD I Challenge state-of-the-art diarization systems are based on x-vector embeddings. Such embeddings are clustered using agglomerative hierarchical clustering (AHC) algorithm by means of PLDA scoring. Current research continues the investigation of deep speaker embedding efficiency for the speaker diarization task. This paper explores new types of embedding extractors with different deep neural network architectures and training strategies. We also used AHC to perform embeddings clustering. Alternatively to the PLDA scoring in our AHC procedure we used discriminatively trained cosine similarity metric learning (CSML) model for scoring. Moreover we focused on the optimal AHC threshold tuning according to the specific speech quality. Environment classifier was preliminary trained on development set to predict acoustic conditions for this purpose. We show that such threshold adaptation scheme allows to reduce diarization error rate compared to common AHC threshold for all conditions",
    "checked": true,
    "id": "627a538c1f1a4ebbb778287d80b725fc491e76b6",
    "semantic_title": "speaker diarization with deep speaker embeddings for dihard challenge ii",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/todisco19_interspeech.html": {
    "title": "ASVspoof 2019: Future Horizons in Spoofed and Fake Audio Detection",
    "volume": "main",
    "abstract": "ASVspoof, now in its third edition, is a series of community-led challenges which promote the development of countermeasures to protect automatic speaker verification (ASV) from the threat of spoofing. Advances in the 2019 edition include: (i) a consideration of both logical access (LA) and physical access (PA) scenarios and the three major forms of spoofing attack, namely synthetic, converted and replayed speech; (ii) spoofing attacks generated with state-of-the-art neural acoustic and waveform models; (iii) an improved, controlled simulation of replay attacks; (iv) use of the tandem detection cost function (t-DCF) that reflects the impact of both spoofing and countermeasures upon ASV reliability. Even if ASV remains the core focus, in retaining the equal error rate (EER) as a secondary metric, ASVspoof also embraces the growing importance of fake audio detection. ASVspoof 2019 attracted the participation of 63 research teams, with more than half of these reporting systems that improve upon the performance of two baseline spoofing countermeasures. This paper describes the 2019 database, protocols and challenge results. It also outlines major findings which demonstrate the real progress made in protecting against the threat of spoofing and fake audio",
    "checked": true,
    "id": "396af8a1241d5689a2d730cc76e947f78fe998dd",
    "semantic_title": "asvspoof 2019: future horizons in spoofed and fake audio detection",
    "citation_count": 369
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lai19b_interspeech.html": {
    "title": "ASSERT: Anti-Spoofing with Squeeze-Excitation and Residual Networks",
    "volume": "main",
    "abstract": "We present JHU's system submission to the ASVspoof 2019 Challenge: Anti-Spoofing with Squeeze-Excitation and Residual neTworks (ASSERT). Anti-spoofing has gathered more and more attention since the inauguration of the ASVspoof Challenges, and ASVspoof 2019 dedicates to address attacks from all three major types: text-to-speech, voice conversion, and replay. Built upon previous research work on Deep Neural Network (DNN), ASSERT is a pipeline for DNN-based approach to anti-spoofing. ASSERT has four components: feature engineering, DNN models, network optimization and system combination, where the DNN models are variants of squeeze-excitation and residual networks. We conducted an ablation study of the effectiveness of each component on the ASVspoof 2019 corpus, and experimental results showed that ASSERT obtained more than 93% and 17% relative improvements over the baseline systems in the two sub-challenges in ASVspoof 2019, ranking ASSERT one of the top performing systems. Code and pretrained models are made publicly available",
    "checked": true,
    "id": "48ae745189239c05b41cceccfbecca138e4c2980",
    "semantic_title": "assert: anti-spoofing with squeeze-excitation and residual networks",
    "citation_count": 109
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chettri19_interspeech.html": {
    "title": "Ensemble Models for Spoofing Detection in Automatic Speaker Verification",
    "volume": "main",
    "abstract": "Detecting spoofing attempts of automatic speaker verification (ASV) systems is challenging, especially when using only one modelling approach. For robustness, we use both deep neural networks and traditional machine learning models and combine them as ensemble models through logistic regression. They are trained to detect logical access (LA) and physical access (PA) attacks on the dataset released as part of the ASV Spoofing and Countermeasures Challenge 2019. We propose dataset partitions that ensure different attack types are present during training and validation to improve system robustness. Our ensemble model outperforms all our single models and the baselines from the challenge for both attack types. We investigate why some models on the PA dataset strongly outperform others and find that spoofed recordings in the dataset tend to have longer silences at the end than genuine ones. By removing them, the PA task becomes much more challenging, with the tandem detection cost function (t-DCF) of our best single model rising from 0.1672 to 0.5018 and equal error rate (EER) increasing from 5.98% to 19.8% on the development set",
    "checked": true,
    "id": "bf0cc0febd0e181f1407acf83f7d3104257592cf",
    "semantic_title": "ensemble models for spoofing detection in automatic speaker verification",
    "citation_count": 72
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cai19_interspeech.html": {
    "title": "The DKU Replay Detection System for the ASVspoof 2019 Challenge: On Data Augmentation, Feature Representation, Classification, and Fusion",
    "volume": "main",
    "abstract": "This paper describes our DKU replay detection system for the ASVspoof 2019 challenge. The goal is to develop spoofing countermeasure for automatic speaker recognition in physical access scenario. We leverage the countermeasure system pipeline from four aspects, including the data augmentation, feature representation, classification, and fusion. First, we introduce an utterance-level deep learning framework for anti-spoofing. It receives the variable-length feature sequence and outputs the utterance-level scores directly. Based on the framework, we try out various kinds of input feature representations extracted from either the magnitude spectrum or phase spectrum. Besides, we also perform the data augmentation strategy by applying the speed perturbation on the raw waveform. Our best single system employs a residual neural network trained by the speed-perturbed group delay gram. It achieves EER of 1.04% on the development set, as well as EER of 1.08% on the evaluation set. Finally, using the simple average score from several single systems can further improve the performance. EER of 0.24% on the development set and 0.66% on the evaluation set is obtained for our primary system",
    "checked": true,
    "id": "3f90b660d49447c778dd1b5f115de31ea881e781",
    "semantic_title": "the dku replay detection system for the asvspoof 2019 challenge: on data augmentation, feature representation, classification, and fusion",
    "citation_count": 47
  },
  "https://www.isca-speech.org/archive/interspeech_2019/biaobrzeski19_interspeech.html": {
    "title": "Robust Bayesian and Light Neural Networks for Voice Spoofing Detection",
    "volume": "main",
    "abstract": "We present a replay attack detection system consisting of two convolutional neural network models. The first model consists of a small Bayesian neural network, motivated by the hypothesis that Bayesian models are robust to overfitting. The second one uses a bigger architecture, LCNN, extended with several regularization techniques to improve generalization. Our experiments, considering both size of the networks and use of the Bayesian approach, indicated that smaller networks are sufficient to achieve competitive results. To better estimate the performance against unseen spoofing methods, the final models were selected using novel Attack-Out Cross-Validation. In this procedure each model was tested on a subset of data containing not only previously unseen speakers, but also unseen spoofing attacks. The system was submitted to ASVspoof 2019 challenge's PA condition and achieved a t-DCF score of 0.0219 and EER of 0.88% on the evaluation dataset, which is a 10 times relative improvement over the baseline",
    "checked": true,
    "id": "0820c4caf3f3988496285e12da165efd482e6dbe",
    "semantic_title": "robust bayesian and light neural networks for voice spoofing detection",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lavrentyeva19_interspeech.html": {
    "title": "STC Antispoofing Systems for the ASVspoof2019 Challenge",
    "volume": "main",
    "abstract": "This paper describes the Speech Technology Center (STC) antispoofing systems submitted to the ASVspoof 2019 challenge. The ASVspoof2019 is the extended version of the previous challenges and includes 2 evaluation conditions: logical access use-case scenario with speech synthesis and voice conversion attack types and physical access use-case scenario with replay attacks. During the challenge we developed anti-spoofing solutions for both scenarios. The proposed systems are implemented using deep learning approach and are based on different types of acoustic features. We enhanced Light CNN architecture previously considered by the authors for replay attacks detection and which performed high spoofing detection quality during the ASVspoof2017 challenge. In particular here we investigate the efficiency of angular margin based softmax activation for training robust deep Light CNN classifier to solve the mentioned-above tasks. Submitted systems achieved EER of 1.86% in logical access scenario and 0.54% in physical access scenario on the evaluation part of the Challenge corpora. High performance obtained for the unknown types of spoofing attacks demonstrates the stability of the offered approach in both evaluation conditions",
    "checked": true,
    "id": "12c3c3ef97ed245268a98a7eb273314c59c3c054",
    "semantic_title": "stc antispoofing systems for the asvspoof2019 challenge",
    "citation_count": 165
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yang19b_interspeech.html": {
    "title": "The SJTU Robust Anti-Spoofing System for the ASVspoof 2019 Challenge",
    "volume": "main",
    "abstract": "The robustness of an anti-spoofing system is progressively more important in order to develop a reliable speaker verification system. Previous challenges and datasets mainly focus on a specific type of spoofing attacks. The ASVspoof 2019 edition is the first challenge to address two major spoofing types — logical and physical access. This paper presents the SJTU's submitted anti-spoofing system to the ASVspoof 2019 challenge. Log-CQT features are developed in conjunction with multi-layer convolutional neural networks for robust performance across both subtasks. CNNs with gradient linear units (GLU) activations are utilized for spoofing detection. The proposed system shows consistent performance improvement over all types of spoofing attacks. Our primary submissions achieve the 5 and 8 positions for the logical and physical access respectively. Moreover, our contrastive submission to the PA task exhibits better generalization compared to our primary submission, and achieves a comparable performance to the 3 position of the challenge",
    "checked": true,
    "id": "ace08cdd71a76b95c29cee1a90f671cb5e123235",
    "semantic_title": "the sjtu robust anti-spoofing system for the asvspoof 2019 challenge",
    "citation_count": 37
  },
  "https://www.isca-speech.org/archive/interspeech_2019/alluri19_interspeech.html": {
    "title": "IIIT-H Spoofing Countermeasures for Automatic Speaker Verification Spoofing and Countermeasures Challenge 2019",
    "volume": "main",
    "abstract": "The ASVspoof 2019 challenge focuses on countermeasures for all major spoofing attacks, namely speech synthesis (SS), voice conversion (VC), and replay spoofing attacks. This paper describes the IIIT-H spoofing countermeasures developed for ASVspoof 2019 challenge. In this study, three instantaneous cepstral features namely, single frequency cepstral coefficients, zero time windowing cepstral coefficients, and instantaneous frequency cepstral coefficients are used as front-end features. A Gaussian mixture model is used as back-end classifier. The experimental results on ASVspoof 2019 dataset reveal that the proposed instantaneous features are efficient in detecting VC and SS based attacks. In detecting replay attacks, proposed features are comparable with baseline systems. Further analysis is carried out using metadata to assess the impact of proposed countermeasures on different synthetic speech generating algorithm/replay configurations",
    "checked": true,
    "id": "6c3d696a643d1791a88b87cf58ff8a90a5439b93",
    "semantic_title": "iiit-h spoofing countermeasures for automatic speaker verification spoofing and countermeasures challenge 2019",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19c_interspeech.html": {
    "title": "Anti-Spoofing Speaker Verification System with Multi-Feature Integration and Multi-Task Learning",
    "volume": "main",
    "abstract": "Speaker anti-spoofing is crucial to prevent security breaches when the speaker verification systems encounter the spoofed attacks from the advanced speech synthesis algorithms and high fidelity replay devices. In this paper, we propose a framework based on multiple features integration and multi-task learning (MFMT) for improving anti-spoofing performance. It is important to integrate the complementary information of multiple spectral features within the network, such as MFCC, CQCC, Fbank, etc., as often a single kind of feature is not enough to grasp the global spoofing cues and it generalizes poorly. Furthermore, we propose a helpful butterfly unit (BU) for multi-task learning to propagate the shared representations between the binary decision task and the other auxiliary task. The BU can obtain task representations of other branch during forward propagation and prevent the gradient from assimilating the branch during back propagation. Our proposed system yielded an EER of 9.01% on ASVspoof 2017, while the best single system and the average scores fusion obtained the evaluation EER of 2.39% and 0.96% on ASVspoof 2019 PA, respectively",
    "checked": true,
    "id": "c6657d083b4343144e51f1ec70b72f117b2e8f9b",
    "semantic_title": "anti-spoofing speaker verification system with multi-feature integration and multi-task learning",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/williams19b_interspeech.html": {
    "title": "Speech Replay Detection with x-Vector Attack Embeddings and Spectral Features",
    "volume": "main",
    "abstract": "We present our system submission to the ASVspoof 2019 Challenge Physical Access (PA) task. The objective for this challenge was to develop a countermeasure that identifies speech audio as either bona fide or intercepted and replayed. The target prediction was a value indicating that a speech segment was bona fide (positive values) or \"spoofed\" (negative values). Our system used convolutional neural networks (CNNs) and a representation of the speech audio that combined x-vector attack embeddings with signal processing features. The x-vector attack embeddings were created from mel-frequency cepstral coefficients (MFCCs) using a time-delay neural network (TDNN). These embeddings jointly modeled 27 different environments and 9 types of attacks from the labeled data. We also used sub-band spectral centroid magnitude coefficients (SCMCs) as features. We included an additive Gaussian noise layer during training as a way to augment the data to make our system more robust to previously unseen attack examples. We report system performance using the tandem detection cost function (tDCF) and equal error rate (EER). Our approach performed better that both of the challenge baselines. Our technique suggests that our x-vector attack embeddings can help regularize the CNN predictions even when environments or attacks are more challenging",
    "checked": true,
    "id": "87d2ff88109add62b4b3fa82b0e13a2e77827e8e",
    "semantic_title": "speech replay detection with x-vector attack embeddings and spectral features",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2019/das19_interspeech.html": {
    "title": "Long Range Acoustic Features for Spoofed Speech Detection",
    "volume": "main",
    "abstract": "Speaker verification systems in practice are vulnerable to spoofing attacks. The high quality recording and playback devices make replay attack a real threat to speaker verification. Additionally, the furtherance in voice conversion and speech synthesis has produced perceptually natural sounding speech. The ASVspoof 2019 challenge is organized to study the robustness of countermeasures against such attacks, which cover two common modes of attacks, logical and physical access. The former deals with synthetic attacks arising from voice conversion and text-to-speech techniques, whereas the latter deals with replay attacks. In this work, we explore several novel countermeasures based on long range acoustic features that are found to be effective for spoofing attack detection. The long range features capture different aspects of long range information as they are computed from subbands and octave power spectrum in contrast to the conventional way from linear power spectrum. These novel features are combined with the other known features for improved detection of spoofing attacks. We obtain a tandem detection cost function of 0.1264 and 0.1381 (equal error rate 4.13% and 5.95%) for logical and physical access on the best combined system submitted to the challenge",
    "checked": true,
    "id": "85b5ef05d49c16d78dcbfbc5f65118b272de46ba",
    "semantic_title": "long range acoustic features for spoofed speech detection",
    "citation_count": 33
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chang19b_interspeech.html": {
    "title": "Transfer-Representation Learning for Detecting Spoofing Attacks with Converted and Synthesized Speech in Automatic Speaker Verification System",
    "volume": "main",
    "abstract": "In this paper, we study a countermeasure module to detect spoofing attacks with converted or synthesized speech in tandem automatic speaker verification (ASV). Our approach integrates representation learning and transfer learning methods. For representation learning, good embedding network functions are learned from audio signals with the goal to distinguish different types of spoofing attacks. For transfer learning, the embedding network functions are used to initialize fine-tuning networks. We experiment well-known neural network architectures and front-end raw features to diversify and strengthen the information source for embedding. We participate in the 2019 Automatic Speaker Verification Spoofing and Countermeasures Challenge (ASVspoof 2019) and evaluate the proposed methods with the logical access condition tasks for detecting converted speech and synthesized speech. On the ASVspoof 2019 development set, our best single system achieves a minimum tandem decision cost function of nearly 0 during system development. On the ASVspoof 2019 evaluation set, our primary system achieves a minimum tandem decision cost of 0.1791, and an equal error rate (EER) of 9.08%. Our system does not have over-training issue as it achieves decent performance with unseen test data of the types presented in training, yet the generalization gap is not small with mismatched test data types",
    "checked": true,
    "id": "370307aecb7d0c627014159cf852346db6a225f7",
    "semantic_title": "transfer-representation learning for detecting spoofing attacks with converted and synthesized speech in automatic speaker verification system",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gomezalanis19_interspeech.html": {
    "title": "A Light Convolutional GRU-RNN Deep Feature Extractor for ASV Spoofing Detection",
    "volume": "main",
    "abstract": "The aim of this work is to develop a single anti-spoofing system which can be applied to effectively detect all the types of spoofing attacks considered in the ASVspoof 2019 Challenge: text-to-speech, voice conversion and replay based attacks. To achieve this, we propose the use of a Light Convolutional Gated Recurrent Neural Network (LC-GRNN) as a deep feature extractor to robustly represent speech signals as utterance-level embeddings, which are later used by a back-end recognizer which performs the final genuine/spoofed classification. This novel architecture combines the ability of light convolutional layers for extracting discriminative features at frame level with the capacity of gated recurrent unit based RNNs for learning long-term dependencies of the subsequent deep features. The proposed system has been presented as a contribution to the ASVspoof 2019 Challenge, and the results show a significant improvement in comparison with the baseline systems. Moreover, experiments were also carried out on the ASVspoof 2015 and 2017 corpora, and the results indicate that our proposal clearly outperforms other popular methods recently proposed and other similar deep feature based systems",
    "checked": true,
    "id": "a692a7c971238a24b2ae882a1b6925946ea5e498",
    "semantic_title": "a light convolutional gru-rnn deep feature extractor for asv spoofing detection",
    "citation_count": 54
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zeinali19_interspeech.html": {
    "title": "Detecting Spoofing Attacks Using VGG and SincNet: BUT-Omilia Submission to ASVspoof 2019 Challenge",
    "volume": "main",
    "abstract": "In this paper, we present the system description of the joint efforts of Brno University of Technology (BUT) and Omilia — Conversational Intelligence for the ASVSpoof2019 Spoofing and Countermeasures Challenge. The primary submission for Physical access (PA) is a fusion of two VGG networks, trained on single and two-channels features. For Logical access (LA), our primary system is a fusion of VGG and the recently introduced SincNet architecture. The results on PA show that the proposed networks yield very competitive performance in all conditions and achieved 86% relative improvement compared to the official baseline. On the other hand, the results on LA showed that although the proposed architecture and training strategy performs very well on certain spoofing attacks, it fails to generalize to certain attacks that are unseen during training",
    "checked": true,
    "id": "de0a7b8fbb3edef9114640889086f63415c9a25a",
    "semantic_title": "detecting spoofing attacks using vgg and sincnet: but-omilia submission to asvspoof 2019 challenge",
    "citation_count": 49
  },
  "https://www.isca-speech.org/archive/interspeech_2019/alzantot19_interspeech.html": {
    "title": "Deep Residual Neural Networks for Audio Spoofing Detection",
    "volume": "main",
    "abstract": "The state-of-art models for speech synthesis and voice conversion are capable of generating synthetic speech that is perceptually indistinguishable from bonafide human speech. These methods represent a threat to the automatic speaker verification (ASV) systems. Additionally, replay attacks where the attacker uses a speaker to replay a previously recorded genuine human speech are also possible. In this paper, we present our solution for the ASVSpoof2019 competition, which aims to develop countermeasure systems that distinguish between spoofing attacks and genuine speeches. Our model is inspired by the success of residual convolutional networks in many classification tasks. We build three variants of a residual convolutional neural network that accept different feature representations (MFCC, log-magnitude STFT, and CQCC) of input. We compare the performance achieved by our model variants and the competition baseline models. In the logical access scenario, the fusion of our models has zero t-DCF cost and zero equal error rate (EER), as evaluated on the development set. On the evaluation set, our model fusion improves the t-DCF and EER by 25% compared to the baseline algorithms. Against physical access replay attacks, our model fusion improves the baseline algorithms t-DCF and EER scores by 71% and 75% on the evaluation set, respectively",
    "checked": true,
    "id": "deb8ee23745d3f11b36321324ce2bf1046459b49",
    "semantic_title": "deep residual neural networks for audio spoofing detection",
    "citation_count": 88
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jung19_interspeech.html": {
    "title": "Replay Attack Detection with Complementary High-Resolution Information Using End-to-End DNN for the ASVspoof 2019 Challenge",
    "volume": "main",
    "abstract": "In this study, we concentrate on replacing the process of extracting hand-crafted acoustic feature with end-to-end DNN using complementary high-resolution spectrograms. As a result of advance in audio devices, typical characteristics of a replayed speech based on conventional knowledge alter or diminish in unknown replay configurations. Thus, it has become increasingly difficult to detect spoofed speech with a conventional knowledge-based approach. To detect unrevealed characteristics that reside in a replayed speech, we directly input spectrograms into an end-to-end DNN without knowledge-based intervention. Explorations dealt in this study that differentiates from existing spectrogram-based systems are twofold: complementary information and high-resolution. Spectrograms with different information are explored, and it is shown that additional information such as the phase information can be complementary. High-resolution spectrograms are employed with the assumption that the difference between a bona-fide and a replayed speech exists in the details. Additionally, to verify whether other features are complementary to spectrograms, we also examine raw waveform and an i-vector based system. Experiments conducted on the ASVspoof 2019 physical access challenge show promising results, where t-DCF and equal error rates are 0.0570 and 2.45% for the evaluation set, respectively",
    "checked": true,
    "id": "d53771e89b31c9663f29a5231b57ef2ed9aa17ec",
    "semantic_title": "replay attack detection with complementary high-resolution information using end-to-end dnn for the asvspoof 2019 challenge",
    "citation_count": 39
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dunbar19_interspeech.html": {
    "title": "The Zero Resource Speech Challenge 2019: TTS Without T",
    "volume": "main",
    "abstract": "We present the Zero Resource Speech Challenge 2019, which proposes to build a speech synthesizer without any text or phonetic labels: hence, TTS without T (text-to-speech without text). We provide raw audio for a target voice in an unknown language (the Voice dataset), but no alignment, text or labels. Participants must discover subword units in an unsupervised way (using the Unit Discovery dataset) and align them to the voice recordings in a way that works best for the purpose of synthesizing novel utterances from novel speakers, similar to the target speaker's voice. We describe the metrics used for evaluation, a baseline system consisting of unsupervised subword unit discovery plus a standard TTS system, and a topline TTS using gold phoneme transcriptions. We present an overview of the 19 submitted systems from 10 teams and discuss the main results",
    "checked": true,
    "id": "fd80c74299a0ba9ea6b582e1ad8397e5e2db82ed",
    "semantic_title": "the zero resource speech challenge 2019: tts without t",
    "citation_count": 106
  },
  "https://www.isca-speech.org/archive/interspeech_2019/feng19b_interspeech.html": {
    "title": "Combining Adversarial Training and Disentangled Speech Representation for Robust Zero-Resource Subword Modeling",
    "volume": "main",
    "abstract": "This study addresses the problem of unsupervised subword unit discovery from untranscribed speech. It forms the basis of the ultimate goal of ZeroSpeech 2019, building text-to-speech systems without text labels. In this work, unit discovery is formulated as a pipeline of phonetically discriminative feature learning and unit inference. One major difficulty in robust unsupervised feature learning is dealing with speaker variation. Here the robustness towards speaker variation is achieved by applying adversarial training and FHVAE based disentangled speech representation learning. A comparison of the two approaches as well as their combination is studied in a DNN-bottleneck feature (DNN-BNF) architecture. Experiments are conducted on ZeroSpeech 2019 and 2017. Experimental results on ZeroSpeech 2017 show that both approaches are effective while the latter is more prominent, and that their combination brings further marginal improvement in across-speaker condition. Results on ZeroSpeech 2019 show that in the ABX discriminability task, our approaches significantly outperform the official baseline, and are competitive to or even outperform the official topline. The proposed unit sequence smoothing algorithm improves synthesis quality, at a cost of slight decrease in ABX discriminability",
    "checked": true,
    "id": "6c67f7439b246cf2866cd89824aa2fdc90107a52",
    "semantic_title": "combining adversarial training and disentangled speech representation for robust zero-resource subword modeling",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yusuf19_interspeech.html": {
    "title": "Temporally-Aware Acoustic Unit Discovery for Zerospeech 2019 Challenge",
    "volume": "main",
    "abstract": "Zero-resource speech processing efforts focus on unsupervised discovery of sub-word acoustic units. Common approaches work with spatial similarities between the acoustic frame representations within Bayesian or neural network-based frameworks. We propose two methods that utilize the temporal proximity information in addition to the acoustic similarity for clustering frames into acoustic units. The first approach uses a temporally biased self-organizing map (SOM) to discover such units. Since the SOM unit indices are correlated with (vector) spatial distance, we pool neighboring units and then train a recurrent neural network to predict each pooled unit. The second approach incorporates temporal awareness by training a recurrent sparse autoencoder, in which unsupervised clustering is done on the intermediate softmax layer. This network is then fine-tuned using aligned pairs of acoustically similar sequences obtained via unsupervised term discovery. Our approaches outperform the provided baseline system on two main metrics of the Zerospeech 2019 challenge, ABX-discriminability and bitrate of the quantized embeddings, both for English and the surprise language. Furthermore, the temporal-awareness and the post-filtering techniques adopted in this work resulted in an enhanced continuity of the decoding, yielding low bitrates",
    "checked": true,
    "id": "10ac90538548a3ac23c524e50854a34ee01c4aaa",
    "semantic_title": "temporally-aware acoustic unit discovery for zerospeech 2019 challenge",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/eloff19_interspeech.html": {
    "title": "Unsupervised Acoustic Unit Discovery for Speech Synthesis Using Discrete Latent-Variable Neural Networks",
    "volume": "main",
    "abstract": "For our submission to the ZeroSpeech 2019 challenge, we apply discrete latent-variable neural networks to unlabelled speech and use the discovered units for speech synthesis. Unsupervised discrete subword modelling could be useful for studies of phonetic category learning in infants or in low-resource speech technology requiring symbolic input. We use an autoencoder (AE) architecture with intermediate discretisation. We decouple acoustic unit discovery from speaker modelling by conditioning the AE's decoder on the training speaker identity. At test time, unit discovery is performed on speech from an unseen speaker, followed by unit decoding conditioned on a known target speaker to obtain reconstructed filterbanks. This output is fed to a neural vocoder to synthesise speech in the target speaker's voice. For discretisation, categorical variational autoencoders (CatVAEs), vector-quantised VAEs (VQ-VAEs) and straight-through estimation are compared at different compression levels on two languages. Our final model uses convolutional encoding, VQ-VAE discretisation, deconvolutional decoding and an FFTNet vocoder. We show that decoupled speaker conditioning intrinsically improves discrete acoustic representations, yielding competitive synthesis quality compared to the challenge baseline",
    "checked": true,
    "id": "2130cb5ddde3efe79ace0246203c4e81ad495809",
    "semantic_title": "unsupervised acoustic unit discovery for speech synthesis using discrete latent-variable neural networks",
    "citation_count": 52
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19c_interspeech.html": {
    "title": "Unsupervised End-to-End Learning of Discrete Linguistic Units for Voice Conversion",
    "volume": "main",
    "abstract": "We present an unsupervised end-to-end training scheme where we discover discrete subword units from speech without using any labels. The discrete subword units are learned under an ASR-TTS autoencoder reconstruction setting, where an ASR-Encoder is trained to discover a set of common linguistic units given a variety of speakers, and a TTS-Decoder trained to project the discovered units back to the designated speech. We propose a discrete encoding method, Multilabel-Binary Vectors (MBV), to make the ASR-TTS autoencoder differentiable. We found that the proposed encoding method offers automatic extraction of speech content from speaker style, and is sufficient to cover full linguistic content in a given language. Therefore, the TTS-Decoder can synthesize speech with the same content as the input of ASR-Encoder but with different speaker characteristics, which achieves voice conversion (VC). We further improve the quality of VC using adversarial training, where we train a TTS-Patcher that augments the output of TTS-Decoder. Objective and subjective evaluations show that the proposed approach offers strong VC results as it eliminates speaker identity while preserving content within speech. In the ZeroSpeech 2019 Challenge, we achieved outstanding performance in terms of low bitrate",
    "checked": true,
    "id": "add193d1d6a70bc50768702daa020d45d4b78f9f",
    "semantic_title": "unsupervised end-to-end learning of discrete linguistic units for voice conversion",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2019/s19_interspeech.html": {
    "title": "Zero Resource Speech Synthesis Using Transcripts Derived from Perceptual Acoustic Units",
    "volume": "main",
    "abstract": "Zerospeech synthesis is the task of building vocabulary independent speech synthesis systems, where transcriptions are unavailable for training data. It is, therefore, necessary to convert training data into a sequence of fundamental acoustic units that can be used for synthesis during the test. This paper attempts to discover, and model perceptual acoustic units consisting of steady state, and transient regions in speech. The transients roughly correspond to CV, VC units, while the steady-state corresponds to sonorants and fricatives. The speech signal is first preprocessed by segmenting the same into CVC-like units using a short-term energy-like contour. These CVC segments are clustered using a connected components-based graph clustering technique. The clustered CVC segments are initialized such that the onset (CV) and decays (VC) correspond to transients, and the rhyme corresponds to steady-states. Following this initialization, the units are allowed to re-organise on the continuous speech into a final set of AUs in an HMM-GMM framework. AU sequences thus obtained are used to train synthesis models. The performance of the proposed approach is evaluated on the Zerospeech 2019 challenge database. Subjective and objective scores show that reasonably good quality synthesis with low bit rate encoding can be achieved using the proposed AUs",
    "checked": true,
    "id": "7fe5fab34291f96008e908ca7942c3141851a00b",
    "semantic_title": "zero resource speech synthesis using transcripts derived from perceptual acoustic units",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tjandra19_interspeech.html": {
    "title": "VQVAE Unsupervised Unit Discovery and Multi-Scale Code2Spec Inverter for Zerospeech Challenge 2019",
    "volume": "main",
    "abstract": "We describe our submitted system for the ZeroSpeech Challenge 2019. The current challenge theme addresses the difficulty of constructing a speech synthesizer without any text or phonetic labels and requires a system that can (1) discover subword units in an unsupervised way, and (2) synthesize the speech with a target speaker's voice. Moreover, the system should also balance the discrimination score ABX, the bit-rate compression rate, and the naturalness and the intelligibility of the constructed voice. To tackle these problems and achieve the best trade-off, we utilize a vector quantized variational autoencoder (VQ-VAE) and a multi-scale codebook-to-spectrogram (Code2Spec) inverter trained by mean square error and adversarial loss. The VQ-VAE extracts the speech to a latent space, forces itself to map it into the nearest codebook and produces compressed representation. Next, the inverter generates a magnitude spectrogram to the target voice, given the codebook vectors from VQ-VAE. In our experiments, we also investigated several other clustering algorithms, including K-Means and GMM, and compared them with the VQ-VAE result on ABX scores and bit rates. Our proposed approach significantly improved the intelligibility (in CER), the MOS, and discrimination ABX scores compared to the official ZeroSpeech 2019 baseline or even the topline",
    "checked": true,
    "id": "50970f392a76ced3054703a21d581377f1cc1086",
    "semantic_title": "vqvae unsupervised unit discovery and multi-scale code2spec inverter for zerospeech challenge 2019",
    "citation_count": 60
  },
  "https://www.isca-speech.org/archive/interspeech_2019/niehues19_interspeech.html": {
    "title": "Survey Talk: A Survey on Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d1a624b3dd6413cb3279230a5ad4ca75feb218ff",
    "semantic_title": "survey talk: a survey on speech translation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jia19_interspeech.html": {
    "title": "Direct Speech-to-Speech Translation with a Sequence-to-Sequence Model",
    "volume": "main",
    "abstract": "We present an attention-based sequence-to-sequence neural network which can directly translate speech from one language into speech in another language, without relying on an intermediate text representation. The network is trained end-to-end, learning to map speech spectrograms into target spectrograms in another language, corresponding to the translated content (in a different canonical voice). We further demonstrate the ability to synthesize translated speech using the voice of the source speaker. We conduct experiments on two Spanish-to-English speech translation datasets, and find that the proposed model slightly underperforms a baseline cascade of a direct speech-to-text translation model and a text-to-speech synthesis model, demonstrating the feasibility of the approach on this very challenging task",
    "checked": true,
    "id": "f93b523fe44e2ad2deaa1c6dac2da878a17eeeaf",
    "semantic_title": "direct speech-to-speech translation with a sequence-to-sequence model",
    "citation_count": 154
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19d_interspeech.html": {
    "title": "End-to-End Speech Translation with Knowledge Distillation",
    "volume": "main",
    "abstract": "End-to-end speech translation (ST), which directly translates from source language speech into target language text, has attracted intensive attentions in recent years. Compared to conventional pipeline systems, end-to-end ST model has potential benefits of lower latency, smaller model size and less error propagation. However, it is notoriously difficult to implement such model which combines automatic speech recognition (ASR) and machine translation (MT) together. In this paper, we propose a knowledge distillation approach to improve ST by transferring the knowledge from text translation. Specifically, we first train a text translation model, regarded as the teacher model, and then ST model is trained to learn the output probabilities of teacher model through knowledge distillation. Experiments on English-French Augmented LibriSpeech and English-Chinese TED corpus show that end-to-end ST is possible to implement on both similar and dissimilar language pairs. In addition, with the instruction of the teacher model, end-to-end ST model can gain significant improvements by over 3.5 BLEU points",
    "checked": true,
    "id": "e4d99f390901df5caac0b587ff685f9cde100342",
    "semantic_title": "end-to-end speech translation with knowledge distillation",
    "citation_count": 133
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gangi19_interspeech.html": {
    "title": "Adapting Transformer to End-to-End Spoken Language Translation",
    "volume": "main",
    "abstract": "Neural end-to-end architectures for sequence-to-sequence learning represent the state of the art in machine translation (MT) and speech recognition (ASR). Their use is also promising for end-to-end spoken language translation (SLT), which combines the main challenges of ASR and MT. Exploiting existing neural architectures, however, requires task-specific adaptations. A network that has obtained state-of-the-art results in MT with reduced training time is Transformer. However, its direct application to speech input is hindered by two limitations of the self-attention network on which it is based: quadratic memory complexity and no explicit modeling of short-range dependencies between input features. High memory complexity poses constraints to the size of models trainable with a GPU, while the inadequate modeling of local dependencies harms final translation quality. This paper presents an adaptation of Transformer to end-to-end SLT that consists in: i) downsampling the input with convolutional neural networks to make the training process feasible on GPUs, ii) modeling the bidimensional nature of a spectrogram, and iii) adding a distance penalty to the attention, so to bias it towards local context. SLT experiments on 8 language directions show that, with our adaptation, Transformer outperforms a strong RNN-based baseline with a significant reduction in training time",
    "checked": true,
    "id": "d0a313a557bd43a7cacb3e5479cd7c491f7faa5c",
    "semantic_title": "adapting transformer to end-to-end spoken language translation",
    "citation_count": 105
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hillis19_interspeech.html": {
    "title": "Unsupervised Phonetic and Word Level Discovery for Speech to Speech Translation for Unwritten Languages",
    "volume": "main",
    "abstract": "We experiment with unsupervised methods for deriving and clustering symbolic representations of speech, working towards speech-to-speech translation for languages without regular (or any) written representations. We consider five low-resource African languages, and we produce three different segmental representations of text data for comparisons against four different segmental representations derived solely from acoustic data for each language. The text and speech data for each language comes from the CMU Wilderness dataset introduced in [1], where speakers read a version of the New Testament in their language. Our goal is to evaluate the translation performance not only of acoustically derived units but also of discovered sequences or \"words\" made from these units, with the intuition that such representations will encode more meaning than phones alone. We train statistical machine translation models for each representation and evaluate their outputs on the basis of BLEU-1 scores to determine their efficacy. Our experiments produce encouraging results: as we cluster our atomic phonetic representations into more word-like units, the amount information retained generally approaches that of the actual words themselves",
    "checked": true,
    "id": "2b34391dc7ca13be1400ac205c95e4aed7325425",
    "semantic_title": "unsupervised phonetic and word level discovery for speech to speech translation for unwritten languages",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bhattacharya19_interspeech.html": {
    "title": "Deep Speaker Recognition: Modular or Monolithic?",
    "volume": "main",
    "abstract": "Speaker recognition has made extraordinary progress with the advent of deep neural networks. In this work, we analyze the performance of end-to-end deep speaker recognizers on two popular text-independent tasks - NIST-SRE 2016 and VoxCeleb. Through a combination of a deep convolutional feature extractor, self-attentive pooling and large-margin loss functions, we achieve state-of-the-art performance on VoxCeleb. Our best individual and ensemble models show a relative improvement of 70% an 82% respectively over the best reported results on this task On the challenging NIST-SRE 2016 task, our proposed end-to-end models show good performance but are unable to match a strong i-vector baseline. State-of-the-art systems for this task use a modular framework that combines neural network embeddings with a probabilistic linear discriminant analysis (PLDA) classifier. Drawing inspiration from this approach we propose to replace the PLDA classifier with a neural network. Our modular neural network approach is able to outperform the i-vector baseline using cosine distance to score verification trials",
    "checked": true,
    "id": "4ebb8e13b68c1676d197b0ebde7c5d63fbfcd11a",
    "semantic_title": "deep speaker recognition: modular or monolithic?",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19d_interspeech.html": {
    "title": "On the Usage of Phonetic Information for Text-Independent Speaker Embedding Extraction",
    "volume": "main",
    "abstract": "Embeddings extracted by deep neural networks have become the state-of-the-art utterance representation in speaker recognition systems. It has recently been shown that incorporating frame-level phonetic information in the embedding extractor can improve the speaker recognition performance. On the other hand, in the final embedding, phonetic information is just an additional source of session variability which may be harmful to the text-independent speaker recognition task. This suggests that at the embedding level phonetic information should be suppressed rather than encouraged. To verify this hypothesis, we perform several experiments that encourage or/and suppress phonetic information at various stages in the network. Our experiments confirm that multitask learning is beneficial if it is applied at the frame-level stage of the network, whereas adversarial training is beneficial if it is used at the segment-level stage of the network. Additionally, the combination of these two approaches improves the performance further, resulting in an equal error rate of 3.17% on the VoxCeleb dataset",
    "checked": true,
    "id": "984263114a79a53699e26e431bf0ef545b47a42c",
    "semantic_title": "on the usage of phonetic information for text-independent speaker embedding extraction",
    "citation_count": 45
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ravanelli19_interspeech.html": {
    "title": "Learning Speaker Representations with Mutual Information",
    "volume": "main",
    "abstract": "Learning good representations is of crucial importance in deep learning. Mutual Information (MI) or similar measures of statistical dependence are promising tools for learning these representations in an unsupervised way. Even though the mutual information between two random variables is hard to measure directly in high dimensional spaces, some recent studies have shown that an implicit optimization of MI can be achieved with an encoder-discriminator architecture similar to that of Generative Adversarial Networks (GANs) In this work, we learn representations that capture speaker identities by maximizing the mutual information between the encoded representations of chunks of speech randomly sampled from the same sentence. The proposed encoder relies on the SincNet architecture and transforms raw speech waveform into a compact feature vector. The discriminator is fed by either positive samples (of the joint distribution of encoded chunks) or negative samples (from the product of the marginals) and is trained to separate them We report experiments showing that this approach effectively learns useful speaker representations, leading to promising results on speaker identification and verification tasks. Our experiments consider both unsupervised and semi-supervised settings and compare the performance achieved with different objective functions",
    "checked": true,
    "id": "a5b94d6be7cd206d88218ba3a3ea4f3c3e8522de",
    "semantic_title": "learning speaker representations with mutual information",
    "citation_count": 75
  },
  "https://www.isca-speech.org/archive/interspeech_2019/you19_interspeech.html": {
    "title": "Multi-Task Learning with High-Order Statistics for x-Vector Based Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "The x-vector based deep neural network (DNN) embedding systems have demonstrated effectiveness for text-independent speaker verification. This paper presents a multi-task learning architecture for training the speaker embedding DNN with the primary task of classifying the target speakers, and the auxiliary task of reconstructing the first- and higher-order statistics of the original input utterance. The proposed training strategy aggregates both the supervised and unsupervised learning into one framework to make the speaker embeddings more discriminative and robust. Experiments are carried out using the NIST SRE16 evaluation dataset and the VOiCES dataset. The results demonstrate that our proposed method outperforms the original x-vector approach with very low additional complexity added",
    "checked": true,
    "id": "70cfbf82cee2f007b3c65cf97373f66f13005b42",
    "semantic_title": "multi-task learning with high-order statistics for x-vector based text-independent speaker verification",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wu19e_interspeech.html": {
    "title": "Data Augmentation Using Variational Autoencoder for Embedding Based Speaker Verification",
    "volume": "main",
    "abstract": "Domain or environment mismatch between training and testing, such as various noises and channels, is a major challenge for speaker verification. In this paper, a variational autoencoder (VAE) is designed to learn the patterns of speaker embeddings extracted from noisy speech segments, including i-vector and x-vector, and generate embeddings with more diversity to improve the robustness of speaker verification systems with probabilistic linear discriminant analysis (PLDA) back-end. The approach is evaluated on the standard NIST SRE 2016 dataset. Compared to manual and generative adversarial network (GAN) based augmentation approaches, the proposed VAE based augmentation achieves a slightly better performance for i-vector on Tagalog and Cantonese with EERs of 15.54% and 7.84%, and a more significant improvement for x-vector on those two languages with EERs of 11.86% and 4.20%",
    "checked": true,
    "id": "c85dd2837138fb1f91758e964b82bc3d7eba659c",
    "semantic_title": "data augmentation using variational autoencoder for embedding based speaker verification",
    "citation_count": 27
  },
  "https://www.isca-speech.org/archive/interspeech_2019/you19b_interspeech.html": {
    "title": "Deep Neural Network Embeddings with Gating Mechanisms for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "In this paper, gating mechanisms are applied in deep neural network (DNN) training for x-vector-based text-independent speaker verification. First, a gated convolution neural network (GCNN) is employed for modeling the frame-level embedding layers. Compared with the time-delay DNN (TDNN), the GCNN can obtain more expressive frame-level representations through carefully designed memory cell and gating mechanisms. Moreover, we propose a novel gated-attention statistics pooling strategy in which the attention scores are shared with the output gate. The gated-attention statistics pooling combines both gating and attention mechanisms into one framework; therefore, we can capture more useful information in the temporal pooling layer. Experiments are carried out using the NIST SRE16 and SRE18 evaluation datasets. The results demonstrate the effectiveness of the GCNN and show that the proposed gated-attention statistics pooling can further improve the performance",
    "checked": true,
    "id": "9f5f12800d0678362d009544b97c7b7c823782f1",
    "semantic_title": "deep neural network embeddings with gating mechanisms for text-independent speaker verification",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bhat19_interspeech.html": {
    "title": "Neural Transition Systems for Modeling Hierarchical Semantic Representations",
    "volume": "main",
    "abstract": "While virtual agents are becoming ubiquitous in our daily life, their functionality is limited to simple commands which involve a single intent and an unstructured set of entities. Typically, in such systems, the natural language understanding (NLU) component uses a sequence tagging model to extract a flat meaning representation. However, in order to support complex user requests with multiple intents with their associated entities, such as those in a product ordering domain, a structured semantic representation is necessary. In this paper, we present hierarchical semantic representations for product ordering in the food services domain and two NLU models that produce such representations efficiently using deep neural networks. The models are based on transition-based algorithms which have been proven to be effective and scalable for multiple NLP tasks such as syntactic parsing and slot filling. The first model uses a multitasking architecture containing multiple transition systems with tree constraints to model the hierarchical annotations, while the second model treats the task as a constituency parsing problem by mapping the target domain annotations to a constituency tree. We demonstrate that both multi-task and constituency-based transition systems achieve competitive results and even show improvements over sequential models, showing their effectiveness in modeling hierarchical structure",
    "checked": true,
    "id": "1518edb6297ce3a89c660a75efef63ac685855d4",
    "semantic_title": "neural transition systems for modeling hierarchical semantic representations",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vukotic19_interspeech.html": {
    "title": "Mining Polysemous Triplets with Recurrent Neural Networks for Spoken Language Understanding",
    "volume": "main",
    "abstract": "The typical RNN (Recurrent Neural Network) pipeline in SLU (Spoken Language Understanding), and specifically in the slot-filling task, consists of three stages: word embedding, context window representation, and label prediction. Label prediction, as a classification task, is the one that creates a sensible context window representation during learning through back-propagation. However, due to natural variations of the data, differences in two same-labeled samples can lead to dissimilar representations, whereas similarities in two differently-labeled samples can lead to them having close representations. In computer vision applications, specifically in face recognition and person re-identification, this problem has recently been successfully tackled by introducing data triplets and a triplet loss function In SLU, each word can be mapped to one or multiple labels depending on small variations of its context. We exploit this fact to construct data triplets consisting of the same words with different contexts that form a pair of datapoints with matching target labels and an another pair with non-matching labels. By using these triplets and an additional loss function, we update the context window representation in order to improve it, make dissimilar samples more distant and similar samples closer, leading to better classification results and an improved rate of convergence",
    "checked": true,
    "id": "40353cb09b4ecb4cc783fb9dace7c3aa18f9b835",
    "semantic_title": "mining polysemous triplets with recurrent neural networks for spoken language understanding",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ray19_interspeech.html": {
    "title": "Iterative Delexicalization for Improved Spoken Language Understanding",
    "volume": "main",
    "abstract": "Recurrent neural network (RNN) based joint intent classification and slot tagging models have achieved tremendous success in recent years for building spoken language understanding and dialog systems. However, these models suffer from poor performance for slots which often encounter large semantic variability in slot values after deployment (e.g. message texts, partial movie/artist names). While greedy delexicalization of slots in the input utterance via substring matching can partly improve performance, it often produces incorrect input. Moreover, such techniques cannot delexicalize slots with out-of-vocabulary slot values not seen at training. In this paper, we propose a novel iterative delexicalization algorithm, which can accurately delexicalize the input, even with out-of-vocabulary slot values. Based on model confidence of the current delexicalized input, our algorithm improves delexicalization in every iteration to converge to the best input having the highest confidence. We show on benchmark and in-house datasets that our algorithm can greatly improve parsing performance for RNN based models, especially for out-of-distribution slot values",
    "checked": true,
    "id": "0de85ba1b1c8921e41f598a7b5f40920c608013d",
    "semantic_title": "iterative delexicalization for improved spoken language understanding",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bhosale19_interspeech.html": {
    "title": "End-to-End Spoken Language Understanding: Bootstrapping in Low Resource Scenarios",
    "volume": "main",
    "abstract": "End-to-end Spoken Language Understanding (SLU) systems, without speech-to-text conversion, are more promising in low resource scenarios. They can be more effective when there is not enough labeled data to train reliable speech recognition and language understanding systems, or where running SLU on edge is preferred over cloud based services. In this paper, we present an approach for bootstrapping end-to-end SLU in low resource scenarios. We show that incorporating layers extracted from pre-trained acoustic models, instead of using the typical Mel filter bank features, lead to better performing SLU models. Moreover, the layers extracted from a model pre-trained on one language perform well even for (a) SLU tasks on a different language and also (b) on utterances from speakers with speech disorder",
    "checked": true,
    "id": "685d4b63e35a8603791dcf672ea5eff3083c96e0",
    "semantic_title": "end-to-end spoken language understanding: bootstrapping in low resource scenarios",
    "citation_count": 26
  },
  "https://www.isca-speech.org/archive/interspeech_2019/takatsu19_interspeech.html": {
    "title": "Recognition of Intentions of Users' Short Responses for Conversational News Delivery System",
    "volume": "main",
    "abstract": "In human-human conversations, listeners often convey intentions to their speakers through feedbacks comprising reflexive short responses. The speakers then recognize these intentions and dynamically change the conversational plans to transmit information more efficiently. For the design of spoken dialogue systems that deliver a massive amount of information, such as news, it is essential to accurately capture users' intentions from reflexive short responses to efficiently select or eliminate the information to be transmitted depending on the user's needs. However, such short responses from users are normally too short to recognize their actual intentions only from the prosodic and linguistic features of their short responses. In this paper, we propose a user's short-response intention-recognition model that accounts for the previous system's utterances as the context of the conversation in addition to prosodic and linguistic features of user's utterances. To achieve this, we define types of short response intentions in terms of effective information transmission and created new dataset by annotating over the interaction data collected using our spoken dialogue system. Our experimental results demonstrate that the classification accuracy can be improved using the linguistic features of the system's previous utterances encoded by Bidirectional Encoder Representations from Transformers (BERT) as the conversational context",
    "checked": true,
    "id": "ee3a4e09e6d295816d88c0f5594ded7e4922e8b6",
    "semantic_title": "recognition of intentions of users' short responses for conversational news delivery system",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/caubriere19_interspeech.html": {
    "title": "Curriculum-Based Transfer Learning for an Effective End-to-End Spoken Language Understanding and Domain Portability",
    "volume": "main",
    "abstract": "We present an end-to-end approach to extract semantic concepts directly from the speech audio signal. To overcome the lack of data available for this spoken language understanding approach, we investigate the use of a transfer learning strategy based on the principles of curriculum learning. This approach allows us to exploit out-of-domain data that can help to prepare a fully neural architecture. Experiments are carried out on the French MEDIA and PORTMEDIA corpora and show that this end-to-end SLU approach reaches the best results ever published on this task. We compare our approach to a classical pipeline approach that uses ASR, POS tagging, lemmatizer, chunker … and other NLP tools that aim to enrich ASR outputs that feed an SLU text to concepts system. Last, we explore the promising capacity of our end-to-end SLU approach to address the problem of domain portability",
    "checked": true,
    "id": "783fc1d48de76e3750d46023b315c75cab97bf40",
    "semantic_title": "curriculum-based transfer learning for an effective end-to-end spoken language understanding and domain portability",
    "citation_count": 45
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dash19b_interspeech.html": {
    "title": "Spatial and Spectral Fingerprint in the Brain: Speaker Identification from Single Trial MEG Signals",
    "volume": "main",
    "abstract": "Brain activity signals are unique subject-specific biological features that can not be forged or stolen. Recognizing this inherent trait, brain waves are recently being acknowledged as a far more secure, sensitive, and confidential biometric approach for user identification. Yet, current electroencephalography (EEG) based biometric systems are still in infancy considering their requirement of a large number of sensors and lower recognition performance compared to present biometric modalities. In this study, we investigated the spatial and spectral fingerprints in the brain with magnetoencephalography (MEG) for speaker identification during rest (pre-stimuli) and speech production. Experimental results suggested that the frontal and the temporal regions of the brain and higher frequency (gamma and high gamma) neural oscillations are more dominating for speaker identification. Moreover, we also found that two optimally located MEG sensors are sufficient to obtain a high speaker classification accuracy during speech tasks whereas at least eight optimally located sensors are needed to accurately identify these subjects during rest-state (pre-stimuli). These results indicated the unique neural traits of speech production across speakers",
    "checked": true,
    "id": "0afd70a1e8365d83649c3fbf9a0fa4875fe4007e",
    "semantic_title": "spatial and spectral fingerprint in the brain: speaker identification from single trial meg signals",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nijveld19_interspeech.html": {
    "title": "ERP Signal Analysis with Temporal Resolution Using a Time Window Bank",
    "volume": "main",
    "abstract": "In order to study the cognitive processes underlying speech comprehension, neuro-physiological measures (e.g., EEG and MEG), or behavioural measures (e.g., reaction times and response accuracy) can be applied. Compared to behavioural measures, EEG signals can provide a more fine-grained and complementary view of the processes that take place during the unfolding of an auditory stimulus EEG signals are often analysed after having chosen specific time windows, which are usually based on the temporal structure of ERP components expected to be sensitive to the experimental manipulation. However, as the timing of ERP components may vary between experiments, trials, and participants, such a-priori defined analysis time windows may significantly hamper the exploratory power of the analysis of components of interest. In this paper, we explore a wide-window analysis method applied to EEG signals collected in an auditory repetition priming experiment This approach is based on a bank of temporal filters arranged along the time axis in combination with linear mixed effects modelling. Crucially, it permits a temporal decomposition of effects in a single comprehensive statistical model which captures the entire EEG trace",
    "checked": true,
    "id": "51755176b6a0f43e50060411f2640f236bb5e67a",
    "semantic_title": "erp signal analysis with temporal resolution using a time window bank",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bosch19_interspeech.html": {
    "title": "Phase Synchronization Between EEG Signals as a Function of Differences Between Stimuli Characteristics",
    "volume": "main",
    "abstract": "The neural processing of speech leads to specific patterns in the brain which can be measured as, e.g., EEG signals. When properly aligned with the speech input and averaged over many tokens, the Event Related Potential (ERP) signal is able to differentiate specific contrasts between speech signals. Well-known effects relate to the difference between expected and unexpected words, in particular in the N400, while effects in N100 and P200 are related to attention and acoustic onset effects. Most EEG studies deal with the amplitude of EEG signals over time, sidestepping the effect of phase and phase synchronization. This paper investigates the relation between phase in the EEG signals measured in an auditory lexical decision task by Dutch participants listening to full and reduced English word forms. We show that phase synchronization takes place across stimulus conditions, and that the so-called circular variance is narrowly related to the type of contrast between stimuli",
    "checked": true,
    "id": "536db6925669eb585d3360ca80e6ff1ea6ea24e4",
    "semantic_title": "phase synchronization between eeg signals as a function of differences between stimuli characteristics",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kharaman19_interspeech.html": {
    "title": "The Processing of Prosodic Cues to Rhetorical Question Interpretation: Psycholinguistic and Neurolinguistics Evidence",
    "volume": "main",
    "abstract": "In many languages, rhetorical questions (RQs) are produced with different prosodic realizations than string-identical information-seeking questions (ISQs). RQs typically have longer constituent durations and breathier voice quality than ISQs and differ in nuclear accent type. This paper reports on an identification experiment (Experiment 1) and an EEG experiment (Experiment 2) on German wh-questions. In the identification experiment, we manipulated nuclear pitch accent type, voice quality and constituent duration and participants indicated whether they judged the realization as ISQ or RQ. The results showed additive effects of the three factors, with pitch accent as strongest predictor. In the EEG experiment, participants heard the stimuli in two contexts, triggering an ISQ or RQ (blocked). We manipulated pitch accent type and voice quality, resulting in RQ-coherent and ISQ-coherent stimuli, based on the outcome of Experiment 1. Results showed a prosodic expectancy positivity (PEP) for prosodic realizations that were incoherent with ISQ-contexts with an onset of ~120ms after the onset of the word with nuclear accent. This effect might reflect the emotional prosodic aspect of RQs. Taken together, participants use prosody to resolve the ambiguity and event-related potentials (ERPs) react to prosodic realizations that do not match contextually triggered expectations",
    "checked": true,
    "id": "9090bab2255bf4c030de8ed600ff23481e399a86",
    "semantic_title": "the processing of prosodic cues to rhetorical question interpretation: psycholinguistic and neurolinguistics evidence",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2019/scharenborg19_interspeech.html": {
    "title": "The Neural Correlates Underlying Lexically-Guided Perceptual Learning",
    "volume": "main",
    "abstract": "There is ample evidence showing that listeners are able to quickly adapt their phoneme classes to ambiguous sounds using a process called lexically-guided perceptual learning. This paper presents the first attempt to examine the neural correlates underlying this process. Specifically, we compared the brain's responses to ambiguous [f/s] sounds in Dutch non-native listeners of English (N=36) before and after exposure to the ambiguous sound to induce learning, using Event-Related Potentials (ERPs). We identified a group of participants who showed lexically-guided perceptual learning in their phonetic categorization behavior as observed by a significant difference in /s/ responses between pretest and posttest and a group who did not. Moreover, we observed differences in mean ERP amplitude to ambiguous phonemes at pretest and posttest, shown by a reliable reduction in amplitude of a positivity over medial central channels from 250 to 550 ms. However, we observed no significant correlation between the size of behavioral and neural pre/posttest effects. Possibly, the observed behavioral and ERP differences between pretest and posttest link to different aspects of the sound classification task. In follow-up research, these differences will be further investigated by assessing their relationship to neural responses to the ambiguous sounds in the exposure phase",
    "checked": true,
    "id": "6b3e2951c1e6e8e30d25257194e437824ab85ebe",
    "semantic_title": "the neural correlates underlying lexically-guided perceptual learning",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/parmonangan19_interspeech.html": {
    "title": "Speech Quality Evaluation of Synthesized Japanese Speech Using EEG",
    "volume": "main",
    "abstract": "As synthesized speech technology becomes more widely used, the synthesized speech quality must be assessed to ensure that it is acceptable. Subjective evaluation metrics, such as mean opinion score (MOS), can only provide an overall impression without any further detailed information about the speech. Therefore, this study proposes predicting speech quality using electroencephalographs (EEG), which are more objective and have high temporal resolution. In this paper, we use one natural speech and four types of synthesized speech lasting two to six seconds. First, to obtain ground truth of MOS, we gathered ten subjects to give opinion score on a scale of one to five for each recording. Second, another nine subjects were asked to measure how close to natural speech each synthesized speech sounded. The subjects' EEGs were recorded while they were listening to and evaluating the listened speech. The best accuracy achieved for classification was 96.61% using support vector machine, 80.36% using linear discriminant analysis, and 59.9% using logistic regression. For regression, we achieved root mean squared error as low as 1.133 using SVR and 1.353 using linear regression. This study demonstrates that EEG could be used to evaluate the perceived speech quality objectively",
    "checked": true,
    "id": "d8ab5835913a42bb14f27b6802b52e75d14180cb",
    "semantic_title": "speech quality evaluation of synthesized japanese speech using eeg",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19d_interspeech.html": {
    "title": "Multi-Microphone Adaptive Noise Cancellation for Robust Hotword Detection",
    "volume": "main",
    "abstract": "Recently we proposed a dual-microphone adaptive noise cancellation (ANC) algorithm with deferred filter coefficients for robust hotword detection in [1]. It exploits two unique hotword-related features: hotwords are the leading phrase of valid voice queries and they are short. These features allow us not to compute a speech-noise mask that is a common prerequisite for many multichannel speech enhancement approaches. This novel idea was found effective against strong and ambiguous speech-like TV noise. In this paper, we show that it can be generalized to support more than two microphones. The development is validated using re-recorded data with background TV noise from a 3-mic array. By adding one more microphone, the false reject (FR) rate can be further reduced relatively by 33.5%",
    "checked": true,
    "id": "44435a85c2309cec8cd2a673e06baa7a5c186fbe",
    "semantic_title": "multi-microphone adaptive noise cancellation for robust hotword detection",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhao19c_interspeech.html": {
    "title": "Multi-Task Multi-Network Joint-Learning of Deep Residual Networks and Cycle-Consistency Generative Adversarial Networks for Robust Speech Recognition",
    "volume": "main",
    "abstract": "Robustness of automatic speech recognition (ASR) systems is a critical issue due to noise and reverberations. Speech enhancement and model adaptation have been studied for long time to address this issue. Recently, the developments of multi-task joint-learning scheme that addresses noise reduction and ASR criteria in a unified modeling framework show promising improvements, but the model training highly relies on paired clean-noisy data. To overcome this limit, the generative adversarial networks (GANs) and the adversarial training method are deployed, which have greatly simplified the model training process without the requirements of complex front-end design and paired training data. Despite the fast developments of GANs for computer visions, only regular GANs have been adopted for robust ASR. In this work, we adopt a more advanced cycle-consistency GAN (CycleGAN) to address the training failure problem due to mode collapse of regular GANs. Using deep residual networks (ResNets), we further expand the multi-task scheme to a multi-task multi-network joint-learning scheme for more robust noise reduction and model adaptation. Experiment results on CHiME-4 show that our proposed approach significantly improves the noise robustness of the ASR system by achieving much lower word error rates (WERs) than the state-of-the-art joint-learning approaches",
    "checked": true,
    "id": "83c92ba230cac393cec20f3853fa2e46be9e43d9",
    "semantic_title": "multi-task multi-network joint-learning of deep residual networks and cycle-consistency generative adversarial networks for robust speech recognition",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/khokhlov19_interspeech.html": {
    "title": "R-Vectors: New Technique for Adaptation to Room Acoustics",
    "volume": "main",
    "abstract": "Distant speech recognition is an important problem which is far from being solved. Reverberation and noise are in the list of main problems in this area. The most popular methods of dealing with them are data augmentation and speech enhancement. In this paper, we propose a novel approach, inspired by modern methods of speaker adaptation First of all, a feed-forward network is trained to classify room impulse responses (RIRs) from speech recordings. Then this network is used for extracting embeddings, which we call R-vectors. These R-vectors are appended to input features of the acoustic model. Due to the lack of labeled data for RIRs classification task, we propose a self-supervised method of training the network, which consists of using artificial audio generated by room simulator Experimental evaluation was conducted on VOiCES19 and AMI single-channel tasks as well as CHiME5 multi-channel task. It is shown that the R-vector-adapted ASR systems achieve up to 14% relative WER reduction. Furthermore, it is additive with gains from state-of-the-art dereverberation (WPE) and speaker adaptation (x-vector) techniques",
    "checked": true,
    "id": "bfb92a178ad45fb20b5a308b7196028a595b88ba",
    "semantic_title": "r-vectors: new technique for adaptation to room acoustics",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kanda19b_interspeech.html": {
    "title": "Guided Source Separation Meets a Strong ASR Backend: Hitachi/Paderborn University Joint Investigation for Dinner Party ASR",
    "volume": "main",
    "abstract": "In this paper, we present Hitachi and Paderborn University's joint effort for automatic speech recognition (ASR) in a dinner party scenario. The main challenges of ASR systems for dinner party recordings obtained by multiple microphone arrays are (1) heavy speech overlaps, (2) severe noise and reverberation, (3) very natural conversational content, and possibly (4) insufficient training data. As an example of a dinner party scenario, we have chosen the data presented during the CHiME-5 speech recognition challenge, where the baseline ASR had a 73.3% word error rate (WER), and even the best performing system at the CHiME-5 challenge had a 46.1% WER. We extensively investigated a combination of the guided source separation-based speech enhancement technique and an already proposed strong ASR backend and found that a tight combination of these techniques provided substantial accuracy improvements. Our final system achieved WERs of 39.94% and 41.64% for the development and evaluation data, respectively, both of which are the best published results for the dataset. We also investigated with additional training data on the official small data in the CHiME-5 corpus to assess the intrinsic difficulty of this ASR task",
    "checked": true,
    "id": "21b772d55c422e337b2ab86bf5732bcd2ba4f3a3",
    "semantic_title": "guided source separation meets a strong asr backend: hitachi/paderborn university joint investigation for dinner party asr",
    "citation_count": 48
  },
  "https://www.isca-speech.org/archive/interspeech_2019/drude19_interspeech.html": {
    "title": "Unsupervised Training of Neural Mask-Based Beamforming",
    "volume": "main",
    "abstract": "We present an unsupervised training approach for a neural network-based mask estimator in an acoustic beamforming application. The network is trained to maximize a likelihood criterion derived from a spatial mixture model of the observations. It is trained from scratch without requiring any parallel data consisting of degraded input and clean training targets. Thus, training can be carried out on real recordings of noisy speech rather than simulated ones. In contrast to previous work on unsupervised training of neural mask estimators, our approach avoids the need for a possibly pre-trained teacher model entirely. We demonstrate the effectiveness of our approach by speech recognition experiments on two different datasets: one mainly deteriorated by noise (CHiME 4) and one by reverberation (REVERB). The results show that the performance of the proposed system is on par with a supervised system using oracle target masks for training and with a system trained using a model-based teacher",
    "checked": true,
    "id": "ad967431765c8b5a11aa7a78d6b836790c181afa",
    "semantic_title": "unsupervised training of neural mask-based beamforming",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ma19_interspeech.html": {
    "title": "Acoustic Model Ensembling Using Effective Data Augmentation for CHiME-5 Challenge",
    "volume": "main",
    "abstract": "CHiME-5 is a research community challenge targeting the problem of far-field and multi-talker conversational speech recognition in dinner party scenarios involving background noises, reverberations and overlapping speech. In this study, we present five different kinds of robust acoustic models which take advantages from both effective data augmentation and ensemble methods to improve the recognition performance for the CHiME-5 challenge. First, we detail the effective data augmentation for far-field scenarios, especially the far-field data simulation. Different from the conventional data simulation methods, we use a signal processing method originally developed for channel identification to estimate the room impulse responses and then simulate the far-field data. Second, we introduce the five different kinds of robust acoustic models. Finally, the effectiveness of our acoustic model ensembling strategies at the lattice level and the state posterior level are evaluated and demonstrated. Our system achieves the best performance of all four tasks among submitted systems in the CHiME-5 challenge",
    "checked": true,
    "id": "9242cc3228e6d43b5cbe68ada0f05477b3405378",
    "semantic_title": "acoustic model ensembling using effective data augmentation for chime-5 challenge",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19d_interspeech.html": {
    "title": "Survey Talk: End-to-End Deep Neural Network Based Speaker and Language Recognition",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3a9de52f8f893dad4501d44c9ae03b190dad26dc",
    "semantic_title": "survey talk: end-to-end deep neural network based speaker and language recognition",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/padi19_interspeech.html": {
    "title": "Attention Based Hybrid i-Vector BLSTM Model for Language Recognition",
    "volume": "main",
    "abstract": "In this paper, a hybrid i-vector neural network framework (i-BLSTM) which models the sequence information present in a series of short segment i-vectors for the task of spoken language recognition (LRE) is proposed. A sequence of short segment i-vectors are extracted for every speech utterance and are then modeled using a bidirectional long short-term memory (BLSTM) recurrent neural network (RNN). Attention mechanism inside the neural network relevantly weights segments of the speech utterance and the model learns to give higher weights to parts of speech data which are more helpful to the classification task. The proposed framework performs better in short duration and noisy environments when compared with the conventional i-vector system. Experiments are performed on clean, noisy and multi-speaker speech data from NIST LRE 2017 and RATS language recognition corpus. In these experiments, the proposed approach yields significant improvements (relative improvements of 7.6–13% in terms of accuracy for noisy conditions) over the conventional i-vector based language recognition approach and also over an end-to-end LSTM-RNN based approach",
    "checked": true,
    "id": "bae21c43d41b3d2adb619ed7b92ccb252082cebf",
    "semantic_title": "attention based hybrid i-vector blstm model for language recognition",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jung19b_interspeech.html": {
    "title": "RawNet: Advanced End-to-End Deep Neural Network Using Raw Waveforms for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "Recently, direct modeling of raw waveforms using deep neural networks has been widely studied for a number of tasks in audio domains. In speaker verification, however, utilization of raw waveforms is in its preliminary phase, requiring further investigation. In this study, we explore end-to-end deep neural networks that input raw waveforms to improve various aspects: front-end speaker embedding extraction including model architecture, pre-training scheme, additional objective functions, and back-end classification. Adjustment of model architecture using a pre-training scheme can extract speaker embeddings, giving a significant improvement in performance. Additional objective functions simplify the process of extracting speaker embeddings by merging conventional two-phase processes: extracting utterance-level features such as i-vectors or x-vectors and the feature enhancement phase, e.g., linear discriminant analysis. Effective back-end classification models that suit the proposed speaker embedding are also explored. We propose an end-to-end system that comprises two deep neural networks, one frontend for utterance-level speaker embedding extraction and the other for back-end classification. Experiments conducted on the VoxCeleb1 dataset demonstrate that the proposed model achieves state-of-the-art performance among systems without data augmentation. The proposed system is also comparable to the state-of-the-art x-vector system that adopts heavy data augmentation",
    "checked": true,
    "id": "d71d85c4f6f8c263ceb0750def148c2a3708979f",
    "semantic_title": "rawnet: advanced end-to-end deep neural network using raw waveforms for text-independent speaker verification",
    "citation_count": 94
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rao19_interspeech.html": {
    "title": "Target Speaker Extraction for Multi-Talker Speaker Verification",
    "volume": "main",
    "abstract": "The performance of speaker verification degrades significantly when the test speech is corrupted by interference from non-target speakers. Speaker diarization separates speakers well only if the speakers are not overlapped. However, if multiple talkers speak at the same time, we need a technique to separate the speech in the spectral domain. In this paper, we study a way to extract the target speaker's speech from an overlapped multi-talker speech. Specifically, given some reference speech samples from the target speaker, the target speaker's speech is firstly extracted from the overlapped multi-talker speech, then the extracted speech is processed in the speaker verification system. Experimental results show that the proposed approach significantly improves the performance of overlapped multi-talker speaker verification and achieves 64.4% relative EER reduction over the zero-effort baseline",
    "checked": true,
    "id": "706c44ae6036939f5eb5d2041289105a0aa07492",
    "semantic_title": "target speaker extraction for multi-talker speaker verification",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mazzawi19_interspeech.html": {
    "title": "Improving Keyword Spotting and Language Identification via Neural Architecture Search at Scale",
    "volume": "main",
    "abstract": "In this paper we present a novel Neural Architecture Search (NAS) framework to improve keyword spotting and spoken language identification models. Even with the huge success of deep neural networks (DNNs) in many different domains, finding the best network architecture is still a laborious task and very computationally expensive at best with existing searching approaches. Our search approach efficiently and robustly finds better model sequences with respect to hand-designed systems. We do this by constructing architectures incrementally, using a custom mutation algorithm and leveraging the power of parameter transfer between layers. We demonstrate that our approach can automatically design DNNs with an order of magnitude fewer parameters that achieves better performance than the current best models. It leads to significant performance improvements: up to 4.09% accuracy increase for language identification (6.1% if we allow an increase in the number of parameters) and 0.3% for phoneme classification in keyword spotting with half the size of the model",
    "checked": true,
    "id": "1bcad4cdfbc01fbb60a815660d034e561843d67a",
    "semantic_title": "improving keyword spotting and language identification via neural architecture search at scale",
    "citation_count": 34
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zheng19_interspeech.html": {
    "title": "Forward-Backward Decoding for Regularizing End-to-End TTS",
    "volume": "main",
    "abstract": "Neural end-to-end TTS can generate very high-quality synthesized speech, and even close to human recording within similar domain text. However, it performs unsatisfactory when scaling it to challenging test sets. One concern is that the encoder-decoder with attention-based network adopts autoregressive generative sequence model with the limitation of \"exposure bias\". To address this issue, we propose two novel methods, which learn to predict future by improving agreement between forward and backward decoding sequence. The first one is achieved by introducing divergence regularization terms into model training objective to reduce the mismatch between two directional models, namely L2R and R2L (which generates targets from left-to-right and right-to-left, respectively). While the second one operates on decoder-level and exploits the future information during decoding. In addition, we employ a joint training strategy to allow forward and backward decoding to improve each other in an interactive process. Experimental results show our proposed methods especially the second one (bidirectional decoder regularization), leads a significantly improvement on both robustness and overall naturalness, as outperforming baseline (the revised version of Tacotron2) with a MOS gap of 0.14 in a challenging test, and achieving close to human quality (4.42 vs. 4.49 in MOS) on general test",
    "checked": true,
    "id": "f44b10c9e9845c96cc497559314a9e5741846f7e",
    "semantic_title": "forward-backward decoding for regularizing end-to-end tts",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2019/guo19b_interspeech.html": {
    "title": "A New GAN-Based End-to-End TTS Training Algorithm",
    "volume": "main",
    "abstract": "End-to-end, autoregressive model-based TTS has shown significant performance improvements over the conventional ones. However, the autoregressive module training is affected by the exposure bias, or the mismatch between different distributions of real and predicted data. While real data is provided in training, in testing, predicted data is available only. By introducing both real and generated data sequences in training, we can alleviate the effects of the exposure bias. We propose to use Generative Adversarial Network (GAN) along with the idea of \"Professor Forcing\" in training. A discriminator in GAN is jointly trained to equalize the difference between real and the predicted data. In AB subjective listening test, the results show that the new approach is preferred over the standard transfer learning with a CMOS improvement of 0.1. Sentence level intelligibility tests also show significant improvement in a pathological test set. The GAN-trained new model is shown more stable than the baseline to produce better alignments for the Tacotron output",
    "checked": true,
    "id": "fa081e91d94cefea2a6908b67bf550b3a8ba0f75",
    "semantic_title": "a new gan-based end-to-end tts training algorithm",
    "citation_count": 44
  },
  "https://www.isca-speech.org/archive/interspeech_2019/he19_interspeech.html": {
    "title": "Robust Sequence-to-Sequence Acoustic Modeling with Stepwise Monotonic Attention for Neural TTS",
    "volume": "main",
    "abstract": "Neural TTS has demonstrated strong capabilities to generate human-like speech with high quality and naturalness, while its generalization to out-of-domain texts is still a challenging task, with regard to the design of attention-based sequence-to-sequence acoustic modeling. Various errors occur in those inputs with unseen context, including attention collapse, skipping, repeating, etc., which limits the broader applications. In this paper, we propose a novel stepwise monotonic attention method in sequence-to-sequence acoustic modeling to improve the robustness on out-of-domain inputs. The method utilizes the strict monotonic property in TTS with constraints on monotonic hard attention that the alignments between inputs and outputs sequence must be not only monotonic but allowing no skipping on inputs. Soft attention could be used to evade mismatch between training and inference. The experimental results show that the proposed method could achieve significant improvements in robustness on out-of-domain scenarios for phoneme-based models, without any regression on the in-domain naturalness test",
    "checked": true,
    "id": "051238432e23a865981c95579deb978e815d8f55",
    "semantic_title": "robust sequence-to-sequence acoustic modeling with stepwise monotonic attention for neural tts",
    "citation_count": 72
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19b_interspeech.html": {
    "title": "Joint Training Framework for Text-to-Speech and Voice Conversion Using Multi-Source Tacotron and WaveNet",
    "volume": "main",
    "abstract": "We investigated the training of a shared model for both text-to-speech (TTS) and voice conversion (VC) tasks. We propose using an extended model architecture of Tacotron, that is a multi-source sequence-to-sequence model with a dual attention mechanism as the shared model for both the TTS and VC tasks. This model can accomplish these two different tasks respectively according to the type of input. An end-to-end speech synthesis task is conducted when the model is given text as the input while a sequence-to-sequence voice conversion task is conducted when it is given the speech of a source speaker as the input. Waveform signals are generated by using WaveNet, which is conditioned by using a predicted mel-spectrogram. We propose jointly training a shared model as a decoder for a target speaker that supports multiple sources. Listening experiments show that our proposed multi-source encoder-decoder model can efficiently achieve both the TTS and VC tasks",
    "checked": true,
    "id": "27376bd3085e30d5264c73d5b74a318f9895949d",
    "semantic_title": "joint training framework for text-to-speech and voice conversion using multi-source tacotron and wavenet",
    "citation_count": 46
  },
  "https://www.isca-speech.org/archive/interspeech_2019/luong19_interspeech.html": {
    "title": "Training Multi-Speaker Neural Text-to-Speech Systems Using Speaker-Imbalanced Speech Corpora",
    "volume": "main",
    "abstract": "When the available data of a target speaker is insufficient to train a high quality speaker-dependent neural text-to-speech (TTS) system, we can combine data from multiple speakers and train a multi-speaker TTS model instead. Many studies have shown that neural multi-speaker TTS model trained with a small amount data from multiple speakers combined can generate synthetic speech with better quality and stability than a speaker-dependent one. However when the amount of data from each speaker is highly unbalanced, the best approach to make use of the excessive data remains unknown. Our experiments showed that simply combining all available data from every speaker to train a multi-speaker model produces better than or at least similar performance to its speaker-dependent counterpart. Moreover by using an ensemble multi-speaker model, in which each subsystem is trained on a subset of available data, we can further improve the quality of the synthetic speech especially for underrepresented speakers whose training data is limited",
    "checked": true,
    "id": "3c3f043d96f6e704467d085633a0d522b7984881",
    "semantic_title": "training multi-speaker neural text-to-speech systems using speaker-imbalanced speech corpora",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2019/okamoto19_interspeech.html": {
    "title": "Real-Time Neural Text-to-Speech with Sequence-to-Sequence Acoustic Model and WaveGlow or Single Gaussian WaveRNN Vocoders",
    "volume": "main",
    "abstract": "This paper investigates real-time high-fidelity neural text-to-speech (TTS) systems. For real-time neural vocoders, WaveGlow is introduced and single Gaussian (SG)WaveRNN is proposed. The proposed SG-WaveRNN can predict continuous valued speech waveforms with half the synthesis time compared with vanilla WaveRNN with dual-softmax for 16 bit audio prediction. Additionally, a sequence-to-sequence (seq2seq) acoustic model (AM) for pitch accent languages, such as Japanese, is investigated by introducing Tacotron 2 architecture. In the seq2seq AM, full-context labels extracted from a text analyzer are used as input and they are directly converted into mel-spectrograms. The results of subjective experiment using a Japanese female corpus indicate that the proposed SG-WaveRNN vocoder with noise shaping can synthesize high-quality speech waveforms and real-time high-fidelity neural TTS systems can be realized with the seq2seq AM and WaveGlow or SG-WaveRNN vocoders. Especially, the seq2seq AM and WaveGlow vocoder conditioned on mel-spectrograms with simple PyTorch implementations can be realized with real-time factors 0.06 and 0.10 for inference using a GPU",
    "checked": true,
    "id": "5bb538f50cbbba2570a181e396b102d09d945aee",
    "semantic_title": "real-time neural text-to-speech with sequence-to-sequence acoustic model and waveglow or single gaussian wavernn vocoders",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kafle19_interspeech.html": {
    "title": "Fusion Strategy for Prosodic and Lexical Representations of Word Importance",
    "volume": "main",
    "abstract": "We investigate whether, and if so when, prosodic features in spoken dialogue aid in modeling the importance of words to the overall meaning of a dialogue turn. Starting from the assumption that acoustic-prosodic cues help identify important speech content, we investigate representation architectures that combine lexical and prosodic features and evaluate them for predicting word importance. We propose an attention-based feature fusion strategy and additionally show how the addition of strategic supervision of the attention weights results in especially competitive models. We evaluate our fusion strategy on spoken dialogues and demonstrate performance increases over state-of-the-art models. Specifically, our approach both achieves the lowest root mean square error on test data and generalizes better over out-of-vocabulary words",
    "checked": true,
    "id": "587621e39b609ff75820faf832b0a11bd5e4c1ea",
    "semantic_title": "fusion strategy for prosodic and lexical representations of word importance",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chien19b_interspeech.html": {
    "title": "Self Attention in Variational Sequential Learning for Summarization",
    "volume": "main",
    "abstract": "Attention mechanism plays a crucial role in sequential learning for many speech and language applications. However, it is challenging to develop a stochastic attention in a sequence-to-sequence model which consists of two recurrent neural networks (RNNs) as the encoder and decoder. The problem of posterior collapse happens in variational inference and results in the estimated latent variables close to a standard Gaussian prior so that the information from input sequence is disregarded in learning process. This paper presents a new recurrent autoencoder for sentence representation where a self attention scheme is incorporated to activate the interaction between inference and generation in training procedure. In particular, a stochastic RNN decoder is implemented to provide additional latent variable to fulfill self attention for sentence reconstruction. The posterior collapse is alleviated. The latent information is sufficiently attended in variational sequential learning. During test phase, the estimated prior distribution of decoder is sampled for stochastic attention and generation. Experiments on Penn Treebank and Yelp 2013 show the desirable generation performance in terms of perplexity. The visualization of attention weights also illustrates the usefulness of self attention. The evaluation on DUC 2007 demonstrates the merit of variational recurrent autoencoder for document summarization",
    "checked": true,
    "id": "2cd2140f806539cb4759c62f0bd5f50b3f4fc52a",
    "semantic_title": "self attention in variational sequential learning for summarization",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sun19_interspeech.html": {
    "title": "Multi-Modal Sentiment Analysis Using Deep Canonical Correlation Analysis",
    "volume": "main",
    "abstract": "This paper learns multi-modal embeddings from text, audio, and video views/modes of data in order to improve upon downstream sentiment classification. The experimental framework also allows investigation of the relative contributions of the individual views in the final multi-modal embedding. Individual features derived from the three views are combined into a multi-modal embedding using Deep Canonical Correlation Analysis (DCCA) in two ways i) One-Step DCCA and ii) Two-Step DCCA. This paper learns text embeddings using BERT, the current state-of-the-art in text encoders. We posit that this highly optimized algorithm dominates over the contribution of other views, though each view does contribute to the final result. Classification tasks are carried out on two benchmark data sets and on a new Debate Emotion data set, and together these demonstrate that the one-Step DCCA outperforms the current state-of-the-art in learning multi-modal embeddings",
    "checked": true,
    "id": "902bd44b3d94603d23f493ca1486361bb56a3e2b",
    "semantic_title": "multi-modal sentiment analysis using deep canonical correlation analysis",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shen19_interspeech.html": {
    "title": "Interpreting and Improving Deep Neural SLU Models via Vocabulary Importance",
    "volume": "main",
    "abstract": "Spoken language understanding (SLU) is a crucial component in virtual personal assistants. It consists of two main tasks: intent detection and slot filling. State-of-the-art deep neural SLU models have demonstrated good performance on benchmark datasets. However, these models suffer from the significant performance drop in practice after deployment due to the data distribution discrepancy between training and real user utterances. In this paper, we first propose four research questions that help to understand what the state-of-the-art deep neural SLU models actually learn. To answer them, we study the vocabulary importance using a novel Embedding Sparse Structure Learning (SparseEmb) approach. It can be applied onto various existing deep SLU models to efficiently prune the useless words without any additional manual hyperparameter tuning. We evaluate SparseEmb on benchmark datasets using two existing SLU models and answer the proposed research questions. Then, we utilize SparseEmb to sanitize the training data based on the selected useless words as well as the model re-validation during training. Using both benchmark and our collected testing data, we show that our sanitized training data helps to significantly improve the SLU model performance. Both SparseEmb and training data sanitization approaches can be applied onto any deep learning based SLU models",
    "checked": true,
    "id": "feb79e1ed36ca9f9efa2792bbbeeabf87227c85f",
    "semantic_title": "interpreting and improving deep neural slu models via vocabulary importance",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tundik19_interspeech.html": {
    "title": "Assessing the Semantic Space Bias Caused by ASR Error Propagation and its Effect on Spoken Document Summarization",
    "volume": "main",
    "abstract": "Ambitions in artificial intelligence involve machine understanding of human language. The state-of-the-art approach for Spoken Language Understanding is using an Automatic Speech Recognizer (ASR) to generate transcripts, which are further processed with text-based tools. ASR yields error prone transcripts, these errors then propagate further into the processing pipeline. Subjective tests show on the other hand, that humans understand quite well ASR closed captions despite the word and punctuation errors. Our goal is to assess and quantify the loss in the semantic space resulting from error propagation and also analyze error propagation into speech summarization as a special use-case. We show, that word errors cause a slight shift in the semantic space, which is fairly below the average semantic distance between the sentences within a document. We also show, that punctuation errors have higher impact on summarization performance, which suggests that proper sentence level tokenization is crucial for this task",
    "checked": true,
    "id": "a14bb8f09fac4c0e8e5acc53e43b5986cdf6998d",
    "semantic_title": "assessing the semantic space bias caused by asr error propagation and its effect on spoken document summarization",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19e_interspeech.html": {
    "title": "Latent Topic Attention for Domain Classification",
    "volume": "main",
    "abstract": "Attention-based bidirectional long short-term network (BiLSTM) models have recently shown promising results in text classification tasks. However, when the amount of training data is restricted, or the distribution of the test data is quite different from the training data, some potential informative words maybe hard to be captured in training. In this work, we propose a new method to learn attention mechanism for domain classification. Unlike the past attention mechanisms only guided by domain tags of training data, we explore using the latent topics in the data set to learn topic attention, and employ it for BiLSTM. Experiments on the SMP-ECDT benchmark corpus show that the proposed latent topic attention mechanism outperforms the state-of-the-art soft and hard attention mechanisms in domain classification. Moreover, experiment result shows that the proposed method can be trained with additional unlabeled data and further improve the domain classification performance",
    "checked": true,
    "id": "c27b022e6cab84c20a27368c74d53d0f374f3401",
    "semantic_title": "latent topic attention for domain classification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/narisetty19_interspeech.html": {
    "title": "A Unified Bayesian Source Modelling for Determined Blind Source Separation",
    "volume": "main",
    "abstract": "This paper proposes a determined blind source separation (BSS) method with a Bayesian generalization for unified modelling of multiple audio sources. Our probabilistic framework allows a flexible multi-source modelling where the number of latent features required for the unified model is optimally estimated. When partitioning the latent features of the unified model to represent individual sources, the proposed approach helps to avoid over-fitting or under-fitting the correlations among sources. This adaptability of our Bayesian generalization therefore adds flexibility to conventional BSS approaches, where the number of latent features in the unified model has to be specified in advance. In the task of separating speech mixture signals, we show that our proposed method models diverse sources in a flexible manner and markedly improves the separation performance as compared to the conventional methods",
    "checked": true,
    "id": "547fc654d1c827fd9e5d1abea4308374c2b77254",
    "semantic_title": "a unified bayesian source modelling for determined blind source separation",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/takahashi19_interspeech.html": {
    "title": "Recursive Speech Separation for Unknown Number of Speakers",
    "volume": "main",
    "abstract": "In this paper we propose a method of single-channel speaker-independent multi-speaker speech separation for an unknown number of speakers. As opposed to previous works, in which the number of speakers is assumed to be known in advance and speech separation models are specific for the number of speakers, our proposed method can be applied to cases with different numbers of speakers using a single model by recursively separating a speaker. To make the separation model recursively applicable, we propose one-and-rest permutation invariant training (OR-PIT). Evaluation on WSJ0-2mix and WSJ0-3mix datasets show that our proposed method achieves state-of-the-art results for two- and three-speaker mixtures with a single model. Moreover, the same model can separate four-speaker mixture, which was never seen during the training. We further propose the detection of the number of speakers in a mixture during recursive separation and show that this approach can more accurately estimate the number of speakers than detection in advance by using a deep neural network based classifier",
    "checked": true,
    "id": "1315f5de75a0fd6d679d27ec5f9b545d967c0dde",
    "semantic_title": "recursive speech separation for unknown number of speakers",
    "citation_count": 65
  },
  "https://www.isca-speech.org/archive/interspeech_2019/appeltans19_interspeech.html": {
    "title": "Practical Applicability of Deep Neural Networks for Overlapping Speaker Separation",
    "volume": "main",
    "abstract": "This paper examines the applicability in realistic scenarios of two deep learning based solutions to the overlapping speaker separation problem. Firstly, we present experiments that show that these methods are applicable for a broad range of languages. Further experimentation indicates limited performance loss for untrained languages, when these have common features with the trained language(s). Secondly, it investigates how the methods deal with realistic background noise and proposes some modifications to better cope with these disturbances. The deep learning methods that will be examined are deep clustering and deep attractor networks",
    "checked": true,
    "id": "e8b6b833fdd98032ddca828c6e447d61dd92f018",
    "semantic_title": "practical applicability of deep neural networks for overlapping speaker separation",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gu19_interspeech.html": {
    "title": "Speech Separation Using Independent Vector Analysis with an Amplitude Variable Gaussian Mixture Model",
    "volume": "main",
    "abstract": "Independent vector analysis (IVA) utilizing Gaussian mixture model (GMM) as source priors has been demonstrated as an effective algorithm for joint blind source separation (JBSS). However, an extra pre-training process is required to provide initial parameter values for successful speech separation. In this paper, we introduce a time-varying parameter in the GMM to adapt to the temporal power fluctuation embedded in the nonstationary speech signal so as to avoid the pre-training process. The expectation-maximization (EM) process updating both the demixing matrix and the signal model is altered correspondingly. Experimental results confirm the efficacy of the proposed method under random initialization and further show its advantage in terms of a competitive separation accuracy and a faster convergence speed",
    "checked": true,
    "id": "baaeca2ea8812a0821e6c0baf53e158dfb545f2f",
    "semantic_title": "speech separation using independent vector analysis with an amplitude variable gaussian mixture model",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yang19c_interspeech.html": {
    "title": "Improved Speech Separation with Time-and-Frequency Cross-Domain Joint Embedding and Clustering",
    "volume": "main",
    "abstract": "Speech separation has been very successful with deep learning techniques. Substantial effort has been reported based on approaches over magnitude spectrogram, which is well known as the standard time-and-frequency cross-domain representation for speech signals. It is highly correlated to the phonetic structure of speech, or \"how the speech sounds\" when perceived by human, but primarily frequency domain features carrying temporal behaviour. Very impressive work achieving speech separation over time domain was reported recently, probably because waveforms in time domain may describe the different realizations of speech in a more precise way than magnitude spectrogram lacking phase information. In this paper, we propose a framework properly integrating the above two directions, hoping to achieve both purposes. We construct a time-and-frequency feature map by concatenating 1-dim convolution encoded feature map (for time domain) and magnitude spectrogram (for frequency domain), which was then processed by an embedding network and clustering approaches very similar to those used in time and frequency domain prior works. In this way, the information in time and frequency domains, as well as the interactions between them, can be jointly considered during embedding and clustering. Very encouraging results (state-of-the-art to our knowledge) were obtained with WSJ0-2mix dataset in preliminary experiments",
    "checked": true,
    "id": "687a450ba981ca3f8f52c556b53ec8393faa63c0",
    "semantic_title": "improved speech separation with time-and-frequency cross-domain joint embedding and clustering",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wichern19_interspeech.html": {
    "title": "WHAM!: Extending Speech Separation to Noisy Environments",
    "volume": "main",
    "abstract": "Recent progress in separating the speech signals from multiple overlapping speakers using a single audio channel has brought us closer to solving the cocktail party problem. However, most studies in this area use a constrained problem setup, comparing performance when speakers overlap almost completely, at artificially low sampling rates, and with no external background noise. In this paper, we strive to move the field towards more realistic and challenging scenarios. To that end, we created the WSJ0 Hipster Ambient Mixtures (WHAM!) dataset, consisting of two speaker mixtures from the wsj0-2mix dataset combined with real ambient noise samples. The samples were collected in coffee shops, restaurants, and bars in the San Francisco Bay Area, and are made publicly available. We benchmark various speech separation architectures and objective functions to evaluate their robustness to noise. While separation performance decreases as a result of noise, we still observe substantial gains relative to the noisy signals for most approaches",
    "checked": true,
    "id": "a6aca3527e123849b740d63e064051a1a46ecec6",
    "semantic_title": "wham!: extending speech separation to noisy environments",
    "citation_count": 173
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nautsch19_interspeech.html": {
    "title": "Survey Talk: Preserving Privacy in Speaker and Speech Characterisation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "9fcd14c6796ccea86c04e6ff1fa8b5d39c98a471",
    "semantic_title": "survey talk: preserving privacy in speaker and speech characterisation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chermaz19_interspeech.html": {
    "title": "Evaluating Near End Listening Enhancement Algorithms in Realistic Environments",
    "volume": "main",
    "abstract": "Speech playback (e.g., TV, radio, public address) becomes harder to understand in the presence of noise and reverberation. NELE (Near End Listening Enhancement) algorithms can improve intelligibility by modifying the signal before it is played back. Substantial intelligibility improvements have been achieved in the lab for both natural and synthetic speech. However, evidence is still scarce on how these algorithms work under conditions of realistic noise and reverberation We present a realistic test platform, featuring two representative everyday scenarios in which speech playback may occur (in the presence of both noise and reverberation): a domestic space (living room) and a public space (cafeteria). The generated stimuli are evaluated by measuring keyword accuracy rates in a listening test with normal hearing subjects We use the new platform to compare three state-of-the-art NELE algorithms, employing either noise-adaptive or non-adaptive strategies, and with or without compensation for reverberation",
    "checked": true,
    "id": "420b3014011bcd426d545b6f5e9fdfd9b033c02f",
    "semantic_title": "evaluating near end listening enhancement algorithms in realistic environments",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/edraki19_interspeech.html": {
    "title": "Improvement and Assessment of Spectro-Temporal Modulation Analysis for Speech Intelligibility Estimation",
    "volume": "main",
    "abstract": "Several recent high-performing intelligibility estimators of acoustically degraded speech signals employ temporal modulation analysis. In this paper, we investigate the utility of using both spectro- and temporal-modulation for estimating speech intelligibility. We modified a pre-existing speech intelligibility estimation scheme (STMI) that was inspired by human auditory spectro-temporal modulation analysis. We produced several variants of the modified STMI and assessed their intelligibility prediction accuracy, in comparison with several high-performing estimators. Among the estimators tested, one of the STMI variants and eSTOI performed consistently well on both noisy and reverberated speech. These results suggest that spectro-temporal modulation analysis is useful for certain degradation conditions such as modulated noise and reverberation",
    "checked": true,
    "id": "90845a4ca26fde66715b5aa39be180e9caaea290",
    "semantic_title": "improvement and assessment of spectro-temporal modulation analysis for speech intelligibility estimation",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19c_interspeech.html": {
    "title": "Listener Preference on the Local Criterion for Ideal Binary-Masked Speech",
    "volume": "main",
    "abstract": "Ideal binary mask (IBM) is a signal-processing technique that retains the time-frequency regions in a mixture of target speech and background noise when the local signal-to-noise ratio (SNR) is higher than a local criterion (LC) and removes the regions otherwise. The intelligibility of IBM-processed speech is typically high and does not depend on the choice of LC for a wide range of LC values. The current study investigates the listeners' preferences on the LC value for IBM processed speech. Concatenated everyday sentences were mixed with three types of background noises (airplane noise, train noise, and multi-talker babble) and were presented continuously to the listeners following the IBM processing. The IBM algorithm was implemented so that the listeners were able to adjust the LC value in real-time using a programmable knob. The listeners were instructed to adjust the LC value until the IBM-processed stimuli reached the most preferable quality. Across 20 listeners, large individual differences were observed for the preferred LC values. A cluster analysis identified that 11 of the 20 listeners exhibited consistent patterns of results. For this main cluster of listeners, the preferred LC value depended on the noise type, overall SNR, and the difficulty of the target sentences",
    "checked": true,
    "id": "700eb8a3e58c972e6378bbb1d40773f6ee823348",
    "semantic_title": "listener preference on the local criterion for ideal binary-masked speech",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dinh19_interspeech.html": {
    "title": "Using a Manifold Vocoder for Spectral Voice and Style Conversion",
    "volume": "main",
    "abstract": "We propose a new type of spectral feature that is both compact and interpolable, and thus ideally suited for regression approaches that involve averaging. The feature is realized by means of a speaker-independent variational autoencoder (VAE), which learns a latent space based on the low-dimensional manifold of high-resolution speech spectra. In vocoding experiments, we showed that using a 12-dimensional VAE feature (VAE-12) resulted in significantly better perceived speech quality compared to a 12-dimensional MCEP feature. In voice conversion experiments, using VAE-12 resulted in significantly better perceived speech quality as compared to 40-dimensional MCEPs, with similar speaker accuracy. In habitual to clear style conversion experiments, we significantly improved the speech intelligibility for one of three speakers, using a custom skip-connection deep neural network, with the average keyword recall accuracy increasing from 24% to 46%",
    "checked": true,
    "id": "781a3242e3ec90828c5d262f0db085cdc63dab4c",
    "semantic_title": "using a manifold vocoder for spectral voice and style conversion",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/platen19_interspeech.html": {
    "title": "Multi-Span Acoustic Modelling Using Raw Waveform Signals",
    "volume": "main",
    "abstract": "Traditional automatic speech recognition (ASR) systems often use an acoustic model (AM) built on handcrafted acoustic features, such as log Mel-filter bank (FBANK) values. Recent studies found that AMs with convolutional neural networks (CNNs) can directly use the raw waveform signal as input. Given sufficient training data, these AMs can yield a competitive word error rate (WER) to those built on FBANK features. This paper proposes a novel multi-span structure for acoustic modelling based on the raw waveform with multiple streams of CNN input layers, each processing a different span of the raw waveform signal. Evaluation on both the single channel CHiME4 and AMI data sets show that multi-span AMs give a lower WER than FBANK AMs by an average of about 5% (relative). Analysis of the trained multi-span model reveals that the CNNs can learn filters that are rather different to the log Mel-filters. Furthermore, the paper shows that a widely used single span raw waveform AM can be improved by using a smaller CNN kernel size and increased stride to yield improved WERs",
    "checked": true,
    "id": "6033241c2c55fed10866dacce60726a780a107f6",
    "semantic_title": "multi-span acoustic modelling using raw waveform signals",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/merboldt19_interspeech.html": {
    "title": "An Analysis of Local Monotonic Attention Variants",
    "volume": "main",
    "abstract": "Speech recognition using attention-based models is an effective approach to transcribing audio directly to text within an integrated end-to-end architecture. Global attention approaches compute a weighting over the complete input sequence, whereas local attention mechanisms are restricted to only a localized window of the sequence. For speech, the latter approach supports the monotonicity property of the speech-text alignment. Therefore, we revise several variants of such models and provide a comprehensive comparison, which has been missing so far in the literature. Additionally, we introduce a simple technique to implement windowed attention. This can be applied on top of an existing global attention model. The goal is to transition into a local attention model, by using a local window for the otherwise unchanged attention mechanism, starting from the temporal position with the most recent most active attention energy. We test this method on Switchboard and LibriSpeech and show that the proposed model can even be trained from random initialization and achieve results comparable to the global attention baseline",
    "checked": true,
    "id": "42b5438a084c69065acc436a4237f6c6369abb72",
    "semantic_title": "an analysis of local monotonic attention variants",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sun19b_interspeech.html": {
    "title": "Layer Trajectory BLSTM",
    "volume": "main",
    "abstract": "Recently, we proposed layer trajectory (LT) LSTM (ltLSTM) which significantly outperforms LSTM by decoupling the functions of senone classification and temporal modeling with separate depth and time LSTMs. We further improved ltLSTM with contextual layer trajectory LSTM (cltLSTM) which uses the future context frames to predict target labels. Given bidirectional LSTM (BLSTM) also uses future context frames to improve its modeling power, in this study we first compare the performance between these two models. Then we apply the layer trajectory idea to further improve BLSTM models, in which BLSTM is in charge of modeling the temporal information while depth-LSTM takes care of senone classification. In addition, we also investigate the model performance among different LT component designs on BLSTM models. Trained with 30 thousand hours of EN-US Microsoft internal data, the proposed layer trajectory BLSTM (ltBLSTM) model improved the baseline BLSTM with up to 14.5% relative word error rate (WER) reduction across different tasks",
    "checked": true,
    "id": "90ff16cbc5a161789f0bbd115924fd1195e59e4b",
    "semantic_title": "layer trajectory blstm",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/karita19_interspeech.html": {
    "title": "Improving Transformer-Based End-to-End Speech Recognition with Connectionist Temporal Classification and Language Model Integration",
    "volume": "main",
    "abstract": "The state-of-the-art neural network architecture named Transformer has been used successfully for many sequence-to-sequence transformation tasks. The advantage of this architecture is that it has a fast iteration speed in the training stage because there is no sequential operation as with recurrent neural networks (RNN). However, an RNN is still the best option for end-to-end automatic speech recognition (ASR) tasks in terms of overall training speed (i.e., convergence) and word error rate (WER) because of effective joint training and decoding methods. To realize a faster and more accurate ASR system, we combine Transformer and the advances in RNN-based ASR. In our experiments, we found that the training of Transformer is slower than that of RNN as regards the learning curve and integration with the naive language model (LM) is difficult. To address these problems, we integrate connectionist temporal classification (CTC) with Transformer for joint training and decoding. This approach makes training faster than with RNNs and assists LM integration. Our proposed ASR system realizes significant improvements in various ASR tasks. For example, it reduced the WERs from 11.1% to 4.5% on the Wall Street Journal and from 16.1% to 11.6% on the TED-LIUM by introducing CTC and LM integration into the Transformer baseline",
    "checked": true,
    "id": "ffe1416bcfde82f567dd280975bebcfeb4892298",
    "semantic_title": "improving transformer-based end-to-end speech recognition with connectionist temporal classification and language model integration",
    "citation_count": 172
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19d_interspeech.html": {
    "title": "Trainable Dynamic Subsampling for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "Jointly optimised attention-based encoder-decoder models have yielded impressive speech recognition results. The recurrent neural network (RNN) encoder is a key component in such models — it learns the hidden representations of the inputs. However, it is difficult for RNNs to model the long sequences characteristic of speech recognition. To address this, subsampling between stacked recurrent layers of the encoder is commonly employed. This method reduces the length of the input sequence and leads to gains in accuracy. However, static subsampling may both include redundant information and miss relevant information We propose using a dynamic subsampling RNN (dsRNN) encoder. Unlike a statically subsampled RNN encoder, the dsRNN encoder can learn to skip redundant frames. Furthermore, the skip ratio may vary at different stages of training, thus allowing the encoder to learn the most relevant information for each epoch. Although the dsRNN is unidirectional, it yields lower phone error rates (PERs) than a bidirectional RNN on TIMIT. The dsRNN encoder has a 16.8% PER on the TIMIT test set, a considerable improvement over static subsampling methods used with unidirectional and bidirectional RNN encoders (23.5% and 20.4% PER respectively)",
    "checked": true,
    "id": "9eb0ea96f501fdbfbbe949680a0a9485df39438d",
    "semantic_title": "trainable dynamic subsampling for end-to-end speech recognition",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhao19d_interspeech.html": {
    "title": "Shallow-Fusion End-to-End Contextual Biasing",
    "volume": "main",
    "abstract": "Contextual biasing to a specific domain, including a user's song names, app names and contact names, is an important component of any production-level automatic speech recognition (ASR) system. Contextual biasing is particularly challenging in end-to-end models because these models keep a small list of candidates during beam search, and also do poorly on proper nouns, which is the main source of biasing phrases. In this paper, we present various algorithmic and training improvements to shallow-fusion-based biasing for end-to-end models. We will show that the proposed approach obtains better performance than a state-of-the-art conventional model across a variety of tasks, the first time this has been demonstrated",
    "checked": true,
    "id": "d6c70a2ce72ccc11461860c3a738a1f7ca8d7309",
    "semantic_title": "shallow-fusion end-to-end contextual biasing",
    "citation_count": 99
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nasir19_interspeech.html": {
    "title": "Modeling Interpersonal Linguistic Coordination in Conversations Using Word Mover's Distance",
    "volume": "main",
    "abstract": "Linguistic coordination is a well-established phenomenon in spoken conversations and often associated with positive social behaviors and outcomes. While there have been many attempts to measure lexical coordination or entrainment in literature, only a few have explored coordination in syntactic or semantic space. In this work, we attempt to combine these different aspects of coordination into a single measure by leveraging distances in a neural word representation space. In particular, we adopt the recently proposed Word Mover's Distance with word2vec embeddings and extend it to measure the dissimilarity in language used in multiple consecutive speaker turns. To validate our approach, we apply this measure for two case studies in the clinical psychology domain. We find that our proposed measure is correlated with the therapist's empathy towards their patient in Motivational Interviewing and with affective behaviors in Couples Therapy. In both case studies, our proposed metric exhibits higher correlation than previously proposed measures. When applied to the couples with relationship improvement, we also notice a significant decrease in the proposed measure over the course of therapy, indicating higher linguistic coordination",
    "checked": true,
    "id": "21aa3b117bbfac0580b6d7b415a9b8beeec85ad3",
    "semantic_title": "modeling interpersonal linguistic coordination in conversations using word mover's distance",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/du19_interspeech.html": {
    "title": "Bag-of-Acoustic-Words for Mental Health Assessment: A Deep Autoencoding Approach",
    "volume": "main",
    "abstract": "Despite the recent success of deep learning, it is generally difficult to apply end-to-end deep neural networks to small datasets, such as those from the health domain, due to the tendency of neural networks to over-fit. In addition, how neural models reach their decisions is not well understood. In this paper, we present a two-stage approach to acoustic-based classification of behavior markers related to mental health disorders: first, a dictionary and the mapping from speech signals to the dictionary are learned jointly by a deep autoencoder, then the bag-of-words representation of speech is used for classification, using classifiers with simple decision boundaries. This deep bag-of-features approach has the advantage of offering more interpretability, while the use of deep autoencoder gains improvements in prediction by learning higher level features with long range dependencies, comparing to previous work using only low-level descriptors. In addition, we demonstrate the use of labeled emotion recognition data from other domains to supervise acoustic word encoding in order to help predict psychological traits. Experiments are conducted on audio recordings of 65 clinically recorded interviews with the self-reported level of post-traumatic stress disorder (PTSD), depression, and rapport with the interviewers",
    "checked": true,
    "id": "a48d363f30a465ec92b13328e5ac63db8741efa0",
    "semantic_title": "bag-of-acoustic-words for mental health assessment: a deep autoencoding approach",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/voleti19_interspeech.html": {
    "title": "Objective Assessment of Social Skills Using Automated Language Analysis for Identification of Schizophrenia and Bipolar Disorder",
    "volume": "main",
    "abstract": "Several studies have shown that speech and language features, automatically extracted from clinical interviews or spontaneous discourse, have diagnostic value for mental disorders such as schizophrenia and bipolar disorder. They typically make use of a large feature set to train a classifier for distinguishing between two groups of interest, i.e. a clinical and control group. However, a purely data-driven approach runs the risk of overfitting to a particular data set, especially when sample sizes are limited. Here, we first down-select the set of language features to a small subset that is related to a well-validated test of functional ability, the Social Skills Performance Assessment (SSPA). This helps establish the concurrent validity of the selected features. We use only these features to train a simple classifier to distinguish between groups of interest. Linear regression reveals that a subset of language features can effectively model the SSPA, with a correlation coefficient of 0.75. Furthermore, the same feature set can be used to build a strong binary classifier to distinguish between healthy controls and a clinical group (AUC = 0.96) and also between patients within the clinical group with schizophrenia and bipolar I disorder (AUC = 0.83)",
    "checked": true,
    "id": "83c4354415811ff2b50a5f11f770730873f13f4e",
    "semantic_title": "objective assessment of social skills using automated language analysis for identification of schizophrenia and bipolar disorder",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2019/matton19_interspeech.html": {
    "title": "Into the Wild: Transitioning from Recognizing Mood in Clinical Interactions to Personal Conversations for Individuals with Bipolar Disorder",
    "volume": "main",
    "abstract": "Bipolar Disorder, a mood disorder with recurrent mania and depression, requires ongoing monitoring and specialty management. Current monitoring strategies are clinically-based, engaging highly specialized medical professionals who are becoming increasingly scarce. Automatic speech-based monitoring via smartphones has the potential to augment clinical monitoring by providing inexpensive and unobtrusive measurements of a patient's daily life. The success of such an approach is contingent on the ability to successfully utilize \"in-the-wild\" data. However, most existing work on automatic mood detection uses datasets collected in clinical or laboratory settings. This study presents experiments in automatically detecting depression severity in individuals with Bipolar Disorder using data derived from clinical interviews and from personal conversations. We find that mood assessment is more accurate using data collected from clinical interactions, in part because of their highly structured nature. We demonstrate that although the features that are most effective in clinical interactions do not extend well to personal conversational data, we can identify alternative features relevant in personal conversational speech to detect mood symptom severity. Our results highlight the challenges unique to working with \"in-the-wild\" data, providing insight into the degree to which the predictive ability of speech features is preserved outside of a clinical interview",
    "checked": true,
    "id": "e16e0e4da49639157a4687694b5a55b0c5d8e6a2",
    "semantic_title": "into the wild: transitioning from recognizing mood in clinical interactions to personal conversations for individuals with bipolar disorder",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rohanian19_interspeech.html": {
    "title": "Detecting Depression with Word-Level Multimodal Fusion",
    "volume": "main",
    "abstract": "Semi-structured clinical interviews are frequently used diagnostic tools for identifying depression during an assessment phase. In addition to the lexical content of a patient's responses, multimodal cues concurrent with the responses are indicators of their motor and cognitive state, including those derivable from their voice quality and gestural behaviour. In this paper, we use information from different modalities in order to train a classifier capable of detecting the binary state of a subject (clinically depressed or not), as well as the level of their depression. We propose a model that is able to perform modality fusion incrementally after each word in an utterance using a time-dependent recurrent approach in a deep learning set-up. To mitigate noisy modalities, we utilize fusion gates that control the degree to which the audio or visual modality contributes to the final prediction. Our results show the effectiveness of word-level multimodal fusion, achieving state-of-the-art results in depression detection and outperforming early feature-level and late fusion techniques",
    "checked": true,
    "id": "023f954745e84062c6190f08c9a250fab446fa35",
    "semantic_title": "detecting depression with word-level multimodal fusion",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/espywilson19_interspeech.html": {
    "title": "Assessing Neuromotor Coordination in Depression Using Inverted Vocal Tract Variables",
    "volume": "main",
    "abstract": "Speech articulation is a complex activity that requires finely timed coordination across articulators, i.e., tongue, jaw, lips, and velum. In a depressed state involving psychomotor retardation, this coordination changes and in turn modifies the perceived speech signal. In previous work, we used the correlation structure of formant trajectories as a proxy for articulatory coordination, from which features were derived for predicting the degree of depression. Ideally, however, we seek coordination of the actual articulators using characteristics such as the degree and place of tongue constriction, often referred to as a tract variable (TV). In this paper, applying a novel articulatory inversion process, we investigate the relation between correlation structure of formant tracks versus that of TVs. We show on a pilot depressed/control dataset that, with the same number of variables, TV coordination-based features, although with some characteristics similar to their counterpart, outperform the corresponding formant track correlation features in detection of the depressed state. We speculate on the latent information being captured by TVs that is not present in formants",
    "checked": true,
    "id": "cb0ddfe586fbde3184bbd02dfcfa1e6f84d97798",
    "semantic_title": "assessing neuromotor coordination in depression using inverted vocal tract variables",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2019/paul19b_interspeech.html": {
    "title": "Towards Universal Dialogue Act Tagging for Task-Oriented Dialogues",
    "volume": "main",
    "abstract": "Machine learning approaches for building task-oriented dialogue systems require large conversational datasets with labels to train on. We are interested in building task-oriented dialogue systems from human-human conversations, which may be available in ample amounts in existing customer care center logs or can be collected from crowd workers. Annotating these datasets can be prohibitively expensive. Recently multiple annotated task-oriented human-machine dialogue datasets have been released, however their annotation schema varies across different collections, even for well-defined categories such as dialogue acts (DAs). We propose a Universal DA schema for task-oriented dialogues and align existing annotated datasets with our schema. Our aim is to train a Universal DA tagger (U-DAT) for task-oriented dialogues and use it for tagging human-human conversations. We investigate multiple datasets, propose manual and automated approaches for aligning the different schema, and present results on a target corpus of human-human dialogues. In unsupervised learning experiments we achieve an F1 score of 54.1% on system turns in human-human dialogues. In a semi-supervised setup, the F1 score increases to 57.7% which would otherwise require at least 1.7K manually annotated turns. For new domains, we show further improvements when unlabeled or labeled target domain data is available",
    "checked": true,
    "id": "58265aeb4fb993cdb479c31bed0d9d31f439eee5",
    "semantic_title": "towards universal dialogue act tagging for task-oriented dialogues",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/goel19_interspeech.html": {
    "title": "HyST: A Hybrid Approach for Flexible and Accurate Dialogue State Tracking",
    "volume": "main",
    "abstract": "Recent works on end-to-end trainable neural network based approaches have demonstrated state-of-the-art results on dialogue state tracking. The best performing approaches estimate a probability distribution over all possible slot values. However, these approaches do not scale for large value sets commonly present in real-life applications and are not ideal for tracking slot values that were not observed in the training set. To tackle these issues, candidate-generation-based approaches have been proposed. These approaches estimate a set of values that are possible at each turn based on the conversation history and/or language understanding outputs, and hence enable state tracking over unseen values and large value sets however, they fall short in terms of performance in comparison to the first group. In this work, we analyze the performance of these two alternative dialogue state tracking methods, and present a hybrid approach (HyST) which learns the appropriate method for each slot type. To demonstrate the effectiveness of HyST on a rich-set of slot types, we experiment with the recently released MultiWOZ-2.0 multi-domain, task-oriented dialogue-dataset. Our experiments show that HyST scales to multi-domain applications. Our best performing model results in a relative improvement of 24% and 10% over the previous SOTA and our best baseline respectively",
    "checked": true,
    "id": "242e9605760be04bd4d411c4320df4e6781dbdd8",
    "semantic_title": "hyst: a hybrid approach for flexible and accurate dialogue state tracking",
    "citation_count": 67
  },
  "https://www.isca-speech.org/archive/interspeech_2019/martinek19_interspeech.html": {
    "title": "Multi-Lingual Dialogue Act Recognition with Deep Learning Methods",
    "volume": "main",
    "abstract": "This paper deals with multi-lingual dialogue act (DA) recognition. The proposed approaches are based on deep neural networks and use word2vec embeddings for word representation. Two multi-lingual models are proposed for this task. The first approach uses one general model trained on the embeddings from all available languages. The second method trains the model on a single pivot language and a linear transformation method is used to project other languages onto the pivot language. The popular convolutional neural network and LSTM architectures with different set-ups are used as classifiers. To the best of our knowledge this is the first attempt at multi-lingual DA recognition using neural networks. The multi-lingual models are validated experimentally on two languages from the Verbmobil corpus",
    "checked": true,
    "id": "0037b62d4dbf2af532d5bb59184f0f4ae38c39a2",
    "semantic_title": "multi-lingual dialogue act recognition with deep learning methods",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chao19_interspeech.html": {
    "title": "BERT-DST: Scalable End-to-End Dialogue State Tracking with Bidirectional Encoder Representations from Transformer",
    "volume": "main",
    "abstract": "An important yet rarely tackled problem in dialogue state tracking (DST) is scalability for dynamic ontology ( e.g., movie, restaurant) and unseen slot values. We focus on a specific condition, where the ontology is unknown to the state tracker, but the target slot value (except for none and dontcare), possibly unseen during training, can be found as word segment in the dialogue context. Prior approaches often rely on candidate generation from n-gram enumeration or slot tagger outputs, which can be inefficient or suffer from error propagation. We propose BERT-DST, an end-to-end dialogue state tracker which directly extracts slot values from the dialogue context. We use BERT as dialogue context encoder whose contextualized language representations are suitable for scalable DST to identify slot values from their semantic context. Furthermore, we employ encoder parameter sharing across all slots with two advantages: (1) Number of parameters does not grow linearly with the ontology. (2) Language representation knowledge can be transferred among slots. Empirical evaluation shows BERT-DST with cross-slot parameter sharing outperforms prior work on the benchmark scalable DST datasets Sim-M and Sim-R, and achieves competitive performance on the standard DSTC2 and WOZ 2.0 datasets",
    "checked": true,
    "id": "b39067d7670b9ffc52ac7cd647f6d95dc946f161",
    "semantic_title": "bert-dst: scalable end-to-end dialogue state tracking with bidirectional encoder representations from transformer",
    "citation_count": 87
  },
  "https://www.isca-speech.org/archive/interspeech_2019/griol19_interspeech.html": {
    "title": "Discovering Dialog Rules by Means of an Evolutionary Approach",
    "volume": "main",
    "abstract": "Designing the rules for the dialog management process is one of the most resources-consuming tasks when developing a dialog system. Although statistical approaches to dialog management are becoming mainstream in research and industrial contexts, still many systems are being developed following the rule-based or hybrid paradigms. For example, when developers require deterministic system responses to keep total control on the decisions made by the system, or because the infrastructure employed is designed for rule-based systems using technologies currently used in commercial platforms. In this paper, we propose the use of evolutionary algorithms to automatically obtain the dialog rules that are implicit in a dialog corpus. Our proposal makes it possible to exploit the benefits of statistical approaches to build rule-based systems. Our proposal has been evaluated with a practical spoken dialog system, for which we have automatically obtained a set of fuzzy rules to successfully manage the dialog",
    "checked": true,
    "id": "e44c9758f069124e4107949c9e67e3d127e17ea0",
    "semantic_title": "discovering dialog rules by means of an evolutionary approach",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19c_interspeech.html": {
    "title": "Active Learning for Domain Classification in a Commercial Spoken Personal Assistant",
    "volume": "main",
    "abstract": "We describe a method for selecting relevant new training data for the LSTM-based domain selection component of our personal assistant system. Adding more annotated training data for any ML system typically improves accuracy, but only if it provides examples not already adequately covered in the existing data. However, obtaining, selecting, and labeling relevant data is expensive. This work presents a simple technique that automatically identifies new helpful examples suitable for human annotation. Our experimental results show that the proposed method, compared with random-selection and entropy-based methods, leads to higher accuracy improvements given a fixed annotation budget. Although developed and tested in the setting of a commercial intelligent assistant, the technique is of wider applicability",
    "checked": true,
    "id": "1a643c6a43341b0480e031193bd57b83a8cb0d47",
    "semantic_title": "active learning for domain classification in a commercial spoken personal assistant",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sadjadi19_interspeech.html": {
    "title": "The 2018 NIST Speaker Recognition Evaluation",
    "volume": "main",
    "abstract": "In 2018, the U.S. National Institute of Standards and Technology (NIST) conducted the most recent in an ongoing series of speaker recognition evaluations (SRE). SRE18 was organized in a similar manner to SRE16, focusing on speaker detection over conversational telephony speech (CTS) collected outside north America. SRE18 also featured several new aspects including: two new data domains, namely voice over internet protocol (VoIP) and audio extracted from amateur online videos (AfV), as well as a new language (Tunisian Arabic). A total of 78 organizations (forming 48 teams) from academia and industry participated in SRE18 and submitted 129 valid system outputs under fixed and open training conditions first introduced in SRE16. This paper presents an overview of the evaluation and several analyses of system performance for all primary conditions in SRE18. The evaluation results suggest that 1) speaker recognition on AfV was more challenging than on telephony data, 2) speaker representations (aka embeddings) extracted using end-to-end neural network frameworks were most effective, 3) top performing systems exhibited similar performance, and 4) greatest performance improvements were largely due to data augmentation, use of extended and more complex models for data representation, as well as effective use of the provided development sets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/villalba19_interspeech.html": {
    "title": "State-of-the-Art Speaker Recognition for Telephone and Video Speech: The JHU-MIT Submission for NIST SRE18",
    "volume": "main",
    "abstract": "We present a condensed description of the joint effort of JHU-CLSP, JHU-HLTCOE, MIT-LL., MIT CSAIL and LSE-EPITA for NIST SRE18. All the developed systems consisted of x-vector/i-vector embeddings with some flavor of PLDA backend. Very deep x-vector architectures — Extended and Factorized TDNN, and ResNets — clearly outperformed shallower x-vectors and i-vectors. The systems were tailored to the video (VAST) or to the telephone (CMN2) condition. The VAST data was challenging, yielding 4 times worse performance than other video based datasets like Speakers in the Wild. We were able to calibrate the VAST data with very few development trials by using careful adaptation and score normalization methods. The VAST primary fusion yielded EER=10.18% and Cprimary=0.431. By improving calibration in post-eval, we reached Cprimary=0.369. In CMN2, we used unsupervised SPLDA adaptation based on agglomerative clustering and score normalization to correct the domain shift between English and Tunisian Arabic models. The CMN2 primary fusion yielded EER=4.5% and Cprimary=0.313. Extended TDNN x-vector was the best single system obtaining EER=11.1% and Cprimary=0.452 in VAST; and 4.95% and 0.354 in CMN2",
    "checked": true,
    "id": "2d198d5209b9f144378ff1f86ce8bfc36249669e",
    "semantic_title": "state-of-the-art speaker recognition for telephone and video speech: the jhu-mit submission for nist sre18",
    "citation_count": 85
  },
  "https://www.isca-speech.org/archive/interspeech_2019/garciaromero19_interspeech.html": {
    "title": "x-Vector DNN Refinement with Full-Length Recordings for Speaker Recognition",
    "volume": "main",
    "abstract": "State-of-the-art text-independent speaker recognition systems for long recordings (a few minutes) are based on deep neural network (DNN) speaker embeddings. Current implementations of this paradigm use short speech segments (a few seconds) to train the DNN. This introduces a mismatch between training and inference when extracting embeddings for long duration recordings. To address this, we present a DNN refinement approach that updates a subset of the DNN parameters with full recordings to reduce this mismatch. At the same time, we also modify the DNN architecture to produce embeddings optimized for cosine distance scoring. This is accomplished using a large-margin strategy with angular softmax. Experimental validation shows that our approach is capable of producing embeddings that achieve record performance on the SITW benchmark",
    "checked": true,
    "id": "ec7ff1cefcd86523f98652150686de7ae1531287",
    "semantic_title": "x-vector dnn refinement with full-length recordings for speaker recognition",
    "citation_count": 41
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lee19_interspeech.html": {
    "title": "I4U Submission to NIST SRE 2018: Leveraging from a Decade of Shared Experiences",
    "volume": "main",
    "abstract": "The I4U consortium was established to facilitate a joint entry to NIST speaker recognition evaluations (SRE). The latest edition of such joint submission was in SRE 2018, in which the I4U submission was among the best-performing systems. SRE'18 also marks the 10-year anniversary of I4U consortium into NIST SRE series of evaluation. The primary objective of the current paper is to summarize the results and lessons learned based on the twelve sub-systems and their fusion submitted to SRE'18. It is also our intention to present a shared view on the advancements, progresses, and major paradigm shifts that we have witnessed as an SRE participant in the past decade from SRE'08 to SRE'18. In this regard, we have seen, among others, a paradigm shift from supervector representation to deep speaker embedding, and a switch of research challenge from channel compensation to domain adaptation",
    "checked": true,
    "id": "f704413b7eb14acdb0a3b231c40be1352b74f3ee",
    "semantic_title": "i4u submission to nist sre 2018: leveraging from a decade of shared experiences",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2019/khoury19_interspeech.html": {
    "title": "Pindrop Labs' Submission to the First Multi-Target Speaker Detection and Identification Challenge",
    "volume": "main",
    "abstract": "This paper summarizes Pindrop Labs' submission to the multi-target speaker detection and identification challenge evaluation (MCE 2018). The MCE challenge is geared towards detecting blacklisted speakers (fraudsters) in the context of call centers. Particularly, it aims to answer the following two questions: Is the speaker of the test utterance on the blacklist? If so, which speaker is it among the blacklisted speakers? While one single system can answer both questions, this work looks at them as two separate tasks: blacklist detection and closed-set identification. The former is addressed using four different systems including probabilistic linear discriminant analysis (PLDA), two deep neural network (DNN) based systems, and a simple system based on cosine similarity and logistic regression. The latter is addressed by combining PLDA and neural network based systems. The proposed system was the best performing system at the challenge on both tasks, reducing the blacklist detection error (Top-S EER) by 31.9% and the identification error (Top-1 EER) by 46.4% over the MCE baseline on the evaluation data",
    "checked": true,
    "id": "351776bc712a12a19197d62c7e744dc1303216d8",
    "semantic_title": "pindrop labs' submission to the first multi-target speaker detection and identification challenge",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/garciaromero19b_interspeech.html": {
    "title": "Speaker Recognition Benchmark Using the CHiME-5 Corpus",
    "volume": "main",
    "abstract": "In this paper, we introduce a speaker recognition benchmark derived from the publicly-available CHiME-5 corpus. Our goal is to foster research that tackles the challenging artifacts introduced by far-field multi-speaker recordings of naturally occurring spoken interactions. The benchmark comprises four tasks that involve enrollment and test conditions with single-speaker and/or multi-speaker recordings. Additionally, it supports performance comparisons between close-talking vs distant/far-field microphone recordings, and single-microphone vs microphone-array approaches. We validate the evaluation design with a single-microphone state-of-the-art DNN speaker recognition and diarization system (that we are making publicly available). The results show that the proposed tasks are very challenging, and can be used to quantify the performance gap due to the degradations present in far-field multi-speaker recordings",
    "checked": true,
    "id": "7e0570f498a5de4f2a861546d4e67ba208f71d12",
    "semantic_title": "speaker recognition benchmark using the chime-5 corpus",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ayllon19_interspeech.html": {
    "title": "Investigating the Effects of Noisy and Reverberant Speech in Text-to-Speech Systems",
    "volume": "main",
    "abstract": "The quality of the voices synthesized by a Text-to-Speech (TTS) system depends on the quality of the training data. In real case scenario of TTS personalization from user's voice recordings, the latter are usually affected by noise and reverberation. Speech enhancement can be useful to clean the corrupted speech but it is necessary to understand the effects that noise and reverberation have on the different statistical models that compose the TTS system. In this work we perform a thorough study of how noise and reverberation impact the acoustic and duration models of the TTS system. We also evaluate the effectiveness of time-frequency masking for cleaning the training data. Objective and subjective evaluations reveal that under normal recording scenarios noise leads to a higher degradation than reverberation in terms of naturalness of the synthesized speech",
    "checked": true,
    "id": "028e72fc9cf7a97ee12854ca4f84fe20fb911f03",
    "semantic_title": "investigating the effects of noisy and reverberant speech in text-to-speech systems",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kuo19_interspeech.html": {
    "title": "Selection and Training Schemes for Improving TTS Voice Built on Found Data",
    "volume": "main",
    "abstract": "This work investigates different selection and training schemes to improve the naturalness of synthesized text-to-speech voices built on found data. The approach outlined in this paper examines the combinations of different metrics to detect and reject segments of training data that can degrade the performance of the system. We conducted a series of objective and subjective experiments on two 24-hour single-speaker corpuses of found data collected from diverse sources. We show that using an even smaller, yet carefully selected, set of data can lead to a text-to-speech system able to generate more natural speech than a system trained on the complete dataset. Moreover, we show that training the system by fine-tuning from the system trained on the whole dataset leads to additional improvement in naturalness by allowing a more aggressive selection of training data",
    "checked": true,
    "id": "68aeae8bef6ad6baaa006d893742118affdcbbfa",
    "semantic_title": "selection and training schemes for improving tts voice built on found data",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/braude19_interspeech.html": {
    "title": "All Together Now: The Living Audio Dataset",
    "volume": "main",
    "abstract": "The ongoing focus in speech technology research on machine learning based approaches leaves the community hungry for data. However, datasets tend to be recorded once and then released, sometimes behind registration requirements or paywalls. In this paper we describe our Living Audio Dataset. The aim is to provide audio data that is in the public domain, multilingual, and expandable by communities. We discuss the role of linguistic resources, given the success of systems such as Tacotron which use direct text-to-speech mappings, and consider how data provenance could be built into such resources. So far the data has been collected for TTS purposes, however, it is also suitable for ASR. At the time of publication audio resources already exist for Dutch, R.P. English, Irish, and Russian",
    "checked": true,
    "id": "3626c3db896fb70e7812f12c0acbd51aae427156",
    "semantic_title": "all together now: the living audio dataset",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zen19_interspeech.html": {
    "title": "LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech",
    "volume": "main",
    "abstract": "This paper introduces a new speech corpus called \"LibriTTS\" designed for text-to-speech use. It is derived from the original audio and text materials of the LibriSpeech corpus, which has been used for training and evaluating automatic speech recognition systems. The new corpus inherits desired properties of the LibriSpeech corpus while addressing a number of issues which make LibriSpeech less than ideal for text-to-speech work. The released corpus consists of 585 hours of speech data at 24kHz sampling rate from 2,456 speakers and the corresponding texts. Experimental results show that neural end-to-end TTS models trained from the LibriTTS corpus achieved above 4.0 in mean opinion scores in naturalness in five out of six evaluation speakers. The corpus is freely available for download from http://www.openslr.org/60/",
    "checked": true,
    "id": "2789b6c84ba1422746246685001accba5563e7c1",
    "semantic_title": "libritts: a corpus derived from librispeech for text-to-speech",
    "citation_count": 440
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shamsi19_interspeech.html": {
    "title": "Corpus Design Using Convolutional Auto-Encoder Embeddings for Audio-Book Synthesis",
    "volume": "main",
    "abstract": "In this study, we propose an approach for script selection in order to design TTS speech corpora. A Deep Convolutional Neural Network (DCNN) is used to project linguistic information to an embedding space. The embedded representation of the corpus is then fed to a selection process to extract a subset of utterances which offers a good linguistic coverage while tending to limit the linguistic unit repetition. We present two selection processes: a clustering approach based on utterance distance and another method that tends to reach a target distribution of linguistic events. We compare the synthetic signal quality of the proposed methods to state of art methods objectively and subjectively. The subjective and objective measures confirm the performance of the proposed methods in order to design speech corpora with better synthetic speech quality. The perceptual test shows that our TTS global cost can be used as an alternative to synthetic overall quality",
    "checked": true,
    "id": "682b9950dc65ca8aaba7034890ac7e75c8ebdd8c",
    "semantic_title": "corpus design using convolutional auto-encoder embeddings for audio-book synthesis",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hojo19_interspeech.html": {
    "title": "Evaluating Intention Communication by TTS Using Explicit Definitions of Illocutionary Act Performance",
    "volume": "main",
    "abstract": "Text-to-speech (TTS) synthesis systems have been evaluated with respect to attributes such as quality, naturalness and intelligibility. However, an evaluation protocol with respect to communication of intentions has not yet been established. Evaluating this sometimes produce unreliable results because participants can misinterpret definitions of intentions. This misinterpretation is caused by the colloquial and implicit description of intentions. To address this problem, this work explicitly defines each intention following theoretical definitions, \"felicity conditions\", in speech-act theory. We define the communication of each intention with one to four necessary and sufficient conditions to be satisfied. In listening tests, participants rated whether each condition was satisfied or not. We compared the proposed protocol with the conventional baseline using four different voice conditions; neutral TTS, conversational TTS w/ and w/o intention inputs, and recorded speech. The experimental results with 10 participants showed that the proposed protocol produced smaller within-group variation and larger between-group variation. These results indicate that the proposed protocol can be used to evaluate intention communication with higher inter-rater reliability and sensitivity",
    "checked": true,
    "id": "67899ac5f3ab6ca862a09d7588e24d60e895f32a",
    "semantic_title": "evaluating intention communication by tts using explicit definitions of illocutionary act performance",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lo19_interspeech.html": {
    "title": "MOSNet: Deep Learning-Based Objective Assessment for Voice Conversion",
    "volume": "main",
    "abstract": "Existing objective evaluation metrics for voice conversion (VC) are not always correlated with human perception. Therefore, training VC models with such criteria may not effectively improve naturalness and similarity of converted speech. In this paper, we propose deep learning-based assessment models to predict human ratings of converted speech. We adopt the convolutional and recurrent neural network models to build a mean opinion score (MOS) predictor, termed as MOSNet. The proposed models are tested on large-scale listening test results of the Voice Conversion Challenge (VCC) 2018. Experimental results show that the predicted scores of the proposed MOSNet are highly correlated with human MOS ratings at the system level while being fairly correlated with human MOS ratings at the utterance level. Meanwhile, we have modified MOSNet to predict the similarity scores, and the preliminary results show that the predicted scores are also fairly correlated with human ratings. These results confirm that the proposed models could be used as a computational evaluator to measure the MOS of VC systems to reduce the need for expensive human rating",
    "checked": true,
    "id": "f45b6ecb84bcae0aec420f7491578f6297fbc81a",
    "semantic_title": "mosnet: deep learning based objective assessment for voice conversion",
    "citation_count": 155
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fong19_interspeech.html": {
    "title": "Investigating the Robustness of Sequence-to-Sequence Text-to-Speech Models to Imperfectly-Transcribed Training Data",
    "volume": "main",
    "abstract": "Sequence-to-sequence (S2S) text-to-speech (TTS) models can synthesise high quality speech when large amounts of annotated training data are available. Transcription errors exist in all data and are especially prevalent in found data such as audiobooks. In previous generations of TTS technology, alignment using Hidden Markov Models (HMMs) was widely used to identify and eliminate bad data. In S2S models, the use of attention replaces HMM-based alignment, and there is no explicit mechanism for removing bad data. It is not yet understood how such models deal with transcription errors in the training data We evaluate the quality of speech from S2S-TTS models when trained on data with imperfect transcripts, simulated using corruption, or provided by an Automatic Speech Recogniser (ASR).We find that attention can skip over extraneous words in the input sequence, providing robustness to insertion errors. But substitutions and deletions pose a problem because there is no ground truth input available to align to the ground truth acoustics during teacher-forced training. We conclude that S2S-TTS systems are only partially robust to training on imperfectly-transcribed data and further work is needed",
    "checked": true,
    "id": "e1b1d2a54928681d7fc0e1e749e7dab540766101",
    "semantic_title": "investigating the robustness of sequence-to-sequence text-to-speech models to imperfectly-transcribed training data",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/govender19_interspeech.html": {
    "title": "Using Pupil Dilation to Measure Cognitive Load When Listening to Text-to-Speech in Quiet and in Noise",
    "volume": "main",
    "abstract": "With increased use of text-to-speech (TTS) systems in real-world applications, evaluating how such systems influence the human cognitive processing system becomes important. Particularly in situations where cognitive load is high, there may be negative implications such as fatigue. For example, noisy situations generally require the listener to exert increased mental effort. A better understanding of this could eventually suggest new ways of generating synthetic speech that demands low cognitive load. In our previous study, pupil dilation was used as an index of cognitive effort. Pupil dilation was shown to be sensitive to the quality of synthetic speech, but there were some uncertainties regarding exactly what was being measured. The current study resolves some of those uncertainties. Additionally, we investigate how the pupil dilates when listening to synthetic speech in the presence of speech-shaped noise. Our results show that, in quiet listening conditions, pupil dilation does not reflect listening effort but rather attention and engagement. In noisy conditions, increased pupil dilation indicates that listening effort increases as signal-to-noise ratio decreases, under all conditions tested",
    "checked": true,
    "id": "97e3f3a3dfef82a64dad34c85f1399f3ef534c45",
    "semantic_title": "using pupil dilation to measure cognitive load when listening to text-to-speech in quiet and in noise",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/douros19b_interspeech.html": {
    "title": "A Multimodal Real-Time MRI Articulatory Corpus of French for Speech Research",
    "volume": "main",
    "abstract": "In this work we describe the creation of ArtSpeechMRIfr: a real-time as well as static magnetic resonance imaging (rtMRI, 3D MRI) database of the vocal tract. The database contains also processed data: denoised audio, its phonetically aligned annotation, articulatory contours, and vocal tract volume information, which provides a rich resource for speech research. The database is built on data from two male speakers of French It covers a number of phonetic contexts in the controlled part, as well as spontaneous speech, 3D MRI scans of sustained vocalic articulations, and of the dental casts of the subjects. The corpus for rtMRI consists of 79 synthetic sentences constructed from a phonetized dictionary that makes possible to shorten the duration of acquisitions while keeping a very good coverage of the phonetic contexts which exist in French. The 3D MRI includes acquisitions for 12 French vowels and 10 consonants, each of which was pronounced in several vocalic contexts. Articulatory contours (tongue, jaw, epiglottis, larynx, velum, lips) as well as 3D volumes were manually drawn for a part of the images",
    "checked": true,
    "id": "a6504d175847081380e25a488bed86e109053c03",
    "semantic_title": "a multimodal real-time mri articulatory corpus of french for speech research",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19d_interspeech.html": {
    "title": "A Chinese Dataset for Identifying Speakers in Novels",
    "volume": "main",
    "abstract": "Identifying speakers in novels aims at determining who says a quote in a given context by text analysis. This task is important for speech synthesis systems to assign appropriate voices to the quotes when producing audio books. Several English datasets have been constructed for this task. However, the difference between English and Chinese impedes processing Chinese novels using the models built on English datasets directly. Therefore, this paper presents a Chinese dataset, which contains 2,548 quotes from World of Plainness, a famous Chinese novel, with manually labelled speaker identities. Furthermore, two baseline speaker identification methods, i.e., a rule-based one and a classifier-based one, are designed and experimented using this Chinese dataset. These two methods achieve accuracies of 53.77% and 58.66% respectively on the test set",
    "checked": true,
    "id": "1e2a1ebd623eaa0807cd1f82c69d0ce60ee77eec",
    "semantic_title": "a chinese dataset for identifying speakers in novels",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/park19c_interspeech.html": {
    "title": "CSS10: A Collection of Single Speaker Speech Datasets for 10 Languages",
    "volume": "main",
    "abstract": "We describe our development of CSS10, a collection of single speaker speech datasets for ten languages. It is composed of short audio clips from LibriVox audiobooks and their aligned texts. To validate its quality we train two neural text-to-speech models on each dataset. Subsequently, we conduct Mean Opinion Score tests on the synthesized speech samples. We make our datasets, pre-trained models, and test resources publicly available. We hope they will be used for future speech tasks",
    "checked": true,
    "id": "ff7f3e65542c323056febe24bc17ea094fa37e4f",
    "semantic_title": "css10: a collection of single speaker speech datasets for 10 languages",
    "citation_count": 69
  },
  "https://www.isca-speech.org/archive/interspeech_2019/karaulov19_interspeech.html": {
    "title": "Attention Model for Articulatory Features Detection",
    "volume": "main",
    "abstract": "Articulatory distinctive features, as well as phonetic transcription, play important role in speech-related tasks: computer-assisted pronunciation training, text-to-speech conversion (TTS), studying speech production mechanisms, speech recognition for low-resourced languages. End-to-end approaches to speech-related tasks got a lot of traction in recent years. We apply Listen, Attend and Spell (LAS) [1] architecture to phones recognition on a small small training set, like TIMIT [2]. Also, we introduce a novel decoding technique that allows to train manners and places of articulation detectors end-to-end using attention models. We also explore joint phones recognition and articulatory features detection in multitask learning setting",
    "checked": true,
    "id": "2c400ac71a6a3ae182474a0ed1bc4d9585285447",
    "semantic_title": "attention model for articulatory features detection",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tong19_interspeech.html": {
    "title": "Unbiased Semi-Supervised LF-MMI Training Using Dropout",
    "volume": "main",
    "abstract": "The lattice-free MMI objective (LF-MMI) with finite-state transducer (FST) supervision lattice has been used in semi-supervised training of state-of-the-art neural network acoustic models for automatic speech recognition (ASR). However, the FST based supervision lattice does not sample from the posterior predictive distribution of word-sequences but only contains the decoding hypotheses corresponding to the Maximum Likelihood estimate of weights, so that the training might be biased towards incorrect hypotheses in the supervision lattice even if the best path is perfectly correct. In this paper, we propose a novel framework which uses Dropout at the test time to sample from the posterior predictive distribution of word-sequences to produce unbiased supervision lattices for semi-supervised training. We investigate the dropout sampling from both the acoustic model and the language model to generate supervision. Results on Fisher English show that the proposed approach achieves WER recovery of ~51.6% over regular semi-supervised LF-MMI training",
    "checked": true,
    "id": "07aa6200b01cfa6569ed063e653a05fa3d0044fe",
    "semantic_title": "unbiased semi-supervised lf-mmi training using dropout",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cui19_interspeech.html": {
    "title": "Acoustic Model Optimization Based on Evolutionary Stochastic Gradient Descent with Anchors for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Evolutionary stochastic gradient descent (ESGD) was proposed as a population-based approach that combines the merits of gradient-aware and gradient-free optimization algorithms for superior overall optimization performance. In this paper we investigate a variant of ESGD for optimization of acoustic models for automatic speech recognition (ASR). In this variant, we assume the existence of a well-trained acoustic model and use it as an anchor in the parent population whose good \"gene\" will prorogate in the evolution to the offsprings. We propose an ESGD algorithm leveraging the anchor models such that it guarantees the best fitness of the population will never degrade from the anchor model. Experiments on 50-hour Broadcast News (BN50) and 300-hour Switchboard (SWB300) show that the ESGD with anchors can further improve the loss and ASR performance over the existing well-trained acoustic models",
    "checked": true,
    "id": "2db65daf64a9285edc08b7237825c0f4d9c8c772",
    "semantic_title": "acoustic model optimization based on evolutionary stochastic gradient descent with anchors for automatic speech recognition",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shah19b_interspeech.html": {
    "title": "Whether to Pretrain DNN or not?: An Empirical Analysis for Voice Conversion",
    "volume": "main",
    "abstract": "Recently, Deep Neural Network (DNN)-based Voice Conversion (VC) techniques have become popular in the VC literature. These techniques suffer from the issue of overfitting due to less amount of available training data from a target speaker. To alleviate this, pre-training is used for better initialization of the DNN parameters, which leads to faster convergence of parameters. Greedy layerwise pre-training of the stacked Restricted Boltzmann Machine (RBM) or the stacked De-noising AutoEncoder (DAE) is used with extra available speaker-pairs‘ data. This pre-training is time-consuming and requires a separate network to learn the parameters of the network. In this work, we propose to analyze the DNN training strategies for the VC task, specifically with and without pre-training. In particular, we investigate whether an extra pre-training step could be avoided by using recent advances in deep learning. The VC experiments were performed on two VC Challenge (VCC) databases 2016 and 2018. Objective and subjective tests show that DNN trained with Adam optimization and Exponential Linear Unit (ELU) performed comparable or better than the pre-trained DNN without compromising on speech quality and speaker similarity of the converted voices",
    "checked": true,
    "id": "c118d2c5b35025eae7ec72a03f19bb939b385e53",
    "semantic_title": "whether to pretrain dnn or not?: an empirical analysis for voice conversion",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/goyal19_interspeech.html": {
    "title": "Detection of Glottal Closure Instants from Raw Speech Using Convolutional Neural Networks",
    "volume": "main",
    "abstract": "Glottal Closure Instants (GCIs) correspond to the temporal locations of significant excitation to the vocal tract occurring during the production of voiced speech. GCI detection from speech signals is a well-studied problem given its importance in speech processing. Most of the existing approaches for GCI detection adopt a two-stage approach (i) Transformation of speech signal into a representative signal where GCIs are localized better, (ii) extraction of GCIs using the representative signal obtained in first stage. The former stage is accomplished using signal processing techniques based on the principles of speech production and the latter with heuristic-algorithms such as dynamic-programming and peak-picking. These methods are thus task-specific and rely on the methods used for representative signal extraction. However in this paper, we formulate the GCI detection problem from a representation learning perspective where appropriate representation is implicitly learned from the raw-speech data samples. Specifically, GCI detection is cast as a supervised multi-task learning problem solved using a deep convolutional neural network jointly optimizing a classification and regression cost. The learning capability is demonstrated with several experiments on standard datasets. The results compare well with the state-of- the-art algorithms while performing better in the case of presence of real-world non-stationary noise",
    "checked": true,
    "id": "434861fc575dba916e56a719ae977b00dc53a72a",
    "semantic_title": "detection of glottal closure instants from raw speech using convolutional neural networks",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fainberg19_interspeech.html": {
    "title": "Lattice-Based Lightly-Supervised Acoustic Model Training",
    "volume": "main",
    "abstract": "In the broadcast domain there is an abundance of related text data and partial transcriptions, such as closed captions and subtitles. This text data can be used for lightly supervised training, in which text matching the audio is selected using an existing speech recognition model. Current approaches to light supervision typically filter the data based on matching error rates between the transcriptions and biased decoding hypotheses. In contrast, semi-supervised training does not require matching text data, instead generating a hypothesis using a background language model. State-of-the-art semi-supervised training uses lattice-based supervision with the lattice-free MMI (LF-MMI) objective function. We propose a technique to combine inaccurate transcriptions with the lattices generated for semi-supervised training, thus preserving uncertainty in the lattice where appropriate. We demonstrate that this combined approach reduces the expected error rates over the lattices, and reduces the word error rate (WER) on a broadcast task",
    "checked": true,
    "id": "ec1e685d48d337a1f8abe8bbbae5f9da3856d19e",
    "semantic_title": "lattice-based lightly-supervised acoustic model training",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/michel19_interspeech.html": {
    "title": "Comparison of Lattice-Free and Lattice-Based Sequence Discriminative Training Criteria for LVCSR",
    "volume": "main",
    "abstract": "Sequence discriminative training criteria have long been a standard tool in automatic speech recognition for improving the performance of acoustic models over their maximum likelihood / cross entropy trained counterparts. While previously a lattice approximation of the search space has been necessary to reduce computational complexity, recently proposed methods use other approximations to dispense of the need for the computationally expensive step of separate lattice creation In this work we present a memory efficient implementation of the forward-backward computation that allows us to use unigram word-level language models in the denominator calculation while still doing a full summation on GPU. This allows for a direct comparison of lattice-based and lattice-free sequence discriminative training criteria such as MMI and sMBR, both using the same language model during training We compared performance, speed of convergence, and stability on large vocabulary continuous speech recognition tasks like Switchboard and Quaero. We found that silence modeling seriously impacts the performance in the lattice-free case and needs special treatment. In our experiments lattice-free MMI comes on par with its lattice-based counterpart. Lattice-based sMBR still outperforms all lattice-free training criteria",
    "checked": true,
    "id": "11f9664eb42cdbe6f434aea3381458d4055328f6",
    "semantic_title": "comparison of lattice-free and lattice-based sequence discriminative training criteria for lvcsr",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/masumura19b_interspeech.html": {
    "title": "End-to-End Automatic Speech Recognition with a Reconstruction Criterion Using Speech-to-Text and Text-to-Speech Encoder-Decoders",
    "volume": "main",
    "abstract": "In this paper, we present a novel end-to-end automatic speech recognition (ASR) method that considers whether an input speech can be reconstructed from a generated text or not. A speech-to-text encoder-decoder model is one of the most powerful end-to-end ASR methods since it does not make any conditional independence assumptions. However, encoder-decoder models often suffer from a problem that is caused from a gap between the teacher forcing in a training phase and the free running in a testing phase. In fact, there is no guarantee that texts can be generated correctly when some generation errors occur in conditioning contexts. In order to mitigate this problem, our proposed method utilizes not only a generation probability of the text computed from a speech-to-text encoder-decoder but also a reconstruction probability of the speech computed from a text-to-speech encoder-decoder on the basis of a maximum mutual information criterion. We can expect that considering the reconstruction criterion can impose a constraint against generation errors. In addition, in order to compute the reconstruction probability, we introduce a mixture density network into the text-to-speech encoder-decoder. Our experiments on Japanese lecture ASR tasks demonstrate that considering the reconstruction criterion can yield ASR performance improvements",
    "checked": true,
    "id": "6f4bc173bb9e965e592c4df9e911216afb416877",
    "semantic_title": "end-to-end automatic speech recognition with a reconstruction criterion using speech-to-text and text-to-speech encoder-decoders",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/heba19_interspeech.html": {
    "title": "Char+CV-CTC: Combining Graphemes and Consonant/Vowel Units for CTC-Based ASR Using Multitask Learning",
    "volume": "main",
    "abstract": "Previous work has shown that end-to-end neural-based speech recognition systems can be improved by adding auxiliary tasks at intermediate layers. In this paper, we report multitask learning (MTL) experiments in the context of connectionist temporal classification (CTC) based speech recognition at character level. We compare several MTL architectures that jointly learn to predict characters (sometimes called graphemes) and consonant/vowel (CV) binary labels. The best approach, which we call Char+CV-CTC, adds up the character and CV logits to obtain the final character predictions. The idea is to put more weight on the vowel (consonant) characters when the vowel (consonant) symbol ‘V' (‘C') is predicted in the auxiliary-task branch of the network. Experiments were carried out on the Wall Street Journal (WSJ) corpus. Char+CV-CTC achieved the best ASR results with a 2.2% Character Error Rate and a 6.1% Word Error Rate (WER) on the Eval92 evaluation subset. This model outperformed its monotask model counterpart by 0.7% absolute in WER and also achieved almost the same performance of 6.0% as a strong baseline phone-based Time Delay Neural Network (\"TDNN-Phone+TR2\") model",
    "checked": true,
    "id": "94cb5163d3e0c04459d46f56d2e363960987cb88",
    "semantic_title": "char+cv-ctc: combining graphemes and consonant/vowel units for ctc-based asr using multitask learning",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kurata19_interspeech.html": {
    "title": "Guiding CTC Posterior Spike Timings for Improved Posterior Fusion and Knowledge Distillation",
    "volume": "main",
    "abstract": "Conventional automatic speech recognition (ASR) systems trained from frame-level alignments can easily leverage posterior fusion to improve ASR accuracy and build a better single model with knowledge distillation. End-to-end ASR systems trained using the Connectionist Temporal Classification (CTC) loss do not require frame-level alignment and hence simplify model training. However, sparse and arbitrary posterior spike timings from CTC models pose a new set of challenges in posterior fusion from multiple models and knowledge distillation between CTC models. We propose a method to train a CTC model so that its spike timings are guided to align with those of a pre-trained guiding CTC model. As a result, all models that share the same guiding model have aligned spike timings. We show the advantage of our method in various scenarios including posterior fusion of CTC models and knowledge distillation between CTC models with different architectures. With the 300-hour Switchboard training data, the single word CTC model distilled from multiple models improved the word error rates to 13.7%/23.1% from 14.9%/24.1% on the Hub5 2000 Switchboard/CallHome test sets without using any data augmentation, language model, or complex decoder",
    "checked": true,
    "id": "cbfc99c47296ee1f958f5860d07587bf190a9cfb",
    "semantic_title": "guiding ctc posterior spike timings for improved posterior fusion and knowledge distillation",
    "citation_count": 39
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fukuda19_interspeech.html": {
    "title": "Direct Neuron-Wise Fusion of Cognate Neural Networks",
    "volume": "main",
    "abstract": "This paper proposes a method to create a robust acoustic model by directly fusing multiple neural networks that have dissimilar characteristics without any additional layers/nodes involving retraining procedures. The fused neural networks derive from a shared parent neural network and are referred to as cognate (child) neural networks in this paper. The neural networks are fused by interpolating weight and bias parameters associated with each neuron with a different fusion weight, assuming that cognate neural networks to be fused have the same topology. Therefore, no extra computational cost during decoding is required. The fusion weight is determined by considering a cosine similarity estimated from parameters connecting to the neuron and the fusion is performed for every neuron. Experiments were carried out using a test suite consisting of various acoustic conditions with a wide SNR range, speakers including foreign accented speakers, and speaking styles. From the experiments, the network created by fusing cognate neural networks showed consistent improvement on average compared with the commercial-grade domain-free network originating from the parent model. In addition, we demonstrate that the fusion considering input connections to the neuron achieves the highest accuracy in our experiments",
    "checked": true,
    "id": "950621a7899811bf7d1dd167667a5dd87b8db541",
    "semantic_title": "direct neuron-wise fusion of cognate neural networks",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ladkat19_interspeech.html": {
    "title": "Two Tiered Distributed Training Algorithm for Acoustic Modeling",
    "volume": "main",
    "abstract": "We present a hybrid approach for scaling distributed training of neural networks by combining Gradient Threshold Compression (GTC) algorithm — a variant of stochastic gradient descent (SGD) — which compresses gradients with thresholding and quantization techniques and Blockwise Model Update Filtering (BMUF) algorithm — a variant of model averaging (MA). In this proposed method, we divide total number of workers into smaller subgroups in a hierarchical manner and limit frequent communication across subgroups. We update local model using GTC within a subgroup and global model using BMUF across different subgroups. We evaluate this approach in an Automatic Speech Recognition (ASR) task, by training deep long short-term memory (LSTM) acoustic models on 2000 hours of speech. Experiments show that, for a wide range in the number of GPUs used for distributed training, the proposed approach achieves a better trade-off between accuracy and scalability compared to GTC and BMUF",
    "checked": true,
    "id": "00eb80de2f38a6dd9a31e934e82d986d5ea2408a",
    "semantic_title": "two tiered distributed training algorithm for acoustic modeling",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19f_interspeech.html": {
    "title": "Exploring the Encoder Layers of Discriminative Autoencoders for LVCSR",
    "volume": "main",
    "abstract": "Discriminative autoencoders (DcAEs) have been proven to improve generalization of the learned acoustic models by increasing their reconstruction capacity of input features from the frame embeddings. In this paper, we integrate DcAEs into two models, namely TDNNs and LSTMs, which have been commonly adopted in the Kaldi recipes for LVCSR in recent years, using the modified nnet3 neural network library. We also explore two kinds of skip-connection mechanisms for DcAEs, namely concatenation and addition. The results of LVCSR experiments on the MATBN Mandarin Chinese corpus and the WSJ English corpus show that the proposed DcAE-TDNN-based system achieves relative word error rate reductions of 3% and 10% over the TDNN-based baseline system, respectively. The DcAE-TDNN-LSTM-based system also outperforms the TDNN-LSTM-based baseline system. The results imply the flexibility of DcAEs to be integrated with other existing or prospective neural network-based acoustic models",
    "checked": true,
    "id": "2c379326e66abe8879d21a1dd0a1f7d5a2eb8843",
    "semantic_title": "exploring the encoder layers of discriminative autoencoders for lvcsr",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kurata19b_interspeech.html": {
    "title": "Multi-Task CTC Training with Auxiliary Feature Reconstruction for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "We present a multi-task Connectionist Temporal Classification (CTC) training for end-to-end (E2E) automatic speech recognition with input feature reconstruction as an auxiliary task. Whereas the main task of E2E CTC training and the auxiliary reconstruction task share the encoder network, the auxiliary task tries to reconstruct the input feature from the encoded information. In addition to standard feature reconstruction, we distort the input feature only in the auxiliary reconstruction task, such as (1) swapping the former and latter parts of an utterance, or (2) using a part of an utterance by stripping the beginning or end parts. These distortions intentionally suppress long-span dependencies in the time domain, which avoids overfitting to the training data. We trained phone-based CTC and word-based CTC models with the proposed multi-task learning and demonstrated that it improves ASR accuracy on various test sets that are matched and unmatched with the training data",
    "checked": true,
    "id": "b8e3658a4940df5528bda0d3aaa4ad8650184be9",
    "semantic_title": "multi-task ctc training with auxiliary feature reconstruction for end-to-end speech recognition",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19e_interspeech.html": {
    "title": "Framewise Supervised Training Towards End-to-End Speech Recognition Models: First Results",
    "volume": "main",
    "abstract": "Recurrent neural networks (RNNs) trained with connectionist temporal classification (CTC) technique have delivered promising results in many speech recognition tasks. However, the forward-backward algorithm that CTC takes for model optimization requires a huge amount of computation. This paper introduces a new training method towards RNN-based end-to-end models, which significantly saves computing power without losing accuracy. Unlike CTC, the label sequence is aligned to the labelling hypothesis and then to the input sequence by the Weighted Minimum Edit-Distance Aligning (WMEDA) algorithm. Based on the alignment, the framewise supervised training is conducted. Moreover, Pronunciation Embedding (PE), the acoustic representation towards a linguistic target, is proposed in order to calculate the weights in WMEDA algorithm. The model is evaluated on TIMIT and AIShell-1 datasets for English phoneme and Chinese character recognitions. For TIMIT, the model achieves a comparable 18.57% PER to the 18.4% PER of the CTC baseline. As for AIShell-1, a joint Pinyin-character model is trained, giving a 19.38% CER, which is slightly better than the 19.43% CER obtained by the CTC character model, and the training time of this model is only 54.3% of the CTC model's",
    "checked": true,
    "id": "904be254fe5173050260a72b438b2693f475b261",
    "semantic_title": "framewise supervised training towards end-to-end speech recognition models: first results",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/georgiou19_interspeech.html": {
    "title": "Deep Hierarchical Fusion with Application in Sentiment Analysis",
    "volume": "main",
    "abstract": "Recognizing the emotional tone in spoken language is a challenging research problem that requires modeling not only the acoustic and textual modalities separately but also their cross-interactions. In this work, we introduce a hierarchical fusion scheme for sentiment analysis of spoken sentences. Two bidirectional Long-Short-Term-Memory networks (BiLSTM), followed by multiple fully connected layers, are trained in order to extract feature representations for each of the textual and audio modalities. The representations of the unimodal encoders are both fused at each layer and propagated forward, thus achieving fusion at the word, sentence and high/sentiment levels. The proposed approach of deep hierarchical fusion achieves state-of-the-art results for sentiment analysis tasks. Through an ablation study, we show that the proposed fusion method achieves greater performance gains over the unimodal baseline compared to other fusion approaches in the literature",
    "checked": true,
    "id": "9783154a3790a988b50c763af322b5086fc5ae8a",
    "semantic_title": "deep hierarchical fusion with application in sentiment analysis",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mitra19_interspeech.html": {
    "title": "Leveraging Acoustic Cues and Paralinguistic Embeddings to Detect Expression from Voice",
    "volume": "main",
    "abstract": "Millions of people reach out to digital assistants such as Siri every day, asking for information, making phone calls, seeking assistance, and much more. The expectation is that such assistants should understand the intent of the user's query. Detecting the intent of a query from a short, isolated utterance is a difficult task. Intent cannot always be obtained from speech-recognized transcriptions. A transcription-driven approach can interpret what has been said but fails to acknowledge how it has been said, and as a consequence, may ignore the expression present in the voice. Our work investigates whether a system can reliably detect vocal expression in queries using acoustic and paralinguistic embedding. Results show that the proposed method offers a relative equal error rate (EER) decrease of 60% compared to a bag-of-word based system, corroborating that expression is significantly represented by vocal attributes, rather than being purely lexical. Addition of emotion embedding helped to reduce the EER by 30% relative to the acoustic embedding, demonstrating the relevance of emotion in expressive voice",
    "checked": true,
    "id": "4976a2ee115624ca0c06f5bd7b07faa6fbb903fd",
    "semantic_title": "leveraging acoustic cues and paralinguistic embeddings to detect expression from voice",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/parry19_interspeech.html": {
    "title": "Analysis of Deep Learning Architectures for Cross-Corpus Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Speech Emotion Recognition (SER) is an important and challenging task for human-computer interaction. In the literature deep learning architectures have been shown to yield state-of-the-art performance on this task when the model is trained and evaluated on the same corpus. However, prior work has indicated that such systems often yield poor performance on unseen data. To improve the generalisation capabilities of emotion recognition systems one possible approach is cross-corpus training, which consists of training the model on an aggregation of different corpora. In this paper we present an analysis of the generalisation capability of deep learning models using cross-corpus training with six different speech emotion corpora. We evaluate the models on an unseen corpus and analyse the learned representations using the t-SNE algorithm, showing that architectures based on recurrent neural networks are prone to overfit the corpora present in the training set, while architectures based on convolutional neural networks (CNNs) show better generalisation capabilities. These findings indicate that (1) cross-corpus training is a promising approach for improving generalisation and (2) CNNs should be the architecture of choice for this approach",
    "checked": true,
    "id": "001456851ed0a4d10cf0ac71e7b5fb1f2359181c",
    "semantic_title": "analysis of deep learning architectures for cross-corpus speech emotion recognition",
    "citation_count": 44
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19e_interspeech.html": {
    "title": "A Path Signature Approach for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Automatic speech emotion recognition (SER) remains a difficult task within human-computer interaction, despite increasing interest in the research community. One key challenge is how to effectively integrate short-term characterisation of speech segments with long-term information such as temporal variations. Motivated by the numerical approximation theory of stochastic differential equations (SDEs), we propose the novel use of path signatures. The latter provide a pathwise definition to solve SDEs, for the integration of short speech frames. Furthermore we propose a hierarchical tree structure of path signatures, to capture both global and local information. A simple tree-based convolutional neural network (TBCNN) is used for learning the structural information stemming from dyadic path-tree signatures. Our experimental results on a widely used benchmark dataset demonstrate comparable performance to complex neural network based systems",
    "checked": true,
    "id": "cf5305c414b2ec2c1e7651e411a17a31b3c393e2",
    "semantic_title": "a path signature approach for speech emotion recognition",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2019/egorow19_interspeech.html": {
    "title": "Employing Bottleneck and Convolutional Features for Speech-Based Physical Load Detection on Limited Data Amounts",
    "volume": "main",
    "abstract": "The detection of different levels of physical load from speech has many applications: Besides telemedicine, non-contact detection of certain heart rate ranges can be useful for sports and other leisure time devices. Available approaches mainly use a high number of spectral and prosodic features. In this setting of typically small data sets, such as the Talk & Run data set and the Munich Biovoice Corpus, the high-dimensional feature spaces are only sparsely populated. Therefore, we aim at a reduction of the feature number using modern neural net inspired features: Bottleneck layer features, obtained from standard low-level descriptors via a feed-forward neural network, and activation map features, obtained from spectrograms via a convolutional neural network. We use these features for an SVM classification of high and low physical load and compare their performance. We also discuss the possibility of hyperparameter transfer of the extracting networks between different data sets. We show that even for limited amounts of data, deep learning based methods can bring a substantial improvement over \"conventional\" features",
    "checked": true,
    "id": "d6fb2d82fb5327ab47a2595c286c0435e4b64cba",
    "semantic_title": "employing bottleneck and convolutional features for speech-based physical load detection on limited data amounts",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhao19e_interspeech.html": {
    "title": "Speech Emotion Recognition in Dyadic Dialogues with Attentive Interaction Modeling",
    "volume": "main",
    "abstract": "In dyadic human-human interactions, a more complex interaction scenario, a person's emotional state can be influenced by both self emotional evolution and the interlocutor's behaviors. However, previous speech emotion recognition studies infer the speaker's emotional state mainly based on the targeted speech segment without considering the above two contextual factors. In this paper, we propose an Attentive Interaction Model (AIM) to capture both self- and interlocutor-context to enhance the speech emotion recognition in the dyadic dialog. The model learns to dynamically focus on long-term relevant contexts of the speaker and the interlocutor via the self-attention mechanism and fuse the adaptive context with the present behavior to predict the current emotional state. We carry out extensive experiments on the IEMOCAP corpus for dimensional emotion recognition in arousal and valence. Our model achieves on par performance with baselines for arousal recognition and significantly outperforms baselines for valence recognition, which demonstrates the effectiveness of the model to select useful contexts for emotion recognition in dyadic interactions",
    "checked": true,
    "id": "1ad3be3e455559fd4f2f3dc0c489c1b925cbbc3d",
    "semantic_title": "speech emotion recognition in dyadic dialogues with attentive interaction modeling",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhong19_interspeech.html": {
    "title": "Predicting Group Performances Using a Personality Composite-Network Architecture During Collaborative Task",
    "volume": "main",
    "abstract": "Personality has not only been studied at an individual level, its composite effect between team members has also been indicated to be related to the overall group performance. In this work, we propose a Personality Composite-Network (P-CompN) architecture that models the group-level personality composition with its intertwining effect being integrated into the network modeling of team members vocal behaviors in order to predict the group performances during collaborative problem solving tasks. In specific, we evaluate our proposed P-CompN in a large-scale dataset consist of three-person small group interactions. Our framework achieves a promising group performance classification accuracy of 70.0%, which outperforms baseline model of using only vocal behaviors without personality attributes by 14.4% absolutely. Our analysis further indicates that our proposed personality composite network impacts the vocal behavior models more significantly on the high performing groups versus the low performing groups",
    "checked": true,
    "id": "140653a1b1b70dece4259ef9a283152e634f7b18",
    "semantic_title": "predicting group performances using a personality composite-network architecture during collaborative task",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chao19b_interspeech.html": {
    "title": "Enforcing Semantic Consistency for Cross Corpus Valence Regression from Speech Using Adversarial Discrepancy Learning",
    "volume": "main",
    "abstract": "Issues of mismatch between databases remain a major challenge in performing emotion recognition on target unlabeled corpus from labeled source data. While studies have shown that by means of aligning source and target data distribution to learn a common feature space can mitigate these issues partially, they neglect the effect of distortion in emotion semantics across different databases. This distortion is especially crucial when regressing higher level emotion attribute such as valence. In this work, we propose a maximum regression discrepancy (MRD) network, which enforces cross corpus semantic consistency by learning a common acoustic feature space that minimizes discrepancy on those maximally-distorted samples through adversarial training. We evaluate our framework on two large emotion corpus, the USC IEMOCAP and the MSP-IMPROV, for the task of cross corpus valence regression from speech. Our MRD demonstrates a significant 10% and 5% improvement in concordance correlation coefficients (CCC) compared to using baseline source-only methods, and we also show that it outperforms two state-of-art domain adaptation techniques. Further analysis reveals that our model is more effective in reducing semantic distortion on low valence than high valence samples",
    "checked": true,
    "id": "740df49ffcb268a548fdbf67b97a1432ef4f7e46",
    "semantic_title": "enforcing semantic consistency for cross corpus valence regression from speech using adversarial discrepancy learning",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mao19_interspeech.html": {
    "title": "Deep Learning of Segment-Level Feature Representation with Multiple Instance Learning for Utterance-Level Speech Emotion Recognition",
    "volume": "main",
    "abstract": "In this paper, we propose to combine the deep learning of feature representation with multiple instance learning (MIL) to recognize emotion from speech. The key idea of our approach is to first consciously classify the emotional state of each segment. Then the utterance-level classification is constructed as an aggregation of the segment-level decisions. For the segment-level classification, we attempt two different deep neural network (DNN) architectures called SegMLP and SegCNN, respectively. SegMLP is a multilayer perceptron (MLP) that extracts high-level feature representation from the manually designed perceptual features, and SegCNN is a convolutional neural network (CNN) that automatically learn emotion-specific features from the log Mel filterbanks. Extensive emotion recognition experiments are carried out on the CASIA corpus and the IEMOCAP database. We find that: (1) the aggregation of segment-level decisions provides richer information than the statistics over the low-level descriptors (LLDs) across the whole utterance; (2) automatic feature learning outperforms manual features. Our experimental results are also compared with those of state-of-the-art methods, further demonstrating the effectiveness of the proposed approach",
    "checked": true,
    "id": "5a72ea19ac47f5cb3d049cc7356e4f644e46c86c",
    "semantic_title": "deep learning of segment-level feature representation with multiple instance learning for utterance-level speech emotion recognition",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2019/triantafyllopoulos19_interspeech.html": {
    "title": "Towards Robust Speech Emotion Recognition Using Deep Residual Networks for Speech Enhancement",
    "volume": "main",
    "abstract": "The use of deep learning (DL) architectures for speech enhancement has recently improved the robustness of voice applications under diverse noise conditions. These improvements are usually evaluated based on the perceptual quality of the enhanced audio or on the performance of automatic speech recognition (ASR) systems. We are interested instead in the usefulness of these algorithms in the field of speech emotion recognition (SER), and specifically in whether an enhancement architecture can effectively remove noise while preserving enough information for an SER algorithm to accurately identify emotion in speech. We first show how a scalable DL architecture can be trained to enhance audio signals in a large number of unseen environments, and go on to show how that can benefit common SER pipelines in terms of noise robustness. Our results show that incorporating a speech enhancement architecture is beneficial, especially for low signal-to-noise ratio (SNR) conditions",
    "checked": true,
    "id": "a31dd89ac8258b2fc27c82d30dcb5c7beb49dc92",
    "semantic_title": "towards robust speech emotion recognition using deep residual networks for speech enhancement",
    "citation_count": 35
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19f_interspeech.html": {
    "title": "Towards Discriminative Representations and Unbiased Predictions: Class-Specific Angular Softmax for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Speech emotion recognition (SER) is a challenging task: the complex emotional expressions make it difficult to discriminate different emotions; the unbalanced data misleads models to give biased predictions. In this work, we tackle these two problems by the angular softmax loss. First, we replace the vanilla softmax with angular softmax to learn emotional representations with strong discriminant power. Besides, inspired by its novel geometric interpretation, we establish a general calculation model and deduce a concise formula of decision domain. Based on these derivations, we propose our solution to data imbalance: class-specific angular softmax by which we can directly adjust decision domains of different emotion classes. Experimental results on the IEMOCAP corpus indicate significant improvements on two state-of-the-art models therefore demonstrate the effectiveness of our proposed methods",
    "checked": true,
    "id": "e2489151e20164a354952a3f17b266d8b4e13600",
    "semantic_title": "towards discriminative representations and unbiased predictions: class-specific angular softmax for speech emotion recognition",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jalal19_interspeech.html": {
    "title": "Learning Temporal Clusters Using Capsule Routing for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Emotion recognition from speech plays a significant role in adding emotional intelligence to machines and making human-machine interaction more natural. One of the key challenges from machine learning standpoint is to extract patterns which bear maximum correlation with the emotion information encoded in this signal while being as insensitive as possible to other types of information carried by speech. In this paper, we propose a novel temporal modelling framework for robust emotion classification using bidirectional long short-term memory network (BLSTM), CNN and Capsule networks. The BLSTM deals with the temporal dynamics of the speech signal by effectively representing forward/backward contextual information while the CNN along with the dynamic routing of the Capsule net learn temporal clusters which altogether provide a state-of-the-art technique for classifying the extracted patterns. The proposed approach was compared with a wide range of architectures on the FAU-Aibo and RAVDESS corpora and remarkable gain over state-of-the-art systems were obtained. For FAO-Aibo and RAVDESS 77.6% and 56.2% accuracy was achieved, respectively, which is 3% and 14% (absolute) higher than the best-reported result for the respective tasks",
    "checked": true,
    "id": "221f95be1855f01ce00970120d856cbe95a1b491",
    "semantic_title": "learning temporal clusters using capsule routing for speech emotion recognition",
    "citation_count": 39
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dapolito19_interspeech.html": {
    "title": "L2 Pronunciation Accuracy and Context: A Pilot Study on the Realization of Geminates in Italian as L2 by French Learners",
    "volume": "main",
    "abstract": "This paper investigates the interaction between the characteristics of both L1 and L2 phonetic-phonological systems and how the context, in terms of the amount of information available (less vs more information), may influence the accuracy in producing L2 sounds as well as speech fluency. Specifically, it focuses on how French learners of Italian as L2, representing two different competence levels (lower and higher), realize geminates (non-native sounds) in two different contexts (less and more rich). A rich context is expected to induce lower accuracy. Acoustic data of nine subjects (three beginners, three advanced and three natives as control) were collected and analyzed in order to observe: 1) the realization of geminates (duration of the consonant and preceding vowel as an index of accuracy); and 2) the speech fluency (number and duration of disfluencies; speech/articulation rate). Results suggest that learners' productions are affected by L1, above all in the case of beginners, who show a lower degree of accuracy. As regards the accuracy and context interaction, results show that the production of geminates is more accurate (longer duration) in poor than in rich context. Further, a higher number of disfluencies is found in rich than in poor context",
    "checked": true,
    "id": "68db7dba7e641dd80dcc0237734f5a615bca3143",
    "semantic_title": "l2 pronunciation accuracy and context: a pilot study on the realization of geminates in italian as l2 by french learners",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jamakovic19_interspeech.html": {
    "title": "The Monophthongs of Formal Nigerian English: An Acoustic Analysis",
    "volume": "main",
    "abstract": "Postcolonial varieties of English, used in countries such as Nigeria, the Philippines and India, are influenced by local (\"endonormative\") and external (\"exonormative\") forces, the latter often in the form of British/American English. In the ensuing stylistic continuum, informal speech is more endonormatively oriented than formal/educated speech — which is, in turn, clearly distinguishable from British/American English. The formal subvariety is often regarded as the incipient local standard and is commonly less marked by L1 influence than the informal subvariety Nigerian English (NigE) is the most widely spoken African variety of English, but empirical/quantitative descriptions are rare. In this pilot study, we present an acoustic analysis of eleven phonological monophthongs and two phonological diphthongs that are commonly monophthongised. A total of 811 occurrences, produced in formal contexts by nine educated speakers of NigE with L1 Igbo, was extracted from the ICE Nigeria corpus and analysed acoustically (Lobanov-normalised vowel formants at vowel midpoint) Results show that the NigE speakers reduced the thirteen vowel system to a total of nine distinct phonemes that closely resembles the L1 Igbo vowel inventory. This result suggests substantial L1 influence even at the level of Formal NigE",
    "checked": true,
    "id": "b1b84144c9caea63b386ad6d1392dc44d827920b",
    "semantic_title": "the monophthongs of formal nigerian english: an acoustic analysis",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/arantes19_interspeech.html": {
    "title": "Quantifying Fundamental Frequency Modulation as a Function of Language, Speaking Style and Speaker",
    "volume": "main",
    "abstract": "In this study, we outline a methodology to quantify the degree of similarity between pairs of f distributions based on the Anderson-Darling measure that underlies its namesake goodness-of-fit test. The procedure emphasizes differences due to more fine-grained f modulations rather than differences in measures of central tendency, such as the mean and median. In order to assess the procedure's usefulness for speaker comparison, we applied it to a multilingual corpus in which participants contributed speech delivered in three speaking styles. The similarity measure was calculated separately as function of speaking style and speaker. Between-speaker variability (different speakers, same style) in distribution similarity varied significantly between styles — spontaneous interview shows greater variability than read sentences and word list in five languages (English, French, Italian, Portuguese and Swedish); in Estonian and German, read sentences yield more variability. Within-speaker variability (same speaker, different styles) levels are lower than between-speaker in the style that exhibit the greatest variability. The results point to the potential use of the proposed methodology as a way to identify possible idiosyncratic traits in f distributions. Also, they further demonstrate the effect of speaking styles on intonation patterns",
    "checked": true,
    "id": "86458a284a34ad335714d75425e49286b500f5d3",
    "semantic_title": "quantifying fundamental frequency modulation as a function of language, speaking style and speaker",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kelly19_interspeech.html": {
    "title": "The Voicing Contrast in Stops and Affricates in the Western Armenian of Lebanon",
    "volume": "main",
    "abstract": "Research on Western Armenian has described it as having a contrast between voiceless aspirated stops and affricates, and voiced stops and affricates [1, 2]. The variety of Western Armenian spoken by a large population in Lebanon has not yet been examined phonetically, to determine the acoustic correlates of this contrast. The current study examines the alveolar and postalveolar affricates and alveolar stops (voiceless aspirated and voiced) in both word-initial and word-medial position, using nonsense words written in the Armenian script. The results indicate that voiced sounds have prevoicing, voiceless affricates have some aspiration, but voiceless stops have very short VOT, which aligns better with an analysis of them being classified as unaspirated. It was also found that position in the word does not affect VOT, duration of the closure or frication",
    "checked": true,
    "id": "fa966a27f718ffa28c0aefb0a26cd8967c5f4021",
    "semantic_title": "the voicing contrast in stops and affricates in the western armenian of lebanon",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jatteau19_interspeech.html": {
    "title": "Gra[f] e!\" Word-Final Devoicing of Obstruents in Standard French: An Acoustic Study Based on Large Corpora",
    "volume": "main",
    "abstract": "This study investigates the tendency towards word-final devoicing of voiced obstruents in Standard French, and how devoicing is influenced by domain, speech style, manner and place of articulation. Three large corpora with automatic segmentations produced by forced alignment are used: ESTER, ETAPE and NCCFr. A voicing-ratio is established for each obstruent via F0 extraction in Praat, and the percentage of fully voiced segments is computed. We find a salient pattern of devoicing before pause, with no clear effect of speech style. Fricatives devoice more than stops, and posterior fricatives devoice more than anterior ones. Since voicing plays a central role in the cross-linguistic pattern of word-final [voice] neutralisation, this study gives insight into the potential phonetic precursors of this process",
    "checked": true,
    "id": "de779c29686147e7e091cbbf7fbb964d350cad8a",
    "semantic_title": "gra[f] e!\" word-final devoicing of obstruents in standard french: an acoustic study based on large corpora",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19g_interspeech.html": {
    "title": "Acoustic Indicators of Deception in Mandarin Daily Conversations Recorded from an Interactive Game",
    "volume": "main",
    "abstract": "Being able to distinguish the differences between deceptive and truthful statements in a dialogue is an important skill in daily life. Extensive studies on the acoustic features of deceptive English speech have been reported, but such research in Mandarin is relatively scarce. We constructed a Mandarin deception database of daily dialogues from native speakers in Taiwan. College students were recruited to participate in a game in which they were encouraged to lie and convince their opponents of experiences that they did not have. After data collection, acoustic-prosodic features were extracted. The statistics of these features were calculated so that the differences between truthful and deceptive sentences, both as they were intended and perceived, can be compared. Results indicate that different people tend to use different acoustic features when telling a lie; the participants could be put into 10 categories in a dendrogram, with an exception of 31 people from whom no acoustic indicators for deception were found. Without considering interpersonal differences, our best classifier reached an F1 score of 53.37% in distinguishing deceptive and truthful segmentation units. We hope to present this new database as a corpus for future studies on deception in Mandarin conversations",
    "checked": true,
    "id": "48adbfbc6b82bf3ee133f7078b72913951f671bc",
    "semantic_title": "acoustic indicators of deception in mandarin daily conversations recorded from an interactive game",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schuppler19_interspeech.html": {
    "title": "Prosodic Effects on Plosive Duration in German and Austrian German",
    "volume": "main",
    "abstract": "This study investigates the acoustic cues used to mark prosodic boundaries in two varieties of German, with a specific focus on variations in production of fortis and lenis plosives. We extracted prosodic-boundary-adjacent and non-boundary-adjacent plosives from GRASS (Austrian German) and the Kiel Corpus of Read Speech (Northern German), and investigated closure duration, burst features, and duration characteristics of the surrounding segments. We find that closure and burst duration features, as well as duration of a preceding adjacent segment, vary consistently in relationship to the presence or absence of a prosodic boundary, but that the relative weights of these features differ in the two varieties studied",
    "checked": true,
    "id": "88f25fd2b7a570a3f51264e0fb02a9a605f1b551",
    "semantic_title": "prosodic effects on plosive duration in german and austrian german",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/johny19_interspeech.html": {
    "title": "Cross-Lingual Consistency of Phonological Features: An Empirical Study",
    "volume": "main",
    "abstract": "The concept of a phoneme arose historically as a theoretical abstraction that applies language-internally. Using phonemes and phonological features in cross-linguistic settings raises an important question of conceptual validity: Are contrasts that are meaningful within a language also empirically robust across languages? This paper develops a method for assessing the crosslinguistic consistency of phonological features in phoneme inventories. The method involves training separate binary neural classifiers for several phonological contrast in audio spans centered on particular segments within continuous speech. To assess cross-linguistic consistency, these classifiers are evaluated on held-out languages and classification quality is reported. We apply this method to several common phonological contrasts, including vowel height, vowel frontness, and retroflex consonants, in the context of multi-speaker corpora for ten languages from three language families (Indo-Aryan, Dravidian, and Malayo-Polynesian). We empirically evaluate and discuss the consistency of phonological contrasts derived from features found in phonological ontologies such as PanPhon and PHOIBLE",
    "checked": true,
    "id": "f03740d071738682fbc1b9dd46c5748f6ba8697c",
    "semantic_title": "cross-lingual consistency of phonological features: an empirical study",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/guitardivent19_interspeech.html": {
    "title": "Are IP Initial Vowels Acoustically More Distinct? Results from LDA and CNN Classifications",
    "volume": "main",
    "abstract": "Past results have suggested that initial strengthening (IS) effects target the contrastive phonetic properties of segments, with a maximization of acoustic contrasts in initial position of strong prosodic domains. Here, we investigate whether IS effects translate into a better acoustic discriminability within the French oral vowels system. Discriminability is assessed on the basis of classification results of two types of classifiers: a linear discriminant analysis (LDA) based on the four formants frequencies, and a deep convolutional neural network (CNN) based on spectrograms. The test set includes 720 exemplars of /i, y, e, ε, a, x, u, o, ɔ/ (with /x/=/ø, œ/) produced in a labial context, either in intonational phrase initial (IPi) or word initial (Wi) position. Classifiers were trained using a set of 4500 vowels extracted from a large read speech corpus. Results show a better discriminability of vowels (overall better classification rate) in IPi than in Wi with the two methods. Less confusion in IPi is found between rounded and unrounded, and between back and front vowels, but not between the vowels along the four-way height contrast. Less confusion between peripheral and central vowels also expresses a maximization of contrasts within the acoustic space in IPi position",
    "checked": true,
    "id": "a3420b87393a1ed8ada6ba862d9c16b093986cb5",
    "semantic_title": "are ip initial vowels acoustically more distinct? results from lda and cnn classifications",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wei19_interspeech.html": {
    "title": "Neural Network-Based Modeling of Phonetic Durations",
    "volume": "main",
    "abstract": "A deep neural network (DNN)-based model has been developed to predict non-parametric distributions of durations of phonemes in specified phonetic contexts and used to explore which factors influence durations most. Major factors in US English are pre-pausal lengthening, lexical stress, and speaking rate. The model can be used to check that text-to-speech (TTS) training speech follows the script and words are pronounced as expected. Duration prediction is poorer with training speech for automatic speech recognition (ASR) because the training corpus typically consists of single utterances from many speakers and is often noisy or casually spoken. Low probability durations in ASR training material nevertheless mostly correspond to non-standard speech, with some having disfluencies. Children's speech is disproportionately present in these utterances, since children show much more variation in timing",
    "checked": true,
    "id": "480288deec8081b36ed33805b91f0498236ee6ec",
    "semantic_title": "neural network-based modeling of phonetic durations",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/moczanow19_interspeech.html": {
    "title": "An Acoustic Study of Vowel Undershoot in a System with Several Degrees of Prominence",
    "volume": "main",
    "abstract": "The paper presents the results of a pilot study investigating the relationship between vowel quality and duration in Ukrainian. In this language, lexical stress is cued by increased duration; smaller but systematic differences in length occur between unstressed, rhythmic stress-bearing, and pretonic syllables. The presence of several degrees of lengthening within one word makes it possible to test the long-established theories of vowel reduction posing a direct link between decreased duration and vowel undershoot. Overall, the analysis of the aggregated data collected from four native speakers of Ukrainian points to a strong correlation between decreasing duration and the undershoot of F1 targets. However, in separate by-position and by-speaker analyses, no correlation between F1 and duration is observed in the positions of rhythmic and lexical stress. We thus conclude that the stability of the F1 target vis-à-vis temporal parameters may constitute another parameter expressing metrical prominence. In addition, our data suggests that formant undershoot may be affected by an articulatory effort",
    "checked": true,
    "id": "88cb975099f83d4e7af9de1f589085f6c90b40c3",
    "semantic_title": "an acoustic study of vowel undershoot in a system with several degrees of prominence",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/berger19_interspeech.html": {
    "title": "A Preliminary Study of Charismatic Speech on YouTube: Correlating Prosodic Variation with Counts of Subscribers, Views and Likes",
    "volume": "main",
    "abstract": "This paper is a first investigation into the influence of the pitch range and the intensity variation on the number of subscribers, views and likes of YouTube Creators. A total of ten minutes of speech material from five English and five North-American YouTubers was analyzed. The results for pitch range and intensity variation suggest that an increase in both parameters results in higher subscriber counts. For views, there was no influence of pitch range, but an increase in intensity variation results in a lower number of views. Pitch range and intensity variation had no influence on the like count. Furthermore, both origin and gender had an influence on the results. Ultimately, this study will provide further information for the phonetic research of charisma (i.e., the perceived charm, competence, power, and persuasiveness of a speaker), as it is suspected that the acoustic features that have so far been connected to charisma also play an important role in the success of a YouTuber and their channel",
    "checked": true,
    "id": "4f1af0125197ec6bf72a5a85b56a618c7ff0da78",
    "semantic_title": "a preliminary study of charismatic speech on youtube: correlating prosodic variation with counts of subscribers, views and likes",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/luo19b_interspeech.html": {
    "title": "Phonetic Detail Encoding in Explaining the Size of Speech Planning Window",
    "volume": "main",
    "abstract": "With the ultimate goal of understanding the production planning scope, this study manipulates phonetic information (place of articulation and voicing) and measures three acoustic cues to analyze consonant clusters across words produced by English (L1) and Mandarin (L2) speakers. We continue to explore a) how phonetic detail interacts with prosodic boundary in modulating surface realization, and b) the roles of phonetic information in speech planning motor control. The results show that L2 speakers exhibited different acoustic deviations varying with their proficiency level. The group with lower L2 proficiency significantly deviated from the L1 group in release likelihood and closure shortening, while the higher-proficiency group exhibited less nativelike performance in terms of closure durations. The results also discover that all speakers are subject to language-independent articulatory constraint at word boundaries, while language-specific phonetic detail accounts for more nonnative deviations. The core findings highlight a long-distance speech planning scope in native speech, with cross-word phonetic information interacting with prosodic encoding. It is argued that phonology applies blindly across words and is independent of lexical cognitive load",
    "checked": true,
    "id": "7ecea15ab19fe879f9a665b76f74bb4f1fbddecf",
    "semantic_title": "phonetic detail encoding in explaining the size of speech planning window",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zarka19_interspeech.html": {
    "title": "Acoustic Cues to Topic and Narrow Focus in Egyptian Arabic",
    "volume": "main",
    "abstract": "This study investigates acoustic cues (duration, scaling and alignment of peaks and valleys) to the prosodic realization of topics and narrow subject foci in a declarative SVO sentence in Egyptian Arabic. Morpho-syntactically identical sentences were elicited in appropriately designed contexts from 18 native speakers by means of a question-answer paradigm. The results show that the stressed syllable of a focused word is longer than the stressed syllable of the same word in topic condition. Additionally, the peaks of foci are generally scaled higher than those of topics. These differences clearly point to varying degrees of prosodic prominence. Furthermore, the alignment of the F0 peak and the subsequent low endpoint of a rising-falling tonal contour is earlier in foci than in topics, indicating that focus is signaled by an early sharp fall whereas the falling part of the tonal gesture starts later and is shallower in the case of a topic. Overall, our results suggest that narrow subject foci and topics tend to be associated with different pitch events",
    "checked": true,
    "id": "0b7a73c553db407f3fd17475721c82453d9c20d5",
    "semantic_title": "acoustic cues to topic and narrow focus in egyptian arabic",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/alowonou19_interspeech.html": {
    "title": "Acoustic and Articulatory Study of Ewe Vowels: A Comparative Study of Male and Female",
    "volume": "main",
    "abstract": "In order to investigate the difference in Ewe males and Ewe females during the production of Ewe vowels, results from the comparative quantitative and qualitative assessments of tongue shape and movement using ultrasound imaging as well as the comparative evaluation of F1 and F2 frequency values from data collected from 9 Ewe male speakers and 6 Ewe female speakers, were presented in this study. The results showed that vowels are produced with higher formant frequencies by Ewe female speakers compared to Ewe male speakers, except for the vowel /ε/ produced with a lower F1 frequency by Ewe females. The articulatory results showed a higher and more forwarder tongue configuration for Ewe male compared to female counterparts",
    "checked": true,
    "id": "a68ce3e616e1a3d73563657a6d7fbcbd2fcabd35",
    "semantic_title": "acoustic and articulatory study of ewe vowels: a comparative study of male and female",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/guo19c_interspeech.html": {
    "title": "Speech Augmentation via Speaker-Specific Noise in Unseen Environment",
    "volume": "main",
    "abstract": "Speech augmentation is a common and effective strategy to avoid overfitting and improve on the robustness of an emotion recognition model. In this paper, we investigate for the first time the intrinsic attributes in a speech signal using the multi-resolution analysis theory and the Hilbert-Huang Spectrum, with the goal of developing a robust speech augmentation approach from raw speech data. Specifically, speech decomposition in a double tree complex wavelet transform domain is realized, to obtain sub-speech signals; then, the Hilbert Spectrum using Hilbert-Huang Transform is calculated for each sub-band to capture the noise content in unseen environments with the voice restriction to 100–4000 Hz; finally, the speech-specific noise that varies with the speaker individual, scenarios, environment, and voice recording equipment, can be reconstructed from the top two high-frequency sub-bands to enhance the raw signal. Our proposed speech augmentation is demonstrated using five robust machine learning architectures based on the RAVDESS database, achieving up to 9.3% higher accuracy compared to the performance on raw data for an emotion recognition task",
    "checked": true,
    "id": "244fb2ad025aea8a74e6f4899d480be2d10262f7",
    "semantic_title": "speech augmentation via speaker-specific noise in unseen environment",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hao19_interspeech.html": {
    "title": "UNetGAN: A Robust Speech Enhancement Approach in Time Domain for Extremely Low Signal-to-Noise Ratio Condition",
    "volume": "main",
    "abstract": "Speech enhancement at extremely low signal-to-noise ratio (SNR) condition is a very challenging problem and rarely investigated in previous works. This paper proposes a robust speech enhancement approach (UNetGAN) based on U-Net and generative adversarial learning to deal with this problem. This approach consists of a generator network and a discriminator network, which operate directly in the time domain. The generator network adopts a U-Net like structure and employs dilated convolution in the bottleneck of it. We evaluate the performance of the UNetGAN at low SNR conditions (up to -20dB) on the public benchmark. The result demonstrates that it significantly improves the speech quality and substantially outperforms the representative deep learning models, including SEGAN, cGAN fo SE, Bidirectional LSTM using phase-sensitive spectrum approximation cost function (PSA-BLSTM) and Wave-U-Net regarding Short-Time Objective Intelligibility (STOI) and Perceptual evaluation of speech quality (PESQ)",
    "checked": true,
    "id": "7c90f01050817d1f8ddba9359737f12cdb96560c",
    "semantic_title": "unetgan: a robust speech enhancement approach in time domain for extremely low signal-to-noise ratio condition",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pascual19b_interspeech.html": {
    "title": "Towards Generalized Speech Enhancement with Generative Adversarial Networks",
    "volume": "main",
    "abstract": "The speech enhancement task usually consists of removing additive noise or reverberation that partially mask spoken utterances, affecting their intelligibility. However, little attention is drawn to other, perhaps more aggressive signal distortions like clipping, chunk elimination, or frequency-band removal. Such distortions can have a large impact not only on intelligibility, but also on naturalness or even speaker identity, and require of careful signal reconstruction. In this work, we give full consideration to this generalized speech enhancement task, and show it can be tackled with a time-domain generative adversarial network (GAN). In particular, we extend a previous GAN-based speech enhancement system to deal with mixtures of four types of aggressive distortions. Firstly, we propose the addition of an adversarial acoustic regression loss that promotes a richer feature extraction at the discriminator. Secondly, we also make use of a two-step adversarial training schedule, acting as a warm up-and-fine-tune sequence. Both objective and subjective evaluations show that these two additions bring improved speech reconstructions that better match the original speaker identity and naturalness",
    "checked": true,
    "id": "1bc219f54053ba7d97d55db119785f1777af597a",
    "semantic_title": "towards generalized speech enhancement with generative adversarial networks",
    "citation_count": 27
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19g_interspeech.html": {
    "title": "A Convolutional Neural Network with Non-Local Module for Speech Enhancement",
    "volume": "main",
    "abstract": "Convolution neural networks (CNNs) are achieving increasing attention for the speech enhancement task recently. However, the convolutional operations only process a local neighborhood (several nearest neighboring neurons) at a time across either space or time direction. The long-range dependencies can only be captured when the convolutional operations are applied recursively, but the problems of computationally inefficient and optimization difficulties are introduced. Inspired by the recent impressive performance of the non-local module in many computer vision tasks, we propose a convolutional neural network with non-local module for speech enhancement in this paper. The non-local operations are capable of capturing the global information in the frequency domain through passing information between distant time-frequency units. The non-local operations are able to set the dimension of the input as an arbitrary value, which results in the easy integration with our proposed network framework. Experimental results demonstrate that the proposed method not only improves the computational efficiency significantly but also outperforms the competing methods in terms of objective speech intelligibility and quality metrics",
    "checked": true,
    "id": "4a7e7742eaf11beb7dddacc966b0fc3e94c1a277",
    "semantic_title": "a convolutional neural network with non-local module for speech enhancement",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lin19b_interspeech.html": {
    "title": "IA-NET: Acceleration and Compression of Speech Enhancement Using Integer-Adder Deep Neural Network",
    "volume": "main",
    "abstract": "Numerous compression and acceleration techniques achieved state-of-the-art results for classification tasks in speech processing. However, the same techniques produce unsatisfactory performance for regression tasks, because of the different natures of classification and regression tasks. This paper presents a novel integer-adder deep neural network (IA-Net), which compresses model size and accelerates the inference process in speech enhancement, an important task in speech-signal processing, by replacing the floating-point multiplier with an integer-adder. The experimental results show that the inference time of IA-Net can be significantly reduced by 20% and the model size can be compressed by 71.9% without any performance degradation. To the best of our knowledge, this is the first study that decreases the inference time and compresses the model size, simultaneously, while producing good performance for speech enhancement. Based on the promising results, we believe that the proposed framework can be deployed in various mobile and edge-computing devices",
    "checked": true,
    "id": "7c8f6b581d36a99bd323ad540964f15899f7db18",
    "semantic_title": "ia-net: acceleration and compression of speech enhancement using integer-adder deep neural network",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chai19_interspeech.html": {
    "title": "KL-Divergence Regularized Deep Neural Network Adaptation for Low-Resource Speaker-Dependent Speech Enhancement",
    "volume": "main",
    "abstract": "In this paper, we propose a Kullback-Leibler divergence (KLD) regularized approach to adapting speaker-independent (SI) speech enhancement model based on regression deep neural networks (DNNs) to another speaker-dependent (SD) model using a tiny amount of speaker-specific adaptation data. This algorithm adapts the DNN model conservatively by forcing the conditional target distribution estimated from the SD model to be close to that from the SI model. The constraint is realized by adding KLD regularization to our previously proposed maximum likelihood objective function. Experimental results demonstrate that, even with only 10 seconds of SD adaptation data, the proposed framework consistently achieves speech intelligibility improvements under all 15 unseen noise types evaluated and at all signal-to-noise ratio levels for all 8 test speakers from the WSJ0 evaluation set",
    "checked": true,
    "id": "c04558ac3e8558aebbb9dbf6be6154717bfbf821",
    "semantic_title": "kl-divergence regularized deep neural network adaptation for low-resource speaker-dependent speech enhancement",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/llombart19_interspeech.html": {
    "title": "Speech Enhancement with Wide Residual Networks in Reverberant Environments",
    "volume": "main",
    "abstract": "This paper proposes a speech enhancement method which exploits the high potential of residual connections in a Wide Residual Network architecture. This is supported on single dimensional convolutions computed alongside the time domain, which is a powerful approach to process contextually correlated representations through the temporal domain, such as speech feature sequences. We find the residual mechanism extremely useful for the enhancement task since the signal always has a linear shortcut and the non-linear path enhances it in several steps by adding or subtracting corrections. The enhancement capability of the proposal is assessed by objective quality metrics evaluated with simulated and real samples of reverberated speech signals. Results show that the proposal outperforms the state-of-the-art method called WPE, which is known to effectively reduce reverberation and greatly enhance the signal. The proposed model, trained with artificial synthesized reverberation data, was able to generalize to real room impulse responses for a variety of conditions (e.g. different room sizes, RT , near & far field). Furthermore, it achieves accuracy for real speech with reverberation from two different datasets",
    "checked": true,
    "id": "f6753201f2553cad294046046bf4683f45e1019f",
    "semantic_title": "speech enhancement with wide residual networks in reverberant environments",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/reddy19_interspeech.html": {
    "title": "A Scalable Noisy Speech Dataset and Online Subjective Test Framework",
    "volume": "main",
    "abstract": "Background noise is a major source of quality impairments in Voice over Internet Protocol (VoIP) and Public Switched Telephone Network (PSTN) calls. Recent work shows the efficacy of deep learning for noise suppression, but the datasets have been relatively small compared to those used in other domains (e.g., ImageNet) and the associated evaluations have been more focused. In order to better facilitate deep learning research in Speech Enhancement, we present a noisy speech dataset (MS-SNSD) that can scale to arbitrary sizes depending on the number of speakers, noise types, and Speech to Noise Ratio (SNR) levels desired. We show that increasing dataset sizes increases noise suppression performance as expected. In addition, we provide an open-source evaluation methodology to evaluate the results subjectively at scale using crowdsourcing, with a reference algorithm to normalize the results. To demonstrate the dataset and evaluation framework we apply it to several noise suppressors and compare the subjective Mean Opinion Score (MOS) with objective quality measures such as SNR, PESQ, POLQA, and VISQOL and show why MOS is still required. Our subjective MOS evaluation is the first large scale evaluation of Speech Enhancement algorithms that we are aware of",
    "checked": true,
    "id": "cc3b0272547e5d1d7afdb450e9df5d2805f5c42e",
    "semantic_title": "a scalable noisy speech dataset and online subjective test framework",
    "citation_count": 86
  },
  "https://www.isca-speech.org/archive/interspeech_2019/adiga19_interspeech.html": {
    "title": "Speech Enhancement for Noise-Robust Speech Synthesis Using Wasserstein GAN",
    "volume": "main",
    "abstract": "The quality of speech synthesis systems can be significantly deteriorated by the presence of background noise in the recordings. Despite the existence of speech enhancement techniques for effectively suppressing additive noise under low signal-to-noise (SNR) conditions, these techniques have been neither designed nor tested in speech synthesis tasks where background noise has relatively lower energy. In this paper, we propose a speech enhancement technique based on generative adversarial networks (GANs) which acts as a preprocessing step of speech synthesis. Motivated by the speech enhancement generative adversarial network (SEGAN) approach and recent advances in deep learning, we propose to use Wasserstein GAN (WGAN) with gradient penalty and gated activation functions to the autoencoder network of SEGAN. We studied the impact of the proposed method on a data set consisting of 28 speakers and different noise types with 3 different SNR level. The effectiveness of the proposed method in the context of speech synthesis is demonstrated through the training of WaveNet vocoder. We compare our method against SEGAN. Both subjective and objective metrics confirm that the proposed speech enhancement approach outperforms SEGAN in terms of speech synthesis quality",
    "checked": true,
    "id": "5d8811ea4ed096b65572d9192c4e3cc26572523f",
    "semantic_title": "speech enhancement for noise-robust speech synthesis using wasserstein gan",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pv19_interspeech.html": {
    "title": "A Non-Causal FFTNet Architecture for Speech Enhancement",
    "volume": "main",
    "abstract": "In this paper, we suggest a new parallel, non-causal and shallow waveform domain architecture for speech enhancement based on FFTNet, a neural network for generating high quality audio waveform. In contrast to other waveform based approaches like WaveNet, FFTNet uses an initial wide dilation pattern. Such an architecture better represents the long term correlated structure of speech in the time domain, where noise is usually highly non-correlated, and therefore it is suitable for waveform domain based speech enhancement. To further strengthen this feature of FFTNet, we suggest a non-causal FFTNet architecture, where the present sample in each layer is estimated from the past and future samples of the previous layer. By suggesting a shallow network and applying non-causality within certain limits, the suggested FFTNet for speech enhancement (SE-FFTNet) uses much fewer parameters compared to other neural network based approaches for speech enhancement like WaveNet and SEGAN. Specifically, the suggested network has considerably reduced model parameters: 32% fewer compared to WaveNet and 87% fewer compared to SEGAN. Finally, based on subjective and objective metrics, SE-FFTNet outperforms WaveNet in terms of enhanced signal quality, while it provides equally good performance as SEGAN",
    "checked": true,
    "id": "51788627f814026348ba0941ab70b7275edccf0d",
    "semantic_title": "a non-causal fftnet architecture for speech enhancement",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/braithwaite19_interspeech.html": {
    "title": "Speech Enhancement with Variance Constrained Autoencoders",
    "volume": "main",
    "abstract": "Recent machine learning based approaches to speech enhancement operate in the time domain and have been shown to outperform the classical enhancement methods. Two such models are SE-GAN and SE-WaveNet, both of which rely on complex neural network architectures, making them expensive to train. We propose using the Variance Constrained Autoencoder (VCAE) for speech enhancement. Our model uses a more straightforward neural network structure than competing solutions and is a natural model for the task of speech enhancement. We demonstrate experimentally that the proposed enhancement model outperforms SE-GAN and SE-WaveNet in terms of perceptual quality of enhanced signals",
    "checked": true,
    "id": "a5ac6be3ea4543fb0b87e38d9cfeb7f8181880ab",
    "semantic_title": "speech enhancement with variance constrained autoencoders",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kyriakopoulos19_interspeech.html": {
    "title": "A Deep Learning Approach to Automatic Characterisation of Rhythm in Non-Native English Speech",
    "volume": "main",
    "abstract": "A speaker's rhythm contributes to the intelligibility of their speech and can be characteristic of their language and accent. For non-native learners of a language, the extent to which they match its natural rhythm is an important predictor of their proficiency. As a learner improves, their rhythm is expected to become less similar to their L1 and more to the L2. Metrics based on the variability of the durations of vocalic and consonantal intervals have been shown to be effective at detecting language and accent. In this paper, pairwise variability (PVI, CCI) and variance (varcoV, varcoC) metrics are first used to predict proficiency and L1 of non-native speakers taking an English spoken exam. A deep learning alternative to generalise these features is then presented, in the form of a tunable duration embedding, based on attention over an RNN over durations. The RNN allows relationships beyond pairwise to be captured, while attention allows sensitivity to the different relative importance of durations. The system is trained end-to-end for proficiency and L1 prediction and compared to the baseline. The values of both sets of features for different proficiency levels are then visualised and compared to native speech in the L1 and the L2",
    "checked": true,
    "id": "5779cff11e1968416c5914cf228c5ca578363230",
    "semantic_title": "a deep learning approach to automatic characterisation of rhythm in non-native english speech",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/merkx19_interspeech.html": {
    "title": "Language Learning Using Speech to Image Retrieval",
    "volume": "main",
    "abstract": "Humans learn language by interaction with their environment and listening to other humans. It should also be possible for computational models to learn language directly from speech but so far most approaches require text. We improve on existing neural network approaches to create visually grounded embeddings for spoken utterances. Using a combination of a multi-layer GRU, importance sampling, cyclic learning rates, ensembling and vectorial self-attention our results show a remarkable increase in image-caption retrieval performance over previous work. Furthermore, we investigate which layers in the model learn to recognise words in the input. We find that deeper network layers are better at encoding word presence, although the final layer has slightly lower performance. This shows that our visually grounded sentence encoder learns to recognise words from the input even though it is not explicitly trained for word recognition",
    "checked": true,
    "id": "c3465f1ebc248b9c955a1baff8778afe08988b8a",
    "semantic_title": "language learning using speech to image retrieval",
    "citation_count": 40
  },
  "https://www.isca-speech.org/archive/interspeech_2019/skidmore19_interspeech.html": {
    "title": "Using Alexa for Flashcard-Based Learning",
    "volume": "main",
    "abstract": "Despite increasing awareness of Alexa's potential as an educational tool, there remains a limited scope for Alexa skills to accommodate the features required for effective language learning. This paper describes an investigation into implementing ‘spaced-repetition', a non-trivial feature of flashcard-based learning, through the development of an Alexa skill called ‘Japanese Flashcards'. Here we show that existing Alexa development features such as skill persistence allow for the effective implementation of spaced-repetition and suggest a heuristic adaptation of the spaced-repetition model that is appropriate for use with voice assistants (VAs). We also highlight areas of the Alexa development process that limit the facilitation of language learning, namely the lack of multilingual speech recognition, and offer solutions to these current limitations. Overall, the investigation shows that Alexa can successfully facilitate simple L2-L1 flashcard-based language learning and highlights the potential for Alexa to be used as a sophisticated and effective language learning tool",
    "checked": true,
    "id": "00e673536b1948db76cc0560d82b005013e65e2e",
    "semantic_title": "using alexa for flashcard-based learning",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hansen19_interspeech.html": {
    "title": "The 2019 Inaugural Fearless Steps Challenge: A Giant Leap for Naturalistic Audio",
    "volume": "main",
    "abstract": "The 2019 FEARLESS STEPS (FS-1) Challenge is an initial step to motivate a streamlined and collaborative effort from the speech and language community towards addressing massive naturalistic audio, the first of its kind. The Fearless Steps Corpus is a collection of 19,000 hours of multi-channel recordings of spontaneous speech from over 450 speakers under multiple noise conditions. A majority of the Apollo Missions original analog data is unlabeled and has thus far motivated the development of both unsupervised and semi-supervised strategies. This edition of the challenge encourages the development of core speech and language technology systems for data with limited ground-truth / low resource availability and is intended to serve as the \"First Step\" towards extracting high-level information from such massive unlabeled corpora. In conjunction with the Challenge, 11,000 hours of synchronized 30-channel Apollo-11 audio data has also been released to the public by CRSS-UTDallas. We describe in this paper the Fearless Steps Corpus, Challenge Tasks, their associated baseline systems, and results. In conclusion, we also provide insights gained by the CRSS-UTDallas team during the inaugural Fearless Steps Challenge",
    "checked": true,
    "id": "181d41545fbf71953399f2df48bc9f1cc3612743",
    "semantic_title": "the 2019 inaugural fearless steps challenge: a giant leap for naturalistic audio",
    "citation_count": 30
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19e_interspeech.html": {
    "title": "Completely Unsupervised Phoneme Recognition by a Generative Adversarial Network Harmonized with Iteratively Refined Hidden Markov Models",
    "volume": "main",
    "abstract": "Producing a large annotated speech corpus for training ASR systems remains difficult for more than 95% of languages all over the world which are low-resourced, but collecting a relatively big unlabeled data set for such languages is more achievable. This is why some initial effort have been reported on completely unsupervised speech recognition learned from unlabeled data only, although with relatively high error rates. In this paper, we develop a Generative Adversarial Network (GAN) to achieve this purpose, in which a Generator and a Discriminator learn from each other iteratively to improve the performance. We further use a set of Hidden Markov Models (HMMs) iteratively refined from the machine generated labels to work in harmony with the GAN. The initial experiments on TIMIT data set achieve an phone error rate of 33.1%, which is 8.5% lower than the previous state-of-the-art",
    "checked": true,
    "id": "478be23706b0a9de1b4f002f909c211a18ed724d",
    "semantic_title": "completely unsupervised phoneme recognition by a generative adversarial network harmonized with iteratively refined hidden markov models",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/trisitichoke19_interspeech.html": {
    "title": "Analysis of Native Listeners' Facial Microexpressions While Shadowing Non-Native Speech — Potential of Shadowers' Facial Expressions for Comprehensibility Prediction",
    "volume": "main",
    "abstract": "Recently, researchers' attention has been paid to pronunciation assessment not based on comparison between L2 speech and native models, but based on comprehensibility of L2 speech [1, 2, 3]. In our previous studies [4, 5, 6], native listeners' shadowing of L2 speech was examined and it was shown that delay of shadowing and accuracy of articulation in shadowing utterances, both of which were acoustically calculated, are strongly influenced by the amount of cognitive load imposed for understanding L2 speech, especially when it is with strong accents. In this paper, aside from acoustic analysis of shadowings, we focus on shadowers' facial microexpressions and examine how they are correlated with perceived comprehensibility. To extract facial expression features, two methods are tested. One is a computer-vision-based method and recorded videos of shadowers' facial expressions are analyzed. The other is a method using a physiological sensor that can detect subtle movements of facial muscles. In experiments, four shadowers' facial expressions are analyzed, each of whom shadowed approximately 800 L2 utterances. Results show that some of shadowers' facial expressions are highly correlated with perceived comprehensibility, and that those facial expressions are strongly shadower-dependent. These results indicate a high potential of shadowers' facial expressions for comprehensibility prediction",
    "checked": true,
    "id": "3fb2aae7f3584075997668f2425839a48223b076",
    "semantic_title": "analysis of native listeners' facial microexpressions while shadowing non-native speech - potential of shadowers' facial expressions for comprehensibility prediction",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/karhila19_interspeech.html": {
    "title": "Transparent Pronunciation Scoring Using Articulatorily Weighted Phoneme Edit Distance",
    "volume": "main",
    "abstract": "For researching effects of gamification in foreign language learning for children in the \"Say It Again, Kid!\" project we developed a feedback paradigm that can drive gameplay in pronunciation learning games. We describe our scoring system based on the difference between a reference phone sequence and the output of a multilingual CTC phoneme recogniser. We present a white-box scoring model of mapped weighted Levenshtein edit distance between reference and error with error weights for articulatory differences computed from a training set of scored utterances. The system can produce a human-readable list of each detected mispronunciation's contribution to the utterance score. We compare our scoring method to established black box methods",
    "checked": true,
    "id": "01b41bea197ce7abfeb45dba44e5928c313e8807",
    "semantic_title": "transparent pronunciation scoring using articulatorily weighted phoneme edit distance",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yoon19_interspeech.html": {
    "title": "Development of Robust Automated Scoring Models Using Adversarial Input for Oral Proficiency Assessment",
    "volume": "main",
    "abstract": "In this study, we developed an automated scoring model for an oral proficiency test eliciting spontaneous speech from non-native speakers of English. In a large-scale oral proficiency test, a small number of responses may have atypical characteristics that make it difficult even for state-of-the-art automated scoring models to assign fair scores. The oral proficiency test in this study consisted of questions asking about content in materials provided to the test takers, and the atypical responses frequently had serious content abnormalities. In order to develop an automated scoring system that is robust to these atypical responses, we first developed a set of content features to capture content abnormalities. Next, we trained scoring models using the augmented training dataset, including synthetic atypical responses. Compared to the baseline scoring model, the new model showed comparable performance in scoring normal responses, while it assigned fairer scores for authentic atypical responses extracted from operational test administrations",
    "checked": true,
    "id": "acfefebe39c39c9e38363b81c6c729febc360581",
    "semantic_title": "development of robust automated scoring models using adversarial input for oral proficiency assessment",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lu19b_interspeech.html": {
    "title": "Impact of ASR Performance on Spoken Grammatical Error Detection",
    "volume": "main",
    "abstract": "Computer assisted language learning (CALL) systems aid learners to monitor their progress by providing scoring and feedback on language assessment tasks. Free speaking tests allow assessment of what a learner has said, as well as how they said it. For these tasks, Automatic Speech Recognition (ASR) is required to generate transcriptions of a candidate's responses, the quality of these transcriptions is crucial to provide reliable feedback in downstream processes. This paper considers the impact of ASR performance on Grammatical Error Detection (GED) for free speaking tasks, as an example of providing feedback on a learner's use of English. The performance of an advanced deep-learning based GED system, initially trained on written corpora, is used to evaluate the influence of ASR errors. One consequence of these errors is that grammatical errors can result from incorrect transcriptions as well as learner errors, this may yield confusing feedback. To mitigate the effect of these errors, and reduce erroneous feedback, ASR confidence scores are incorporated into the GED system. By additionally adapting the written text GED system to the speech domain, using ASR transcriptions, significant gains in performance can be achieved. Analysis of the GED performance for different grammatical error types and across grade is also presented",
    "checked": true,
    "id": "da7587615403b0d8889eabe1c37f516ff14669c5",
    "semantic_title": "impact of asr performance on spoken grammatical error detection",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yang19d_interspeech.html": {
    "title": "Self-Imitating Feedback Generation Using GAN for Computer-Assisted Pronunciation Training",
    "volume": "main",
    "abstract": "Self-imitating feedback is an effective and learner-friendly method for non-native learners in Computer-Assisted Pronunciation Training. Acoustic characteristics in native utterances are extracted and transplanted onto learner's own speech input, and given back to the learner as a corrective feedback. Previous works focused on speech conversion using prosodic transplantation techniques based on PSOLA algorithm. Motivated by the visual differences found in spectrograms of native and non-native speeches, we investigated applying GAN to generate self-imitating feedback by utilizing generator' s ability through adversarial training. Because this mapping is highly under-constrained, we also adopt cycle consistency loss to encourage the output to preserve the global structure, which is shared by native and non-native utterances. Trained on 97,200 spectrogram images of short utterances produced by native and non-native speakers of Korean, the generator is able to successfully transform the non-native spectrogram input to a spectrogram with properties of self-imitating feedback. Furthermore, the transformed spectrogram shows segmental corrections that cannot be obtained by prosodic transplantation. Perceptual test comparing the self-imitating and correcting abilities of our method with the baseline PSOLA method shows that the generative approach with cycle consistency loss is promising",
    "checked": true,
    "id": "5b378c2a4ae964fb8ac25d7b6ff689fb31880c6c",
    "semantic_title": "self-imitating feedback generation using gan for computer-assisted pronunciation training",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hori19_interspeech.html": {
    "title": "Joint Student-Teacher Learning for Audio-Visual Scene-Aware Dialog",
    "volume": "main",
    "abstract": "Multimodal fusion of audio, vision, and text has demonstrated significant benefits in advancing the performance of several tasks, including machine translation, video captioning, and video summarization. Audio-Visual Scene-aware Dialog (AVSD) is a new and more challenging task, proposed recently, that focuses on generating sentence responses to questions that are asked in a dialog about video content. While prior approaches designed to tackle this task have shown the need for multimodal fusion to improve response quality, the best-performing systems often rely heavily on human-generated summaries of the video content, which are unavailable when such systems are deployed in real-world. This paper investigates how to compensate for such information, which is missing in the inference phase but available during the training phase. To this end, we propose a novel AVSD system using student-teacher learning, in which a student network is (jointly) trained to mimic the teacher's responses. Our experiments demonstrate that in addition to yielding state-of-the-art accuracy against the baseline DSTC7-AVSD system, the proposed approach (which does not use human-generated summaries at test time) performs competitively with methods that do use those summaries",
    "checked": true,
    "id": "f78c136471778771c29fb385d3a8c1a1def28de1",
    "semantic_title": "joint student-teacher learning for audio-visual scene-aware dialog",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gopalakrishnan19_interspeech.html": {
    "title": "Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations",
    "volume": "main",
    "abstract": "Building socialbots that can have deep, engaging open-domain conversations with humans is one of the grand challenges of artificial intelligence (AI). To this end, bots need to be able to leverage world knowledge spanning several domains effectively when conversing with humans who have their own world knowledge. Existing knowledge-grounded conversation datasets are primarily stylized with explicit roles for conversation partners. These datasets also do not explore depth or breadth of topical coverage with transitions in conversations. We introduce Topical-Chat, a knowledge-grounded human-human conversation dataset where the underlying knowledge spans 8 broad topics and conversation partners don't have explicitly defined roles, to help further research in open-domain conversational AI. We also train several state-of-the-art encoder-decoder conversational models on Topical-Chat and perform automated and human evaluation for benchmarking",
    "checked": true,
    "id": "980456f50cd4b6e30649592afb693d5b6af8a703",
    "semantic_title": "topical-chat: towards knowledge-grounded open-domain conversations",
    "citation_count": 225
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kubasova19_interspeech.html": {
    "title": "Analyzing Verbal and Nonverbal Features for Predicting Group Performance",
    "volume": "main",
    "abstract": "This work analyzes the efficacy of verbal and nonverbal features of group conversation for the task of automatic prediction of group task performance. We describe a new publicly available survival task dataset that was collected and annotated to facilitate this prediction task. In these experiments, the new dataset is merged with an existing survival task dataset, allowing us to compare feature sets on a much larger amount of data than has been used in recent related work. This work is also distinct from related research on social signal processing (SSP) in that we compare verbal and nonverbal features, whereas SSP is almost exclusively concerned with nonverbal aspects of social interaction. A key finding is that nonverbal features from the speech signal are extremely effective for this task, even on their own. However, the most effective individual features are verbal features, and we highlight the most important ones",
    "checked": true,
    "id": "93891ca3b4d54ac985b6e1cef329c27ac73b6354",
    "semantic_title": "analyzing verbal and nonverbal features for predicting group performance",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2019/martinez19_interspeech.html": {
    "title": "Identifying Therapist and Client Personae for Therapeutic Alliance Estimation",
    "volume": "main",
    "abstract": "Psychotherapy, from a narrative perspective, is the process in which a client relates an on-going life-story to a therapist. In each session, a client will recount events from their life, some of which stand out as more significant than others. These significant stories can ultimately shape one's identity. In this work we study these narratives in the context of therapeutic alliance — a self-reported measure on the perception of a shared bond between client and therapist. We propose that alliance can be predicted from the interactions between certain types of clients with types of therapists. To validate this method, we obtained 1235 transcribed sessions with client-reported alliance to train an unsupervised approach to discover groups of therapists and clients based on common types of narrative characters, or personae. We measure the strength of the relation between personae and alliance in two experiments. Our results show that (1) alliance can be explained by the interactions between the discovered character types, and (2) models trained on therapist and client personae achieve significant performance gains compared to competitive supervised baselines. Finally, exploratory analysis reveals important character traits that lead to an improved perception of alliance",
    "checked": true,
    "id": "13ba6810b92a262a0af875d325dd79a7b892cd47",
    "semantic_title": "identifying therapist and client personae for therapeutic alliance estimation",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/haake19_interspeech.html": {
    "title": "Do Hesitations Facilitate Processing of Partially Defective System Utterances? An Exploratory Eye Tracking Study",
    "volume": "main",
    "abstract": "Spoken dialogue systems are predominantly evaluated using offline methods such as user ratings or task-oriented measures. Various phenomena in conversational speech, however, are known to affect the way the listener's comprehension unfolds over time, and not necessarily the final result of the comprehension process. For instance, in human reference comprehension, conversational signals like hesitations have been shown to ease processing of expressions referring to difficult-to-describe targets, as can primarily be observed in listeners' anticipatory eye movements rather than in their final reference resolution decision. In this study, we explore eye tracking for testing conversational dialogue systems, looking at how listeners process automatically generated referring expressions containing defective attributes. We investigate whether hesitations facilitate the processing of partially defective system utterances and track the user's eye movements when listening to expressions with: (i) semantically defective but fluently synthesized adjectives, (ii) defective and lengthened adjectives, i.e. containing a conversational uncertainty signal. Our results are encouraging: whereas the offline measure of task success does not show any differences between the two conditions, the listeners' eye movements suggest that processing of partially defective utterances might be facilitated by conversational hesitations",
    "checked": true,
    "id": "b50615d6f20d69ed2ddaacb8910b34e1682e9c8f",
    "semantic_title": "do hesitations facilitate processing of partially defective system utterances? an exploratory eye tracking study",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19h_interspeech.html": {
    "title": "Influence of Contextuality on Prosodic Realization of Information Structure in Chinese Dialogues",
    "volume": "main",
    "abstract": "In this paper, we present a detailed investigation on the influence of contextuality on the prosodic realization of information structure in Chinese dialogues. The materials were selected from the 863 corpus, which contains both isolated sentences and spontaneous dialogues. RefLex was selected as the annotation scheme, which differentiates information structure on the lexical and referential levels. Prosodic data (including duration and pitch range) from 12 groups of spontaneous dialogues were analyzed with the linear mixed effects mode, and each of them consists of 13–22 turns. The isolated sentences corresponding to these dialogues were also analyzed. The analysis results reveal the influence of contextuality. Specifically, the features of prosodic realization of information structure on the lexical and referential levels show a contrary tendency. The statistical analysis indicates that the speakers use duration and pitch ranges as phonetic cues to distinguish information structures on both levels. On the other hand, duration on the referential level is the only phonetic cue affected by contextuality",
    "checked": true,
    "id": "1424284bb70a3417be9b2b41c5566230c02722c1",
    "semantic_title": "influence of contextuality on prosodic realization of information structure in chinese dialogues",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gjoreski19_interspeech.html": {
    "title": "Cross-Lingual Transfer Learning for Affective Spoken Dialogue Systems",
    "volume": "main",
    "abstract": "This paper presents a case study of cross-lingual transfer learning applied for affective computing in the domain of spoken dialogue systems. Prosodic features of correction dialog acts are modeled on a group of languages and compared with languages excluded from the analysis Speech from different languages was recorded in carefully staged Wizard-of-Oz experiments, however, without the possibility to ensure balanced distribution of speakers per language. In order to assess the possibility of cross-lingual transfer learning and to ensure reliable classification of corrections independently of language, we employed different machine learning approaches along with relevant acoustic-prosodic features sets The results of the experiments with mono-lingual corpora (trained and tested on a single language) and cross-lingual (trained on several languages and tested on the rest) were analyzed and compared in the terms of accuracy and F1 score",
    "checked": true,
    "id": "f3b9a79b1b099fa0f2f22512f823e14063b8ae0a",
    "semantic_title": "cross-lingual transfer learning for affective spoken dialogue systems",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yu19_interspeech.html": {
    "title": "Identifying Personality Traits Using Overlap Dynamics in Multiparty Dialogue",
    "volume": "main",
    "abstract": "Research on human spoken language has shown that speech plays an important role in identifying speaker personality traits. In this work, we propose an approach for identifying speaker personality traits using overlap dynamics in multiparty spoken dialogues. We first define a set of novel features representing the overlap dynamics of each speaker. We then investigate the impact of speaker personality traits on these features using ANOVA tests. We find that features of overlap dynamics significantly vary for speakers with different levels of both Extraversion and Conscientiousness. Finally, we find that classifiers using only overlap dynamics features outperform random guessing in identifying Extraversion and Agreeableness, and that the improvements are statistically significant",
    "checked": true,
    "id": "9e515d969535a4b35f02cb4644ac427806c6d113",
    "semantic_title": "identifying personality traits using overlap dynamics in multiparty dialogue",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/aldeneh19_interspeech.html": {
    "title": "Identifying Mood Episodes Using Dialogue Features from Clinical Interviews",
    "volume": "main",
    "abstract": "Bipolar disorder, a severe chronic mental illness characterized by pathological mood swings from depression to mania, requires ongoing symptom severity tracking to both guide and measure treatments that are critical for maintaining long-term health. Mental health professionals assess symptom severity through semi-structured clinical interviews. During these interviews, they observe their patients' spoken behaviors, including both what the patients say and how they say it. In this work, we move beyond acoustic and lexical information, investigating how higher-level interactive patterns also change during mood episodes. We then perform a secondary analysis, asking if these interactive patterns, measured through dialogue features, can be used in conjunction with acoustic features to automatically recognize mood episodes. Our results show that it is beneficial to consider dialogue features when analyzing and building automated systems for predicting and monitoring mood",
    "checked": true,
    "id": "b5b5777a4b2ba959457822412a8e6d5131bca870",
    "semantic_title": "identifying mood episodes using dialogue features from clinical interviews",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lubold19_interspeech.html": {
    "title": "Do Conversational Partners Entrain on Articulatory Precision?",
    "volume": "main",
    "abstract": "The communication phenomenon known as conversational entrainment occurs when dialogue partners align or adapt their behavior to one another while conversing. Associated with rapport, trust, and communicative efficiency, entrainment appears to facilitate conversational success. In this work, we explore how conversational partners entrain or align on articulatory precision or the clarity with which speakers articulate their spoken productions. Articulatory precision also has implications for conversational success as precise articulation can enhance speech understanding and intelligibility. However, in conversational speech, speakers tend to reduce their articulatory precision, preferring low-cost, imprecise speech. Speakers may adapt their articulation and become more precise depending on feedback from their listeners. Given the potential of entrainment, we are interested in how conversational partners adapt or entrain their articulatory precision to one another. We explore this phenomenon in 57 task-based dialogues. Controlling for the influence of speaking rate, we find that speakers entrain on articulatory precision, with significant alignment on articulation of consonants. We discuss the potential applications that speaker alignment on precision might have for modeling conversation and implementing strategies for enhancing communicative success in human-human and human-computer interactions",
    "checked": true,
    "id": "d6a65211033fb4cf62523f4a293c976477dcad8d",
    "semantic_title": "do conversational partners entrain on articulatory precision?",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lian19_interspeech.html": {
    "title": "Conversational Emotion Analysis via Attention Mechanisms",
    "volume": "main",
    "abstract": "Different from the emotion recognition in individual utterances, we propose a multimodal learning framework using relation and dependencies among the utterances for conversational emotion analysis. The attention mechanism is applied to the fusion of the acoustic and lexical features. Then these fusion representations are fed into the self-attention based bi-directional gated recurrent unit (GRU) layer to capture long-term contextual information. To imitate real interaction patterns of different speakers, speaker embeddings are also utilized as additional inputs to distinguish the speaker identities during conversational dialogs. To verify the effectiveness of the proposed method, we conduct experiments on the IEMOCAP database. Experimental results demonstrate that our method shows absolute 2.42% performance improvement over the state-of-the-art strategies",
    "checked": true,
    "id": "7ca22be345ec2b0af792e695dd6846e6218881b1",
    "semantic_title": "conversational emotion analysis via attention mechanisms",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2019/oneill19_interspeech.html": {
    "title": "The Effect of Phoneme Distribution on Perceptual Similarity in English",
    "volume": "main",
    "abstract": "This paper investigates the extent to which native speaker perceptions regarding the similarity between phonemes of English are influenced by their distributional properties. A similarity hierarchy model based on the distribution of consonantal phonemes in the English language was generated by creating phoneme-embeddings from contextual information. We compare this to similarity models based on phonological feature theory and on native speaker perception. Characteristics of the perception-based model are shown to appear in the distribution-based model whilst not being captured by the feature-based model. This not only provides evidence of similarity perceptions being influenced by distributional properties but is an argument for incorporating distributional information alongside phonological features when modelling perceptual similarity",
    "checked": true,
    "id": "ea0f373e9b4a0e7bc67a2525d26ae2e89fc3ded3",
    "semantic_title": "the effect of phoneme distribution on perceptual similarity in english",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kakouros19_interspeech.html": {
    "title": "Prosodic Representations of Prominence Classification Neural Networks and Autoencoders Using Bottleneck Features",
    "volume": "main",
    "abstract": "Prominence perception has been known to correlate with a complex interplay of the acoustic features of energy, fundamental frequency, spectral tilt, and duration. The contribution and importance of each of these features in distinguishing between prominent and non-prominent units in speech is not always easy to determine, and more so, the prosodic representations that humans and automatic classifiers learn have been difficult to interpret. This work focuses on examining the acoustic prosodic representations that binary prominence classification neural networks and autoencoders learn for prominence. We investigate the complex features learned at different layers of the network as well as the 10-dimensional bottleneck features (BNFs), for the standard acoustic prosodic correlates of prominence separately and in combination. We analyze and visualize the BNFs obtained from the prominence classification neural networks as well as their network activations. The experiments are conducted on a corpus of Dutch continuous speech with manually annotated prominence labels. Our results show that the prosodic representations obtained from the BNFs and higher-dimensional non-BNFs provide good separation of the two prominence categories, with, however, different partitioning of the BNF space for the distinct features, and the best overall separation obtained for F0",
    "checked": true,
    "id": "9099691d93d630c23575cd2725d3d33e50c1ee42",
    "semantic_title": "prosodic representations of prominence classification neural networks and autoencoders using bottleneck features",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/peperkamp19_interspeech.html": {
    "title": "Compensation for French Liquid Deletion During Auditory Sentence Processing",
    "volume": "main",
    "abstract": "Phonological rules change the surface realization of words. Listeners undo these changes in order to retrieve the canonical word form. We investigate this so-called compensation for a French deletion rule, i.e. liquid deletion. This rule optionally deletes the final consonant of a word-final obstruent-liquid cluster. It can apply both before consonants and before vowels, but its application is about twice as frequent before consonants. Using a word detection task, we find an overall relatively low rate of compensation, which we argue is due to the relatively high perceptual salience of the rule. We also observe a clear effect of context, though: listeners compensate more than twice as often for a deleted liquid before a consonant than before a vowel. This is evidence that compensation involves fine-grained knowledge about the probability of the rule's application in different contexts",
    "checked": true,
    "id": "5cdb5182871bb33e1b785c5e580eb906d5515163",
    "semantic_title": "compensation for french liquid deletion during auditory sentence processing",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kocharov19_interspeech.html": {
    "title": "Prosodic Factors Influencing Vowel Reduction in Russian",
    "volume": "main",
    "abstract": "Unstressed vowels in Russian are reduced in both duration and quality, but these two manifestations of vowel reduction do not have to be observed simultaneously. In order to investigate this question, we analysed the reduction pattern of words in such contexts where lengthening is induced by prosodic factors: prominence and pre-boundary lengthening. The study is based on a large corpus of read speech. The following results were obtained: (1) as expected, both contexts increase vowel duration; (2) under prosodic prominence vowels undergo less qualitative reduction, while pre-boundary lengthening has no effect on qualitative reduction; (3) additionally, it was shown that prominence mainly affects the pretonic part of the word, while pre-boundary lengthening — the post-tonic part. Thus, an increase in vowel duration does not always cause a decrease in qualitative reduction, which may serve as evidence against the idea that qualitative reduction is caused by quantitative reduction. Additionally, these results may serve as an argument for the idea that the two processes — vowel reduction and temporal organization of utterance — are autonomous",
    "checked": true,
    "id": "b69b002748b77b11000faa32b833f644cc3b6baa",
    "semantic_title": "prosodic factors influencing vowel reduction in russian",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gobl19_interspeech.html": {
    "title": "Time to Frequency Domain Mapping of the Voice Source: The Influence of Open Quotient and Glottal Skew on the Low End of the Source Spectrum",
    "volume": "main",
    "abstract": "This paper explores the mapping of time and frequency domain aspects of the voice source, focussing on the low end of the source spectrum. It refines and extends an earlier study, where the LF model was used to explore the correspondences between the open quotient (O ), glottal skew (R ) and harmonic levels of the source spectrum, including the H1-H2 measure, widely assumed to reflect differences in O Here we use a different model (the F-model) as it better reflects the effective open quotient and glottal skew in certain conditions. As in the earlier study, a series of glottal pulses were generated, keeping peak glottal flow constant, while systematically varying O and R Results suggest that the effects of R on the low harmonics is considerably less than estimated in the earlier study, and its main impact is on the level of H2 (and consequently H1-H2) when O is relatively high. The conclusion remains that the H1-H2 is not simply a direct reflection of O However, for O values of up to about 0.6, it maps closely to H1-H2: beyond this point, H1-H2 reflects a more complex interaction of open quotient and glottal skew",
    "checked": true,
    "id": "a19d235b005978fafec7766d828e09f72beafc17",
    "semantic_title": "time to frequency domain mapping of the voice source: the influence of open quotient and glottal skew on the low end of the source spectrum",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chodroff19_interspeech.html": {
    "title": "Testing the Distinctiveness of Intonational Tunes: Evidence from Imitative Productions in American English",
    "volume": "main",
    "abstract": "Understanding the structure of intonational variation is a longstanding issue in prosodic research. A given utterance can be realized with countless intonational contours, and while variation in prosodic meaning is also large, listeners nevertheless converge on relatively consistent form-function mappings. While this suggests the existence of abstract intonational representations, it has been unclear how exactly these are defined. The present study examines the validity of a well-defined set of phonological representations for the generation of intonation in the nuclear region of an intonational phrase in American English: namely, the combination of binary pitch accents (H*/L*), phrase accents (H-/L-), and boundary tones (H%/L%) proposed in Pierrehumbert (1980). In an exploratory study, we examined whether speakers maintained the eight-way distinction among intonational contours posited to exist in this representational system. We created eight synthesized contours according to Pierrehumbert (1980) and examined whether listeners generalized these contours to novel productions. Speakers largely distinguished rising from non-rising contours in production, but few other distinctions were maintained. While this does not rule out the existence of additional contours in production, these findings do suggest that the representation of rising and non-rising contours may be privileged and more readily accessible in the intonational grammar",
    "checked": true,
    "id": "51fad0610ef0a42738f75d11be2af8313fee3afe",
    "semantic_title": "testing the distinctiveness of intonational tunes: evidence from imitative productions in american english",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/park19d_interspeech.html": {
    "title": "A Study of a Cross-Language Perception Based on Cortical Analysis Using Biomimetic STRFs",
    "volume": "main",
    "abstract": "For those in the early stage of learning a foreign language, they commonly experience difficulties in understanding spoken words in the second language, while they have no problem in recognizing words spoken in their mother tongue. This paper examines this phenomenon using biomimetic receptive fields that can be interpreted as a transfer function between acoustic stimulus and cortical responses in the brain. While receptive fields of individual subjects are often optimized to recognize unique phonemes in their mother language, it is unclear whether challenges associated with acquiring a new language (especially in adulthood) is due to a mismatch between phonemic characteristics in the new language and optimized processing in the system. We explore this question by contrasting biomimetic systems optimized for four different languages with sufficiently different characteristics. We perform English phoneme classification with these language-optimized systems. We observed distinctive characteristics in receptive fields emerging from each language, and the differences of English phoneme recognition performance accordingly",
    "checked": true,
    "id": "90e41f373a7b203267fa3f34896c07ce7a58af0d",
    "semantic_title": "a study of a cross-language perception based on cortical analysis using biomimetic strfs",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sturm19_interspeech.html": {
    "title": "Perceptual Evaluation of Early versus Late F0 Peaks in the Intonation Structure of Czech Question-Word Questions",
    "volume": "main",
    "abstract": "Question-word questions in Czech lexically mark their interrogative function in the initial position: in their standard form, they begin with an interrogative lexeme. For many linguists, this is a sufficient reason for resigning on intonation marking, so they claim that the speech melody in these questions is identical to the melody of statements. A careful observation of the current Czech speech suggests otherwise This paper presents a perceptual experiment in which Czech speakers evaluated two contrastive forms of the interrogative melody, specifically the one with a late peak modelled after statements (as suggested by some authors), and the one with an early peak modelled after our empirical data collected previously. Thirty-two listeners expressed a statistically significant preference for the early peak in a perception test. This outcome resonates with the sample of speech production of the questions. However, the late peak is also possible and acceptable: we assume that it might be a signal of contrastive emphasis or an implicational cue",
    "checked": true,
    "id": "6ad86ae559e4adddc005001d4738edd4a1af6ed9",
    "semantic_title": "perceptual evaluation of early versus late f0 peaks in the intonation structure of czech question-word questions",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kelterer19_interspeech.html": {
    "title": "Acoustic Correlates of Phonation Type in Chichimec",
    "volume": "main",
    "abstract": "Chichimec is an Oto-Manguean language of Mexico with a phonological contrast between modal, breathy and creaky vowels. This study is the first acoustic investigation of this contrast in Chichimec, based on spectral tilt and Cepstral Peak Prominence (CPP) measures. We consider the change of these measures over the course of the vowel and include a high vowel, which was omitted in most phonation studies of other languages. The present study not only contributes to the description of Chichimec with respect to the different portions of the vowel, but also explores the adequacy of the acoustic measures of phonation type for low and high vowels Our results show that phonation changes in the course of the vowel, and that this change is a relevant factor for phonation types in Chichimec. We find that CPP is the best measure to characterize Chichimec phonation contrasts in all vowels. For the vowel /a/, spectral tilt measures are better indicators of phonation type for women than for men. The results for /i/ indicate that spectral tilt distinguishes breathy from modal vowels for men, but that these measures might generally not be appropriate to describe phonation contrasts in women's high vowels",
    "checked": true,
    "id": "ff8bef401b7d23286da6851407d1c1f736f87e88",
    "semantic_title": "acoustic correlates of phonation type in chichimec",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chien19c_interspeech.html": {
    "title": "F0 Variability Measures Based on Glottal Closure Instants",
    "volume": "main",
    "abstract": "The periodicity of a voiced-sound signal can reflect physiological conditions such as identity, age, and voice disorder. One way to look into this periodicity is to measure the temporal variability of vocal fundamental frequency (F0). This paper proposes 2 measures of F0 variability based on glottal closure instant (GCI). GCI is essential to the detection of F0 when the signal waveform varies substantially between adjacent cycles, e.g., in breathy voice. Frequency-selective variability measurements are taken from an interpolated sequence of fundamental-period values based on GCIs, including certain spectral-shape parameters which constitute a multi-variate measure. The utility of these measures was demonstrated in two experiments designed for inter- and intra-speaker comparisons, respectively",
    "checked": true,
    "id": "0fea803b7937fc4c331fc68e15e239367b0eb498",
    "semantic_title": "f0 variability measures based on glottal closure instants",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tavi19_interspeech.html": {
    "title": "Recognition of Creaky Voice from Emergency Calls",
    "volume": "main",
    "abstract": "Although creaky voice, or vocal fry, is widely studied phonation mode, open questions still exist in creak's acoustic characterization and automatic recognition. Many questions are open since creak varies significantly depending on conversational context. In this study, we introduce an exploratory creak recognizer based on convolutional neural network (CNN), which is generated specifically for emergency calls. The study focuses on recognition of creaky voice from authentic emergency calls because creak detection could potentially provide information about the caller's emotional state or attempt of voice disguise. We generated the CNN recognition system using emergency call recordings and other out-of-domain speech recordings and compared the results with an already existing and widely used creaky voice detection system: using poor quality emergency call recordings as test data, this system achieved F1 of 0.41 whereas our CNN system accomplished an F1 of 0.64. The results show that the CNN system can perform moderately well using a limited amount of training data on challenging testing data and has the potential to achieve higher F scores when more emergency calls are used for model training",
    "checked": true,
    "id": "6b3ed4c4a1c35d6b4c33b2aad5051900cc68e42b",
    "semantic_title": "recognition of creaky voice from emergency calls",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xu19b_interspeech.html": {
    "title": "Direct F0 Estimation with Neural-Network-Based Regression",
    "volume": "main",
    "abstract": "Pitch tracking, or the continuous extraction of fundamental frequency from speech waveforms, is of vital importance to many applications in speech analysis and synthesis. Many existing trackers, including conventional ones such as Praat, RAPT and YIN, and newly proposed neural-network-based ones such as DNN-CLS, CREPE and RNN-REG, have conducted an extensive investigation into speech pitch tracking. This work developed a different end-to-end regression model based on neural networks, where a voice detector and a newly proposed value estimator work jointly to highlight the trajectory of fundamental frequency. Experiments on the PTDB-TUG corpus showed that the system surpasses canonical neural networks in terms of gross error rate. It further outperformed conventional trackers under clean condition and neural-network classifiers under noisy condition by the NOISEX-92 corpus",
    "checked": true,
    "id": "8467f264c2b3cda6de7da2371e6bab5e9eee366b",
    "semantic_title": "direct f0 estimation with neural-network-based regression",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sharma19b_interspeech.html": {
    "title": "Real Time Online Visual End Point Detection Using Unidirectional LSTM",
    "volume": "main",
    "abstract": "Visual Voice Activity Detection (V-VAD) involves the detection of speech activity of a speaker using visual features. The V-VAD is useful in detecting the end point of an utterance under noisy acoustic conditions or for maintaining speaker privacy. In this paper, we propose a speaker independent, real-time solution for V-VAD. The focus is on real-time aspect and accuracy as such algorithms will play a key role in detecting end point especially while interacting with speech assistants. We propose two novel methods one using CNN and the other using 2D-DCT features. Unidirectional LSTMs are used in both the methods to make it online and learn temporal dependence. The methods are tested on two publicly available datasets. Additionally the methods are also tested on a locally collected dataset which further validates our hypothesis. Additionally it has been observed through experiments that both the approaches generalize to unseen speakers. It has been shown that our best approach gives substantial improvement over earlier methods done on the same dataset",
    "checked": true,
    "id": "58fce6a18b888d96a41354bfc55b75f7f312ae21",
    "semantic_title": "real time online visual end point detection using unidirectional lstm",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ardaillon19_interspeech.html": {
    "title": "Fully-Convolutional Network for Pitch Estimation of Speech Signals",
    "volume": "main",
    "abstract": "The estimation of fundamental frequency (F ) from audio is a necessary step in many speech processing tasks such as speech synthesis, that require to accurately analyze big datasets, or real-time voice transformations, that require low computation times. New approaches using neural networks have been recently proposed for F estimation, outperforming previous approaches in terms of accuracy. The work presented here aims at bringing some more improvements over such CNN-based state-of-the-art approaches, especially when targeting speech data. More specifically, we first propose to use the recent PaN speech synthesis engine in order to generate a high-quality speech database with a reliable ground truth F annotation. Then, we propose 3 variants of a new fully-convolutional network (FCN) architecture that are shown to perform better than other similar data-driven methods, with a significantly reduced computational load making them more suitable for real-time purposes",
    "checked": true,
    "id": "fd350e10baac1e51f730d548c037e3d02b5ed938",
    "semantic_title": "fully-convolutional network for pitch estimation of speech signals",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dong19_interspeech.html": {
    "title": "Vocal Pitch Extraction in Polyphonic Music Using Convolutional Residual Network",
    "volume": "main",
    "abstract": "Pitch extraction, also known as fundamental frequency estimation, is a long-term task in audio signal processing. Especially, due to the presence of accompaniment, vocal pitch extraction in polyphonic music is more challenging. So far, most of deep learning approaches use log mel spectrogram as input, which neglect the phase information. In addition, shallow networks have been applied on waveform directly, which may not handle contaminated vocal data well. In this paper, a deep convolutional residual network is proposed. It analyzes and extracts effective feature from waveform automatically. Residual learning can reduce model degradation due to the skip connection and residual mapping. In comparison to reported results, the proposed approach shows 5% and 4% improvement on overall accuracy(OA) and raw pitch accuracy(RPA) respectively",
    "checked": true,
    "id": "53823d31235e6e22fb4e8dd9997c182232e99714",
    "semantic_title": "vocal pitch extraction in polyphonic music using convolutional residual network",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sharma19c_interspeech.html": {
    "title": "Multi-Level Adaptive Speech Activity Detector for Speech in Naturalistic Environments",
    "volume": "main",
    "abstract": "Speech activity detection (SAD) is a part of many speech processing applications. The traditional SAD approaches use signal energy as the evidence to identify the speech regions. However, such methods perform poorly under uncontrolled environments. In this work, we propose a novel SAD approach using a multi-level decision with signal knowledge in an adaptive manner. The multi-level evidence considered are modulation spectrum and smoothed Hilbert envelope of linear prediction (LP) residual. Modulation spectrum has compelling parallels to the dynamics of speech production and captures information only for the speech component. Contrarily, Hilbert envelope of LP residual captures excitation source aspect of speech. Under uncontrolled scenario, these evidence are found to be robust towards the signal distortions and thus expected to work well. In view of different levels of interference present in the signal, we propose to use a quality factor to control the speech/non-speech decision in an adaptive manner. We refer this method as multi-level adaptive SAD and evaluate on Fearless Steps corpus that is collected during Apollo-11 Mission in naturalistic environments. We achieve a detection cost function of 7.35% with the proposed multi-level adaptive SAD on the evaluation set of Fearless Steps 2019 challenge corpus",
    "checked": true,
    "id": "359c9784d1eaea21f32f4208b106c8ec1a07e603",
    "semantic_title": "multi-level adaptive speech activity detector for speech in naturalistic environments",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sharma19d_interspeech.html": {
    "title": "On the Importance of Audio-Source Separation for Singer Identification in Polyphonic Music",
    "volume": "main",
    "abstract": "Singer identification is to automatically identify the singer in a music recording, such as a polyphonic song. A song has two major acoustic components that are singing vocals and background accompaniment. Although identifying singers is similar to speaker identification, it is challenging due to the interference of background accompaniment on the singer-specific information in singing vocals. We believe that separating the background accompaniment from the singing vocal will help us to overcome the interference. In this work, we extract the singing vocals from polyphonic songs using Wave-U-Net based audio-source separation approach. The extracted singing vocals are then used in i-vector based singer identification system. Further, we explore different state-of-the-art audio-source separation methods to establish the role of considered method in application to singer identification. The proposed singer identification framework achieves an absolute accuracy improvement of 5.66% over the baseline without audio-source separation",
    "checked": true,
    "id": "05385b7f05b28b4fca0df6e4c6ae5281c92447e0",
    "semantic_title": "on the importance of audio-source separation for singer identification in polyphonic music",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/terasawa19_interspeech.html": {
    "title": "Investigating the Physiological and Acoustic Contrasts Between Choral and Operatic Singing",
    "volume": "main",
    "abstract": "In this study, the difference in glottal vibration and timbre of singing voice in choral and operatic singing was investigated. Eight professional singers with active careers in operatic and choral performances participated in the experiment and sang excerpts from three operatic songs and two choral songs. Audio and electroglottograph signals were simultaneously recorded. The open quotient (O ) and singing power ratio (SPR) of the voices were analyzed, and it was found that the O of choral singing tends to be higher and the SPR of choral singing tends to be lower than those of operatic singing. This suggests that choral singing is conducted with laxer vocal fold coordination, and it has less ringing timbre than operatic singing. However, the O and SPR were not directly correlated: the degree of adjustment of SPR differed across singers, suggesting that the strategy to achieve a desired voice quality is individualistic in nature",
    "checked": true,
    "id": "2f86a382d9ca02cff6c4e01ad6ba424668a65f2e",
    "semantic_title": "investigating the physiological and acoustic contrasts between choral and operatic singing",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lin19c_interspeech.html": {
    "title": "Optimizing Voice Activity Detection for Noisy Conditions",
    "volume": "main",
    "abstract": "In this work, we focus our attention on how to improve Voice Activity Detection (VAD) in noisy conditions. We propose a Convolutional Neural Network (CNN) based model, as well as a Denoising Autoencoder (DAE), and experiment against acoustic features and their delta features in noise levels ranging from SNR 35 dB to 0 dB. The experiments compare and find the best model configuration for robust performance in noisy conditions. We observe that combining more expressive audio features with the use of DAEs improve accuracy, especially as noise increases. At 0 dB, the proposed model trained with the best feature set could achieve a lab test accuracy of 93.2% (averaged across all noise levels) and 88.6% inference accuracy on device. We also compress the neural network and deploy the inference model that is optimized for the app so that the average on-device CPU usage is reduced to 14% from 37%",
    "checked": true,
    "id": "1c36b20a2c11eb7382c766fe9574010c195ca2f1",
    "semantic_title": "optimizing voice activity detection for noisy conditions",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yamamoto19c_interspeech.html": {
    "title": "Small-Footprint Magic Word Detection Method Using Convolutional LSTM Neural Network",
    "volume": "main",
    "abstract": "The number of consumer devices which can be operated by voice is increasing every year. Magic Word Detection (MWD), the detection of an activation keyword in continuous speech, has become an essential technology for the hands-free operation of such devices. Because MWD systems need to run constantly in order to detect Magic Words at any time, many studies have focused on the development of a small-footprint system. In this paper, we propose a novel, small-footprint MWD method which uses a convolutional Long Short-Term Memory (LSTM) neural network to capture frequency and time domain features over time. As a result, the proposed method outperforms the baseline method while reducing the number of parameters by more than 80%. An experiment on a small-scale device demonstrates that our model is efficient enough to function in real time",
    "checked": true,
    "id": "dc3e777f8c13d6aaa750bc368c326aac0602d1f0",
    "semantic_title": "small-footprint magic word detection method using convolutional lstm neural network",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gupta19b_interspeech.html": {
    "title": "Acoustic Modeling for Automatic Lyrics-to-Audio Alignment",
    "volume": "main",
    "abstract": "Automatic lyrics to polyphonic audio alignment is a challenging task not only because the vocals are corrupted by background music, but also there is a lack of annotated polyphonic corpus for effective acoustic modeling. In this work, we propose (1) using additional speech and music-informed features and (2) adapting the acoustic models trained on a large amount of solo singing vocals towards polyphonic music using a small amount of in-domain data. Incorporating additional information such as voicing and auditory features together with conventional acoustic features aims to bring robustness against the increased spectro-temporal variations in singing vocals. By adapting the acoustic model using a small amount of polyphonic audio data, we reduce the domain mismatch between training and testing data. We perform several alignment experiments and present an in-depth alignment error analysis on acoustic features, and model adaptation techniques. The results demonstrate that the proposed strategy provides a significant error reduction of word boundary alignment over comparable existing systems, especially on more challenging polyphonic data with long-duration musical interludes",
    "checked": true,
    "id": "60469bda847b93abe548f85e3ea4a52d8ef0c70b",
    "semantic_title": "acoustic modeling for automatic lyrics-to-audio alignment",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vafeiadis19_interspeech.html": {
    "title": "Two-Dimensional Convolutional Recurrent Neural Networks for Speech Activity Detection",
    "volume": "main",
    "abstract": "Speech Activity Detection (SAD) plays an important role in mobile communications and automatic speech recognition (ASR). Developing efficient SAD systems for real-world applications is a challenging task due to the presence of noise. We propose a new approach to SAD where we treat it as a two-dimensional multilabel image classification problem. To classify the audio segments, we compute their Short-time Fourier Transform spectrograms and classify them with a Convolutional Recurrent Neural Network (CRNN), traditionally used in image recognition. Our CRNN uses a sigmoid activation function, max-pooling in the frequency domain, and a convolutional operation as a moving average filter to remove misclassified spikes. On the development set of Task 1 of the 2019 Fearless Steps Challenge, our system achieved a decision cost function (DCF) of 2.89%, a 66.4% improvement over the baseline. Moreover, it achieved a DCF score of 3.318% on the evaluation dataset of the challenge, ranking first among all submissions",
    "checked": true,
    "id": "e0f90ca1aa83bc5eadfbcc33830811ebe3fef871",
    "semantic_title": "two-dimensional convolutional recurrent neural networks for speech activity detection",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kaburagi19_interspeech.html": {
    "title": "A Study of Soprano Singing in Light of the Source-Filter Interaction",
    "volume": "main",
    "abstract": "We examined the physical interaction between the voice source system in the larynx and the acoustic filter of the vocal tract. The vocal tract of a soprano was first scanned in three dimensions using magnetic resonance imaging (MRI) while she produced four musical notes with different vowels. These images were used to simulate voice production, including the vibratory motion of the vocal folds and the behavior of glottal airflow. Images for the /i/ vowel were used in the simulation, because a good proximity relationship was found between the fundamental frequency and the first impedance peak of the vocal tract. The simulation results revealed that the fundamental frequency (vibration frequency of the vocal folds) was decreased to a large extent by the source-filter interaction especially when their natural frequency was in the proximity of the impedance peak. In a specific case, this frequency lowering had the effect of changing the acoustic load of the vocal tract exerted on the vocal folds so that their vibratory motion was effectively assisted",
    "checked": true,
    "id": "eb55231871f61f36000344b6a2d88936f26fa52e",
    "semantic_title": "a study of soprano singing in light of the source-filter interaction",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zou19_interspeech.html": {
    "title": "Boosting Character-Based Chinese Speech Synthesis via Multi-Task Learning and Dictionary Tutoring",
    "volume": "main",
    "abstract": "Recent character-based end-to-end text-to-speech (TTS) systems have shown promising performance in natural speech generation, especially for English. However, for Chinese TTS, the character-based model is easy to generate speech with wrong pronunciation due to the label sparsity issue. To address this issue, we introduce an additional learning task of character-to-pinyin mapping to boost the pronunciation learning of characters, and leverage a pre-trained dictionary network to correct the pronunciation mistake through joint training. Specifically, our model predicts pinyin labels as an auxiliary task to assist learning better hidden representations of Chinese characters, where pinyin is a standard phonetic representation for Chinese characters. The dictionary network plays a role as a tutor to further help hidden representation learning. Experiments demonstrate that employing the pinyin auxiliary task and an external dictionary network clearly enhances the naturalness and intelligibility of the synthetic speech directly from the Chinese character sequences",
    "checked": true,
    "id": "c82047fcb6f757e3daad1fdacb79553fc706c2c3",
    "semantic_title": "boosting character-based chinese speech synthesis via multi-task learning and dictionary tutoring",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xue19_interspeech.html": {
    "title": "Building a Mixed-Lingual Neural TTS System with Only Monolingual Data",
    "volume": "main",
    "abstract": "When deploying a Chinese neural Text-to-Speech (TTS) system, one of the challenges is to synthesize Chinese utterances with English phrases or words embedded. This paper looks into the problem in the encoder-decoder framework when only monolingual data from a target speaker is available. Specifically, we view the problem from two aspects: speaker consistency within an utterance and naturalness. We start the investigation with an average voice model which is built from multi-speaker monolingual data, i.e., Mandarin and English data. On the basis of that, we look into speaker embedding for speaker consistency within an utterance and phoneme embedding for naturalness and intelligibility, and study the choice of data for model training. We report the findings and discuss the challenges to build a mixed-lingual TTS system with only monolingual data",
    "checked": true,
    "id": "9db5a3d62bf97898ebfae3e5acd946710ad66c65",
    "semantic_title": "building a mixed-lingual neural tts system with only monolingual data",
    "citation_count": 30
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sokolov19_interspeech.html": {
    "title": "Neural Machine Translation for Multilingual Grapheme-to-Phoneme Conversion",
    "volume": "main",
    "abstract": "Grapheme-to-phoneme (G2P) models are a key component in Automatic Speech Recognition (ASR) systems, such as the ASR system in Alexa, as they are used to generate pronunciations for out-of-vocabulary words that do not exist in the pronunciation lexicons (mappings like \"e c h o\" → \"E k oU\") Most G2P systems are monolingual and based on traditional joint-sequence based n-gram models [1, 2]. As an alternative, we present a single end-to-end trained neural G2P model that shares same encoder and decoder across multiple languages. This allows the model to utilize a combination of universal symbol inventories of Latin-like alphabets and cross-linguistically shared feature representations. Such model is especially useful in the scenarios of low resource languages and code switching/ foreign words, where the pronunciations in one language need to be adapted to other locales or accents. We further experiment with word language distribution vector as an additional training target in order to improve system performance by helping the model decouple pronunciations across a variety of languages in the parameter space. We show 7.2% average improvement in phoneme error rate over low resource languages and no degradation over high resource ones compared to monolingual baselines",
    "checked": true,
    "id": "6821e76c20efc7f4d45434dcd92a142a2baab295",
    "semantic_title": "neural machine translation for multilingual grapheme-to-phoneme conversion",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/taylor19_interspeech.html": {
    "title": "Analysis of Pronunciation Learning in End-to-End Speech Synthesis",
    "volume": "main",
    "abstract": "Ensuring correct pronunciation for the widest possible variety of text input is vital for deployed text-to-speech (TTS) systems. For languages such as English that do not have trivial spelling, systems have always relied heavily upon a lexicon, both for pronunciation lookup and for training letter-to-sound (LTS) models as a fall-back to handle out-of-vocabulary words (OOVs). In contrast, recently proposed models that are trained \"end-to-end\" (E2E) aim to avoid linguistic text analysis and any explicit phone representation, instead learning pronunciation implicitly as part of a direct mapping from input characters to speech audio. This might be termed implicit LTS. In this paper, we explore the nature of this approach by training explicit LTS models with datasets commonly used to build E2E systems. We compare their performance with LTS models trained on a high quality English lexicon. We find that LTS errors for words with ambiguous or unpredictable pronunciations are mirrored as mispronunciations by an E2E model. Overall, our analysis suggests that limited and unbalanced lexical coverage in E2E training data may pose significant confounding factors that complicate learning accurate pronunciations in a purely E2E system",
    "checked": true,
    "id": "c9e17537d8b7d1d3fc19d95a77e97f3db2175248",
    "semantic_title": "analysis of pronunciation learning in end-to-end speech synthesis",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19f_interspeech.html": {
    "title": "End-to-End Text-to-Speech for Low-Resource Languages by Cross-Lingual Transfer Learning",
    "volume": "main",
    "abstract": "End-to-end text-to-speech (TTS) has shown great success on large quantities of paired text plus speech data. However, laborious data collection remains difficult for at least 95% of the languages over the world, which hinders the development of TTS in different languages. In this paper, we aim to build TTS systems for such low-resource (target) languages where only very limited paired data are available. We show such TTS can be effectively constructed by transferring knowledge from a high-resource (source) language. Since the model trained on source language cannot be directly applied to target language due to input space mismatch, we propose a method to learn a mapping between source and target linguistic symbols. Benefiting from this learned mapping, pronunciation information can be preserved throughout the transferring procedure. Preliminary experiments show that we only need around 15 minutes of paired data to obtain a relatively good TTS system. Furthermore, analytic studies demonstrated that the automatically discovered mapping correlate well with the phonetic expertise",
    "checked": true,
    "id": "ae2e7e0132019fb3302bab08e8e7248672440cb4",
    "semantic_title": "end-to-end text-to-speech for low-resource languages by cross-lingual transfer learning",
    "citation_count": 67
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19e_interspeech.html": {
    "title": "Learning to Speak Fluently in a Foreign Language: Multilingual Speech Synthesis and Cross-Language Voice Cloning",
    "volume": "main",
    "abstract": "We present a multispeaker, multilingual text-to-speech (TTS) synthesis model based on Tacotron that is able to produce high quality speech in multiple languages. Moreover, the model is able to transfer voices across languages, e.g. synthesize fluent Spanish speech using an English speaker's voice, without training on any bilingual or parallel examples. Such transfer works across distantly related languages, e.g. English and Mandarin Critical to achieving this result are: 1. using a phonemic input representation to encourage sharing of model capacity across languages, and 2. incorporating an adversarial loss term to encourage the model to disentangle its representation of speaker identity (which is perfectly correlated with language in the training data) from the speech content. Further scaling up the model by training on multiple speakers of each language, and incorporating an autoencoding input to help stabilize attention during training, results in a model which can be used to consistently synthesize intelligible speech for training speakers in all languages seen during training, and in native or foreign accents",
    "checked": true,
    "id": "4b45ec780aed51ed3847c6167cab9c4ece1caeec",
    "semantic_title": "learning to speak fluently in a foreign language: multilingual speech synthesis and cross-language voice cloning",
    "citation_count": 138
  },
  "https://www.isca-speech.org/archive/interspeech_2019/juzova19_interspeech.html": {
    "title": "Unified Language-Independent DNN-Based G2P Converter",
    "volume": "main",
    "abstract": "We introduce a unified Grapheme-to-phoneme conversion framework based on the composition of deep neural networks. In contrary to the usual approaches building the G2P frameworks from the dictionary, we use whole phrases, which allows us to capture various language properties, e.g. cross-word assimilation, without the need for any special care or topology adjustments. The evaluation is carried out on three different languages — English, Czech and Russian. Each requires dealing with specific properties, stressing the proposed framework in various ways. The very first results show promising performance of the proposed framework, dealing with all the phenomena specific to the tested languages. Thus, we consider the framework to be language-independent for a wide range of languages",
    "checked": true,
    "id": "d5135a81173b95c5c1b2aa20bf6abd1972e682b2",
    "semantic_title": "unified language-independent dnn-based g2p converter",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dai19_interspeech.html": {
    "title": "Disambiguation of Chinese Polyphones in an End-to-End Framework with Semantic Features Extracted by Pre-Trained BERT",
    "volume": "main",
    "abstract": "Grapheme-to-phoneme (G2P) conversion serves as an essential component in Chinese Mandarin text-to-speech (TTS) system, where polyphone disambiguation is the core issue. In this paper, we propose an end-to-end framework to predict the pronunciation of polyphonic character, which accepts sentence containing polyphonic character as input in the form of Chinese character sequence without the necessity of any preprocessing. The proposed method consists of a pre-trained bidirectional encoder representations from Transformers (BERT) model and a neural network (NN) based classifier. The pre-trained BERT model extracts semantic features from raw Chinese character sequence and the NN based classifier predicts the polyphonic character's pronunciation according to BERT output. To explore the impact of contextual information on polyphone disambiguation, three different classifiers are investigated: a fully-connected network based classifier, a long short-term memory (LSTM) network based classifier and a Transformer block based classifier. Experimental results demonstrate the effectiveness of the proposed end-to-end framework for polyphone disambiguation and the semantic features extracted by BERT can greatly enhance the performance",
    "checked": true,
    "id": "28e9825fd3171f9d50bad544e160f675d0cdcaa5",
    "semantic_title": "disambiguation of chinese polyphones in an end-to-end framework with semantic features extracted by pre-trained bert",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yolchuyeva19_interspeech.html": {
    "title": "Transformer Based Grapheme-to-Phoneme Conversion",
    "volume": "main",
    "abstract": "Attention mechanism is one of the most successful techniques in deep learning based Natural Language Processing (NLP). The transformer network architecture is completely based on attention mechanisms, and it outperforms sequence-to-sequence models in neural machine translation without recurrent and convolutional layers. Grapheme-to-phoneme (G2P) conversion is a task of converting letters (grapheme sequence) to their pronunciations (phoneme sequence). It plays a significant role in text-to-speech (TTS) and automatic speech recognition (ASR) systems. In this paper, we investigate the application of transformer architecture to G2P conversion and compare its performance with recurrent and convolutional neural network based approaches. Phoneme and word error rates are evaluated on the CMUDict dataset for US English and the NetTalk dataset. The results show that transformer based G2P outperforms the convolutional-based approach in terms of word error rate and our results significantly exceeded previous recurrent approaches (without attention) regarding word and phoneme error rates on both datasets. Furthermore, the size of the proposed model is much smaller than the size of the previous approaches",
    "checked": true,
    "id": "4cb319f5c1a7208465d32b4beceb82e958634c1e",
    "semantic_title": "transformer based grapheme-to-phoneme conversion",
    "citation_count": 41
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bleyan19_interspeech.html": {
    "title": "Developing Pronunciation Models in New Languages Faster by Exploiting Common Grapheme-to-Phoneme Correspondences Across Languages",
    "volume": "main",
    "abstract": "We discuss two methods that let us easily create grapheme-to-phoneme (G2P) conversion systems for languages without any human-curated pronunciation lexicons, as long as we know the phoneme inventory of the target language and as long as we have some pronunciation lexicons for other languages written in the same script. We use these resources to infer what grapheme-to-phoneme correspondences we would expect, and predict pronunciations for words in the target language with minimal or no language-specific human work. Our first approach uses finite-state transducers, while our second approach uses a sequence-to-sequence neural network. Our G2P models reach high degrees of accuracy, and can be used for various applications, e.g. in developing an automatic speech recognition system. Our methods greatly simplify a task that has historically required extensive manual labor",
    "checked": true,
    "id": "c72131f83a11af2fec96eb3343d7eed2a7954fbe",
    "semantic_title": "developing pronunciation models in new languages faster by exploiting common grapheme-to-phoneme correspondences across languages",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19g_interspeech.html": {
    "title": "Cross-Lingual, Multi-Speaker Text-To-Speech Synthesis Using Neural Speaker Embedding",
    "volume": "main",
    "abstract": "Neural network-based model for text-to-speech (TTS) synthesis has made significant progress in recent years. In this paper, we present a cross-lingual, multi-speaker neural end-to-end TTS framework which can model speaker characteristics and synthesize speech in different languages. We implement the model by introducing a separately trained neural speaker embedding network, which can represent the latent structure of different speakers and language pronunciations. We train the speech synthesis network bilingually and prove the possibility of synthesizing Chinese speaker's English speech and vice versa. We explore different methods to fit a new speaker using only a few speech samples. The experimental results show that, with only several minutes of audio from a new speaker, the proposed model can synthesize speech bilingually and acquire decent naturalness and similarity for both languages",
    "checked": true,
    "id": "f1705f684a9ad65a7cc7455b9e54c2a69f193ffa",
    "semantic_title": "cross-lingual, multi-speaker text-to-speech synthesis using neural speaker embedding",
    "citation_count": 42
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cai19b_interspeech.html": {
    "title": "Polyphone Disambiguation for Mandarin Chinese Using Conditional Neural Network with Multi-Level Embedding Features",
    "volume": "main",
    "abstract": "This paper describes a conditional neural network architecture for Mandarin Chinese polyphone disambiguation. The system is composed of a bidirectional recurrent neural network component acting as a sentence encoder to accumulate the context correlations, followed by a prediction network that maps the polyphonic character embeddings along with the conditions to corresponding pronunciations. We obtain the word-level condition from a pre-trained word-to-vector lookup table. One goal of polyphone disambiguation is to address the homograph problem existing in the front-end processing of Mandarin Chinese text-to-speech system. Our system achieves an accuracy of 94.69% on a publicly available polyphonic character dataset. To further validate our choices on the conditional feature, we investigate polyphone disambiguation systems with multi-level conditions respectively. The experimental results show that both the sentence-level and the word-level conditional embedding features are able to attain good performance for Mandarin Chinese polyphone disambiguation",
    "checked": true,
    "id": "a7cc5347c4bb84a59b3c8df57657749682c7928d",
    "semantic_title": "polyphone disambiguation for mandarin chinese using conditional neural network with multi-level embedding features",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sun19c_interspeech.html": {
    "title": "Token-Level Ensemble Distillation for Grapheme-to-Phoneme Conversion",
    "volume": "main",
    "abstract": "Grapheme-to-phoneme (G2P) conversion is an important task in automatic speech recognition and text-to-speech systems. Recently, G2P conversion is viewed as a sequence to sequence task and modeled by RNN or CNN based encoder-decoder framework. However, previous works do not consider the practical issues when deploying G2P model in the production system, such as how to leverage additional unlabeled data to boost the accuracy, as well as reduce model size for online deployment. In this work, we propose token-level ensemble distillation for G2P conversion, which can (1) boost the accuracy by distilling the knowledge from additional unlabeled data, and (2) reduce the model size but maintain the high accuracy, both of which are very practical and helpful in the online production system. We use token-level knowledge distillation, which results in better accuracy than the sequence-level counterpart. What is more, we adopt the Transformer instead of RNN or CNN based models to further boost the accuracy of G2P conversion. Experiments on the publicly available CMUDict dataset and an internal English dataset demonstrate the effectiveness of our proposed method. Particularly, our method achieves 19.88% WER on CMUDict dataset, outperforming the previous works by more than 4.22% WER, and setting the new state-of-the-art results",
    "checked": true,
    "id": "04265022434f6c0ea2782952830ea344bff5705a",
    "semantic_title": "token-level ensemble distillation for grapheme-to-phoneme conversion",
    "citation_count": 48
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19i_interspeech.html": {
    "title": "Multilingual Speech Recognition with Corpus Relatedness Sampling",
    "volume": "main",
    "abstract": "Multilingual acoustic models have been successfully applied to low-resource speech recognition. Most existing works have combined many small corpora together, and pretrained a multilingual model by sampling from each corpus uniformly. The model is eventually fine-tuned on each target corpus. This approach, however, fails to exploit the relatedness and similarity among corpora in the training set. For example, the target corpus might benefit more from a corpus in the same domain or a corpus from a close language. In this work, we propose a simple but useful sampling strategy to take advantage of this relatedness. We first compute the corpus-level embeddings and estimate the similarity between each corpus. Next we start training the multilingual model with uniform-sampling from each corpus at first, then we gradually increase the probability to sample from related corpora based on its similarity with the target corpus. Finally the model would be fine-tuned automatically on the target corpus. Our sampling strategy outperforms the baseline multilingual model on 16 low-resource tasks. Additionally, we demonstrate that our corpus embeddings capture the language and domain information of each corpus",
    "checked": true,
    "id": "9e39c4bf71af7b551e3bf747f20e701b77a72f37",
    "semantic_title": "multilingual speech recognition with corpus relatedness sampling",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2019/arsikere19_interspeech.html": {
    "title": "Multi-Dialect Acoustic Modeling Using Phone Mapping and Online i-Vectors",
    "volume": "main",
    "abstract": "This paper proposes a simple phone mapping approach to multi-dialect acoustic modeling. In contrast to the widely used shared hidden layer (SHL) training approach (hidden layers are shared across dialects whereas output layers are kept separate), phone mapping simplifies model training and maintenance by allowing all the network parameters to be shared; it also simplifies online adaptation via HMM-based i-vectors by allowing the same T-matrix to be used for all the dialects. Using the LSTM-HMM framework, we compare phone mapping with transfer learning and SHL training, and we also compare the efficacy of online i-vectors with that of one-hot dialect encoding. Experiments with a 2K hour dataset comprising four English dialects show that (1) phone mapping yields significant WER reductions over dialect-specific training (14%, on average) and transfer learning (5%, on average); (2) SHL training is only slightly better than phone mapping; and (3) i-vectors provide useful additional reductions (3%, on average) while one-hot encoding has little effect. Even with a large 40K hour dataset (comprising the same four English dialects) and fully optimized sequence discriminative training, we show that phone mapping provides healthy WER reductions over dialect-specific models (10%, on average)",
    "checked": true,
    "id": "6d879c254c7725005ef2414589a9ad96c372ceaf",
    "semantic_title": "multi-dialect acoustic modeling using phone mapping and online i-vectors",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kannan19_interspeech.html": {
    "title": "Large-Scale Multilingual Speech Recognition with a Streaming End-to-End Model",
    "volume": "main",
    "abstract": "Multilingual end-to-end (E2E) models have shown great promise in expansion of automatic speech recognition (ASR) coverage of the world's languages. They have shown improvement over monolingual systems, and have simplified training and serving by eliminating language-specific acoustic, pronunciation, and language models. This work presents an E2E multilingual system which is equipped to operate in low-latency interactive applications, as well as handle a key challenge of real world data: the imbalance in training data across languages. Using nine Indic languages, we compare a variety of techniques, and find that a combination of conditioning on a language vector and training language-specific adapter layers produces the best model. The resulting E2E multilingual model achieves a lower word error rate (WER) than both monolingual E2E models (eight of nine languages) and monolingual conventional systems (all nine languages)",
    "checked": true,
    "id": "5cf3e46e6d427a87726c18f22def612519176938",
    "semantic_title": "large-scale multilingual speech recognition with a streaming end-to-end model",
    "citation_count": 135
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mendes19_interspeech.html": {
    "title": "Recognition of Latin American Spanish Using Multi-Task Learning",
    "volume": "main",
    "abstract": "In the broadcast news domain, national wide newscasters typically interact with communities with a diverse set of accents. One of the challenges in speech recognition is the performance degradation in the presence of these diverse conditions. Performance further aggravates when the accents are from other countries that share the same language. Extensive work has been conducted in this topic for languages such as English and Mandarin. Recently, TDNN based multi-task learning has received some attention in this area, with interesting results, typically using models trained with a variety of different accented corpora from a particular language. In this work, we look at the case of LATAM (Latin American) Spanish for its unique and distinctive accent variations. Because LATAM Spanish has historically been influenced by non-Spanish European migrations, we anticipated that LATAM based speech recognition performance can be further improved by including these influential languages, during a TDNN based multi-task training. Experiments show that including such languages in the training setup outperforms the single task acoustic model baseline. We also propose an automatic per-language weight selection strategy to regularize each language contribution during multi-task training",
    "checked": true,
    "id": "4cc4c860b11fefabee4d486fb0fa9d8870fbe248",
    "semantic_title": "recognition of latin american spanish using multi-task learning",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/viglino19_interspeech.html": {
    "title": "End-to-End Accented Speech Recognition",
    "volume": "main",
    "abstract": "Correct pronunciation is known to be the most difficult part to acquire for (native or non-native) language learners. The accented speech is thus more variable, and standard Automatic Speech Recognition (ASR) training approaches that rely on intermediate phone alignment might introduce errors during the ASR training. With end-to-end training we could alleviate this problem. In this work, we explore the use of multi-task training and accent embedding in the context of end-to-end ASR trained with the connectionist temporal classification loss. Comparing to the baseline developed using conventional ASR framework exploiting time-delay neural networks trained on accented English, we show significant relative improvement of about 25% in word error rate. Additional evaluation on unseen accent data yields relative improvements of of 31% and 2% for New Zealand English and Indian English, respectively",
    "checked": true,
    "id": "974c3a3ed4a18990fd7f77edea511b22de16811c",
    "semantic_title": "end-to-end accented speech recognition",
    "citation_count": 47
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19j_interspeech.html": {
    "title": "End-to-End Articulatory Attribute Modeling for Low-Resource Multilingual Speech Recognition",
    "volume": "main",
    "abstract": "The end-to-end (E2E) model allows for training of automatic speech recognition (ASR) systems without the hand-designed language-specific pronunciation lexicons. However, constructing the multilingual low-resource E2E ASR system is still challenging due to the vast number of symbols (e.g., words and characters). In this paper, we investigate an efficient method of encoding multilingual transcriptions for training E2E ASR systems. We directly encode the symbols of multilingual writing systems to universal articulatory representations, which is much more compact than characters and words. Compared with traditional multilingual modeling methods, we directly build a single acoustic-articulatory within recent transformer-based E2E framework for ASR tasks. The speech recognition results of our proposed method significantly outperform the conventional word-based and character-based E2E models",
    "checked": true,
    "id": "c4f2490ce85e1c6834e8c4a54227378822efbf66",
    "semantic_title": "end-to-end articulatory attribute modeling for low-resource multilingual speech recognition",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/taneja19_interspeech.html": {
    "title": "Exploiting Monolingual Speech Corpora for Code-Mixed Speech Recognition",
    "volume": "main",
    "abstract": "One of the main challenges in building code-mixed ASR systems is the lack of annotated speech data. Often, however, monolingual speech corpora are available in abundance for the languages in the code-mixed speech. In this paper, we explore different techniques that use monolingual speech to create synthetic code-mixed speech and examine their effect on training models for code-mixed ASR. We assume access to a small amount of real code-mixed text, from which we extract probability distributions that govern the transition of phones across languages at code-switch boundaries and the span lengths corresponding to a particular language. We extract segments from monolingual data and concatenate them to form code-mixed utterances such that these probability distributions are preserved. Using this synthetic speech, we show significant improvements in Hindi-English code-mixed ASR performance compared to using synthetic speech naively constructed from complete utterances in different languages. We also present language modelling experiments that use synthetically constructed code-mixed text and discuss their benefits",
    "checked": true,
    "id": "4642d0e08964976e2fd960c0360f06ff5dc8e3ca",
    "semantic_title": "exploiting monolingual speech corpora for code-mixed speech recognition",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hu19_interspeech.html": {
    "title": "Phoneme-Based Contextualization for Cross-Lingual Speech Recognition in End-to-End Models",
    "volume": "main",
    "abstract": "Contextual automatic speech recognition, i.e., biasing recognition towards a given context (e.g. user's playlists, or contacts), is challenging in end-to-end (E2E) models. Such models maintain a limited number of candidates during beam-search decoding, and have been found to recognize rare named entities poorly. The problem is exacerbated when biasing towards proper nouns in foreign languages, e.g., geographic location names, which are virtually unseen in training and are thus out-of-vocabulary (OOV). While grapheme or wordpiece E2E models might have a difficult time spelling OOV words, phonemes are more acoustically salient and past work has shown that E2E phoneme models can better predict such words. In this work, we propose an E2E model containing both English wordpieces and phonemes in the modeling space, and perform contextual biasing of foreign words at the phoneme level by mapping pronunciations of foreign words into similar English phonemes. In experimental evaluations, we find that the proposed approach performs 16% better than a grapheme-only biasing model, and 8% better than a wordpiece-only biasing model on a foreign place name recognition task, with only slight degradation on regular English tasks",
    "checked": true,
    "id": "e2f30027b2a2fa6ecc271934cc6de2e4beb10035",
    "semantic_title": "phoneme-based contextualization for cross-lingual speech recognition in end-to-end models",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/khassanov19_interspeech.html": {
    "title": "Constrained Output Embeddings for End-to-End Code-Switching Speech Recognition with Only Monolingual Data",
    "volume": "main",
    "abstract": "The lack of code-switch training data is one of the major concerns in the development of end-to-end code-switching automatic speech recognition (ASR) models. In this work, we propose a method to train an improved end-to-end code-switching ASR using only monolingual data. Our method encourages the distributions of output token embeddings of monolingual languages to be similar, and hence, promotes the ASR model to easily code-switch between languages. Specifically, we propose to use Jensen-Shannon divergence and cosine distance based constraints. The former will enforce output embeddings of monolingual languages to possess similar distributions, while the later simply brings the centroids of two distributions to be close to each other. Experimental results demonstrate high effectiveness of the proposed method, yielding up to 4.5% absolute mixed error rate improvement on Mandarin-English code-switching ASR task",
    "checked": true,
    "id": "298f209baa6c98afd72469f9823bf6a3f39f5f0c",
    "semantic_title": "constrained output embeddings for end-to-end code-switching speech recognition with only monolingual data",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zeng19_interspeech.html": {
    "title": "On the End-to-End Solution to Mandarin-English Code-Switching Speech Recognition",
    "volume": "main",
    "abstract": "Code-switching (CS) refers to a linguistic phenomenon where a speaker uses different languages in an utterance or between alternating utterances. In this work, we study end-to-end (E2E) approaches to the Mandarin-English code-switching speech recognition task. We first examine the effectiveness of using data augmentation and byte-pair encoding (BPE) subword units. More importantly, we propose a multitask learning recipe, where a language identification task is explicitly learned in addition to the E2E speech recognition task. Furthermore, we introduce an efficient word vocabulary expansion method for language modeling to alleviate data sparsity issues under the code-switching scenario. Experimental results on the SEAME data, a Mandarin-English code-switching corpus, demonstrate the effectiveness of the proposed methods",
    "checked": true,
    "id": "0306bd168cb36b22b76e1731702ed0e7e8c3f3d9",
    "semantic_title": "on the end-to-end solution to mandarin-english code-switching speech recognition",
    "citation_count": 72
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19f_interspeech.html": {
    "title": "Towards Language-Universal Mandarin-English Speech Recognition",
    "volume": "main",
    "abstract": "Multilingual and code-switching speech recognition are two challenging tasks that are studied separately in many previous works. In this work, we jointly study multilingual and code-switching problems, and present a language-universal bilingual system for Mandarin-English speech recognition. Specifically, we propose a novel bilingual acoustic model, which consists of two monolingual system initialized subnets and a shared output layer corresponding to the Character-Subword acoustic modeling units. The bilingual acoustic model is trained using a large Mandarin-English corpus with CTC and sMBR criteria. We find that this model, which is not given any information about language identity, can achieve comparable performance in monolingual Mandarin and English test sets compared to the well-trained language-specific Mandarin and English ASR systems, respectively. More importantly, the proposed bilingual model can automatically learn the language switching. Experimental results on a Mandarin-English code-switching test set show that it can achieve 11.8% and 17.9% relative error reduction on Mandarin and English parts, respectively",
    "checked": true,
    "id": "1786d90b757bba799f6ec29485a5d575d1fdef65",
    "semantic_title": "towards language-universal mandarin-english speech recognition",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/swarup19_interspeech.html": {
    "title": "Improving ASR Confidence Scores for Alexa Using Acoustic and Hypothesis Embeddings",
    "volume": "main",
    "abstract": "In automatic speech recognition, confidence measures provide a quantitative representation used to assess whether a generated hypothesis text is correct or not. For personal assistant devices like Alexa, automatic speech recognition (ASR) errors are inevitable due to the imperfection of today's speech recognition technology. Hence, confidence scores provide an important metric to gauge the correctness of ASR hypothesis text and enable downstream consumers to subsequently initiate appropriate actions. In this work, our aim is to improve the correctness of our confidence scores by enhancing our baseline model architecture with learned features, namely acoustic and 1-best hypothesis embeddings. These embeddings are obtained by training separate networks on acoustic features and ASR 1-best hypothesis respectively. We present an experimental evaluation on a large US English data set showing a 6% relative equal error rate reduction and 13% relative normalized cross-entropy improvement over our baseline system by incorporating these embeddings. We also present a deeper analysis of the embeddings revealing that the acoustic embedding results in a better prediction of insertion errors whereas the 1-best hypothesis embedding helps to better predict substitution errors",
    "checked": true,
    "id": "0a0bcac192dd64ed79a95ea7792480b26fc6d604",
    "semantic_title": "improving asr confidence scores for alexa using acoustic and hypothesis embeddings",
    "citation_count": 32
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19g_interspeech.html": {
    "title": "Investigation of Transformer Based Spelling Correction Model for CTC-Based End-to-End Mandarin Speech Recognition",
    "volume": "main",
    "abstract": "Connectionist Temporal Classification (CTC) based end-to-end speech recognition system usually need to incorporate an external language model by using WFST-based decoding in order to achieve promising results. This is more essential to Mandarin speech recognition since it owns a special phenomenon, namely homophone, which causes a lot of substitution errors. The linguistic information introduced by language model is somehow helpful to distinguish these substitution errors. In this work, we propose a transformer based spelling correction model to automatically correct errors, especially the substitution errors, made by CTC-based Mandarin speech recognition system. Specifically, we investigate to use the recognition results generated by CTC-based systems as input and the ground-truth transcriptions as output to train a transformer with encoder-decoder architecture, which is much similar to machine translation. Experimental results in a 20,000 hours Mandarin speech recognition task show that the proposed spelling correction model can achieve a CER of 3.41%, which results in 22.9% and 53.2% relative improvement compared to the baseline CTC-based systems decoded with and without language model, respectively",
    "checked": true,
    "id": "fda800775bc3653b9ea83f66cf4223cb323e2303",
    "semantic_title": "investigation of transformer based spelling correction model for ctc-based end-to-end mandarin speech recognition",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2019/peyser19_interspeech.html": {
    "title": "Improving Performance of End-to-End ASR on Numeric Sequences",
    "volume": "main",
    "abstract": "Recognizing written domain numeric utterances (e.g., I need 1.25.) can be challenging for ASR systems, particularly when numeric sequences are not seen during training. This out-of-vocabulary (OOV) issue is addressed in conventional ASR systems by training part of the model on spoken domain utterances (e.g., I need one dollar and twenty five cents.), for which numeric sequences are composed of in-vocabulary numbers, and then using an FST verbalizer to denormalize the result. Unfortunately, conventional ASR models are not suitable for the low memory setting of on-device speech recognition. E2E models such as RNN-T are attractive for on-device ASR, as they fold the AM, PM and LM of a conventional model into one neural network. However, in the on-device setting the large memory footprint of an FST denormer makes spoken domain training more difficult. In this paper, we investigate techniques to improve E2E model performance on numeric data. We find that using a text-to-speech system to generate additional numeric training data, as well as using a small-footprint neural network to perform spoken-to-written domain denorming, yields improvement in several numeric classes. In the case of the longest numeric sequences, we see reduction of WER by up to a factor of 8",
    "checked": true,
    "id": "a2fef91c2887bcc0520998c71aa3b50e3f7446e8",
    "semantic_title": "improving performance of end-to-end asr on numeric sequences",
    "citation_count": 29
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bai19_interspeech.html": {
    "title": "A Time Delay Neural Network with Shared Weight Self-Attention for Small-Footprint Keyword Spotting",
    "volume": "main",
    "abstract": "Keyword spotting requires a small memory footprint to run on mobile devices. However, previous works still use several hundred thousand parameters to achieve good performance. To address this issue, we propose a time delay neural network with shared weight self-attention for small-footprint keyword spotting. By sharing weights, the parameters of self-attention are reduced but without performance reduction. The publicly available Google Speech Commands dataset is used to evaluate the models. The number of parameters (12K) of our model is 1/20 of state-of-the-art ResNet model (239K). The proposed model achieves an error rate of 4.19% , which is comparable to the ResNet model",
    "checked": true,
    "id": "a76a76cb959c2ca53b8d62bd2baa7db3d50659e6",
    "semantic_title": "a time delay neural network with shared weight self-attention for small-footprint keyword spotting",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kao19_interspeech.html": {
    "title": "Sub-Band Convolutional Neural Networks for Small-Footprint Spoken Term Classification",
    "volume": "main",
    "abstract": "This paper proposes a Sub-band Convolutional Neural Network for spoken term classification. Convolutional neural networks (CNNs) have proven to be very effective in acoustic applications such as spoken term classification, keyword spotting, speaker identification, acoustic event detection, etc. Unlike applications in computer vision, the spatial invariance property of 2D convolutional kernels does not fit acoustic applications well since the meaning of a specific 2D kernel varies a lot along the feature axis in an input feature map. We propose a sub-band CNN architecture to apply different convolutional kernels on each feature sub-band, which makes the overall computation more efficient. Experimental results show that the computational efficiency brought by sub-band CNN is more beneficial for small-footprint models. Compared to a baseline full band CNN for spoken term classification on a publicly available Speech Commands dataset, the proposed sub-band CNN architecture reduces the computation by 39.7% on commands classification, and 49.3% on digits classification with accuracy maintained",
    "checked": true,
    "id": "1f45395944568f7062f8faa8a801a2995e70897e",
    "semantic_title": "sub-band convolutional neural networks for small-footprint spoken term classification",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19k_interspeech.html": {
    "title": "Investigating Radical-Based End-to-End Speech Recognition Systems for Chinese Dialects and Japanese",
    "volume": "main",
    "abstract": "Training automatic speech recognition (ASR) systems for East Asian languages (e.g., Chinese and Japanese) is tough work because of the characters existing in the writing systems of these languages. Traditionally, we first need to get the pronunciation of these characters by morphological analysis. The end-to-end (E2E) model allows for directly using characters or words as the modeling unit. However, since different groups of people (e.g., residents in Chinese mainland, Hong Kong, Taiwan, and Japan) adopts different writing forms for a character, this also leads to a large increase in the number of vocabulary, especially when building ASR systems across languages or dialects. In this paper, we propose a new E2E ASR modeling method by decomposing the characters into a set of radicals. Our experiments demonstrate that it is possible to effectively reduce the vocabulary size by sharing the basic radicals across different dialect of Chinese. Moreover, we also demonstrate this method could also be used to construct a Japanese E2E ASR system. The system modeled with radicals and kana achieved similar performance compared to state-of-the-art E2E system built with word-piece units",
    "checked": true,
    "id": "f069944718f3e29ca4ffdd20ba486828459a6e8b",
    "semantic_title": "investigating radical-based end-to-end speech recognition systems for chinese dialects and japanese",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/guo19d_interspeech.html": {
    "title": "Joint Decoding of CTC Based Systems for Speech Recognition",
    "volume": "main",
    "abstract": "Connectionist temporal classification (CTC) has been successfully used in speech recognition. It learns the alignments between speech frames and label sequences automatically without explicit pre-generated frame-level labels. While this property is convenient for shortening the training pipeline, it may become a potential disadvantage for the frame-level system combination due to inaccurate alignments. In this paper, a novel Dynamic Time Warping (DTW) based position calibration algorithm is proposed for joint decoding on two CTC based acoustic models. Furthermore, joint decoding for CTC and conventional hybrid NN-HMM models is also explored. Experiments on a large vocabulary Mandarin speech recognition task show that the proposed joint decoding of both CTC based and CTC-Hybrid based systems can achieve a significant and consistent character error rate reduction",
    "checked": true,
    "id": "cd15d9939b0c308218a9ce8a6e5037ff0016ac02",
    "semantic_title": "joint decoding of ctc based systems for speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tanaka19_interspeech.html": {
    "title": "A Joint End-to-End and DNN-HMM Hybrid Automatic Speech Recognition System with Transferring Sharable Knowledge",
    "volume": "main",
    "abstract": "This paper presents joint end-to-end and deep neural network-hidden Markov model (DNN-HMM) hybrid automatic speech recognition (ASR) systems that share network components. End-to-end ASR systems have been shown competitive performance compared with the DNN-HMM hybrid ASR systems in recent studies. These systems have different advantages, which are an estimation ability based on the totally optimized model of the end-to-end ASR system and a stable processing based on a frame-by-frame manner of the DNN-HMM hybrid ASR system. In our previous study, we proposed a method to utilize an end-to-end ASR system for rescoring hypotheses generated from a DNN-HMM hybrid ASR system. However, the conventional method cannot efficiently leverage the advantages since network components are independently modeled. In order to tackle this problem, we propose a joint end-to-end and DNN-HMM hybrid ASR systems that share the network to transfer knowledge of the systems. In the proposed method, end-to-end ASR systems utilize the information from an output of an internal layer in a DNN acoustic model in the DNN-HMM hybrid ASR system for enhancing the end-to-end ASR system. This enables us to efficiently leverage sharable information for improving the joint ASR system. Experimental results show that the proposed method outperforms the conventional method",
    "checked": true,
    "id": "98acb379edbf8e7c91223509e8e6f046566ff871",
    "semantic_title": "a joint end-to-end and dnn-hmm hybrid automatic speech recognition system with transferring sharable knowledge",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/malhotra19_interspeech.html": {
    "title": "Active Learning Methods for Low Resource End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "Recently developed end-to-end (E2E) automatic speech recognition (ASR) systems demand abundance of transcribed speech data, there are several scenarios where the labeling of speech data is cumbersome and expensive. For a fixed annotation cost, active learning for speech recognition allows to efficiently train the ASR model. In this work, we advance the most common approach for active learning methods which relies on uncertainty sampling technique. In particular, we explore the use of path probability of the decoded sequence as a confidence measure and select the samples with the least confidence for active learning. In order to reduce the sampling bias in active learning, we propose a regularized uncertainty sampling approach that incorporates an i-vector diversity measure. Thus, the active learning in the proposed framework uses a joint score of uncertainty and i-vector diversity. The benefits of the proposed approach are illustrated for an E2E ASR task performed on CSJ and Librispeech datasets. In these experiments, we show that the proposed approach yields considerable improvements over the baseline model using random sampling",
    "checked": true,
    "id": "0c8c118694b8f4dfcd57d4990a6c80ad8d71238a",
    "semantic_title": "active learning methods for low resource end-to-end speech recognition",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/karafiat19_interspeech.html": {
    "title": "Analysis of Multilingual Sequence-to-Sequence Speech Recognition Systems",
    "volume": "main",
    "abstract": "This paper investigates the applications of various multilingual approaches developed in conventional deep neural network - hidden Markov model (DNN-HMM) systems to sequence-to-sequence (seq2seq) automatic speech recognition (ASR). We employ a joint connectionist temporal classification-attention network as our base model. Our main contribution is separated into two parts. First, we investigate the effectiveness of the seq2seq model with stacked multilingual bottle-neck features obtained from a conventional DNN-HMM system on the Babel multilingual speech corpus. Second, we investigate the effectiveness of transfer learning from a pre-trained multilingual seq2seq model with and without the target language included in the original multilingual training data. In this experiment, we also explore various architectures and training strategies of the multilingual seq2seq model by making use of knowledge obtained in the DNN-HMM based transfer-learning. Although both approaches significantly improved the performance from a monolingual seq2seq baseline, interestingly, we found the multilingual bottle-neck features to be superior to multilingual models with transfer learning. This finding suggests that we can efficiently combine the benefits of the DNN-HMM system with the seq2seq system through multilingual bottle-neck feature techniques",
    "checked": true,
    "id": "aa0b93501f79d57fe8542e72ccc8843ea50443c9",
    "semantic_title": "analysis of multilingual sequence-to-sequence speech recognition systems",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zapotoczny19_interspeech.html": {
    "title": "Lattice Generation in Attention-Based Speech Recognition Models",
    "volume": "main",
    "abstract": "Attention-based neural speech recognition models are frequently decoded with beam search, which produces a tree of hypotheses. In many cases, such as when using external language models, numerous decoding hypotheses need to be considered, requiring large beam sizes during decoding. We demonstrate that it is possible to merge certain nodes in a tree of hypotheses, in order to obtain a decoding lattice, which increases the number of decoding hypotheses without increasing the number of candidates that are scored by the neural network. We propose a convolutional architecture, which facilitates comparing states of the model at different pi The experiments are carried on the Wall Street Journal dataset, where the lattice decoder obtains lower word error rates with smaller beam sizes, than an otherwise similar architecture with regular beam search",
    "checked": true,
    "id": "1ded3d23b5b5a9264fe5e67d07ce217f0adf9765",
    "semantic_title": "lattice generation in attention-based speech recognition models",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jansche19_interspeech.html": {
    "title": "Sampling from Stochastic Finite Automata with Applications to CTC Decoding",
    "volume": "main",
    "abstract": "Stochastic finite automata arise naturally in many language and speech processing tasks. They include stochastic acceptors, which represent certain probability distributions over random strings. We consider the problem of efficient sampling: drawing random string variates from the probability distribution represented by stochastic automata and transformations of those. We show that path-sampling is effective and can be efficient if the epsilon-graph of a finite automaton is acyclic. We provide an algorithm that ensures this by conflating epsilon-cycles within strongly connected components. Sampling is also effective in the presence of non-injective transformations of strings. We illustrate this in the context of decoding for Connectionist Temporal Classification (CTC), where the predictive probabilities yield auxiliary sequences which are transformed into shorter labeling strings. We can sample efficiently from the transformed labeling distribution and use this in two different strategies for finding the most probable CTC labeling",
    "checked": true,
    "id": "cffaf577458b3b5730bcaf35a5a00a1b1e762f2d",
    "semantic_title": "sampling from stochastic finite automata with applications to ctc decoding",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dudziak19_interspeech.html": {
    "title": "ShrinkML: End-to-End ASR Model Compression Using Reinforcement Learning",
    "volume": "main",
    "abstract": "End-to-end automatic speech recognition (ASR) models are increasingly large and complex to achieve the best possible accuracy. In this paper, we build an AutoML system that uses reinforcement learning (RL) to optimize the per-layer compression ratios when applied to a state-of-the-art attention based end-to-end ASR model composed of several LSTM layers. We use singular value decomposition (SVD) low-rank matrix factorization as the compression method. For our RL-based AutoML system, we focus on practical considerations such as the choice of the reward/punishment functions, the formation of an effective search space, and the creation of a representative but small data set for quick evaluation between search steps. Finally, we present accuracy results on LibriSpeech of the model compressed by our AutoML system, and we compare it to manually-compressed models. Our results show that in the absence of retraining our RL-based search is an effective and practical method to compress a production-grade ASR system. When retraining is possible, we show that our AutoML system can select better highly-compressed seed models compared to manually hand-crafted rank selection, thus allowing for more compression than previously possible",
    "checked": true,
    "id": "a2cf8b5ac4925188cf6e3becf8f65558301a0246",
    "semantic_title": "shrinkml: end-to-end asr model compression using reinforcement learning",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gaur19_interspeech.html": {
    "title": "Acoustic-to-Phrase Models for Speech Recognition",
    "volume": "main",
    "abstract": "Directly emitting words and sub-words from speech spectrogram has been shown to produce good results using end-to-end (E2E) trained models. Connectionist Temporal Classification (CTC) and Sequence-to-Sequence attention (Seq2Seq) models have both shown better success when directly targeting words or sub-words. In this work, we ask the question: Can an E2E model go beyond words and transcribe directly to phrases (i.e., a group of words)? Directly modeling frequent phrases might be better than modeling its constituent words. Also, emitting multiple words together might speed up inference in models like Seq2Seq where decoding is inherently sequential. To answer this, we undertake a study on a 3400-hour Microsoft Cortana voice assistant task. We present a side-by-side comparison for CTC and Seq2Seq models that have been trained to target a variety of tokens including letters, sub-words, words and phrases. We show that an E2E model can indeed transcribe directly to phrases. We see that while CTC has difficulty in accurately modeling phrases, a more powerful model like Seq2Seq can effortlessly target phrases that are up to 4 words long, with only a reasonable degradation in the final word error rate",
    "checked": true,
    "id": "e02f2a7bb5e3793d0b44c82dff0fbccf10566664",
    "semantic_title": "acoustic-to-phrase models for speech recognition",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19l_interspeech.html": {
    "title": "Performance Monitoring for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "Measuring performance of an automatic speech recognition (ASR) system without ground-truth could be beneficial in many scenarios, especially with data from unseen domains, where performance can be highly inconsistent. In conventional ASR systems, several performance monitoring (PM) techniques have been well-developed to monitor performance by looking at tri-phone posteriors or pre-softmax activations from neural network acoustic modeling. However, strategies for monitoring more recently developed end-to-end ASR systems have not yet been explored, and so that is the focus of this paper. We adapt previous PM measures (Entropy, M-measure and Autoencoder) and apply our proposed RNN predictor in the end-to-end setting. These measures utilize the decoder output layer and attention probability vectors, and their predictive power is measured with simple linear models. Our findings suggest that decoder-level features are more feasible and informative than attention-level probabilities for PM measures, and that M-measure on the decoder posteriors achieves the best overall predictive performance with an average prediction error 8.8%. Entropy measures and RNN-based prediction also show competitive predictability, especially for unseen conditions",
    "checked": true,
    "id": "035af6e1dd6e138243f5d75b6a4cfad7a80ecd6f",
    "semantic_title": "performance monitoring for end-to-end speech recognition",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cohn19b_interspeech.html": {
    "title": "The Role of Musical Experience in the Perceptual Weighting of Acoustic Cues for the Obstruent Coda Voicing Contrast in American English",
    "volume": "main",
    "abstract": "This study examines the role of musical experience on listeners' phoneme judgments across noise conditions. Individuals with 10+ years of musical training were compared with nonmusicians in their use of three acoustic cues in categorizing post-vocalic obstruent voicing: fundamental frequency, vowel duration, and spectral composition in two listening conditions (silence and multitalker babble, MTB). Results demonstrate that musicians display steeper phonemic categorization for coda /t/ and /d/ on the basis of all three cues of interest. Additionally, musicians and nonmusicians show different cue weighting patterns in MTB than in silence. The findings are discussed with reference to their implications for theories of experience-driven plasticity and individual differences in the perceptual organization of phonemes",
    "checked": true,
    "id": "a26cac35b9bf006f3fcd12dfe87117f2fd2807d7",
    "semantic_title": "the role of musical experience in the perceptual weighting of acoustic cues for the obstruent coda voicing contrast in american english",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lewandowski19_interspeech.html": {
    "title": "Individual Differences in Implicit Attention to Phonetic Detail in Speech Perception",
    "volume": "main",
    "abstract": "We present a study on the interactions between implicit attention to acoustic-phonetic detail in speech and individual differences (IDs). Attention to phonetic detail was assessed with acoustically manipulated speech stimuli within a computer game, an alternative to regular highly-controlled categorization tests. Twenty-two native German speakers (11f) completed the game and further tests including individual attention test measures (e.g. Simon Test), the BFI-10 (short version of the Big Five Inventory), and a Self-monitoring Test (need for social approval). With this study, we contribute to the understanding of the processes underlying human speech perception and the impact of cognitive and personality features on the attention to phonetic detail. Our results show that the general (non-verbal) attention capacity (mental flexibility, inhibition), interacts with implicit attention to phonetic detail. Furthermore, IDs in personality, such as sensitivity to social cues or conscientiousness significantly add to the effects. Understanding these interactions, especially arising in an intuitive and non-explicit study design, is an important step on the way towards explaining not only the influence of IDs on attention to phonetic detail, but also the dynamics of speech interaction (e.g. phonetic convergence)",
    "checked": true,
    "id": "e7ec589ec2e0900ec0a7dbf599d393de1686861a",
    "semantic_title": "individual differences in implicit attention to phonetic detail in speech perception",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lalonde19_interspeech.html": {
    "title": "Effects of Natural Variability in Cross-Modal Temporal Correlations on Audiovisual Speech Recognition Benefit",
    "volume": "main",
    "abstract": "In audiovisual (AV) speech, correlations over time between visible mouth movements and the amplitude envelope of auditory speech help to reduce uncertainty as to when peaks in the auditory signal will occur. Previous studies demonstrated greater AV benefit to speech detection in noise for sentences with higher cross-modal correlations than sentences with lower cross-modal correlations This study examined whether the mechanisms that underlie AV detection benefits have downstream effects on speech recognition in noise. Participants were presented 72 sentences in noise, in auditory-only and AV conditions, at either their 50% auditory speech recognition threshold in noise (SRT-50) or at a signal-to-noise ratio (SNR) 6 dB poorer than their SRT-50. They were asked to repeat each sentence. Mean AV benefit across subjects was calculated for each sentence. Pearson correlations and mixed modeling were used to examined whether variability in AV benefit across sentences was related to natural variation in the degree of cross-modal correlation across sentences In the more difficult listening condition, higher cross-modal correlations were associated with higher AV sentence recognition benefit. The relationship was strongest in the 0.8–2.2 kHz and 0.8–6 kHz frequency regions. These results demonstrate that cross-modal correlations contribute to variability in AV speech recognition in noise",
    "checked": true,
    "id": "687915220f8904fd30119e06e3daf07505683056",
    "semantic_title": "effects of natural variability in cross-modal temporal correlations on audiovisual speech recognition benefit",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bentum19_interspeech.html": {
    "title": "Listening with Great Expectations: An Investigation of Word Form Anticipations in Naturalistic Speech",
    "volume": "main",
    "abstract": "The event-related potential (ERP) component named phonological mismatch negativity (PMN) arises when listeners hear an unexpected word form in a spoken sentence [1]. The PMN is thought to reflect the mismatch between expected and perceived auditory speech input. In this paper, we use the PMN to test a central premise in the predictive coding framework [2], namely that the mismatch between prior expectations and sensory input is an important mechanism of perception. We test this with natural speech materials containing approximately 50,000 word tokens. The corresponding EEG-signal was recorded while participants (n = 48) listened to these materials. Following [3], we quantify the mismatch with two word probability distributions (WPD): a WPD based on preceding context, and a WPD that is additionally updated based on the incoming audio of the current word. We use the between-WPD cross entropy for each word in the utterances and show that a higher cross entropy correlates with a more negative PMN. Our results show that listeners anticipate auditory input while processing each word in naturalistic speech. Moreover, complementing previous research, we show that predictive language processing occurs across the whole probability spectrum",
    "checked": true,
    "id": "b1f917f87a386ecbab76d2b8b3e9a0a1fada6e96",
    "semantic_title": "listening with great expectations: an investigation of word form anticipations in naturalistic speech",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bentum19b_interspeech.html": {
    "title": "Quantifying Expectation Modulation in Human Speech Processing",
    "volume": "main",
    "abstract": "The mismatch between top-down predicted and bottom-up perceptual input is an important mechanism of perception according to the predictive coding framework (Friston, [1]). In this paper we develop and validate a new information-theoretic measure that quantifies the mismatch between expected and observed auditory input during speech processing. We argue that such a mismatch measure is useful for the study of speech processing. To compute the mismatch measure, we use naturalistic speech materials containing approximately 50,000 word tokens. For each word token we first estimate the prior word probability distribution with the aid of statistical language modelling, and next use automatic speech recognition to update this word probability distribution based on the unfolding speech signal. We validate the mismatch measure with multiple analyses, and show that the auditory-based update improves the probability of the correct word and lowers the uncertainty of the word probability distribution. Based on these results, we argue that it is possible to explicitly estimate the mismatch between predicted and perceived speech input with the cross entropy between word expectations computed before and after an auditory update",
    "checked": true,
    "id": "3f64e8e8566a23468e3584efff3c504b85694195",
    "semantic_title": "quantifying expectation modulation in human speech processing",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/turner19_interspeech.html": {
    "title": "Perception of Pitch Contours in Speech and Nonspeech",
    "volume": "main",
    "abstract": "The pitch perception literature has been largely built on experimental data collected using nonspeech stimuli, which has then been generalized to speech. In the present study, we compare the perceptibility of identical pitch movements in speech and nonspeech that vary in duration and in pitch range. Our nonspeech results closely replicate earlier findings and we show that speech is a significantly more difficult medium for pitch discrimination. Pitch movements in speech have to be larger and longer to achieve the salience of the most common speech analog, pulse trains. The direction of pitch movement also affects one's ability to discern pitch; in particular falling excursions are the most difficult. We found that the perceptual threshold for falling pitch in speech was more than 100 times that of previous estimates with nonspeech stimuli. Our findings show that the perceptual response to nonspeech does not adequately map onto speech, and future work in speech research and its applications should use speech-like stimuli, rather than convenient substitutes like pulse trains, pure tones, or isolated vowels",
    "checked": true,
    "id": "f5873ef515f3d80742805b7ec863c0aa7adcb7bf",
    "semantic_title": "perception of pitch contours in speech and nonspeech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bosch19b_interspeech.html": {
    "title": "Analyzing Reaction Time and Error Sequences in Lexical Decision Experiments",
    "volume": "main",
    "abstract": "Reaction times (RTs) are used widely in psychological and psycholinguistic research as inexpensive measures of underlying cognitive processes. However, inferring cognitive processes from RTs is hampered by the fact that actual responses are the result of multiple factors, many of which may not be related to the process of interest. In lexical decision experiments, the use of RTs is further complicated by the fact that the response to some stimuli is missing, and the fact that part of the responses are ‘incorrect' In this paper we investigate the distribution of missing and incorrect responses in the RT sequences of two large lexical decision experiments. It appears that a substantial part of incorrect responses cluster together. Then, we investigate the effect of clusters of incorrect responses on surrounding RTs Also, we extend previous research on methods for discovering and removing so-called local speed effects from RT sequences. For this purpose, we show that a recently introduced graph-based RT analysis method can help to better understand and analyze RT sequences",
    "checked": true,
    "id": "bf64612957e3af3645a72fdee75586d0d9b16106",
    "semantic_title": "analyzing reaction time and error sequences in lexical decision experiments",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19e_interspeech.html": {
    "title": "Automatic Detection of the Temporal Segmentation of Hand Movements in British English Cued Speech",
    "volume": "main",
    "abstract": "Cued Speech (CS) is a multi-modal system, which complements the lip reading with manual hand cues in the phonetic level to make the spoken language visible. It has been found that lip and hand movements are asynchronous in CS, and thus the study of hand temporal organization is very important for the multi-modal CS feature fusion. In this work, we propose a novel diphthong-hand preceding model (D-HPM) by investigating the relationship between hand preceding time (HPT) and diphthong time instants in sentences for British English CS. Besides, we demonstrate that HPT of the first and second parts of diphthongs has a very strong correlation. Combining the monophthong-HPM (M-HPM) and D-HPM, we present a hybrid temporal segmentation detection algorithm (HTSDA) for the hand movement in CS. The evaluation of the proposed algorithm is carried out by a hand position recognition experiment using the multi-Gaussian classifier as well as the long-short term memory (LSTM). The results show that the HTSDA significantly improves the recognition performance compared with the baseline (i.e., audio-based segmentation) and the state-of-the-art M-HPM. To the best of our knowledge, this is the first work to study the temporal organization of hand movements in British English CS",
    "checked": true,
    "id": "c7413c108f97665a67598bf7cd941ed379072683",
    "semantic_title": "automatic detection of the temporal segmentation of hand movements in british english cued speech",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yokoe19_interspeech.html": {
    "title": "Place Shift as an Autonomous Process: Evidence from Japanese Listeners",
    "volume": "main",
    "abstract": "A perception experiment with Japanese listeners is conducted to investigate the nature of place shift phenomenon that was previously found with French and English listeners. Hallé et al. [1] showed that unattested consonant sequences /tl, dl/ are perceptually repaired to form grammatically acceptable consonant clusters /kl, gl/ in the listeners' native language In this study, a similar experiment with Japanese listeners, whose mother tongue lacks the onset clusters altogether, is conducted. The result explicitly shows that the place shift phenomenon ought not to be interpreted in relation to the top-down phonotactic feedback. Rather, I will argue that both labial and velar shift reflect an autonomous, signal-driven process. As such, language specificity in speech perception must reside in the listeners' cue weighting, rather than encoded linguistic knowledge",
    "checked": true,
    "id": "cb81e17aa6362e803f257a11fc1172ce0bf6c794",
    "semantic_title": "place shift as an autonomous process: evidence from japanese listeners",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/meyer19_interspeech.html": {
    "title": "A Perceptual Study of CV Syllables in Both Spoken and Whistled Speech: A Tashlhiyt Berber Perspective",
    "volume": "main",
    "abstract": "The present study compares the perceptual categorization of four CV syllables /ta, da, ka, ga/ in two different speech registers — modal speech and whistled speech — of Tashlhiyt Berber used in the Moroccan High Atlas. Whistled speech in a non-tonal language such as Tashlhiyt is a special speech register used for long distance dialogues that consists of the natural production of vocalic and consonantal qualities in a simple modulated whistled signal. The technique of whistling imposes various restrictions on speech articulation, which result in a simplification of the phonetics of spoken speech into a ‘whistled formant'. Here, we describe this simplification for Tashlhiyt syllables /ta, da, ka, ga/ and use them as stimuli in a behavioral experiment. We analyze and compare the perceptual categorization obtained from native Tashlhiyt listeners (trained since childhood in whistled speech) for both speech registers on these 4 syllable types. Results show that whistled stimuli were fairly well identified (~42%) above chance (25%), though less well than spoken ones (~84%). The detailed analysis of confusions between CVs enabled us to understand better how whistled consonants are perceived, highlighting the phonological contrasts that are best perceived and retained from spoken to whistled speech in this language",
    "checked": true,
    "id": "ccdf06ff6134ed5fa7c3026a19d322a8b703877d",
    "semantic_title": "a perceptual study of cv syllables in both spoken and whistled speech: a tashlhiyt berber perspective",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hsieh19_interspeech.html": {
    "title": "Consonant Classification in Mandarin Based on the Depth Image Feature: A Pilot Study",
    "volume": "main",
    "abstract": "The consonant is an important element in Mandarin, and various categories of consonant generation effectuate various facial expressions. Specifically, there are changes in facial muscles when speaking, and these changes are closely related to pronunciation; the facial muscles are associated with these hidden articulators, and the effects on the facial changes can be seen as 3D changes. However, in most studies, 2D images are used to analyze facial features when people talk. The 2D images serve to provide information in two dimensions (x- and y-axis); however, subtle deep motions (z-axis changes) of facial muscles when speaking can be difficult to detect accurately. Hence, the depth feature of the face (the point cloud feature in this study) was used to investigate the potential for consonant recognition, recorded by a time-of-flight 3D camera. In this study, we propose an algorithm to recognize the seven categories of Mandarin consonants using the depth features of the speaker's face. The proposed system yielded suitable classification accuracy for the recognition of seven categories of Mandarin consonants. This result implies that depth features can be used for speech-processing applications",
    "checked": true,
    "id": "2feb35fd19c07ac94e6f31acb4b89e6270bf12f4",
    "semantic_title": "consonant classification in mandarin based on the depth image feature: a pilot study",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/levari19_interspeech.html": {
    "title": "The Different Roles of Expectations in Phonetic and Lexical Processing",
    "volume": "main",
    "abstract": "The way people speak reflects their demographic background. Listeners exploit this contingent variation and make use of information about speakers' background to process their speech. Evidence for this comes from both phonetic and lexical tasks, and the two are assumed to tap into the same mechanism and provide equivalent results. Curiously, this assumption has never been tested. Additionally, while it has been established that expectations can influence language processing in general, the role of individual differences in susceptibility to this influence is relatively unexplored. We investigate these two questions in the context of Southern and General American speech varieties in the USA. We show that phonetic and lexical tasks are not equivalent, and furthermore, that the two are driven by mechanisms that are sensitive to different individual variables: while performance at the lexical level is influenced by implicit bias, performance at the phonetic level is influenced by working memory. These results thus change our understanding of how expectations influence processing, and have implications for how to conduct and interpret studies on the topic",
    "checked": true,
    "id": "d1deff1f548acaa1f81409158b1785627b81ef54",
    "semantic_title": "the different roles of expectations in phonetic and lexical processing",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/segedin19_interspeech.html": {
    "title": "Perceptual Adaptation to Device and Human Voices: Learning and Generalization of a Phonetic Shift Across Real and Voice-AI Talkers",
    "volume": "main",
    "abstract": "Voice-activated artificially-intelligent digital devices are a new type of interlocutor. Like for human talkers, they have idiosyncratic speech patterns that require listeners to perceptually adapt to during language comprehension. One question is how perceptual adaptation to a novel accent in speech produced by a digital device voice compares to adaptation to human voices. Furthermore, adaptation to one talker can generalize to novel voices. Hence, we also tested whether perceptual adaptation to accented device voices generalizes to novel human voices, and vice versa. In this study, listeners were first exposed to words with a shifted phoneme realization in either a device or human voice. Later, participants were tested on whether they shifted their identification of words in the shifted talker. Additionally, we tested whether listeners applied the shift to novel device and human voices not heard in exposure. Results reveal talker-specific learning for both device and human voices. Yet, the size of the shift was larger for the device voices. Furthermore, listeners exposed to the shift in device voices showed generalization to novel human voices, and vice versa. These patterns of adaptation and generalization for device and human talkers have implications for models of speech perception models and human-computer interaction",
    "checked": true,
    "id": "8aae33403309248db948f15019e6061e88056616",
    "semantic_title": "perceptual adaptation to device and human voices: learning and generalization of a phonetic shift across real and voice-ai talkers",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/papadimitriou19_interspeech.html": {
    "title": "End-to-End Convolutional Sequence Learning for ASL Fingerspelling Recognition",
    "volume": "main",
    "abstract": "Although fingerspelling is an often overlooked component of sign languages, it has great practical value in the communication of important context words that lack dedicated signs. In this paper we consider the problem of fingerspelling recognition in videos, introducing an end-to-end lexicon-free model that consists of a deep auto-encoder image feature learner followed by an attention-based encoder-decoder for prediction. The feature extractor is a vanilla auto-encoder variant, employing a quadratic activation function. The learned features are subsequently fed into the attention-based encoder-decoder. The latter deviates from traditional recurrent neural network architectures, being a fully convolutional attention-based encoder-decoder that is equipped with a multi-step attention mechanism relying on a quadratic alignment function and gated linear units over the convolution output. The introduced model is evaluated on the TTIC/UChicago fingerspelling video dataset, where it outperforms previous approaches in letter accuracy under all three, signer-dependent, -adapted, and -independent, experimental paradigms",
    "checked": true,
    "id": "52b3734318715ad74fe4f5e58653c10f9abc51ec",
    "semantic_title": "end-to-end convolutional sequence learning for asl fingerspelling recognition",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/somandepalli19_interspeech.html": {
    "title": "Multiview Shared Subspace Learning Across Speakers and Speech Commands",
    "volume": "main",
    "abstract": "In many speech processing applications, the objective is to model different modes of variability to obtain robust speech features. In this paper, we learn speech representations in a multiview paradigm by constraining the views to known modes of variability such as speakers or spoken words. We use deep multiset canonical correlation (dMCCA) because it can model more than two views in parallel to learn a shared subspace across them. In order to model thousands of views (e.g., speakers), we demonstrate that stochastically sampling a small number of views generalizes dMCCA to the larger set of views. To evaluate our approach, we study two different aspects of the Speech Commands Dataset: variability among the speakers and speech commands. We show that, by treating observations from one mode of variability as multiple parallel views, we can learn representations that are discriminative to the other mode. We first consider different speakers as views of the same word to learn their shared subspace to represent an utterance. We then constrain the different words spoken by the same person as multiple views to learn speaker representations. Using classification and unsupervised clustering, we evaluate the efficacy of multiview representations to identify speech commands and speakers",
    "checked": true,
    "id": "1285648bc0b400cce7294b882b0e14a62c02aabf",
    "semantic_title": "multiview shared subspace learning across speakers and speech commands",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/belitz19_interspeech.html": {
    "title": "A Machine Learning Based Clustering Protocol for Determining Hearing Aid Initial Configurations from Pure-Tone Audiograms",
    "volume": "main",
    "abstract": "Of the nearly 35 million people in the USA who are hearing impaired, only an estimated 25% use hearing aids (HA). A good number of HAs are prescribed but not used partially because of the time to convergence for best operation between the audiologist and user. To improve HA retention, it is suggested that a machine learning (ML) protocol could be established which improves initial HA configurations given a user's pure-tone audiogram. This study examines a ML clustering method to predict the best initial HA fitting from a corpus of over 90,000 audiogram-fitting pairs collected from hearing centers throughout the USA. We first examine the final HA comfort targets to determine a limited number of preset configurations using several multi-dimensional clustering methods (Birch, Ward, and k-means). The goal is to reduce the amount of adjustments between the centroid, selected as a fitting configuration to represent the cluster, and the final HA configurations. This may be used to reduce the adjustment cycles for HAs or as preset starting configurations for personal sound amplification products (PSAPs). Using various classification methods, audiograms are mapped to a limited number of potential preset configurations. Finally, the average adjustment between the preset fitting targets and the final fitting targets is examined",
    "checked": true,
    "id": "39e4829812dd06a4abfee4867833eb91bc10239a",
    "semantic_title": "a machine learning based clustering protocol for determining hearing aid initial configurations from pure-tone audiograms",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nguyen19_interspeech.html": {
    "title": "Acoustic Scene Classification with Mismatched Devices Using CliqueNets and Mixup Data Augmentation",
    "volume": "main",
    "abstract": "Deep learning (DL) is key for the recent boost of acoustic scene classification (ASC) performance. Especially, convolutional neural networks (CNNs) are widely adopted with affirmed success. However, models are large and cumbersome, i.e. they have many layers, parallel branches or large ensemble of individual models. In this paper, we propose a resource-efficient model using CliqueNets for feature learning and a mixture-of-experts (MoEs) layer. CliqueNets are a recurrent feedback structure enabling feature refinement by the alternate propagation between constructed loop layers. In addition, we use mixup data augmentation to construct adversarial training examples. It is used for balancing the dataset of DCASE 2018 task 1B over the recordings of the mismatched devices A, B and C. This prevents over-fitting on the dataset of Device A, caused by the gap of data amount between the different recording devices. Experimental results show that the proposed model achieves 64.7% average classification accuracy for Device C and B, and 70.0% for Device A with less than one million of parameters",
    "checked": true,
    "id": "f8804dc626642c1ac65222accc535e8bb2b990c4",
    "semantic_title": "acoustic scene classification with mismatched devices using cliquenets and mixup data augmentation",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ahmed19_interspeech.html": {
    "title": "DeepLung: Smartphone Convolutional Neural Network-Based Inference of Lung Anomalies for Pulmonary Patients",
    "volume": "main",
    "abstract": "DeepLung is an end-to-end deep learning based audio sensing and classification framework for lung anomaly (e.g. cough, wheeze) detection for pulmonary patients from streaming audio and inertial sensor data from a chest-held smartphone. We design and develop 1-D and 2-D convolutional neural networks for DeepLung, and train them using the Interspeech 2010 Paralinguistic Challenge features. Two different audio windowing schemes: i) real-time respiration cycle based natural windowing, and ii) static length windowing are compared and experimented with. Classifiers are developed considering 2 different system architectures: i) mobile-cloud hybrid architecture, and ii) mobile in-situ architecture. Patient privacy is preserved in the phone by filtering speech with a shallow classifier. To evaluate DeepLung, a novel and rigorous lung activity dataset is made by collecting audio and inertial sensor data from more than 131 real pulmonary patients and healthy subjects and annotated accurately by professional crowdsourcing. Experimental results show that the best combination of DeepLung convolutional neural network is 15–27% more accurate when compared to a state-of-the-art smartphone based body sound detection system, with a best F1 score of 98%",
    "checked": true,
    "id": "57ea2ffe5c411a5455e1ac800029d492d778633d",
    "semantic_title": "deeplung: smartphone convolutional neural network-based inference of lung anomalies for pulmonary patients",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/moore19_interspeech.html": {
    "title": "On the Use/Misuse of the Term ‘Phoneme",
    "volume": "main",
    "abstract": "The term ‘phoneme' lies at the heart of speech science and technology, and yet it is not clear that the research community fully appreciates its meaning and implications. In particular, it is suspected that many researchers use the term in a casual sense to refer to the sounds of speech, rather than as a well defined abstract concept. If true, this means that some sections of the community may be missing an opportunity to understand and exploit the implications of this important psychological phenomenon. Here we review the correct meaning of the term ‘phoneme' and report the results of an investigation into its use/misuse in the accepted papers at INTERSPEECH-2018. It is confirmed that a significant proportion of the community (i) may not be aware of the critical difference between ‘phonetic' and ‘phonemic' levels of description, (ii) may not fully understand the significance of ‘phonemic contrast', and as a consequence, (iii) consistently misuse the term ‘phoneme'. These findings are discussed, and recommendations are made as to how this situation might be mitigated",
    "checked": true,
    "id": "eae039961febf6db4d5dcc0f40d9595a206ffaaa",
    "semantic_title": "on the use/misuse of the term 'phoneme",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2019/muckenhirn19_interspeech.html": {
    "title": "Understanding and Visualizing Raw Waveform-Based CNNs",
    "volume": "main",
    "abstract": "Modeling directly raw waveforms through neural networks for speech processing is gaining more and more attention. Despite its varied success, a question that remains is: what kind of information are such neural networks capturing or learning for different tasks from the speech signal? Such an insight is not only interesting for advancing those techniques but also for understanding better speech signal characteristics. This paper takes a step in that direction, where we develop a gradient based approach to estimate the relevance of each speech sample input on the output score. We show that analysis of the resulting \"relevance signal\" through conventional speech signal processing techniques can reveal the information modeled by the whole network. We demonstrate the potential of the proposed approach by analyzing raw waveform CNN-based phone recognition and speaker identification systems",
    "checked": true,
    "id": "8543142a2f163d474cd69ecb0b2ff93c956a3606",
    "semantic_title": "understanding and visualizing raw waveform-based cnns",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kilgour19_interspeech.html": {
    "title": "Fréchet Audio Distance: A Reference-Free Metric for Evaluating Music Enhancement Algorithms",
    "volume": "main",
    "abstract": "We propose the Fréchet Audio Distance (FAD), a novel, reference-free evaluation metric for music enhancement algorithms. We demonstrate how typical evaluation metrics for speech enhancement and blind source separation can fail to accurately measure the perceived effect of a wide variety of distortions. As an alternative, we propose adapting the Fréchet Inception Distance (FID) metric used to evaluate generative image models to the audio domain. FAD is validated using a wide variety of artificial distortions and is compared to the signal based metrics signal to distortion ratio (SDR), cosine distance, and magnitude L2 distance. We show that, with a correlation coefficient of 0.52, FAD correlates more closely with human perception than either SDR, cosine distance or magnitude L2 distance, with correlation coefficients of 0.39, -0.15 and -0.01 respectively",
    "checked": true,
    "id": "2ae702e06d524dc286f962fc7c7e1e74d9719459",
    "semantic_title": "fréchet audio distance: a reference-free metric for evaluating music enhancement algorithms",
    "citation_count": 61
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gong19_interspeech.html": {
    "title": "ReMASC: Realistic Replay Attack Corpus for Voice Controlled Systems",
    "volume": "main",
    "abstract": "This paper introduces a new database of voice recordings with the goal of supporting research on vulnerabilities and protection of voice-controlled systems (VCSs). In contrast to prior efforts, the proposed database contains both genuine voice commands and replayed recordings of such commands, collected in realistic VCSs usage scenarios and using modern voice assistant development kits. Specifically, the database contains recordings from four systems (each with a different microphone array) in a variety of environmental conditions with different forms of background noise and relative positions between speaker and device. To the best of our knowledge, this is the first publicly available database1 that has been specifically designed for the protection of state-of-the-art voice-controlled systems against various replay attacks in various conditions and environments",
    "checked": true,
    "id": "7243898e62464d15ff4c38521bd2488812c4b808",
    "semantic_title": "remasc: realistic replay attack corpus for voice controlled systems",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bt19_interspeech.html": {
    "title": "Analyzing Intra-Speaker and Inter-Speaker Vocal Tract Impedance Characteristics in a Low-Dimensional Feature Space Using t-SNE",
    "volume": "main",
    "abstract": "In an earlier study [1], we have successfully classified a vowel-gesture parameter, gamma γ(f) (relative vocal tract impedance spectrum measured using broadband signal excitation applied at the speaker's mouth during vowel phonation), via ensemble classification yielding accuracy exceeding 80% for six nominal regions of the vowel plane. In this follow-up investigation, we analyze gamma using t-SNE, a dimension reduction technique to allow visualizing gamma in low dimensional space, at two levels: inter-speaker and intra-speaker. Examining the same gamma dataset from [1], t-SNE yielded good spatial clustering in identifying the 6 different speakers with an accuracy exceeding 90%, attributable to the inter-speaker variation. Next, we further evaluated gamma of measurements only from a particular speaker in the lower dimension, which indicates intra-speaker distribution which may be associated with different measurement sessions. Using gamma may be seen as a meaningful parameter deserving further study, because it is inherently a function of the calibration load — unique for every speaker and measurement session. Because the calibration is made with the subject's mouth closed, so the measurement field during calibration is loaded solely by the impedance of the radiation field as seen at the subject's lips and baffled by the subject's face (geometrical information)",
    "checked": true,
    "id": "6818f06b552d4c8f4356eea301f5f3dc0fffe938",
    "semantic_title": "analyzing intra-speaker and inter-speaker vocal tract impedance characteristics in a low-dimensional feature space using t-sne",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lee19b_interspeech.html": {
    "title": "Directional Audio Rendering Using a Neural Network Based Personalized HRTF",
    "volume": "main",
    "abstract": "Multi-channel speech/audio separation and enhancement methods are popularly used for many speech/audio related applications. However, these methods may cause a loss of spatial cues, including the interaural time difference and interaural level difference, for further processing of monoaural signals. Thus, listeners may encounter difficulties in understanding the direction of the source signal. We present a directional audio renderer using a personalized HRTF, which is estimated by a neural network that combines DNN and CNN with anthropometric parameters and ear images of the listener. This demonstrated directional audio renderer concept aims to help foster research on audio processing for virtual reality/augmented reality to improve the quality of service of such devices",
    "checked": true,
    "id": "8b5a7fe9f8bdaac03c46cf9d0b692b7ac1c690ee",
    "semantic_title": "directional audio rendering using a neural network based personalized hrtf",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pienaar19_interspeech.html": {
    "title": "Online Speech Processing and Analysis Suite",
    "volume": "main",
    "abstract": "Proper phonological analyses, descriptions and explanations as well as gaining insight into language variation and change rely heavily upon ample and trustworthy phonetic data. Our Online Speech Processing and Analysis Suite is a positive development in just this direction",
    "checked": true,
    "id": "5920f542879256ad706a654ba6b5c926712f26a1",
    "semantic_title": "online speech processing and analysis suite",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/maurer19_interspeech.html": {
    "title": "Formant Pattern and Spectral Shape Ambiguity of Vowel Sounds, and Related Phenomena of Vowel Acoustics — Exemplary Evidence",
    "volume": "main",
    "abstract": "In the specialist literature on vowel acoustics, there is an extensive and often controversial debate on whether the primary acoustic cues of vowel quality are contained in the formant patterns or, alternatively, in the spectral shape. Yet, recent studies have shown that neither formant patterns nor spectral shapes are vowel quality-specific but that they are ambiguous because of a complex interaction between pitch and vowel-related spectral characteristics. In order to give insight into the phenomenon of formant pattern and spectral shape ambiguity of vowel sounds and its role for vowel acoustics, exemplary series of speech and of vowel sounds are presented in an online documentation, most of them selected from the Zurich Corpus. The presentation includes sound playbacks and results of an acoustic analysis (FFT spectra, LPC curves, spectrograms, f contours, formant patterns) and of a vowel recognition test. A Klatt synthesiser is also included for resynthesis and synthesis purposes. The presentation intends (i) to support researchers in their evaluation of existing and future studies, questioning whether the actual variation and pitch-dependency of the vowel spectrum is taken into account when attempting to generalise experimental results, and (ii) to support students in their acquisition of state-of-the-art knowledge of vowel acoustics",
    "checked": true,
    "id": "583fd9683e51ce0406205fa09a6fb2d5ebce4353",
    "semantic_title": "formant pattern and spectral shape ambiguity of vowel sounds, and related phenomena of vowel acoustics - exemplary evidence",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/noll19_interspeech.html": {
    "title": "Sound Tools eXtended (STx) 5.0 — A Powerful Sound Analysis Tool Optimized for Speech",
    "volume": "main",
    "abstract": "In this paper, we introduce Sound Tools eXtended (STx) version 5.0, an acoustic speech and sound processing application. STx 5.0 contains an integrated, simplified and compact GUI, specifically designed for speech analysis for phoneticians, linguists, psychologists, and researchers in related fields. It features a well structured user interface, compatibility with established tools (TextGrid [1], MAUS [2]), and top-notch signal analysis tools. STx 5.0 enables researchers as well as students to conduct advanced analysis of audio files, especially of speech recordings. STx 5.0 implements a new interface for the already established profiles in STx 5.0, which helps customize settings according to the researcher's needs",
    "checked": true,
    "id": "67097a6a2ecbefe01b11beaced95bcf91a15e3fe",
    "semantic_title": "sound tools extended (stx) 5.0 - a powerful sound analysis tool optimized for speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/eldesouki19_interspeech.html": {
    "title": "FarSpeech: Arabic Natural Language Processing for Live Arabic Speech",
    "volume": "main",
    "abstract": "This paper presents FarSpeech, QCRI's combined Arabic speech recognition, natural language processing (NLP), and dialect identification pipeline. It features modern web technologies to capture live audio, transcribes Arabic audio, NLP processes the transcripts, and identifies the dialect of the speaker. For transcription, we use QATS, which is a Kaldi-based ASR system that uses Time Delay Neural Networks (TDNN). For NLP, we use a SOTA Arabic NLP toolkit that employs various deep neural network and SVM based models. Finally, our dialect identification system uses multi-modality from both acoustic and linguistic input. FarSpeech1 presents different screens to display the transcripts, text segmentation, part-of-speech tags, recognized named entities, diacritized text, and the identified dialect of the speech",
    "checked": true,
    "id": "c6bfb1689d55e049382ba3d443a99047bdb1e8f8",
    "semantic_title": "farspeech: arabic natural language processing for live arabic speech",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/haider19_interspeech.html": {
    "title": "A System for Real-Time Privacy Preserving Data Collection for Ambient Assisted Living",
    "volume": "main",
    "abstract": "Ambient Assisted Living (AAL) technologies are being developed which could assist elderly people to live healthy and active lives. These technologies have been used to monitor people's daily exercises, consumption of calories and sleeping patterns, and to provide coaching interventions to foster positive behaviour. Speech and audio processing can be used to complement such AAL technologies to inform interventions for healthy ageing by analyzing acoustic data captured in the user's home. However, collection of data in home settings present a number of challenges. One of the most pressing challenges concerns how to manage privacy and data protection. To address this issue, we have developed a low-cost system which can extract audio features while protecting the actual spoken content upon detection of voice activity, and store audio features for further processing which offer privacy guarantees. These privacy preserving features are being tested in the context of a larger project which includes health and well-being monitoring and coaching",
    "checked": true,
    "id": "dbf8babb66c7ed4a0c89b83cb381263fee60d725",
    "semantic_title": "a system for real-time privacy preserving data collection for ambient assisted living",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gupta19c_interspeech.html": {
    "title": "NUS Speak-to-Sing: A Web Platform for Personalized Speech-to-Singing Conversion",
    "volume": "main",
    "abstract": "Singing like a professional singer is extremely appealing to the general public. However, many individuals are not able to sing like a singer who has received formal training over several years. We develop a web platform, where users can perform personalized singing synthesis. A user has to read and record the lyrics of a song in our web platform, and enjoy good quality singing vocals synthesized in his/her own voice. We perform a template-based speech-to-singing voice conversion at the backend of the web interface, that uses the prosody characteristics of the song derived from good quality singing by a trained singer and retains the speaker characteristics from the respective user. We utilize an improved temporal alignment scheme between speech and singing signals using tandem features, and employ a deep-spectral map to incorporate singing spectral characteristics into user's voice. The singing vocals are later synthesized by a vocoder. Using this web platform, we advocate that ‘everyone can sing as they desire'",
    "checked": true,
    "id": "bd1e469912eac4e50a1fd2cd63394bceb31e5e56",
    "semantic_title": "nus speak-to-sing: a web platform for personalized speech-to-singing conversion",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kaltenbacher19_interspeech.html": {
    "title": "Physiology and Physics of Voice Production",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f050d2a8c4421951bb8090fb3445659d3a0cf70f",
    "semantic_title": "physiology and physics of voice production",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schuller19_interspeech.html": {
    "title": "The INTERSPEECH 2019 Computational Paralinguistics Challenge: Styrian Dialects, Continuous Sleepiness, Baby Sounds & Orca Activity",
    "volume": "main",
    "abstract": "The INTERSPEECH 2019 Computational Paralinguistics Challenge addresses four different problems for the first time in a research competition under well-defined conditions: In the Styrian Dialects Sub-Challenge, three types of Austrian-German dialects have to be classified; in the Continuous Sleepiness Sub-Challenge, the sleepiness of a speaker has to be assessed as regression problem; in the Baby Sound Sub-Challenge, five types of infant sounds have to be classified; and in the Orca Activity Sub-Challenge, orca sounds have to be detected. We describe the Sub-Challenges and baseline feature extraction and classifiers, which include data-learnt (supervised) feature representations by the ‘usual' ComParE and BoAWfeatures, and deep unsupervised representation learning using the auDeep toolkit",
    "checked": true,
    "id": "9ddc3c65e08bcd664fec0c5a135ac02fd332d766",
    "semantic_title": "the interspeech 2019 computational paralinguistics challenge: styrian dialects, continuous sleepiness, baby sounds & orca activity",
    "citation_count": 67
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dubagunta19_interspeech.html": {
    "title": "Using Speech Production Knowledge for Raw Waveform Modelling Based Styrian Dialect Identification",
    "volume": "main",
    "abstract": "This paper addresses the Styrian Dialect sub-challenge of the INTERSPEECH 2019 Computational Paralinguistics Challenge. We treat this challenge as dialect identification with no linguistic resources/knowledge and with limited acoustic resources, and develop end-to-end raw waveform modelling based methods that incorporate knowledge related to speech production. In this direction, we investigate two methods: (a) modelling the signals after source system decomposition and (b) transferring knowledge from articulatory feature models trained on English language. Our investigations show that the proposed approaches on the ComParE 2019 Styrian dialect data yield systems that perform better than low level descriptor-based and bag-of-audio-word representation based approaches and comparable to sequence-to-sequence auto-encoder based approach",
    "checked": true,
    "id": "df97ab32b881e0da164f8113612d9d0a261d7c7d",
    "semantic_title": "using speech production knowledge for raw waveform modelling based styrian dialect identification",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/elsner19_interspeech.html": {
    "title": "Deep Neural Baselines for Computational Paralinguistics",
    "volume": "main",
    "abstract": "Detecting sleepiness from spoken language is an ambitious task, which is addressed by the Interspeech 2019 Computational Paralinguistics Challenge (ComParE). We propose an end-to-end deep learning approach to detect and classify patterns reflecting sleepiness in the human voice. Our approach is based solely on a moderately complex deep neural network architecture. It may be applied directly on the audio data without requiring any specific feature engineering, thus remaining transferable to other audio classification tasks. Nevertheless, our approach performs similar to state-of-the-art machine learning models",
    "checked": true,
    "id": "9debec42528f3bc02ff429241ecd40e29fe0c61f",
    "semantic_title": "deep neural baselines for computational paralinguistics",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kisler19_interspeech.html": {
    "title": "Styrian Dialect Classification: Comparing and Fusing Classifiers Based on a Feature Selection Using a Genetic Algorithm",
    "volume": "main",
    "abstract": "Many classifiers struggle when confronted with a high dimensional feature space like in the data sets provided for the Interspeech ComParE challenge. This is because most features do not significantly contribute to the prediction. To alleviate this problem, we propose a feature selection based on a Genetic Algorithm (GA) that uses an SVM as the fitness function. We show that this yields a reduced subset (1) which results in an Unweighted Average Recall (UAR) that beats the challenge baseline on the development set for the 3-class classification problem. Further, we extract an additional per-phoneme feature set, where the features are inspired by the ComParE features. On this set the same GA-based feature selection is performed and the resulting set is used for training in isolation (2) and in combination with the aforementioned reduced challenge features (3). Five classifiers were tested on the three subsets, namely SVMs, DNNs, GBMs, RFs, and regularized regression. All classifiers achieved a UAR above the baseline on all three sets. The best performance on set (1) was achieved by an SVM using an RBF kernel and on sets (2) and (3) by a fusion of classifiers",
    "checked": true,
    "id": "f7b40ab9d1d1af035e8bd80db941cd5ab09dbf0f",
    "semantic_title": "styrian dialect classification: comparing and fusing classifiers based on a feature selection using a genetic algorithm",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yeh19_interspeech.html": {
    "title": "Using Attention Networks and Adversarial Augmentation for Styrian Dialect Continuous Sleepiness and Baby Sound Recognition",
    "volume": "main",
    "abstract": "In this study, we present extensive attention-based networks with data augmentation methods to participate in the INTERSPEECH 2019 ComPareE Challenge, specifically the three Sub-challenges: Styrian Dialect Recognition, Continuous Sleepiness Regression, and Baby Sound Classification. For Styrian Dialect Sub-challenge, these dialects are classified into Northern Styrian (NorthernS), Urban Sytrian (UrbanS), and Eastern Styrian (EasternS). Our proposed model achieves an UAR 49.5% on the test set, which is 2.5% higher than the baseline. For Continuous Sleepiness Sub-challenge, it is defined as a regression task with score range from 1 (extremely alert) to 9 (very sleepy). In this work, our proposed architecture achieves a Spearman correlation 0.369 on the test set, which surpasses the baseline model by 0.026. For Baby Sound Sub-challenge, the infant sounds are classified into canonical babbling, non-canonical babbling, crying, laughing and junk/other, and our proposed augmentation framework achieves an UAR of 62.39% on the test set, which outperforms the baseline by about 3.7%. Overall, our analyses demonstrate that by fusing attention network models with conventional support vector machine benefits the test set robustness, and the recognition rates of these paralinguistic attributes generally improve when performing data augmentation",
    "checked": true,
    "id": "aec57052b4afc66c1db071453f67db484a4e1fa4",
    "semantic_title": "using attention networks and adversarial augmentation for styrian dialect continuous sleepiness and baby sound recognition",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wu19f_interspeech.html": {
    "title": "Ordinal Triplet Loss: Investigating Sleepiness Detection from Speech",
    "volume": "main",
    "abstract": "In this paper we present our submission to the INTERSPEECH 2019 ComParE Sleepiness challenge. By nature, the given speech dataset is an archetype of one with relatively limited samples, a complex underlying data distribution, and subjective ordinal labels. We propose a novel approach termed ordinal triplet loss (OTL) that can be readily added to any deep architecture in order to address the above data constraints. Ordinal triplet loss implicitly maps inputs into a space where similar samples are closer to each other than different ones. We demonstrate the efficacy of our approach on the aforementioned task",
    "checked": true,
    "id": "17b674d628358864ae2548eaf41ff1c9cd384d59",
    "semantic_title": "ordinal triplet loss: investigating sleepiness detection from speech",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ravi19_interspeech.html": {
    "title": "Voice Quality and Between-Frame Entropy for Sleepiness Estimation",
    "volume": "main",
    "abstract": "Sleepiness monitoring and prediction has many potential applications, such as being a safety feature in driver-assistance systems. In this study, we address the ComparE 2019 Continuous Sleepiness task of estimating the degree of sleepiness from voice data. The voice quality feature set was proposed to capture the acoustic characteristics related to the degree of sleepiness of a speaker, and between-frame entropy was proposed as an instantaneous measure of the speaking rate. An outlier elimination on the training data using between-frame entropy enhanced the system robustness in all conditions. This was followed by a regression system to predict the degree of sleepiness. Utterances were represented using i-vectors computed from voice quality features. Similar systems were also developed using mel-frequency cepstral coefficients and the ComParE16 feature set. These three systems were combined using score-level fusion. Results suggested complementarity between these feature sets. The complete system outperformed the baseline system which used the ComParE16 feature set. A relative improvement of 19.5% and 5.4% was achieved on the development and the test datasets, respectively",
    "checked": true,
    "id": "dda75ac54b6d36a143542922b348ab1d33c7ab06",
    "semantic_title": "voice quality and between-frame entropy for sleepiness estimation",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gosztolya19b_interspeech.html": {
    "title": "Using Fisher Vector and Bag-of-Audio-Words Representations to Identify Styrian Dialects, Sleepiness, Baby & Orca Sounds",
    "volume": "main",
    "abstract": "The 2019 INTERSPEECH Computational Paralinguistics Challenge (ComParE) consists of four Sub-Challenges, where the tasks are to identify different German (Austrian) dialects, estimate how sleepy the speaker is, what type of sound a given baby uttered, and whether there is a sound of an orca (killer whale) present in the recording. Following our team's last year entry, we continue our research by looking for feature set types that might be employed on a wide variety of tasks without alteration. This year, besides the standard 6373-sized ComParE functionals, we experimented with the Fisher vector representation along with the Bag-of-Audio-Words technique. To adapt Fisher vectors from the field of image processing, we utilized them on standard MFCC features instead of the originally intended SIFT attributes (which describe local objects found in the image). Our results indicate that using these feature representation techniques was indeed beneficial, as we could outperform the baseline values in three of the four Sub-Challenges; the performance of our approach seems to be even higher if we consider that the baseline scores were obtained by combining different methods as well",
    "checked": true,
    "id": "ccf9ace40abb91c9577eb42cdf27906dac29a09b",
    "semantic_title": "using fisher vector and bag-of-audio-words representations to identify styrian dialects, sleepiness, baby & orca sounds",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2019/das19b_interspeech.html": {
    "title": "Instantaneous Phase and Long-Term Acoustic Cues for Orca Activity Detection",
    "volume": "main",
    "abstract": "The orca activity detection is a challenging task that prevails in underwater acoustics. Signal level discrimination of orca activity to that of noise signal is minimum, hence a topic of interest. The orca activity detection is a subtask of Computational Paralinguistics Challenge (ComParE) 2019. In this work, we study a few novel acoustic cues based on phase and long-term information to capture the artifacts from signal to detect orca activity. The phase of signal possesses definite signal characteristics which is completely random in case of noise signal. In this regard, we investigate instantaneous phase as an artifact for orca activity detection. Additionally, we believe that the long-term features can be more helpful to detect such artifacts than the conventional short-term acoustic features. We explore these two directions along with the state-of-the-art baselines on ComParE functionals, bag-of-audio-words and auDeep features for ComParE 2019. The studies reveal that the instantaneous phase as a single feature can perform better than the fusion of three baselines given as a benchmark for the challenge. Further, we perform a score level fusion of the acoustic features and the three baselines that further enhances the performance",
    "checked": true,
    "id": "f6db69821155d48f29e6e8c420042c87fec172eb",
    "semantic_title": "instantaneous phase and long-term acoustic cues for orca activity detection",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schiller19_interspeech.html": {
    "title": "Relevance-Based Feature Masking: Improving Neural Network Based Whale Classification Through Explainable Artificial Intelligence",
    "volume": "main",
    "abstract": "Underwater sounds provide essential information for marine researchers to study sea mammals. During long-term studies large amounts of sound signals are being recorded using hydrophones. To facilitate the time consuming process of manually evaluating the recorded data, computational systems are often employed. Recent approaches utilize Convolutional Neural Networks (CNNs) to analyze spectrograms extracted from the audio signal. In this paper we explore the potential of relevance analysis to enhance the performance of existing CNN approaches. For this purpose, we present a fusion system that utilizes intermediate outputs of three state of the art CNNs, which are fine tuned to recognize whale sounds in spectrograms. Hereby we use Explainable Artificial Intelligence (XAI) to asses the relevance of each feature within the obtained representations. Based on those relevance values, we create novel masking algorithms to extract significant subsets of respective representations. These subsets are used to train an ensemble of classification systems that are serving as input for the final fusion step. We observe that a classification system can benefit from the inclusion of Relevance-based Feature Masking in terms of improved performance and reduced input dimensionality. The presented work is part of the INTERSPEECH 2019 Computational Paralinguistics Challenge",
    "checked": true,
    "id": "24eaaf22e6bbbfdd7fdf54f4c0dbf217fe6a67ce",
    "semantic_title": "relevance-based feature masking: improving neural network based whale classification through explainable artificial intelligence",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/caraty19_interspeech.html": {
    "title": "Spatial, Temporal and Spectral Multiresolution Analysis for the INTERSPEECH 2019 ComParE Challenge",
    "volume": "main",
    "abstract": "The INTERSPEECH 2019 Orca Activity Challenge consists in the detection of the Orca sounds from underwater audio signal. Orca can produce a wide variety of sounds categorized in clicks, whistles and pulsed calls. Clicks are useful for echolocation, whistles and pulsed calls are used as social signals. Experiments were conducted on DeepAL Fieldwork Data (DLFD). Underwater sounds were recorded in northern British Columbia by a hydrophones array. Recordings were labeled by marine biologists in Orca sounds or Noise. We have investigated multiresolution analysis according to the three main relevant acoustic levels: spatial, temporal and spectral. For this purpose, we studied the beamforming array analysis, the multitemporal resolution and the multilevel wavelet decomposition. For the spatial level, a beamforming algorithm was used for denoising the underwater audio signal. For the temporal level, two sets of multitemporal three-level features were extracted using pyramidal representation. For the spectral level, in order to detect transient sound, wavelet analysis was computed using various wavelet families. At last, an Orca Activity detector was designed combining ComParE set with multitemporal and multilevel wavelet features. Experiments on the Test set have shown a significant improvement of 0.051, compared to the baseline performance of the Challenge (0.866)",
    "checked": true,
    "id": "48c763de64d69a47d4d9cc1dac6953b0de1097d1",
    "semantic_title": "spatial, temporal and spectral multiresolution analysis for the interspeech 2019 compare challenge",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wu19g_interspeech.html": {
    "title": "The DKU-LENOVO Systems for the INTERSPEECH 2019 Computational Paralinguistic Challenge",
    "volume": "main",
    "abstract": "This paper introduces our approaches for the orca activity and continuous sleepiness tasks in the Interspeech ComParE Challenge 2019. For the orca activity detection task, we extract deep embeddings using several deep convolutional neural networks, followed by the Support Vector Machine (SVM) based back end classifier. Both STFT spectrogram and log mel-spectrogram are explored as input features. To increase the size of training data and deal with the data imbalance, we propose four kinds of data augmentation. We also investigate the different ways of fusion for multi-channel input data. Besides the official baseline system, to better evaluate the performance of our deep embedding system, we employ the Fisher Vector (FV) encoding on various kinds of acoustic features as an alternative baseline. Experimental results show that our proposed methods significantly outperform the baselines and achieve 0.948 AUC and 0.365 Spearman's Correlation Coefficient on the orca activity and continuous sleepiness evaluation data, respectively",
    "checked": true,
    "id": "b535aec4d232dcb409135f54698d2f81c9b32bef",
    "semantic_title": "the dku-lenovo systems for the interspeech 2019 computational paralinguistic challenge",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nandwana19_interspeech.html": {
    "title": "The VOiCES from a Distance Challenge 2019",
    "volume": "main",
    "abstract": "The VOiCES from a Distance Challenge 2019 was designed to foster research in the area of speaker recognition and automatic speech recognition (ASR) with a special focus on single-channel distant/far-field audio under various noisy conditions. The challenge was based on the recently released VOiCES corpus, with 60 international teams involved, of which 24 teams participated in the evaluation. In this paper, we separately present the challenge's speaker recognition and ASR tasks. For each task, we outline the training, development, and test data, as well as the evaluation metrics. Then, we report and discuss the results in light of the participant-provided system descriptions, to highlight the major factors contributing to high performance in distant speech processing",
    "checked": true,
    "id": "ad9ae55f07e0311fe7288855e208d9fb2a799b71",
    "semantic_title": "the voices from a distance challenge 2019",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2019/novoselov19b_interspeech.html": {
    "title": "STC Speaker Recognition Systems for the VOiCES from a Distance Challenge",
    "volume": "main",
    "abstract": "This paper presents the Speech Technology Center (STC) speaker recognition (SR) systems submitted to the VOiCES From a Distance challenge 2019. The challenge's SR task is focused on the problem of speaker recognition in single channel distant/far-field audio under noisy conditions. In this work we investigate different deep neural networks architectures for speaker embedding extraction to solve the task. We show that deep networks with residual frame level connections outperform more shallow architectures. Simple energy based speech activity detector (SAD) and automatic speech recognition (ASR) based SAD are investigated in this work. We also address the problem of data preparation for robust embedding extractors training. The reverberation for the data augmentation was performed using automatic room impulse response generator. In our systems we used discriminatively trained cosine similarity metric learning model as embedding backend. Scores normalization procedure was applied for each individual subsystem we used. Our final submitted systems were based on the fusion of different subsystems. The results obtained on the VOiCES development and evaluation sets demonstrate effectiveness and robustness of the proposed systems when dealing with distant/far-field audio under noisy conditions",
    "checked": true,
    "id": "a361525837548d4e754b11adb652f19d403d32db",
    "semantic_title": "stc speaker recognition systems for the voices from a distance challenge",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2019/matejka19_interspeech.html": {
    "title": "Analysis of BUT Submission in Far-Field Scenarios of VOiCES 2019 Challenge",
    "volume": "main",
    "abstract": "This paper is a post-evaluation analysis of our efforts in VOiCES 2019 Speaker Recognition challenge. All systems in the fixed condition are based on x-vectors with different features and DNN topologies. The single best system reaches minDCF of 0.38 (5.25% EER) and a fusion of 3 systems yields minDCF of 0.34 (4.87% EER).We also analyze how speaker verification (SV) systems evolved in last few years and show results also on SITW 2016 Challenge. EER on the core-core condition of the SITW 2016 challenge dropped from 5.85% to 1.65% for system fusions submitted for SITW 2016 and VOiCES 2019, respectively. The less restrictive open condition allowed us to use external data for PLDA adaptation and achieve additional small performance improvement. In our submission to open condition, we used three x-vector systems and also one system based on i-vectors",
    "checked": true,
    "id": "291ac9470ce55c3bf8804b0d89483bab6b7c4ba6",
    "semantic_title": "analysis of but submission in far-field scenarios of voices 2019 challenge",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2019/medennikov19_interspeech.html": {
    "title": "The STC ASR System for the VOiCES from a Distance Challenge 2019",
    "volume": "main",
    "abstract": "This paper is a description of the Speech Technology Center (STC) automatic speech recognition (ASR) system for the \"VOiCES from a Distance Challenge 2019\". We participated in the Fixed condition of the ASR task, which means that the only training data available was an 80-hour subset of the LibriSpeech corpus. The main difficulty of the challenge is a mismatch between clean training data and distant noisy development/ evaluation data. In order to tackle this, we applied room acoustics simulation and weighted prediction error (WPE) dereverberation. We also utilized well-known speaker adaptation using x-vector speaker embeddings, as well as novel room acoustics adaptation with R-vector room impulse response (RIR) embeddings. The system used a lattice-level combination of 6 acoustic models based on different pronunciation dictionaries and input features. N-best hypotheses were rescored with 3 neural network language models (NNLMs) trained on both words and sub-word units. NNLMs were also explored for out-of-vocabulary (OOV) words handling by means of artificial texts generation. The final system achieved Word Error Rate (WER) of 14.7% on the evaluation data, which is the best result in the challenge",
    "checked": true,
    "id": "d348735a8e25716ca5a3b25175b604c3acbe7b7a",
    "semantic_title": "the stc asr system for the voices from a distance challenge 2019",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chong19_interspeech.html": {
    "title": "The I2R's ASR System for the VOiCES from a Distance Challenge 2019",
    "volume": "main",
    "abstract": "This paper describes the development of the automatic speech recognition (ASR) system for the submission to the VOiCES from a Distance Challenge 2019. In this challenge, we focused on the fixed condition, where the task is to recognize reverberant and noisy speech based on a limited amount of clean training data. In our system, the mismatch between the training and testing conditions was reduced by using multi-style training where the training data was artificially contaminated with different reverberation and noise sources. Also, the Weighted Prediction Error (WPE) algorithm was used to reduce the reverberant effect in the evaluation data. To boost the system performance, acoustic models of different neural network architectures were trained and the respective systems were fused to give the final output. Moreover, an LSTM language model was used to rescore the lattice to compensate the weak n-gram model trained from only the transcription text. Evaluated on the development set, our system showed an average word error rate (WER) of 27.04%",
    "checked": true,
    "id": "2a80f285e9b18a766ea278fd46f404f2c3c98a4b",
    "semantic_title": "the i2r's asr system for the voices from a distance challenge 2019",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nandwana19b_interspeech.html": {
    "title": "The VOiCES from a Distance Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ad9ae55f07e0311fe7288855e208d9fb2a799b71",
    "semantic_title": "the voices from a distance challenge 2019",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2019/novoselov19c_interspeech.html": {
    "title": "STC Speaker Recognition Systems for the VOiCES from a Distance Challenge",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "a361525837548d4e754b11adb652f19d403d32db",
    "semantic_title": "stc speaker recognition systems for the voices from a distance challenge",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2019/matejka19b_interspeech.html": {
    "title": "Analysis of BUT Submission in Far-Field Scenarios of VOiCES 2019 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "291ac9470ce55c3bf8804b0d89483bab6b7c4ba6",
    "semantic_title": "analysis of but submission in far-field scenarios of voices 2019 challenge",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2019/medennikov19b_interspeech.html": {
    "title": "The STC ASR System for the VOiCES from a Distance Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d348735a8e25716ca5a3b25175b604c3acbe7b7a",
    "semantic_title": "the stc asr system for the voices from a distance challenge 2019",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chong19b_interspeech.html": {
    "title": "The I2R's ASR System for the VOiCES from a Distance Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2a80f285e9b18a766ea278fd46f404f2c3c98a4b",
    "semantic_title": "the i2r's asr system for the voices from a distance challenge 2019",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jati19_interspeech.html": {
    "title": "Multi-Task Discriminative Training of Hybrid DNN-TVM Model for Speaker Verification with Noisy and Far-Field Speech",
    "volume": "main",
    "abstract": "The paper aims to address the task of speaker verification with single-channel, noisy and far-field speech by learning an embedding or feature representation that is invariant to different acoustic environments. We approach from two different directions. First, we adopt a newly proposed discriminative model that hybridizes Deep Neural Network (DNN) and Total Variability Model (TVM) with the goal of integrating their strengths. DNN helps learning a unique variable length representation of the feature sequence while TVM accumulates them into a fixed dimensional vector. Second, we propose a multitask training scheme with cross entropy and triplet losses in order to obtain good classification performance as well as distinctive speaker embeddings. The multi-task training is applied on both the DNN-TVM model and state-of-the-art x-vector system. The results on the development and evaluation sets of the VOiCES challenge reveal that the proposed multi-task training helps improving models that are solely based on cross entropy, and it works better with DNN-TVM architecture than x-vector for the current task. Moreover, the multi-task models tend to show complementary relationship with cross entropy models, and thus improved performance is observed after fusion",
    "checked": true,
    "id": "e011f26bffc46af9771764366272022184ad7b68",
    "semantic_title": "multi-task discriminative training of hybrid dnn-tvm model for speaker verification with noisy and far-field speech",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/snyder19b_interspeech.html": {
    "title": "The JHU Speaker Recognition System for the VOiCES 2019 Challenge",
    "volume": "main",
    "abstract": "This paper describes the systems developed by the JHU team for the speaker recognition track of the 2019 VOiCES from a Distance Challenge. On this far-field task, we achieved good performance using systems based on state-of-the-art deep neural network (DNN) embeddings. In this paradigm, a DNN maps variable-length speech segments to speaker embeddings, called x-vectors, that are then classified using probabilistic linear discriminant analysis (PLDA). Our submissions were composed of three x-vector-based systems that differed primarily in the DNN architecture, temporal pooling mechanism, and training objective function. On the evaluation set, our best single-system submission used an extended time-delay architecture, and achieved 0.435 in actual DCF, the primary evaluation metric. A fusion of all three x-vector systems was our primary submission, and it obtained an actual DCF of 0.362",
    "checked": true,
    "id": "aa3ba08a4451871da8a2e92ab11b3ed26c292bcf",
    "semantic_title": "the jhu speaker recognition system for the voices 2019 challenge",
    "citation_count": 29
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19h_interspeech.html": {
    "title": "Intel Far-Field Speaker Recognition System for VOiCES Challenge 2019",
    "volume": "main",
    "abstract": "This paper describes Intel's speaker recognition systems for the VOiCES from a Distance Challenge 2019. Our submission consists of a Resnet50, and four Xvector systems trained with different data augmentation and input features. Our novel contributions include the use of additive margin softmax loss function and the use of invariant representation learning for some of our systems. To our knowledge, this has not been proposed for speaker recognition. We found that such complementary subsystems greatly improved the performance on the development set by late fusion on score level based on linear logistic regression. After fusion our system achieved on the development set EER, minDCF and actDCF of 2.2%, 0.27 and 0.27; and on the evaluation set 6.08%, 0.451 and 0.458, respectively. We discuss our results and give some insight on accuracy with respect to recording distance",
    "checked": true,
    "id": "095150661b2ee4abf04271399935554198f5151c",
    "semantic_title": "intel far-field speaker recognition system for voices challenge 2019",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sun19d_interspeech.html": {
    "title": "The I2R's Submission to VOiCES Distance Speaker Recognition Challenge 2019",
    "volume": "main",
    "abstract": "This paper is about the I2R's submission to the VOiCES from a distance speaker recognition challenge 2019. The submissions were based on the fusion of two x-vectors and two i-vectors subsystems. Main efforts have been focused on the frontend de-reverberation processing, PLDA backend design, score normalization and fusion studies in order to improve the system performance on single channel distant/far-field audio, under noisy conditions. We contribute to the fixed condition task under specific training and development data set. The experimental results showed that the de-reverberation approach can achieve 5% to 10% relative improvement on both EER and DCF for all subsystems and more than 10% improvement in the final fusion system on the Dev dataset and more than 15% relative improvement on the final evaluation dataset. Our final fusion system achieved about 2% EER rate and 0.240 minDCF on the Development Dataset",
    "checked": true,
    "id": "f4b602468d1f9d8b380554c14268c0d559a9c505",
    "semantic_title": "the i2r's submission to voices distance speaker recognition challenge 2019",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liang19_interspeech.html": {
    "title": "The LeVoice Far-Field Speech Recognition System for VOiCES from a Distance Challenge 2019",
    "volume": "main",
    "abstract": "This paper describes our submission to the \"VOiCES from a Distance Challenge 2019\", which is designed to foster research in the area of speaker recognition and automatic speech recognition (ASR) with a special focus on single channel distant/far-field audio under noisy conditions. We focused on the ASR task under a fixed condition in which the training data was clean and small, but the development data and test data were noisy and unmatched. Thus we developed the following major technical points for our system, which included data augmentation, weighted-prediction-error based speech enhancement, acoustic models based on different networks, TDNN or LSTM based language model rescore, and ROVER. Experiments on the development set and the evaluation set showed that the front-end processing, data augmentation and system fusion made the main contributions for the performance increasing, and the final word error rate results based on our system scored 15.91% and 19.6% respectively",
    "checked": true,
    "id": "4542fa8712f83dfea78074448e6ebbe92c98e007",
    "semantic_title": "the levoice far-field speech recognition system for voices from a distance challenge 2019",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19f_interspeech.html": {
    "title": "The JHU ASR System for VOiCES from a Distance Challenge 2019",
    "volume": "main",
    "abstract": "This paper describes the system developed by the JHU team for automatic speech recognition (ASR) of the VOiCES from a Distance Challenge 2019, focusing on single channel distant/farfield audio under noisy conditions. We participated in the Fixed Condition track, where the systems are only trained on an 80-hour subset of the Librispeech corpus provided by the organizer. The training data was first augmented with both background noises and simulated reverberation. We then trained factorized TDNN acoustic models that differed only in their use of i-vectors for adaptation. Both systems utilized RNN language models trained on original and reversed text for rescoring. We submitted three systems: the system using i-vectors with WER 19.4% on the development set, the system without i-vectors that achieved WER 19.0%, and the their lattice-level fusion with WER 17.8%. On the evaluation set, our best system achieves 23.9% WER",
    "checked": true,
    "id": "7597acccb162251283a03be367130a37e25b1c8d",
    "semantic_title": "the jhu asr system for voices from a distance challenge 2019",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cai19c_interspeech.html": {
    "title": "The DKU System for the Speaker Recognition Task of the 2019 VOiCES from a Distance Challenge",
    "volume": "main",
    "abstract": "In this paper, we present the DKU system for the speaker recognition task of the VOiCES from a distance challenge 2019. We investigate the whole system pipeline for the far-field speaker verification, including data pre-processing, short-term spectral feature representation, utterance-level speaker modeling, backend scoring, and score normalization. Our best single system employs a residual neural network trained with angular softmax loss. Also, the weighted prediction error algorithms can further improve performance. It achieves 0.3668 minDCF and 5.58% EER on the evaluation set by using a simple cosine similarity scoring. Finally, the submitted primary system obtains 0.3532 minDCF and 4.96% EER on the evaluation set",
    "checked": true,
    "id": "a0be4087fed0b7bd21c76cb81d8fc9346c4b60d8",
    "semantic_title": "the dku system for the speaker recognition task of the 2019 voices from a distance challenge",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hauptman19_interspeech.html": {
    "title": "Identifying Distinctive Acoustic and Spectral Features in Parkinson's Disease",
    "volume": "main",
    "abstract": "In this paper we try to identify spectral and acoustic features that are distinctive of Parkinson's disease patients' speech. We investigate the contribution of several features' families to a simple classification task that distinguishes between two balanced groups — patients with Parkinson's disease and their age and gender matched group of Healthy Controls, both uttering sustained vowels. We achieve over 75% correct classification using a combination of acoustic and spectral features. We show that combining a few statistical functionals of these features yields very good results. This can be explained by two reasons: the first is that the statistics of Parkinson's disease patients' speech defer from those of Healthy people's speech; the second and more important one is the gradual nature of the Parkinsonian speech that is manifested by the changes within an utterance. We speculate that the feature families that most contribute to the classification task are the most distinctive for detecting the disease and suggest testing this hypothesis by performing long-term analysis of both patient and healthy control subjects. Similar accuracy is obtained when analyzing spontaneous speech where each utterance is represented by a single normalized i-vector",
    "checked": true,
    "id": "dd96bd71150b684f85171b7e35e812afdf3d2b2c",
    "semantic_title": "identifying distinctive acoustic and spectral features in parkinson's disease",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/drioli19_interspeech.html": {
    "title": "Aerodynamics and Lumped-Masses Combined with Delay Lines for Modeling Vertical and Anterior-Posterior Phase Differences in Pathological Vocal Fold Vibration",
    "volume": "main",
    "abstract": "We discuss the representation of anterior-posterior (A-P) phase differences in vocal cord oscillations through a numerical biomechanical model involving lumped elements as well as distributed elements, i.e., delay lines. A dynamic glottal source model is illustrated in which the fold displacement along the vertical and the longitudinal dimensions is explicitly modeled by numerical waveguide components representing the propagation on the fold cover tissue. In contrast to other models of the same class, in which the reproduction of longitudinal phase differences are intrinsically impossible (e.g., in two-mass models) or not easy to control explicitly (e.g., in 3D 16-mass and multi-mass models in general), the one proposed here provides direct control over the amount of phase delay between folds oscillations at the posterior and anterior side of the glottis, while keeping the dynamic model simple and computationally efficient. The model is assessed by addressing the reproduction of typical oscillatory patterns observed in high-speed videoendoscopic data, in which A-P phase differences are observed. Experimental results are provided which demonstrate the ability of the approach to effectively reproduce different oscillatory patterns of the vocal folds",
    "checked": true,
    "id": "29e5aead68d5709908d8f16658cdade4517db529",
    "semantic_title": "aerodynamics and lumped-masses combined with delay lines for modeling vertical and anterior-posterior phase differences in pathological vocal fold vibration",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kadiri19_interspeech.html": {
    "title": "Mel-Frequency Cepstral Coefficients of Voice Source Waveforms for Classification of Phonation Types in Speech",
    "volume": "main",
    "abstract": "Voice source characteristics in different phonation types vary due to the tension of laryngeal muscles along with the respiratory effort. This study investigates the use of mel-frequency cepstral coefficients (MFCCs) derived from voice source waveforms for classification of phonation types in speech. The cepstral coefficients are computed using two source waveforms: (1) glottal flow waveforms estimated by the quasi-closed phase (QCP) glottal inverse filtering method and (2) approximate voice source waveforms obtained using the zero frequency filtering (ZFF) method. QCP estimates voice source waveforms based on the source-filter decomposition while ZFF yields source waveforms without explicitly computing the source-filter decomposition. Experiments using MFCCs computed from the two source waveforms show improved accuracy in classification of phonation types compared to the existing voice source features and conventional MFCC features. Further, it is observed that the proposed features have complimentary information to the existing features",
    "checked": true,
    "id": "1b0e69c1a6814215306d049f0b3fecce58796467",
    "semantic_title": "mel-frequency cepstral coefficients of voice source waveforms for classification of phonation types in speech",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cho19_interspeech.html": {
    "title": "Automatic Detection of Autism Spectrum Disorder in Children Using Acoustic and Text Features from Brief Natural Conversations",
    "volume": "main",
    "abstract": "Autism Spectrum Disorder (ASD) is increasingly prevalent [1], but long waitlists hinder children's access to expedient diagnosis and treatment. To begin addressing this problem, we developed an automated system to detect ASD using acoustic and text features drawn from short, unstructured conversations with naïve conversation partners (confederates). Seventy children (35 with ASD and 35 typically developing (TD)) discussed a range of generic topics (e.g., pets, family, hobbies, and sports) with confederates for approximately 5 minutes. A total of 624 features (352 acoustic + 272 text) were incorporated into a Gradient Boosting Model. To reduce dimensionality and avoid overfitting, we dropped insignificant features and applied feature reduction using Principal Component Analysis. Our final model was accurate substantially above chance levels. Predictive features were both acoustic-phonetic and lexical, from both participants and confederates. The goal of this project is to develop an automatic detection system for ASD that relies on very brief, generic, and natural conversations, which can eventually be used for ASD prescreening and triage in real-world settings such as doctor's offices and schools",
    "checked": true,
    "id": "78e41e239c82fff19419fc89f1e975b192826540",
    "semantic_title": "automatic detection of autism spectrum disorder in children using acoustic and text features from brief natural conversations",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schoentgen19_interspeech.html": {
    "title": "Analysis and Synthesis of Vocal Flutter and Vocal Jitter",
    "volume": "main",
    "abstract": "Perturbations of the strict periodicity of the glottal vibrations are relevant features of the voice quality of normophonic and dysphonic speakers. Vocal perturbations in healthy speakers are assigned different names according to the range of the typical perturbation frequencies. The objective of the presentation is to model jitter and flutter, which are in the > 20Hz and 10Hz – 20Hz range respectively, via a simulation of the fluctuations of the tension of the thyro-arytenoid muscle and compare simulated perturbations to jitter and flutter observed in vowels sustained by normophonic speakers. Perturbations of the strict periodicity of the glottal vibrations are relevant features of the voice quality of normophonic and dysphonic speakers. Vocal perturbations in healthy speakers are assigned different names according to the range of the typical perturbation frequencies. The objective of the presentation is to model jitter and flutter, which are in the > 20Hz and 10Hz – 20Hz range respectively, via a simulation of the fluctuations of the tension of the thyro-arytenoid muscle and compare simulated perturbations to jitter and flutter observed in vowels sustained by normophonic speakers",
    "checked": true,
    "id": "d21e328e26963ecdb6042d5840102f8f1de0b944",
    "semantic_title": "analysis and synthesis of vocal flutter and vocal jitter",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schaeffler19_interspeech.html": {
    "title": "Reliability of Clinical Voice Parameters Captured with Smartphones — Measurements of Added Noise and Spectral Tilt",
    "volume": "main",
    "abstract": "Smartphones have become powerful tools for data capture due to their computational power, internet connectivity, high quality sensors and user-friendly interfaces. This also makes them attractive for the recording of voice data that can be analysed for clinical or other voice health purposes. This however requires detailed assessment of the reliability of voice parameters extracted from smartphone recordings. In a previous study we analysed reliability of measures of periodicity and periodicity deviation, with very mixed results across parameters. In the present study we extended this analysis to measures of added noise and spectral tilt. We analysed systematic and random error for six frequently used acoustic parameters in clinical acoustic voice quality analysis. 22 speakers recorded sustained [a] and a short passage with a studio microphone and four popular smartphones simultaneously. Acoustic parameters were extracted with Praat and smartphone recordings were compared to the studio microphone. Results indicate a small systematic error for almost all parameters and smartphones. Random errors differed substantially between parameters. Our results suggest that extraction of acoustic voice parameters with mobile phones is not without problems and different parameters show substantial differences in reliability. Careful individual assessment of parameters is therefore recommended before use in practice",
    "checked": true,
    "id": "fc5446b152a25f37674b55ae5039c0bb2b7d5308",
    "semantic_title": "reliability of clinical voice parameters captured with smartphones - measurements of added noise and spectral tilt",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2019/moore19b_interspeech.html": {
    "title": "Say What? A Dataset for Exploring the Error Patterns That Two ASR Engines Make",
    "volume": "main",
    "abstract": "We present a new metadataset which provides insight into where and how two ASR systems make errors on several different speech datasets. By making this data readily available to researchers, we hope to stimulate research in the area of WER estimation models, in order to gain a deeper understanding of how intelligibility is encoded in speech. Using this dataset, we attempt to estimate intelligibility using a state-of-the-art model for speech quality estimation and found that this model did not work to model speech intelligibility. This finding sheds light on the relationship between how speech quality is encoded in acoustic features and how intelligibility is encoded. It shows that we have a lot more to learn in how to effectively model intelligibility. It is our hope that the metadataset we present will stimulate research into creating systems that more effectively model intelligibility",
    "checked": true,
    "id": "840dad25135d4e5f59df01b792818d507e35cf9c",
    "semantic_title": "say what? a dataset for exploring the error patterns that two asr engines make",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ward19_interspeech.html": {
    "title": "Survey Talk: Prosody Research and Applications: The State of the Art",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "2cbfa884e179ebf77c784e8e914ae3f65071a5a0",
    "semantic_title": "survey talk: prosody research and applications: the state of the art",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/roessig19_interspeech.html": {
    "title": "Dimensions of Prosodic Prominence in an Attractor Model",
    "volume": "main",
    "abstract": "Speakers of intonation languages use bundles of cues to express prosodic prominence. This work contributes further evidence for the multi-dimensionality of prosodic prominence in German reporting articulatory (3D EMA) and acoustic recordings from 27 speakers. In particular, we show that speakers use specific categorical and continuous modifications of the laryngeal system (tonal onglide) as well as continuous modifications of the supra-laryngeal system (lip aperture and tongue body position) to mark focus structure prosodically. These modifications are found between unaccented and accented but also within the group of accented words, revealing that speakers use prosodic modulations to directly encode prominence. On the basis of these findings we develop a dynamical model of prosodic patterns that is able to capture the manipulations as the modulation of an attractor landscape that is shaped by the different prosodic dimensions involved",
    "checked": true,
    "id": "bb6abf246841ed62d2668f3e2be9395ee5c0effb",
    "semantic_title": "dimensions of prosodic prominence in an attractor model",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/suni19_interspeech.html": {
    "title": "Comparative Analysis of Prosodic Characteristics Using WaveNet Embeddings",
    "volume": "main",
    "abstract": "We present a methodology for assessing similarities and differences between language varieties and dialects in terms of prosodic characteristics. A multi-speaker, multi-dialect WaveNet network is trained on low sample-rate signal retaining only prosodic characteristics of the original speech. The network is conditioned on labels related to speakers' region or dialect. The resulting conditioning embeddings are subsequently used as a multi-dimensional characteristics of different language varieties, with results consistent with dialectological studies. The method and results are illustrated on a Swedia 2000 corpus of Swedish dialectal variation",
    "checked": true,
    "id": "ed04ce2241e7fbc089fae34f0a848a6ee9f384c1",
    "semantic_title": "comparative analysis of prosodic characteristics using wavenet embeddings",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/murphy19_interspeech.html": {
    "title": "The Role of Voice Quality in the Perception of Prominence in Synthetic Speech",
    "volume": "main",
    "abstract": "This paper explores how prominence can be modelled in speech synthesis through voice quality variation. Synthetic utterances varying in voice quality (breathy, modal, tense) were generated using a glottal source model where the global waveshape parameter R was the main control parameter and f was not varied. A manipulation task perception experiment was conducted to establish perceptually salient R values in the signalling of focus. The participants were presented with mini-dialogues designed to elicit narrow focus (with different focal syllable locations) and were asked to manipulate an unknown parameter in the synthetic utterances to produce a natural response. The results showed that participants manipulated R not only in focal syllables, but also in the pre- and postfocal material. The direction of R manipulation in the focal syllables was the same across the three voice qualities — towards decreased R values (tenser phonation). The magnitude of the decrease in R was significantly less for tense voice compared to breathy and modal voice, but did not vary with the location of the focal syllable in the utterance. Overall, the results suggest that R is effective as a control parameter for modelling prominence in synthetic speech",
    "checked": true,
    "id": "9dc0bed77cc2c1cc16111fae4ba6c5d644eb37bd",
    "semantic_title": "the role of voice quality in the perception of prominence in synthetic speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/albar19_interspeech.html": {
    "title": "Phonological Awareness of French Rising Contours in Japanese Learners",
    "volume": "main",
    "abstract": "We investigate Japanese learners' ability to produce and understand the French continuative rising contour. In French, rising contours can be linked to syntactic, metrical, interactional and phrasing functions, while in Japanese, prosodic boundaries are marked with a default low tone (L%) Our main hypothesis is that Japanese learners' proficiency is linked to their phonological awareness of rising contours in French. We expect that advanced learners will be able to correctly produce rising contours in internal AP and IP positions, and even distinguish between subtle differences in rising contours We present the results from two different experiments. To test learners' ability to produce rising contours, subjects were asked to naturally reproduce utterances containing violations in certain prosodic contours. Results show that, although the task remains difficult, learners were able to correct non-rising contours to varying degrees. We then conducted a sentence completion task where subjects listened to the beginning of a statement and chose the adequate sequence of words that followed what they had heard. Results show that Japanese learners, no matter their proficiency, are not able to distinguish the different types of rising contours that are dependent on different syntactic boundaries",
    "checked": true,
    "id": "8c17bd6bb8a59056430eb17353cf8772d1219396",
    "semantic_title": "phonological awareness of french rising contours in japanese learners",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/okawa19_interspeech.html": {
    "title": "Audio Classification of Bit-Representation Waveform",
    "volume": "main",
    "abstract": "This study investigated the waveform representation for audio signal classification. Recently, many studies on audio waveform classification such as acoustic event detection and music genre classification have been published. Most studies on audio waveform classification have proposed the use of a deep learning (neural network) framework. Generally, a frequency analysis method such as Fourier transform is applied to extract the frequency or spectral information from the input audio waveform before inputting the raw audio waveform into the neural network. In contrast to these previous studies, in this paper, we propose a novel waveform representation method, in which audio waveforms are represented as a bit sequence, for audio classification. In our experiment, we compare the proposed bit representation waveform, which is directly given to a neural network, to other representations of audio waveforms such as a raw audio waveform and a power spectrum with two classification tasks: one is an acoustic event classification task and the other is a sound/music classification task. The experimental results showed that the bit representation waveform achieved the best classification performance for both the tasks",
    "checked": true,
    "id": "bc9e48aa9560b8bae68a5e491eefcd7760638789",
    "semantic_title": "audio classification of bit-representation waveform",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mulimani19_interspeech.html": {
    "title": "Locality-Constrained Linear Coding Based Fused Visual Features for Robust Acoustic Event Classification",
    "volume": "main",
    "abstract": "In this paper, a novel Fused Visual Features (FVFs) are proposed for Acoustic Event Classification (AEC) in the meeting room and office environments. The codes of Visual Features (VFs) are evaluated from row vectors and Scale Invariant Feature Transform (SIFT) vectors of the grayscale Gammatonegram of an acoustic event separately using Locality-constrained Linear Coding (LLC). Further, VFs from row vectors and SIFT vectors of the grayscale Gammatonegram are fused to get FVFs. Performance of the proposed FVFs is evaluated on acoustic events of publicly available UPC-TALP and DCASE datasets in clean and noisy conditions. Results show that proposed FVFs are robust to noise and achieve overall recognition accuracy of 96.40% and 90.45% on UPC-TALP and DCASE datasets, respectively",
    "checked": true,
    "id": "ac75d61feb768ba61afe6c77d77f1300f50c0f10",
    "semantic_title": "locality-constrained linear coding based fused visual features for robust acoustic event classification",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shen19b_interspeech.html": {
    "title": "Learning How to Listen: A Temporal-Frequential Attention Model for Sound Event Detection",
    "volume": "main",
    "abstract": "In this paper, we propose a temporal-frequential attention model for sound event detection (SED). Our network learns how to listen with two attention models: a temporal attention model and a frequential attention model. Proposed system learns when to listen using the temporal attention model while it learns where to listen on the frequency axis using the frequential attention model. With these two models, we attempt to make our system pay more attention to important frames or segments and important frequency components for sound event detection. Our proposed method is demonstrated on the task 2 of Detection and Classification of Acoustic Scenes and Events (DCASE) 2017 Challenge and outperforms state-of-the-art methods",
    "checked": true,
    "id": "411af2ef58b84a14805f3e99e505170695647a6e",
    "semantic_title": "learning how to listen: a temporal-frequential attention model for sound event detection",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ford19_interspeech.html": {
    "title": "A Deep Residual Network for Large-Scale Acoustic Scene Analysis",
    "volume": "main",
    "abstract": "Many of the recent advances in audio event detection, particularly on the AudioSet data set, have focused on improving performance using the released embeddings produced by a pre-trained model. In this work, we instead study the task of training a multi-label event classifier directly from the audio recordings of AudioSet. Using the audio recordings, not only are we able to reproduce results from prior work, we have also confirmed improvements of other proposed additions, such as an attention module. Moreover, by training the embedding network jointly with the additions, we achieve an mAP of 0.392 and an AUC of 0.971, surpassing the state of the art without transfer learning from a large data set. We also analyze the output activations of the network and find that the models are able to localize audio events when a finer time resolution is needed",
    "checked": true,
    "id": "4616ea39c9c4fcaec65b1cf4769898b441511fd0",
    "semantic_title": "a deep residual network for large-scale acoustic scene analysis",
    "citation_count": 43
  },
  "https://www.isca-speech.org/archive/interspeech_2019/reddy19b_interspeech.html": {
    "title": "Supervised Classifiers for Audio Impairments with Noisy Labels",
    "volume": "main",
    "abstract": "Voice-over-Internet-Protocol (VoIP) calls are prone to various speech impairments due to environmental and network conditions resulting in bad user experience. A reliable audio impairment classifier helps to identify the cause for bad audio quality. The user feedback after the call can act as the ground truth labels for training a supervised classifier on a large audio dataset. However, the labels are noisy as most of the users lack the expertise to precisely articulate the impairment in the perceived speech. In this paper, we analyze the effects of massive noise in labels in training dense networks and Convolutional Neural Networks (CNN) using engineered features, spectrograms and raw audio samples as inputs. We demonstrate that CNN can generalize better on the training data with a large number of noisy labels and gives remarkably higher test performance. The classifiers were trained both on randomly generated label noise and the label noise introduced by human errors. We also show that training with noisy labels requires a significant increase in the training dataset size, which is in proportion to the amount of noise in the labels",
    "checked": true,
    "id": "0af609ba9110e103623424a20699dc9139e18bb6",
    "semantic_title": "supervised classifiers for audio impairments with noisy labels",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tarantino19_interspeech.html": {
    "title": "Self-Attention for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Speech Emotion Recognition (SER) has been shown to benefit from many of the recent advances in deep learning, including recurrent based and attention based neural network architectures as well. Nevertheless, performance still falls short of that of humans. In this work, we investigate whether SER could benefit from the self-attention and global windowing of the transformer model. We show on the IEMOCAP database that this is indeed the case. Finally, we investigate whether using the distribution of, possibly conflicting, annotations in the training data, as soft targets could outperform a majority voting. We prove that this performance increases with the agreement level of the annotators",
    "checked": true,
    "id": "cd67ea63905c5e51baa980fc6b6e27b555bfe8db",
    "semantic_title": "self-attention for speech emotion recognition",
    "citation_count": 78
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nachmani19_interspeech.html": {
    "title": "Unsupervised Singing Voice Conversion",
    "volume": "main",
    "abstract": "We present a deep learning method for singing voice conversion. The proposed network is not conditioned on the text or on the notes, and it directly converts the audio of one singer to the voice of another. Training is performed without any form of supervision: no lyrics or any kind of phonetic features, no notes, and no matching samples between singers. The proposed network employs a single CNN encoder for all singers, a single WaveNet decoder, and a classifier that enforces the latent representation to be singer-agnostic. Each singer is represented by one embedding vector, which the decoder is conditioned on. In order to deal with relatively small datasets, we propose a new data augmentation scheme, as well as new training losses and protocols that are based on backtranslation. Our evaluation presents evidence that the conversion produces natural signing voices that are highly recognizable as the target singer",
    "checked": true,
    "id": "d0eb03edd253fb99230ce4490c38962543a2f4d2",
    "semantic_title": "unsupervised singing voice conversion",
    "citation_count": 44
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lee19c_interspeech.html": {
    "title": "Adversarially Trained End-to-End Korean Singing Voice Synthesis System",
    "volume": "main",
    "abstract": "In this paper, we propose an end-to-end Korean singing voice synthesis system from lyrics and a symbolic melody using the following three novel approaches: 1) phonetic enhancement masking, 2) local conditioning of text and pitch to the super-resolution network, and 3) conditional adversarial training. The proposed system consists of two main modules; a mel-synthesis network that generates a mel-spectrogram from the given input information, and a super-resolution network that upsamples the generated mel-spectrogram into a linear-spectrogram. In the mel-synthesis network, phonetic enhancement masking is applied to generate implicit formant masks solely from the input text, which enables a more accurate phonetic control of singing voice. In addition, we show that two other proposed methods — local conditioning of text and pitch, and conditional adversarial training — are crucial for a realistic generation of the human singing voice in the super-resolution process. Finally, both quantitative and qualitative evaluations are conducted, confirming the validity of all proposed methods",
    "checked": true,
    "id": "9a5e80de3216f945c992e397bfd10b364a5d6731",
    "semantic_title": "adversarially trained end-to-end korean singing voice synthesis system",
    "citation_count": 67
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yi19_interspeech.html": {
    "title": "Singing Voice Synthesis Using Deep Autoregressive Neural Networks for Acoustic Modeling",
    "volume": "main",
    "abstract": "This paper presents a method of using autoregressive neural networks for the acoustic modeling of singing voice synthesis (SVS). Singing voice differs from speech and it contains more local dynamic movements of acoustic features, e.g., vibratos. Therefore, our method adopts deep autoregressive (DAR) models to predict the F0 and spectral features of singing voice in order to better describe the dependencies among the acoustic features of consecutive frames. For F0 modeling, discretized F0 values are used and the influences of the history length in DAR are analyzed by experiments. An F0 post-processing strategy is also designed to alleviate the inconsistency between the predicted F0 contours and the F0 values determined by music notes. Furthermore, we extend the DAR model to deal with continuous spectral features, and a prenet module with self-attention layers is introduced to process historical frames. Experiments on a Chinese singing voice corpus demonstrate that our method using DARs can produce F0 contours with vibratos effectively, and can achieve better objective and subjective performance than the conventional method using recurrent neural networks (RNNs)",
    "checked": true,
    "id": "907eea00cb12fb761bf225330f98464c6ec0561d",
    "semantic_title": "singing voice synthesis using deep autoregressive neural networks for acoustic modeling",
    "citation_count": 25
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dahmani19_interspeech.html": {
    "title": "Conditional Variational Auto-Encoder for Text-Driven Expressive AudioVisual Speech Synthesis",
    "volume": "main",
    "abstract": "In recent years, the performance of speech synthesis systems has been improved thanks to deep learning-based models, but generating expressive audiovisual speech is still an open issue. The variational auto-encoders (VAE)s are recently proposed to learn latent representations of data. In this paper, we present a system for expressive text-to-audiovisual speech synthesis that learns a latent embedding space of emotions using a conditional generative model based on the variational auto-encoder framework. When conditioned on textual input, the VAE is able to learn an embedded representation that captures emotion characteristics from the signal, while being invariant to the phonetic content of the utterances. We applied this method in an unsupervised manner to generate duration, acoustic and visual features of speech. This conditional variational auto-encoder (CVAE) has been used to blend emotions together. This model was able to generate nuances of a given emotion or to generate new emotions that do not exist in our database. We conducted three perceptive experiments to evaluate our findings",
    "checked": true,
    "id": "7cc106cc903926a975653349c9b2bfb60d475890",
    "semantic_title": "conditional variational auto-encoder for text-driven expressive audiovisual speech synthesis",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ayllon19b_interspeech.html": {
    "title": "A Strategy for Improved Phone-Level Lyrics-to-Audio Alignment for Speech-to-Singing Synthesis",
    "volume": "main",
    "abstract": "Speech-to-Singing refers to techniques that transform speech to a singing voice. A major performance factor of this process relies on the precision to align the phonetic sequence of the input speech to the timing of the target singing. Unfortunately, the precision of existing techniques for phone-level lyrics-to-audio alignment has been found insufficient for this task. We propose a complete pipeline for automatic phone-level lyrics-to-audio alignment based on an HMM-based forced-aligner and singing acoustics normalization. The system obtains phone-level precision in the range of a few tens of milliseconds as we report in the objective evaluation. The subjective evaluation reveals that the smoothness of the singing voice generated with the proposed methodology was found close to the one obtained using manual alignments",
    "checked": true,
    "id": "b93578b9ade9351f96e8dfff693e2e5599729f79",
    "semantic_title": "a strategy for improved phone-level lyrics-to-audio alignment for speech-to-singing synthesis",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/biasuttolervat19_interspeech.html": {
    "title": "Modeling Labial Coarticulation with Bidirectional Gated Recurrent Networks and Transfer Learning",
    "volume": "main",
    "abstract": "In this study, we investigate how to learn labial coarticulation to generate a sparse representation of the face from speech. To do so, we experiment a sequential deep learning model, bidirectional gated recurrent networks, which have reached nice result in addressing the articulatory inversion problem and so should be able to handle coarticulation effects. As acquiring audiovisual corpora is expensive and time-consuming, we designed our solution to counteract the lack of data. Firstly, we have used phonetic information (phoneme label and respective duration) as input to ensure speaker independence, and in second hand, we have experimented around pretraining strategies to reach acceptable performances. We demonstrate how a careful initialization of the last layers of the network can greatly ease the training and help to handle coarticulation effect. This initialization relies on dimensionality reduction strategies, allowing injecting knowledge of useful latent representation of the visual data into the network. We focused on two data-driven tools (PCA and autoencoder) and one hand-crafted latent space coming from animation community, blendshapes decomposition. We have trained and evaluated the model with a corpus consisting of 4 hours of French speech, and we have gotten an average RMSE close to 1.3mm",
    "checked": true,
    "id": "2adc286cbca5d1a4e497435bb303f7ee12908c86",
    "semantic_title": "modeling labial coarticulation with bidirectional gated recurrent networks and transfer learning",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/park19e_interspeech.html": {
    "title": "SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "We present SpecAugment, a simple data augmentation method for speech recognition. SpecAugment is applied directly to the feature inputs of a neural network (i.e., filter bank coefficients). The augmentation policy consists of warping the features, masking blocks of frequency channels, and masking blocks of time steps. We apply SpecAugment on Listen, Attend and Spell networks for end-to-end speech recognition tasks. We achieve state-of-the-art performance on the LibriSpeech 960h and Switchboard 300h tasks, outperforming all prior work. On LibriSpeech, we achieve 6.8% WER on test-other without the use of a language model, and 5.8% WER with shallow fusion with a language model. This compares to the previous state-of-the-art hybrid system of 7.5% WER. For Switchboard, we achieve 7.2%/14.6% on the Switchboard/CallHome portion of the Hub5'00 test set without the use of a language model, and 6.8%/14.1% with shallow fusion, which compares to the previous state-of-the-art hybrid system at 8.3%/17.3% WER",
    "checked": true,
    "id": "b0fae9fbb4e580d92395eabafe73e317ae6510e3",
    "semantic_title": "specaugment: a simple data augmentation method for automatic speech recognition",
    "citation_count": 2445
  },
  "https://www.isca-speech.org/archive/interspeech_2019/audhkhasi19_interspeech.html": {
    "title": "Forget a Bit to Learn Better: Soft Forgetting for CTC-Based Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Prior work has shown that connectionist temporal classification (CTC)-based automatic speech recognition systems perform well when using bidirectional long short-term memory (BLSTM) networks unrolled over the whole speech utterance. This is because whole-utterance BLSTMs better capture long-term context. We hypothesize that this also leads to overfitting and propose soft forgetting as a solution. During training, we unroll the BLSTM network only over small non-overlapping chunks of the input utterance. We randomly pick a chunk size for each batch instead of a fixed global chunk size. In order to retain some utterance-level information, we encourage the hidden states of the BLSTM network to approximate those of a pre-trained whole-utterance BLSTM. Our experiments on the 300-hour English Switchboard dataset show that soft forgetting improves the word error rate (WER) above a competitive whole-utterance phone CTC BLSTM by an average of 7–9% relative. We obtain WERs of 9.1%/17.4% using speaker-independent and 8.7%/16.8% using speaker-adapted models respectively on the Hub5-2000 Switchboard/CallHome test sets. We also show that soft forgetting improves the WER when the model is used with limited temporal context for streaming recognition. Finally, we present some empirical insights into the regularization and data augmentation effects of soft forgetting",
    "checked": true,
    "id": "9f217410d90af085cda1ddb182675228195c2703",
    "semantic_title": "forget a bit to learn better: soft forgetting for ctc-based automatic speech recognition",
    "citation_count": 26
  },
  "https://www.isca-speech.org/archive/interspeech_2019/miao19_interspeech.html": {
    "title": "Online Hybrid CTC/Attention Architecture for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "The hybrid CTC/attention end-to-end automatic speech recognition (ASR) combines CTC ASR system and attention ASR system into a single neural network. Although the hybrid CTC/attention ASR system takes the advantages of both CTC and attention architectures in training and decoding, it remains challenging to be used for streaming speech recognition for its attention mechanism, CTC prefix probability and bidirectional encoder. In this paper, we propose a stable monotonic chunkwise attention (sMoChA) to stream its attention branch and a truncated CTC prefix probability (T-CTC) to stream its CTC branch. On the acoustic model side, we utilize the latency-controlled bidirectional long short-term memory (LC-BLSTM) to stream its encoder. On the joint CTC/attention decoding side, we propose the dynamic waiting joint decoding (DWDJ) algorithm to collect the decoding hypotheses from the CTC and attention branches. Through the combination of the above methods, we stream the hybrid CTC/attention ASR system without much word error rate degradation",
    "checked": true,
    "id": "7f39f8275ac2dffdfec9c8fb0ea26b5eaeda0bd7",
    "semantic_title": "online hybrid ctc/attention architecture for end-to-end speech recognition",
    "citation_count": 49
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19h_interspeech.html": {
    "title": "A Highly Efficient Distributed Deep Learning System for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Modern Automatic Speech Recognition (ASR) systems rely on distributed deep learning to for quick training completion. To enable efficient distributed training, it is imperative that the training algorithms can converge with a large mini-batch size. In this work, we discovered that Asynchronous Decentralized Parallel Stochastic Gradient Descent (ADPSGD) can work with much larger batch size than commonly used Synchronous SGD (SSGD) algorithm. On commonly used public SWB-300 and SWB-2000 ASR datasets, ADPSGD can converge with a batch size 3X as large as the one used in SSGD, thus enable training at a much larger scale. Further, we proposed a Hierarchical-ADPSGD (H-ADPSGD) system in which learners on the same computing node construct a super learner via a fast allreduce implementation, and super learners deploy ADPSGD algorithm among themselves. On a 64 Nvidia V100 GPU cluster connected via a 100Gb/s Ethernet network, our system is able to train SWB-2000 to reach a 7.6% WER on the Hub5-2000 Switchboard (SWB) test-set and a 13.2% WER on the Call-Home (CH) test-set in 5.2 hours. To the best of our knowledge, this is the fastest ASR training system that attains this level of model accuracy for SWB-2000 task to be ever reported in the literature",
    "checked": true,
    "id": "eb9ad2aab783927b0a99852a34e7033ad303e675",
    "semantic_title": "a highly efficient distributed deep learning system for automatic speech recognition",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19i_interspeech.html": {
    "title": "Knowledge Distillation for End-to-End Monaural Multi-Talker ASR System",
    "volume": "main",
    "abstract": "End-to-end models for monaural multi-speaker automatic speech recognition (ASR) have become an important and interesting approach when dealing with the multi-talker mixed speech under cocktail party scenario. However, there is still a large performance gap between the multi-speaker and single-speaker speech recognition systems. In this paper, we propose a novel framework that integrates teacher-student training with the attention-based end-to-end ASR model, which can do the knowledge distillation from the single-talker ASR system to multi-talker one effectively. First the objective function is revised to combine the knowledge from both single-talker and multi-talker labels. Then we extend the original single attention to speaker parallel attention modules in the teacher-student training based end-to-end framework to boost the performance more. Moreover, a curriculum learning strategy on the training data with an ordered signal-to-noise ratios (SNRs) is designed to obtain a further improvement. The proposed methods are evaluated on two-speaker mixed speech generated from the WSJ0 corpus, which is commonly used for this task recently. The experimental results show that the newly proposed knowledge transfer architecture with an end-to-end model can significantly improve the system performance for monaural multi-talker speech recognition, and more than 15% relative WER reduction is achieved against the traditional end-to-end model",
    "checked": true,
    "id": "bdfafbfaedb757f0d661b21cb9168ed9f01eeab9",
    "semantic_title": "knowledge distillation for end-to-end monaural multi-talker asr system",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/menne19_interspeech.html": {
    "title": "Analysis of Deep Clustering as Preprocessing for Automatic Speech Recognition of Sparsely Overlapping Speech",
    "volume": "main",
    "abstract": "Significant performance degradation of automatic speech recognition (ASR) systems is observed when the audio signal contains cross-talk. One of the recently proposed approaches to solve the problem of multi-speaker ASR is the deep clustering (DPCL) approach. Combining DPCL with a state-of-the-art hybrid acoustic model, we obtain a word error rate (WER) of 16.5% on the commonly used wsj0-2mix dataset, which is the best performance reported thus far to the best of our knowledge. The wsj0-2mix dataset contains simulated cross-talk where the speech of multiple speakers overlaps for almost the entire utterance. In a more realistic ASR scenario the audio signal contains significant portions of single-speaker speech and only part of the signal contains speech of multiple competing speakers. This paper investigates obstacles of applying DPCL as a preprocessing method for ASR in such a scenario of sparsely overlapping speech. To this end we present a data simulation approach, closely related to the wsj0-2mix dataset, generating sparsely overlapping speech datasets of arbitrary overlap ratio. The analysis of applying DPCL to sparsely overlapping speech is an important interim step between the fully overlapping datasets like wsj0-2mix and more realistic ASR datasets, such as CHiME-5 or AMI",
    "checked": true,
    "id": "ad5d4ed726c1deb8b5aea3f3e508bc2ae1656998",
    "semantic_title": "analysis of deep clustering as preprocessing for automatic speech recognition of sparsely overlapping speech",
    "citation_count": 29
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bradlow19_interspeech.html": {
    "title": "Survey Talk: Recognition of Foreign-Accented Speech: Challenges and Opportunities for Human and Computer Speech Communication",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "8f02408c16a01ff8fd30fcb2a0e11f87854f4222",
    "semantic_title": "survey talk: recognition of foreign-accented speech: challenges and opportunities for human and computer speech communication",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/novak19_interspeech.html": {
    "title": "The Effects of Time Expansion on English as a Second Language Individuals",
    "volume": "main",
    "abstract": "When speaking to second language learners, talkers often reduce their rate of speech to assist their listeners' understanding and comprehension. This study grants English as a Second Language subjects fine-grained, real-time control over the playback rates of lengthy audio tracks of conversational speech, and tests the subjects' listening comprehension at their desired playback speeds and at unmodified speeds. We find evidence that slower playback rates are preferred, but no evidence that such playback rates affect listener comprehension",
    "checked": true,
    "id": "498988512a5249cf91f1db2325351d814b5fbf36",
    "semantic_title": "the effects of time expansion on english as a second language individuals",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shi19_interspeech.html": {
    "title": "Capturing L1 Influence on L2 Pronunciation by Simulating Perceptual Space Using Acoustic Features",
    "volume": "main",
    "abstract": "Theories of second language (L2) acquisition of phonology / phonetics / pronunciation / accent often resort to the similarity/ dissimilarity between the first language (L1) and L2 sound inventories. Measuring the similarity of two speech sounds could involve many acoustic dimensions, e.g., fundamental frequency (F0), formants, duration, etc.. The measurement of the sound inventories of two languages can be further complicated by the distribution of sounds within each inventory as well as the interaction of phonology and phonetics between the two inventories. This paper attempts to propose a tentative approach to quantify similarity/dissimilarity of sound pairs between two language inventories and to incorporate phonological influence in the acoustic measures used. The language pairs studied are English and Mandarin Chinese and only their vowel inventories are considered. Mel-Frequency Cepstral Coefficients (MFCCs) are used as features, and Principle Component Analysis (PCA) is used and slightly adjusted to simulate the perceptual space. Similarity/dissimilarity of sound pairs between the language inventories are examined and potential L2 error patterns are predicted based on the proposed approach. Results showed that predicted results using the proposed approach can be well related to those by Speech Learning Model (SLM), Perceptual Assimilation Model for L2 (PAM-L2) and Native Language Magnet Model (NLM)",
    "checked": true,
    "id": "5be9a79d189b6d73f358357d197e5b352fa83884",
    "semantic_title": "capturing l1 influence on l2 pronunciation by simulating perceptual space using acoustic features",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19h_interspeech.html": {
    "title": "Cognitive Factors in Thai-Naïve Mandarin Speakers' Imitation of Thai Lexical Tones",
    "volume": "main",
    "abstract": "The present study investigated how cognitive factors, memory load and attention control, affected imitation of Thai tones by Mandarin speakers with no prior Thai experience. Mandarin speakers lengthened the syllable duration, enlarged the F0 excursion and moved some F0 max location earlier compared with the stimuli, even in the immediate imitation condition. Talker variability had a larger impact on imitation than memory load, whereas vowel variability did not have any effect. Perceptual assimilation patterns partially influenced imitation performance, suggesting phonological categorization in imitation and a perception-production link",
    "checked": true,
    "id": "eb763dda8295b013953432f2b687394acc3977da",
    "semantic_title": "cognitive factors in thai-naïve mandarin speakers' imitation of thai lexical tones",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tremblay19_interspeech.html": {
    "title": "Foreign-Language Knowledge Enhances Artificial-Language Segmentation",
    "volume": "main",
    "abstract": "This study investigates whether and how foreign-language knowledge affects the use of non-native cues in speech segmentation. It does so by testing whether Dutch listeners' French knowledge enhances their use of word-final fundamental-frequency (F0) rise — consistent with the typical French prosodic pattern — in artificial-language (AL) speech segmentation. More specifically, this study examines whether Dutch listeners with good French knowledge outperform Dutch listeners with limited French knowledge in the selection of AL words over (nonword or partword) foils, following exposure to an AL with word-final F0 rises. Dutch listeners with good French knowledge completed the AL-segmentation task from Kim et al.'s [2] word-final F0-rise condition. The results were compared to Kim et al.'s [2] Dutch listeners with limited French knowledge and Tremblay et al.'s [1] native French listeners in the same condition. Dutch listeners with good French knowledge performed more accurately than Dutch listeners with limited French knowledge but less accurately than native French listeners on trials with partword foils, with the three groups not differing on trials with nonword foils. Given these results, we propose that foreign-language knowledge can help listeners compute the conditional probability of co-occurrence of successive syllables in an AL and can thus enhance AL speech segmentation",
    "checked": true,
    "id": "575a218c44785bd82875e653f6ea5a4637d2af1f",
    "semantic_title": "foreign-language knowledge enhances artificial-language segmentation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/abujabal19_interspeech.html": {
    "title": "Neural Named Entity Recognition from Subword Units",
    "volume": "main",
    "abstract": "Named entity recognition (NER) is a vital task in spoken language understanding, which aims to identify mentions of named entities in text e.g., from transcribed speech. Existing neural models for NER rely mostly on dedicated word-level representations, which suffer from two main shortcomings. First, the vocabulary size is large, yielding large memory requirements and training time. Second, these models are not able to learn morphological or phonological representations. To remedy the above shortcomings, we adopt a neural solution based on bidirectional LSTMs and conditional random fields, where we rely on subword units, namely characters, phonemes, and bytes. For each word in an utterance, our model learns a representation from each of the subword units. We conducted experiments in a real-world large-scale setting for the use case of a voice-controlled device covering four languages with up to 5.5M utterances per language. Our experiments show that (1) with increasing training data, performance of models trained solely on subword units becomes closer to that of models with dedicated word-level embeddings (91.35 vs 93.92 F1 for English), while using a much smaller vocabulary size (332 vs 74K), (2) subword units enhance models with dedicated word-level embeddings, and (3) combining different subword units improves performance",
    "checked": true,
    "id": "80b2341a629c056bd559a7935603ff41d316b5e0",
    "semantic_title": "neural named entity recognition from subword units",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bhati19_interspeech.html": {
    "title": "Unsupervised Acoustic Segmentation and Clustering Using Siamese Network Embeddings",
    "volume": "main",
    "abstract": "Unsupervised discovery of acoustic units from the raw speech signal forms the core objective of zero-resource speech processing. It involves identifying the acoustic segment boundaries and consistently assigning unique labels to acoustically similar segments. In this work, the possible candidates for segment boundaries are identified in an unsupervised manner from the kernel Gram matrix computed from the Mel-frequency cepstral coefficients (MFCC). These segment boundary candidates are used to train a siamese network, that is intended to learn embeddings that minimize intrasegment distances and maximize the intersegment distances. The siamese embeddings capture phonetic information from longer contexts of the speech signal and enhance the intersegment discriminability. These properties make the siamese embeddings better suited for acoustic segmentation and clustering than the raw MFCC features. The Gram matrix computed from the siamese embeddings provides unambiguous evidence for boundary locations. The initial candidate boundaries are refined using this evidence, and siamese embeddings are extracted for the new acoustic segments. A graph growing approach is used to cluster the siamese embeddings, and a unique label is assigned to acoustically similar segments. The performance of the proposed method for acoustic segmentation and clustering is evaluated on Zero Resource 2017 database",
    "checked": true,
    "id": "110a9c92bd185f8cea29105cd8d7176534e3383b",
    "semantic_title": "unsupervised acoustic segmentation and clustering using siamese network embeddings",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yusuf19b_interspeech.html": {
    "title": "An Empirical Evaluation of DTW Subsampling Methods for Keyword Search",
    "volume": "main",
    "abstract": "State of the art vocabulary-independent spoken term detection methods are typically based on variants of the dynamic time warping (DTW) algorithm since DTW, being based on acoustic sequence matching, allows robust retrieval in settings with scarcity of linguistic resources. However, the DTW comes with a high computational cost which limits its practicality in a deployed server. To this end, we investigate the efficacy of subsampling and propose a neural network architecture to reduce the computational load of DTW-based keyword search. We use a time-subsampled RNN to reduce the frame rate of the document as well as the dimensionality of representation while training it to maintain the cost incurred along the DTW alignment path, thus allowing us to reduce the computational complexity (both space and time) of the search algorithm Experiments on the Turkish and Zulu limited language packs of the IARPA Babel program show that the proposed methods allow considerable reduction in CPU time (88 times) and memory usage (18 times) without significant loss in search accuracy (0.0270 ATWV). Moreover, even at very high compression levels with lower search precision, high recall rates are maintained, allowing the potential of multi-resolution search",
    "checked": true,
    "id": "2d95a960471072046008540de527f4a288c8d4b3",
    "semantic_title": "an empirical evaluation of dtw subsampling methods for keyword search",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yang19e_interspeech.html": {
    "title": "Linguistically-Informed Training of Acoustic Word Embeddings for Low-Resource Languages",
    "volume": "main",
    "abstract": "Acoustic word embeddings have been proven to be useful in query-by-example keyword search. Such embeddings are typically trained to distinguish the same word from a different word using exact orthographic representations; so, two different words will have dissimilar embeddings even if they are pronounced similarly or share the same stem. However, in real-world applications such as keyword search in low-resource languages, models are expected to find all derived and inflected forms for a certain keyword. In this paper, we address this mismatch by incorporating linguistic information when training neural acoustic word embeddings. We propose two linguistically-informed methods for training these embeddings, both of which, when we use metrics that consider non-exact matches, outperform state-of-the-art models on the Switchboard dataset. We also present results on Sinhala to show that models trained on English can be directly transferred to embed spoken words in a very different language with high accuracy",
    "checked": true,
    "id": "a7127e4a107ceae0b08c2ee62b0b4c14d32b8262",
    "semantic_title": "linguistically-informed training of acoustic word embeddings for low-resource languages",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19g_interspeech.html": {
    "title": "Multimodal Word Discovery and Retrieval with Phone Sequence and Image Concepts",
    "volume": "main",
    "abstract": "This paper demonstrates three different systems capable of performing the multimodal word discovery task. A multimodal word discovery system accepts, as input, a database of spoken descriptions of images (or a set of corresponding phone transcripts), and learns a lexicon which is a mapping from phone strings to their associated image concepts. Three systems are demonstrated: one based on a statistical machine translation (SMT) model, two based on neural machine translation (NMT). On Flickr8k, the SMT-based model performs much better than the NMT-based one, achieving a 49.6% F1 score. Finally, we apply our word discovery system to the task of image retrieval and achieve 29.1% recall@10 on the standard 1000-image Flickr8k tests set",
    "checked": true,
    "id": "dba99c0ffd69a2a438ad4281a661c0133c646f55",
    "semantic_title": "multimodal word discovery and retrieval with phone sequence and image concepts",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/boito19_interspeech.html": {
    "title": "Empirical Evaluation of Sequence-to-Sequence Models for Word Discovery in Low-Resource Settings",
    "volume": "main",
    "abstract": "Since Bahdanau et al. [1] first introduced attention for neural machine translation, most sequence-to-sequence models made use of attention mechanisms [2, 3, 4]. While they produce soft-alignment matrices that could be interpreted as alignment between target and source languages, we lack metrics to quantify their quality, being unclear which approach produces the best alignments. This paper presents an empirical evaluation of 3 of the main sequence-to-sequence models for word discovery from unsegmented phoneme sequences: CNN, RNN and Transformer-based. This task consists in aligning word sequences in a source language with phoneme sequences in a target language, inferring from it word segmentation on the target side [5]. Evaluating word segmentation quality can be seen as an extrinsic evaluation of the soft-alignment matrices produced during training. Our experiments in a low-resource scenario on Mboshi and English languages (both aligned to French) show that RNNs surprisingly outperform CNNs and Transformer for this task. Our results are confirmed by an intrinsic evaluation of alignment quality through the use Average Normalized Entropy (ANE). Lastly, we improve our best word discovery model by using an alignment entropy confidence measure that accumulates ANE over all the occurrences of a given alignment pair in the collection",
    "checked": true,
    "id": "715129cbfd4202cb7c60e6415592bc73066ab739",
    "semantic_title": "empirical evaluation of sequence-to-sequence models for word discovery in low-resource settings",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xue19b_interspeech.html": {
    "title": "Direct-Path Signal Cross-Correlation Estimation for Sound Source Localization in Reverberation",
    "volume": "main",
    "abstract": "Sound source localization (SSL) is challenging in presence of reverberation since the cross-correlation between the direct-path signals in different microphones, which indicates the spatial information of the sound source, is interfered by the reverberation signal components. A novel algorithm is proposed in this paper to estimate the cross-correlation of the direct-path speech signals, such that the robustness of SSL to reverberation can be improved. The proposed method follows a similar scheme to the multichannel linear prediction (MCLP), which is commonly used for speech dereverberation, while avoids the explicit estimation of the direct-path signal of each channel. This is achieved by revealing the relationship between the direct-path signal cross-correlation (DPCC) and the MCLP coefficient vector, and finally deriving the DPCC by using only the multichannel reverberant signals. It is also shown that the pre-whitening operation, which is widely used for SSL, can be inherently integrated into the estimated DPCC. An adaptive method is further derived to facilitate online frame-level SSL. The proposed method can be easily applied to conventional cross-correlation based SSL methods by using the DPCC rather than the full cross-correlation. Experiments conducted in various reverberant conditions demonstrate the effectiveness of the proposed method",
    "checked": true,
    "id": "6f9b4908237c968d17af540d141d82a1b8ba3903",
    "semantic_title": "direct-path signal cross-correlation estimation for sound source localization in reverberation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/grondin19_interspeech.html": {
    "title": "Multiple Sound Source Localization with SVD-PHAT",
    "volume": "main",
    "abstract": "This paper introduces a modification of phase transform on singular value decomposition (SVD-PHAT) to localize multiple sound sources. This work aims to improve localization accuracy and keeps the algorithm complexity low for real-time applications. This method relies on multiple scans of the search space, with projection of each low-dimensional observation onto orthogonal subspaces. We show that this method localizes multiple sound sources more accurately than discrete SRP-PHAT, with a reduction in the Root Mean Square Error up to 0.0395 radians",
    "checked": true,
    "id": "a22b2f00f514515d49b5218a9a81dc9dbb23776b",
    "semantic_title": "multiple sound source localization with svd-phat",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19j_interspeech.html": {
    "title": "Robust DOA Estimation Based on Convolutional Neural Network and Time-Frequency Masking",
    "volume": "main",
    "abstract": "In the scenario with noise and reverberation, the performance of current methods for direction of arrival (DOA) estimation usually degrades significantly. Inspired by the success of time-frequency masking in speech enhancement and speech separation, this paper proposes new methods to better utilize time-frequency masking in convolution neural network to improve the robustness of localization. First a mask estimation network is developed to assist DOA estimation by either appending or multiplying the estimated masks to the original input feature. Then we further propose a multi-task learning architecture to optimize the mask and DOA estimation networks jointly, and two modes are designed and compared. Experiments show that all the proposed methods have better robustness and generalization in noisy and reverberant conditions compared to the conventional methods, and the multi-task methods have the best performance among all approaches",
    "checked": true,
    "id": "f8645de56b8709d2a1db2fa9617379b3a175f01e",
    "semantic_title": "robust doa estimation based on convolutional neural network and time-frequency masking",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2019/masuyama19_interspeech.html": {
    "title": "Multichannel Loss Function for Supervised Speech Source Separation by Mask-Based Beamforming",
    "volume": "main",
    "abstract": "In this paper, we propose two mask-based beamforming methods using a deep neural network (DNN) trained by multichannel loss functions. Beamforming technique using time-frequency (TF)-masks estimated by a DNN have been applied to many applications where TF-masks are used for estimating spatial covariance matrices. To train a DNN for mask-based beamforming, loss functions designed for monaural speech enhancement/separation have been employed. Although such a training criterion is simple, it does not directly correspond to the performance of mask-based beamforming. To overcome this problem, we use multichannel loss functions which evaluate the estimated spatial covariance matrices based on the multichannel Itakura–Saito divergence. DNNs trained by the multichannel loss functions can be applied to construct several beamformers. Experimental results confirmed their effectiveness and robustness to microphone configurations",
    "checked": true,
    "id": "29ebdcdf40fb4576afa5855c6accd3d7b024bc43",
    "semantic_title": "multichannel loss function for supervised speech source separation by mask-based beamforming",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19m_interspeech.html": {
    "title": "Direction-Aware Speaker Beam for Multi-Channel Speaker Extraction",
    "volume": "main",
    "abstract": "SpeakerBeam is a state-of-the-art method for extracting a speech signal of target speaker from a mixture using an adaption utterance. The existing multi-channel SpeakerBeam utilizes the spectral features of the signals with the ignorance of the spatial discriminability of the multi-channel processing. In this paper, we tightly integrate spectral and spatial information for target speaker extraction. In the proposed scheme, a multi-channel mixture signal is firstly filtered into a set of beamformed signals using fixed beam patterns. An attention network is then designed to identify the direction of the target speaker and to combine the beamformed signals into an enhanced signal dominated by the target speaker energy. Further, SpeakerBeam inputs the enhanced signal and outputs the mask of the target speaker. Finally, the attention network and SpeakerBeam are jointly trained. Experimental results demonstrate that the proposed scheme largely improves the existing multi-channel SpeakerBeam in low signal-to-interference ratio or same-gender scenarios",
    "checked": true,
    "id": "9aa66a30628512ed596b5f6d85fb8e4905a068ff",
    "semantic_title": "direction-aware speaker beam for multi-channel speaker extraction",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ochiai19_interspeech.html": {
    "title": "Multimodal SpeakerBeam: Single Channel Target Speech Extraction with Audio-Visual Speaker Clues",
    "volume": "main",
    "abstract": "Recently, with the advent of deep learning, there has been significant progress in the processing of speech mixtures. In particular, the use of neural networks has enabled target speech extraction, which extracts speech signal of a target speaker from a speech mixture by utilizing auxiliary clue representing the characteristics of the target speaker. For example, audio clues derived from an auxiliary utterance spoken by the target speaker have been used to characterize the target speaker. Audio clues should capture the fine-grained characteristic of the target speaker's voice (e.g., pitch). Alternatively, visual clues derived from a video of the target speaker's face speaking in the mixture have also been investigated. Visual clues should mainly capture the phonetic information derived from lip movements. In this paper, we propose a novel target speech extraction scheme that combines audio and visual clues about the target speaker to take advantage of the information provided by both modalities. We introduce an attention mechanism that emphasizes the most informative speaker clue at every time frame. Experiments on mixture of two speakers demonstrated that our proposed method using audio-visual speaker clues significantly improved the extraction performance compared with the conventional methods using either audio or visual speaker clues",
    "checked": true,
    "id": "0d09d1c92ba1e0497a8414c596fe46ff512c7e1e",
    "semantic_title": "multimodal speakerbeam: single channel target speech extraction with audio-visual speaker clues",
    "citation_count": 29
  },
  "https://www.isca-speech.org/archive/interspeech_2019/germain19_interspeech.html": {
    "title": "Speech Denoising with Deep Feature Losses",
    "volume": "main",
    "abstract": "We present an end-to-end deep learning approach to denoising speech signals by processing the raw waveform directly. Given input audio containing speech corrupted by an additive background signal, the system aims to produce a processed signal that contains only the speech content. Recent approaches have shown promising results using various deep network architectures. In this paper, we propose to train a fully-convolutional context aggregation network using a deep feature loss. That loss is based on comparing the internal feature activations in a different network, trained for audio classification. Our approach outperforms the state of the art in objective speech quality metrics and in large-scale perceptual experiments with human listeners. It also outperforms an identical network trained using traditional regression losses. The advantage of the new approach is particularly pronounced for the hardest data with the most intrusive background noise, for which denoising is most needed and most challenging",
    "checked": true,
    "id": "a2b231bc08cf3d9ed66eb60e6e45a0d84e94c090",
    "semantic_title": "speech denoising with deep feature losses",
    "citation_count": 143
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19h_interspeech.html": {
    "title": "VoiceFilter: Targeted Voice Separation by Speaker-Conditioned Spectrogram Masking",
    "volume": "main",
    "abstract": "In this paper, we present a novel system that separates the voice of a target speaker from multi-speaker signals, by making use of a reference signal from the target speaker. We achieve this by training two separate neural networks: (1) A speaker recognition network that produces speaker-discriminative embeddings; (2) A spectrogram masking network that takes both noisy spectrogram and speaker embedding as input, and produces a mask. Our system significantly reduces the speech recognition WER on multi-speaker signals, with minimal WER degradation on single-speaker signals",
    "checked": true,
    "id": "8c9db2d4588bd774474bc87540bbf561c57a910f",
    "semantic_title": "voicefilter: targeted voice separation by speaker-conditioned spectrogram masking",
    "citation_count": 265
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liao19_interspeech.html": {
    "title": "Incorporating Symbolic Sequential Modeling for Speech Enhancement",
    "volume": "main",
    "abstract": "In a noisy environment, a lossy speech signal can be automatically restored by a listener if he/she knows the language well. That is, with the built-in knowledge of a \"language model\", a listener may effectively suppress noise interference and retrieve the target speech signals. Accordingly, we argue that familiarity with the underlying linguistic content of spoken utterances benefits speech enhancement (SE) in noisy environments. In this study, in addition to the conventional modeling for learning the acoustic noisy-clean speech mapping, an abstract symbolic sequential modeling is incorporated into the SE framework. This symbolic sequential modeling can be regarded as a \"linguistic constraint\" in learning the acoustic noisy-clean speech mapping function. In this study, the symbolic sequences for acoustic signals are obtained as discrete representations with a Vector Quantized Variational Autoencoder algorithm. The obtained symbols are able to capture high-level phoneme-like content from speech signals. The experimental results demonstrate that the proposed framework can obtain notable performance improvement in terms of perceptual evaluation of speech quality (PESQ) and short-time objective intelligibility (STOI) on the TIMIT dataset",
    "checked": true,
    "id": "9bb8593d0006849ff1f3c4b94265a4315dc870c6",
    "semantic_title": "incorporating symbolic sequential modeling for speech enhancement",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mowlaee19_interspeech.html": {
    "title": "Maximum a posteriori Speech Enhancement Based on Double Spectrum",
    "volume": "main",
    "abstract": "While the acoustic frequency domain has been widely used for speech enhancement, usage of the modulation domain is less common. In this paper, we investigate single-channel speech enhancement in the recently proposed Double Spectrum (DS) framework and provide insights on the statistical properties of speech and noise in the DS domain. Relying on our statistical analysis in the DS, we derive a maximum a posteriori estimator of speech in the DS domain. By means of experiments, we evaluate the speech enhancement performance of the proposed method and relevant benchmarks in the acoustic frequency and modulation domains and show that the proposed method achieves a good balance between noise attenuation and speech distortion for various SNRs and noise types",
    "checked": true,
    "id": "83fb0615e3cb21912aea23402b865016f679512a",
    "semantic_title": "maximum a posteriori speech enhancement based on double spectrum",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yao19_interspeech.html": {
    "title": "Coarse-to-Fine Optimization for Speech Enhancement",
    "volume": "main",
    "abstract": "In this paper, we propose the coarse-to-fine optimization for the task of speech enhancement. Cosine similarity loss [1] has proven to be an effective metric to measure similarity of speech signals. However, due to the large variance of the enhanced speech with even the same cosine similarity loss in high dimensional space, a deep neural network learnt with this loss might not be able to predict enhanced speech with good quality. Our coarse-to-fine strategy optimizes the cosine similarity loss for different granularities so that more constraints are added to the prediction from high dimension to relatively low dimension. In this way, the enhanced speech will better resemble the clean speech. Experimental results show the effectiveness of our proposed coarse-to-fine optimization in both discriminative models and generative models. Moreover, we apply the coarse-to-fine strategy to the adversarial loss in generative adversarial network (GAN) and propose dynamic perceptual loss, which dynamically computes the adversarial loss from coarse resolution to fine resolution. Dynamic perceptual loss further improves the accuracy and achieves state-of-the-art results compared with other generative models",
    "checked": true,
    "id": "b89ff374f139110003f9017cbb4e29c1155a5eb3",
    "semantic_title": "coarse-to-fine optimization for speech enhancement",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hui19b_interspeech.html": {
    "title": "Kernel Machines Beat Deep Neural Networks on Mask-Based Single-Channel Speech Enhancement",
    "volume": "main",
    "abstract": "We apply a fast kernel method for mask-based single-channel speech enhancement. Specifically, our method solves a kernel regression problem associated to a non-smooth kernel function (exponential power kernel) with a highly efficient iterative method (EigenPro). Due to the simplicity of this method, its hyper-parameters such as kernel bandwidth can be automatically and efficiently selected using line search with subsamples of training data. We observe an empirical correlation between the regression loss (mean square error) and regular metrics for speech enhancement. This observation justifies our training target and motivates us to achieve lower regression loss by training separate kernel models for different frequency subbands. We compare our method with the state-of-the-art deep neural networks on mask-based HINT and TIMIT. Experimental results show that our kernel method consistently outperforms deep neural networks while requiring less training time",
    "checked": true,
    "id": "6e2d850951af6b307a07374fb83c711890c8b467",
    "semantic_title": "kernel machines beat deep neural networks on mask-based single-channel speech enhancement",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/metze19_interspeech.html": {
    "title": "Survey Talk: Multimodal Processing of Speech and Language",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3619ade24e09a6d8bc7d1cb7c6ddce3405208351",
    "semantic_title": "survey talk: multimodal processing of speech and language",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shrivastava19_interspeech.html": {
    "title": "MobiVSR : Efficient and Light-Weight Neural Network for Visual Speech Recognition on Mobile Devices",
    "volume": "main",
    "abstract": "Visual speech recognition (VSR) is the task of recognizing spoken language from video input only, without any audio. VSR has many applications as an assistive technology, especially if it could be deployed in mobile devices and embedded systems. The need for intensive computational resources and large memory footprint are two major obstacles in deploying neural network models for VSR in a resource constrained environment. We propose a novel end-to-end deep neural network architecture for word level VSR called MobiVSR with a design parameter that aids in balancing the model's accuracy and parameter count. We use depthwise 3D convolution along with channel shuffling for the first time in the domain of VSR and show how it makes our model efficient. MobiVSR achieves an accuracy of 70% on a challenging Lip Reading in the Wild dataset with 6 times fewer parameters and 20 times smaller memory footprint than the current state of the art. MobiVSR can also be compressed to 6 MB by applying post training quantization",
    "checked": true,
    "id": "cf0e67c36a35e78e520817ccbffc2cf2a79f37d9",
    "semantic_title": "mobivsr : efficient and light-weight neural network for visual speech recognition on mobile devices",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kandala19_interspeech.html": {
    "title": "Speaker Adaptation for Lip-Reading Using Visual Identity Vectors",
    "volume": "main",
    "abstract": "Visual speech recognition or lip-reading suffers from high word error rate (WER) as lip-reading is based solely on articulators that are visible to the camera. Recent works mitigated this problem using complex architectures of deep neural networks. I-vector based speaker adaptation is a well known technique in ASR systems used to reduce WER on unseen speakers. In this work, we explore speaker adaptation of lip-reading models using latent identity vectors (visual i-vectors) obtained by factor analysis on visual features. In order to estimate the visual i-vectors, we employ two ways to collect sufficient statistics: first using GMM based universal background model (UBM) and second using RNN-HMM based UBM. The speaker-specific visual i-vector is given as an additional input to the hidden layers of the lip-reading model during train and test phases. On GRID corpus, use of visual i-vectors results in 15% and 10% relative improvements over current state of the art lip-reading architectures on unseen speakers using RNN-HMM and GMM based methods respectively. Furthermore, we explore the variation of WER with dimension of visual i-vectors, and with the amount of unseen speaker data required for visual i-vector estimation. We also report the results on Korean visual corpus that we created",
    "checked": true,
    "id": "cf9f309610abad174685f5bc8f80f14ab79ce689",
    "semantic_title": "speaker adaptation for lip-reading using visual identity vectors",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/koumparoulis19_interspeech.html": {
    "title": "MobiLipNet: Resource-Efficient Deep Learning Based Lipreading",
    "volume": "main",
    "abstract": "Recent works in visual speech recognition utilize deep learning advances to improve accuracy. Focus however has been primarily on recognition performance, while ignoring the computational burden of deep architectures. In this paper we address these issues concurrently, aiming at both high computational efficiency and recognition accuracy in lipreading. For this purpose, we investigate the MobileNet convolutional neural network architectures, recently proposed for image classification. In addition, we extend the 2D convolutions of MobileNets to 3D ones, in order to better model the spatio-temporal nature of the lipreading problem. We investigate two architectures in this extension, introducing the temporal dimension as part of either the depthwise or the pointwise MobileNet convolutions. To further boost computational efficiency, we also consider using pointwise convolutions alone, as well as networks operating on half the mouth region. We evaluate the proposed architectures on speaker-independent visual-only continuous speech recognition on the popular TCD-TIMIT corpus. Our best system outperforms a baseline CNN by 4.27% absolute in word error rate and over 12 times in computational efficiency, whereas, compared to a state-of-the-art ResNet, it is 37 times more efficient at a minor 0.07% absolute error rate degradation",
    "checked": true,
    "id": "442ee1d9a38309fc850455b5e63e4e1499a29a50",
    "semantic_title": "mobilipnet: resource-efficient deep learning based lipreading",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/qu19_interspeech.html": {
    "title": "LipSound: Neural Mel-Spectrogram Reconstruction for Lip Reading",
    "volume": "main",
    "abstract": "Lip reading, also known as visual speech recognition, has recently received considerable attention. Although advanced feature engineering and powerful deep neural network architectures have been proposed for this task, the performance still cannot be competitive with speech recognition tasks using the audio modality as input. This is mainly because compared with audio, visual features carry less information relevant to word recognition. For example, the voiced sound made while the vocal cords vibrate can be represented by audio but is not reflected by mouth or lip movement. In this paper, we map the sequence of mouth movement images directly to mel-spectrogram to reconstruct the speech relevant information. Our proposed architecture consists of two components: (a) the mel-spectrogram reconstruction front-end which includes an encoder-decoder architecture with attention mechanism to predict mel-spectrogram from videos; (b) the lip reading back-end consisting of convolutional layers, bi-directional gated recurrent units, and connectionist temporal classification loss, which consumes the generated mel-spectrogram representation to predict text transcriptions. The speaker-dependent evaluation results demonstrate that our proposed model not only generates quality mel-spectrograms but also outperforms state-of-the-art models on the GRID benchmark lip reading dataset, with 0.843% character error rate and 2.525% word error rate",
    "checked": true,
    "id": "dfba89013c4ca7d90d86ac91fe98586756256440",
    "semantic_title": "lipsound: neural mel-spectrogram reconstruction for lip reading",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sainath19_interspeech.html": {
    "title": "Two-Pass End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "The requirements for many applications of state-of-the-art speech recognition systems include not only low word error rate (WER) but also low latency. Specifically, for many use-cases, the system must be able to decode utterances in a streaming fashion and faster than real-time. Recently, a streaming recurrent neural network transducer (RNN-T) end-to-end (E2E) model has shown to be a good candidate for on-device speech recognition, with improved WER and latency metrics compared to conventional on-device models [1]. However, this model still lags behind a large state-of-the-art conventional model in quality [2]. On the other hand, a non-streaming E2E Listen, Attend and Spell (LAS) model has shown comparable quality to large conventional models [3]. This work aims to bring the quality of an E2E streaming model closer to that of a conventional system by incorporating a LAS network as a second-pass component, while still abiding by latency constraints. Our proposed two-pass model achieves a 17%–22% relative reduction in WER compared to RNN-T alone and increases latency by a small fraction over RNN-T",
    "checked": true,
    "id": "06183bd57548b74e38acf4733837766c9fdcdef2",
    "semantic_title": "two-pass end-to-end speech recognition",
    "citation_count": 119
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lam19_interspeech.html": {
    "title": "Extract, Adapt and Recognize: An End-to-End Neural Network for Corrupted Monaural Speech Recognition",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) in challenging conditions, such as in the presence of interfering speakers or music, remains an unsolved problem. This paper presents Extract, Adapt, and Recognize (EAR), an end-to-end neural network that allows fully learnable separation and recognition components towards optimizing the ASR criterion. In between a state-of-the-art speech separation module as an extractor and an acoustic modeling module as a recognizer, the EAR introduces an adaptor, where adapted acoustic features are learned from the separation outputs using a bi-directional long short term memory network trained to minimize the recognition loss directly. Relative to a conventional joint training model, the EAR model can achieve 8.5% to 22.3%, and 1.2% to 26.9% word error rate reductions (WERR), under various dBs of music corruption and speaker interference respectively. With speaker tracing the WERR can be further promoted to 12.4% to 29.0%",
    "checked": true,
    "id": "47f28923e2bedae71d4ff378b59a168d8fa81a20",
    "semantic_title": "extract, adapt and recognize: an end-to-end neural network for corrupted monaural speech recognition",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gowda19_interspeech.html": {
    "title": "Multi-Task Multi-Resolution Char-to-BPE Cross-Attention Decoder for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "In this paper we present a new hierarchical character to byte-pair encoding (C2B) end-to-end neural network architecture for improving the performance of attention based encoder-decoder ASR models. We explore different strategies for building the hierarchical C2B models such as building the individual blocks one at a time, as well as training the entire model as a monolith in a single step. We show that C2B model trained simultaneously with four losses, two for character and two for BPE sequences help regularize the learning of character sequences as well as BPE sequences. The proposed multi-task multi-resolution hierarchical architecture improves the WER of a small footprint bidirectional full-attention E2E model on the 960 hours LibriSpeech corpus by around 15% relative and is comparable to the state-of-the-art performance of an almost 3 times bigger model on the same dataset",
    "checked": true,
    "id": "e288f196d5deaa3d81794911944bd0af89a39158",
    "semantic_title": "multi-task multi-resolution char-to-bpe cross-attention decoder for end-to-end speech recognition",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/han19b_interspeech.html": {
    "title": "Multi-Stride Self-Attention for Speech Recognition",
    "volume": "main",
    "abstract": "In contrast to the huge success of self-attention based neural networks in various NLP tasks, the efficacy of self-attention in speech applications is yet limited. This is partly because the full effectiveness of the self-attention mechanism could not be achieved without proper down-sampling schemes in speech tasks. To address this issue, we propose a new self-attention mechanism suitable for speech recognition, namely, multi-stride self-attention. The proposed multi-stride approach lets each group of heads in self-attention process speech frames with a unique stride over neighboring frames. Thus, the entire attention mechanism would not be confined in a fixed frame shift and can have diverse contextual views for a given frame to determine attention weights more effectively. To validate our proposal we evaluated it on various speech corpora for speech recognition, both English and Chinese, and observed a consistent improvement, especially in terms of substitution and deletion errors, without the increase of model complexity. The average WER improvement of 7.5% (relative) obtained by the TDNNs having the multi-stride self-attention layer as compared to the baseline TDNN model shows the effectiveness of the proposed multi-stride self-attention mechanism",
    "checked": true,
    "id": "155810b46c8cbc7e393786505f6768d187116aec",
    "semantic_title": "multi-stride self-attention for speech recognition",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hu19b_interspeech.html": {
    "title": "LF-MMI Training of Bayesian and Gaussian Process Time Delay Neural Networks for Speech Recognition",
    "volume": "main",
    "abstract": "Discriminative training techniques define state-of-the-art performance for deep neural networks (DNNs) based speech recognition systems across a wide range of tasks. Conventional discriminative training methods produce deterministic DNN parameter estimates. They are inherently prone to overfitting, leading to poor generalization when given limited training data. In order to address this issue, this paper investigates the use of Bayesian learning and Gaussian Process (GP) based hidden activations to replace the deterministic parameter estimates of standard lattice-free maximum mutual information (LF-MMI) criterion trained time delay neural network (TDNN) acoustic models. Experiments conducted on the Switchboard conversational telephone speech recognition tasks suggest the proposed technique consistently outperforms the baseline LF-MMI trained TDNN systems using fixed parameter hidden activations",
    "checked": true,
    "id": "8969807b10218e2eb76f57c1628311f6adbdbd3e",
    "semantic_title": "lf-mmi training of bayesian and gaussian process time delay neural networks for speech recognition",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lu19c_interspeech.html": {
    "title": "Self-Teaching Networks",
    "volume": "main",
    "abstract": "We propose self-teaching networks to improve the generalization capacity of deep neural networks. The idea is to generate soft supervision labels using the output layer for training the lower layers of the network. During the network training, we seek an auxiliary loss that drives the lower layer to mimic the behavior of the output layer. The connection between the two network layers through the auxiliary loss can help the gradient flow, which works similar to the residual networks. Furthermore, the auxiliary loss also works as a regularizer, which improves the generalization capacity of the network. We evaluated the self-teaching network with deep recurrent neural networks on speech recognition tasks, where we trained the acoustic model using 30 thousand hours of data. We tested the acoustic model using data collected from 4 scenarios. We show that the self-teaching network can achieve consistent improvements and outperform existing methods such as label smoothing and confidence penalization",
    "checked": true,
    "id": "925e7c48d52802459f6caadbdc33748ae05c8093",
    "semantic_title": "self-teaching networks",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19n_interspeech.html": {
    "title": "Improved End-to-End Speech Emotion Recognition Using Self Attention Mechanism and Multitask Learning",
    "volume": "main",
    "abstract": "Accurately recognizing emotion from speech is a necessary yet challenging task due to the variability in speech and emotion. In this paper, we propose a speech emotion recognition (SER) method using end-to-end (E2E) multitask learning with self attention to deal with several issues. First, we extract features directly from speech spectrogram instead of using traditional hand-crafted features to better represent emotion. Second, we adopt self attention mechanism to focus on the salient periods of emotion in speech utterances. Finally, giving consideration to mutual features between emotion and gender classification tasks, we incorporate gender classification as an auxiliary task by using multitask learning to share useful information with emotion classification task. Evaluation on IEMOCAP (a commonly used database for SER research) demonstrates that the proposed method outperforms the state-of-the-art methods and improves the overall accuracy by an absolute of 7.7% compared to the best existing result",
    "checked": true,
    "id": "edd7ec0207340f6b3f3b723882f1f8f45f76ad3d",
    "semantic_title": "improved end-to-end speech emotion recognition using self attention mechanism and multitask learning",
    "citation_count": 100
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schmitt19_interspeech.html": {
    "title": "Continuous Emotion Recognition in Speech — Do We Need Recurrence?",
    "volume": "main",
    "abstract": "Emotion recognition in speech is a meaningful task in affective computing and human-computer interaction. As human emotion is a frequently changing state, it is usually represented as a densely sampled time series of emotional dimensions, typically arousal and valence. For this, recurrent neural network (RNN) architectures are employed by default when it comes to modelling the contours with deep learning approaches. However, the amount of temporal context required is questionable, and it has not yet been clarified whether the consideration of long-term dependencies is actually beneficial. In this contribution, we demonstrate that RNNs are not necessary to accomplish the task of time-continuous emotion recognition. Indeed, results gained indicate that deep neural networks incorporating less complex convolutional layers can provide more accurate models. We highlight the pros and cons of recurrent and non-recurrent approaches and evaluate our methods on the public SEWA database, which was used as a benchmark in the 2017 and 2018 editions of the Audio-Visual Emotion Challenge",
    "checked": true,
    "id": "d60827fa02fc5e7ea935f77bc9cc45ba045e1fa1",
    "semantic_title": "continuous emotion recognition in speech - do we need recurrence?",
    "citation_count": 30
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ouyang19_interspeech.html": {
    "title": "Speech Based Emotion Prediction: Can a Linear Model Work?",
    "volume": "main",
    "abstract": "Speech based continuous emotion prediction systems have predominantly been based on complex non-linear back-ends, with an increasing attention on long-short term memory recurrent neural networks. While this has led to accurate predictions, complex models may suffer from issues with interpretability, model selection and overfitting. In this paper, we demonstrate that a linear model can capture most of the relationship between speech features and emotion labels in the continuous arousal-valence space. Specifically, an autoregressive exogenous model (ARX) is shown to be an effective backend. This approach is validated on three commonly used databases, namely RECOLA, SEWA and USC CreativeIT, and shown to be comparable in terms of performance to state-of-the-art LSTM systems. More importantly, this approach allows for the use of well-established linear system theory to aid with model interpretability",
    "checked": true,
    "id": "ab563d2c8e2578f4c0a36504e27122104e801570",
    "semantic_title": "speech based emotion prediction: can a linear model work?",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ando19_interspeech.html": {
    "title": "Speech Emotion Recognition Based on Multi-Label Emotion Existence Model",
    "volume": "main",
    "abstract": "This paper presents a novel speech emotion recognition method that addresses the ambiguous nature of emotions in speech. Most conventional methods assume there is only a single ground truth, the dominant emotion, though utterances can contain multiple emotions. In order to solve this problem, several methods that consider ambiguous emotions (e.g. soft-target training) have been proposed. Unfortunately, training them is difficult since they work by estimating the proportions of all emotions. The proposed method improves both frameworks by evaluating the presence or absence of each emotion. We expect that it is much easier to estimate just presence/absence of emotions rather than trying to determine proportions of each, and the deliberate assessment of emotion existence information will help to estimate the proportion of each or dominant class more precisely. The proposed method employs two-step training. Multi-Label Emotion Existence (MLEE) model is trained first to estimate whether each emotion is present or absent. Then, the dominant emotion recognition model with hard- or soft-target labels is trained by means of the intermediate outputs of the MLEE model so as to utilize cues of emotion existence for inferring the dominant. Experiments demonstrate that the proposed method outperforms both hard- or soft-target based conventional emotion recognition schemes",
    "checked": true,
    "id": "26c0433ab78f89711339af7b0fd10ea489a44d83",
    "semantic_title": "speech emotion recognition based on multi-label emotion existence model",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gorrostieta19_interspeech.html": {
    "title": "Gender De-Biasing in Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Machine learning can unintentionally encode and amplify negative bias and stereotypes present in humans, be they conscious or unconscious. This has led to high-profile cases where machine learning systems have been found to exhibit bias towards gender, race, and ethnicity, among other demographic categories. Negative bias can be encoded in these algorithms based on: the representation of different population categories in the training data; bias arising from manual human labeling of these data; as well as modeling types and optimisation approaches. In this paper we assess the effect of gender bias in speech emotion recognition and find that emotional activation model accuracy is consistently lower for female compared to male audio samples. Further, we demonstrate that a fairer and more consistent model accuracy can be achieved by applying a simple de-biasing training technique",
    "checked": true,
    "id": "12772d8732bceb0613b70e834689fda9395419a3",
    "semantic_title": "gender de-biasing in speech emotion recognition",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bao19_interspeech.html": {
    "title": "CycleGAN-Based Emotion Style Transfer as Data Augmentation for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Cycle consistent adversarial networks (CycleGAN) have shown great success in image style transfer with unpaired datasets. Inspired by this, we investigate emotion style transfer to generate synthetic data, which aims at addressing the data scarcity problem in speech emotion recognition. Specifically, we propose a CycleGAN-based method to transfer feature vectors extracted from a large unlabeled speech corpus into synthetic features representing the given target emotions. We extend the CycleGAN framework with a classification loss which improves the discriminability of the generated data. To show the effectiveness of the proposed method, we present results for speech emotion recognition using the generated feature vectors as (i) augmentation of the training data, and (ii) as standalone training set. Our experimental results reveal that when utilizing synthetic feature vectors, the classification performance improves in within-corpus and cross-corpus evaluation",
    "checked": true,
    "id": "8efcf96dcb93f992025e5cb5ad9d72b1e1795481",
    "semantic_title": "cyclegan-based emotion style transfer as data augmentation for speech emotion recognition",
    "citation_count": 55
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bollepalli19_interspeech.html": {
    "title": "Lombard Speech Synthesis Using Transfer Learning in a Tacotron Text-to-Speech System",
    "volume": "main",
    "abstract": "Currently, there is increasing interest to use sequence-to-sequence models in text-to-speech (TTS) synthesis with attention like that in Tacotron models. These models are end-to-end, meaning that they learn both co-articulation and duration properties directly from text and speech. Since these models are entirely data-driven, they need large amounts of data to generate synthetic speech of good quality. However, in challenging speaking styles, such as Lombard speech, it is difficult to record sufficiently large speech corpora. Therefore, we propose a transfer learning method to adapt a TTS system of normal speaking style to Lombard style. We also experiment with a WaveNet vocoder along with a traditional vocoder (WORLD) in the synthesis of Lombard speech. The subjective and objective evaluation results indicated that the proposed adaptation system coupled with the WaveNet vocoder clearly outperformed the conventional deep neural network based TTS system in the synthesis of Lombard speech",
    "checked": true,
    "id": "d3fe09ff06cc6494132cd2471f8ddda14a10c49d",
    "semantic_title": "lombard speech synthesis using transfer learning in a tacotron text-to-speech system",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/seshadri19_interspeech.html": {
    "title": "Augmented CycleGANs for Continuous Scale Normal-to-Lombard Speaking Style Conversion",
    "volume": "main",
    "abstract": "Lombard speech is a speaking style associated with increased vocal effort that is naturally used by humans to improve intelligibility in the presence of noise. It is hence desirable to have a system capable of converting speech from normal to Lombard style. Moreover, it would be useful if one could adjust the degree of Lombardness in the converted speech so that the system is more adaptable to different noise environments. In this study, we propose the use of recently developed Augmented cycle-consistent adversarial networks (Augmented CycleGANs) for conversion between normal and Lombard speaking styles. The proposed system gives a smooth control on the degree of Lombardness of the mapped utterances by traversing through different points in the latent space of the trained model. We utilize a parametric approach that uses the Pulse Model in Log domain (PML) vocoder to extract features from normal speech that are then mapped to Lombard-style features using the Augmented CycleGAN. Finally, the mapped features are converted to Lombard speech with PML. The model is trained on multi-language data recorded in different noise conditions, and we compare its effectiveness to a previously proposed CycleGAN system in experiments for intelligibility and quality of mapped speech",
    "checked": true,
    "id": "a7adc1dc73e5e09b874068fca12aba197abd6371",
    "semantic_title": "augmented cyclegans for continuous scale normal-to-lombard speaking style conversion",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhao19f_interspeech.html": {
    "title": "Foreign Accent Conversion by Synthesizing Speech from Phonetic Posteriorgrams",
    "volume": "main",
    "abstract": "Methods for foreign accent conversion (FAC) aim to generate speech that sounds similar to a given non-native speaker but with the accent of a native speaker. Conventional FAC methods borrow excitation information (F0 and aperiodicity; produced by a conventional vocoder) from a reference (i.e., native) utterance during synthesis time. As such, the generated speech retains some aspects of the voice quality of the native speaker. We present a framework for FAC that eliminates the need for conventional vocoders (e.g., STRAIGHT, World) and therefore the need to use the native speaker's excitation. Our approach uses an acoustic model trained on a native speech corpus to extract speaker-independent phonetic posteriorgrams (PPGs), and then train a speech synthesizer to map PPGs from the non-native speaker into the corresponding spectral features, which in turn are converted into the audio waveform using a high-quality neural vocoder. At runtime, we drive the synthesizer with the PPG extracted from a native reference utterance. Listening tests show that the proposed system produces speech that sounds more clear, natural, and similar to the non-native speaker compared with a baseline system, while significantly reducing the perceived foreign accent of non-native utterances",
    "checked": true,
    "id": "8386d03827eabc8446883cd16e46ef10d3d318d4",
    "semantic_title": "foreign accent conversion by synthesizing speech from phonetic posteriorgrams",
    "citation_count": 39
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shankar19b_interspeech.html": {
    "title": "A Multi-Speaker Emotion Morphing Model Using Highway Networks and Maximum Likelihood Objective",
    "volume": "main",
    "abstract": "We introduce a new model for emotion conversion in speech based on highway neural networks. Our model uses the contextual pitch, energy and spectral information of a source emotional utterance to predict the framewise fundamental frequency and signal intensity under a target emotion. We also incorporate a latent gender representation to promote cross-speaker generalizability. Our neural network is trained to maximize the error log-likelihood under an assumed Laplacian distribution. We validate our model on the VESUS repository collected at Johns Hopkins University, which contains parallel emotional utterances from 10 actors across 5 emotional classes. The proposed algorithm outperforms three state-of-the-art baselines in terms of the mean absolute error and correlation between the predicted and target values. We evaluate the quality of our emotion manipulations via crowd-sourcing. Finally, we apply our emotion morphing model to utterances generated by Wavenet to demonstrate our unique ability to inject emotion into synthetic speech",
    "checked": true,
    "id": "a0eb25728978a262350e196468e76bf532878780",
    "semantic_title": "a multi-speaker emotion morphing model using highway networks and maximum likelihood objective",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lapidot19_interspeech.html": {
    "title": "Effects of Waveform PMF on Anti-Spoofing Detection",
    "volume": "main",
    "abstract": "In the context of detection of speaker recognition identity impersonation, we observed that the waveform probability mass function (PMF) of genuine speech differs from significantly of of PMF from identity theft extracts. This is true for synthesized or converted speech as well as for replayed speech. In this work, we mainly ask whether this observation has a significant impact on spoofing detection performance. In a second step, we want to reduce the distribution gap of waveforms between authentic speech and spoofing speech. We propose a genuinization of the spoofing speech (by analogy with Gaussianisation), i.e. to obtain spoofing speech with a PMF close to the PMF of genuine speech. Our genuinization is evaluated on ASVspoof 2019 challenge datasets, using the baseline system provided by the challenge organization. In the case of constant Q cepstral coefficients (CQCC) features, the genuinization leads to a degradation of the baseline system performance by a factor of 10, which shows a potentially large impact of the distribution os waveforms on spoofing detection performance. However, by \"playing\" with all configurations, we also observed different behaviors, including performance improvements in specific cases. This leads us to conclude that waveform distribution plays an important role and must be taken into account by anti-spoofing systems",
    "checked": true,
    "id": "8e1b097f661de5ff8fc1d8ec5116d0593f3f9b15",
    "semantic_title": "effects of waveform pmf on anti-spoofing detection",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gao19b_interspeech.html": {
    "title": "Nonparallel Emotional Speech Conversion",
    "volume": "main",
    "abstract": "We propose a nonparallel data-driven emotional speech conversion method. It enables the transfer of emotion-related characteristics of a speech signal while preserving the speaker's identity and linguistic content. Most existing approaches require parallel data and time alignment, which is not available in many real applications. We achieve nonparallel training based on an unsupervised style transfer technique, which learns a translation model between two distributions instead of a deterministic one-to-one mapping between paired examples. The conversion model consists of an encoder and a decoder for each emotion domain. We assume that the speech signal can be decomposed into an emotion-invariant content code and an emotion-related style code in latent space. Emotion conversion is performed by extracting and recombining the content code of the source speech and the style code of the target emotion. We tested our method on a nonparallel corpora with four emotions. The evaluation results show the effectiveness of our approach",
    "checked": true,
    "id": "6b535318781a4c895e3503c3e97f5e6ed324c49a",
    "semantic_title": "nonparallel emotional speech conversion",
    "citation_count": 57
  },
  "https://www.isca-speech.org/archive/interspeech_2019/stafylakis19_interspeech.html": {
    "title": "Self-Supervised Speaker Embeddings",
    "volume": "main",
    "abstract": "Contrary to i-vectors, speaker embeddings such as x-vectors are incapable of leveraging unlabelled utterances, due to the classification loss over training speakers. In this paper, we explore an alternative training strategy to enable the use of unlabelled utterances in training. We propose to train speaker embedding extractors via reconstructing the frames of a target speech segment, given the inferred embedding of another speech segment of the same utterance. We do this by attaching to the standard speaker embedding extractor a decoder network, which we feed not merely with the speaker embedding, but also with the estimated phone sequence of the target frame sequence The reconstruction loss can be used either as a single objective, or be combined with the standard speaker classification loss. In the latter case, it acts as a regularizer, encouraging generalizability to speakers unseen during training. In all cases, the proposed architectures are trained from scratch and in an end-to-end fashion. We demonstrate the benefits from the proposed approach on the VoxCeleb and Speakers in the Wild Databases, and we report notable improvements over the baseline",
    "checked": true,
    "id": "54eef13b3fe487cc48a74faf8f486a312f545cc9",
    "semantic_title": "self-supervised speaker embeddings",
    "citation_count": 39
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nautsch19b_interspeech.html": {
    "title": "Privacy-Preserving Speaker Recognition with Cohort Score Normalisation",
    "volume": "main",
    "abstract": "In many voice biometrics applications there is a requirement to preserve privacy, not least because of the recently enforced General Data Protection Regulation (GDPR). Though progress in bringing privacy preservation to voice biometrics is lagging behind developments in other biometrics communities, recent years have seen rapid progress, with secure computation mechanisms such as homomorphic encryption being applied successfully to speaker recognition. Even so, the computational overhead incurred by processing speech data in the encrypted domain is substantial. While still tolerable for single biometric comparisons, most state-of-the-art systems perform some form of cohort-based score normalisation, requiring many thousands of biometric comparisons. The computational overhead is then prohibitive, meaning that one must accept either degraded performance (no score normalisation) or potential for privacy violations. This paper proposes the first computationally feasible approach to privacy-preserving cohort score normalisation. Our solution is a cohort pruning scheme based on secure multi-party computation which enables privacy-preserving score normalisation using probabilistic linear discriminant analysis (PLDA) comparisons. The solution operates upon binary voice representations. While the binarisation is lossy in biometric rank-1 performance, it supports computationally-feasible biometric rank-n comparisons in the encrypted domain",
    "checked": true,
    "id": "67e36a050f17446868a35933c85913e4aa62768c",
    "semantic_title": "privacy-preserving speaker recognition with cohort score normalisation",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19f_interspeech.html": {
    "title": "Large Margin Softmax Loss for Speaker Verification",
    "volume": "main",
    "abstract": "In neural network based speaker verification, speaker embedding is expected to be discriminative between speakers while the intra-speaker distance should remain small. A variety of loss functions have been proposed to achieve this goal. In this paper, we investigate the large margin softmax loss with different configurations in speaker verification. Ring loss and minimum hyperspherical energy criterion are introduced to further improve the performance. Results on VoxCeleb show that our best system outperforms the baseline approach by 15% in EER, and by 13%, 33% in minDCF08 and minDCF10, respectively",
    "checked": true,
    "id": "5b65716709a2a7a4da2e2aeb611f82e7aacfbbf0",
    "semantic_title": "large margin softmax loss for speaker verification",
    "citation_count": 114
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hajavi19_interspeech.html": {
    "title": "A Deep Neural Network for Short-Segment Speaker Recognition",
    "volume": "main",
    "abstract": "Today's interactive devices such as smart-phone assistants and smart speakers often deal with short-duration speech segments. As a result, speaker recognition systems integrated into such devices will be much better suited with models capable of performing the recognition task with short-duration utterances. In this paper, a new deep neural network, UtterIdNet, capable of performing speaker recognition with short speech segments is proposed. Our proposed model utilizes a novel architecture that makes it suitable for short-segment speaker recognition through an efficiently increased use of information in short speech segments. UtterIdNet has been trained and tested on the VoxCeleb datasets, the latest benchmarks in speaker recognition. Evaluations for different segment durations show consistent and stable performance for short segments, with significant improvement over the previous models for segments of 2 seconds, 1 second, and especially sub-second durations (250 ms and 500 ms)",
    "checked": true,
    "id": "016062d74b4a369fa7855f40827b5afa4add23f3",
    "semantic_title": "a deep neural network for short-segment speaker recognition",
    "citation_count": 43
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhou19_interspeech.html": {
    "title": "Deep Speaker Embedding Extraction with Channel-Wise Feature Responses and Additive Supervision Softmax Loss Function",
    "volume": "main",
    "abstract": "In speaker verification, the convolutional neural networks (CNN) have been successfully leveraged to achieve a great performance. Most of the models based on CNN primarily focus on learning the distinctive speaker embedding from the horizontal direction (time-axis). However, the feature relationship between channels is usually neglected. In this paper, we firstly aim toward an alternate direction of recalibrating the channel-wise features by introducing the recently proposed \"squeeze-and-excitation\" (SE) module for image classification. We effectively incorporate the SE blocks in the deep residual networks (ResNet-SE) and demonstrate a slightly improvement on VoxCeleb corpuses. Additionally, we propose a new loss function, namely additive supervision softmax (AS-Softmax), to make full use of the prior knowledge of the mis-classified samples at training stage by imposing more penalty on the mis-classified samples to regularize the training process. The experimental results on VoxCeleb corpuses demonstrate that the proposed loss could further improve the performance of speaker system, especially on the case that the combination of the ResNet-SE and the AS-Softmax",
    "checked": true,
    "id": "7f56f42408791b41572fa941289211af17ba6463",
    "semantic_title": "deep speaker embedding extraction with channel-wise feature responses and additive supervision softmax loss function",
    "citation_count": 51
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shon19b_interspeech.html": {
    "title": "VoiceID Loss: Speech Enhancement for Speaker Verification",
    "volume": "main",
    "abstract": "In this paper, we propose VoiceID loss, a novel loss function for training a speech enhancement model to improve the robustness of speaker verification. In contrast to the commonly used loss functions for speech enhancement such as the L2 loss, the VoiceID loss is based on the feedback from a speaker verification model to generate a ratio mask. The generated ratio mask is multiplied pointwise with the original spectrogram to filter out unnecessary components for speaker verification. In the experiments, we observed that the enhancement network, after training with the VoiceID loss, is able to ignore a substantial amount of time-frequency bins, such as those dominated by noise, for verification. The resulting model consistently improves the speaker verification system on both clean and noisy conditions",
    "checked": true,
    "id": "290e9e69b0d7c6973986b76a8861670d34685d17",
    "semantic_title": "voiceid loss: speech enhancement for speaker verification",
    "citation_count": 61
  },
  "https://www.isca-speech.org/archive/interspeech_2019/avila19_interspeech.html": {
    "title": "Blind Channel Response Estimation for Replay Attack Detection",
    "volume": "main",
    "abstract": "Recently, automatic speaker verification (ASV) systems have been acknowledged to be vulnerable to replay attacks. Multiple efforts have been taken by the research community to improve ASV robustness. In this paper, we propose a replay attack countermeasure based on the blind estimation of the magnitude of channel responses. For that, the log-spectrum average of the clean speech signal is predicted from a Gaussian mixture model (GMM) of RASTA filtered mel-frequency cepstral coefficients (MFCCs) trained on clean speech. The magnitude response of the channel is obtained by subtracting the log-spectrum of the observed signal from the predicted log-spectrum average of the clean signal. Two datasets are used in our experiments: (1) the TIMIT dataset, which is used to train the log-spectrum average of the clean signal; and (2) a dataset containing replay attacks used during the second Automatic Speaker Verification Spoofing and Countermeasures Challenge (ASVspoof 2017). Performance is compared to two benchmarks. The discrete Fourier transform power spectral (DFTspec) and the constant Q cepstral coefficients (CQCCs). Results show the proposed method outperforming the two benchmarks in most scenarios with equal error rate (EER) as low as 6.87% when testing on the development set and as low as 11.28% on the evaluation set",
    "checked": true,
    "id": "631646a4de886a536cbb89882fe09d2f344170a4",
    "semantic_title": "blind channel response estimation for replay attack detection",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/patil19_interspeech.html": {
    "title": "Energy Separation-Based Instantaneous Frequency Estimation for Cochlear Cepstral Feature for Replay Spoof Detection",
    "volume": "main",
    "abstract": "Replay attack poses significant threat to Automatic Speaker Verification (ASV) system among various spoofing attacks, as it is easily accessible by low cost and high quality recording and playback devices. This paper presents a novel feature set, i.e., Cochlear Filter Cepstral Coefficient Instantaneous Frequency using Energy Separation Algorithm (CFCCIF-ESA) to develop countermeasure against replay spoofing attacks. Experimental results on ASVspoof 2017 Version 2.0 database reveal that the proposed CFCCIF-ESA performs better than the earlier proposed CFCCIF (using analytic signal generation via Hilbert transform) feature set. This is because ESA uses extremely short window to estimate instantaneous frequency being able to adapt during speech transitions across phonemes. Experiments are performed using Gaussian Mixture Model (GMM) as a classifier. Baseline Constant Q Cepstral Coefficient (CQCC) performs slightly better than CFCCIF-ESA on development set (i.e., 12.47% and 12.98% Equal Error Rate (EER) for CQCC and CFCCIF-ESA, respectively). However, contrasting results on evaluation set (i.e., 18.81% and 14.77% EER for CQCC and CFCCIF-ESA, respectively) indicates that the proposed CFCCIF-ESA gives relatively better performance for unseen attacks in evaluation data. Also, the proposed feature set gives an EER of 11.56% and 13.26% on development and evaluation dataset when fused with state-of-the-art Mel Frequency Cepstral Coefficient (MFCC)",
    "checked": true,
    "id": "b3e5af44330ff8c4b1952bd19e8d5c7b56ae8a47",
    "semantic_title": "energy separation-based instantaneous frequency estimation for cochlear cepstral feature for replay spoof detection",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mingote19_interspeech.html": {
    "title": "Optimization of False Acceptance/Rejection Rates and Decision Threshold for End-to-End Text-Dependent Speaker Verification Systems",
    "volume": "main",
    "abstract": "Currently, most Speaker Verification (SV) systems based on neural networks use Cross-Entropy and/or Triplet loss functions. Despite these functions provide competitive results, they might not fully exploit the system performance, because they are not designed to optimize the verification task considering the performance measures, e.g. the Detection Cost Function (DCF) or the Equal Error Rate (EER). This paper proposes a first approach to this issue through the optimization of a loss function based on the DCF. This mechanism allows the end-to-end system to directly manage the threshold used to compute the ratio between the False Rejection Rate (FRR) and the False Acceptance Rate (FAR). This way connecting the system training directly to the operating point. Results in a text-dependent speaker verification framework, based on neural network super-vectors over the RSR2015 dataset, outperform reference systems using Cross-Entropy and Triplet loss, as well as our previously proposal based on an approximation of the Area Under the Curve ( aAUC)",
    "checked": true,
    "id": "31c7bf7767af1032031d624b197129d11e04cddd",
    "semantic_title": "optimization of false acceptance/rejection rates and decision threshold for end-to-end text-dependent speaker verification systems",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fan19_interspeech.html": {
    "title": "Deep Hashing for Speaker Identification and Retrieval",
    "volume": "main",
    "abstract": "Speaker identification and retrieval have been widely used in real applications. To overcome the inefficiency problem caused by real-valued representations, there have appeared some speaker hashing methods for speaker identification and retrieval by learning binary codes as representations. However, these hashing methods are based on i-vector and cannot achieve satisfactory retrieval accuracy as they cannot learn discriminative feature representations. In this paper, we propose a novel deep hashing method, called deep additive margin hashing (DAMH), to improve retrieval performance for speaker identification and retrieval task. Compared with existing speaker hashing methods, DAMH can perform feature learning and binary code learning seamlessly by incorporating these two procedures into an end-to-end architecture. Experimental results on a large-scale audio dataset VoxCeleb2 show that DAMH can outperform existing speaker hashing methods to achieve state-of-the-art performance",
    "checked": true,
    "id": "f2c8ff78058adeeb29fb1b6e0f1b1f2d26ff9b6f",
    "semantic_title": "deep hashing for speaker identification and retrieval",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/marras19_interspeech.html": {
    "title": "Adversarial Optimization for Dictionary Attacks on Speaker Verification",
    "volume": "main",
    "abstract": "In this paper, we assess vulnerability of speaker verification systems to dictionary attacks. We seek master voices, i.e., adversarial utterances optimized to match against a large number of users by pure chance. First, we perform menagerie analysis to identify utterances which intrinsically hold this property. Then, we propose an adversarial optimization approach for generating master voices synthetically. Our experiments show that, even in the most secure configuration, on average, a master voice can match approx. 20% of females and 10% of males without any knowledge about the population. We demonstrate that dictionary attacks should be considered as a feasible threat model for sensitive and high-stakes deployments of speaker verification",
    "checked": true,
    "id": "4c636645d66b93b88b9805928532abd4cf76d573",
    "semantic_title": "adversarial optimization for dictionary attacks on speaker verification",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gunendradasan19_interspeech.html": {
    "title": "An Adaptive-Q Cochlear Model for Replay Spoofing Detection",
    "volume": "main",
    "abstract": "Replay attack poses a key threat for automatic speaker verification systems. Spoofing detection systems inspired by auditory perception have shown promise to date, however some aspects of auditory processing have not been investigated in this context. In this paper, a transmission line cochlear model that incorporates an active feedback mechanism is proposed for replay attack detection. This model compresses the considerable energy variation in each auditory sub-band filter by boosting low-amplitude signal, an effect that is not considered in many auditory models. To perform the compression, the parameters of each auditory sub-band filter are modified based on the sub-band energy, analogous to the effect of the closed-loop adaptation mechanism that allows perception of a wide dynamic range from a physically constrained system, which we term adaptive-Q. Evaluation on the ASVspoof 2017 version 2 database suggests that the adaptive-Q compression provided by the proposed model helps to improve the performance of replay detection, and a relative reduction in EER of 26% was achieved compared with the best results reported for auditory system-based feature proposed for replay attack detection",
    "checked": true,
    "id": "929ca3bfa014d43c40e456eda0f9c5f5764d43b4",
    "semantic_title": "an adaptive-q cochlear model for replay spoofing detection",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yun19_interspeech.html": {
    "title": "An End-to-End Text-Independent Speaker Verification Framework with a Keyword Adversarial Network",
    "volume": "main",
    "abstract": "This paper presents an end-to-end text-independent speaker verification framework by jointly considering the speaker embedding (SE) network and automatic speech recognition (ASR) network. The SE network learns to output an embedding vector which distinguishes the speaker characteristics of the input utterance, while the ASR network learns to recognize the phonetic context of the input. In training our speaker verification framework, we consider both the triplet loss minimization and adversarial gradient of the ASR network to obtain more discriminative and text-independent speaker embedding vectors. With the triplet loss, the distances between the embedding vectors of the same speaker are minimized while those of different speakers are maximized. Also, with the adversarial gradient of the ASR network, the text-dependency of the speaker embedding vector can be reduced. In the experiments, we evaluated our speaker verification framework using the LibriSpeech and CHiME 2013 dataset, and the evaluation results show that our speaker verification framework shows lower equal error rate and better text-independency compared to the other approaches",
    "checked": true,
    "id": "9baabb9d7efbc8ee207e6d27344c0bda3e9850f1",
    "semantic_title": "an end-to-end text-independent speaker verification framework with a keyword adversarial network",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/seo19_interspeech.html": {
    "title": "Shortcut Connections Based Deep Speaker Embeddings for End-to-End Speaker Verification System",
    "volume": "main",
    "abstract": "The objective of speaker verification is to reject or accept whether or not the input speech is that of a enrolled speaker. Traditionally, i-vector or speaker embeddings system such as d-vector representing the speaker information has been showing high performance with similarity metrics at the backend. Recently it has been proposed an end-to-end system based on previous speaker embeddings approach without additional strategy after extraction. Among the various models, CNN based end-to-end system is showing state-of-the-art performance. CNN based model is trained to classify multiple speakers and speaker embeddings are extracted In this paper, we propose shortcut connections based deep speaker embeddings for end-to-end speaker verification system. We construct modified ResNet-18 model so that the activation outputs from bottleneck architecture have shortcut connections to speaker embeddings. Deep speaker embeddings are extracted by jointly training in end-to-end approach. The model was constructed without other sophisticated methods such as length normalization, or additive margin softmax loss. When we tested proposed model on the unconstrained conditions data set called VoxCeleb1, the result showed EER of 3.03% when tested with high dimensional deep speaker embeddings. This is the state-of-the-art performance of end-to-end speaker verification model on VoxCeleb1",
    "checked": true,
    "id": "bd956379ef8937ed3ce65fce4389c8264d4ad391",
    "semantic_title": "shortcut connections based deep speaker embeddings for end-to-end speaker verification system",
    "citation_count": 27
  },
  "https://www.isca-speech.org/archive/interspeech_2019/you19c_interspeech.html": {
    "title": "Device Feature Extractor for Replay Spoofing Detection",
    "volume": "main",
    "abstract": "Device feature, which contains the information of both recording channel and playback channel, is the critical trait for replay spoofing detection. So far there have not been any technical reports about the usage of device information in spoofing detection for speaker verification. In this paper, we propose to build a replay device feature (RDF) extractor on the basis of the genuine-replay-pair training database. The RDF extractor is trained in constant-Q transform (CQT) spectrum domain. A bidirectional long short-term memory (BLSTM) is used in the neural network and finally the RDF extractor is formed by applying discrete cosine transform (DCT) to the output vector of the BLSTM. The experimental result on ASVspoof 2017 corpus version 2.0 shows that equal error rate (EER) of replay detection system with the proposed RDF reaches 15.08%. Furthermore, by combining the RDF with constant-Q cepstral coefficients plus log energy (CQCCE), the EER of the detection system can reduce to 8.99%. In addition, the experimental results also show that the RDF has much complementarity with conventional features",
    "checked": true,
    "id": "13b2ea0c40f1f0581db0662afcd7ec09c6ec3c8c",
    "semantic_title": "device feature extractor for replay spoofing detection",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19i_interspeech.html": {
    "title": "Cross-Domain Replay Spoofing Attack Detection Using Domain Adversarial Training",
    "volume": "main",
    "abstract": "Replay spoofing attacks are a major threat for speaker verification systems. Although many anti-spoofing systems or countermeasures are proposed to detect dataset-specific replay attacks with promising performance, they generalize poorly when applied on unseen datasets. In this work, the cross-dataset scenario is treated as a domain-mismatch problem and dealt with using a domain adversarial training framework. Compared with previous approaches, features learned from this newly-designed architecture are more discriminative for spoofing detection, but more indistinguishable across different domains. Only labeled source-domain data and unlabeled target-domain data are required during the adversarial training process, which can be regarded as unsupervised domain adaptation. Experiments on the ASVspoof 2017 V.2 dataset as well as the physical access condition part of BTAS 2016 dataset demonstrate that a significant EER reduction of over relative 30% can be obtained after applying the proposed domain adversarial training framework. It is shown that our proposed model can benefit from a large amount of unlabeled target-domain training data to improve detection accuracy",
    "checked": true,
    "id": "72a8fd18652d55aa2c9e99bc629233fcfb6fe61a",
    "semantic_title": "cross-domain replay spoofing attack detection using domain adversarial training",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kanagasundaram19_interspeech.html": {
    "title": "A Study of x-Vector Based Speaker Recognition on Short Utterances",
    "volume": "main",
    "abstract": "The aim of this work is to gain insights into how the deep neural network (DNN) models should be trained for short utterance evaluation conditions in an x-vector based speaker verification system. The study suggests that the speaker embedding can be extracted with reduced dimensions for short utterance evaluation conditions. When the speaker embedding is extracted from deeper layer which has lower dimension, the x-vector system achieves 14% relative improvement over baseline approach on EER on NIST2010 5sec-5sec truncated conditions. We surmise that since short utterances have less phonetic information speaker discriminative x-vectors can be extracted from a deeper layer of the DNN which captures less phonetic information. Another interesting finding is that the x-vector system achieves 5% relative improvement on NIST2010 5sec-5sec evaluation condition when the back-end PLDA is trained using short utterance development data. The results confirms the intuitive expectation that duration of development utterances and the duration of evaluation utterances should be matched. Finally, for the duration mismatch condition, we propose a variance normalization approach for PLDA training that provides a 4% relative improvement on EER over baseline approach",
    "checked": true,
    "id": "e6b11a9ebeeb164dc350d92204736f5d3144719a",
    "semantic_title": "a study of x-vector based speaker recognition on short utterances",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19i_interspeech.html": {
    "title": "Tied Mixture of Factor Analyzers Layer to Combine Frame Level Representations in Neural Speaker Embeddings",
    "volume": "main",
    "abstract": "In this paper, a novel neural network layer is proposed to combine frame-level into utterance-level representations for speaker modeling. We followed the assumption that the frame-level outputs of the speaker embedding (a.k.a x-vector) encoder are multi-modal. Therefore, we modeled the frame-level information as a mixture of factor analyzers with latent variable (utterance embedding) tied across frames and mixture components, in as similar way as in the i-vector approach. We denote this layer as Tied Mixture of Factor Analyzers (TMFA) layer. The optimal value of the embedding is obtained by minimizing the reconstruction error of the frame-level representations given the embedding and the TMFA model parameters. However, the TMFA layer parameters (factor loading matrices, means and precisions) were trained with cross-entropy loss as the rest of parameters of the network. We experimented on the Speaker Recognition Evaluation 2016 Cantonese as well as in the Speaker in the Wild datasets. The proposed pooling layer improved w.r.t. mean plus standard deviation pooling — standard in x-vector approach — in most of the conditions evaluated; and obtained competitive performance w.r.t. the recently proposed learnable dictionary encoding pooling method, which also assumes multi-modal frame-level representations",
    "checked": true,
    "id": "f8799df9306ddf660f4ab1874bff154303042f00",
    "semantic_title": "tied mixture of factor analyzers layer to combine frame level representations in neural speaker embeddings",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wickramasinghe19_interspeech.html": {
    "title": "Biologically Inspired Adaptive-Q Filterbanks for Replay Spoofing Attack Detection",
    "volume": "main",
    "abstract": "Development of generalizable countermeasures for replay spoofing attacks on Automatic Speaker Verification (ASV) systems is still an open problem. Many countermeasures to date utilize bandpass filters to extract a variety of frequency band-based features. This paper proposes the use of adaptive bandpass filters, a concept adopted from human cochlear modelling to improve detection performance. Gains of filters used for subband based feature extraction are adaptively adjusted by varying their Q factors (Quality factor) as a function of input signal level to boost low amplitude signal components and improve the front-end's sensitivity to them. This method is used to enhance information embedded in speech signals such as device channel effects which could be instrumental in distinguishing genuine speech signals from replayed ones. Three features extracted using the adaptive filter process yielded performance improvements over other auditory concepts-based baselines, showing the potential of using an adaptive filter mechanism for replay spoofing attack detection",
    "checked": true,
    "id": "bdceab5598568f8ee76de85f6961b40f464d29b8",
    "semantic_title": "biologically inspired adaptive-q filterbanks for replay spoofing attack detection",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bousquet19_interspeech.html": {
    "title": "On Robustness of Unsupervised Domain Adaptation for Speaker Recognition",
    "volume": "main",
    "abstract": "Current speaker recognition systems, that are learned by using wide training datasets and include sophisticated modelings, turn out to be very specific, providing sometimes disappointing results in real-life applications. Any shift between training and test data, in terms of device, language, duration, noise or other tends to degrade accuracy of speaker detection. This study investigates unsupervised domain adaptation,when only a scarce and unlabeled \"in-domain\" development dataset is available. Details and relevance of different approaches are described and commented, leading to a new robust method that we call feature-Distribution Adaptor. Efficiency of the proposed technique is experimentally validated on the recent NIST 2016 and 2018 Speaker Recognition Evaluation datasets",
    "checked": true,
    "id": "fc0d5b01c9eb21eda5f5338fc35f0f356d8874a7",
    "semantic_title": "on robustness of unsupervised domain adaptation for speaker recognition",
    "citation_count": 30
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shon19c_interspeech.html": {
    "title": "Large-Scale Speaker Retrieval on Random Speaker Variability Subspace",
    "volume": "main",
    "abstract": "This paper describes a fast speaker search system to retrieve segments of the same voice identity in the large-scale data. A recent study shows that Locality Sensitive Hashing (LSH) enables quick retrieval of a relevant voice in the large-scale data in conjunction with i-vector while maintaining accuracy. In this paper, we proposed Random Speaker-variability Subspace (RSS) projection to map a data into LSH based hash tables. We hypothesized that rather than projecting on completely random subspace without considering data, projecting on randomly generated speaker variability space would give more chance to put the same speaker representation into the same hash bins, so we can use less number of hash tables. Multiple RSS can be generated by randomly selecting a subset of speakers from a large speaker cohort. From the experimental result, the proposed approach shows 100 times and 7 times faster than the linear search and LSH, respectively",
    "checked": true,
    "id": "cab4a6de1bbbba12762592e322bc25d50c4b44e9",
    "semantic_title": "large-scale speaker retrieval on random speaker variability subspace",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yoshioka19_interspeech.html": {
    "title": "Meeting Transcription Using Asynchronous Distant Microphones",
    "volume": "main",
    "abstract": "We describe a system that generates speaker-annotated transcripts of meetings by using multiple asynchronous distant microphones. The system is composed of continuous audio stream alignment, blind beamforming, speech recognition, speaker diarization, and system combination. While the idea of improving the meeting transcription accuracy by leveraging multiple recordings has been investigated in certain specific technology areas such as beamforming, our objective is to assess the feasibility of a complete system with a set of mobile devices and conduct a detailed analysis. With seven input audio streams, our system achieves a word error rate (WER) of 22.3% and a speaker-attributed WER (SAWER) of 26.7%, and comes within 3% of the close-talking microphone WER on non-overlapping speech. The relative gains in SAWER over a single-device system are 14.8%, 20.3%, and 22.4% for three, five, and seven microphones, respectively. The full system achieves a 13.6% diarization error rate, 10% of which are due to overlapped speech",
    "checked": true,
    "id": "0a13c7809726b88a20fd08465ee5a56cab830366",
    "semantic_title": "meeting transcription using asynchronous distant microphones",
    "citation_count": 32
  },
  "https://www.isca-speech.org/archive/interspeech_2019/thomas19_interspeech.html": {
    "title": "Detection and Recovery of OOVs for Improved English Broadcast News Captioning",
    "volume": "main",
    "abstract": "In this paper we present a study on building various deep neural network-based speech recognition systems for automatic caption generation that can deal with out-of-vocabulary (OOV) words. We develop several kinds of systems using various acoustic (hybrid, CTC, attention-based neural networks) and language modeling (n-gram and RNN-based neural networks) techniques on broadcast news. We discuss various limitations that the proposed systems have and introduce methods to effectively use them to detect OOVs. For automatic OOV recovery, we compare the use of different kinds of phonetic and graphemic sub-word units, that can be synthesized into word outputs. On an experimental three hour broadcast news test set with a 4% OOV rate, the proposed CTC and attention-based systems are capable of reliably detecting OOVs much better (0.52 F-score) than a traditional hybrid baseline system (0.21 F-score). These improved detection gains translate further to better WER performance. With reference to a non-OOV oracle baseline, the proposed systems at just 12% relative (1.4% absolute) loss in word error rate (WER), perform significantly better than the traditional hybrid system (with close to 50% relative loss), by recovering OOVs using their sub-word outputs",
    "checked": true,
    "id": "e798b8a6c9054021525096782c54a54df9df4faf",
    "semantic_title": "detection and recovery of oovs for improved english broadcast news captioning",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/farooq19_interspeech.html": {
    "title": "Improving Large Vocabulary Urdu Speech Recognition System Using Deep Neural Networks",
    "volume": "main",
    "abstract": "Development of Large Vocabulary Continuous Speech Recognition (LVCSR) system is a cumbersome task, especially for low resource languages. Urdu is the national language and lingua franca of Pakistan, with 100 million speakers worldwide. Due to resource scarcity, limited work has been done in the domain of Urdu speech recognition. In this paper, collection of Urdu speech corpus and development of Urdu speech recognition system is presented. Urdu LVCSR is developed using 300 hours of read speech data with a vocabulary size of 199K words. Microphone speech is recorded from 1671 Urdu and Punjabi speakers in both indoor and outdoor environments. Different acoustic modeling techniques such as Gaussian Mixture Models based Hidden Markov Models (GMM-HMM), Time Delay Neural Networks (TDNN), Long-Short Term Memory (LSTM) and Bidirectional Long-Short Term Memory (BLSTM) networks are investigated. Cross entropy and Lattice Free Maximum Mutual Information (LF-MMI) objective functions are employed during acoustic modeling. In addition, Recurrent Neural Network Language Model (RNNLM) is also being used for re-scoring. Developed speech recognition system has been evaluated on 9.5 hours of collected test data and a minimum Word Error Rate (%WER) of 13.50% is achieved",
    "checked": true,
    "id": "da55e34e0da4956107151edf1e6dd866d87d6896",
    "semantic_title": "improving large vocabulary urdu speech recognition system using deep neural networks",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tang19b_interspeech.html": {
    "title": "Hybrid Arbitration Using Raw ASR String and NLU Information — Taking the Best of Both Embedded World and Cloud World",
    "volume": "main",
    "abstract": "Hybrid arbitration is a process where we select the best Automatic Speech Recognition (ASR) and Natural Language Understanding (NLU) result from embedded/client and cloud-based system outputs. It is a common approach that a lot of real world applications use to unify knowledge sources that are not available to client and cloud at the same time. In the past, people primarily relied on ASR confidence features and some application specific heuristics in the arbitration process. However, confidence features are unable to capture subtle context specific differences. In this paper, besides confidence, we also use raw ASR strings and NLU results in the hybrid arbitration process. We model the arbitration process as two steps — first, decide whether to wait for a slower system, and second, pick the best result. We compared multiple machine learning approaches and it turns out the Deep Neural Network (DNN) based classifier, using word embeddings to process ASR strings and NLU embeddings to process NLU information, can deliver the best performance. We conducted experiments on two production system setups, using field data from real users. Compared with traditional confidence score based approach, we obtain about 30% relative word error reduction and 30% relative sentence error rate reduction",
    "checked": true,
    "id": "6d5694649a5074c153a4fa895627e68569d1046c",
    "semantic_title": "hybrid arbitration using raw asr string and nlu information - taking the best of both embedded world and cloud world",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/szaszak19_interspeech.html": {
    "title": "Leveraging a Character, Word and Prosody Triplet for an ASR Error Robust and Agglutination Friendly Punctuation Approach",
    "volume": "main",
    "abstract": "Punctuating ASR transcript has received increasing attention recently, and well-performing approaches were presented based on sequence-to-sequence modelling, exploiting textual (word and character) and/or acoustic-prosodic features. In this work we propose to consider character, word and prosody based features all at once to provide a robust and highly language independent platform for punctuation recovery, which can deal also well with highly agglutinating languages with less constrained word order. We demonstrate that using such a feature triplet improves ASR error robustness of punctuation in two quite differently organized languages, English and Hungarian. Moreover, in the highly agglutinating Hungarian, where word-based approaches suffer from the exploding vocabulary (poorer semantic representation through embeddings) and less constrained word order, we show that prosodic cues and the character-based model can powerfully counteract this loss of information. We also perform a deep analysis of punctuation w.r.t. both ASR errors and agglutination to explain the improvements we observed on a solid basis",
    "checked": true,
    "id": "1937c6934528bcf9950c39bbe5360a36365315fb",
    "semantic_title": "leveraging a character, word and prosody triplet for an asr error robust and agglutination friendly punctuation approach",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pellegrini19_interspeech.html": {
    "title": "The Airbus Air Traffic Control Speech Recognition 2018 Challenge: Towards ATC Automatic Transcription and Call Sign Detection",
    "volume": "main",
    "abstract": "In this paper, we describe the outcomes of the challenge organized and run by Airbus and partners in 2018 on Air Traffic Control (ATC) speech recognition. The challenge consisted of two tasks applied to English ATC speech: 1) automatic speech-to-text transcription, 2) call sign detection (CSD). The registered participants were provided with 40 hours of speech along with manual transcriptions. Twenty-two teams submitted predictions on a five hour evaluation set. ATC speech processing is challenging for several reasons: high speech rate, foreign-accented speech with a great diversity of accents, noisy communication channels. The best ranked team achieved a 7.62% Word Error Rate and a 82.41% CSD F1-score. Transcribing pilots' speech was found to be twice as harder as controllers' speech. Remaining issues towards solving ATC ASR are also discussed in the paper",
    "checked": true,
    "id": "6d8802b7a316113d620d63872d3d1229c9d76c10",
    "semantic_title": "the airbus air traffic control speech recognition 2018 challenge: towards atc automatic transcription and call sign detection",
    "citation_count": 32
  },
  "https://www.isca-speech.org/archive/interspeech_2019/oneata19_interspeech.html": {
    "title": "Kite: Automatic Speech Recognition for Unmanned Aerial Vehicles",
    "volume": "main",
    "abstract": "This paper addresses the problem of building a speech recognition system attuned to the control of unmanned aerial vehicles (UAVs). Even though UAVs are becoming widespread, the task of creating voice interfaces for them is largely unaddressed. To this end, we introduce a multi-modal evaluation dataset for UAV control, consisting of spoken commands and associated images, which represent the visual context of what the UAV \"sees\" when the pilot utters the command. We provide baseline results and address two research directions: (i) how robust the language models are, given an incomplete list of commands at train time; (ii) how to incorporate visual information in the language model. We find that recurrent neural networks (RNNs) are a solution to both tasks: they can be successfully adapted using a small number of commands and they can be extended to use visual cues. Our results show that the image-based RNN outperforms its text-only counterpart even if the command–image training associations are automatically generated and inherently imperfect",
    "checked": true,
    "id": "75307b5ac9df2197d132ada7f251e6410d113818",
    "semantic_title": "kite: automatic speech recognition for unmanned aerial vehicles",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19j_interspeech.html": {
    "title": "Exploring Methods for the Automatic Detection of Errors in Manual Transcription",
    "volume": "main",
    "abstract": "Quality of data plays an important role in most deep learning tasks. In the speech community, transcription of speech recording is indispensable. Since the transcription is usually generated artificially, automatically finding errors in manual transcriptions not only saves time and labors but benefits the performance of tasks that need the training process. Inspired by the success of hybrid automatic speech recognition using both language model and acoustic model, two approaches of automatic error detection in the transcriptions have been explored in this work. Previous study using a biased language model approach, relying on a strong transcription-dependent language model, has been reviewed. In this work, we propose a novel acoustic model based approach, focusing on the phonetic sequence of speech. Both methods have been evaluated on a completely real dataset, which was originally transcribed with errors and strictly corrected manually afterwards",
    "checked": true,
    "id": "25b208072576358e1bcfd925faf94e11349ed475",
    "semantic_title": "exploring methods for the automatic detection of errors in manual transcription",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/biswas19_interspeech.html": {
    "title": "Improved Low-Resource Somali Speech Recognition by Semi-Supervised Acoustic and Language Model Training",
    "volume": "main",
    "abstract": "We present improvements in automatic speech recognition (ASR) for Somali, a currently extremely under-resourced language. This forms part of a continuing United Nations (UN) effort to employ ASR-based keyword spotting systems to support humanitarian relief programmes in rural Africa. Using just 1.57 hours of annotated speech data as a seed corpus, we increase the pool of training data by applying semi-supervised training to 17.55 hours of untranscribed speech. We make use of factorised time-delay neural networks (TDNN-F) for acoustic modelling, since these have recently been shown to be effective in resource-scarce situations. Three semi-supervised training passes were performed, where the decoded output from each pass was used for acoustic model training in the subsequent pass. The automatic transcriptions from the best performing pass were used for language model augmentation. To ensure the quality of automatic transcriptions, decoder confidence is used as a threshold. The acoustic and language models obtained from the semi-supervised approach show significant improvement in terms of WER and perplexity compared to the baseline. Incorporating the automatically generated transcriptions yields a 6.55% improvement in language model perplexity. The use of 17.55 hour of Somali acoustic data in semi-supervised training shows an improvement of 7.74% relative over the baseline",
    "checked": true,
    "id": "146128184cd585b5a1298d3e75e15fb8fbbbd7a5",
    "semantic_title": "improved low-resource somali speech recognition by semi-supervised acoustic and language model training",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/helgadottir19_interspeech.html": {
    "title": "The Althingi ASR System",
    "volume": "main",
    "abstract": "All performed speeches in the Icelandic parliament, Althingi, are transcribed and published. An automatic speech recognition system (ASR) has been developed to reduce the manual work involved. To our knowledge, this is the first open source speech recognizer in use for Icelandic. In this paper the development of the ASR is described. In-lab system performance is evaluated and first results from the users are described. A word error rate (WER) of 7.91% was obtained on our in-lab speech recognition test set using time-delay deep neural network (TDNN) and re-scoring with a bidirectional recurrent neural network language model (RNN-LM). No further processing of the text is included in that number. In-lab F-score for the punctuation model is 80.6 and 61.6 for the paragraph model. The WER of the ASR, including punctuation marks and other post-processing, was 15.0 ± 6.0%, over 625 speeches, when tested in the wild. This is an upper limit since not all mismatches with the reference text are true errors of the ASR. The transcribers of Althingi graded 77% of the speech transcripts as Good. The Althingi corpus and ASR recipe, constitute a valuable resource for further developments within Icelandic language technology",
    "checked": true,
    "id": "c9782dc9425acf83ad337c41a6e1ffdb0acb3741",
    "semantic_title": "the althingi asr system",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gupta19d_interspeech.html": {
    "title": "CRIM's Speech Transcription and Call Sign Detection System for the ATC Airbus Challenge Task",
    "volume": "main",
    "abstract": "The Airbus air traffic control challenge evaluates speech recognition and call sign detection using real conversations between air traffic controllers and pilots at Toulouse airport in France. CRIM's main contribution in acoustic modeling for transcribing these conversations is experimentation with bidirectional LSTM (BLSTM) models and lattice-free MMI (LF-MMI) trained TDNN models. Adapting these acoustic models trained from a large dataset to 40 hours of ATC acoustic training data reduces WER significantly compared to training them with the ATC data only. Multiple iterations of adaptation reduce WER for the BLSTM acoustic models significantly, but only marginally for the LF-MMI TDNN acoustic models. Constrained dialog between the air traffic controller and the pilot leads to language model perplexity below 12, and WER for leaderboard and evaluation sets of 9.98% and 9.41% respectively For call sign detection from the decoded transcript, we use a bidirectional LSTM followed by conditional random field classifier. This DNN architecture worked better than a finite state transducer based call sign detection. Taking a majority vote over call signs from multiple decodes reduced the call sign errors. The best F1 for call sign detection for leaderboard was 0.8289 and for evaluation 0.8017. Overall, we came 3rd in this evaluation",
    "checked": true,
    "id": "2de6fd6f4302768f8103bf12efaa9b20391980c3",
    "semantic_title": "crim's speech transcription and call sign detection system for the atc airbus challenge task",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rutowski19_interspeech.html": {
    "title": "Optimizing Speech-Input Length for Speaker-Independent Depression Classification",
    "volume": "main",
    "abstract": "Machine learning models for speech-based depression classification offer promise for health care applications. Despite growing work on depression classification, little is understood about how the length of speech-input impacts model performance. We analyze results for speaker-independent depression classification using a corpus of over 1400 hours of speech from a human-machine health screening application. We examine performance as a function of response input length for two NLP systems that differ in overall performance Results for both systems show that performance depends on natural length, elapsed length, and ordering of the response within a session. Systems share a minimum length threshold, but differ in a response saturation threshold, with the latter higher for the better system. At saturation it is better to pose a new question to the speaker, than to continue the current response. These and additional reported results suggest how applications can be better designed to both elicit and process optimal input lengths for depression classification",
    "checked": true,
    "id": "c97431766ebd68443dd6e48a50366accbe88ef0f",
    "semantic_title": "optimizing speech-input length for speaker-independent depression classification",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pietrowicz19_interspeech.html": {
    "title": "A New Approach for Automating Analysis of Responses on Verbal Fluency Tests from Subjects At-Risk for Schizophrenia",
    "volume": "main",
    "abstract": "What if young people at risk for developing schizophrenia could be identified early, via a fast, automated, non-invasive test of language, which could be administered remotely? These youths could then receive intervention which might mitigate course and possibly prevent psychosis. Timed word fluency tests, in which individuals name words starting with a designated sound (typically F/A/S) or represent a given concept category (commonly animals/fruits/vegetables), have been used in the assessment of schizophrenia and its risk states, and in many other mental health conditions. Typically, psychologists manually record the number and size of valid phoneme clusters and switches observed in the phonemic tests and count the number of valid words belonging to a given category in the categorical tests. We present a new technique for automating the analysis of category fluency data and apply it to the problem of detecting youths at risk of developing schizophrenia, with best results over 85% accuracy when applying phonemic analysis to categorical data. The technique supports the separate quantification of structural and sequential phonemic similarity measures, supports an arbitrary range of pronunciations and dialects in the analysis, and may be extended to the assessment of other mental and physical health conditions, and their risk states",
    "checked": true,
    "id": "367d2ffad45a5fe61b28864eb7450d3a682d2379",
    "semantic_title": "a new approach for automating analysis of responses on verbal fluency tests from subjects at-risk for schizophrenia",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jeancolas19_interspeech.html": {
    "title": "Comparison of Telephone Recordings and Professional Microphone Recordings for Early Detection of Parkinson's Disease, Using Mel-Frequency Cepstral Coefficients with Gaussian Mixture Models",
    "volume": "main",
    "abstract": "Vocal impairments are among the earliest symptoms in Parkinson's Disease (PD). We adapted a method classically used in speech and speaker recognition, based on Mel-Frequency Cepstral Coefficients (MFCC) extraction and Gaussian Mixture Model (GMM) to detect recently diagnosed and pharmacologically treated PD patients. We classified early PD subjects from controls with an accuracy of 83%, using recordings obtained with a professional microphone. More interestingly, we were able to classify PD from controls with an accuracy of 75% based on telephone recordings. As far as we know, this is the first time that audio recordings from telephone network have been used for early PD detection. This is a promising result for a potential future telediagnosis of Parkinson's disease",
    "checked": true,
    "id": "6f13a530c2d98d15ab1a802579c7baa15c9cf0c0",
    "semantic_title": "comparison of telephone recordings and professional microphone recordings for early detection of parkinson's disease, using mel-frequency cepstral coefficients with gaussian mixture models",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/janbakhshi19_interspeech.html": {
    "title": "Spectral Subspace Analysis for Automatic Assessment of Pathological Speech Intelligibility",
    "volume": "main",
    "abstract": "Speech intelligibility is an important assessment criterion of the communicative performance of pathological speakers. To assist clinicians in their assessment, time- and cost-efficient automatic intelligibility measures offering a repeatable and reliable assessment are desired. In this paper, we propose to automatically assess pathological speech intelligibility based on a distance measure between the subspaces of spectral patterns of the pathological speech signal and of a fully intelligible (healthy) speech signal. To extract the subspace of spectral patterns we investigate two linear decomposition methods, i.e., Principal Component Analysis and Approximate Joint Diagonalization. Pathological speech intelligibility is then derived using a Grassman distance measure which quantifies the difference between the extracted subspaces of pathological and healthy speech. Experiments on an English database of Cerebral Palsy patients show that the proposed intelligibility measure is significantly correlated with subjective intelligibility ratings. In addition, comparisons to state-of-the-art measures show that the proposed subspace-based measure achieves a high performance with a significantly lower computational cost and without imposing any constraints on the speech material of the speakers",
    "checked": true,
    "id": "45c4a20116b1f0f3634e342a46013e85f6f02ee3",
    "semantic_title": "spectral subspace analysis for automatic assessment of pathological speech intelligibility",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pasquale19_interspeech.html": {
    "title": "An Investigation of Therapeutic Rapport Through Prosody in Brief Psychodynamic Psychotherapy",
    "volume": "main",
    "abstract": "Therapeutic alliance, a concept closely related to rapport, is one of the most important variables in psychotherapy. High degrees of synchrony/coordination in the therapeutic session are considered to contribute to rapport, and have received attention in the psychotherapy literature Coordinative behaviours are observable in speech, and they manifest in phenomena such as prosodic accommodation, a dynamic phenomenon closely related to conversational success A preliminary investigation of interpersonal prosodic dynamics in psychotherapy was performed on a database obtained in collaboration with the University of Padua, consisting of 16 recordings making up the entire course of a brief psychodynamic psychotherapy intervention for a 25 year old female volunteer and a 41 years old male psychotherapist The data was analysed with Time Aligned Moving Averages, a method commonly used in interpersonal speech research. Issues of data sparsity are discussed, and preliminary results on the relationship between empathy and anxiety with interpersonal speech dynamics are presented",
    "checked": true,
    "id": "8b7743fa6fe88c287707d9ab66428d9646dd1bb3",
    "semantic_title": "an investigation of therapeutic rapport through prosody in brief psychodynamic psychotherapy",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rueda19_interspeech.html": {
    "title": "Feature Representation of Pathophysiology of Parkinsonian Dysarthria",
    "volume": "main",
    "abstract": "This paper focuses on selecting features that can best represent the pathophysiology of Parkinson's disease (PD) dysarthria. PD dysarthria has often been the subject of feature selection and classification experiments, but rarely have the selected features been attempted to be matched to the pathophysiology of PD dysarthria. PD dysarthria manifests through changes in control of a person's speech production muscles and affects respiration, articulation, resonance, and laryngeal properties, resulting in speech characteristics such as short phrases separated by pauses, reduced speed for non-repetitive syllables or supernormal speed of repetitive syllables, reduced resonance, irregular vowel generation, etc. Articulation, phonation, diadochokinesis (DDK) rhythm, and Empirical Mode Decomposition (EMD) features were extracted from the DDK and sustained /a/ recordings of the Spanish GITA Corpus. These recordings were captured from 50 healthy (HC) and 50 PD subjects. A two-stage filter-wrapper feature selection process was applied to reduce the number of features from 3,534 to 15. These 15 features mainly represent the instability of the voice and rhythm. SVM, Random Forest and Naive Bayes were used to test the discriminative power of the selected features. The results showed that these sustained /a/ and /pa-ta-ka/ stability features could successfully discriminate PD from HC with 70% accuracy",
    "checked": true,
    "id": "786388dc472bb9605122761feb628c23fadf0bdb",
    "semantic_title": "feature representation of pathophysiology of parkinsonian dysarthria",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2019/onu19_interspeech.html": {
    "title": "Neural Transfer Learning for Cry-Based Diagnosis of Perinatal Asphyxia",
    "volume": "main",
    "abstract": "Despite continuing medical advances, the rate of newborn morbidity and mortality globally remains high, with over 6 million casualties every year. The prediction of pathologies affecting newborns based on their cry is thus of significant clinical interest, as it would facilitate the development of accessible, low-cost diagnostic tools. However, the inadequacy of clinically annotated datasets of infant cries limits progress on this task. This study explores a neural transfer learning approach to developing accurate and robust models for identifying infants that have suffered from perinatal asphyxia. In particular, we explore the hypothesis that representations learned from adult speech could inform and improve performance of models developed on infant speech. Our experiments show that models based on such representation transfer are resilient to different types and degrees of noise, as well as to signal loss in time and frequency domains",
    "checked": true,
    "id": "879af323844b284e0082713b45797ef510a984fb",
    "semantic_title": "neural transfer learning for cry-based diagnosis of perinatal asphyxia",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hong19_interspeech.html": {
    "title": "Investigating the Variability of Voice Quality and Pain Levels as a Function of Multiple Clinical Parameters",
    "volume": "main",
    "abstract": "Pain is an internal construct with vocal manifestation that varies as a function of personal and clinical attributes. Understanding the vocal indicators of pain-levels is important in providing an objective analytic in clinical assessment and intervention. In this work, we focus on investigating the variability of voice quality as a function of multiple clinical parameters at different pain-levels, specifically for emergency room patients during triage. Their pain-induced pathological voice quality characteristics are naturally affected by an individual attributes such as age, gender and pain-sites. We conduct a detailed multivariate statistical analysis on a 181 unique patient's vocal quality using recordings of real triage sessions. Our analysis show several important insights, 1) voice quality only varies statistically with pain-levels when interacting effect from other clinical parameters is considered, 2) senior group shows a higher value of voicing probability and shimmer when experiencing severe pain, 3) patients with abdomen pain have a lower jitter and shimmer during severe pain that is different from patients experiencing musculoskeletal pathology, and 4) there could be a relationship between the variation in the voice quality and the neural pathway of pain as evident by interacting with the pain-site factor",
    "checked": true,
    "id": "bfd09ad9c0f1d30982d5d3dfee003cebc413ad80",
    "semantic_title": "investigating the variability of voice quality and pain levels as a function of multiple clinical parameters",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lopez19_interspeech.html": {
    "title": "Assessing Parkinson's Disease from Speech Using Fisher Vectors",
    "volume": "main",
    "abstract": "Parkinson's Disease (PD) is a neuro-degenerative disorder that affects primarily the motor system of the body. Besides other functions, the subject's speech also deteriorates during the disease, which allows for a non-invasive way of automatic screening. In this study, we represent the utterances of subjects having PD and those of healthy controls by means of the Fisher Vector approach. This technique is very common in the area of image recognition, where it provides a representation of the local image descriptors via frequency and high order statistics. In the present work, we used four frame-level feature sets as the input of the FV method, and applied (linear) Support Vector Machines (SVM) for classifying the speech of subjects. We found that our approach offers superior performance compared to classification based on the i-vector and cosine distance approach, and it also provides an efficient combination of machine learning models trained on different feature sets or on different speaker tasks",
    "checked": true,
    "id": "fcd3a04bbcb482b0a0e3e8a11708f9230e288972",
    "semantic_title": "assessing parkinson's disease from speech using fisher vectors",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/klumpp19_interspeech.html": {
    "title": "Feature Space Visualization with Spatial Similarity Maps for Pathological Speech Data",
    "volume": "main",
    "abstract": "The feature vectors of a data set encode information about relations between speaker groups, clusters and outliers. Based on the assumption that these relations are conserved within the spatial properties of feature vectors, we introduce similarity maps to visualize consistencies and deviations in magnitude and orientation between two feature vectors. We also present an iterative approach to find subspaces of a high-dimensional feature space that encode information about predefined speaker clusters. The methods were evaluated with two different data sets, one from chronically hoarse speakers and a second one from Parkinson's Disease patients and a healthy control group. The results showed that similarity maps provide a decent visualization of speaker groups and the spatial properties of their respective feature vectors. With the iterative optimization, it was possible to find features that show pronounced spatial differences between predefined clusters",
    "checked": true,
    "id": "18b6d8d0d0293fd1b6dcf68925b4fdab7f6e0cd7",
    "semantic_title": "feature space visualization with spatial similarity maps for pathological speech data",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chakravarthula19_interspeech.html": {
    "title": "Predicting Behavior in Cancer-Afflicted Patient and Spouse Interactions Using Speech and Language",
    "volume": "main",
    "abstract": "Cancer impacts the quality of life of those diagnosed as well as their spouse caregivers, in addition to potentially influencing their day-to-day behaviors. There is evidence that effective communication between spouses can improve well-being related to cancer but it is difficult to efficiently evaluate the quality of daily life interactions using manual annotation frameworks. Automated recognition of behaviors based on the interaction cues of speakers can help analyze interactions in such couples and identify behaviors which are beneficial for effective communication. In this paper, we present and detail a dataset of dyadic interactions in 85 real-life cancer-afflicted couples and a set of observational behavior codes pertaining to interpersonal communication attributes. We describe and employ neural network-based systems for classifying these behaviors based on turn-level acoustic and lexical speech patterns. Furthermore, we investigate the effect of controlling for factors such as gender, patient/caregiver role and conversation content on behavior classification. Analysis of our preliminary results indicates the challenges in this task due to the nature of the targeted behaviors and suggests that techniques incorporating contextual processing might be better suited to tackle this problem",
    "checked": true,
    "id": "6949c0f959b4729f4212638127413cee11bb0625",
    "semantic_title": "predicting behavior in cancer-afflicted patient and spouse interactions using speech and language",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/qin19_interspeech.html": {
    "title": "Automatic Assessment of Language Impairment Based on Raw ASR Output",
    "volume": "main",
    "abstract": "For automatic assessment of language impairment in natural speech, properly designed text-based features are needed. The feature design relies on experts' domain knowledge and the feature extraction process may undesirably involve manual effort on transcribing. This paper describes a novel approach to automatic assessment of language impairment in narrative speech of people with aphasia (PWA), without explicit knowledge-driven feature design. A convolutional neural network (CNN) is used to extract language impairment related text features from the output of an automatic speech recognition (ASR) system or, if available, the manual transcription of input speech. To mitigate the adverse effect of ASR errors, confusion network is adopted to improve the robustness of embedding representation of ASR output. The proposed approach is evaluated on the task of discriminating severe PWA from mild PWA based on Cantonese narrative speech. Experimental results confirm the effectiveness of automatically learned text features. It is also shown that CNN models trained with text input and acoustic features are complementary to each other",
    "checked": true,
    "id": "172dbbeab0b145f7fe8b25a2d80050451bb0a4ea",
    "semantic_title": "automatic assessment of language impairment based on raw asr output",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fu19_interspeech.html": {
    "title": "Effects of Spectral and Temporal Cues to Mandarin Concurrent-Vowels Identification for Normal-Hearing and Hearing-Impaired Listeners",
    "volume": "main",
    "abstract": "In Mandarin Chinese, lexical Tones are inherently bonded with vowels, making both spectral and temporal cues available for speech perception. Temporal cues provided by Tone contrast have been shown facilitating segregation in Mandarin concurrent-vowels identification (MCVI). The present study investigated the effect of spectral cue measured by vowel contrast within the syllable-pair on MCVI, both for normal-hearing (NH) and hearing-impaired (HI) listeners. Acoustic cues of duration and mean F0 difference were carefully controlled. Results exhibited that facilitation from vowel contrast existed for NH listeners but was reduced for HI listeners. Identification score positively correlated with the spectral envelope contrast of different vowel-pairs for both groups, but the coefficient for HI listeners was lower. Further analyses based on a power function model revealed more weighting of temporal cues than spectral cues for NH listeners, while the contributions were equal for HI listeners. These results suggested that the spectral cue provided by vowel contrast could facilitate the MCVI, and auditory processing of temporal cues might be more susceptible to hearing loss than that of spectral cues. These findings have instructions for designing speech processing algorithms for Mandarin-speaking HI listeners",
    "checked": true,
    "id": "737f2943ccea6df5d217137b1f0f19c793bef116",
    "semantic_title": "effects of spectral and temporal cues to mandarin concurrent-vowels identification for normal-hearing and hearing-impaired listeners",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zayats19_interspeech.html": {
    "title": "Disfluencies and Human Speech Transcription Errors",
    "volume": "main",
    "abstract": "This paper explores contexts associated with errors in transcription of spontaneous speech, shedding light on human perception of disfluencies and other conversational speech phenomena. A new version of the Switchboard corpus is provided with disfluency annotations for careful speech transcripts, together with results showing the impact of transcription errors on evaluation of automatic disfluency detection",
    "checked": true,
    "id": "c58bad68e621333b6804990212fb9986e6a69f72",
    "semantic_title": "disfluencies and human speech transcription errors",
    "citation_count": 27
  },
  "https://www.isca-speech.org/archive/interspeech_2019/parhammer19_interspeech.html": {
    "title": "The Influence of Distraction on Speech Processing: How Selective is Selective Attention?",
    "volume": "main",
    "abstract": "The present study investigated the effects of selective attention on the processing of morphosyntactic errors in unattended parts of speech. Two groups of German native (L1) speakers participated in the present study. Participants listened to sentences in which irregular verbs were manipulated in three different conditions (correct, incorrect but attested ablaut pattern, incorrect and crosslinguistically unattested ablaut pattern). In order to track fast dynamic neural reactions to the stimuli, electroencephalography was used. After each sentence, participants in Experiment 1 performed a semantic judgement task, which deliberately distracted the participants from the syntactic manipulations and directed their attention to the semantic content of the sentence. In Experiment 2, participants carried out a syntactic judgement task, which put their attention on the critical stimuli. The use of two different attentional tasks allowed for investigating the impact of selective attention on speech processing and whether morphosyntactic processing steps are performed automatically. In Experiment 2, the incorrect attested condition elicited a larger N400 component compared to the correct condition, whereas in Experiment 1 no differences between conditions were found. These results suggest that the processing of morphosyntactic violations in irregular verbs is not entirely automatic but seems to be strongly affected by selective attention",
    "checked": true,
    "id": "b50ced46d85ad3149e0ae849d1f0d54b518f7beb",
    "semantic_title": "the influence of distraction on speech processing: how selective is selective attention?",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hazan19_interspeech.html": {
    "title": "Subjective Evaluation of Communicative Effort for Younger and Older Adults in Interactive Tasks with Energetic and Informational Masking",
    "volume": "main",
    "abstract": "The impact of energetic (EM) and informational masking (IM) on speech communication is typically evaluated using perception tests that do not involve actual communication. Here, ratings of effort, concentration and degree of interference were obtained for 51 young, middle-aged and older adults after they had completed communicative tasks (Diapix) with another participant in conditions in which no noise, speech-shaped noise, or three voices were heard in the background. They also completed background sensory and cognitive tests and a quality of hearing questionnaire. The EM condition was perceived as less effortful, requiring less concentration and easier to ignore than those involving IM. Effects were generally greater for talkers taking the lead in the interaction. Even though the two older groups were more affected by IM than young adults in a speech in noise perception test, age did not impact on ratings of effort and ability to ignore the noise in the diapix communicative task. Only for concentration ratings, did the Older Adult group give similar ratings in quiet as when EM was present. Together, these results suggest that evaluations that purely assess receptive speech in older adults do not fully represent the impact of sources of interference on speech communication",
    "checked": true,
    "id": "31aca13c21768398049a4dd270202d8de8716112",
    "semantic_title": "subjective evaluation of communicative effort for younger and older adults in interactive tasks with energetic and informational masking",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/davis19_interspeech.html": {
    "title": "Perceiving Older Adults Producing Clear and Lombard Speech",
    "volume": "main",
    "abstract": "We investigated the perceptual salience of clear and Lombard speech adaptations by older adults (OA) communicating to a younger partner in a diapix task. The aim was to determine whether these two speech styles are perceptually distinct (for auditory and visual speech). The communication setting involved either the younger partner only in babble noise (BAB_partner) or both talkers in babble noise (BAB_both). In the control condition (NORM), both talkers heard normally. To determine how perceptible OA adaptions to these noise conditions were, short (1–4 s) auditory only and visual only recordings of the OA talking to their partner were presented in two perception experiments. In Experiment 1, half of the OA stimuli were from the BAB_partner and half from the NORM condition; and participants were asked to judge whether the older adult was taking to a person who could hear them well or to someone who has trouble hearing them. In Experiment 2 participants decided between NORM and BAB_both stimuli. Participants did both sound-only and visual-only versions. Results showed both adaptations were perceived better than chance; the BAB_both condition was discriminated better from NORM than the BAB_partner one, and auditory judgements were better than visual ones (although these were correlated)",
    "checked": true,
    "id": "bdd5e8a0df5a8e324b9531e51472cc8fa7acfe4c",
    "semantic_title": "perceiving older adults producing clear and lombard speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ariasvergara19_interspeech.html": {
    "title": "Phone-Attribute Posteriors to Evaluate the Speech of Cochlear Implant Users",
    "volume": "main",
    "abstract": "People with pre- and postlingual onset of deafness, i.e, age of occurrence of hearing loss, often present speech production problems even after hearing rehabilitation by cochlear implantation. In this paper, the speech of 20 prelinguals (aged between 18 to 71 years old), 20 postlinguals (aged between 33 to 78 years old) and 20 healthy control (aged between 31 to 62 years old) German native speakers are analyzed considering phone-attribute features extracted with pre-trained Deep Neural Networks. Speech signals are analyzed with reference to the manner of articulation of consonants according to 5 groups: nasals, sibilants, fricatives, voiced-stops, and voiceless-stops. According to the results, it is possible to detect alterations in the consonant production of CI users when compared with healthy speakers. A comprehensive evaluation of speech changes of CI users will help in the rehabilitation after deafening",
    "checked": true,
    "id": "458176de2ae441a128c2073e841db8771fb33e8d",
    "semantic_title": "phone-attribute posteriors to evaluate the speech of cochlear implant users",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hodoshima19_interspeech.html": {
    "title": "Effects of Urgent Speech and Congruent/Incongruent Text on Speech Intelligibility in Noise and Reverberation",
    "volume": "main",
    "abstract": "Public-address (PA) announcements are widely used, but noise and reverberation can render them unintelligible. Furthermore, in an emergency, textual information available to smartphone users or displayed on electronic bulletin boards may not coincide with PA announcements, and this mismatch may degrade the intelligibility of PA announcements. This study investigated how speech spoken in a normal/urgent style and preceding congruent/incongruent textual information affected word intelligibility and perceived urgency in noisy and reverberant environments. The results obtained from 18 participants showed that the word correct rate (WCR) was significantly higher for urgently spoken speech than for normal speech, and for congruent text than for incongruent/no text. However, there was no speaking style-text interaction, indicating that the improvement in WCR provided by urgent speech over normal speech was the same regardless of the preceding text condition. This suggests that listeners rely more on visual information when speech intelligibility is poor. The results for perceived urgency also showed that the congruent condition was rated \"evacuate now\", while the incongruent condition was rated \"wait and see\". These results suggest that simple combinations of speaking style and textual information decrease the intelligibility of emergency PA announcements, and audio-visual incongruence must be considered",
    "checked": true,
    "id": "d7f92f2437da076260a9eede0d3d813763679950",
    "semantic_title": "effects of urgent speech and congruent/incongruent text on speech intelligibility in noise and reverberation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mamun19_interspeech.html": {
    "title": "Quantifying Cochlear Implant Users' Ability for Speaker Identification Using CI Auditory Stimuli",
    "volume": "main",
    "abstract": "Speaker recognition is a biometric modality that uses underlying speech information to determine the identity of the speaker. Speaker Identification (SID) under noisy conditions is one of the challenging topics in the field of speech processing, specifically when it comes to individuals with cochlear implants (CI). This study analyzes and quantifies the ability of CI-users to perform speaker identification based on direct electric auditory stimuli. CI users employ a limited number of frequency bands (8~22) and use electrodes to directly stimulate the Basilar Membrane/Cochlear in order to recognize the speech signal. The sparsity of electric stimulation within the CI frequency range is a prime reason for loss in human speech recognition, as well as SID performance. Therefore, it is assumed that CI-users might be unable to recognize and distinguish a speaker given dependent information such as formant frequencies, pitch etc. which are lost to un-simulated electrodes. To quantify this assumption, the input speech signal is processed using a CI Advanced Combined Encoder (ACE) signal processing strategy to construct the CI auditory electrodogram. The proposed study uses 50 speakers from each of three different databases for training the system using two different classifiers under quiet, and tested under both quiet and noisy conditions. The objective result shows that, the CI users can effectively identify a limited number of speakers. However, their performance decreases when more speakers are added in the system, as well as when noisy conditions are introduced. This information could therefore be used for improving CI-user signal processing techniques to improve human SID",
    "checked": true,
    "id": "075d6447a6d27f3187297313b7bc7eda8d15ee33",
    "semantic_title": "quantifying cochlear implant users' ability for speaker identification using ci auditory stimuli",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/felker19_interspeech.html": {
    "title": "Lexically Guided Perceptual Learning of a Vowel Shift in an Interactive L2 Listening Context",
    "volume": "main",
    "abstract": "Lexically guided perceptual learning has traditionally been studied with ambiguous consonant sounds to which native listeners are exposed in a purely receptive listening context. To extend previous research, we investigate whether lexically guided learning applies to a vowel shift encountered by non-native listeners in an interactive dialogue. Dutch participants played a two-player game in English in either a control condition, which contained no evidence for a vowel shift, or a lexically constraining condition, in which onscreen lexical information required them to re-interpret their interlocutor's /ɪ/ pronunciations as representing /ε/. A phonetic categorization pre-test and post-test were used to assess whether the game shifted listeners' phonemic boundaries such that more of the /ε/-/ɪ/ continuum came to be perceived as /ε/. Both listener groups showed an overall post-test shift toward /ɪ/, suggesting that vowel perception may be sensitive to directional biases related to properties of the speaker's vowel space. Importantly, listeners in the lexically constraining condition made relatively more post-test /ε/ responses than the control group, thereby exhibiting an effect of lexically guided adaptation. The results thus demonstrate that non-native listeners can adjust their phonemic boundaries on the basis of lexical information to accommodate a vowel shift learned in interactive conversation",
    "checked": true,
    "id": "da648048f436eb9b9801cc082ad52e514ab88e6c",
    "semantic_title": "lexically guided perceptual learning of a vowel shift in an interactive l2 listening context",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/paulus19_interspeech.html": {
    "title": "Talker Intelligibility and Listening Effort with Temporally Modified Speech",
    "volume": "main",
    "abstract": "Individual differences in talker acoustics substantially affect intelligibility in adverse listening conditions. Spectral enhancement has been found to reliably boost intelligibility in noise while temporal enhancement remains less effective. A potentially mediating factor that has been ignored so far is listening effort, as objectively assessed by the pupil dilation response. In two perception experiments, we measured intelligibility (keyword recall scores) and listening effort (pupil dilation) for two talkers in two listening conditions and with varying degrees of temporal modification. Results suggest that while keyword recall scores are sensitive to individual talker differences across listening conditions, the pupil dilation response reflects the degree of temporal and spectral distortion introduced by the signal processing techniques",
    "checked": true,
    "id": "c8489ba729f33c1fe61e2196c984c4451d352d13",
    "semantic_title": "talker intelligibility and listening effort with temporally modified speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ward19b_interspeech.html": {
    "title": "R2SPIN: Re-Recording the Revised Speech Perception in Noise Test",
    "volume": "main",
    "abstract": "Speech in noise tests are an important clinical and research tool for understanding speech perception in realistic, adverse listening conditions. Though relatively simple to implement, their development is time and resource intensive. As a result, many tests still in use (and their corresponding recordings) are outdated and no longer fit for purpose. This work takes the popular Revised Speech Perception In Noise (RSPIN) Test and updates it with improved recordings and the addition of a female speaker. It outlines and evaluates a methodology which others can apply to legacy recordings of speech in noise tests to update them and ensure their ongoing usability. This paper describes the original test along with its use over the last four decades and the rationale for re-recording. The new speakers, new accent (Received Pronunciation) and recording methodology are then outlined. Subjective and objective analysis of the new recordings for normal hearing listeners are then given. The paper concludes with recommendations for using the R SPIN",
    "checked": true,
    "id": "2e37daa18ae5b945cb5b8326f7e28172a21221e5",
    "semantic_title": "r2spin: re-recording the revised speech perception in noise test",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19j_interspeech.html": {
    "title": "Contributions of Consonant-Vowel Transitions to Mandarin Tone Identification in Simulated Electric-Acoustic Hearing",
    "volume": "main",
    "abstract": "For hearing-impaired listeners fitted with cochlear implants (CIs), they rely on electric (E) stimulation with primarily slow-varying temporal information but limited spectral information for their speech perception. Many recent studies showed that for those implanted listeners with residual low-frequency hearing, the combined electric-acoustic (E+A) stimulation could significantly improve their speech perception in adverse listening conditions. The present work assessed the contributions of consonant-vowel transitions to Mandarin tone identification via a vocoder based simulation of E+A stimulation. Isolated Mandarin words were processed to preserve full consonants and vowel onsets across consonant-vowel transitions, and replace the rest with noise. The two types of vocoded stimuli, simulating E and E+A stimulations, were presented to normal-hearing Mandarin-speaking listeners to identify lexical tones. Results consistently showed the advantage of E+A stimulation over E-only stimulation when full consonants and the same amount of vowel onset segments were preserved for lexical tone identification. In addition, compared with E stimulation with full vowel segments, the combined-stimulation advantage was observed even when only a small portion of vowel onset segments were presented. Results in this work suggested that in E+A stimulation, segmental contributions were able to provide tone identification benefit relative to E stimulation with the entire Mandarin words",
    "checked": true,
    "id": "40594e5689aa9367cda42ad650e11dd9c6e99ff9",
    "semantic_title": "contributions of consonant-vowel transitions to mandarin tone identification in simulated electric-acoustic hearing",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pirhosseinloo19_interspeech.html": {
    "title": "Monaural Speech Enhancement with Dilated Convolutions",
    "volume": "main",
    "abstract": "In this study, we propose a novel dilated convolutional neural network for enhancing speech in noisy and reverberant environments. The proposed model incorporates dilated convolutions for tracking a target speaker through context aggregations, skip connections, and residual learning for mapping-based monaural speech enhancement. The performance of our model was evaluated in a variety of simulated environments having different reverberation times and quantified using two objective measures. Experimental results show that the proposed model outperforms a long short-term memory (LSTM), a gated residual network (GRN) and convolutional recurrent network (CRN) model in terms of objective speech intelligibility and speech quality in noisy and reverberant environments. Compared to LSTM, CRN and GRN, our method has improved generalization to untrained speakers and noise, and has fewer training parameters resulting in greater computational efficiency",
    "checked": true,
    "id": "d4f21a5808824e9f67c6d9da4d84ed01e6dde40d",
    "semantic_title": "monaural speech enhancement with dilated convolutions",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liao19b_interspeech.html": {
    "title": "Noise Adaptive Speech Enhancement Using Domain Adversarial Training",
    "volume": "main",
    "abstract": "In this study, we propose a novel noise adaptive speech enhancement (SE) system, which employs a domain adversarial training (DAT) approach to tackle the issue of a noise type mismatch between the training and testing conditions. Such a mismatch is a critical problem in deep-learning-based SE systems. A large mismatch may cause a serious performance degradation to the SE performance. Because we generally use a well-trained SE system to handle various unseen noise types, a noise type mismatch commonly occurs in real-world scenarios. The proposed noise adaptive SE system contains an encoder-decoder-based enhancement model and a domain discriminator model. During adaptation, the DAT approach encourages the encoder to produce noise-invariant features based on the information from the discriminator model and consequentially increases the robustness of the enhancement model to unseen noise types. Herein, we regard stationary noises as the source domain (with the ground truth of clean speech) and non-stationary noises as the target domain (without the ground truth). We evaluated the proposed system on TIMIT sentences. The experiment results show that the proposed noise adaptive SE system successfully provides significant improvements in PESQ (19.0%), SSNR (39.3%), and STOI (27.0%) over the SE system without an adaptation",
    "checked": true,
    "id": "2dd2140f08f67d3cd790a394cbe10150ca3c7507",
    "semantic_title": "noise adaptive speech enhancement using domain adversarial training",
    "citation_count": 41
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ge19_interspeech.html": {
    "title": "Environment-Dependent Attention-Driven Recurrent Convolutional Neural Network for Robust Speech Enhancement",
    "volume": "main",
    "abstract": "Speech enhancement aims to keep the real speech signal and reduce noise for building robust communication systems. Under the success of DNN, significant progress has been made. Nevertheless, accuracy of the speech enhancement system is not satisfactory due to insufficient consideration of varied environmental and contextual information in complex cases. To address these problems, this research proposes an end-to-end environment-dependent attention-driven approach. The local frequency-temporal pattern via convolutional neural network is fully employed without pooling operation. It then integrates an attention mechanism into bidirectional long short-term memory to acquire the weighted dynamic context between consecutive frames. Furthermore, dynamic environment estimation and phase correction further improve the generalization ability. Extensive experimental results on REVERB challenge demonstrated that the proposed approach outperformed existing methods, improving PESQ from 2.56 to 2.87 and SRMR from 4.95 to 5.50 compared with conventional DNN",
    "checked": true,
    "id": "b4386efbf7f3decabe63e5b08ae6a61081500e30",
    "semantic_title": "environment-dependent attention-driven recurrent convolutional neural network for robust speech enhancement",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pariente19_interspeech.html": {
    "title": "A Statistically Principled and Computationally Efficient Approach to Speech Enhancement Using Variational Autoencoders",
    "volume": "main",
    "abstract": "Recent studies have explored the use of deep generative models of speech spectra based on variational autoencoders (VAEs), combined with unsupervised noise models, to perform speech enhancement. These studies developed iterative algorithms involving either Gibbs sampling or gradient descent at each step, making them computationally expensive. This paper proposes a variational inference method to iteratively estimate the power spectrogram of the clean speech. Our main contribution is the analytical derivation of the variational steps in which the encoder of the pre-learned VAE can be used to estimate the variational approximation of the true posterior distribution, using the very same assumption made to train VAEs. Experiments show that the proposed method produces results on par with the aforementioned iterative methods using sampling, while decreasing the computational cost by a factor 36 to reach a given performance",
    "checked": true,
    "id": "1814be2b0090c07ec9fe9e884fce45ced579e366",
    "semantic_title": "a statistically principled and computationally efficient approach to speech enhancement using variational autoencoders",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lin19d_interspeech.html": {
    "title": "Speech Enhancement Using Forked Generative Adversarial Networks with Spectral Subtraction",
    "volume": "main",
    "abstract": "Speech enhancement techniques that use a generative adversarial network (GAN) can effectively suppress noise while allowing models to be trained end-to-end. However, such techniques directly operate on time-domain waveforms, which are often highly-dimensional and require extensive computation. This paper proposes a novel GAN-based speech enhancement method, referred to as S-ForkGAN, that operates on log-power spectra rather than on time-domain speech waveforms, and uses a forked GAN structure to extract both speech and noise information. By operating on log-power spectra, one can seamlessly include conventional spectral subtraction techniques, and the parameter space typically has a lower dimension. The performance of S-ForkGAN is assessed for automatic speech recognition (ASR) using the TIMIT data set and a wide range of noise conditions. It is shown that S-ForkGAN outperforms existing GAN-based techniques and that it has a lower complexity",
    "checked": true,
    "id": "9af9555fcfb9764e688713f088b9111323bda2d3",
    "semantic_title": "speech enhancement using forked generative adversarial networks with spectral subtraction",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zezario19_interspeech.html": {
    "title": "Specialized Speech Enhancement Model Selection Based on Learned Non-Intrusive Quality Assessment Metric",
    "volume": "main",
    "abstract": "Previous studies have shown that a specialized speech enhancement model can outperform a general model when the test condition is matched to the training condition. Therefore, choosing the correct (matched) candidate model from a set of ensemble models is critical to achieve generalizability. Although the best decision criterion should be based directly on the evaluation metric, the need for a clean reference makes it impractical for employment. In this paper, we propose a novel specialized speech enhancement model selection (SSEMS) approach that applies a non-intrusive quality estimation model, termed Quality-Net, to solve this problem. Experimental results first confirm the effectiveness of the proposed SSEMS approach. Moreover, we observe that the correctness of Quality-Net in choosing the most suitable model increases as input noisy SNR increases, and thus the results of the proposed systems outperform another auto-encoder-based model selection and a general model, particularly under high SNR conditions",
    "checked": true,
    "id": "f1a4f16e9847a6bfb95ba0b9aa81405f9ce0af05",
    "semantic_title": "specialized speech enhancement model selection based on learned non-intrusive quality assessment metric",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chuang19_interspeech.html": {
    "title": "Speaker-Aware Deep Denoising Autoencoder with Embedded Speaker Identity for Speech Enhancement",
    "volume": "main",
    "abstract": "Previous studies indicate that noise and speaker variations can degrade the performance of deep-learning-based speech-enhancement systems. To increase the system performance over environmental variations, we propose a novel speaker-aware system that integrates a deep denoising autoencoder (DDAE) with an embedded speaker identity. The overall system first extracts embedded speaker identity features using a neural network model; then the DDAE takes the augmented features as input to generate enhanced spectra. With the additional embedded features, the speech-enhancement system can be guided to generate the optimal output corresponding to the speaker identity. We tested the proposed speech-enhancement system on the TIMIT dataset. Experimental results showed that the proposed speech-enhancement system could improve the sound quality and intelligibility of speech signals from additive noise-corrupted utterances. In addition, the results suggested system robustness for unseen speakers when combined with speaker features",
    "checked": true,
    "id": "847c4304d4b9596a3638dc3c75d42fbf2292c598",
    "semantic_title": "speaker-aware deep denoising autoencoder with embedded speaker identity for speech enhancement",
    "citation_count": 25
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19g_interspeech.html": {
    "title": "Investigation of Cost Function for Supervised Monaural Speech Separation",
    "volume": "main",
    "abstract": "Speech separation aims to improve the speech quality of noisy speech. Deep learning based speech separation methods usually use mean square error (MSE) as the cost function, which measures the distance between model output and training target. However, the MSE does not match the evaluation metrics perfectly. Optimizing the MSE does not directly lead to improvement in the commonly used metrics, such as short-time objective intelligibility (STOI), perceptual evaluation of speech quality (PESQ), signal-to-noise ratio (SNR) and source-to-distortion ratio (SDR). In this study, we inspect some other cost function candidates which are based on divergence, e.g., Kullback-Leibler and Itakura-Saito divergence. A conjecture about the correlation between cost function and evaluation metrics is proposed and examined to explain why these cost functions behave differently. On the basis of the proposed conjecture, the optimal cost function candidate is selected. The experimental results validate our conjecture",
    "checked": true,
    "id": "7bff9c71de4ee15415ef69eb330f89ad599abca9",
    "semantic_title": "investigation of cost function for supervised monaural speech separation",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shi19b_interspeech.html": {
    "title": "Deep Attention Gated Dilated Temporal Convolutional Networks with Intra-Parallel Convolutional Modules for End-to-End Monaural Speech Separation",
    "volume": "main",
    "abstract": "Monaural speech separation techniques are far from satisfactory and are a challenging task due to interference from multiple sources. Recently the deep dilated temporal convolutional networks (TCN) has proven to be very effective in sequence modeling. This work explores how to extend TCN to result a new, state-of-the-art monaural speech separation method. First, a new gating mechanism is introduced and added to generate a gated TCN. The gated activation controls the flow of information. Further in order to combine multiple training models to reduce the performance variance and improve the effect of speech separation, we propose to use the principle of ensemble learning in the gated TCN architecture by replacing the convolutional modules corresponding to each dilated factor with multiple identical branches of the convolutional components. For the sake of objectivity, we propose to train the network by directly optimizing in a permutation invariant training (PIT) style of the utterance level signal-to-distortion ratio (SDR). Our experiments with the public WSJ0-2mix data corpus resulted in an 18.2 dB improvement in SDR, indicating that our proposed network can improve the performance of speaker separation tasks",
    "checked": true,
    "id": "99154320cb5ca51ef218ea784151416f3160ac51",
    "semantic_title": "deep attention gated dilated temporal convolutional networks with intra-parallel convolutional modules for end-to-end monaural speech separation",
    "citation_count": 26
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19k_interspeech.html": {
    "title": "Masking Estimation with Phase Restoration of Clean Speech for Monaural Speech Enhancement",
    "volume": "main",
    "abstract": "Deep neural network (DNN) has become a popular means for separating target speech from noisy speech due to its good performance for learning a mapping relationship between the training target and noisy speech. For the DNN-based methods, the time-frequency (T-F) mask commonly used as the training target has a significant impact on the performance of speech restoration. However, the T-F mask generally modifies magnitude spectrum of noisy speech and leaves phase spectrum unchanged in enhancing process. The recent studies have revealed that incorporating phase spectrum information into the T-F mask can effectively improve perceptual quality of the enhanced speech. So, in this paper, we present two T-F masks to simultaneously enhance magnitude and phase of speech spectrum based on non-correlation assumption of real part and imaginary part about speech spectrum, and use them as the training target of the DNN model. Experimental results show that, in comparison with the reference methods, the proposed method can obtain an effective improvement in speech quality for different signal to noise ratio (SNR) conditions",
    "checked": true,
    "id": "ff13daa057ed01854738f9d4923668590a10b415",
    "semantic_title": "masking estimation with phase restoration of clean speech for monaural speech enhancement",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/llombart19b_interspeech.html": {
    "title": "Progressive Speech Enhancement with Residual Connections",
    "volume": "main",
    "abstract": "This paper studies the Speech Enhancement based on Deep Neural Networks. The proposed architecture gradually follows the signal transformation during enhancement by means of a visualization probe at each network block. Alongside the process, the enhancement performance is visually inspected and evaluated in terms of regression cost. This progressive scheme is based on Residual Networks. During the process, we investigate a residual connection with a constant number of channels, including internal state between blocks, and adding progressive supervision. The insights provided by the interpretation of the network enhancement process leads us to design an improved architecture for the enhancement purpose. Following this strategy, we are able to obtain speech enhancement results beyond the state-of-the-art, achieving a favorable trade-off between dereverberation and the amount of spectral distortion",
    "checked": true,
    "id": "2eb14480a3e10bf314499cd9a4e8e3f5a0745219",
    "semantic_title": "progressive speech enhancement with residual connections",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19k_interspeech.html": {
    "title": "Acoustic Model Bootstrapping Using Semi-Supervised Learning",
    "volume": "main",
    "abstract": "This work aims at bootstrapping acoustic model training for automatic speech recognition with small amounts of human-labeled speech data and large amounts of machine-labeled speech data. Semi-supervised learning is investigated to select the machine-transcribed training samples. Two semi-supervised learning methods are proposed: one is the local-global uncertainty based method which introduces both the local uncertainty from the current utterance and the global uncertainty from the whole data pool into the data selection; the other is the margin based data selection, which selects the utterances near to the decision boundary through language model tuning. The experimental results based on a Japanese far-field automatic speech recognition system indicate that the acoustic model trained by automatically transcribed speech data achieve about 17% relative gain when in-domain human annotated data was not available for initialization. While 3.7% relative gain was obtained when the initial acoustic model was trained by small amount of in-domain data",
    "checked": true,
    "id": "7f5f1aaf94176c5d9e25cc3184dd06c33846d1cd",
    "semantic_title": "acoustic model bootstrapping using semi-supervised learning",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mantena19_interspeech.html": {
    "title": "Bandwidth Embeddings for Mixed-Bandwidth Speech Recognition",
    "volume": "main",
    "abstract": "In this paper, we tackle the problem of handling narrowband and wideband speech by building a single acoustic model (AM), also called mixed bandwidth AM. In the proposed approach, an auxiliary input feature is used to provide the bandwidth information to the model, and bandwidth embeddings are jointly learned as part of acoustic model training. Experimental evaluations show that using bandwidth embeddings helps the model to handle the variability of the narrow and wideband speech, and makes it possible to train a mixed-bandwidth AM. Furthermore, we propose to use parallel convolutional layers to handle the mismatch between the narrow and wideband speech better, where separate convolution layers are used for each type of input speech signal. Our best system achieves 13% relative improvement on narrowband speech, while not degrading on wideband speech",
    "checked": true,
    "id": "a0f7e1f456f7195c9ad5b180246e663ae16facd4",
    "semantic_title": "bandwidth embeddings for mixed-bandwidth speech recognition",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/khare19_interspeech.html": {
    "title": "Adversarial Black-Box Attacks on Automatic Speech Recognition Systems Using Multi-Objective Evolutionary Optimization",
    "volume": "main",
    "abstract": "Fooling deep neural networks with adversarial input have exposed a significant vulnerability in the current state-of-the-art systems in multiple domains. Both black-box and white-box approaches have been used to either replicate the model itself or to craft examples which cause the model to fail. In this work, we propose a framework which uses multi-objective evolutionary optimization to perform both targeted and un-targeted black-box attacks on Automatic Speech Recognition (ASR) systems. We apply this framework on two ASR systems: Deepspeech and Kaldi-ASR, which increases the Word Error Rates (WER) of these systems by upto 980%, indicating the potency of our approach. During both un-targeted and targeted attacks, the adversarial samples maintain a high acoustic similarity of 0.98 and 0.97 with the original audio",
    "checked": true,
    "id": "e380347727234a8bd7ed86ea0dc6832acf94f6b4",
    "semantic_title": "adversarial black-box attacks on automatic speech recognition systems using multi-objective evolutionary optimization",
    "citation_count": 36
  },
  "https://www.isca-speech.org/archive/interspeech_2019/soomro19_interspeech.html": {
    "title": "Towards Debugging Deep Neural Networks by Generating Speech Utterances",
    "volume": "main",
    "abstract": "Deep neural networks (DNN) are able to successfully process and classify speech utterances. However, understanding the reason behind a classification by DNN is difficult. One such debugging method used with image classification DNNs is activation maximization, which generates example-images that are classified as one of the classes. In this work, we evaluate applicability of this method to speech utterance classifiers as the means to understanding what DNN \"listens to\". We trained a classifier using the speech command corpus and then use activation maximization to pull samples from the trained model. Then we synthesize audio from features using WaveNet vocoder for subjective analysis. We measure the quality of generated samples by objective measurements and crowd-sourced human evaluations. Results show that when combined with the prior of natural speech, activation maximization can be used to generate examples of different classes. Based on these results, activation maximization can be used to start opening up the DNN black-box in speech tasks",
    "checked": true,
    "id": "0406e639951dcc03241574480aa1ce6ff41dcaa8",
    "semantic_title": "towards debugging deep neural networks by generating speech utterances",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ding19b_interspeech.html": {
    "title": "Compression of CTC-Trained Acoustic Models by Dynamic Frame-Wise Distillation or Segment-Wise N-Best Hypotheses Imitation",
    "volume": "main",
    "abstract": "Knowledge distillation (KD) has been widely used for model compression by learning a simpler student model to imitate the outputs or intermediate representations of a more complex teacher model. The most commonly used KD technique is to minimize a Kullback-Leibler divergence between the output distributions of the teacher and student models. When it is applied to compressing CTC-trained acoustic models, an assumption is made that the teacher and student share the same frame-wise feature-transcription alignment, which is usually not true due to the topology difference of the teacher and student models. In this paper, by making more appropriate assumptions, we propose two KD methods, namely dynamic frame-wise distillation and segment-wise N-best hypotheses imitation. Experimental results on Switchboard-I speech recognition task show that the segment-wise N-best hypotheses imitation outperforms the frame-level and other sequence-level distillation methods, and achieves a relative word error rate reduction of 5%–8% compared with models trained from scratch",
    "checked": true,
    "id": "7854177fff7a8663ec6ee513e9585421ab0dcdf0",
    "semantic_title": "compression of ctc-trained acoustic models by dynamic frame-wise distillation or segment-wise n-best hypotheses imitation",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lopezespejo19_interspeech.html": {
    "title": "Keyword Spotting for Hearing Assistive Devices Robust to External Speakers",
    "volume": "main",
    "abstract": "Keyword spotting (KWS) is experiencing an upswing due to the pervasiveness of small electronic devices that allow interaction with them via speech. Often, KWS systems are speaker-independent, which means that any person — user or not — might trigger them. For applications like KWS for hearing assistive devices this is unacceptable, as only the user must be allowed to handle them. In this paper we propose KWS for hearing assistive devices that is robust to external speakers. A state-of-the-art deep residual network for small-footprint KWS is regarded as a basis to build upon. By following a multi-task learning scheme, this system is extended to jointly perform KWS and users' own-voice/external speaker detection with a negligible increase in the number of parameters. For experiments, we generate from the Google Speech Commands Dataset a speech corpus emulating hearing aids as a capturing device. Our results show that this multi-task deep residual network is able to achieve a KWS accuracy relative improvement of around 32% with respect to a system that does not deal with external speakers",
    "checked": true,
    "id": "db653070854c34a634f723e2f7764e361a5b10d4",
    "semantic_title": "keyword spotting for hearing assistive devices robust to external speakers",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/doulaty19_interspeech.html": {
    "title": "Latent Dirichlet Allocation Based Acoustic Data Selection for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Selecting in-domain data from a large pool of diverse and out-of-domain data is a non-trivial problem. In most cases simply using all of the available data will lead to sub-optimal and in some cases even worse performance compared to carefully selecting a matching set. This is true even for data-inefficient neural models. Acoustic Latent Dirichlet Allocation (aLDA) is shown to be useful in a variety of speech technology related tasks, including domain adaptation of acoustic models for automatic speech recognition and entity labeling for information retrieval. In this paper we propose to use aLDA as a data similarity criterion in a data selection framework. Given a large pool of out-of-domain and potentially mismatched data, the task is to select the best-matching training data to a set of representative utterances sampled from a target domain. Our target data consists of around 32 hours of meeting data (both far-field and close-talk) and the pool contains 2k hours of meeting, talks, voice search, dictation, command-and-control, audio books, lectures, generic media and telephony speech data. The proposed technique for training data selection, significantly outperforms random selection, posterior-based selection as well as using all of the available data",
    "checked": true,
    "id": "7d3cae81b4a689c2a01dbf88ff7d378e5dcc8311",
    "semantic_title": "latent dirichlet allocation based acoustic data selection for automatic speech recognition",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19o_interspeech.html": {
    "title": "Target Speaker Recovery and Recognition Network with Average x-Vector and Global Training",
    "volume": "main",
    "abstract": "It is very challenging to do multi-talker automatic speech recognition (ASR). Some speaker-aware selective methods have been proposed to recover the speech of the target speaker, relying on the auxiliary speaker information provided by an anchor (a clean audio sample of the target speaker). But the performance is unstable depending on the quality of the provided anchors. To address this limitation, we propose to take advantage of the average speaker embeddings to build the target speaker recovery network (TRnet). The TRnet takes the mixed speech and the stable average speaker embeddings to produce the TF masks for the target speech. During training of the TRnet, we summarize the speaker embeddings on the whole training dataset for each speaker, instead of extracting on a randomly picked anchor. On the testing stage, one or very few anchors are enough to get decent recovery results. The results of the TRnet trained with average speaker embeddings show 13% and 12.5% relative improvements on WER and SDR, compared with the short-anchor trained model. Moreover, to mitigate the mismatch between the TRnet and the acoustic model (AM), we adopted two strategies: fine-tuning the AM and training an global TRnet. Both of them bring considerable reductions on WER. The results show that the global trained framework gets superior performance",
    "checked": true,
    "id": "1be6aea7962045aefb28dfe6d983d71539b2d2a3",
    "semantic_title": "target speaker recovery and recognition network with average x-vector and global training",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/suzuki19b_interspeech.html": {
    "title": "Lyrics Recognition from Singing Voice Focused on Correspondence Between Voice and Notes",
    "volume": "main",
    "abstract": "Lyrics recognition from singing voice is one of the most important techniques for query-by-singing music information retrieval systems. Lyrics information realizes a higher retrieval performance than retrieval using only melody information However, recognizing a song lyrics from singing voice is very difficult. In order to improve recognition, a new method focused on correspondence between voice and notes has been proposed. Note boundary scores are calculated for each frame, and these values are included in feature vectors by expanding their dimensions. The marker HMM is defined to correspond to feature vectors located at note boundaries, and the marker HMM is inserted among all morae in a pronunciation dictionary. As a result, the recognizer restricts an individual mora to correspond to only one note We also modified the marker HMM in order to account for short pauses in a particular position. A short pause corresponding to a musical rest or breath may occur after any morae, even if inside a word. The short pause HMM is concatenated to the marker HMM, and a skip transition arc of the short pause HMM is also introduced From experimental results, the proposed model provided higher word accuracy than the baseline model. It improved word accuracy from 85.71% to 93.18%, which means that 52.3% of the word error rate decreased. Insertion errors, especially, were drastically suppressed",
    "checked": true,
    "id": "2f2d8742224cfeae9828f3d976894cca4dcf93b7",
    "semantic_title": "lyrics recognition from singing voice focused on correspondence between voice and notes",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hsu19b_interspeech.html": {
    "title": "Transfer Learning from Audio-Visual Grounding to Speech Recognition",
    "volume": "main",
    "abstract": "Transfer learning aims to reduce the amount of data required to excel at a new task by re-using the knowledge acquired from learning other related tasks. This paper proposes a novel transfer learning scenario, which distills robust phonetic features from grounding models that are trained to tell whether a pair of image and speech are semantically correlated, without using any textual transcripts. As semantics of speech are largely determined by its lexical content, grounding models learn to preserve phonetic information while disregarding uncorrelated factors, such as speaker and channel. To study the properties of features distilled from different layers, we use them as input separately to train multiple speech recognition models. Empirical results demonstrate that layers closer to input retain more phonetic information, while following layers exhibit greater invariance to domain shift. Moreover, while most previous studies include training data for speech recognition for feature extractor training, our grounding models are not trained on any of those data, indicating more universal applicability to new domains",
    "checked": true,
    "id": "5b1d97b4b914dfc8df314e26e7454464a4dccc29",
    "semantic_title": "transfer learning from audio-visual grounding to speech recognition",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2019/luo19c_interspeech.html": {
    "title": "Cross-Corpus Speech Emotion Recognition Using Semi-Supervised Transfer Non-Negative Matrix Factorization with Adaptation Regularization",
    "volume": "main",
    "abstract": "This paper focuses on a cross-corpus speech emotion recognition (SER) task, in which there are some mismatches between the training corpus and the testing corpus. Meanwhile, the label information of the training corpus is known, while the label information of the testing corpus is entirely unknown. To alleviate the influence of these mismatches on the recognition system under this setting, we present a non-negative matrix factorization (NMF) based cross-corpus speech emotion recognition method, called semi-supervised adaptation regularized transfer NMF (SATNMF). The core idea of SATNMF is to incorporate the label information of training corpus into NMF, and seek a latent low-rank feature space, in which the marginal and conditional distribution differences between the two corpora can be minimized simultaneously. Specifically, in this induced feature space, the maximum mean discrepancy (MMD) criterion is used to measure the discrepancies of not only two corpora, but also each class within the two corpora. Moreover, to further exploit the knowledge of the marginal distributions, their underlying manifold structure is considered by using the manifold regularization. Experiments on four popular emotional corpora show that the proposed method achieves better recognition accuracies than state-of-the-art methods",
    "checked": true,
    "id": "a7177f4192dfdb725f8a5982d12c41098b5aa94b",
    "semantic_title": "cross-corpus speech emotion recognition using semi-supervised transfer non-negative matrix factorization with adaptation regularization",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tammewar19_interspeech.html": {
    "title": "Modeling User Context for Valence Prediction from Narratives",
    "volume": "main",
    "abstract": "Automated prediction of valence, one key feature of a person's emotional state, from individuals' personal narratives may provide crucial information for mental healthcare (e.g. early diagnosis of mental diseases, supervision of disease course, etc.). In the Interspeech 2018 ComParE Self-Assessed Affect challenge, the task of valence prediction was framed as a three-class classification problem using 8 seconds fragments from individuals' narratives. As such, the task did not allow for exploring contextual information of the narratives. In this work, we investigate the intrinsic information from multiple narratives recounted by the same individual in order to predict their current state-of-mind. Furthermore, with generalizability in mind, we decided to focus our experiments exclusively on textual information as the public availability of audio narratives is limited compared to text. Our hypothesis is that context modeling might provide insights about emotion triggering concepts (e.g. events, people, places) mentioned in the narratives that are linked to an individual's state of mind. We explore multiple machine learning techniques to model narratives. We find that the models are able to capture inter-individual differences, leading to more accurate predictions of an individual's emotional state, as compared to single narratives",
    "checked": true,
    "id": "c5d796bdce25f9a65b56db9dde65bb767897e92e",
    "semantic_title": "modeling user context for valence prediction from narratives",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chakraborty19_interspeech.html": {
    "title": "Front-End Feature Compensation and Denoising for Noise Robust Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Front-end processing is one of the ways to impart noise robustness to speech emotion recognition systems in mismatched scenarios. Here, we implement and compare different frontend robustness techniques for their efficacy in speech emotion recognition. First, we use a feature compensation technique based on the Vector Taylor Series (VTS) expansion of noisy Mel-Frequency Cepstral Coefficients (MFCCs). Next, we improve upon the feature compensation technique by using the VTS expansion with auditory masking formulation. We have also looked into the applicability of 10 -root compression in MFCC computation. Further, a Time Delay Neural Network based Denoising Autoencoder (TDNN-DAE) is implemented to estimate the clean MFCCs from the noisy MFCCs. These techniques have not been investigated yet for their suitability to robust speech emotion recognition task. The performance of these front-end techniques are compared with the Non-Negative Matrix Factorization (NMF) based front-end. Relying on extensive experiments done on two standard databases (EmoDB and IEMOCAP), contaminated with 5 types of noise, we show that these techniques provide significant performance gain in emotion recognition task. We also show that along with front-end compensation, applying feature selection to non-MFCC high-level descriptors results in better performance",
    "checked": true,
    "id": "297030886dd4fa4a57df41448b55fa1742df5bd9",
    "semantic_title": "front-end feature compensation and denoising for noise robust speech emotion recognition",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19p_interspeech.html": {
    "title": "The Contribution of Acoustic Features Analysis to Model Emotion Perceptual Process for Language Diversity",
    "volume": "main",
    "abstract": "The multi-layered perceptual process of emotion in human speech plays an essential role in the field of affective computing for underlying a speaker's state. However, a comprehensive process analysis of emotion perception is still challenging due to the lack of powerful acoustic features allowing accurate inference of emotion across speaker and language diversities. Most previous research works study acoustic features mostly using Fourier transform, short time Fourier transform or linear predictive coding. Even though these features may be useful for stationary signal within short frames, they may not capture the localized event adequately as speech transmits emotion information dynamically over time. This case introduces a set of acoustic features via wavelet transform analysis of the speech signal, and specifically, models the perceptual process of emotion for language diversity. For this aim, the proposed features are analyzed in a three-layer emotion perception model across multiple languages. Experiments show that the proposed acoustic features significantly enhance the perceptual process of emotion and render a better result in multilingual emotion recognition when compared it to the widely used prosodic and spectral features, as well as their combination in literature",
    "checked": true,
    "id": "ce4d2dfd0bbc46ecdffbe541a3504e415d93820d",
    "semantic_title": "the contribution of acoustic features analysis to model emotion perceptual process for language diversity",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rajan19_interspeech.html": {
    "title": "Design and Development of a Multi-Lingual Speech Corpora (TaMaR-EmoDB) for Emotion Analysis",
    "volume": "main",
    "abstract": "This paper presents the design, the development of a new multilingual emotional speech corpus, TaMaR- EmoDB (Tamil Malayalam Ravula - Emotion DataBase) and its evaluation using a deep neural network (DNN)-baseline system. The corpus consists of utterances from three languages, namely, Malayalam, Tamil and Ravula, a tribal language. The database consists of short speech utterances in four emotions - anger, anxiety, happiness, and sadness, along with neutral utterances. The subset of the corpus is first evaluated using a perception test, in order to understand how well the emotional state in emotional speech is identified by humans. Later, machine testing is performed using the fusion of spectral and prosodic features with DNN framework. During the classification phase, the system reports an average precision of 0.78, 0.60, 0.61 and recall of 0.84, 0.61 and 0.53 for Malayalam, Tamil, and Ravula, respectively. This database can potentially be used as a new linguistic resource that will enable future research in speech emotion detection, corpus-based prosody analysis, and speech synthesis",
    "checked": true,
    "id": "259fc41870af8daebde547c9e287cfe251f5dded",
    "semantic_title": "design and development of a multi-lingual speech corpora (tamar-emodb) for emotion analysis",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sridhar19_interspeech.html": {
    "title": "Speech Emotion Recognition with a Reject Option",
    "volume": "main",
    "abstract": "Speech emotion recognition (SER) for categorical descriptors is a difficult task when the recordings come from everyday spontaneous interactions. The boundaries between emotional classes are less clear, resulting in complex, mixed emotions. Since the performance of a SER system varies across speech recordings, it is important to understand the reliability associated with its prediction. An intriguing formulation in machine learning related to this problem is the reject option, where a classifier only provides predictions over samples with reliability above a given threshold. This paper proposes a classification technique with a reject option using deep neural networks (DNNs) that increases its performance by selectively trading its coverage in the testing set. We use two different criteria to develop a SER system with a reject option, where it can accept or reject a sample as needed. Using the MSP-Podcast corpus, we evaluate this idea by comparing different classification performance as a function of coverage. By selectively defining a coverage of 75% of the samples, we obtain relative gains in F1-score of up to 25.71% for a five-class problem and 20.63% for an eight-class problem. The sentences that are rejected are analyzed in the evaluation, confirming that they have lower inter-evaluator agreement",
    "checked": true,
    "id": "a3c7918b74a67896f86024c7340f82e7c42c836c",
    "semantic_title": "speech emotion recognition with a reject option",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jin19_interspeech.html": {
    "title": "Development of Emotion Rankers Based on Intended and Perceived Emotion Labels",
    "volume": "main",
    "abstract": "In emotion datasets, intended emotion labels and perceived emotion labels both contain valuable information about how human express and perceive emotions, and there is a considerable mismatch between the two. In this paper, we propose a novel method to derive relative labels for preference learning using both the intended labels during emotion expression and the perceived labels given by all raters during perceptual evaluation. Based on analyzing the agreement between the intended and perceived labels, as well as the consistence among all perceptual ratings, we propose three pairwise ranking rules to generate multi-scale relevant scores for preference learning. We further build three sets of rankers for six basic emotions based on the three ranking rules. Through evaluation on the CREMA-D database, we demonstrate that, by considering both intended and perceived labels, our proposed rankers significantly outperform the rankers only relying on the perceptual ratings. We further combine the ranking scores of individual emotions for multi-class classification. Through experiments, we show that the emotion classification systems with ranking information significantly outperform the conventional SVM classifiers",
    "checked": true,
    "id": "049981a833dafeb842fcf325f43298ef96c28020",
    "semantic_title": "development of emotion rankers based on intended and perceived emotion labels",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gideon19_interspeech.html": {
    "title": "Emotion Recognition from Natural Phone Conversations in Individuals with and without Recent Suicidal Ideation",
    "volume": "main",
    "abstract": "Suicide is a serious public health concern in the U.S., taking the lives of over 47,000 people in 2017. Early detection of suicidal ideation is key to prevention. One promising approach to symptom monitoring is suicidal speech prediction, as speech can be passively collected and may indicate changes in risk. However, directly identifying suicidal speech is difficult, as characteristics of speech can vary rapidly compared with suicidal thoughts. Suicidal ideation is also associated with emotion dysregulation. Therefore, in this work, we focus on the detection of emotion from speech and its relation to suicide. We introduce the Ecological Measurement of Affect, Speech, and Suicide (EMASS) dataset, which contains phone call recordings of individuals recently discharged from the hospital following admission for suicidal ideation or behavior, along with controls. Participants self-report their emotion periodically throughout the study. However, the dataset is relatively small and has uncertain labels. Because of this, we find that most features traditionally used for emotion classification fail. We demonstrate how outside emotion datasets can be used to generate more relevant features, making this analysis possible. Finally, we use emotion predictions to differentiate healthy controls from those with suicidal ideation, providing evidence for suicidal speech detection using emotion",
    "checked": true,
    "id": "77654c1a22414bb85c656b5837598d5e4785699c",
    "semantic_title": "emotion recognition from natural phone conversations in individuals with and without recent suicidal ideation",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nazareth19_interspeech.html": {
    "title": "An Acoustic and Lexical Analysis of Emotional Valence in Spontaneous Speech: Autobiographical Memory Recall in Older Adults",
    "volume": "main",
    "abstract": "Analyzing emotional valence in spontaneous speech remains complex and challenging. We present an acoustic and lexical analysis of emotional valence in spontaneous speech of older adults. Data was collected by recalling autobiographical memories through a word association task. Due to the complex and personal nature of memories, we propose a novel coding scheme for emotional valence. We explore acoustic properties of speech as well as the use of affective words to predict emotional valence expressed in autobiographical memories. Using mixed-effect regression modelling, we compared predictive models based on acoustic information only, lexical information only, or a combination of both. Results show that the combined model accounts for the highest proportion of explained variance, with the acoustic features accounting for a smaller share of the total variance than the lexical features. Several acoustic and lexical features predicted valence. As a first attempt at analyzing spontaneous emotional speech in older adults autobiographical memories, the study provides more insight in which acoustic features can be used to predict valence (automatically) in a more ecologically valid setting",
    "checked": true,
    "id": "1517848a617e10950f72b574296e190226d987bc",
    "semantic_title": "an acoustic and lexical analysis of emotional valence in spontaneous speech: autobiographical memory recall in older adults",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhao19g_interspeech.html": {
    "title": "Does the Lombard Effect Improve Emotional Communication in Noise? — Analysis of Emotional Speech Acted in Noise",
    "volume": "main",
    "abstract": "Speakers usually adjust their way of talking in noisy environments involuntarily for effective communication. This adaptation is known as the Lombard effect. Although speech accompanying the Lombard effect can improve the intelligibility of a speaker's voice, the changes in acoustic features (e.g. fundamental frequency, speech intensity, and spectral tilt) caused by the Lombard effect may also affect the listener's judgment of emotional content. To the best of our knowledge, there is no published study on the influence of the Lombard effect in emotional speech. Therefore, we recorded parallel emotional speech waveforms uttered by 12 speakers under both quiet and noisy conditions in a professional recording studio in order to explore how the Lombard effect interacts with emotional speech. By analyzing confusion matrices and acoustic features, we aim to answer the following questions: 1) Can speakers express their emotions correctly even under adverse conditions? 2) Can listeners recognize the emotion contained in speech signals even under noise? 3) How does emotional speech uttered in noise differ from emotional speech uttered in quiet conditions in terms of acoustic characteristic?",
    "checked": true,
    "id": "3b87d7265dde1ebed4d9a2b0f30aca660e24aaab",
    "semantic_title": "does the lombard effect improve emotional communication in noise? - analysis of emotional speech acted in noise -",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gharsellaoui19_interspeech.html": {
    "title": "Linear Discriminant Differential Evolution for Feature Selection in Emotional Speech Recognition",
    "volume": "main",
    "abstract": "In this paper, an evolutionary algorithm is used to select an optimal set of acoustic features for emotional speech recognition. A new algorithm that combines differential evolution (DE) optimization and linear discriminant analysis (LDA) is proposed to design an effective feature selection and classification model. An original acoustic feature framework based on auditory modeling is also presented. The auditory-based features are provided as inputs to the DE-LDA based emotional speech recognition system. To evaluate the effectiveness of the DE-LDA approach, a subset of the Emotion Prosody Speech and Transcript corpus covering five emotional states (happiness, anger, panic, sadness, and interest) is used throughout the experiments. The results show that the proposed DE-LDA model performs significantly better than the baseline systems. It achieves a classification rate of 91.6% using only 50 input parameters that are optimally selected from 128 original acoustic features",
    "checked": true,
    "id": "e3cc0b8019e5f83684334f40b030ee3eb15c4ac3",
    "semantic_title": "linear discriminant differential evolution for feature selection in emotional speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sahu19_interspeech.html": {
    "title": "Multi-Modal Learning for Speech Emotion Recognition: An Analysis and Comparison of ASR Outputs with Ground Truth Transcription",
    "volume": "main",
    "abstract": "In this paper we plan to leverage multi-modal learning and automated speech recognition (ASR) systems toward building a speech-only emotion recognition model. Previous studies have shown that emotion recognition models using only acoustic features do not perform satisfactorily in detecting valence level. Text analysis has been shown to be helpful for sentiment classification. We compared classification accuracies obtained from an audio-only model, a text-only model and a multi-modal system leveraging both by performing a cross-validation analysis on IEMOCAP dataset. Confusion matrices show it's the valence level detection that is being improved by incorporating textual information. In the second stage of experiments, we used two ASR application programming interfaces (APIs) to get the transcriptions. We compare the performances of multi-modal systems using the ASR transcriptions with each other and with that of one using ground truth transcription. We analyze the confusion matrices to determine the effect of using ASR transcriptions instead of ground truth ones on class-wise accuracies. We investigate the generalisability of such a model by performing a cross-corpus study",
    "checked": true,
    "id": "c4310f6c4c1edd76f1898c560e1a1d2b82f50069",
    "semantic_title": "multi-modal learning for speech emotion recognition: an analysis and comparison of asr outputs with ground truth transcription",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2019/spinu19_interspeech.html": {
    "title": "Articulatory Characteristics of Secondary Palatalization in Romanian Fricatives",
    "volume": "main",
    "abstract": "This study explores the articulatory characteristics of plain and palatalized fricatives in Romanian. Based on earlier acoustic findings, we hypothesize that there are differences in tongue raising and fronting depending on the primary place of articulation, with more subtle gestures produced in the vicinity of the palatal area. We also predict more individual variation in the realization of secondary palatalization in postalveolars, based on general cross-linguistic patterns Ten native speakers participated in an ultrasound experiment. The stimuli included real words containing labial, dental, and postalveolar fricatives. The fricatives at all three places were either plain or palatalized word-finally (the only position available for secondary palatalization in this language). Tongue contours at the consonant midpoint were compared using Smoothing Spline ANOVAs individually with radius distance from the ultrasound probe The findings indicate differences in tongue shape between plain and palatalized consonants, with stronger palatalization effects in labials compared to coronals, as well as in dentals compared to postalveolars. The latter also revealed higher individual variation. Our findings thus suggest that tongue configurations for secondary palatalization in Romanian differ by place of articulation. The contrast is also overall less robust in postalveolars, confirming previous reports and explaining its rarity cross-linguistically",
    "checked": true,
    "id": "7eede068394171141aca082d288c86c32c75b8bb",
    "semantic_title": "articulatory characteristics of secondary palatalization in romanian fricatives",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ratko19_interspeech.html": {
    "title": "Articulation of Vowel Length Contrasts in Australian English",
    "volume": "main",
    "abstract": "The articulatory realisation of phonemic vowel length contrasts is still imperfectly understood. Australian English (AusE) /ɐː/ and /ɐ/ differ primarily in duration and therefore provide an ideal case for examining the articulatory properties of long vs. short vowels. Patterns of compression, acceleration ratios and VC coordination were examined using electromagnetic articulography (EMA) in /pVːp/ and /pVp/ syllables produced by three speakers of AusE at two speech rates. Short vowels were less compressible and had higher acceleration ratios than long vowels. VC rimes had proportionately earlier coda onsets than VːC rimes. These findings suggest that long and short vowels are characterised by different patterns of both intra- and intergestural organisation in AusE",
    "checked": true,
    "id": "b7eda0d2b8bd4daf62c445da11e70f91fb2a9b40",
    "semantic_title": "articulation of vowel length contrasts in australian english",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/deme19_interspeech.html": {
    "title": "V-to-V Coarticulation Induced Acoustic and Articulatory Variability of Vowels: The Effect of Pitch-Accent",
    "volume": "main",
    "abstract": "In the present study we analyzed vowel variation induced by carryover V-to-V coarticulation under the effect of pitch-accent as a function of vowel quality (using a minimally constrained intervening consonant to maximize V-to-V effects). We tested if /i/ is more resistant to coarticulation than /u/, and if both vowels show increased coarticulatory resistance in pitch-accented syllables. Our approach was unprecedented in the sense that it involved the analysis of parallel acoustic (F ) and articulatory (x-axis dorsum position) data in a great number of speakers (9 speaker), and real words of Hungarian. To analyze the degree of coarticulation, we adopted the locus equation approach, and fitted linear models on vowel onset and midpoint data, and calculated the differences between coarticulated and non-coarticulated vowels in both domains. To measure variability, we calculated standard deviations of midpoint F values and dorsum positions The results showed that accent clearly exerted an effect on the phonetic realization of vowels, but the effect we found was dependent on both the vowel quality, and the domain (articulation/acoustics) at hand. Observation of the patterns we found in parallel acoustic and articulatory data warrants for reconsideration of the term ‘coarticulatory resistance', and how it should be conceptualized",
    "checked": true,
    "id": "cdd9559f394072d209014989960d087ebe67d4e5",
    "semantic_title": "v-to-v coarticulation induced acoustic and articulatory variability of vowels: the effect of pitch-accent",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/king19_interspeech.html": {
    "title": "The Contribution of Lip Protrusion to Anglo-English /r/: Evidence from Hyper- and Non-Hyperarticulated Speech",
    "volume": "main",
    "abstract": "Articulatory variation of /r/ has been widely observed in rhotic varieties of English, particularly with regards to tongue body shapes, which range from retroflex to bunched. However, little is known about the production of /r/ in modern non-rhotic varieties, particularly in Anglo-English. Although it is generally agreed that /r/ may be accompanied by lip protrusion, it is unclear whether there is a relationship between tongue shape and the accompanying degree of protrusion. We present acoustic and articulatory data (via ultrasound tongue imaging and lip videos) from Anglo-English /r/ produced in both hyper- and non-hyperarticulated speech. Hyperarticulation was elicited by engaging speakers in error resolution with a simulated \"silent speech\" recognition programme. Our analysis indicates that hyperarticulated /r/ induces more lip protrusion than non-hyperarticulated /r/. However, bunched /r/ variants present more protrusion than retroflex variants, regardless of hyperarticulation. Despite some methodological limitations, the use of Deep Neural Networks seems to confirm these results. An articulatory trading relation between tongue shape and accompanying lip protrusion is proposed",
    "checked": true,
    "id": "20aaac9ebaeaba3ccdb18a0609dd490f4d067afe",
    "semantic_title": "the contribution of lip protrusion to anglo-english /r/: evidence from hyper- and non-hyperarticulated speech",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/marko19_interspeech.html": {
    "title": "Articulatory Analysis of Transparent Vowel /iː/ in Harmonic and Antiharmonic Hungarian Stems: Is There a Difference?",
    "volume": "main",
    "abstract": "The aim of our study is to analyse the articulatory characteristics of /iː/ occurring in Hungarian monosyllabic harmonic and antiharmonic stems. In their frequently cited work, based on 3 speakers' data, Beňuš and Gafos (2007) [1] claimed that the tongue position in transparent vowels of antiharmonic Hungarian stems is less advanced than that of the phonemically identical vowels in harmonic stems. In their study, the authors compared different harmonic and antiharmonic stems (even if the consonantal context was more or less controlled) In the present study, we analysed two homophonous pairs of words /siːv/ and /ɲiːr/, which are antiharmonic in their verbal usage, but are harmonic as nouns. The words were produced by 4 speakers both (i) in isolation and (ii) in sentence-initial position, where they were followed by front and back vowels, in a well-controlled manner. The experiment was carried out using electromagnetic articulography. We compared the sequence of the horizontal position of four receiver coils (ttip, tbl, tbo1, tbo2) across the conditions with Generalized Additive Models. The results showed that the horizontal positions of the receivers did not vary as a function of the harmonicity of the stem in either the isolated or the coarticulated condition",
    "checked": true,
    "id": "bb5de01190d7e473ab4bb1e837e473539272e98c",
    "semantic_title": "articulatory analysis of transparent vowel /iː/ in harmonic and antiharmonic hungarian stems: is there a difference?",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cunha19_interspeech.html": {
    "title": "On the Role of Oral Configurations in European Portuguese Nasal Vowels",
    "volume": "main",
    "abstract": "The characterisation of nasal vowels is not only a question of studying velar aperture. Recent work shows that oropharyngeal articulatory adjustments enhance the acoustics of nasal coupling or, at least, magnify differences between oral/nasal vowel congeners. Despite preliminary studies on the oral configurations of nasal vowels, for European Portuguese, a quantitative analysis is missing, particularly one to be applied systematically to a desirably large number of speakers. The main objective of this study is to adapt and extend previous methodological advances for the analysis of MRI data to further investigate: how velar changes affect oral configurations; the changes to the articulators and constrictions when compared with oral counterparts; and the closest oral counterpart. High framerate RT-MRI images (50fps) are automatically processed to extract the vocal tract contours and the position/configuration for the different articulators. These data are processed by evolving a quantitative articulatory analysis framework, previously proposed by the authors, extended to include information regarding constrictions (degree and place) and nasal port. For this study, while the analysis of data for more speakers is ongoing, we considered a set of two EP native speakers and addressed the study of oral and nasal vowels mainly in the context of stop consonants",
    "checked": true,
    "id": "10ee4f1e5e7d3348c3cafc768013bc3f2019e0d7",
    "semantic_title": "on the role of oral configurations in european portuguese nasal vowels",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xiong19_interspeech.html": {
    "title": "Residual + Capsule Networks (ResCap) for Simultaneous Single-Channel Overlapped Keyword Recognition",
    "volume": "main",
    "abstract": "Overlapped speech poses a significant problem in a variety of applications in speech processing including speaker identification, speaker diarization, and speech recognition among others. To address it, existing systems combine source separation with algorithms for processing non-overlapped speech (e.g. source separation + follow-on speech recognition). In this paper we propose a modified network architecture to simultaneously recognize keywords from overlapped speech without explicitly having to perform source separation. We build our network by adding capsule layers to a ResNet architecture that has shown state-of-the-art performance on a traditional keyword recognition task. We evaluate the model on a series of 10-word overlapped keyword recognition experiments, using speaker dependent and speaker independent training. Results indicate that Residual + Capsule (ResCap) network shows marked improvement in recognizing overlapped speech, especially in experiments where there is a mismatch in the number of overlapped speakers between the training set and the test set",
    "checked": true,
    "id": "d3d667a34b392865c1cd617cc3fa861cc37d43fb",
    "semantic_title": "residual + capsule networks (rescap) for simultaneous single-channel overlapped keyword recognition",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19i_interspeech.html": {
    "title": "A Study for Improving Device-Directed Speech Detection Toward Frictionless Human-Machine Interaction",
    "volume": "main",
    "abstract": "In this paper, we extend our previous work on device-directed utterance detection, which aims to distinguish voice queries intended for a smart-home device from background speech. The task can be phrased as a binary utterance-level classification problem that we approach with a DNN-LSTM model using acoustic features and features from the automatic speech recognition (ASR) decoder as input. In this work, we study the performance of the model for different dialog types and for different categories of decoder features. To address different dialog types, we found that a model with a separate output branch for each dialog type outperforms a model with a shared output branch by a relative 12.5% of equal error rate (EER) reduction. We also found the average number of arcs in a confusion network to be one of the most informative ASR decoder features. In addition, we explore different frequencies of backward propagation for training the acoustic embedding for every k frames (k=1,3,5,7), and mean and attention pooling methods for generating an utterance representation. We found that attention pooling provides the most discriminative utterance representation and outperforms mean pooling by a relative 4.97% of EER reduction",
    "checked": true,
    "id": "c82fcc6a73147862395fd7ddeaa8480cc60679d7",
    "semantic_title": "a study for improving device-directed speech detection toward frictionless human-machine interaction",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2019/su19_interspeech.html": {
    "title": "Unsupervised Methods for Audio Classification from Lecture Discussion Recordings",
    "volume": "main",
    "abstract": "Time allocated for lecturing and student discussions is an important indicator of classroom quality assessment. Automated classification of lecture and discussion recording segments can serve as an indicator of classroom activity in a flipped classroom setting. Segments of lecture are primarily the speech of the lecturer, while segments of discussion include student speech, silence and noise. Multiple audio recorders simultaneously document all class activities. Recordings are coarsely synchronized to a common start time. We note that the lecturer's speech tends to be common across recordings, but student discussions are captured only in the nearby device(s). Therefore, we window each recording at 0.5 s to 5 s duration and 0.1 s analysis rate. We compute the normalized similarity between a given window and temporally proximate window segments in other recordings. Histogram plot categorizes higher similarity windows as lecture and lower ones as discussion. To improve the classification performance, high energy lecture windows and windows with very high and very low similarity are used to train a supervised model, in order to regenerate the classification results of remaining windows. Experimental results show that binary classification accuracy improves from 96.84% to 97.37%",
    "checked": true,
    "id": "42bba7141a0d67ebd613c4e9f2e28c7d196bb07b",
    "semantic_title": "unsupervised methods for audio classification from lecture discussion recordings",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ashihara19_interspeech.html": {
    "title": "Neural Whispered Speech Detection with Imbalanced Learning",
    "volume": "main",
    "abstract": "In this paper, we present a neural whispered-speech detection technique that offers utterance-level classification of whispered and non-whispered speech exhibiting imbalanced data distributions. Previous studies have shown that machine learning models trained on a large amount of whispered and non-whispered utterances perform remarkably well for whispered speech detection. However, it is often difficult to collect large numbers of whispered utterances. In this paper, we propose a method to train neural whispered speech detectors from a small amount of whispered utterances in combination with a large amount of non-whispered utterances. In doing so, special care is taken to ensure that severely imbalanced datasets can effectively train neural networks. Specifically, we use a class-aware sampling method for training neural networks. To evaluate the networks, we gather test samples recorded by both condenser and smartphone microphones at different distances from the speakers to simulate practical environments. Experiments show the importance of imbalanced learning in enhancing the performance of utterance level classifiers",
    "checked": true,
    "id": "3e23c3b4ea307158c5f76f2f132087a705f5e8c3",
    "semantic_title": "neural whispered speech detection with imbalanced learning",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bergler19_interspeech.html": {
    "title": "Deep Learning for Orca Call Type Identification — A Fully Unsupervised Approach",
    "volume": "main",
    "abstract": "Call type classification is an important instrument in bioacoustic research investigating group-specific vocal repertoire, behavioral patterns, and cultures of different animal groups. There is a growing need using robust machine-based techniques to replace human classification due to its advantages in handling large datasets, delivering consistent results, removing perceptual-based classification, and minimizing human errors. The current work is the first adopting a two-stage fully unsupervised approach on previous machine-segmented orca data to identify orca sound types using deep learning together with one of the largest bioacoustic datasets — the Orchive. The proposed methods include: (1) unsupervised feature learning using an undercomplete ResNet18-autoencoder trained on machine-annotated data, and (2) spectral clustering utilizing compressed orca feature representations. An existing human-labeled orca dataset was clustered, including 514 signals distributed over 12 classes. This two-stage fully unsupervised approach is an initial study to (1) examine machine-generated clusters against human-identified orca call type classes, (2) compare supervised call type classification versus unsupervised call type clustering, and (3) verify the general feasibility of a completely unsupervised approach based on machine-labeled orca data resulting in a major progress within the research field of animal linguistics, by deriving a much deeper understanding and facilitating totally new insights and opportunities",
    "checked": true,
    "id": "23c2c9a350daa073ae989ae39749f18ada951d80",
    "semantic_title": "deep learning for orca call type identification - a fully unsupervised approach",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sacchi19_interspeech.html": {
    "title": "Open-Vocabulary Keyword Spotting with Audio and Text Embeddings",
    "volume": "main",
    "abstract": "Keyword Spotting (KWS) systems allow detecting a set of spoken (pre-defined) keywords. Open-vocabulary KWS systems search for the keywords in the set of word hypotheses generated by an automatic speech recognition (ASR) system which is computationally expensive and, therefore, often implemented as a cloud-based service. Besides, KWS systems could use also word classification algorithms that do not allow easily changing the set of words to be recognized, as the classes have to be defined a priori, even before training the system. In this paper, we propose the implementation of an open-vocabulary ASR-free KWS system based on speech and text encoders that allow matching the computed embeddings in order to spot whether a keyword has been uttered. This approach would allow choosing the set of keywords a posteriori while requiring low computational power. The experiments, performed on two different datasets, show that our method is competitive with other state of the art KWS systems while allowing for a flexibility of configuration and being computationally efficient",
    "checked": true,
    "id": "d29716239e5c41cf49a045898a806c0bf845d590",
    "semantic_title": "open-vocabulary keyword spotting with audio and text embeddings",
    "citation_count": 25
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gao19c_interspeech.html": {
    "title": "ToneNet: A CNN Model of Tone Classification of Mandarin Chinese",
    "volume": "main",
    "abstract": "In Mandarin Chinese, correct pronunciation is the key to convey word meaning correctly and the correct pronunciation is closely related to the tone of text. Therefore, tone classification is a critical part of speech evaluation system. Traditional tone classification is based on F0 and energy or MFCCs. But the extraction of these features is often subject to noise and other uncontrollable environmental factors. Thus, in order to reduce the influence of environment, we designed a CNN network named ToneNet which adopts mel-spectrogram as a feature and uses a customed convolutional neural network and multi-layer perceptron to classify Chinese syllables into one of the four tones. We trained and tested ToneNet on the Syllable Corpus of Standard Chinese Dataset (SCSC). The result shows that the best accuracy and f1-score of our method have reached 99.16% and 99.11% respectively. Besides, ToneNet has achieved 97.07% of accuracy and 96.83% of f1-score with the condition of gaussian noise",
    "checked": true,
    "id": "5114994e94b3a95bf4a5d8633c838ee62be52051",
    "semantic_title": "tonenet: a cnn model of tone classification of mandarin chinese",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/choi19_interspeech.html": {
    "title": "Temporal Convolution for Real-Time Keyword Spotting on Mobile Devices",
    "volume": "main",
    "abstract": "Keyword spotting (KWS) plays a critical role in enabling speech-based user interactions on smart devices. Recent developments in the field of deep learning have led to wide adoption of convolutional neural networks (CNNs) in KWS systems due to their exceptional accuracy and robustness. The main challenge faced by KWS systems is the trade-off between high accuracy and low latency. Unfortunately, there has been little quantitative analysis of the actual latency of KWS models on mobile devices. This is especially concerning since conventional convolution-based KWS approaches are known to require a large number of operations to attain an adequate level of performance In this paper, we propose a temporal convolution for real-time KWS on mobile devices. Unlike most of the 2D convolution-based KWS approaches that require a deep architecture to fully capture both low- and high-frequency domains, we exploit temporal convolutions with a compact ResNet architecture. In Google Speech Command Dataset, we achieve more than 385× speedup on Google Pixel 1 and surpass the accuracy compared to the state-of-the-art model. In addition, we release the implementation of the proposed and the baseline models including an end-to-end pipeline for training models and evaluating them on mobile devices",
    "checked": true,
    "id": "b3979724aff6ed34e07c4cc22c02fa4dfef1f781",
    "semantic_title": "temporal convolution for real-time keyword spotting on mobile devices",
    "citation_count": 115
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19j_interspeech.html": {
    "title": "Audio Tagging with Compact Feedforward Sequential Memory Network and Audio-to-Audio Ratio Based Data Augmentation",
    "volume": "main",
    "abstract": "Audio tagging aims to identify the presence or absence of audio events in the audio clip. Recently, a lot of researchers have paid attention to explore different model structures to improve the performance of audio tagging. Convolutional neural network (CNN) is the most popular choice among a wide variety of model structures, and it's successfully applied to audio events prediction task. However, the model complexity of CNN is relatively high, which is not efficient enough to ship in real product. In this paper, compact Feedforward Sequential Memory Network (cFSMN) is proposed for audio tagging task. Experimental results show that cFSMN-based system yields a comparable performance with the CNN-based system. Meanwhile, an audio-to-audio ratio (AAR) based data augmentation method is proposed to further improve the classifier performance. Finally, with raw waveforms of the balanced training set of Audio Set which is a published standard database, our system can achieve a state-of-the-art performance with AUC being 0.932. Moreover, cFSMN-based model has only 1.9 million parameters, which is only about 1/30 of the CNN-based model",
    "checked": true,
    "id": "121ba511cd3fda28ca9131ee4c3eb086a1698f68",
    "semantic_title": "audio tagging with compact feedforward sequential memory network and audio-to-audio ratio based data augmentation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yang19f_interspeech.html": {
    "title": "Music Genre Classification Using Duplicated Convolutional Layers in Neural Networks",
    "volume": "main",
    "abstract": "Music genres are conventional categories that identify some pieces of music as belonging to a shared tradition or set of conventions. In this paper, we proposed an approach to improve music genre classification with convolutional neural networks (CNN). Using mel-scale spectrogram as the input, we used duplicate convolutional layers whose output will be applied to different pooling layers to provide more statistical information for classification. Also, we made some modifications on residual learning by taking more outputs from convolutional layers. By comparing two different network topologies, our experimental results on the GTZAN dataset show that the proposed method can effectively improve the classification accuracy",
    "checked": true,
    "id": "813f8eb58015b54c20d34f542ca8bb30ee570931",
    "semantic_title": "music genre classification using duplicated convolutional layers in neural networks",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/carmi19_interspeech.html": {
    "title": "A Storyteller's Tale: Literature Audiobooks Genre Classification Using CNN and RNN Architectures",
    "volume": "main",
    "abstract": "Identifying acoustic properties that characterize reading literary genres can assist in giving a more personal and human tone to the speech of bots and automatic readings In this paper we consider the following question: given speech segments of audiobooks, how well can we classify them according to their literary genres? In this study we consider three different literary genres: children, horror and suspense, and humorous audio books, taken from two free audio books sites: Librivox and YouTube We ran four classification experiments: three for each pair of genres, and one for all three genres together. We repeated each experiment twice, with two different network architectures: Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) Note that, throughout the reading, there are sections that are more typical to the book's genre than others. As the samples were taken sequentially throughout the reading of the books and were short in duration, we did not expect high classification rates. Nevertheless, the accuracy of all the experiments were at least 72% for all the pair's classifications; and at least 57% for both architectures for the three classes classifications",
    "checked": true,
    "id": "d13b119f2b3be68b228c605d8fc5b2c21bbbadca",
    "semantic_title": "a storyteller's tale: literature audiobooks genre classification using cnn and rnn architectures",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hwang19_interspeech.html": {
    "title": "Parameter Enhancement for MELP Speech Codec in Noisy Communication Environment",
    "volume": "main",
    "abstract": "In this paper, we propose a deep learning (DL)-based parameter enhancement method for a mixed excitation linear prediction (MELP) speech codec in noisy communication environment. Unlike conventional speech enhancement modules that are designed to obtain clean speech signal by removing noise components before speech codec processing, the proposed method directly enhances codec parameters on either the encoder or decoder side. As the proposed method has been implemented by a small network without any additional processes required in conventional enhancement systems, e.g., time-frequency (T-F) analysis/synthesis modules, its computational complexity is very low. By enhancing the noise-corrupted codec parameters with the proposed DL framework, we achieved an enhancement system that is much simpler and faster than conventional T-F mask-based speech enhancement methods, while the quality of its performance remains similar",
    "checked": true,
    "id": "c3d4d3eb0c2a915c06247b46cf01344c86f5eb61",
    "semantic_title": "parameter enhancement for melp speech codec in noisy communication environment",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhen19_interspeech.html": {
    "title": "Cascaded Cross-Module Residual Learning Towards Lightweight End-to-End Speech Coding",
    "volume": "main",
    "abstract": "Speech codecs learn compact representations of speech signals to facilitate data transmission. Many recent deep neural network (DNN) based end-to-end speech codecs achieve low bitrates and high perceptual quality at the cost of model complexity. We propose a cross-module residual learning (CMRL) pipeline as a module carrier with each module reconstructing the residual from its preceding modules. CMRL differs from other DNN-based speech codecs, in that rather than modeling speech compression problem in a single large neural network, it optimizes a series of less-complicated modules in a two-phase training scheme. The proposed method shows better objective performance than AMR-WB and the state-of-the-art DNN-based speech codec with a similar network architecture. As an end-to-end model, it takes raw PCM signals as an input, but is also compatible with linear predictive coding (LPC), showing better subjective quality at high bitrates than AMR-WB and OPUS. The gain is achieved by using only 0.9 million trainable parameters, a significantly less complex architecture than the other DNN-based codecs in the literature",
    "checked": true,
    "id": "efb04ef7c61b589ade1ebbfd69935a40e069910d",
    "semantic_title": "cascaded cross-module residual learning towards lightweight end-to-end speech coding",
    "citation_count": 30
  },
  "https://www.isca-speech.org/archive/interspeech_2019/backstrom19_interspeech.html": {
    "title": "End-to-End Optimization of Source Models for Speech and Audio Coding Using a Machine Learning Framework",
    "volume": "main",
    "abstract": "Speech coding is the most commonly used application of speech processing. Accumulated layers of improvements have however made codecs so complex that optimization of individual modules becomes increasingly difficult. This work introduces machine learning methodology to speech and audio coding, such that we can optimize quality in terms of overall entropy. We can then use conventional quantization, coding and perceptual models without modification such that the codec adheres to conventional requirements on algorithmic complexity, latency and robustness to packet loss. Experiments demonstrate that end-to-end optimization of quantization accuracy of the spectral envelope can be used for a lossless reduction in bitrate of 0.4 kbits/s",
    "checked": true,
    "id": "9acd9ec61459e65e5ffbe9aea0216208abd71dac",
    "semantic_title": "end-to-end optimization of source models for speech and audio coding using a machine learning framework",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/valin19_interspeech.html": {
    "title": "A Real-Time Wideband Neural Vocoder at 1.6kb/s Using LPCNet",
    "volume": "main",
    "abstract": "Neural speech synthesis algorithms are a promising new approach for coding speech at very low bitrate. They have so far demonstrated quality that far exceeds traditional vocoders, at the cost of very high complexity. In this work, we present a low-bitrate neural vocoder based on the LPCNet model. The use of linear prediction and sparse recurrent networks makes it possible to achieve real-time operation on general-purpose hardware. We demonstrate that LPCNet operating at 1.6 kb/s achieves significantly higher quality than MELP and that uncompressed LPCNet can exceed the quality of a waveform codec operating at low bitrate. This opens the way for new codec designs based on neural synthesis models",
    "checked": true,
    "id": "75b1934626e05bc295e57517a915e40b9908f1ee",
    "semantic_title": "a real-time wideband neural vocoder at 1.6 kb/s using lpcnet",
    "citation_count": 54
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fuchs19_interspeech.html": {
    "title": "Super-Wideband Spectral Envelope Modeling for Speech Coding",
    "volume": "main",
    "abstract": "Significant improvements in the quality of speech coders have been achieved by widening the coded frequency range from narrowband to wideband. However, existing speech coders still employ a limited band source-filter model extended by parametric coding of the higher band. In the present work, a superwideband source-filter model running at 32 kHz is considered and especially its spectral magnitude envelope modeling. To match super-wideband operating mode, we adapted and compared two methods; Linear Predictive Coding (LPC) and Distribution Quantization (DQ). LPC uses autoregressive modeling, while DQ quantifies the energy ratios between different parts of the spectrum. Parameters of both methods were quantized with a multi-stage vector quantization. Objective and subjective evaluations indicate that both methods used in a super-wideband source-filter coding scheme offer the same quality range, making them an attractive alternative to conventional speech coders that require additional bandwidth extension",
    "checked": true,
    "id": "6c81cb63431d8b53f772c5a3cb42d39b15cdc627",
    "semantic_title": "super-wideband spectral envelope modeling for speech coding",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19q_interspeech.html": {
    "title": "Speech Audio Super-Resolution for Speech Recognition",
    "volume": "main",
    "abstract": "Automatic bandwidth extension (restoring high-frequency information from low sample rate audio) has a number of applications in speech processing. We introduce an end-to-end deep learning based system for speech bandwidth extension for use in a downstream automatic speech recognition (ASR) system. Specifically we propose a conditional generative adversarial network enriched with ASR-specific loss functions designed to upsample the speech audio while maintaining good ASR performance. Evaluations on the speech commands dataset and the LibriSpeech corpus show that our approach outperforms a number of traditional bandwidth extension methods with respect to word error rate",
    "checked": true,
    "id": "4be02149d31235b3077c12afdc0df9fbfa300a09",
    "semantic_title": "speech audio super-resolution for speech recognition",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gupta19e_interspeech.html": {
    "title": "Artificial Bandwidth Extension Using H∞ Optimization",
    "volume": "main",
    "abstract": "This work proposes a new method for artificial bandwidth extension (ABE) that aims to extend the bandwidth of speech signals in narrowband voice communications. We extract a signal model which consists of the wideband information. Using the signal model, we obtain an infinite impulse response (IIR) interpolation filter with the help of H∞ optimization. Interpolation filters are going to be distinct for the speech signals because of their non-stationary (time-variant) nature. In narrowband communications, only narrowband signal is accessible. Hence, a codebook approach is intended to keep the IIR interpolation filters information (wideband feature) together with their corresponding narrowband signal characteristic (narrowband attribute). For that, the Gaussian mixture modeling (GMM) codebook approach is utilized to estimate the wideband feature for a given narrowband attribute of the signal. Performances are assessed for the two sorts of narrowband attributes",
    "checked": true,
    "id": "82eeb82cf0990e4147e200f42df353abf06de1ea",
    "semantic_title": "artificial bandwidth extension using h∞ optimization",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mittag19_interspeech.html": {
    "title": "Quality Degradation Diagnosis for Voice Networks — Estimating the Perceived Noisiness, Coloration, and Discontinuity of Transmitted Speech",
    "volume": "main",
    "abstract": "We present a single-ended quality diagnosis model for super-wideband speech communication networks, which predicts the perceived Noisiness, Coloration, and Discontinuity of transmitted speech. The model is an extension to the single-ended speech quality prediction model NISQA and can additionally indicate the cause of quality degradation. Service providers can use the model independently of the communication system's technology since it is based on universal perceptual quality dimensions. The prediction model consists of a convolutional neural network that firstly calculates per-frame features of a speech signal and subsequently aggregates the features over time with a recurrent neural network, to estimate the speech quality dimensions. The proposed diagnosis model achieves promising results with an average RMSE* of 0.24",
    "checked": true,
    "id": "f6fed495714868535a7f2168291e0169d650fe34",
    "semantic_title": "quality degradation diagnosis for voice networks - estimating the perceived noisiness, coloration, and discontinuity of transmitted speech",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chai19b_interspeech.html": {
    "title": "A Cross-Entropy-Guided (CEG) Measure for Speech Enhancement Front-End Assessing Performances of Back-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "One challenging problem of robust automatic speech recognition (ASR) is how to measure the goodness of a speech enhancement algorithm without calculating word error rate (WER) due to the high costs of manual transcriptions, language modeling and decoding process. In this study, a novel cross-entropy-guided (CEG) measure is proposed for assessing if enhanced speech predicted by a speech enhancement algorithm would produce a good performance for robust ASR. CEG consists of three consecutive steps, namely the low-level representations via the feature extraction, high-level representations via the nonlinear mapping with the acoustic model, and the final CEG calculation between the high-level representations of clean and enhanced speech. Specifically, state posterior probabilities from the output of the neural network for the acoustic model are adopted as the high-level representations and a cross-entropy criterion is used to calculate CEG. Experimental results show that CEG could consistently yield the highest correlations with WER and achieve the most accurate assessment of the ASR performance when compared to distortion measures based on human auditory perception and an acoustic confidence measure. Potentially, CEG could be adopted to guide the parameter optimization of deep learning based speech enhancement algorithms to further improve the ASR performance",
    "checked": true,
    "id": "1228f4ca45d8282aa442e8a87b391615e7dc6ba9",
    "semantic_title": "a cross-entropy-guided (ceg) measure for speech enhancement front-end assessing performances of back-end automatic speech recognition",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/moller19_interspeech.html": {
    "title": "Extending the E-Model Towards Super-Wideband and Fullband Speech Communication Scenarios",
    "volume": "main",
    "abstract": "In order to plan speech communication services regarding the quality experienced by their users, parametric models have been used since a long time. These models predict the overall quality experienced by a communication partner on the basis of parameters describing the elements of the transmission channel and the terminal equipment. The mostly used model is the E-model which is standardized in ITU-T Rec. G.107 for narrowband and in ITU-T Rec. G.107.1 for wideband scenarios. However, with the advent of super-wideband and fullband transmission, the E-model needs to be extended. In this paper, we propose a first version of an extended E-model which addresses both super-wideband and fullband scenarios, and which predicts the effects of speech codecs, packet loss, and delay as the most important degradations to be expected in such scenarios. Predictions are compared to the results of listening-only and conversational tests as well as to signal-based predictions, showing a reasonable prediction accuracy",
    "checked": true,
    "id": "cb6b46bb3e70c8a35393404705225c5bdb0038f9",
    "semantic_title": "extending the e-model towards super-wideband and fullband speech communication scenarios",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sadhu19_interspeech.html": {
    "title": "Modulation Vectors as Robust Feature Representation for ASR in Domain Mismatched Conditions",
    "volume": "main",
    "abstract": "In this work, we demonstrate the robustness of Modulation Vectors, in domain mismatches between the training and test conditions in an Automatic Speech Recognition (ASR) system. Our work focuses on the specific task of dealing with mismatches caused by reverberation. We use simulated data from TIMIT and real reverberant speech from the REVERB challenge data to evaluate the performance of our system. The paper also describes a multistream system to combine information from Mel Frequency Cepstral Coefficient (MFCC) and M-vectors to improve the ASR performance in both matched and mismatched datasets. The proposed multistream system achieves a relative improvement of 25% in recognition accuracy on the mismatched condition, while a M-vector trained hybrid ASR system shows a 7–8% improvement in recognition accuracy, both w.r.t. a MFCC trained hybrid ASR system",
    "checked": true,
    "id": "325021c80755bf709ede748845bf9529957ee1ff",
    "semantic_title": "modulation vectors as robust feature representation for asr in domain mismatched conditions",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19r_interspeech.html": {
    "title": "Prosody Usage Optimization for Children Speech Recognition with Zero Resource Children Speech",
    "volume": "main",
    "abstract": "Children's speech recognition remains a big challenge for automatic speech recognition. Due to the more difficult process and higher cost on data collection, most current ASR systems are optimized only using lots of adult speech with limited or even none children's speech. Accordingly, the acoustic mismatch between children's and adult speech is the primary reason for the ASR performance degradation when facing children's speech. To overcome this problem, we proposed several approaches to improve children's speech recognition without using any children's speech data. A better utilization strategy on prosody-based features is developed. First, pitch and prosody modification is explored in both training and testing respectively, which can significantly reduce the mismatch between two types of speech. Furthermore, joint-decoding with both the prosody modified speech and the original speech is designed to get a more robust performance on both children's and adult speech. Experiments are evaluated on a Mandarin speech recognition task, with only 400-hour adult speech in the training. The results show that our proposed method can obtain a large gain on children's speech, with relative ~20% WER reduction compared to the baseline, and also no obvious degradation is observed on the adult speech for the proposed system",
    "checked": true,
    "id": "05874ef3e3de191c36dbec1d192d264c5ee63775",
    "semantic_title": "prosody usage optimization for children speech recognition with zero resource children speech",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/agrawal19_interspeech.html": {
    "title": "Unsupervised Raw Waveform Representation Learning for ASR",
    "volume": "main",
    "abstract": "In this paper, we propose a deep representation learning approach using the raw speech waveform in an unsupervised learning paradigm. The first layer of the proposed deep model performs acoustic filtering while the subsequent layer performs modulation filtering. The acoustic filterbank is implemented using cosine-modulated Gaussian filters whose parameters are learned. The modulation filtering is performed on log transformed outputs of the first layer and this is achieved using a skip connection based architecture. The outputs from this two layer filtering are fed to the variational autoencoder model. All the model parameters including the filtering layers are learned using the VAE cost function. We employ the learned representations (second layer outputs) in a speech recognition task. Experiments are conducted on Aurora-4 (additive noise with channel artifact) and CHiME-3 (additive noise with reverberation) databases. In these experiments, the learned representations from the proposed framework provide significant improvements in ASR results over the baseline filterbank features and other robust front-ends (average relative improvements of 16% and 6% in word error rate over baseline features on clean and multi-condition training, respectively on Aurora-4 dataset, and 21% over the baseline features on CHiME-3 database)",
    "checked": true,
    "id": "5467f75b2fb50d9c1277b519cd34b9f7fd68645c",
    "semantic_title": "unsupervised raw waveform representation learning for asr",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ramsay19_interspeech.html": {
    "title": "Low-Dimensional Bottleneck Features for On-Device Continuous Speech Recognition",
    "volume": "main",
    "abstract": "Low power digital signal processors (DSPs) typically have a very limited amount of memory in which to cache data. In this paper we develop efficient bottleneck feature (BNF) extractors that can be run on a DSP, and retrain a baseline large-vocabulary continuous speech recognition (LVCSR) system to use these BNFs with only a minimal loss of accuracy. The small BNFs allow the DSP chip to cache more audio features while the main application processor is suspended, thereby reducing the overall battery usage. Our presented system is able to reduce the footprint of standard, fixed point DSP spectral features by a factor of 10 without any loss in word error rate (WER) and by a factor of 64 with only a 5.8% relative increase in WER",
    "checked": true,
    "id": "ad90cd9093553b3a292cd85d0066dab1ca3eb751",
    "semantic_title": "low-dimensional bottleneck features for on-device continuous speech recognition",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/riviello19_interspeech.html": {
    "title": "Binary Speech Features for Keyword Spotting Tasks",
    "volume": "main",
    "abstract": "Keyword spotting is a classification task which aims to detect a specific set of spoken words. In general, this type of task runs on a power-constrained device such as a smartphone. One method to reduce the power consumption of a keyword spotting algorithm (typically a neural network) is to reduce the precision of the network weights and activations. In this paper, we propose a new representation of speech features which is more adapted to low-precision networks and compatible with binary/ternary neural networks. The new representation is based on the log-Mel spectrogram and models the variation of power over time. Tested on a ResNet, this representation produces results nearly as accurate as full-precision MFCCs, which are traditionally used in speech recognition applications",
    "checked": true,
    "id": "769669aee55ecd6d6ab10504bfb582a34c7cb001",
    "semantic_title": "binary speech features for keyword spotting tasks",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schneider19_interspeech.html": {
    "title": "wav2vec: Unsupervised Pre-Training for Speech Recognition",
    "volume": "main",
    "abstract": "We explore unsupervised pre-training for speech recognition by learning representations of raw audio. wav2vec is trained on large amounts of unlabeled audio data and the resulting representations are then used to improve acoustic model training. We pre-train a simple multi-layer convolutional neural network optimized via a noise contrastive binary classification task. Our experiments on WSJ reduce WER of a strong character-based log-mel filterbank baseline by up to 36%when only a few hours of transcribed data is available. Our approach achieves 2.43% WER on the nov92 test set. This outperforms Deep Speech 2, the best reported character-based system in the literature while using three orders of magnitude less labeled training data",
    "checked": true,
    "id": "96bd1cd9b37cc9eea6ecc1b46afc29f95a10d424",
    "semantic_title": "wav2vec: unsupervised pre-training for speech recognition",
    "citation_count": 858
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cho19b_interspeech.html": {
    "title": "Automatic Detection of Prosodic Focus in American English",
    "volume": "main",
    "abstract": "Focus, which is usually modulated by prosodic prominence, highlights a particular element within a sentence for emphasis or contrast. Despite its importance in communication, it has received little attention in the field of speech recognition. This paper developed an automatic detection system of prosodic focus in American English, using telephone-number strings. Our data were 100 10-digit phone number strings read by 5 speakers (3 females and 2 males). We extracted 18 prosodic features from each digit within the strings and one categorical variable and trained a Random Forest model to detect where the focused digit is within a given string. We also compared the model performance to human judgment rates from a perception experiment with 67 native speakers of American English. Our final model shows 92% of accuracy in detecting the location of prosodic focus, which is slightly lower than the human perception (97.2%) but much better than the chance level (10%). We discuss the predictive features in our model and potential features to add in the future study",
    "checked": true,
    "id": "0cae228143bccf8a2ac0124377f77f5c502dcb54",
    "semantic_title": "automatic detection of prosodic focus in american english",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/menon19_interspeech.html": {
    "title": "Feature Exploration for Almost Zero-Resource ASR-Free Keyword Spotting Using a Multilingual Bottleneck Extractor and Correspondence Autoencoders",
    "volume": "main",
    "abstract": "We compare features for dynamic time warping (DTW) when used to bootstrap keyword spotting (KWS) in an almost zero-resource setting. Such quickly-deployable systems aim to support United Nations (UN) humanitarian relief efforts in parts of Africa with severely under-resourced languages. Our objective is to identify acoustic features that provide acceptable KWS performance in such environments. As supervised resource, we restrict ourselves to a small, easily acquired and independently compiled set of isolated keywords. For feature extraction, a multilingual bottleneck feature (BNF) extractor, trained on well-resourced out-of-domain languages, is integrated with a correspondence autoencoder (CAE) trained on extremely sparse in-domain data. On their own, BNFs and CAE features are shown to achieve a more than 2% absolute performance improvement over baseline MFCCs. However, by using BNFs as input to the CAE, even better performance is achieved, with a more than 11% absolute improvement in ROC AUC over MFCCs and more than twice as many top-10 retrievals for two evaluated languages, English and Luganda. We conclude that integrating BNFs with the CAE allows both large out-of-domain and sparse in-domain resources to be exploited for improved ASR-free keyword spotting",
    "checked": true,
    "id": "b8b5abbd1545f76dc482457cd3d8660ca996d504",
    "semantic_title": "feature exploration for almost zero-resource asr-free keyword spotting using a multilingual bottleneck extractor and correspondence autoencoders",
    "citation_count": 27
  },
  "https://www.isca-speech.org/archive/interspeech_2019/loweimi19_interspeech.html": {
    "title": "On Learning Interpretable CNNs with Parametric Modulated Kernel-Based Filters",
    "volume": "main",
    "abstract": "We investigate the problem of direct waveform modelling using parametric kernel-based filters in a convolutional neural network (CNN) framework, building on SincNet, a CNN employing the cardinal sine (sinc) function to implement learnable bandpass filters. To this end, the general problem of learning a filterbank consisting of modulated kernel-based baseband filters is studied. Compared to standard CNNs, such models have fewer parameters, learn faster, and require less training data. They are also more amenable to human interpretation, paving the way to embedding some perceptual prior knowledge in the architecture. We have investigated the replacement of the rectangular filters of SincNet with triangular, gammatone and Gaussian filters, resulting in higher model flexibility and a reduction to the phone error rate. We also explore the properties of the learned filters learned for TIMIT phone recognition from both perceptual and statistical standpoints. We find that the filters in the first layer, which directly operate on the waveform, are in accord with the prior knowledge utilised in designing and engineering standard filters such as mel-scale triangular filters. That is, the networks learn to pay more attention to perceptually significant spectral neighbourhoods where the data centroid is located, and the variance and Shannon entropy are highest",
    "checked": true,
    "id": "55ba2ea67e754c361d839aa0b971ab7df0229ffe",
    "semantic_title": "on learning interpretable cnns with parametric modulated kernel-based filters",
    "citation_count": 28
  },
  "https://www.isca-speech.org/archive/interspeech_2019/verwimp19_interspeech.html": {
    "title": "Reverse Transfer Learning: Can Word Embeddings Trained for Different NLP Tasks Improve Neural Language Models?",
    "volume": "main",
    "abstract": "Natural language processing (NLP) tasks tend to suffer from a paucity of suitably annotated training data, hence the recent success of transfer learning across a wide variety of them. The typical recipe involves: (i) training a deep, possibly bidirectional, neural network with an objective related to language modeling, for which training data is plentiful; and (ii) using the trained network to derive contextual representations that are far richer than standard linear word embeddings such as word2vec, and thus result in important gains. In this work, we wonder whether the opposite perspective is also true: can contextual representations trained for different NLP tasks improve language modeling itself? Since language models (LMs) are predominantly locally optimized, other NLP tasks may help them make better predictions based on the entire semantic fabric of a document. We test the performance of several types of pre-trained embeddings in neural LMs, and we investigate whether it is possible to make the LM more aware of global semantic information through embeddings pre-trained with a domain classification model. Initial experiments suggest that as long as the proper objective criterion is used during training, pre-trained embeddings are likely to be beneficial for neural language modeling",
    "checked": true,
    "id": "3dbb476a4bccfa8d401f4d1b08e38ee972920447",
    "semantic_title": "reverse transfer learning: can word embeddings trained for different nlp tasks improve neural language models?",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19l_interspeech.html": {
    "title": "Joint Grapheme and Phoneme Embeddings for Contextual End-to-End ASR",
    "volume": "main",
    "abstract": "End-to-end approaches to automatic speech recognition, such as Listen-Attend-Spell (LAS), blend all components of a traditional speech recognizer into a unified model. Although this simplifies training and decoding pipelines, a unified model is hard to adapt when mismatch exists between training and test data, especially if this information is dynamically changing. The Contextual LAS (CLAS) framework tries to solve this problem by encoding contextual entities into fixed-dimensional embeddings and utilizing an attention mechanism to model the probabilities of seeing these entities. In this work, we improve the CLAS approach by proposing several new strategies to extract embeddings for the contextual entities. We compare these embedding extractors based on graphemic and phonetic input and/or output sequences and show that an encoder-decoder model trained jointly towards graphemes and phonemes outperforms other approaches. Leveraging phonetic information obtains better discrimination for similarly written graphemic sequences and also helps the model generalize better to graphemic sequences unseen in training. We show significant improvements over the original CLAS approach and also demonstrate that the proposed method scales much better to a large number of contextual entities across multiple domains",
    "checked": true,
    "id": "4af08d465168c9b5fffe8cf1de6ee649ca4a8ac9",
    "semantic_title": "joint grapheme and phoneme embeddings for contextual end-to-end asr",
    "citation_count": 35
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19h_interspeech.html": {
    "title": "Character-Aware Sub-Word Level Language Modeling for Uyghur and Turkish ASR",
    "volume": "main",
    "abstract": "Uyghur and Turkish are two typical agglutinative languages, which suffer heavily from the data sparsity problem. Due to this, we first apply a statistical morphological segmentation and change the number of morphs to get a better sub-word level automatic speech recognition (ASR) system. The best systems, which yield 2.03% and 1.65% absolute WER reductions from the word level systems for Uyghur and Turkish respectively, are used for further n-best rescoring. To further alleviate the data sparsity problem, we use both convolutional neural network (CNN) based and bi-directional long short-term memory (BLSTM) based character-aware language models on the two languages. In order to alleviate the information missing of the middle steps of the BLSTM based character aware language model, we propose to use the weighted average of each time-steps' outputs. The proposed weighting methods can be divided into three categories: decay based, position-based and attention-based. Results show that the decay based weighting method leads to the most significant WER reductions, which are 2.38% and 1.96%, compared with the sub-word level 1-pass ASR system for Uyghur and Turkish respectively",
    "checked": true,
    "id": "2bd8043d4abb8ce7652e5ca9263420400a2d6ea2",
    "semantic_title": "character-aware sub-word level language modeling for uyghur and turkish asr",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pusateri19_interspeech.html": {
    "title": "Connecting and Comparing Language Model Interpolation Techniques",
    "volume": "main",
    "abstract": "In this work, we uncover a theoretical connection between two language model interpolation techniques, count merging and Bayesian interpolation. We compare these techniques as well as linear interpolation in three scenarios with abundant training data per component model. Consistent with prior work, we show that both count merging and Bayesian interpolation outperform linear interpolation. We include the first (to our knowledge) published comparison of count merging and Bayesian interpolation, showing that the two techniques perform similarly. Finally, we argue that other considerations will make Bayesian interpolation the preferred approach in most circumstances",
    "checked": true,
    "id": "909d6ba86fe59d727d9182b8fc2cc6958f46c67e",
    "semantic_title": "connecting and comparing language model interpolation techniques",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/khassanov19b_interspeech.html": {
    "title": "Enriching Rare Word Representations in Neural Language Models by Embedding Matrix Augmentation",
    "volume": "main",
    "abstract": "The neural language models (NLM) achieve strong generalization capability by learning the dense representation of words and using them to estimate probability distribution function. However, learning the representation of rare words is a challenging problem causing the NLM to produce unreliable probability estimates. To address this problem, we propose a method to enrich representations of rare words in pre-trained NLM and consequently improve its probability estimation performance. The proposed method augments the word embedding matrices of pre-trained NLM while keeping other parameters unchanged. Specifically, our method updates the embedding vectors of rare words using embedding vectors of other semantically and syntactically similar words. To evaluate the proposed method, we enrich the rare street names in the pre-trained NLM and use it to rescore 100-best hypotheses output from the Singapore English speech recognition system. The enriched NLM reduces the word error rate by 6% relative and improves the recognition accuracy of the rare words by 16% absolute as compared to the baseline NLM",
    "checked": true,
    "id": "a7554b087d68abb2d9809c94788fb2da38fb5844",
    "semantic_title": "enriching rare word representations in neural language models by embedding matrix augmentation",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yu19b_interspeech.html": {
    "title": "Comparative Study of Parametric and Representation Uncertainty Modeling for Recurrent Neural Network Language Models",
    "volume": "main",
    "abstract": "Recurrent neural network language models (RNNLMs) have shown superior performance across a range of tasks, including speech recognition. The hidden layer of RNNLMs plays a vital role in learning the suitable representation of contexts for word prediction. However, the deterministic model parameters and fixed hidden vectors in conventional RNNLMs have limited power in modeling the uncertainty over hidden representations. In order to address this issue, in this paper, a comparative study of parametric and hidden representation uncertainty modeling approaches based on Bayesian gates and variational RNNLMs respectively is investigated on long short-term memory (LSTM) and gated recurrent units (GRU) LMs. Experimental results are presented on two tasks: PennTreebank (PTB) corpus, Switchboard conversational telephone speech (SWBD). Consistent performance improvements were obtained over conventional RNNLMs in terms of both perplexity and word error rate",
    "checked": true,
    "id": "6fbb13052ad08ecaa0d194e329321ceb1395681f",
    "semantic_title": "comparative study of parametric and representation uncertainty modeling for recurrent neural network language models",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/agenbag19_interspeech.html": {
    "title": "Improving Automatically Induced Lexicons for Highly Agglutinating Languages Using Data-Driven Morphological Segmentation",
    "volume": "main",
    "abstract": "We present a method of improving the performance of automatically induced lexicons for highly agglutinating languages. Our previous work demonstrated the feasibility of using automatic sub-word unit discovery and lexicon induction to enable ASR for under-resourced languages. However, a particularly challenging case for such approaches is found in agglutinating languages, which have large vocabularies of infrequently used words. In this study, we address the unfavorable vocabulary distribution of such languages by performing data-driven morphological segmentation of the orthography prior to lexicon induction. We apply this novel step to a corpus of recorded radio broadcasts in Luganda, which is a highly agglutinating and severely under-resourced language. The intervention leads to a 10% (relative) reduction in WER, which puts the resulting ASR performance on par with an expert lexicon. When context is added to the morphological segments prior to lexicon induction, a further 1% WER reduction is achieved. This demonstrates that it is feasible to perform ASR in an under-resourced setting using an automatically induced lexicon even in the case of a highly agglutinating language",
    "checked": true,
    "id": "bb364cc62d97dabfd2d717a16ca7c14ef6f21c4e",
    "semantic_title": "improving automatically induced lexicons for highly agglutinating languages using data-driven morphological segmentation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/coucheirolimeres19_interspeech.html": {
    "title": "Attention-Based Word Vector Prediction with LSTMs and its Application to the OOV Problem in ASR",
    "volume": "main",
    "abstract": "We propose three architectures for a word vector prediction system (WVPS) built with LSTMs that consider both past and future contexts of a word for predicting a vector in an embedded space where its surrounding area is semantically related to the considered word. We introduce an attention mechanism in one of the architectures so the system is able to assess the specific contribution of each context word to the prediction. All the architectures are trained under the same conditions and the same training material, following a curricular-learning fashion in the presentation of the data. For the inputs, we employ pre-trained word embeddings. We evaluate the systems after the same number of training steps, over two different corpora composed of ground-truth speech transcriptions in Spanish language from TCSTAR and TV recordings used in the Search on Speech Challenge of IberSPEECH 2018. The results show that we are able to reach significant differences between the architectures, consistently across both corpora. The attention-based architecture achieves the best results, suggesting its adequacy for the task. Also, we illustrate the usefulness of the systems for resolving out-of-vocabulary (OOV) regions marked by an ASR system capable of detecting OOV occurrences",
    "checked": true,
    "id": "eeeba14b16bde90f63e2ba53a5cc858d7874a4d6",
    "semantic_title": "attention-based word vector prediction with lstms and its application to the oov problem in asr",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gao19d_interspeech.html": {
    "title": "Code-Switching Sentence Generation by Bert and Generative Adversarial Networks",
    "volume": "main",
    "abstract": "Code-switching has become a common linguistic phenomenon. Comparing to monolingual ASR tasks, insufficient data is a major challenge for code-switching speech recognition. In this paper, we propose an approach to compositionally employ the Bidirectional Encoder Representations from Transformers (Bert) model and Generative Adversarial Net (GAN) model for code-switching text data generation. It improves upon previous work by (1) applying Bert as a masked language model to predict the mixed-in foreign words and (2) basing on the GAN framework with Bert for both the generator and discriminator to further assure the generated sentences similar enough to the natural examples. We evaluate the effectiveness of the generated data by its contribution to ASR. Experiments show our approach can reduce the English word error rate by 1.5% with the Mandarin-English code-switching spontaneous speech corpus OC16-CE80",
    "checked": true,
    "id": "4a5509705673d3c5323af7c9a432192e1f8a7326",
    "semantic_title": "code-switching sentence generation by bert and generative adversarial networks",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ritchie19_interspeech.html": {
    "title": "Unified Verbalization for Speech Recognition & Synthesis Across Languages",
    "volume": "main",
    "abstract": "We describe a new approach to converting written tokens to their spoken form, which can be shared by automatic speech recognition (ASR) and text-to-speech synthesis (TTS) systems. Both ASR and TTS need to map from the written to the spoken domain, and we present an approach that enables us to share verbalization grammars between the two systems while exploiting linguistic commonalities to provide simple default verbalizations. We also describe improvements to an induction system for number names grammars. Between these shared ASR/TTS verbalizers and the improved induction system for number names grammars, we achieve significant gains in development time and scalability across languages",
    "checked": true,
    "id": "e2402e2378c8488440c295de03871726f3c71fa6",
    "semantic_title": "unified verbalization for speech recognition & synthesis across languages",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sharma19e_interspeech.html": {
    "title": "Better Morphology Prediction for Better Speech Systems",
    "volume": "main",
    "abstract": "Prediction of morphological forms is a well-studied problem and can lead to better speech systems either directly by rescoring models for correcting morphology, or indirectly by more accurate dialog systems with improved natural language generation and understanding. This includes both lemmatization, i.e. deriving the lemma or root word from a given surface form as well as morphological inflection, i.e. deriving surface forms from the lemma. We train and evaluate various language-agnostic end-to-end neural sequence-to-sequence models for these tasks and compare their effectiveness. We further augment our models with pronunciation information which is typically available in speech systems to further improve the accuracies of the same tasks. We present the results across both morphologically modest and rich languages to show robustness of our approach",
    "checked": true,
    "id": "61cf8d5adf6497a6a00ccc504ec3e2613c34f3ff",
    "semantic_title": "better morphology prediction for better speech systems",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sennema19_interspeech.html": {
    "title": "Vietnamese Learners Tackling the German /ʃt/ in Perception",
    "volume": "main",
    "abstract": "Previous observations from didactic studies have indicated that Vietnamese learners of German as a foreign language often fail to realize consonantal clusters in German [1, 2, 3]. The present study investigated whether this problem occurs already at the level of perception, i.e., whether Vietnamese learners find it difficult to perceive the difference between a cluster and a single consonant. We focused on the discrimination between the German cluster /ʃt/ and the single consonants /t/ and /ʃ/, both in onset and coda position. Due to different phonotactic restrictions on coda consonants in Vietnamese, we expected the coda position to pose a bigger challenge for correct discrimination than the onset position. With an AX discrimination task, we tested how 83 university students from Hanoi perceived these contrasts. Our findings show that only the distinction between /ʃt/-/ʃ/ in coda position posed a real challenge to our listeners. We attribute this difficulty to the weak and non-native auditory cues for the plosive in this position. For all other contrasts our participants performed surprisingly well. We propose that this is due to the influence of English as first L2 that facilitates the acquisition of phonological contrasts in German as an L3",
    "checked": true,
    "id": "ad01fc5d104b34d16659b3f054df4ec9a92189b5",
    "semantic_title": "vietnamese learners tackling the german /ʃt/ in perception",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lewis19_interspeech.html": {
    "title": "An Articulatory-Acoustic Investigation into GOOSE-Fronting in German-English Bilinguals Residing in London, UK",
    "volume": "main",
    "abstract": "This study explores L2 acquisition of socially conditioned phonetic variation in 13 German-English sequential bilinguals residing in London, UK. The phonetic variable analysed is GOOSE-fronting, i.e. the more front pronunciation of /u/ in words like ‘goose', acoustically manifested through an increased F2 frequency. In the South of England, GOOSE-fronting is a sound change considered to be led by young females. We investigated whether bilinguals adhered to this pattern, e.g. whether younger female German-English bilinguals exhibited a relatively higher F2 frequency in words like ‘goose' than other bilinguals. The bilinguals' English /u/ productions were compared against their German /u/ (lower F2 as more back) and /y/ (higher F2 as more front) to determine the degree of GOOSE-fronting and whether their F2 values were closer to /y/ than /u/. Normalised formant values were considered in relation to lingual measurements obtained using ultrasound tongue imaging. The acoustic and articulatory results revealed that female bilinguals indeed produced more front English /u/ vowels than their male counterparts. Within female speakers, age and length of residence in the UK were found to be significant, with younger speakers who had lived in the UK longer than five years displaying the greatest degree of GOOSE-fronting",
    "checked": true,
    "id": "d0bf377aa82da04e9d2df927cd88d0bdafe284a0",
    "semantic_title": "an articulatory-acoustic investigation into goose-fronting in german-english bilinguals residing in london, uk",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jenne19_interspeech.html": {
    "title": "Multimodal Articulation-Based Pronunciation Error Detection with Spectrogram and Acoustic Features",
    "volume": "main",
    "abstract": "Articulation-based pronunciation error detection is concerned with the task of diagnosing mispronounced segments in non-native speech on the level of broad phonological properties, such as place of articulation or voicing. Using acoustic features and visual spectrograms extracted from native English utterances, we train several neural classifiers that deduce articulatory properties from segments extracted from non-native English utterances. Visual cues are thereby processed by convolutional neural networks, whereas acoustic cues are processed by recurrent neural networks We show that combining both modalities increases performance over using models in isolation, with important implications for user satisfaction. Furthermore, we test the impact of alignment quality on model performance by comparing results on manually corrected segments and force-aligned segments, showing that the proposed pipeline can dispense with manual correction",
    "checked": true,
    "id": "a8d02876cf8d67d8a34674dd2f1a514f028d13bc",
    "semantic_title": "multimodal articulation-based pronunciation error detection with spectrogram and acoustic features",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/foltz19_interspeech.html": {
    "title": "Using Prosody to Discover Word Order Alternations in a Novel Language",
    "volume": "main",
    "abstract": "Native speakers of a language can use prosodic phrasing to disambiguate syntactically ambiguous sentences [1]. The current paper explores whether prosodic phrasing can help learners determine within-clause word order differences in a new language. Unlike many previous studies, we did not train participants in an artificial language, but exploited word order differences that occur in German. Native English speakers with no knowledge of German were trained with simple main clause sentences as well as complex sentences containing a subordinate clause. During training, prosodic phrasing of complex sentences either aligned or did not align with the sentences' clause structure. The results from two experiments showed that the non-aligned prosodic phrasing helps learners discover clause internal word order differences in German, but only if syntactic variability in the test sessions is low. Overall, the results suggest that learners can exploit prosodic structure to learn word order alternations in certain contexts",
    "checked": true,
    "id": "479a393c153f1b7623f3ff3455c07c135cc49eda",
    "semantic_title": "using prosody to discover word order alternations in a novel language",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bradlow19b_interspeech.html": {
    "title": "Speaking Rate, Information Density, and Information Rate in First-Language and Second-Language Speech",
    "volume": "main",
    "abstract": "Using a corpus of multilingual recordings of a standard text (the North Wind and the Sun passage, NWS) in 11 languages, speaking rate (SR, syllables/second) and information density (ID, number of syllables for the NWS text) were examined in first-language (L1) and second-language (L2) speech. Replicating prior work, cross-language comparison of L1 speech showed a trade-off between SR and ID such that relatively low density languages (many syllables for the NWS text) tended to be produced at relatively fast rates, and vice versa. Furthermore, L2 English was characterized by both slower rate and lower ID than L1 English. That is, L2 English involved more syllables than L1 English for the same NWS text. A comparison of the number of acoustic syllables (i.e. amplitude peaks) with the number of orthographic syllables (i.e. dictionary-based syllable counts for the NWS text) indicated that L1 speech involved substantial syllable reduction (fewer acoustic than orthographic syllables) while L2 speech involved substantial syllable epenthesis (more acoustic than orthographic syllables). These findings suggest that L2 speech production involves temporal restructuring beyond increased segment, syllable and word durations, and that the resultant information rate (information bits transmitted/second) of L2 speech diverges substantially from that of L1 speech",
    "checked": true,
    "id": "b5f488a5c5552a894aa6c7acd0620acdc6d2481b",
    "semantic_title": "speaking rate, information density, and information rate in first-language and second-language speech",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/graham19_interspeech.html": {
    "title": "Articulation Rate as a Metric in Spoken Language Assessment",
    "volume": "main",
    "abstract": "Automated evaluation of non-native pronunciation provides a consistent and more cost-efficient alternative to human evaluation. To that end, there is considerable interest in deriving metrics that are based on the cues human listeners use to judge pronunciation. Previous research reported the use of phonetic features such as vowel characteristics in automated spoken language evaluation. The present study extends this line of work on the significance of phonetic features in automated evaluation of L2 speech (both assessment and feedback). Predictive modelling techniques examined the relationship between various articulation rate metrics one the one hand, and the proficiency and L1 background of non-native English speakers on the other. It was found that the optimal predictive model was one in which the phonetic details of phoneme articulation were factored in the analysis of articulation rate. Model performance varied also according to the L1 background of speakers. The implications for assessment and feedback are discussed",
    "checked": true,
    "id": "72c36196bdca74fafd0023d7f7a8053b8ae753be",
    "semantic_title": "articulation rate as a metric in spoken language assessment",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xu19c_interspeech.html": {
    "title": "Learning Alignment for Multimodal Emotion Recognition from Speech",
    "volume": "main",
    "abstract": "Speech emotion recognition is a challenging problem because human convey emotions in subtle and complex ways. For emotion recognition on human speech, one can either extract emotion related features from audio signals or employ speech recognition techniques to generate text from speech and then apply natural language processing to analyze the sentiment. Further, emotion recognition will be beneficial from using audio-textual multimodal information, it is not trivial to build a system to learn from multimodality. One can build models for two input sources separately and combine them in a decision level, but this method ignores the interaction between speech and text in the temporal domain. In this paper, we propose to use an attention mechanism to learn the alignment between speech frames and text words, aiming to produce more accurate multimodal feature representations. The aligned multimodal features are fed into a sequential model for emotion recognition. We evaluate the approach on the IEMOCAP dataset and the experimental results show the proposed approach achieves the state-of-the-art performance on the dataset",
    "checked": true,
    "id": "101e22de9ef604aa586c806c81926648ac583b99",
    "semantic_title": "learning alignment for multimodal emotion recognition from speech",
    "citation_count": 84
  },
  "https://www.isca-speech.org/archive/interspeech_2019/peperkamp19b_interspeech.html": {
    "title": "Liquid Deletion in French Child-Directed Speech",
    "volume": "main",
    "abstract": "In spoken language, words can have different surface realizations due to the application of language-specific phonological rules. Young children must acquire these rules in order to be able to undo their effects and recognize the intended words during language processing. Evidence so far suggests that they achieve this early on, but the learning mechanisms that they exploit are unknown. As a first step in examining this question, it is necessary to know to what extent phonological rules occur in their input. Here, we investigate the occurrence of liquid deletion, i.e. the optional deletion of the liquid in word-final obstruent-liquid clusters, in French child-directed speech. Analyzing a corpus from the Childes database that contains video recordings, we find that words finishing in obstruent-liquid clusters occur on average once every 13 utterances, and that in more than half of the cases the liquid is deleted. As in adult-directed speech, deletion applies more often before consonants than before vowels and pauses. Furthermore, pairs of tokens of the same word with and without deletion tend to cluster together, with a median distance of 49 seconds of speech. This clustering could be a powerful cue in the process of the acquisition of liquid deletion",
    "checked": true,
    "id": "9d460d695812ad7479f965d91de6a971aa6deab6",
    "semantic_title": "liquid deletion in french child-directed speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/seidl19_interspeech.html": {
    "title": "Towards Detection of Canonical Babbling by Citizen Scientists: Performance as a Function of Clip Length",
    "volume": "main",
    "abstract": "Theoretical, empirical, and intervention research requires access to a large, unbiased, annotated dataset of infant vocalizations for training speech technology to detect and differentiate consonant-vowel (canonical) syllables in infants' vocalizations from less mature vocalizations. Citizen scientists could help us to achieve the goal of this dataset, if classification is accurate regardless of coders' native language and training and can be completed on clips short enough to avoid revealing personal identifying information. Three groups of coders participated in an experiment: trained native, semi-trained native, and minimally-trained foreign. When vocalizations were presented whole, reliability was highest across the trained coders, with little difference between the semi-trained and minimally-trained coders. Among minimally-trained coders, reliability for 400ms-long clips was very similar to that found for full clips, with lower values for 200 and 600ms clips. Finally, error rates were minimized when 400ms-long clips were used. In sum, minimally-trained coders can achieve fairly reliable and accurate results, even when their native language does not match infants' target language and when provided with very short clips. Since shorter clips protect the identity of the child and her family, this manner of data annotation may provide us with a way of building a large, unbiased dataset of infant vocalizations",
    "checked": true,
    "id": "254eb38f4a05c8c294e3b6537153c09dfd88c862",
    "semantic_title": "towards detection of canonical babbling by citizen scientists: performance as a function of clip length",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ludusan19b_interspeech.html": {
    "title": "Nasal Consonant Discrimination in Infant- and Adult-Directed Speech",
    "volume": "main",
    "abstract": "Infant-directed speech (IDS) is thought to play a facilitating role in language acquisition, by simplifying the input infants receive. In particular, the hypothesis that the acoustic level is enhanced to make the input more clear for infants, has been extensively studied in the case of vowels, but less so in the case of consonants. An investigation into how nasal consonants can be discriminated in infant- compared to adult-directed speech (ADS) was performed, on a corpus of Japanese mother-infant spontaneous conversations, by examining all bilabial and alveolar nasals occurring in intervocalic position. The Pearson correlation between corresponding spectrum slices of nasal consonants, in identical vowel contexts, was employed as similarity measure and a statistical model was fit using this information. It revealed a decrease in similarity between the nasal classes, in IDS compared to ADS, although the effect was not statistically significant. We confirmed these results, using an unsupervised machine learning algorithm to discriminate between the two nasal classes, obtaining similar classification performance in IDS and ADS. We discuss our findings in the context of the current literature on infant-directed speech",
    "checked": true,
    "id": "65414f5c9a823b1c89a5306659c44feedd58e0a0",
    "semantic_title": "nasal consonant discrimination in infant- and adult-directed speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/marklund19_interspeech.html": {
    "title": "No Distributional Learning in Adults from Attended Listening to Non-Speech",
    "volume": "main",
    "abstract": "Distributional learning is a perceptual process hypothesized to underlie the phenomena of phonetic recalibration and selective adaptation, as well as infant speech sound category learning. However, in order to be conclusively tied to the earliest stages of speech sound category development, that is, the formation of novel perceptual categories, distributional learning must be shown to operate on stimuli for which there are no pre-existing categories. We investigated this in a previous study, finding no evidence of distributional learning in adults from unattended listening to non-speech. Since attention to stimuli impacts distributional learning, the present study focused on distributional learning from attended listening to non-speech. The same paradigm was used as in the previous study, except that participants' attention was directed towards stimuli by means of a cover task. Non-speech stimuli were spectrally rotated vowels and the mismatch negativity was used to measure perceptual categorization. No distributional learning was found, that is, no effect of attention on distributional learning was demonstrated. This could mean that the distributional learning process does not operate on stimuli where perceptual categories do not already exist, or that the mismatch negativity measure does not capture the earliest stages of perceptual category development",
    "checked": true,
    "id": "eb71420d2ad6620291c116292703a4d4e26bd74e",
    "semantic_title": "no distributional learning in adults from attended listening to non-speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rasanen19_interspeech.html": {
    "title": "A Computational Model of Early Language Acquisition from Audiovisual Experiences of Young Infants",
    "volume": "main",
    "abstract": "Earlier research has suggested that human infants might use statistical dependencies between speech and non-linguistic multimodal input to bootstrap their language learning before they know how to segment words from running speech. However, feasibility of this hypothesis in terms of real-world infant experiences has remained unclear. This paper presents a step towards a more realistic test of the multimodal bootstrapping hypothesis by describing a neural network model that can learn word segments and their meanings from referentially ambiguous acoustic input. The model is tested on recordings of real infant-caregiver interactions using utterance-level labels for concrete visual objects that were attended by the infant when caregiver spoke an utterance containing the name of the object, and using random visual labels for utterances during absence of attention. The results show that beginnings of lexical knowledge may indeed emerge from individually ambiguous learning scenarios. In addition, the hidden layers of the network show gradually increasing selectivity to phonetic categories as a function of layer depth, resembling models trained for phone recognition in a supervised manner",
    "checked": true,
    "id": "b5e8bf3ff49c8cbe88627b97e9a1947d3536fc03",
    "semantic_title": "a computational model of early language acquisition from audiovisual experiences of young infants",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/du19b_interspeech.html": {
    "title": "The Production of Chinese Affricates /ts/ and /tsh/ by Native Urdu Speakers",
    "volume": "main",
    "abstract": "Previous studies have shown that learners with different native language backgrounds have common difficulties in learning Chinese affricates but demonstrate in various patterns. While few studies investigated this issue of native Urdu speakers. To address the production of Chinese affricates /ts/ and /ts / by native Urdu speakers, speech materials, produced by two groups of subjects with different Chinese proficiency, were selected from the BLCU-SAIT speech corpus. The error rate and error types of their production of Chinese affricates /ts/ and /ts / have been discussed after transcription and data analysis. The results show that though there are no counterparts of Chinese affricates /ts/ and /ts / in Urdu, the error and the acquisition pattern of these two affricates, to some extent, affected by individual differences of their roles in Urdu except universal similarities and differences between two languages. The findings of this study shed some light on second language learning and teaching",
    "checked": true,
    "id": "a00c185e153a927316ed36d38706f43688528dd4",
    "semantic_title": "the production of chinese affricates /ts/ and /tsh/ by native urdu speakers",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19s_interspeech.html": {
    "title": "Multi-Stream Network with Temporal Attention for Environmental Sound Classification",
    "volume": "main",
    "abstract": "Environmental sound classification systems often do not perform robustly across different sound classification tasks and audio signals of varying temporal structures. We introduce a multi-stream convolutional neural network with temporal attention that addresses these problems. The network relies on three input streams consisting of raw audio and spectral features and utilizes a temporal attention function computed from energy changes over time. Training and classification utilizes decision fusion and data augmentation techniques that incorporate uncertainty. We evaluate this network on three commonly used datasets for environmental sound and audio scene classification and achieve new state-of-the-art performance without any changes in network architecture or front-end preprocessing, thus demonstrating better generalizability",
    "checked": true,
    "id": "29df280ff54d7478fee6a626fbb9ca1b21234287",
    "semantic_title": "multi-stream network with temporal attention for environmental sound classification",
    "citation_count": 47
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cerutti19_interspeech.html": {
    "title": "Neural Network Distillation on IoT Platforms for Sound Event Detection",
    "volume": "main",
    "abstract": "In most classification tasks, wide and deep neural networks perform and generalize better than their smaller counterparts, in particular when they are exposed to large and heterogeneous training sets. However, in the emerging field of Internet of Things memory footprint and energy budget pose severe limits on the size and complexity of the neural models that can be implemented on embedded devices. The Student-Teacher approach is an attractive strategy to distill knowledge from a large network into smaller ones, that can fit on low-energy low-complexity embedded IoT platforms. In this paper, we consider the outdoor sound event detection task as a use case. Building upon the VGGish network, we investigate different distillation strategies to substantially reduce the classifier's size and computational cost with minimal performance losses. Experiments on the UrbanSound8K dataset show that extreme compression factors (up to 4.2 • 10 for parameters and 1.2 • 10 for operations with respect to VGGish) can be achieved, limiting the accuracy degradation from 75% to 70%. Finally, we compare different embedded platforms to analyze the trade-off between available resources and achievable accuracy",
    "checked": true,
    "id": "cfa27675ad6f866337cc62847afdec754d1fbd39",
    "semantic_title": "neural network distillation on iot platforms for sound event detection",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lu19d_interspeech.html": {
    "title": "Class-Wise Centroid Distance Metric Learning for Acoustic Event Detection",
    "volume": "main",
    "abstract": "Designing good feature extraction and classifier models is essential for obtaining high performances of acoustic event detection (AED) systems. Current state-of-the-art algorithms are based on deep neural network models that jointly learn the feature representation and classifier models. As a typical pipeline in these algorithms, several network layers with nonlinear transforms are stacked for feature extraction, and a classifier layer with a softmax transform is applied on top of these extracted features to obtain normalized probability outputs. This pipeline is directly connected to a final goal for class discrimination without explicitly considering how the features should be distributed for inter-class and intra-class samples. In this paper, we explicitly add a distance metric constraint on feature extraction process with a goal to reduce intra-class sample distances and increase inter-class sample distances. Rather than estimating the pair-wise distances of samples, the distances are efficiently calculated between samples and class cluster centroids. With this constraint, the learned features have a good property for improving the generalization of the classification models. AED experiments on an urban sound classification task were carried out to test the algorithm. Results showed that the proposed algorithm efficiently improved the performance on the current state-of-the-art deep learning algorithms",
    "checked": true,
    "id": "2881fa487096384f21d8e49c980396fd74e0e242",
    "semantic_title": "class-wise centroid distance metric learning for acoustic event detection",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bai19b_interspeech.html": {
    "title": "A Hybrid Approach to Acoustic Scene Classification Based on Universal Acoustic Models",
    "volume": "main",
    "abstract": "For the acoustic scenes classification, the main challenge is distinguishing similar acoustic segments between different scenes. To solve this problem, many deep learning based approaches have been proposed without considering the relevance of different acoustic scenes. In this paper, we propose a novel acoustic segment model (ASM) for acoustic scene classification. ASM aims at giving finer segmentation and covering all acoustic scenes through searching for the underlying phoneme like acoustic units. Furthermore, acoustic segments are modeled by Hidden Markov Models (HMMs) and each audio is decoded into ASM sequences without prior linguistic knowledge. Similar to the term vector of a text document, these ASM sequences are converted into co-occurrence statistics feature vectors and SVM/DNN is used as classifier back-end. Validated on the DCASE 2018 task, the proposed approach can achieve a competitive performance with single model and no data augment. By using visualization analysis, we excavate the potential similar units hidden in auditory sense",
    "checked": true,
    "id": "634cec4190787db40c6648ed5a6c66016a62841d",
    "semantic_title": "a hybrid approach to acoustic scene classification based on universal acoustic models",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/he19b_interspeech.html": {
    "title": "Hierarchical Pooling Structure for Weakly Labeled Sound Event Detection",
    "volume": "main",
    "abstract": "Sound event detection with weakly labeled data is considered as a problem of multi-instance learning. And the choice of pooling function is the key to solving this problem. In this paper, we proposed a hierarchical pooling structure to improve the performance of weakly labeled sound event detection system. Proposed pooling structure has made remarkable improvements on three types of pooling function without adding any parameters. Moreover, our system has achieved competitive performance on Task 4 of Detection and Classification of Acoustic Scenes and Events (DCASE) 2017 Challenge using hierarchical pooling structure",
    "checked": true,
    "id": "41ea945b7414fa42f4e0fff98bdfd7e70585d994",
    "semantic_title": "hierarchical pooling structure for weakly labeled sound event detection",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xia19_interspeech.html": {
    "title": "Sound Event Detection in Multichannel Audio Using Convolutional Time-Frequency-Channel Squeeze and Excitation",
    "volume": "main",
    "abstract": "In this study, we introduce a convolutional time-frequency-channel \"Squeeze and Excitation\" (tfc-SE) module to explicitly model inter-dependencies between the time-frequency domain and multiple channels. The tfc-SE module consists of two parts: tf-SE block and c-SE block which are designed to provide attention on time-frequency and channel domain, respectively, for adaptively recalibrating the input feature map. The proposed tfc-SE module, together with a popular Convolutional Recurrent Neural Network (CRNN) model, are evaluated on a multi-channel sound event detection task with overlapping audio sources: the training and test data are synthesized TUT Sound Events 2018 datasets, recorded with microphone arrays. We show that the tfc-SE module can be incorporated into the CRNN model at a small additional computational cost and bring significant improvements on sound event detection accuracy. We also perform detailed ablation studies by analyzing various factors that may influence the performance of the SE blocks. We show that with the best tfc-SE block, error rate (ER) decreases from 0.2538 to 0.2026, relative 20.17% reduction of ER, and 5.72% improvement of F1 score. The results indicate that the learned acoustic embeddings with the tfc-SE module efficiently strengthen time-frequency and channel-wise feature representations to improve the discriminative performance",
    "checked": true,
    "id": "0b0392ac52e794d94996195098c90de13e68dce1",
    "semantic_title": "sound event detection in multichannel audio using convolutional time-frequency-channel squeeze and excitation",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pham19b_interspeech.html": {
    "title": "A Robust Framework for Acoustic Scene Classification",
    "volume": "main",
    "abstract": "Acoustic scene classification (ASC) using front-end time-frequency features and back-end neural network classifiers has demonstrated good performance in recent years. However a profusion of systems has arisen to suit different tasks and datasets, utilising different feature and classifier types. This paper aims at a robust framework that can explore and utilise a range of different time-frequency features and neural networks, either singly or merged, to achieve good classification performance. In particular, we exploit three different types of front-end time-frequency feature; log energy Mel filter, Gammatone filter and constant Q transform. At the back-end we evaluate effective a two-stage model that exploits a Convolutional Neural Network for pre-trained feature extraction, followed by Deep Neural Network classifiers as a post-trained feature adaptation model and classifier. We also explore the use of a data augmentation technique for these features that effectively generates a variety of intermediate data, reinforcing model learning abilities, particularly for marginal cases. We assess performance on the DCASE2016 dataset, demonstrating good classification accuracies exceeding 90%, significantly outperforming the DCASE2016 baseline and highly competitive compared to state-of-the-art systems",
    "checked": true,
    "id": "cbc26c7bc1e121a52ca6002c290de06dbae98710",
    "semantic_title": "a robust framework for acoustic scene classification",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shi19c_interspeech.html": {
    "title": "Compression of Acoustic Event Detection Models with Quantized Distillation",
    "volume": "main",
    "abstract": "Acoustic Event Detection (AED), aiming at detecting categories of events based on audio signals, has found application in many intelligent systems. Recently deep neural network significantly advances this field and reduces detection errors to a large scale. However how to efficiently execute deep models in AED has received much less attention. Meanwhile state-of-the-art AED models are based on large deep models, which are computational demanding and challenging to deploy on devices with constrained computational resources. In this paper, we present a simple yet effective compression approach which jointly leverages knowledge distillation and quantization to compress larger network (teacher model) into compact network (student model). Experimental results show proposed technique not only lowers error rate of original compact network by 15% through distillation but also further reduces its model size to a large extent (2% of teacher, 12% of full-precision student) through quantization",
    "checked": true,
    "id": "d465dc8b416857a90b35f762376a9a636ca7b578",
    "semantic_title": "compression of acoustic event detection models with quantized distillation",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19m_interspeech.html": {
    "title": "An End-to-End Audio Classification System Based on Raw Waveforms and Mix-Training Strategy",
    "volume": "main",
    "abstract": "Audio classification can distinguish different kinds of sounds, which is helpful for intelligent applications in daily life. However, it remains a challenging task since the sound events in an audio clip is probably multiple, even overlapping. This paper introduces an end-to-end audio classification system based on raw waveforms and mix-training strategy. Compared to human-designed features which have been widely used in existing research, raw waveforms contain more complete information and are more appropriate for multi-label classification. Taking raw waveforms as input, our network consists of two variants of ResNet structure which can learn a discriminative representation. To explore the information in intermediate layers, a multi-level prediction with attention structure is applied in our model. Furthermore, we design a mix-training strategy to break the performance limitation caused by the amount of training data. Experiments show that the mean average precision of the proposed audio classification system on Audio Set dataset is 37.2%. Without using extra training data, our system exceeds the state-of-the-art multi-level attention model",
    "checked": true,
    "id": "7850c415d3af282d19b41718739f40ff22c105a3",
    "semantic_title": "an end-to-end audio classification system based on raw waveforms and mix-training strategy",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19k_interspeech.html": {
    "title": "Few-Shot Audio Classification with Attentional Graph Neural Networks",
    "volume": "main",
    "abstract": "Few-shot learning is a very promising and challenging field of machine learning as it aims to understand new concepts from very few labeled examples. In this paper, we propose attentional framework to extend recently proposed few-shot learning with graph neural network [1] in audio classification scenario. The objective of proposed attentional framework is to introduce a flexible framework to implement selectively concentration procedure on support examples for each query process. we also present an empirical study on confidence measure for few-shot learning application by combining posterior probability with normalized entropy of the network's probability output. The efficiency of the proposed method is demonstrated with experiments on balanced training set of Audio set for training and a 5-way test set composed of about 5-hour audio data for testing",
    "checked": true,
    "id": "e26474050b0edab933efba2ed85fb2e99fddc271",
    "semantic_title": "few-shot audio classification with attentional graph neural networks",
    "citation_count": 32
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lu19e_interspeech.html": {
    "title": "Semi-Supervised Audio Classification with Consistency-Based Regularization",
    "volume": "main",
    "abstract": "Consistency-based semi-supervised learning methods such as the Mean Teacher method are state-of-the-art on image datasets, but have yet to be applied to audio data. Such methods encourage model predictions to be consistent on perturbed input data. In this paper, we incorporate audio-specific perturbations into the Mean Teacher algorithm and demonstrate the effectiveness of the resulting method on audio classification tasks. Specifically, we perturb audio inputs by mixing in other environmental audio clips, and leverage other training examples as sources of noise. Experiments on the Google Speech Command Dataset and UrbanSound8K Dataset show that the method can achieve comparable performance to a purely supervised approach while using only a fraction of the labels",
    "checked": true,
    "id": "ed2f2da0b2a625de08152498f00075e69bc461d3",
    "semantic_title": "semi-supervised audio classification with consistency-based regularization",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mizgajski19_interspeech.html": {
    "title": "Avaya Conversational Intelligence: A Real-Time System for Spoken Language Understanding in Human-Human Call Center Conversations",
    "volume": "main",
    "abstract": "Avaya Conversational Intelligence (ACI) is an end-to-end, cloud-based solution for real-time Spoken Language Understanding for call centers. It combines large vocabulary, real-time speech recognition, transcript refinement, and entity and intent recognition in order to convert live audio into a rich, actionable stream of structured events. These events can be further leveraged with a business rules engine, thus serving as a foundation for real-time supervision and assistance applications. After the ingestion, calls are enriched with unsupervised keyword extraction, abstractive summarization, and business-defined attributes, enabling offline use cases, such as business intelligence, topic mining, full-text search, quality assurance, and agent training. ACI comes with a pretrained, configurable library of hundreds of intents and a robust intent training environment that allows for efficient, cost-effective creation and customization of customer-specific intents",
    "checked": true,
    "id": "4670b1a5f171d853e3e59176f01748e31b277747",
    "semantic_title": "avaya conversational intelligence: a real-time system for spoken language understanding in human-human call center conversations",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/an19b_interspeech.html": {
    "title": "Robust Keyword Spotting via Recycle-Pooling for Mobile Game",
    "volume": "main",
    "abstract": "We present an effective method to solve a small-footprint keyword spotting (KWS) task via deep neural network for mobile game. Our goal is to improve the accuracy of KWS in various environments. To this end, we propose a new neural network layer named recycle-pooling. Extensive experiments indicate that our recycle-pooling based convolutional neural network (RP-CNN) indeed improves the performance of KWS in both clean and noisy data for mobile game. We will perform live demonstration of RP-CNN based KWS integrated into a full-sized, production-quality mobile game A3: Still Alive, which is one of the major games from Netmarble this year and will be available on market soon",
    "checked": true,
    "id": "f2e74d2b5d6e6be87d94720f2f4e0c6f474fa158",
    "semantic_title": "robust keyword spotting via recycle-pooling for mobile game",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chylek19_interspeech.html": {
    "title": "Multimodal Dialog with the MALACH Audiovisual Archive",
    "volume": "main",
    "abstract": "In this paper, we present a multimodal dialog system capable of information retrieval from the large audiovisual archive MALACH of Holocaust testimonies. The users can use spoken natural language queries to search the archive. A graphical user interface allows the users to quickly view footage with the answers and explore their context. The dialog was deployed in two languages — English and Czech. The system uses automatic speech recognition and natural language processing for knowledge base construction and for processing of the user's input",
    "checked": true,
    "id": "7080b6165c724c5afe0ebc61d5b9fb6d0579318b",
    "semantic_title": "multimodal dialog with the malach audiovisual archive",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jelil19_interspeech.html": {
    "title": "SpeechMarker: A Voice Based Multi-Level Attendance Application",
    "volume": "main",
    "abstract": "This work describes a multi-level speaker verification (SV) framework that is accessible via a graphical user interface (GUI) with attendance as an application. This framework has three different modalities of SV system, namely, voice-password, text-dependent and text-independent. The decision for attendance marking can be taken from each of the modalities or by fusion. There are two operating modes of the developed GUI, which are user and debug modes. The user mode is for general users to mark attendance, whereas the debug mode is to study the behavior of the three modalities from deployment point of view. The speech waveforms, different plots and scores can be analyzed in the debug mode for analysis. The system has been deployed successfully for regular attendance marking among a closed group in a laboratory environment",
    "checked": true,
    "id": "db517ce2cc7294bbcd153107935bf8760f368398",
    "semantic_title": "speechmarker: a voice based multi-level attendance application",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wu19h_interspeech.html": {
    "title": "Robust Sound Recognition: A Neuromorphic Approach",
    "volume": "main",
    "abstract": "Humans perform remarkably well at sound classification that is used as cues to support high-level cognitive functions. Inspired by the anatomical structure of human cochlea and auditory attention mechanism, we present a novel neuromorphic sound recognition system that integrates an event-driven auditory front-end and a biologically plausible spiking neural network classifier (SNN) for robust sound and speech recognition. Due to its event-driven nature, the SNN classifier is several orders of magnitude more energy efficient than deep learning classifier, therefore, it is suitable for many applications in wearable devices",
    "checked": true,
    "id": "a75401f9cdf945c66d1031c5d5badebde8a59945",
    "semantic_title": "robust sound recognition: a neuromorphic approach",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hu19c_interspeech.html": {
    "title": "The CUHK Dysarthric Speech Recognition Systems for English and Cantonese",
    "volume": "main",
    "abstract": "Speech disorders affect many people around the world and introduce a negative impact on their quality of life. Dysarthria is a neural-motor speech disorder that obstructs the normal production of speech. Current automatic speech recognition (ASR) systems are developed for normal speech. They are not suitable for accurate recognition of disordered speech. To the best of our knowledge, the majority of disordered speech recognition systems developed to date are for English. In this paper, we present two disordered speech recognition systems for both English and Cantonese. Both systems demonstrate competitive performance when compared with the Google speech recognition API and human recognition results",
    "checked": true,
    "id": "21f55376e76a0602525bdfbe54c00ff97226c30a",
    "semantic_title": "the cuhk dysarthric speech recognition systems for english and cantonese",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schiel19_interspeech.html": {
    "title": "BAS Web Services for Automatic Subtitle Creation and Anonymization",
    "volume": "main",
    "abstract": "In this Show&Tell contribution we will demonstrate two new public web services provided by the CLARIN centre Bavarian Archive for Speech Signals at the university of Munich. ‘Subtitle' is a service that allows users to automatically create and add a subtitle track to video recordings; ‘Anonymizer' can be applied to media files and their respective annotations in order to mask user-defined spoken terms in the signal as well as in the annotation. Both services are accessible via a RESTful API or a user-friendly web-interface. In the demo we will demonstrate both services independently and in combination (anonymizing subtitles) using the web interface",
    "checked": true,
    "id": "1ecf92704c90bc1c93f57e0c67914b89f402a5d3",
    "semantic_title": "bas web services for automatic subtitle creation and anonymization",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/voe19_interspeech.html": {
    "title": "A User-Friendly and Adaptable Re-Implementation of an Acoustic Prominence Detection and Annotation Tool",
    "volume": "main",
    "abstract": "Annotating prominence phenomena in speech corpora is a challenging task, as it requires many resources. Therefore, several approaches have emerged in the past decades to automatise the process of detecting and annotating prominence. Among these, [1] propose a fully automatically operating acoustic prominence detection and annotation tool that yields promising results. The present work aims at making this tool accessible to a broader community and more inviting in the manipulation of features. To do so, we re-implemented the prominence annotation approach of [1] in the programming language of the software Praat [2], which is commonly used for speech analysis purposes within several areas of research. By implementing a user-friendly interface, the Praat-based prominence detection and annotation tool can be controlled without any source code interaction, which makes it accessible to users with differing levels of programming experience. More experienced users have the option to directly work with the comprehensively commented and documented source code to manipulate or add features within the prominence detection and annotation process. Providing a more accessible and easier to manipulate re-implementation of the tool of [1], we want to contribute to further developments in the area of automatic prominence detection and annotation",
    "checked": true,
    "id": "b2c51d9f56f95d74ddac6d89211253a52773a217",
    "semantic_title": "a user-friendly and adaptable re-implementation of an acoustic prominence detection and annotation tool",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dominguez19_interspeech.html": {
    "title": "PyToBI: A Toolkit for ToBI Labeling Under Python",
    "volume": "main",
    "abstract": "PyToBI is introduced as a user-friendly toolkit for the automatic annotation of intonation contours using the Tones and Breaks Indexes convention, known as ToBI",
    "checked": true,
    "id": "ffa89a893bd973eb24f7ba6687de33b6b4bdab67",
    "semantic_title": "pytobi: a toolkit for tobi labeling under python",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/levy19_interspeech.html": {
    "title": "GECKO — A Tool for Effective Annotation of Human Conversations",
    "volume": "main",
    "abstract": "With the dramatic improvement in automated speech recognition (ASR) accuracy, a variety of machine learning (ML) and natural language processing (NLP) algorithms are designed for human conversation data. Supervised machine learning and particularly deep neural networks (DNNs) require large annotated datasets in order to train high quality models. In this paper we describe Gecko, a tool for annotation of speech and language features of conversations. Gecko allows efficient and effective segmentation of the voice signal by speaker as well as annotation of the linguistic content of the conversation. A key feature of Gecko is the presentation of the output of automatic segmentation and transcription systems in an intuitive user interface for editing. Gecko allows annotation of Voice Activity Detection (VAD), Diarization, Speaker Identification and ASR outputs on a large scale. Both annotators and data scientists have reported improvement in the speed and accuracy of work. Gecko is publicly available for the benefit of the community at https://github.com/gong-io/gecko",
    "checked": true,
    "id": "cbd3d928c3af040ea2286544e884490ba1f7e33c",
    "semantic_title": "gecko - a tool for effective annotation of human conversations",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lo19b_interspeech.html": {
    "title": "SLP-AA: Tools for Sign Language Phonetic and Phonological Research",
    "volume": "main",
    "abstract": "This paper describes the features of a free, open-source software tool, Sign Language Phonetic Annotator+Analyzer (SLP-AA), which is designed to facilitate phonetic/phonological transcription and analysis on sign languages The software supports two modes: the Annotator mode allows the user to build phonetically transcribed corpora of sign languages, and the Analyzer mode lets the user perform phonological searches or analyses on the built corpora. We give a detailed description of one type of phonological search — the extended finger search function — and point out a potential application of this function with respect to sign language research",
    "checked": true,
    "id": "95e15671dd3b089a97f67756b09d2c085d1b5d8d",
    "semantic_title": "slp-aa: tools for sign language phonetic and phonological research",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19t_interspeech.html": {
    "title": "SANTLR: Speech Annotation Toolkit for Low Resource Languages",
    "volume": "main",
    "abstract": "While low resource speech recognition has attracted a lot of attention from the speech community, there are a few tools available to facilitate low resource speech collection. In this work, we present SANTLR: Speech Annotation Toolkit for Low Resource Languages. It is a web-based toolkit which allows researchers to easily collect and annotate a corpus of speech in a low resource language. Annotators may use this toolkit for two purposes: transcription or recording. In transcription, annotators would transcribe audio files provided by the researchers; in recording, annotators would record their voice by reading provided texts. We highlight two properties of this toolkit. First, SANTLR has a very user-friendly User Interface (UI). Both researchers and annotators may use this simple web interface to interact. There is no requirement for the annotators to have any expertise in audio or text processing. The toolkit would handle all preprocessing and postprocessing steps. Second, we employ a multi-step ranking mechanism facilitate the annotation process. In particular, the toolkit would give higher priority to utterances which are easier to annotate and are more beneficial to achieving the goal of the annotation, e.g. quickly training an acoustic model",
    "checked": true,
    "id": "92ff24b053b95c0196f03e944e52fe64dcbe9c8c",
    "semantic_title": "santlr: speech annotation toolkit for low resource languages",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gruber19_interspeech.html": {
    "title": "Web-Based Speech Synthesis Editor",
    "volume": "main",
    "abstract": "This paper presents a web-based GUI frontend for a backend TTS system, including an editor of the synthesized speech. The tool allows synthesizing speech from general texts using all available synthesis methods with both modifications within the speech synthesis process and subsequent modifications of the synthesized speech targeting for instance speech prolongation, shortening, pitch or volume increasing or decreasing, etc",
    "checked": true,
    "id": "5bd3a606a49dca526150bc10680961cb8716cfce",
    "semantic_title": "web-based speech synthesis editor",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/perrotin19_interspeech.html": {
    "title": "GFM-Voc: A Real-Time Voice Quality Modification System",
    "volume": "main",
    "abstract": "This article introduces GFM-Voc, a new system that allows high-quality and real-time voice modification, including both vocalic formants shifting, and voice quality manipulation. In particular, the system is based on the implementation of a newly developed source-filter decomposition method, called GFM-IAIF, that allows the extraction of both vocal tract and glottis spectral envelopes as a compact set of filter parameters. The latter are then controllable through a GUI, before re-synthesis of the speech with the modified parameters. The system requires no training, and operates on any voice, male or female, without tuning. Given the close link between spectral parameters and speech perception, this system provides an intuitive way to independently manipulate the vocalic formants and the spectral shape of the glottal flow that is responsible for voice quality perception. Additionally, rules have been implemented to link the glottis parameters to high-level voice quality parameters such as vocal force and tenseness. Examples of applications for this system include expressive speech synthesis, by adding the system at the end of a speech synthesiser pipeline, auditory feedback perturbation to study a speaker's response to modified speech, and speech therapy",
    "checked": true,
    "id": "aade4cafd1c9513c083afb83718dc8d1ed150e4e",
    "semantic_title": "gfm-voc: a real-time voice quality modification system",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/szekely19_interspeech.html": {
    "title": "Off the Cuff: Exploring Extemporaneous Speech Delivery with TTS",
    "volume": "main",
    "abstract": "Extemporaneous speech is a delivery type in public speaking which uses a structured outline but is otherwise delivered conversationally, off the cuff. This demo uses a natural-sounding spontaneous conversational speech synthesiser to simulate this delivery style. We resynthesised the beginnings of two Interspeech keynote speeches with TTS that produces multiple different versions of each utterance that vary in fluency and filled-pause placement. The platform allows the user to mark the samples according to any perceptual aspect of interest, such as certainty, authenticity, confidence, etc. During the speech delivery, they can decide on the fly which realisation to play, addressing their audience in a connected, conversational fashion. Our aim is to use this platform to explore speech synthesis evaluation options from a production perspective and in situational contexts",
    "checked": true,
    "id": "01f23ebf72661ea3087eeaa2169917ef0cd18035",
    "semantic_title": "off the cuff: exploring extemporaneous speech delivery with tts",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kessler19_interspeech.html": {
    "title": "Synthesized Spoken Names: Biases Impacting Perception",
    "volume": "main",
    "abstract": "Utilizing a existing neural text-to-speech synthesis architecture to generate person names and comparing them to reference names read aloud in a formal context, we explore how bias resulting from training data impacts the synthesis of person names, focusing on frequency and origin of names. Long-term, we aim to apply voice conversion of person names to aid the effective reading aloud of such names in celebratory ceremonies",
    "checked": true,
    "id": "3a041aa3bca033613f7489fae5fbdc6eaf6cde26",
    "semantic_title": "synthesized spoken names: biases impacting perception",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bernardo19_interspeech.html": {
    "title": "Unbabel Talk — Human Verified Translations for Voice Instant Messaging",
    "volume": "main",
    "abstract": "Unbabel Talk is a speech-to-speech translation application that provides human certified translations for voice instant messaging (IM) in multilingual scenarios. By combining Unbabel's translation pipeline with state-of-the-art automatic speech recognition (ASR) and text-to-speech (TTS) models, Unbabel Talk can be used to send a voice message in a language of choice through popular messaging platforms. The app further ensures that translations have high quality, either by certifying them through Unbabel's own quality estimation (QE) tool and/or through Unbabel's community of translators. There are two versions of the app. On version 1, the app synthesizes audio that can be delivered with male or female standard voices. Version 2 has features that are currently being developed, such as voice morphing and transcription correction through Unbabel's community",
    "checked": true,
    "id": "ca32913fec47e33b286f132248e48c8786dddc7b",
    "semantic_title": "unbabel talk - human verified translations for voice instant messaging",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rabiee19_interspeech.html": {
    "title": "Adjusting Pleasure-Arousal-Dominance for Continuous Emotional Text-to-Speech Synthesizer",
    "volume": "main",
    "abstract": "Emotion is not limited to discrete categories of happy, sad, angry, fear, disgust, surprise, and so on. Instead, each emotion category is projected into a set of nearly independent dimensions, named pleasure (or valence), arousal, and dominance, known as PAD. The value of each dimension varies from -1 to 1, such that the neutral emotion is in the center with all-zero values. Training an emotional continuous text-to-speech (TTS) synthesizer on the independent dimensions provides the possibility of emotional speech synthesis with unlimited emotion categories. Our end-to-end neural speech synthesizer is based on the well-known Tacotron. Empirically, we have found the optimum network architecture for injecting the 3D PADs. Moreover, the PAD values are adjusted for the speech synthesis purpose",
    "checked": true,
    "id": "1aee6d50e408bb3a4c176d5bdaec22c06b68ad87",
    "semantic_title": "adjusting pleasure-arousal-dominance for continuous emotional text-to-speech synthesizer",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lapata19_interspeech.html": {
    "title": "Learning Natural Language Interfaces with Neural Models",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "9e98709ea1a13a7264071d6beb927c0d2a8f10e7",
    "semantic_title": "learning natural language interfaces with neural models",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nautsch19c_interspeech.html": {
    "title": "The GDPR & Speech Data: Reflections of Legal and Technology Communities, First Steps Towards a Common Understanding",
    "volume": "main",
    "abstract": "Privacy preservation and the protection of speech data is in high demand, not least as a result of recent regulation, e.g. the General Data Protection Regulation (GDPR) in the EU. While there has been a period with which to prepare for its implementation, its implications for speech data is poorly understood. This assertion applies to both the legal and technology communities, and is hardly surprising since there is no universal definition of ‘privacy', let alone a clear understanding of when or how the GDPR applies to the capture, storage and processing of speech data. In aiming to initiate the discussion that is needed to establish a level of harmonisation that is thus far lacking, this contribution presents some reflections of both legal and technology communities on the implications of the GDPR as regards speech data. The article outlines the need for taxonomies at the intersection of speech technology and data privacy — a discussion that is still very much in its infancy — and describes the ways to safeguards and priorities for future research. In being agnostic to any specific application, the treatment should be of interest to the speech communication community at large",
    "checked": true,
    "id": "e59fc0c397f4fdca183fcdf1d242e14f4d21d527",
    "semantic_title": "the gdpr & speech data: reflections of legal and technology communities, first steps towards a common understanding",
    "citation_count": 58
  },
  "https://www.isca-speech.org/archive/interspeech_2019/srivastava19_interspeech.html": {
    "title": "Privacy-Preserving Adversarial Representation Learning in ASR: Reality or Illusion?",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) is a key technology in many services and applications. This typically requires user devices to send their speech data to the cloud for ASR decoding. As the speech signal carries a lot of information about the speaker, this raises serious privacy concerns. As a solution, an encoder may reside on each user device which performs local computations to anonymize the representation. In this paper, we focus on the protection of speaker identity and study the extent to which users can be recognized based on the encoded representation of their speech as obtained by a deep encoder-decoder architecture trained for ASR. Through speaker identification and verification experiments on the Librispeech corpus with open and closed sets of speakers, we show that the representations obtained from a standard architecture still carry a lot of information about speaker identity. We then propose to use adversarial training to learn representations that perform well in ASR while hiding speaker identity. Our results demonstrate that adversarial training dramatically reduces the closed-set classification accuracy, but this does not translate into increased open-set verification error hence into increased protection of the speaker identity in practice. We suggest several possible reasons behind this negative result",
    "checked": true,
    "id": "bac1bf436896163cac6a0975e86be1266fd0cab7",
    "semantic_title": "privacy-preserving adversarial representation learning in asr: reality or illusion?",
    "citation_count": 60
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nelus19_interspeech.html": {
    "title": "Privacy-Preserving Siamese Feature Extraction for Gender Recognition versus Speaker Identification",
    "volume": "main",
    "abstract": "In this paper we propose a deep neural-network-based feature extraction scheme with the purpose of reducing the privacy risks encountered in speaker classification tasks. For this we choose a challenging scenario where we wish to perform gender recognition but at the same time prevent an attacker who has intercepted the features to perform speaker identification. Our approach is to employ Siamese training in order to obtain a feature representation that minimizes the Euclidean distance between same gender speakers while maximizing it for different gender speakers. It is experimentally shown that the obtained effect is of anonymizing speakers from the same gender class and thus drastically reducing privacy risks while still permitting class discrimination with a higher accuracy than other previously investigated methods",
    "checked": true,
    "id": "9984c44ddd574ecb76d50f3cf8ae548bc1b14ff8",
    "semantic_title": "privacy-preserving siamese feature extraction for gender recognition versus speaker identification",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nelus19b_interspeech.html": {
    "title": "Privacy-Preserving Variational Information Feature Extraction for Domestic Activity Monitoring versus Speaker Identification",
    "volume": "main",
    "abstract": "In this paper we highlight the privacy risks entailed in deep neural network feature extraction for domestic activity monitoring. We employ the baseline system proposed in the Task 5 of the DCASE 2018 challenge and simulate a feature interception attack by an eavesdropper who wants to perform speaker identification. We then propose to reduce the aforementioned privacy risks by introducing a variational information feature extraction scheme that allows for good activity monitoring performance while at the same time minimizing the information of the feature representation, thus restricting speaker identification attempts. We analyze the resulting model's composite loss function and the budget scaling factor used to control the balance between the performance of the trusted and attacker tasks. It is empirically demonstrated that the proposed method reduces speaker identification privacy risks without significantly deprecating the performance of domestic activity monitoring tasks",
    "checked": true,
    "id": "93d2ba99f747a2486de39bdaf990797e0dd965e9",
    "semantic_title": "privacy-preserving variational information feature extraction for domestic activity monitoring versus speaker identification",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/thaine19_interspeech.html": {
    "title": "Extracting Mel-Frequency and Bark-Frequency Cepstral Coefficients from Encrypted Signals",
    "volume": "main",
    "abstract": "We describe a method for extracting Mel-Frequency and Bark-Frequency Cepstral Coefficient from an encrypted signal without having to decrypt any intermediate values. To do so, we introduce a novel approach for approximating the value of logarithms given encrypted input data. This method works over any interval for which logarithms are defined and bounded Extracting spectral features from encrypted signals is the first step towards achieving secure end-to-end automatic speech recognition over encrypted data. We experimentally determine the appropriate precision thresholds to support accurate WER for ASR over the TIMIT dataset",
    "checked": true,
    "id": "c21cd92d75f5a816c3da49a2fc2118d2b9b67b9e",
    "semantic_title": "extracting mel-frequency and bark-frequency cepstral coefficients from encrypted signals",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zarazaga19_interspeech.html": {
    "title": "Sound Privacy: A Conversational Speech Corpus for Quantifying the Experience of Privacy",
    "volume": "main",
    "abstract": "With the growing popularity of social networks, cloud services and online applications, people are becoming concerned about the way companies store their data and the ways in which the data can be applied. Privacy with devices and services operated by the voice are of particular interest. To enable studies in privacy, this paper presents a database which quantifies the experience of privacy users have in spoken communication. We focus on the effect of the acoustic environment on that perception of privacy. Speech signals are recorded in scenarios simulating real-life situations, where the acoustic environment has an effect on the experience of privacy. The acoustic data is complemented with measures of the speakers' experience of privacy, recorded using a questionnaire. The presented corpus enables studies in how acoustic environments affect peoples' experience of privacy, which in turn, can be used to develop speech operated applications which are respectful of their right to privacy",
    "checked": true,
    "id": "6b5c8a4cd8ff094bae5baae1db9279dc7fa6d3cb",
    "semantic_title": "sound privacy: a conversational speech corpus for quantifying the experience of privacy",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/soto19_interspeech.html": {
    "title": "Improving Code-Switched Language Modeling Performance Using Cognate Features",
    "volume": "main",
    "abstract": "We have found that cognate words, defined as sets of words used in multiple languages that share a common etymology, can in fact elicit code-switching or language mixing between the languages. This paper focuses on how information about cognate words can improve language modeling performance of code-switched English-Spanish (EN-ES) language. We have found that the degree of semantic, phonetic or lexical overlap between a pair of cognate words is a useful feature in identifying code-switching in language. We derive a set of spelling, phonetic and semantic features from a list of of EN-ES cognates and run experiments on a corpus of conversational code-switched EN-ES. First, we show that there exists a strong statistical relationship between these cognate-based features and code-switching in the corpus. Secondly, we demonstrate that language models using these features obtain similar performance improvements as do other manually tagged features including language and part-of-speech tags. We conclude that cognate features can be a useful set of automatically-derived features that can be easily obtained for any pair of languages",
    "checked": true,
    "id": "259a144252e0f535a9bf5721e5c3afc44e85db40",
    "semantic_title": "improving code-switched language modeling performance using cognate features",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lee19d_interspeech.html": {
    "title": "Linguistically Motivated Parallel Data Augmentation for Code-Switch Language Modeling",
    "volume": "main",
    "abstract": "Code-switch language modeling is challenging due to data scarcity as well as expanded vocabulary that involves two languages. We present a novel computational method to generate synthetic code-switch data using the Matrix Language Frame theory to alleviate the issue of data scarcity. The proposed method makes use of augmented parallel data to supplement the real code-switch data. We use the synthetic data to pre-train the language model. We show that the pre-trained language model can match the performance of vanilla models when it is finetuned with 2.5 times less real code-switch data. We also show that the perplexity of a RNN based language model pre-trained on synthetic code-switch data and fine-tuned with real code-switch data is significantly lower than that of the model trained on real code-switch data alone and the reduction in perplexity translates into 1.45% absolute reduction in WER in a speech recognition experiment",
    "checked": true,
    "id": "8b4e05ecc2ba92b299ba9cf125876e6c9dd7b416",
    "semantic_title": "linguistically motivated parallel data augmentation for code-switch language modeling",
    "citation_count": 33
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rallabandi19_interspeech.html": {
    "title": "Variational Attention Using Articulatory Priors for Generating Code Mixed Speech Using Monolingual Corpora",
    "volume": "main",
    "abstract": "Code Mixing — phenomenon where lexical items from one language are embedded in the utterance of another — is relatively frequent in multilingual communities and therefore speech systems should be able to process such content. However, building a voice capable of synthesizing such content typically requires bilingual recordings from the speaker which might not always be easy to obtain. In this work, we present an approach for building mixed lingual systems using only monolingual corpora. Specifically we present a way to train multi speaker text to speech system by incorporating stochastic latent variables into the attention mechanism with the objective of synthesizing code mixed content. We subject the prior distribution for such latent variables to match articulatory constraints. Subjective evaluation shows that our systems are capable of generating high quality synthesis in code mixed scenarios",
    "checked": true,
    "id": "a3dd6af8c693f3bc867bc42b2f6b498cb452df44",
    "semantic_title": "variational attention using articulatory priors for generating code mixed speech using monolingual corpora",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19l_interspeech.html": {
    "title": "Code-Switching Detection Using ASR-Generated Language Posteriors",
    "volume": "main",
    "abstract": "Code-switching (CS) detection refers to the automatic detection of language switches in code-mixed utterances. This task can be achieved by using a CS automatic speech recognition (ASR) system that can handle such language switches. In our previous work, we have investigated the code-switching detection performance of the Frisian-Dutch CS ASR system by using the time alignment of the most likely hypothesis and found that this technique suffers from over-switching due to numerous very short spurious language switches. In this paper, we propose a novel method for CS detection aiming to remedy this shortcoming by using the language posteriors which are the sum of the frame-level posteriors of phones belonging to the same language. The CS ASR-generated language posteriors contain more complete language-specific information on frame level compared to the time alignment of the ASR output. Hence, it is expected to yield more accurate and robust CS detection. The CS detection experiments demonstrate that the proposed language posterior-based approach provides higher detection accuracy than the baseline system in terms of equal error rate. Moreover, a detailed CS detection error analysis reveals that using language posteriors reduces the false alarms and results in more robust CS detection",
    "checked": true,
    "id": "af376ccac42c1e5bc8afdf73c3adc78afa5f9a1f",
    "semantic_title": "code-switching detection using asr-generated language posteriors",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/biswas19b_interspeech.html": {
    "title": "Semi-Supervised Acoustic Model Training for Five-Lingual Code-Switched ASR",
    "volume": "main",
    "abstract": "This paper presents recent progress in the acoustic modelling of under-resourced code-switched (CS) speech in multiple South African languages. We consider two approaches. The first constructs separate bilingual acoustic models corresponding to language pairs (English-isiZulu, English-isiXhosa, English-Setswana and English-Sesotho). The second constructs a single unified five-lingual acoustic model representing all the languages (English, isiZulu, isiXhosa, Setswana and Sesotho). For these two approaches we consider the effectiveness of semi-supervised training to increase the size of the very sparse acoustic training sets. Using approximately 11 hours of untranscribed speech, we show that both approaches benefit from semi-supervised training. The bilingual TDNN-F acoustic models also benefit from the addition of CNN layers (CNN-TDNN-F), while the five-lingual system does not show any significant improvement. Furthermore, because English is common to all language pairs in our data, it dominates when training a unified language model, leading to improved English ASR performance at the expense of the other languages. Nevertheless, the five-lingual model offers flexibility because it can process more than two languages simultaneously, and is therefore an attractive option as an automatic transcription system in a semi-supervised training pipeline",
    "checked": true,
    "id": "844592f3cc1eda0458db75577ef514b7f560b9a6",
    "semantic_title": "semi-supervised acoustic model training for five-lingual code-switched asr",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ylmaz19b_interspeech.html": {
    "title": "Multi-Graph Decoding for Code-Switching ASR",
    "volume": "main",
    "abstract": "In the FAME! Project, a code-switching (CS) automatic speech recognition (ASR) system for Frisian-Dutch speech is developed that can accurately transcribe the local broadcaster's bilingual archives with CS speech. This archive contains recordings with monolingual Frisian and Dutch speech segments as well as Frisian-Dutch CS speech, hence the recognition performance on monolingual segments is also vital for accurate transcriptions. In this work, we propose a multi-graph decoding and rescoring strategy using bilingual and monolingual graphs together with a unified acoustic model for CS ASR. The proposed decoding scheme gives the freedom to design and employ alternative search spaces for each (monolingual or bilingual) recognition task and enables the effective use of monolingual resources of the high-resourced mixed language in low-resourced CS scenarios. In our scenario, Dutch is the high-resourced and Frisian is the low-resourced language. We therefore use additional monolingual Dutch text resources to improve the Dutch language model (LM) and compare the performance of single- and multi-graph CS ASR systems on Dutch segments using larger Dutch LMs. The ASR results show that the proposed approach outperforms baseline single-graph CS ASR systems, providing better performance on the monolingual Dutch segments without any accuracy loss on monolingual Frisian and code-mixed segments",
    "checked": true,
    "id": "3c3b103fc59a8668373207358280f5fbb894722c",
    "semantic_title": "multi-graph decoding for code-switching asr",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/seki19_interspeech.html": {
    "title": "End-to-End Multilingual Multi-Speaker Speech Recognition",
    "volume": "main",
    "abstract": "The expressive power of end-to-end automatic speech recognition (ASR) systems enables direct estimation of a character or word label sequence from a sequence of acoustic features. Direct optimization of the whole system is advantageous because it not only eliminates the internal linkage necessary for hybrid systems, but also extends the scope of potential applications by training the model for various objectives. In this paper, we tackle the challenging task of multilingual multi-speaker ASR using such an all-in-one end-to-end system. Several multilingual ASR systems were recently proposed based on a monolithic neural network architecture without language-dependent modules, showing that modeling of multiple languages is well within the capabilities of an end-to-end framework. There has also been growing interest in multi-speaker speech recognition, which enables generation of multiple label sequences from single-channel mixed speech. In particular, a multi-speaker end-to-end ASR system that can directly model one-to-many mappings without additional auxiliary clues was recently proposed. The proposed model, which integrates the capabilities of these two systems, is evaluated using mixtures of two speakers generated by using 10 languages, including code-switching utterances",
    "checked": true,
    "id": "f9e3b7c6ca7d534694148bd0c7c37c1ef896a784",
    "semantic_title": "end-to-end multilingual multi-speaker speech recognition",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/guasch19_interspeech.html": {
    "title": "Survey Talk: Realistic Physics-Based Computational Voice Production",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "f4be8bfb56dda24825ce92e251611e7e612cbbca",
    "semantic_title": "survey talk: realistic physics-based computational voice production",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mohapatra19_interspeech.html": {
    "title": "An Extended Two-Dimensional Vocal Tract Model for Fast Acoustic Simulation of Single-Axis Symmetric Three-Dimensional Tubes",
    "volume": "main",
    "abstract": "The simulation of two-dimensional (2D) wave propagation is an affordable computational task and its use can potentially improve time performance in vocal tracts' acoustic analysis. Several models have been designed that rely on 2D wave solvers and include 2D representations of three-dimensional (3D) vocal tract-like geometries. However, until now, only the acoustics of straight 3D tubes with circular cross-sections have been successfully replicated with this approach. Furthermore, the simulation of the resulting 2D shapes requires extremely high spatiotemporal resolutions, dramatically reducing the speed boost deriving from the usage of a 2D wave solver. In this paper, we introduce an in-progress novel vocal tract model that extends the 2D Finite-Difference Time-Domain wave solver (2.5D FDTD) by adding tube depth, derived from the area functions, to the acoustic solver. The model combines the speed of a light 2D numerical scheme with the ability to natively simulate 3D tubes that are symmetric in one dimension, hence relaxing previous resolution requirements. An implementation of the 2.5D FDTD is presented, along with evaluation of its performance in the case of static vowel modeling. The paper discusses the current features and limits of the approach, and the potential impact on computational acoustics applications",
    "checked": true,
    "id": "fa0e56550542b408c6d85da9fd7e6fd68964f521",
    "semantic_title": "an extended two-dimensional vocal tract model for fast acoustic simulation of single-axis symmetric three-dimensional tubes",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/birkholz19_interspeech.html": {
    "title": "Perceptual Optimization of an Enhanced Geometric Vocal Fold Model for Articulatory Speech Synthesis",
    "volume": "main",
    "abstract": "We present a geometric vocal fold model that describes the glottal area between the lower and upper vocal fold edges as a function of time. It is based on a glottis model by Titze [J. Acoust. Soc. Am., 75(2), 570–580 (1984)] and has been enhanced to allow the generation of skewed (asymmetric) glottal area waveforms and diplophonic double pulsing. Embedded in the articulatory speech synthesizer VocalTractLab, the model was used for the synthesis of German words with a range of settings for the vocal fold model parameters to generate different male and female voices. A perception experiment was conducted to determine the parameter settings that generate the most natural-sounding voices. The most natural-sounding male voice was generated with a slightly divergent prephonatory glottal shape, with a phase delay of 70° between the lower and upper vocal fold edges, symmetric glottal area pulses, and a little shimmer (double pulsing). The most natural-sounding female voice was generated with a straight prephonatory glottal channel, with a phase delay of 50° between the vocal fold edges, slightly asymmetric glottal area pulses, and a little shimmer",
    "checked": true,
    "id": "1687770f4c4ebb9ef7d04cbaead723f13110e059",
    "semantic_title": "perceptual optimization of an enhanced geometric vocal fold model for articulatory speech synthesis",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gao19e_interspeech.html": {
    "title": "Articulatory Copy Synthesis Based on a Genetic Algorithm",
    "volume": "main",
    "abstract": "This paper describes a novel approach for copy synthesis of human speech with the articulatory speech synthesizer VocalTractLab (VTL). For a given natural utterance, an appropriate gestural score (an organized pattern of articulatory movements) was obtained in two steps: initialization and optimization. In the first step, we employed a rule-based method to create an initial gestural score. In the second step, this initial gestural score was optimized by a genetic algorithm such that the cosine distance of acoustic features between the synthetic and natural utterances was minimized. The optimization was regularized by limiting certain gestural score parameters to reasonable values during the analysis-by-synthesis procedure. The experiment results showed that, compared to a baseline coordinate descent algorithm, the genetic algorithm performed better in terms of acoustic distance. In addition, a perceptual experiment was conducted to rate the similarity between the optimized synthetic speech and the original human speech. Here, similarity scores of optimized utterances with regularization were significantly higher than those without regularization",
    "checked": true,
    "id": "e2b7332b1caafccc758b2e3a44daa8e2ac7a6cca",
    "semantic_title": "articulatory copy synthesis based on a genetic algorithm",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shahrebabaki19_interspeech.html": {
    "title": "A Phonetic-Level Analysis of Different Input Features for Articulatory Inversion",
    "volume": "main",
    "abstract": "The challenge of articulatory inversion is to determine the temporal movement of the articulators from the speech waveform, or from acoustic-phonetic knowledge, e.g. derived from information about the linguistic content of the utterance. The actual position of the articulators is typically obtained from measured data, in our case position measurements obtained using EMA (Electromagnetic articulography). In this paper, we investigate the impact on articulatory inversion problem by using features derived from the acoustic waveform relative to using linguistic features related to the time aligned phone sequence of the utterance. Filterbank energies (FBE) are used as acoustic features, while phoneme identities and (binary) phonetic attributes are used as linguistic features. Experiments are performed on a speech corpus with synchronously recorded EMA measurements and employing a bidirectional long short-term memory (BLSTM) that estimates the articulators' position. Acoustic FBE features performed better for vowel sounds. Phonetic features attained better results for nasal and fricative sounds except for /h/. Further improvements were obtained by combining FBE and linguistic features, which led to an average relative RMSE reduction of 9.8%, and a 3% relative improvement of the Pearson correlation coefficient",
    "checked": true,
    "id": "c9d77e7a4cb151a4a42260573190cde4112e8f39",
    "semantic_title": "a phonetic-level analysis of different input features for articulatory inversion",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tuske19_interspeech.html": {
    "title": "Advancing Sequence-to-Sequence Based Speech Recognition",
    "volume": "main",
    "abstract": "The paper presents our endeavor to improve state-of-the-art speech recognition results using attention based neural network approaches. Our test focus was LibriSpeech, a well-known, publicly available, large, speech corpus, but the methodologies are clearly applicable to other tasks. After systematic application of standard techniques — sophisticated data augmentation, various dropout schemes, scheduled sampling, warm-restart —, and optimizing search configurations, our model achieves 4.0% and 11.7% word error rate (WER) on the test-clean and test-other sets, without any external language model. A powerful recurrent language model drops the error rate further to 2.7% and 8.2%. Thus, we not only report the lowest sequence-to-sequence model based numbers on this task to date, but our single system even challenges the best result known in the literature, namely a hybrid model together with recurrent language model rescoring. A simple ROVER combination of several of our attention based systems achieved 2.5% and 7.3% WER on the clean and other test sets",
    "checked": true,
    "id": "26b0f0de64bc3c240c4439201b0fea45be47dff7",
    "semantic_title": "advancing sequence-to-sequence based speech recognition",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hannun19_interspeech.html": {
    "title": "Sequence-to-Sequence Speech Recognition with Time-Depth Separable Convolutions",
    "volume": "main",
    "abstract": "We propose a fully convolutional sequence-to-sequence encoder architecture with a simple and efficient decoder. Our model improves WER on LibriSpeech while being an order of magnitude more efficient than a strong RNN baseline. Key to our approach is a time-depth separable convolution block which dramatically reduces the number of parameters in the model while keeping the receptive field large. We also give a stable and efficient beam search inference procedure which allows us to effectively integrate a language model. Coupled with a convolutional language model, our time-depth separable convolution architecture improves by more than 22% relative WER over the best previously reported sequence-to-sequence results on the noisy LibriSpeech test set",
    "checked": true,
    "id": "52fc149d40429bcf5090f956b343b779932215ed",
    "semantic_title": "sequence-to-sequence speech recognition with time-depth separable convolutions",
    "citation_count": 80
  },
  "https://www.isca-speech.org/archive/interspeech_2019/baskar19_interspeech.html": {
    "title": "Semi-Supervised Sequence-to-Sequence ASR Using Unpaired Speech and Text",
    "volume": "main",
    "abstract": "Sequence-to-sequence automatic speech recognition (ASR) models require large quantities of data to attain high performance. For this reason, there has been a recent surge in interest for unsupervised and semi-supervised training in such models. This work builds upon recent results showing notable improvements in semi-supervised training using cycle-consistency and related techniques. Such techniques derive training procedures and losses able to leverage unpaired speech and/or text data by combining ASR with Text-to-Speech (TTS) models. In particular, this work proposes a new semi-supervised loss combining an end-to-end differentiable ASR→TTS loss with TTS→ASR loss. The method is able to leverage both unpaired speech and text data to outperform recently proposed related techniques in terms of %WER. We provide extensive results analyzing the impact of data quantity and speech and text modalities and show consistent gains across WSJ and Librispeech corpora. Our code is provided in ESPnet to reproduce the experiments",
    "checked": true,
    "id": "0d7c7984812ad7410fc78b53c6c96bc6c3a7e391",
    "semantic_title": "self-supervised sequence-to-sequence asr using unpaired speech and text",
    "citation_count": 59
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bai19c_interspeech.html": {
    "title": "Learn Spelling from Teachers: Transferring Knowledge from Language Models to Sequence-to-Sequence Speech Recognition",
    "volume": "main",
    "abstract": "Integrating an external language model into a sequence-to-sequence speech recognition system is non-trivial. Previous works utilize linear interpolation or a fusion network to integrate external language models. However, these approaches introduce external components, and increase decoding computation. In this paper, we instead propose a knowledge distillation based training approach to integrating external language models into a sequence-to-sequence model. A recurrent neural network language model, which is trained on large scale external text, generates soft labels to guide the sequence-to-sequence model training. Thus, the language model plays the role of the teacher. This approach does not add any external component to the sequence-to-sequence model during testing. And this approach is flexible to be combined with shallow fusion technique together for decoding. The experiments are conducted on public Chinese datasets AISHELL-1 and CLMAD. Our approach achieves a character error rate of 9.3%, which is relatively reduced by 18.42% compared with the vanilla sequence-to-sequence model",
    "checked": true,
    "id": "aeec7b8ae2b63d0b8ffcce74a81677d48ad2c691",
    "semantic_title": "learn spelling from teachers: transferring knowledge from language models to sequence-to-sequence speech recognition",
    "citation_count": 31
  },
  "https://www.isca-speech.org/archive/interspeech_2019/irie19_interspeech.html": {
    "title": "On the Choice of Modeling Unit for Sequence-to-Sequence Speech Recognition",
    "volume": "main",
    "abstract": "In conventional speech recognition, phoneme-based models outperform grapheme-based models for non-phonetic languages such as English. The performance gap between the two typically reduces as the amount of training data is increased. In this work, we examine the impact of the choice of modeling unit for attention-based encoder-decoder models. We conduct experiments on the LibriSpeech 100hr, 460hr, and 960hr tasks, using various target units (phoneme, grapheme, and word-piece); across all tasks, we find that grapheme or word-piece models consistently outperform phoneme-based models, even though they are evaluated without a lexicon or an external language model. We also investigate model complementarity: we find that we can improve WERs by up to 9% relative by rescoring N-best lists generated from a strong word-piece based baseline with either the phoneme or the grapheme model. Rescoring an N-best list generated by the phonemic system, however, provides limited improvements. Further analysis shows that the word-piece-based models produce more diverse N-best hypotheses, and thus lower oracle WERs, than phonemic models",
    "checked": true,
    "id": "9be8c1b661e4ffa7e3d061c70c7a771313b39a8f",
    "semantic_title": "Model Unit Exploration for Sequence-to-Sequence Speech Recognition",
    "citation_count": 58
  },
  "https://www.isca-speech.org/archive/interspeech_2019/weninger19b_interspeech.html": {
    "title": "Listen, Attend, Spell and Adapt: Speaker Adapted Sequence-to-Sequence ASR",
    "volume": "main",
    "abstract": "Sequence-to-sequence (seq2seq) based ASR systems have shown state-of-the-art performances while having clear advantages in terms of simplicity. However, comparisons are mostly done on speaker independent (SI) ASR systems, though speaker adapted conventional systems are commonly used in practice for improving robustness to speaker and environment variations. In this paper, we apply speaker adaptation to seq2seq models with the goal of matching the performance of conventional ASR adaptation. Specifically, we investigate Kullback-Leibler divergence (KLD) as well as Linear Hidden Network (LHN) based adaptation for seq2seq ASR, using different amounts (up to 20 hours) of adaptation data per speaker. Our SI models are trained on large amounts of dictation data and achieve state-of-the-art results. We obtained 25% relative word error rate (WER) improvement with KLD adaptation of the seq2seq model vs. 18.7% gain from acoustic model adaptation in the conventional system. We also show that the WER of the seq2seq model decreases log-linearly with the amount of adaptation data. Finally, we analyze adaptation based on the minimum WER criterion and adapting the language model (LM) for score fusion with the speaker adapted seq2seq model, which result in further improvements of the seq2seq system performance",
    "checked": true,
    "id": "e90bd23a26288cb030844894fd6794ac9012b181",
    "semantic_title": "listen, attend, spell and adapt: speaker adapted sequence-to-sequence asr",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2019/runarsdottir19_interspeech.html": {
    "title": "Lattice Re-Scoring During Manual Editing for Automatic Error Correction of ASR Transcripts",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) systems are increasingly used to transcribe text for publication or official uses. However, even the best ASR systems make mistakes that can change the meaning of the recognition results. The results from these systems are therefore often reviewed by human editors, who fix the errors that arise. Offering automatic updates of utterances, with lattice re-scoring, could decrease the manual labor needed to fix errors from these systems. The research presented in this paper is conducted within an ASR-based transcription system with human post-editing for the Icelandic parliament, Althingi, and aims to automatically correct down-stream errors once the first error of a sentence has been manually corrected. After manually correcting the first error of the utterances, a new path is computed through the correction, using the lattice created during the ASR decoding process. With re-scoring, the sentence error rate (SER) for utterances containing two errors (and hence with SER=100%) drops to 82.77% and for utterances containing three errors drops to 95.88%. This paper demonstrates that the trade-off between automatically fixed errors and new errors introduced in the re-scoring heavily favours adding this process to the transcription system",
    "checked": true,
    "id": "2ed4b5a8ab59b875610f813a0db2f60dbd279a0c",
    "semantic_title": "lattice re-scoring during manual editing for automatic error correction of asr transcripts",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fukunaga19_interspeech.html": {
    "title": "GPU-Based WFST Decoding with Extra Large Language Model",
    "volume": "main",
    "abstract": "Weighted finite-state transducer (WFST) decoding in speech recognition can be accelerated by using graphics processing units (GPUs). To obtain a high recognition accuracy in a WFST-based speech recognition system, a very large language model (LM) represented as a WFST with more than 10 GB of data is required. Since a GPU typically has only several GB of memory, it is impossible to store such a large LM in GPU memory. In this paper, we propose a new method for WFST decoding on a GPU. The method utilizes the on-the-fly rescoring algorithm, which performs the Viterbi search on a WFST with a small LM and rescores hypotheses using a large LM during decoding. We solve the problem of insufficient GPU memory by storing most of the large LM in a memory on the host and copying the data from the host memory to the GPU memory as needed during runtime. Our evaluation of the proposed method on the LibriSpeech test sets using an NVIDIA Tesla V100 GPU shows that it achieves a ten times faster decoding than an equivalent CPU implementation without recognition accuracy degradation",
    "checked": true,
    "id": "8124aa94db0e4eb5bc7d2f21ce7a9d293bc8e353",
    "semantic_title": "gpu-based wfst decoding with extra large language model",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jorge19_interspeech.html": {
    "title": "Real-Time One-Pass Decoder for Speech Recognition Using LSTM Language Models",
    "volume": "main",
    "abstract": "Recurrent Neural Networks, in particular Long-Short Term Memory (LSTM) networks, are widely used in Automatic Speech Recognition for language modelling during decoding, usually as a mechanism for rescoring hypothesis. This paper proposes a new architecture to perform real-time one-pass decoding using LSTM language models. To make decoding efficient, the estimation of look-ahead scores was accelerated by precomputing static look-ahead tables. These static tables were precomputed from a pruned n-gram model, reducing drastically the computational cost during decoding. Additionally, the LSTM language model evaluation was efficiently performed using Variance Regularization along with a strategy of lazy evaluation. The proposed one-pass decoder architecture was evaluated on the well-known LibriSpeech and TED-LIUMv3 datasets. Results showed that the proposed algorithm obtains very competitive WERs with ~0.6 RTFs. Finally, our one-pass decoder is compared with a decoupled two-pass decoder",
    "checked": true,
    "id": "d4c47ad1cc9889cdd1b19d7273cb0e66b18e3b28",
    "semantic_title": "real-time one-pass decoder for speech recognition using lstm language models",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2019/seki19b_interspeech.html": {
    "title": "Vectorized Beam Search for CTC-Attention-Based Speech Recognition",
    "volume": "main",
    "abstract": "This paper investigates efficient beam search techniques for end-to-end automatic speech recognition (ASR) with attention-based encoder-decoder architecture. We accelerate the decoding process by vectorizing multiple hypotheses during the beam search, where we replace the score accumulation steps for each hypothesis with vector-matrix operations for the vectorized hypotheses. This modification allows us to take advantage of the parallel computing capabilities of multi-core CPUs and GPUs, resulting in significant speedups and also enabling us to process multiple utterances in a batch simultaneously. Moreover, we extend the decoding method to incorporate a recurrent neural network language model (RNNLM) and connectionist temporal classification (CTC) scores, which typically improve ASR accuracy but have not been investigated for the use of such parallelized decoding algorithms. Experiments with LibriSpeech and Corpus of Spontaneous Japanese datasets have demonstrated that the vectorized beam search achieves 1.8× speedup on a CPU and 33× speedup on a GPU compared with the original CPU implementation. When using joint CTC/attention decoding with an RNNLM, we also achieved 11× speedup on a GPU while maintaining the benefits of CTC and RNNLM. With these benefits, we achieved almost real-time processing with a small latency of 0.1× real-time without streaming search process",
    "checked": true,
    "id": "b20cadef0c59e80f7dfdf825b07442619d920fd5",
    "semantic_title": "vectorized beam search for ctc-attention-based speech recognition",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2019/serrino19_interspeech.html": {
    "title": "Contextual Recovery of Out-of-Lattice Named Entities in Automatic Speech Recognition",
    "volume": "main",
    "abstract": "As voice-driven intelligent assistants become commonplace, adaptation to user context becomes critical for Automatic Speech Recognition (ASR) systems. For example, ASR systems may be expected to recognize a user's contact names containing improbable or out-of-vocabulary (OOV) words We introduce a method to identify contextual cues in a first-pass ASR system's output and to recover out-of-lattice hypotheses that are contextually relevant. Our proposed module is agnostic to the architecture of the underlying recognizer, provided it generates a word lattice of hypotheses; it is sufficiently compact for use on device. The module identifies subgraphs in the lattice likely to contain named entities (NEs), recovers phoneme hypotheses over corresponding time spans, and inserts NEs that are phonetically close to those hypotheses. We measure a decrease in the mean word error rate (WER) of word lattices from 11.5% to 4.9% on a test set of NEs",
    "checked": true,
    "id": "ef0781c5fd594b388b978195d630787f179cda1e",
    "semantic_title": "contextual recovery of out-of-lattice named entities in automatic speech recognition",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/novitasari19_interspeech.html": {
    "title": "Sequence-to-Sequence Learning via Attention Transfer for Incremental Speech Recognition",
    "volume": "main",
    "abstract": "Attention-based sequence-to-sequence automatic speech recognition (ASR) requires a significant delay to recognize long utterances because the output is generated after receiving entire input sequences. Although several studies recently proposed sequence mechanisms for incremental speech recognition (ISR), using different frameworks and learning algorithms is more complicated than the standard ASR model. One main reason is because the model needs to decide the incremental steps and learn the transcription that aligns with the current short speech segment. In this work, we investigate whether it is possible to employ the original architecture of attention-based ASR for ISR tasks by treating a full-utterance ASR as the teacher model and the ISR as the student model. We design an alternative student network that, instead of using a thinner or a shallower model, keeps the original architecture of the teacher model but with shorter sequences (few encoder and decoder states). Using attention transfer, the student network learns to mimic the same alignment between the current input short speech segments and the transcription. Our experiments show that by delaying the starting time of recognition process with about 1.7 sec, we can achieve comparable performance to one that needs to wait until the end",
    "checked": true,
    "id": "ba5f82993335a1e4a7313ef9000a0dc60a4e43f1",
    "semantic_title": "sequence-to-sequence learning via attention transfer for incremental speech recognition",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lian19b_interspeech.html": {
    "title": "Unsupervised Representation Learning with Future Observation Prediction for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Prior works on speech emotion recognition utilize various unsupervised learning approaches to deal with low-resource samples. However, these methods pay less attention to modeling the long-term dynamic dependency, which is important for speech emotion recognition. To deal with this problem, this paper combines the unsupervised representation learning strategy — Future Observation Prediction (FOP), with transfer learning approaches (such as Fine-tuning and Hypercolumns). To verify the effectiveness of the proposed method, we conduct experiments on the IEMOCAP database. Experimental results demonstrate that our method is superior to currently advanced unsupervised learning strategies",
    "checked": true,
    "id": "34e2a89db8ec53ef2f6682af8337bd265324da4c",
    "semantic_title": "unsupervised representation learning with future observation prediction for speech emotion recognition",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/phan19_interspeech.html": {
    "title": "Spatio-Temporal Attention Pooling for Audio Scene Classification",
    "volume": "main",
    "abstract": "Acoustic scenes are rich and redundant in their content. In this work, we present a spatio-temporal attention pooling layer coupled with a convolutional recurrent neural network to learn from patterns that are discriminative while suppressing those that are irrelevant for acoustic scene classification. The convolutional layers in this network learn invariant features from time-frequency input. The bidirectional recurrent layers are then able to encode the temporal dynamics of the resulting convolutional features. Afterwards, a two-dimensional attention mask is formed via the outer product of the spatial and temporal attention vectors learned from two designated attention layers to weigh and pool the recurrent output into a final feature vector for classification. The network is trained with between-class examples generated from between-class data augmentation. Experiments demonstrate that the proposed method not only outperforms a strong convolutional neural network baseline but also sets new state-of-the-art performance on the LITIS Rouen dataset",
    "checked": true,
    "id": "880c4487ce52a7d502b611dc04b89e2aa319ddda",
    "semantic_title": "spatio-temporal attention pooling for audio scene classification",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shi19d_interspeech.html": {
    "title": "Subspace Pooling Based Temporal Features Extraction for Audio Event Recognition",
    "volume": "main",
    "abstract": "Currently, most popular methods of Audio Event Recognition (AER) firstly split audio event signals into multiple short segments, then the features of these segments are pooled for recognition. However, the temporal features between segments, which highly affect the semantic representation of signals, are usually discarded in the above pooling step. Thus, how to introduce the temporal features to the pooling step requires further investigation. Unfortunately, on the one hand, only a few studies have been conducted towards solving this problem so far. On the other hand, the effective temporal features should not only capture the temporal dynamics but also have the signal reconstruction ability, while most of the above studies mainly focus on the former but ignore the latter. In addition, the effective features of high-dimensional original signals usually inhabit a low-dimensional subspace. Therefore, we propose two novel pooling based methods which try to consider both the temporal dynamics and signal reconstruction ability of temporal features in the low-dimensional subspace. The proposed methods are evaluated on the AudioEvent database, and experimental results show that our methods can outperform most of the typical methods",
    "checked": true,
    "id": "65e9031647c1b454b06325ab41950ce8d80fc24a",
    "semantic_title": "subspace pooling based temporal features extraction for audio event recognition",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19l_interspeech.html": {
    "title": "Multi-Scale Time-Frequency Attention for Acoustic Event Detection",
    "volume": "main",
    "abstract": "Most attention-based methods only concentrate along the time axis, which is insufficient for Acoustic Event Detection (AED). Meanwhile, previous methods for AED rarely considered that target events possess distinct temporal and frequential scales. In this work, we propose a Multi-Scale Time-Frequency Attention (MTFA) module for AED. MTFA gathers information at multiple resolutions to generate a time-frequency attention mask which tells the model where to focus along both time and frequency axis. With MTFA, the model could capture the characteristics of target events with different scales. We demonstrate the proposed method on Task 2 of Detection and Classification of Acoustic Scenes and Events (DCASE) 2017 Challenge. Our method achieves competitive results on both development dataset and evaluation dataset",
    "checked": true,
    "id": "a168fca2cbf0b90428fd4787664260e30e027c2d",
    "semantic_title": "multi-scale time-frequency attention for acoustic event detection",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/song19b_interspeech.html": {
    "title": "Acoustic Scene Classification by Implicitly Identifying Distinct Sound Events",
    "volume": "main",
    "abstract": "In this paper, we propose a new strategy for acoustic scene classification (ASC) , namely recognizing acoustic scenes through identifying distinct sound events. This differs from existing strategies, which focus on characterizing global acoustical distributions of audio or the temporal evolution of short-term audio features, without analysis down to the level of sound events. To identify distinct sound events for each scene, we formulate ASC in a multi-instance learning (MIL) framework, where each audio recording is mapped into a bag-of-instances representation. Here, instances can be seen as high-level representations for sound events inside a scene. We also propose a MIL neural networks model, which implicitly identifies distinct instances (i.e., sound events). Furthermore, we propose two specially designed modules that model the multi-temporal scale and multi-modal natures of the sound events respectively. The experiments were conducted on the official development set of the DCASE2018 Task1 Subtask B, and our best-performing model improves over the official baseline by 9.4% (68.3% vs 58.9%) in terms of classification accuracy. This study indicates that recognizing acoustic scenes by identifying distinct sound events is effective and paves the way for future studies that combine this strategy with previous ones",
    "checked": true,
    "id": "072b22336a6b24d471c05c2aa5134332765810ab",
    "semantic_title": "acoustic scene classification by implicitly identifying distinct sound events",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2019/qi19_interspeech.html": {
    "title": "Parameter-Transfer Learning for Low-Resource Individualization of Head-Related Transfer Functions",
    "volume": "main",
    "abstract": "Individualized head-related transfer functions (HRTFs) play an important role in accurate localization perception. However, it is a great challenge to efficiently measure continuous HRTFs for each person in full space. In this paper, we propose a parameter-transfer learning method termed PTL to obtain individualized HRTFs based on a small set of HRTF measurements. The key idea behind PTL is to transfer a HRTF generation model from other database to a target individual. To this end, PTL first pretrains a deep neural network (DNN)-based universal model on a large database of HRTFs with the assist of domain knowledge. Domain knowledge is used to generate the input features derived from the solution to sound wave propagation equation at the physical level, and to design the loss function based on the knowledge of objective evaluation criterion. Then, the universal model is transferred to a target individual by adapting the parameters of a hidden layer of DNN with a small set of HRTF measurements. The adaptation layer is determined by experimental verification. We also conduct the objective and subjective experiments, and the results show that the proposed method outperforms the state-of-the-arts methods in terms of LSD and localization accuracy",
    "checked": true,
    "id": "5885ac73f2ff7f86f0ea6f5fc12f50fa0f939b41",
    "semantic_title": "parameter-transfer learning for low-resource individualization of head-related transfer functions",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19i_interspeech.html": {
    "title": "Prosodic Characteristics of Mandarin Declarative and Interrogative Utterances in Parkinson's Disease",
    "volume": "main",
    "abstract": "This work investigated the prosodic characteristics of declarative and interrogative utterances produced by speakers with Parkinson's disease (PD), in comparison to healthy controls (HC). Forty native speakers of Mandarin, including 20 PDs and 20 age-matched HCs, recorded 32 utterances varying in sentence type, sentence length, and sentence-final tone. SS-ANOVA was used to show the F0 contours and the global and final-syllable F0 level, F0 slope, speech rate, and intensity ratio were statistically analyzed using linear mixed-effects models. For the HC group, interrogative utterances showed a significantly higher mean F0 than declarative utterances. The PD group exhibited no significant F0 difference between declarative and interrogative utterances, coinciding with our subjective impression on PD's monotonous voice of tone. This suggests that PD's ability to control fundamental frequency degraded in comparison to HC. Also, the PD group produced significantly faster speech, especially final syllable, than the HC group, suggesting that PD's articulatory control degraded at the end of an utterance",
    "checked": true,
    "id": "dc3a3ad29b5e5c743ef37fc4f0861f6fd24f344e",
    "semantic_title": "prosodic characteristics of mandarin declarative and interrogative utterances in parkinson's disease",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/morovelazquez19_interspeech.html": {
    "title": "Study of the Performance of Automatic Speech Recognition Systems in Speakers with Parkinson's Disease",
    "volume": "main",
    "abstract": "Parkinson's Disease (PD) affects motor capabilities of patients, who in some cases need to use human-computer assistive technologies to regain independence. The objective of this work is to study in detail the differences in error patterns from state-of-the-art Automatic Speech Recognition (ASR) systems on speech from people with and without PD. Two different speech recognizers (attention-based end-to-end and Deep Neural Network - Hidden Markov Models hybrid systems) were trained on a Spanish language corpus and subsequently tested on speech from 43 speakers with PD and 46 without PD. The differences related to error rates, substitutions, insertions and deletions of characters and phonetic units between the two groups were analyzed, showing that the word error rate is 27% higher in speakers with PD than in control speakers, with a moderated correlation between that rate and the developmental stage of the disease. The errors were related to all manner classes, and were more pronounced in the vowel /u/. This study is the first to evaluate ASR systems' responses to speech from patients at different stages of PD in Spanish. The analyses showed general trends but individual speech deficits must be studied in the future when designing new ASR systems for this population",
    "checked": true,
    "id": "4ab7b65e1a3b76eb3db064523c862f1325e04971",
    "semantic_title": "study of the performance of automatic speech recognition systems in speakers with parkinson's disease",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19m_interspeech.html": {
    "title": "Towards the Speech Features of Mild Cognitive Impairment: Universal Evidence from Structured and Unstructured Connected Speech of Chinese",
    "volume": "main",
    "abstract": "Language impairment is a sensitive biomarker for the detection of cognitive decline associated with mild cognitive impairment (MCI). Recently, knowledge about distinctive linguistic features identifying language deficits in MCI has progressively been enriched and accumulated. However, the employment of a single speech task to elicit connected speech (e.g., structured vs. spontaneous conversations) might limit the generalization of salient linguistic features associated with MCI. Not to mention the scarcity of reports on analysis of extended speech of Chinese. The present study aimed to examine if connected speech production in both situational picture description and spontaneous self-introduction tasks could be used to distinguish individuals with psychometric evidence of MCI and those who were cognitively intact. Speech samples produced by 75 elderly native speakers of Mandarin Chinese, including 19 with MCI and 56 healthy controls were obtained. Macrostructural aspects of language, including lexico-semantic, syntactic, speech fluency, and acoustics were analyzed by applying the linear mixed-effect regression model. Our study revealed decreasing linear trends in semantic contents and syntactic complexity, as well as significantly greater signs of disfluency and reduced speech production in participants with MCI. The findings extended what was reported in the literature, and carry important implications to the screening and diagnosis of suspected MCI",
    "checked": true,
    "id": "6a5c9c5f14519801cb17b0511a687892776576a0",
    "semantic_title": "towards the speech features of mild cognitive impairment: universal evidence from structured and unstructured connected speech of chinese",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19n_interspeech.html": {
    "title": "Child Speech Disorder Detection with Siamese Recurrent Network Using Speech Attribute Features",
    "volume": "main",
    "abstract": "Acoustics-based automatic assessment is a highly desirable approach to detecting speech sound disorder (SSD) in children. The performance of an automatic speech assessment system depends greatly on the availability of a good amount of properly annotated disordered speech, which is a critical problem particularly for child speech. This paper presents a novel design of child speech disorder detection system that requires only normal speech for model training. The system is based on a Siamese recurrent network, which is trained to learn the similarity and discrepancy of pronunciations between a pair of phones in the embedding space. For detection of speech sound disorder, the trained network measures a distance that contrasts the test phone to the desired phone and the distance is used to train a binary classifier. Speech attribute features are incorporated to measure the pronunciation quality and provide diagnostic feedback. Experimental results show that Siamese recurrent network with a combination of speech attribute features and phone posterior features could attain an optimal detection accuracy of 0.941",
    "checked": true,
    "id": "f6f513652139cf9160457e0c33fb0cc2486db408",
    "semantic_title": "child speech disorder detection with siamese recurrent network using speech attribute features",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2019/korzekwa19_interspeech.html": {
    "title": "Interpretable Deep Learning Model for the Detection and Reconstruction of Dysarthric Speech",
    "volume": "main",
    "abstract": "We present a novel deep learning model for the detection and reconstruction of dysarthric speech. We train the model with a multi-task learning technique to jointly solve dysarthria detection and speech reconstruction tasks. The model key feature is a low-dimensional latent space that is meant to encode the properties of dysarthric speech. It is commonly believed that neural networks are black boxes that solve problems but do not provide interpretable outputs. On the contrary, we show that this latent space successfully encodes interpretable characteristics of dysarthria, is effective at detecting dysarthria, and that manipulation of the latent space allows the model to reconstruct healthy speech from dysarthric speech. This work can help patients and speech pathologists to improve their understanding of the condition, lead to more accurate diagnoses and aid in reconstructing healthy speech for afflicted patients",
    "checked": true,
    "id": "54410da49e62026932edf3559a21b60d946c56f0",
    "semantic_title": "interpretable deep learning model for the detection and reconstruction of dysarthric speech",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/noufi19_interspeech.html": {
    "title": "Vocal Biomarker Assessment Following Pediatric Traumatic Brain Injury: A Retrospective Cohort Study",
    "volume": "main",
    "abstract": "Recommendations following pediatric traumatic brain injury (TBI) support the integration of instrumental measurement to aid perceptual assessment in recovery and treatment plans. A comprehensive set of sensitive, robust and non-invasive measurements is therefore essential in assessing variations in speech characteristics over time following pediatric TBI. In this paper, we discuss a method for measuring changes in the speech patterns of a pediatric cohort of ten subjects diagnosed with severe TBI. We apply a diverse set of both well-known and novel feature measurements to child speech recorded throughout the year following diagnosis. We analyze these features individually and by speech subsystem for each subject as well as for the entire cohort. In children older than 72 months, we find highly significant (p < 0.01) increases in pitch variation and number of unique phonemes spoken, shortened pause length, and steadying articulation rate variability. Younger children exhibit similar steadied rate variability alongside an increase in articulation complexity. Nearly all speech features significantly change (p < 0.05) for the cohort as a whole, confirming that acoustic measures expanding upon perceptual assessment are needed to identify efficacious treatment targets for speech therapy following TBI",
    "checked": true,
    "id": "466203dae1f63f5f3e640107a08e10866c262725",
    "semantic_title": "vocal biomarker assessment following pediatric traumatic brain injury: a retrospective cohort study",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/scharenborg19b_interspeech.html": {
    "title": "Survey Talk: Reaching Over the Gap: Cross- and Interdisciplinary Research on Human and Automatic Speech Processing",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "298b89c6834081397a580a4db70bea9f2ade90ac",
    "semantic_title": "survey talk: reaching over the gap: cross- and interdisciplinary research on human and automatic speech processing",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ogawa19_interspeech.html": {
    "title": "Improved Deep Duel Model for Rescoring N-Best Speech Recognition List Using Backward LSTMLM and Ensemble Encoders",
    "volume": "main",
    "abstract": "We have proposed a neural network (NN) model called a deep duel model (DDM) for rescoring N-best speech recognition hypothesis lists. A DDM is composed of a long short-term memory (LSTM)-based encoder followed by a fully-connected linear layer-based binary-class classifier. Given the feature vector sequences of two hypotheses in an N-best list, the DDM encodes the features and selects the hypothesis that has the lower word error rate (WER) based on the output binary-class probabilities. By repeating this one-on-one hypothesis comparison (duel) for each hypothesis pair in the N-best list, we can find the oracle (lowest WER) hypothesis as the survivor of the duels. We showed that the DDM can exploit the score provided by a forward LSTM-based recurrent NN language model (LSTMLM) as an additional feature to accurately select the hypotheses. In this study, we further improve the selection performance by introducing two modifications, i.e. adding the score provided by a backward LSTMLM, which uses succeeding words to predict the current word, and employing ensemble encoders, which have a high feature encoding capability. By combining these two modifications, our DDM achieves an over 10% relative WER reduction from a strong baseline obtained using both the forward and backward LSTMLMs",
    "checked": true,
    "id": "129090c1463fb17168316e5f742e8b1bba1c2933",
    "semantic_title": "improved deep duel model for rescoring n-best speech recognition list using backward lstmlm and ensemble encoders",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/irie19b_interspeech.html": {
    "title": "Language Modeling with Deep Transformers",
    "volume": "main",
    "abstract": "We explore deep autoregressive Transformer models in language modeling for speech recognition. We focus on two aspects. First, we revisit Transformer model configurations specifically for language modeling. We show that well configured Transformer models outperform our baseline models based on the shallow stack of LSTM recurrent neural network layers. We carry out experiments on the open-source LibriSpeech 960hr task, for both 200K vocabulary word-level and 10K byte-pair encoding subword-level language modeling. We apply our word-level models to conventional hybrid speech recognition by lattice rescoring, and the subword-level models to attention based encoder-decoder models by shallow fusion. Second, we show that deep Transformer language models do not require positional encoding. The positional encoding is an essential augmentation for the self-attention mechanism which is invariant to sequence ordering. However, in autoregressive setup, as is the case for language modeling, the amount of information increases along the position dimension, which is a positional signal by its own. The analysis of attention weights shows that deep autoregressive self-attention models can automatically make use of such positional information. We find that removing the positional encoding even slightly improves the performance of these models",
    "checked": true,
    "id": "3928b2177086532775fbf607ae3e05a0375a5061",
    "semantic_title": "language modeling with deep transformers",
    "citation_count": 133
  },
  "https://www.isca-speech.org/archive/interspeech_2019/raju19_interspeech.html": {
    "title": "Scalable Multi Corpora Neural Language Models for ASR",
    "volume": "main",
    "abstract": "Neural language models (NLM) have been shown to outperform conventional n-gram language models by a substantial margin in Automatic Speech Recognition (ASR) and other tasks. There are, however, a number of challenges that need to be addressed for an NLM to be used in a practical large-scale ASR system. In this paper, we present solutions to some of the challenges, including training NLM from heterogenous corpora, limiting latency impact and handling personalized bias in the second-pass rescorer. Overall, we show that we can achieve a 6.2% relative WER reduction using neural LM in a second-pass n-best rescoring framework with a minimal increase in latency",
    "checked": true,
    "id": "8dea407aab8dd9bbd664ad1f471f7c9b5fd17980",
    "semantic_title": "scalable multi corpora neural language models for asr",
    "citation_count": 25
  },
  "https://www.isca-speech.org/archive/interspeech_2019/likhomanenko19_interspeech.html": {
    "title": "Who Needs Words? Lexicon-Free Speech Recognition",
    "volume": "main",
    "abstract": "Lexicon-free speech recognition naturally deals with the problem of out-of-vocabulary (OOV) words. In this paper, we show that character-based language models (LM) can perform as well as word-based LMs for speech recognition, in word error rates (WER), even without restricting the decoding to a lexicon. We study character-based LMs and show that convolutional LMs can effectively leverage large (character) contexts, which is key for good speech recognition performance downstream. We specifically show that the lexicon-free decoding performance (WER) on utterances with OOV words using character-based LMs is better than lexicon-based decoding, both with character or word-based LMs",
    "checked": true,
    "id": "6249b4b1f2886bf79690b4d9d419ddca87d390ac",
    "semantic_title": "who needs words? lexicon-free speech recognition",
    "citation_count": 28
  },
  "https://www.isca-speech.org/archive/interspeech_2019/latif19_interspeech.html": {
    "title": "Direct Modelling of Speech Emotion from Raw Speech",
    "volume": "main",
    "abstract": "Speech emotion recognition is a challenging task and heavily depends on hand-engineered acoustic features, which are typically crafted to echo human perception of speech signals. However, a filter bank that is designed from perceptual evidence is not always guaranteed to be the best in a statistical modelling framework where the end goal is for example emotion classification. This has fuelled the emerging trend of learning representations from raw speech especially using deep learning neural networks. In particular, a combination of Convolution Neural Networks (CNNs) and Long Short Term Memory (LSTM) have gained great traction for the intrinsic property of LSTM in learning contextual information crucial for emotion recognition; and CNNs been used for its ability to overcome the scalability problem of regular neural networks. In this paper, we show that there are still opportunities to improve the performance of emotion recognition from the raw speech by exploiting the properties of CNN in modelling contextual information. We propose the use of parallel convolutional layers to harness multiple temporal resolutions in the feature extraction block that is jointly trained with the LSTM based classification network for the emotion recognition task. Our results suggest that the proposed model can reach the performance of CNN trained with hand-engineered features from both IEMOCAP and MSP-IMPROV datasets",
    "checked": true,
    "id": "79782efb91aa5abe89fed495252ee7f2657561f9",
    "semantic_title": "direct modelling of speech emotion from raw speech",
    "citation_count": 83
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sarma19_interspeech.html": {
    "title": "Improving Emotion Identification Using Phone Posteriors in Raw Speech Waveform Based DNN",
    "volume": "main",
    "abstract": "We propose to exploit phone posteriors as an additional feature in Deep Neural Network (DNN) to recognize emotions from raw speech waveform. The proposed DNN setup uses a time domain approach of learning filters within the network. The frame-level phone posteriors are combined with the learned feature representation through the network. Appended learned time domain features and phone posteriors are used as an input to the temporal context modeling layers which interleaves TDNN-LSTM with time-restricted self-attention. We achieve 16.48% relative error rate improvement in IEMOCAP categorical problem (with a final weighted accuracy of 75.03%) using phone posteriors compared to DNN setup which uses only learned time domain features for temporal context modeling. Further, we study the effect of learning emotion categories leveraging dimensional primitives in multi-task learning DNN model",
    "checked": true,
    "id": "d8d6ce03b270fbd5e31f9a8fff7af5b39ad5902e",
    "semantic_title": "improving emotion identification using phone posteriors in raw speech waveform based dnn",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cao19_interspeech.html": {
    "title": "Pyramid Memory Block and Timestep Attention for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "As a sequence model, Deep Feedforward Sequential Memory Network (DFSMN) has shown superior performance on many tasks, such as language modeling and speech recognition. Based on this work, we propose an improved speech emotion recognition (SER) end-to-end system. Our model comprises both CNN layers and pyramid FSMN layers, where CNN layers are added at the front of the network to extract more sophisticated features. A timestep attention mechanism is also integrated into our SER system, which makes the system learn how to focus on the more robust or informative segments in the input signal. Furthermore, different from traditional SER systems, the proposed model is applied directly to spectrograms which contain more raw speech information, rather than well-established hand-crafted speech features such as spectral, cepstral and pitch. Finally, we evaluate our system on the Interactive Emotional Motion Capture (IEMOCAP) database. The experimental results show that our system achieves 2.67% improvement compared to the commonly used CNN-biLSTM model which requires much more computing resource",
    "checked": true,
    "id": "fbc0395c40444027d6f8985327e51512f842dd8a",
    "semantic_title": "pyramid memory block and timestep attention for speech emotion recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/oates19_interspeech.html": {
    "title": "Robust Speech Emotion Recognition Under Different Encoding Conditions",
    "volume": "main",
    "abstract": "In an era where large speech corpora annotated for emotion are hard to come by, and especially ones where emotion is expressed freely instead of being acted, the importance of using free online sources for collecting such data cannot be overstated. Most of those sources, however, contain encoded audio due to storage and bandwidth constraints, often in very low bitrates. In addition, with the increased industry interest on voice-based applications, it is inevitable that speech emotion recognition (SER) algorithms will soon find their way into production environments, where the audio might be encoded in a different bitrate than the one available during training. Our contribution is threefold. First, we show that encoded audio still contains enough relevant information for robust SER. Next, we investigate the effects of mismatched encoding conditions in the training and test set both for traditional machine learning algorithms built on hand-crafted features and modern end-to-end methods. Finally, we investigate the robustness of those algorithms in the multi-condition scenario, where the training set is augmented with encoded audio, but still differs from the training set. Our results indicate that end-to-end methods are more robust even in the more challenging scenario of mismatched conditions",
    "checked": true,
    "id": "270b40579e0154f51d28f1d9b2e3dd6cafe958de",
    "semantic_title": "robust speech emotion recognition under different encoding conditions",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gosztolya19c_interspeech.html": {
    "title": "Using the Bag-of-Audio-Word Feature Representation of ASR DNN Posteriors for Paralinguistic Classification",
    "volume": "main",
    "abstract": "The Bag-of-Audio-Word (or BoAW) representation is an utterance-level feature representation approach that was successfully applied in the past in various computational paralinguistic tasks. Here, we extend the BoAW feature extraction process with the use of Deep Neural Networks: first we train a DNN acoustic model on an acoustic dataset consisting of 22 hours of speech for phoneme identification, then we evaluate this DNN on a standard paralinguistic dataset. To construct utterance-level features from the frame-level posterior vectors, we calculate their BoAW representation. We found that this approach can be utilized even on its own, although the results obtained lag behind those of the standard paralinguistic approach, and the optimal size of the extracted feature vectors tends to be large. Our approach, however, can be easily and efficiently combined with the standard paralinguistic one, resulting in the highest Unweighted Average Recall (UAR) score achieved so far for a general paralinguistic dataset",
    "checked": true,
    "id": "333e7bae168b4f7f31f556293577da4e3f2fc757",
    "semantic_title": "using the bag-of-audio-word feature representation of asr dnn posteriors for paralinguistic classification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/williams19c_interspeech.html": {
    "title": "Disentangling Style Factors from Speaker Representations",
    "volume": "main",
    "abstract": "Our goal is to separate out speaking style from speaker identity in utterance-level representations of speech such as i-vectors and x-vectors. We first show that both i-vectors and x-vectors contain information not only about speaker but also about speaking style (for one data set) or emotion (for another data set), even when projected into a low-dimensional space. To disentangle these factors, we use an autoencoder in which the latent space is split into two subspaces. The entangled information about speaker and style/emotion is pushed apart by the use of auxiliary classifiers that take one of the two latent subspaces as input and that are jointly learned with the autoencoder. We evaluate how well the latent subspaces separate the factors by using them as input to separate style/emotion classification tasks. In traditional speaker identification tasks, speaker-invariant characteristics are factorized from channel and then the channel information is ignored. Our results suggest that this so-called channel may contain exploitable information, which we refer to as style factors. Finally, we propose future work to use information theory to formalize style factors in the context of speaker identity",
    "checked": true,
    "id": "57e41198769e171a6863c737093b010bd912dcd3",
    "semantic_title": "disentangling style factors from speaker representations",
    "citation_count": 31
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hsu19c_interspeech.html": {
    "title": "Sentence Prosody and Wh-Indeterminates in Taiwan Mandarin",
    "volume": "main",
    "abstract": "We report results of a speech production experiment about the intonation of three sentence types in Taiwan Mandarin, and discuss our results with implications for focus acoustics, and semantic-syntactic theories of sentence final particles and wh-indeterminates. Wh-indeterminates refer to wh-phrases that are ambiguous between interrogative and indefinite readings. In Mandarin, different interpretations of wh-indeterminates are not morphologically marked, but can be disambiguated in specific sentence contexts marked by sentence final particles. In this study, we systematically examined the intonation of wh-questions and yes/no questions by using declarative sentences as the baseline. The results show that both wh- and yes/no questions exhibit F0 prominence, and lengthening effects on regions containing sentence-final particles and wh-phrases, but the effects were stronger in wh-questions. Examining the duration and F0 range, we found that wh-phrases and sentence final particles together formed specific acoustic patterns to distinguish questions from declarative sentences. The findings suggest that the prosodic organization interacts with other internal structural organization",
    "checked": true,
    "id": "c05e183340f09c823e6d7c2696d35a30eca66162",
    "semantic_title": "sentence prosody and wh-indeterminates in taiwan mandarin",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hu19d_interspeech.html": {
    "title": "Frication as a Vowel Feature? — Evidence from the Rui'an Wu Chinese Dialect",
    "volume": "main",
    "abstract": "Frication is not a common feature in characterizing vowels. However, Chinese dialects are known for having apical vowels. Additionally, there are fricative high vowels in a few dialects. This paper describes the phonetics and phonology of the vowels in the Rui'an Wu Chinese dialect, with an emphasis on vowel features distinguishing the high vowels. Rui'an has 12 monophthongs [i y ʉ e ø ε a ɿ ɔ o u ɯ]; and half of them [i y ʉ ɿ u ɯ] are high vowels. Formant data from 10 native speakers, 5 male and 5 female, were analyzed. And acoustic results reveal that [ɿ] is an apical vowel with significantly higher frication than other high vowels, whereas the difference in frication between [ʉ ɯ] and [y u] respectively is not confirmed. Rather, spectral difference plays a more important role in the distinction between labiodental high vowels [ʉ ɯ] and their plain rounded counterparts [y u]",
    "checked": true,
    "id": "aebfbd8909bda96d40a961d1490897896b933f8c",
    "semantic_title": "frication as a vowel feature? - evidence from the rui'an wu chinese dialect",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19m_interspeech.html": {
    "title": "Vowels and Diphthongs in the Xupu Xiang Chinese Dialect",
    "volume": "main",
    "abstract": "Based on an acoustic analysis of speech data from 10 speakers, 5 male and 5 female, this paper describes the phonetics and phonology of the vowels and diphthongs in the Xupu Xiang Chinese dialect. Results suggest that monophthongs and falling diphthongs should be grouped together, since the production of them is a single articulatory event. Falling diphthongs are composed of a dynamic spectral target, while monophthongs are composed of a static spectral target. But rising diphthongs are sequences of two spectral targets",
    "checked": true,
    "id": "c353e0534abfa111ab2a47bb88981879d2e6c1d2",
    "semantic_title": "vowels and diphthongs in the xupu xiang chinese dialect",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/albuquerque19_interspeech.html": {
    "title": "Age-Related Changes in European Portuguese Vowel Acoustics",
    "volume": "main",
    "abstract": "This study addresses effects of age and gender on acoustics of European Portuguese oral vowels, given to the fact of conflicting findings reported in prior research. Fundamental frequency (F0), formant frequencies (F1 and F2) and duration of vowels produced by a group of 113 adults, aged between 35 and 97 years old, were measured. Vowel space area (VSA) according to gender and age was also analysed. The results revealed that the most consistent age-related effect was an increase in vowel duration in both genders. F0 decreases above [50–64] for female and for male data suggests a slight drop over the age range [35–64] and then an increase in an older age. That is, F0 tends to be closer between genders as age increases. In general, there is no evidence that F1 and F2 frequencies were lowering as age increased. Furthermore, there were no changes to VSA with ageing. These results provide a base of information to establish vowel acoustics normal patterns of ageing among Portuguese adults",
    "checked": true,
    "id": "acc32b198ba66044730b63f9cd260f3d1c994143",
    "semantic_title": "age-related changes in european portuguese vowel acoustics",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lalhminghlui19_interspeech.html": {
    "title": "Vowel-Tone Interaction in Two Tibeto-Burman Languages",
    "volume": "main",
    "abstract": "Intrinsic F0 (IF0) has been considered a phonetic phenomenon that has a physiological basis. However, considering cross linguistic variation in IF0, it is also assumed that there is an amount of speaker intended control on IF0. This work looks into the two tone languages spoken in North East India and confirms the evidence of IF0 in the languages. However, it also shows that as soon as speakers exert control over F0 for tone production, IF0 differences diminish. As previously reported, in this study too, IF0 differences were noticed to be more pronounced in the higher F0 regions",
    "checked": true,
    "id": "e0f2919bc50648092f8f3aa959b6a34a765a7a42",
    "semantic_title": "vowel-tone interaction in two tibeto-burman languages",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rodriguez19_interspeech.html": {
    "title": "The Vowel System of Korebaju",
    "volume": "main",
    "abstract": "Korebaju [kòrèβáhᵼ̀] (ISO 639-3: coe) is a Western Tukanoan language from the South-Western part of Colombia. A study conducted in 2017 and 2018 with six native speakers (3 female and 3 male) shows that Korebaju has an inventory of 17 consonants /p, t, k, p , t , k , β, ɸ, s, h, tʃ͡, m, n, ɲ, m, ŋ, r/ and 6 oral vowels /i, e, a, o, u, ɨ/, 6 nasal vowels /ĩ, ẽ, ã, õ, ũ, ᵼ̃/ and 3 glottal vowels /aˀ, eˀ, oˀ/. Contrary to previous studies, this paper shows that Korebaju does not include the vowel [ɯ] in its phonemic inventory. The vowel [ɯ] is an allophone of the high back vowel /u/ when it follows a palatal consonant. In the same context, the high central vowel /ɨ/ also has an allophone [ɪ]. This paper focuses on an acoustic and articulatory description. Data come from a set of words recorded with synchronized audio and EGG signals",
    "checked": true,
    "id": "34485cab2b46f46f61d6522f132af11e833efea4",
    "semantic_title": "the vowel system of korebaju",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ibrahim19_interspeech.html": {
    "title": "Fundamental Frequency Accommodation in Multi-Party Human-Robot Game Interactions: The Effect of Winning or Losing",
    "volume": "main",
    "abstract": "In human-human interactions, the situational context plays a large role in the degree of speakers' accommodation. In this paper, we investigate whether the degree of accommodation in a human-robot computer game is affected by (a) the duration of the interaction and (b) the success of the players in the game. 30 teams of two players played two card games with a conversational robot in which they had to find a correct order of five cards. After game 1, the players received the result of the game on a success scale from 1 (lowest success) to 5 (highest). Speakers' f accommodation was measured as the Euclidean distance between the human speakers and each human and the robot. Results revealed that (a) the duration of the game had no influence on the degree of f accommodation and (b) the result of Game 1 correlated with the degree of f accommodation in Game 2 (higher success equals lower Euclidean distance). We argue that game success is most likely considered as a sign of the success of players' cooperation during the discussion, which leads to a higher accommodation behavior in speech",
    "checked": true,
    "id": "0e6726015053a1878cae71e0cadfd9eaa758f276",
    "semantic_title": "fundamental frequency accommodation in multi-party human-robot game interactions: the effect of winning or losing",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wagner19_interspeech.html": {
    "title": "Pitch Accent Trajectories Across Different Conditions of Visibility and Information Structure — Evidence from Spontaneous Dyadic Interaction",
    "volume": "main",
    "abstract": "Previous research identified a differential contribution of information structure and the visibility of facial and contextual information to the acoustic-prosodic expression of pitch accents. However, it is unclear whether pitch accent shapes are affected by these conditions as well. To investigate whether varying context cues have a differentiated impact on pitch accent trajectories produced in conversational interaction, we modified the visibility conditions in a spontaneous dyadic interaction task, i.e. a verbalized version of TicTacToe. Besides varying visibility, the game task allows for measuring the impact of information-structure on pitch accent trajectories, differentiating important and unpredictable game moves. Using GAMMs on four speaker groups (identified by a cluster analysis), we could isolate varying strategies of prosodic adaptation to contextual change. While few speaker groups showed a reaction to the availability of visible context cues (facial prosody or executed game moves), all groups differentiated the verbalization of unpredictable and predictable game moves with a group-specific trajectory adaptation. The importance of game moves resulted in differentiated adaptations in two out of four speaker groups. The detected strategic trajectory adaptations were characterized by different characteristics of boundary tones, adaptations of the global f0-level, or the shape of the corresponding pitch accent",
    "checked": true,
    "id": "7998c7debcdc40c686cd3dc5ce204b2f8945e1e9",
    "semantic_title": "pitch accent trajectories across different conditions of visibility and information structure - evidence from spontaneous dyadic interaction",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/betz19_interspeech.html": {
    "title": "The Greennn Tree — Lengthening Position Influences Uncertainty Perception",
    "volume": "main",
    "abstract": "Synthetic speech can be used to express uncertainty in dialogue systems by means of hesitation. If a phrase like \"Next to the green tree\" is uttered in a hesitant way, that is, containing lengthening, silences, and fillers, the listener can infer that the speaker is not certain about the concepts referred to. However, we do not know anything about the referential domain of the uncertainty; if only a particular word in this sentence would be uttered hesitantly, e.g. \"the greee:n tree\", the listener could infer that the uncertainty refers to the color in the statement, but not to the object. In this study, we show that the domain of the uncertainty is controllable. We conducted an experiment in which color words in sentences like \"search for the green tree\" were lengthened in two different positions: word onsets or final consonants, and participants were asked to rate the uncertainty regarding color and object. The results show that initial lengthening is predominantly associated with uncertainty about the word itself, whereas final lengthening is primarily associated with the following object. These findings enable dialogue system developers to finely control the attitudinal display of uncertainty, adding nuances beyond the lexical content to message delivery",
    "checked": true,
    "id": "a0943442c6b3b6565e58963e7510cc768e2669a7",
    "semantic_title": "the greennn tree - lengthening position influences uncertainty perception",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/si19_interspeech.html": {
    "title": "CNN-BLSTM Based Question Detection from Dialogs Considering Phase and Context Information",
    "volume": "main",
    "abstract": "Question detection from dialogs is important in human-computer interaction systems. Recent studies on question detection mostly use recurrent neural network (RNN) based methods to process low-level descriptors (LLD) of the utterance. However, there are three main problems in these studies. Firstly, traditional LLD features are defined based on human a priori knowledge, some of which are difficult to be extracted accurately. Secondly, previous studies of question detection only consider features from amplitude information and ignored phase information. Thirdly, previous studies show that the context in an utterance is helpful to detect question, while the context between utterances is not well investigated in this task. To cope with the aforementioned problems, we propose a CNN-BLSTM based framework, where amplitude information is obtained from the combination of spectrogram and LLD, and processed together with the phase information. Our framework also models the context information in the dialog. From the experiments on Mandarin dialog corpus, we revealed the effectiveness of the integrated feature with both amplitude and phase in question detection. The results indicated that the phase feature was helpful to detect the questions with a short duration, and the context between utterances was beneficial to detect questions without special interrogative forms",
    "checked": true,
    "id": "4aeb63ee45e11b767ca19e4eb49fc85c0101470d",
    "semantic_title": "cnn-blstm based question detection from dialogs considering phase and context information",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/metcalf19_interspeech.html": {
    "title": "Mirroring to Build Trust in Digital Assistants",
    "volume": "main",
    "abstract": "We describe experiments towards building a conversational digital assistant that considers the preferred conversational style of the user. In particular, these experiments are designed to measure whether users prefer and trust an assistant whose conversational style matches their own. To this end we conducted a user study where subjects interacted with a digital assistant whose response either matched their conversational style, or did not. We found that people strongly prefer a digital assistant that mirrors their \"chattiness\" and that this preference can be reliably detected",
    "checked": true,
    "id": "3114ac88e926c0669ec25f97c89eefe0f4eaa1ed",
    "semantic_title": "mirroring to build trust in digital assistants",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2019/raveh19_interspeech.html": {
    "title": "Three's a Crowd? Effects of a Second Human on Vocal Accommodation with a Voice Assistant",
    "volume": "main",
    "abstract": "This study examines how the presence of other speakers affects the interaction with a spoken dialogue system. We analyze participants' speech regarding several phonetic features, viz., fundamental frequency, intensity, and articulation rate, in two conditions: with and without additional speech input from a human confederate as a third interlocutor. The comparison was made via tasks performed by participants using a commercial voice assistant under both conditions in alternation. We compare the distributions of the features across the two conditions to investigate whether speakers behave differently when a confederate is involved. Temporal analysis exposes continuous changes in the feature productions. In particular, we measured overall accommodation between the participants and the system throughout the interactions. Results show significant differences in a majority of cases for two of the three features, which are more pronounced in cases where the user first interacted with the device alone. We also analyze factors such as the task performed, participant gender, and task order, providing additional insight into the participants' behavior",
    "checked": true,
    "id": "1b2a9df143245f98815ef41a1c12045bcdd72cef",
    "semantic_title": "three's a crowd? effects of a second human on vocal accommodation with a voice assistant",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19o_interspeech.html": {
    "title": "Adversarial Regularization for End-to-End Robust Speaker Verification",
    "volume": "main",
    "abstract": "Deep learning has been successfully used in speaker verification (SV), especially in end-to-end SV systems which have attracted more interest recently. It has been shown in image as well as speech applications that deep neural networks are vulnerable to adversarial examples. In this study, we explore two methods to generate adversarial examples for advanced SV: (i) fast gradient-sign method (FGSM), and (ii) local distributional smoothness (LDS) method. To explore this issue, we use adversarial examples to attack an end-to-end SV system. Experiments will show that the neural network can be easily disturbed by adversarial examples. Next, we propose to train an end-to-end robust SV model using the two proposed adversarial examples for model regularization. Experimental results with the TIMIT dataset indicate that the EER is improved relatively by (i) +18.89% and (ii) +5.54% for the original test set using the regularized model. In addition, the regularized model improves EER of the adversarial example test set by a relative (i) +30.11% and (ii) +22.12%, which therefore suggests more consistent performance against adversarial example attacks",
    "checked": true,
    "id": "16cea48282ecbde2f3b18fe952ec4e36879fdb9a",
    "semantic_title": "adversarial regularization for end-to-end robust speaker verification",
    "citation_count": 31
  },
  "https://www.isca-speech.org/archive/interspeech_2019/monteiro19_interspeech.html": {
    "title": "Combining Speaker Recognition and Metric Learning for Speaker-Dependent Representation Learning",
    "volume": "main",
    "abstract": "In this paper, we tackle automatic speaker verification under a text-independent setting. Speaker modelling is performed by a deep convolutional neural network on top of time-frequency speech representations. Convolutions performed over the time dimension provide the means for the model to take both short-term dependencies into account, given the nature of the learned filters which operate over short-windows, as well as long-term dependencies, since depth in a convolutional stack implies dependency of outputs across large portions of input samples. Additionally, various pooling strategies across the time dimension are compared so as to effectively map varying length recordings into fixed dimensional representations while simultaneously providing the neural network with an extra mechanism to model long-term dependencies. We finally propose a training scheme under which well-known metric learning approaches, namely triplet loss minimization, is performed along with speaker recognition in a multi-class classification setting. Evaluation on well-known datasets and comparisons with state-of-the-art benchmarks show that the proposed setting is effective in yielding speaker-dependent representations, thus is well-suited for voice biometrics downstream tasks",
    "checked": true,
    "id": "365dc7ca3ee6f058dc23d00782421e86000eddb0",
    "semantic_title": "combining speaker recognition and metric learning for speaker-dependent representation learning",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19n_interspeech.html": {
    "title": "VAE-Based Regularization for Deep Speaker Embedding",
    "volume": "main",
    "abstract": "Deep speaker embedding has achieved state-of-the-art performance in speaker recognition. A potential problem of these embedded vectors (called ‘x-vectors') are not Gaussian, causing performance degradation with the famous PLDA back-end scoring. In this paper, we propose a regularization approach based on Variational Auto-Encoder (VAE). This model transforms x-vectors to a latent space where mapped latent codes are more Gaussian, hence more suitable for PLDA scoring",
    "checked": true,
    "id": "f5a461f607228cd01f11e878ef84e4500b4e4ec7",
    "semantic_title": "vae-based regularization for deep speaker embedding",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mingote19b_interspeech.html": {
    "title": "Language Recognition Using Triplet Neural Networks",
    "volume": "main",
    "abstract": "In this paper, we propose a novel neural network back-end approach based on triplets for the language recognition task, due to its success application in the related field of text-dependent speaker verification. A triplet is a training example constructed of three audio samples; two from the same class and one from a different class. In presenting two pairs of samples to the network, the triplet neural network learns to discriminate between samples from the same languages and pairs of different languages. Triplet-based training optimizes the Area Under the Curve (AUC) in contrast to other triplet loss functions proposed in the literature. The optimization of the AUC as cost function is appropriate for a detection task as it directly correlates with end-use performance of the system. Moreover, we show the importance of defining an appropriate method of triplet selection and how this impacts performance of the system. When benchmarked on the LRE09 database, the new triplet backend demonstrated superior performance compared to traditional back-ends used for language recognition. In addition, we performed an evaluation on the LRE15 and LRE17 databases to check the generalization power of the proposed systems",
    "checked": true,
    "id": "3922b0bc41565137ff6fb3c7cb5eef709c626ed5",
    "semantic_title": "language recognition using triplet neural networks",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jung19c_interspeech.html": {
    "title": "Spatial Pyramid Encoding with Convex Length Normalization for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "In this paper, we propose a new pooling method called spatial pyramid encoding (SPE) to generate speaker embeddings for text-independent speaker verification. We first partition the output feature maps from a deep residual network (ResNet) into increasingly fine sub-regions and extract speaker embeddings from each sub-region through a learnable dictionary encoding layer. These embeddings are concatenated to obtain the final speaker representation. The SPE layer not only generates a fixed-dimensional speaker embedding for a variable-length speech segment, but also aggregates the information of feature distribution from multi-level temporal bins. Furthermore, we apply deep length normalization by augmenting the loss function with ring loss. By applying ring loss, the network gradually learns to normalize the speaker embeddings using model weights themselves while preserving convexity, leading to more robust speaker embeddings. Experiments on the VoxCeleb1 dataset show that the proposed system using the SPE layer and ring loss-based deep length normalization outperforms both i-vector and d-vector baselines",
    "checked": true,
    "id": "334e7576e58a5320d51fa060909f3ac943732662",
    "semantic_title": "spatial pyramid encoding with convex length normalization for text-independent speaker verification",
    "citation_count": 30
  },
  "https://www.isca-speech.org/archive/interspeech_2019/heo19b_interspeech.html": {
    "title": "End-to-End Losses Based on Speaker Basis Vectors and All-Speaker Hard Negative Mining for Speaker Verification",
    "volume": "main",
    "abstract": "In recent years, speaker verification has primarily performed using deep neural networks that are trained to output embeddings from input features such as spectrograms or Mel-filterbank energies. Studies that design various loss functions, including metric learning have been widely explored. In this study, we propose two end-to-end loss functions for speaker verification using the concept of speaker bases, which are trainable parameters. One loss function is designed to further increase the inter-speaker variation, and the other is designed to conduct the identical concept with hard negative mining. Each speaker basis is designed to represent the corresponding speaker in the process of training deep neural networks. In contrast to the conventional loss functions that can consider only a limited number of speakers included in a mini-batch, the proposed loss functions can consider all the speakers in the training set regardless of the mini-batch composition. In particular, the proposed loss functions enable hard negative mining and calculations of between-speaker variations with consideration of all speakers. Through experiments on VoxCeleb1 and VoxCeleb2 datasets, we confirmed that the proposed loss functions could supplement conventional softmax and center loss functions",
    "checked": true,
    "id": "2364af8e7d51c20115a5218aaaf2587c6da54869",
    "semantic_title": "end-to-end losses based on speaker basis vectors and all-speaker hard negative mining for speaker verification",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jiang19_interspeech.html": {
    "title": "An Effective Deep Embedding Learning Architecture for Speaker Verification",
    "volume": "main",
    "abstract": "In this paper we present an effective deep embedding learning architecture, which combines a dense connection of dilated convolutional layers with a gating mechanism, for speaker verification (SV) tasks. Compared with the widely used time-delay neural network (TDNN) based architecture, two main improvements are proposed: (1) The dilated filters are designed to effectively capture time-frequency context information, then the convolutional layer outputs are utilized for effective embedding learning. Specifically, we employ the idea of the successful DenseNet to collect the context information by dense connections from each layer to every other layer in a feed-forward fashion. (2) A gating mechanism is further introduced to provide channel-wise attention by exploiting inter-dependencies across channels. Motivated by squeeze-and-excitation networks (SENet), the global time-frequency information is utilized for this feature calibration. To evaluate the proposed network architecture, we conduct extensive experiments on noisy and unconstrained SV tasks, i.e., Speaker in the Wild (SITW) and Voxceleb1. The results demonstrate state-of-the-art SV performance. Specifically, our proposed method reduces equal error rate (EER) from TDNN based method by 25% and 27% for SITW and Voxceleb1, respectively",
    "checked": true,
    "id": "75cc669e89158b3f16cdd776c9c47bf22526b459",
    "semantic_title": "an effective deep embedding learning architecture for speaker verification",
    "citation_count": 25
  },
  "https://www.isca-speech.org/archive/interspeech_2019/qin19b_interspeech.html": {
    "title": "Far-Field End-to-End Text-Dependent Speaker Verification Based on Mixed Training Data with Transfer Learning and Enrollment Data Augmentation",
    "volume": "main",
    "abstract": "In this paper, we focus on the far-field end-to-end text-dependent speaker verification task with a small-scale far-field text dependent dataset and a large scale close-talking text independent database for training. First, we show that simulating far-field text independent data from the existing large-scale clean database for data augmentation can reduce the mismatch. Second, using a small far-field text dependent data set to finetune the deep speaker embedding model pre-trained from the simulated far-field as well as original clean text independent data can significantly improve the system performance. Third, in special applications when using the close-talking clean utterances for enrollment and employing the real far-field noisy utterances for testing, adding reverberant noises on the clean enrollment data can further enhance the system performance. We evaluate our methods on AISHELL ASR0009 and AISHELL 2019B-eval databases and achieve an equal error rate (EER) of 5.75% for far-field text-dependent speaker verification under noisy environments",
    "checked": true,
    "id": "8f96b1cf25b3da5c1bba53eab218dce29e010c3f",
    "semantic_title": "far-field end-to-end text-dependent speaker verification based on mixed training data with transfer learning and enrollment data augmentation",
    "citation_count": 32
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ren19_interspeech.html": {
    "title": "Two-Stage Training for Chinese Dialect Recognition",
    "volume": "main",
    "abstract": "In this paper, we present a two-stage language identification (LID) system based on a shallow ResNet14 followed by a simple 2-layer recurrent neural network (RNN) architecture, which was used for Xunfei (iFlyTek) Chinese Dialect Recognition Challenge and won the first place among 110 teams. The system trains an acoustic model (AM) firstly with connectionist temporal classification (CTC) to recognize the given phonetic sequence annotation and then train another RNN to classify dialect category by utilizing the intermediate features as inputs from the AM. Compared with a three-stage system we further explore, our results show that the two-stage system can achieve high accuracy for Chinese dialects recognition under both short utterance and long utterance conditions with less training time",
    "checked": true,
    "id": "40452909dc5b705e2addb5ab44bbedf6ada7532b",
    "semantic_title": "two-stage training for chinese dialect recognition",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kaminishi19_interspeech.html": {
    "title": "Investigation on Blind Bandwidth Extension with a Non-Linear Function and its Evaluation of x-Vector-Based Speaker Verification",
    "volume": "main",
    "abstract": "This study evaluates the effects of some non-learning blind bandwidth extension (BWE) methods on automatic speaker verification (ASV) systems based on x-vector. Recently, a non-linear bandwidth extension (N-BWE) has been proposed as a blind, non-learning, and light-weight BWE approach. Other non-learning BWEs have also been developed in recent years. For ASV evaluations, most data available to train ASV systems is narrowband (NB) telephone speech. Meanwhile, wideband (WB) data have been used to train the state-of-the-art ASV systems, such as i-vector and x-vector. This can cause sampling rate mismatches when all datasets are used. In this paper, we investigate the influence of sampling rate mismatches in the x-vector-based ASV systems and how non-learning BWE methods perform against them. The results showed that the N-BWE method improved the equal error rate (EER) on ASV systems based on x-vector when the mismatches were present. We researched the relationship between objective measurements and EERs. Consequently, the N-BWE method produced the lowest EER and obtained the lower RMS-LSD value and the higher STOI score",
    "checked": true,
    "id": "439e424159d0f55bb959c7d392c5026e08cbfea4",
    "semantic_title": "investigation on blind bandwidth extension with a non-linear function and its evaluation of x-vector-based speaker verification",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/khan19_interspeech.html": {
    "title": "Auto-Encoding Nearest Neighbor i-Vectors for Speaker Verification",
    "volume": "main",
    "abstract": "In the last years, i-vectors followed by cosine or PLDA scoring techniques were the state-of-the-art approach in speaker verification. PLDA requires labeled background data, and there exists a significant performance gap between the two scoring techniques. In this work, we propose to reduce this gap by using an autoencoder to transform i-vector into a new speaker vector representation, which will be referred to as ae-vector. The autoencoder will be trained to reconstruct neighbor i-vectors instead of the same training i-vectors, as usual. These neighbor i-vectors will be selected in an unsupervised manner according to the highest cosine scores to the training i-vectors. The evaluation is performed on the speaker verification trials of VoxCeleb-1 database. The experiments show that our proposed ae-vectors gain a relative improvement of 42% in terms of EER compared to the conventional i-vectors using cosine scoring, which fills the performance gap between cosine and PLDA scoring techniques by 92%, but without using speaker labels",
    "checked": true,
    "id": "1ec39c44aaddd7df464942f40eea7a959876bde3",
    "semantic_title": "auto-encoding nearest neighbor i-vectors for speaker verification",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zheng19b_interspeech.html": {
    "title": "Towards a Fault-Tolerant Speaker Verification System: A Regularization Approach to Reduce the Condition Number",
    "volume": "main",
    "abstract": "Large-scale deployment of speech interaction devices makes it possible to harvest tremendous data quickly, which also introduces the problem of wrong labeling during data mining. Mislabeled training data has a substantial negative effect on the performance of speaker verification system. This study aims to enhance the generalization ability and robustness of the model when the training data is contaminated by wrong labels. Several regularization approaches are proposed to reduce the condition number of the speaker verification problem, making the model less sensitive to errors in the inputs. They are validated on both NIST SRE corpus and far-field smart speaker data. The results suggest that the performance deterioration caused by mislabeled training data can be significantly ameliorated by proper regularization",
    "checked": true,
    "id": "fc173b5108a0189256213a53cb998b6c919c8b1f",
    "semantic_title": "towards a fault-tolerant speaker verification system: a regularization approach to reduce the condition number",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/taherian19_interspeech.html": {
    "title": "Deep Learning Based Multi-Channel Speaker Recognition in Noisy and Reverberant Environments",
    "volume": "main",
    "abstract": "Despite successful applications of multi-channel signal processing in robust automatic speech recognition (ASR), relatively little research has been conducted on the effectiveness of such techniques in the robust speaker recognition domain. This paper introduces time-frequency (T-F) masking-based beamforming to address text-independent speaker recognition in conditions where strong diffuse noise and reverberation are both present. We examine various masking-based beamformers, such as parameterized multi-channel Wiener filter, generalized eigenvalue (GEV) beamformer and minimum variance distortion-less response (MVDR) beamformer, and evaluate their performance in terms of speaker recognition accuracy for i-vector and x-vector based systems. In addition, we present a different formulation for estimating steering vectors from speech covariance matrices. We show that rank-1 approximation of a speech covariance matrix based on generalized eigenvalue decomposition leads to the best results for the masking-based MVDR beamformer. Experiments on the recently introduced NIST SRE 2010 retransmitted corpus show that the MVDR beamformer with rank-1 approximation provides an absolute reduction of 5.55% in equal error rate compared to a standard masking-based MVDR beamformer",
    "checked": true,
    "id": "13259c77efef7800eb2e8db813c889956aef06a2",
    "semantic_title": "deep learning based multi-channel speaker recognition in noisy and reverberant environments",
    "citation_count": 25
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yang19g_interspeech.html": {
    "title": "Joint Optimization of Neural Acoustic Beamforming and Dereverberation with x-Vectors for Robust Speaker Verification",
    "volume": "main",
    "abstract": "In this paper, we investigate the deep neural network (DNN) supported acoustic beamforming and dereverberation as the front-end of the x-vector speaker verification (SV) framework in a noisy and reverberant environment. Firstly, a DNN for supporting either the classical beamforming (e. g. MVDR) or the dereverberation (e. g. WPE) algorithm is trained on multi-channel speech signals. Next, an x-vector speaker embedding network is trained on top of the enhanced speech features to classify the training speakers. Finally, after the separate training stages are over, either one or both of the DNN supported beamforming and dereverberation modules are serially connected to the x-vector network, and jointly trained to optimize the common objective of speaker classification. Experiments on the artificially generated speech dataset using simulated and real room impulse responses (RIRs) with various types of domestic noise samples show that jointly training the supportive neural network models along with the x-vector network within the classical speech enhancement framework brings significant performance gain for robust text-independent (TI) SV",
    "checked": true,
    "id": "cff4c0b096ad3017e79bd4667a5b1325b91a48c6",
    "semantic_title": "joint optimization of neural acoustic beamforming and dereverberation with x-vectors for robust speaker verification",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/miao19b_interspeech.html": {
    "title": "A New Time-Frequency Attention Mechanism for TDNN and CNN-LSTM-TDNN, with Application to Language Identification",
    "volume": "main",
    "abstract": "In this paper, we aim to improve traditional DNN x-vector language identification (LID) performance by employing Convolutional and Long Short Term Memory-Recurrent (CLSTM) Neural Networks, as they can strengthen feature extraction and capture longer temporal dependencies. We also propose a two-dimensional attention mechanism. Compared with conventional one-dimensional time attention, our method introduces a frequency attention mechanism to give different weights to different frequency bands to generate weighted means and standard deviations. This mechanism can direct attention to either time or frequency information, and can be trained or fused singly or jointly. Experimental results show firstly that CLSTM can significantly outperform a traditional DNN x-vector implementation. Secondly, the proposed frequency attention method is more effective than time attention, particularly when the number of frequency bands matches the feature size. Furthermore, frequency-time score merging is the best, whereas frequency-time feature merge only shows improvements for small frequency dimension",
    "checked": true,
    "id": "987eb8ff8467625d1a9f2c824999967fefae967f",
    "semantic_title": "a new time-frequency attention mechanism for tdnn and cnn-lstm-tdnn, with application to language identification",
    "citation_count": 38
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19n_interspeech.html": {
    "title": "An Attention-Based Hybrid Network for Automatic Detection of Alzheimer's Disease from Narrative Speech",
    "volume": "main",
    "abstract": "Alzheimer's disease (AD) is one of the leading causes of death in the world and affects at least 50 million individuals. Currently, there is no cure for the disease. So a convenient and reliable early detection approach before irreversible brain damage and cognitive decline have occurred is of great importance. One prominent sign of AD is language dysfunction. Some aspects of language are affected at the same time or even before the memory problems emerge. Therefore, we propose an automatic speech analysis framework to identify AD subjects from short narrative speech transcript elicited with a picture description task. The proposed network is based on attention mechanism and is composed of a CNN and a GRU module. We obtained state-of-the-art cross-validation accuracy of 97 in distinguishing individuals with AD from elderly normal controls. The performance of our model makes it reasonable to conclude that our approach reveals a considerable part of the language deficits of AD patients and can help with the diagnosis of the disease from spontaneous speech",
    "checked": true,
    "id": "4e4136382ddab4b5b357dd8c9c81789d930065fb",
    "semantic_title": "an attention-based hybrid network for automatic detection of alzheimer's disease from narrative speech",
    "citation_count": 28
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ma19b_interspeech.html": {
    "title": "Investigating the Lombard Effect Influence on End-to-End Audio-Visual Speech Recognition",
    "volume": "main",
    "abstract": "Several audio-visual speech recognition models have been recently proposed which aim to improve the robustness over audio-only models in the presence of noise. However, almost all of them ignore the impact of the Lombard effect, i.e., the change in speaking style in noisy environments which aims to make speech more intelligible and affects both the acoustic characteristics of speech and the lip movements. In this paper, we investigate the impact of the Lombard effect in audio-visual speech recognition. To the best of our knowledge, this is the first work which does so using end-to-end deep architectures and presents results on unseen speakers. Our results show that properly modelling Lombard speech is always beneficial. Even if a relatively small amount of Lombard speech is added to the training set then the performance in a real scenario, where noisy Lombard speech is present, can be significantly improved. We also show that the standard approach followed in the literature, where a model is trained and tested on noisy plain speech, provides a correct estimate of the video-only performance and slightly underestimates the audio-visual performance. In case of audio-only approaches, performance is overestimated for SNRs higher than -3dB and underestimated for lower SNRs",
    "checked": true,
    "id": "c56ce6aad3f06e9113de910a2bc31f4ef8a724a0",
    "semantic_title": "investigating the lombard effect influence on end-to-end audio-visual speech recognition",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ooster19_interspeech.html": {
    "title": "Computer, Test My Hearing\": Accurate Speech Audiometry with Smart Speakers",
    "volume": "main",
    "abstract": "Speech audiometry based on matrix sentence tests is an important diagnostic tool for hearing impairment and fitting of hearing aids. This paper introduces a self-conducted measurement for estimating the speech reception threshold (SRT) of a subject, i.e., the signal-to-noise ratio corresponding to 50% intelligibility, based on a smart speaker. While the original measurement procedure is well-evaluated and provides a very high measurement accuracy (<1 dB test-retest standard deviation), the measurement using a smart speaker differs in several aspects from the commercially available implementation, such as missing control over the absolute presentation level, mode of presentation (headphones vs. loudspeaker), potential errors from the automated response logging, and influence from room acoustics. The SRT measurement accuracy is evaluated with six normal-hearing subjects conducted with an Amazon Alexa application on an Echo Plus loudspeaker in a quiet office environment. We found a significant difference of 0.6 dB in SRT between the proposed and the commercially available testing procedure. However, this bias is smaller than the inter-subject standard deviation, and the measurement accuracy is similar to the original test for normal-hearing listeners, which indicates that smart speakers may become a helpful addition for the screening of hearing deficits",
    "checked": true,
    "id": "1c299b63769d027cce07ec1d8d11d9e9b052325f",
    "semantic_title": "computer, test my hearing\": accurate speech audiometry with smart speakers",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/eshky19_interspeech.html": {
    "title": "Synchronising Audio and Ultrasound by Learning Cross-Modal Embeddings",
    "volume": "main",
    "abstract": "Audiovisual synchronisation is the task of determining the time offset between speech audio and a video recording of the articulators. In child speech therapy, audio and ultrasound videos of the tongue are captured using instruments which rely on hardware to synchronise the two modalities at recording time. Hardware synchronisation can fail in practice, and no mechanism exists to synchronise the signals post hoc. To address this problem, we employ a two-stream neural network which exploits the correlation between the two modalities to find the offset. We train our model on recordings from 69 speakers, and show that it correctly synchronises 82.9% of test utterances from unseen therapy sessions and unseen speakers, thus considerably reducing the number of utterances to be manually synchronised. An analysis of model performance on the test utterances shows that directed phone articulations are more difficult to automatically synchronise compared to utterances containing natural variation in speech such as words, sentences, or conversations",
    "checked": true,
    "id": "20870c606a8cd495942b10baab67624d4b505ee4",
    "semantic_title": "synchronising audio and ultrasound by learning cross-modal embeddings",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pan19_interspeech.html": {
    "title": "Automatic Hierarchical Attention Neural Network for Detecting AD",
    "volume": "main",
    "abstract": "Picture description tasks are used for the detection of cognitive decline associated with Alzheimer's disease (AD). Recent years have seen work on automatic AD detection in picture descriptions based on acoustic and word-based analysis of the speech. These methods have shown some success but lack an ability to capture any higher level effects of cognitive decline on the patient's language. In this paper, we propose a novel model that encompasses both the hierarchical and sequential structure of the description and detect its informative units by attention mechanism. Automatic speech recognition (ASR) and punctuation restoration are used to transcribe and segment the data. Using the DementiaBank database of people with AD as well as healthy controls (HC), we obtain an F-score of 84.43% and 74.37% when using manual and automatic transcripts respectively. We further explore the effect of adding additional data (a total of 33 descriptions collected using a ‘ digital doctor' ) during model training, and increase the F-score when using ASR transcripts to 76.09%. This outperforms baseline models, including bidirectional LSTM and bidirectional hierarchical neural network without an attention mechanism, and demonstrate that the use of hierarchical models with attention mechanism improves the AD/HC discrimination performance",
    "checked": true,
    "id": "365fab1146a72ad08799db52a07ef2a45038315e",
    "semantic_title": "automatic hierarchical attention neural network for detecting ad",
    "citation_count": 25
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nallanthighal19_interspeech.html": {
    "title": "Deep Sensing of Breathing Signal During Conversational Speech",
    "volume": "main",
    "abstract": "In this paper, we show the first results on the estimation of breathing signal from conversational speech using deep learning algorithms. Respiratory diseases such as COPD, asthma, and respiratory infections are common in the elderly population and patients in health care monitoring and medical alert services in general. In this work, we compare algorithms for the estimation of a known respiratory target signal, measured by respiratory belt transducers positioned across the rib cage and abdomen, from conversational speech. We demonstrate the estimation of the respiratory signal from speech using convolutional and recurrent neural networks. The estimated breathing pattern gives respiratory rate, breathing capacity and thus might provide indications of the pathological condition of the speaker. Evaluation of our model on our database of breathing signal and speech yielded a sensitivity of 91.2% for breath event detection and a mean absolute error of 1.01 breaths per minute for breathing rate estimation",
    "checked": true,
    "id": "57884b551c502fb91f939e8fbaf6b52b8c284b00",
    "semantic_title": "deep sensing of breathing signal during conversational speech",
    "citation_count": 31
  },
  "https://www.isca-speech.org/archive/interspeech_2019/biadsy19_interspeech.html": {
    "title": "Parrotron: An End-to-End Speech-to-Speech Conversion Model and its Applications to Hearing-Impaired Speech and Speech Separation",
    "volume": "main",
    "abstract": "We describe Parrotron, an end-to-end-trained speech-to-speech conversion model that maps an input spectrogram directly to another spectrogram, without utilizing any intermediate discrete representation. The network is composed of an encoder, spectrogram and phoneme decoders, followed by a vocoder to synthesize a time-domain waveform. We demonstrate that this model can be trained to normalize speech from any speaker regardless of accent, prosody, and background noise, into the voice of a single canonical target speaker with a fixed accent and consistent articulation and prosody. We further show that this normalization model can be adapted to normalize highly atypical speech from a deaf speaker, resulting in significant improvements in intelligibility and naturalness, measured via a speech recognizer and listening tests. Finally, demonstrating the utility of this model on other speech tasks, we show that the same model architecture can be trained to perform a speech separation task",
    "checked": true,
    "id": "940016df38b80c5f3fd9db411e722f58a9d7e227",
    "semantic_title": "parrotron: an end-to-end speech-to-speech conversion model and its applications to hearing-impaired speech and speech separation",
    "citation_count": 90
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19j_interspeech.html": {
    "title": "Exploiting Visual Features Using Bayesian Gated Neural Networks for Disordered Speech Recognition",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) for disordered speech is a challenging task. People with speech disorders such as dysarthria often have physical disabilities, leading to severe degradation of speech quality, highly variable voice characteristics and large mismatch against normal speech. It is also difficult to record large amounts of high quality audio-visual data for developing audio-visual speech recognition (AVSR) systems. To address these issues, a novel Bayesian gated neural network (BGNN) based AVSR approach is proposed. Speaker level Bayesian gated control of contributions from visual features allows a more robust fusion of audio and video modality. A posterior distribution over the gating parameters is used to model their uncertainty given limited and variable disordered speech data. Experiments conducted on the UASpeech dysarthric speech corpus suggest the proposed BGNN AVSR system consistently outperforms state-of-the-art deep neural network (DNN) baseline ASR and AVSR systems by 4.5% and 4.7% absolute (14.9% and 15.5% relative) in word error rate",
    "checked": true,
    "id": "22d74507d48ccbffb52a4c2459c0a65b9c706efa",
    "semantic_title": "exploiting visual features using bayesian gated neural networks for disordered speech recognition",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vougioukas19_interspeech.html": {
    "title": "Video-Driven Speech Reconstruction Using Generative Adversarial Networks",
    "volume": "main",
    "abstract": "Speech is a means of communication which relies on both audio and visual information. The absence of one modality can often lead to confusion or misinterpretation of information. In this paper we present an end-to-end temporal model capable of directly synthesising audio from silent video, without needing to transform to-and-from intermediate features. Our proposed approach, based on GANs is capable of producing natural sounding, intelligible speech which is synchronised with the video. The performance of our model is evaluated on the GRID dataset for both speaker dependent and speaker independent scenarios. To the best of our knowledge this is the first method that maps video directly to raw audio and the first to produce intelligible speech when tested on previously unseen speakers. We evaluate the synthesised audio not only based on the sound quality but also on the accuracy of the spoken words",
    "checked": true,
    "id": "e17330208d553f184ab91e1fc63fa18a087d9d8a",
    "semantic_title": "video-driven speech reconstruction using generative adversarial networks",
    "citation_count": 43
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19k_interspeech.html": {
    "title": "On the Use of Pitch Features for Disordered Speech Recognition",
    "volume": "main",
    "abstract": "Pitch features have long been known to be useful for recognition of normal speech. However, for disordered speech, the significant degradation of voice quality renders the prosodic features, such as pitch, not always useful, particularly when the underlying conditions, for example, damages to the cerebellum, introduce a large effect on prosody control. Hence, both acoustic and prosodic information can be distorted. To the best of our knowledge, there has been very limited research on using pitch features for disordered speech recognition. In this paper, a comparative study of multiple approaches designed to incorporate pitch features is conducted to improve the performance of two disordered speech recognition tasks: English UASpeech, and Cantonese CUDYS. A novel gated neural network (GNN) based approach is used to improve acoustic and pitch feature integration over a conventional concatenation between the two. Bayesian estimation of GNNs is also investigated to further improve their robustness",
    "checked": true,
    "id": "26cd586e2e704cc46099e9af18c56b3d7419ec54",
    "semantic_title": "on the use of pitch features for disordered speech recognition",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shillingford19_interspeech.html": {
    "title": "Large-Scale Visual Speech Recognition",
    "volume": "main",
    "abstract": "This work presents a scalable solution to continuous visual speech recognition. To achieve this, we constructed the largest existing visual speech recognition dataset, consisting of pairs of transcriptions and video clips of faces speaking (3,886 hours of video). In tandem, we designed and trained an integrated lipreading system, consisting of a video processing pipeline that maps raw video to stable videos of lips and sequences of phonemes, a scalable deep neural network that maps the lip videos to sequences of phoneme distributions, and a phoneme-to-word speech decoder that outputs sequences of words. The proposed system achieves a word error rate (WER) of 40.9% as measured on a held-out set. In comparison, professional lipreaders achieve either 86.4% or 92.9% WER on the same dataset when having access to additional types of contextual information. Our approach significantly improves on previous lipreading approaches, including variants of LipNet and of Watch, Attend, and Spell (WAS), which are only capable of 89.8% and 76.8% WER respectively",
    "checked": true,
    "id": "e5befd105f7bbd373208522d5b85682116b59c38",
    "semantic_title": "large-scale visual speech recognition",
    "citation_count": 117
  },
  "https://www.isca-speech.org/archive/interspeech_2019/razavi19_interspeech.html": {
    "title": "Investigating Linguistic and Semantic Features for Turn-Taking Prediction in Open-Domain Human-Computer Conversation",
    "volume": "main",
    "abstract": "In this paper we address the problem of turn-taking prediction in open-ended communication between humans and dialogue agents. In a non-task-oriented interaction with dialogue agents, user inputs are apt to be grammatically and lexically diverse, and at times quite lengthy, with many pauses; all of this makes it harder for the system to decide when to jump in. As a result recent turn-taking predictors designed for specific tasks or for human-human interactions will scarcely be applicable. In this paper we focus primarily on the predictive potential of linguistic features, including lexical, syntactic and semantic features, as well as timing features, whereas past work has typically placed more emphasis on prosodic features, sometimes supplemented with non-verbal behaviors such as gaze and head movements. The basis for our study is a corpus of 15 \"friendly\" dialogues between humans and a (Wizard-of-Oz enabled) virtual dialogue agent, annotated for pause times and types. The model of turn-taking obtained by supervised learning predicts turn-taking points with increasing accuracy using only prosodic features, only timing and speech rate features, only lexical and syntactic features, and achieves state-of-the art performance with a mixture-of-experts model combining these features along with a semantic criterion",
    "checked": true,
    "id": "c6774a23a9fd5476ad68263bc5cbcd777fa79bd0",
    "semantic_title": "investigating linguistic and semantic features for turn-taking prediction in open-domain human-computer conversation",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bechet19_interspeech.html": {
    "title": "Benchmarking Benchmarks: Introducing New Automatic Indicators for Benchmarking Spoken Language Understanding Corpora",
    "volume": "main",
    "abstract": "Empirical evaluation is nowadays the main evaluation paradigm in Natural Language Processing for assessing the relevance of a new machine-learning based model. If large corpora are available for tasks such as Automatic Speech Recognition, this is not the case for other tasks such as Spoken Language Understanding (SLU), consisting in translating spoken transcriptions into a formal representation often based on semantic frames. Corpora such as ATIS or SNIPS are widely used to compare systems, however differences in performance among systems are often very small, not statistically significant, and can be produced by biases in the data collection or the annotation scheme, as we presented on the ATIS corpus (\"Is ATIS too shallow?, IS2018\"). We propose in this study a new methodology for assessing the relevance of an SLU corpus. We claim that only taking into account systems performance does not provide enough insight about what is covered by current state-of-the-art models and what is left to be done. We apply our methodology on a set of 4 SLU systems and 5 benchmark corpora (ATIS, SNIPS, M2M, MEDIA) and automatically produce several indicators assessing the relevance (or not) of each corpus for benchmarking SLU models",
    "checked": true,
    "id": "342b91ab1d2965ba1bd43db6ed5d381053f9349a",
    "semantic_title": "benchmarking benchmarks: introducing new automatic indicators for benchmarking spoken language understanding corpora",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19l_interspeech.html": {
    "title": "A Neural Turn-Taking Model without RNN",
    "volume": "main",
    "abstract": "Sequential data such as speech and dialogs are usually modeled by Recurrent Neural Networks (RNN) and derivatives since the information can travel through time with such architecture. However, disadvantages exist with the use of RNNs, including the limited depth of neural networks and the GPU's unfriendly training process Estimating the timing of turn-taking is a critical feature of dialog systems. Such tasks require knowledge about past dialog contexts and have been modeled using RNNs in several studies. In this paper, we propose a non-RNN model for the timing estimation of turn-taking in dialogs. The proposed model takes lexical and acoustic features as its input to predict a turn's end. We conducted experiments on four types of Japanese conversation datasets and show that with proper neural network designs, the long-term information in a dialog could propagate without a recurrent structure. The proposed model outperformed canonical RNN-based architectures on a turn-taking estimation task",
    "checked": true,
    "id": "60bbe9e77b2b73413213c0882c61001125f8ec14",
    "semantic_title": "a neural turn-taking model without rnn",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/coman19_interspeech.html": {
    "title": "An Incremental Turn-Taking Model for Task-Oriented Dialog Systems",
    "volume": "main",
    "abstract": "In a human-machine dialog scenario, deciding the appropriate time for the machine to take the turn is an open research problem. In contrast, humans engaged in conversations are able to timely decide when to interrupt the speaker for competitive or non-competitive reasons. In state-of-the-art turn-by-turn dialog systems the decision on the next dialog action is taken at the end of the utterance. In this paper, we propose a token-by-token prediction of the dialog state from incremental transcriptions of the user utterance. To identify the point of maximal understanding in an ongoing utterance, we a) implement an incremental Dialog State Tracker which is updated on a token basis (iDST) b) re-label the Dialog State Tracking Challenge 2 (DSTC2) dataset and c) adapt it to the incremental turn-taking experimental scenario. The re-labeling consists of assigning a binary value to each token in the user utterance that allows to identify the appropriate point for taking the turn. Finally, we implement an incremental Turn Taking Decider (iTTD) that is trained on these new labels for the turn-taking decision. We show that the proposed model can achieve a better performance compared to a deterministic handcrafted turn-taking algorithm",
    "checked": true,
    "id": "2c252064688336a9826a417e74eead79637854c1",
    "semantic_title": "an incremental turn-taking model for task-oriented dialog systems",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/su19b_interspeech.html": {
    "title": "Personalized Dialogue Response Generation Learned from Monologues",
    "volume": "main",
    "abstract": "Personalized responses are essential for having an informative and human-like conversation. Because it is difficult to collect a large amount of dialogues involved with specific speakers, it is desirable that chatbot can learn to generate personalized responses simply from monologues of individuals. In this paper, we propose a novel personalized dialogue generation method which reduces the training data requirement to dialogues without speaker information and monologues of every target speaker. In the proposed approach, a generative adversarial network ensures the responses containing recognizable personal characteristics of the target speaker, and a backward SEQ2SEQ model reconstructs the input message for keeping the coherence of the generated responses. The proposed model demonstrates its flexibility to respond to open-domain conversations, and the experimental results show that the proposed method performs favorably against prior work in coherence, personality classification, and human evaluation",
    "checked": true,
    "id": "69d5574c9591a58644acd0154fad9ae8a722b72f",
    "semantic_title": "personalized dialogue response generation learned from monologues",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2019/heldner19_interspeech.html": {
    "title": "Voice Quality as a Turn-Taking Cue",
    "volume": "main",
    "abstract": "This work revisits the idea that voice quality dynamics (VQ) contributes to conveying pragmatic distinctions, with two case studies to further test this idea. First, we explore VQ as a turn-taking cue, and then as a cue for distinguishing between different functions of affirmative cue words. We employ acoustic VQ measures claimed to be better suited for continuous speech than those in own previous work. Both cases indicate that the degree of periodicity (as measured by CPPS) is indeed relevant in the production of the different pragmatic functions. In particular, turn-yielding is characterized by lower periodicity, sometimes accompanied by presence of creaky voice. Periodicity also distinguishes between backchannels, agreements and acknowledgements",
    "checked": true,
    "id": "fb88a1dbff3f4bb0efb99ec7f0b50aeca8b7c8e2",
    "semantic_title": "voice quality as a turn-taking cue",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hara19_interspeech.html": {
    "title": "Turn-Taking Prediction Based on Detection of Transition Relevance Place",
    "volume": "main",
    "abstract": "We address turn-taking prediction in which spoken dialogue systems predict when to take the conversational floor. In natural conversations, many turn-taking decisions are arbitrary and subjective. In this study, we propose taking into account the concept of the transition relevance place (TRP) for turn-taking prediction. TRP is defined as a timing when the current speaking turn can be completed and other participants are able to take the turn. We conducted annotation of TRP on a human-robot dialogue corpus, ensuring the objectivity of this annotation among annotators. The proposed turn-taking prediction model adopts a two-step approach that detects TRP at first and then predicts a turn-taking event if TRP is detected. Experimental evaluations demonstrate that the proposed model improves the accuracy of turn-taking prediction by incorporating TRP detection",
    "checked": true,
    "id": "b8fb4571756dd0c7b01cebe902b9a832cfda70e9",
    "semantic_title": "turn-taking prediction based on detection of transition relevance place",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lala19_interspeech.html": {
    "title": "Analysis of Effect and Timing of Fillers in Natural Turn-Taking",
    "volume": "main",
    "abstract": "Turn-taking for spoken dialogue systems is still below the speed of real human conversation due to latency in speech and natural language processing, but fillers can be used by the system to take the turn more quickly without sacrificing naturalness. In this work we analyze fillers which are used at the start of turns in conversation and determine a window of appropriate times to use them. We analyze a human-robot conversation corpus to obtain an average response time of the fillers, and find that this differs according to the filler's form. We then conduct a subjective experiment in which participants dynamically change the timing of responses with and without fillers to designate a window of acceptable response timings. Our results show that the most suitable response time is around 200–500ms after the previous speaker has finished their turn. We also find differences in timing windows depending on existence of a filler used to begin the turn and its particular form. The implications of these results on the design of conversational systems are also discussed",
    "checked": true,
    "id": "d1e39e2d17e65a5ed4bd426826a6e232f546fa43",
    "semantic_title": "analysis of effect and timing of fillers in natural turn-taking",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/horiguchi19_interspeech.html": {
    "title": "Multimodal Response Obligation Detection with Unsupervised Online Domain Adaptation",
    "volume": "main",
    "abstract": "Response obligation detection, which determines whether a dialogue robot has to respond to a detected utterance, is an important function for intelligent dialogue robots. Some studies have tackled this problem; however, they narrow their applicability by impractical assumptions or use of scenario-specific features. Some attempts have been made to widen the applicability by avoiding the use of text modality, which is said to be highly domain dependent, but it decreases the detection accuracy. In this paper, we propose a novel multimodal response obligation detector, which uses visual, audio, and text information for highly-accurate detection, with its unsupervised online domain adaptation to solve the domain dependency problem. Our domain adaptation consists of the weights adaptation of the logistic regression for every modality and an embedding assignment for new words to cope with the high domain dependency of text modality. Experimental results on the dataset collected at a station and commercial building showed that our method achieved high response obligation detection accuracy and was able to handle domain change automatically",
    "checked": true,
    "id": "86cb67db13ac692d346908096b86231ed94fde1f",
    "semantic_title": "multimodal response obligation detection with unsupervised online domain adaptation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/su19c_interspeech.html": {
    "title": "Follow-Up Question Generation Using Neural Tensor Network-Based Domain Ontology Population in an Interview Coaching System",
    "volume": "main",
    "abstract": "This study proposes an approach to follow-up question generation based on a populated domain ontology in a conversational interview coaching system. The purpose of this study is to generate the follow-up questions which are more related to the meaning beyond the literal content in the user's answer based on the background knowledge in a populated domain ontology. Firstly, a convolutional neural tensor network (CNTN) was applied for selecting a key sentence from the user answer. Secondly, the neural tensor network (NTN) was used to model the relationship between the subjects and objects in the resource description framework (RDF) triple, defined as (subject, predicate, object), in each predicate from the ConceptNet for domain ontology population. The words in the key sentence were then used to retrieve relevant triples from the domain ontology for filling into the slots in the question templates to generate potential follow-up questions. Finally, the CNTN-based sentence matching model was employed to choose the one most related to the answer sentence as the final follow-up question. This study used 5-fold cross-validation for performance evaluation. The experimental results showed the generation performance in the proposed model was higher than the traditional method. The performance of key sentence selection model achieved 81.94%, and the sentence matching model achieved 92.28%",
    "checked": true,
    "id": "d5b982b146015038f56b0a4a395a14b640c91be9",
    "semantic_title": "follow-up question generation using neural tensor network-based domain ontology population in an interview coaching system",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tran19_interspeech.html": {
    "title": "On the Role of Style in Parsing Speech with Neural Models",
    "volume": "main",
    "abstract": "The differences in written text and conversational speech are substantial; previous parsers trained on treebanked text have given very poor results on spontaneous speech. For spoken language, the mismatch in style also extends to prosodic cues, though it is less well understood. This paper re-examines the use of written text in parsing speech in the context of recent advances in neural language processing. We show that neural approaches facilitate using written text to improve parsing of spontaneous speech, and that prosody further improves over this state-of-the-art result. Further, we find an asymmetric degradation from read vs. spontaneous mismatch, with spontaneous speech more generally useful for training parsers",
    "checked": true,
    "id": "6658f850d2d7d4fa899bf2c8da93fc5ef1bd00b6",
    "semantic_title": "on the role of style in parsing speech with neural models",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pasad19_interspeech.html": {
    "title": "On the Contributions of Visual and Textual Supervision in Low-Resource Semantic Speech Retrieval",
    "volume": "main",
    "abstract": "Recent work has shown that speech paired with images can be used to learn semantically meaningful speech representations even without any textual supervision. In real-world low-resource settings, however, we often have access to some transcribed speech. We study whether and how visual grounding is useful in the presence of varying amounts of textual supervision. In particular, we consider the task of semantic speech retrieval in a low-resource setting. We use a previously studied data set and task, where models are trained on images with spoken captions and evaluated on human judgments of semantic relevance. We propose a multitask learning approach to leverage both visual and textual modalities, with visual supervision in the form of keyword probabilities from an external tagger. We find that visual grounding is helpful even in the presence of textual supervision, and we analyze this effect over a range of sizes of transcribed data sets. With ~5 hours of transcribed speech, we obtain 23% higher average precision when also using visual supervision",
    "checked": true,
    "id": "441f16f3805f9cdf5cf1c8f3b429df5485a472d9",
    "semantic_title": "on the contributions of visual and textual supervision in low-resource semantic speech retrieval",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19p_interspeech.html": {
    "title": "Automatic Detection of Off-Topic Spoken Responses Using Very Deep Convolutional Neural Networks",
    "volume": "main",
    "abstract": "Test takers in high-stakes speaking assessments may try to inflate their scores by providing a response to a question that they are more familiar with instead of the question presented in the test; such a response is referred to as an off-topic spoken response. The presence of these responses can make it difficult to accurately evaluate a test taker's speaking proficiency, and thus may reduce the validity of assessment scores. This study aims to address this problem by building an automatic system to detect off-topic spoken responses which can inform the downstream automated scoring pipeline. We propose an innovative method to interpret the comparison between a test response and the question used to elicit it as a similarity grid, and then apply very deep convolutional neural networks to determine different degrees of topic relevance. In this study, Inception networks were applied to this task, and the experimental results demonstrate the effectiveness of the proposed method. Our system achieves an F1-score of 92.8% on the class of off-topic responses, which significantly outperforms a baseline system using a range of word embedding-based similarity metrics (F1-score = 85.5%)",
    "checked": true,
    "id": "3a5858ebe5b4903249365c770a288dc5e7855735",
    "semantic_title": "automatic detection of off-topic spoken responses using very deep convolutional neural networks",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/piunova19_interspeech.html": {
    "title": "Rescoring Keyword Search Confidence Estimates with Graph-Based Re-Ranking Using Acoustic Word Embeddings",
    "volume": "main",
    "abstract": "Postprocessing of confidence scores in keyword search (KWS) task is known to be an efficient way of improving retrieval performance. In this paper, we extend the existing graph-based re-ranking algorithm proposed for KWS score calibration. We replace the originally used Dynamic TimeWarping (DTW) distance measure between prospective hits with distances between their Acoustic Word Embeddings (AWEs) learned from Neural Networks. We argue that AWEs trained to discriminate between the same and different words should improve the graph-based re-ranking performance. Experimental results on two languages from IARPA Babel program show that our approach outperforms the DTW and improves the baseline KWS result between 3.0–7.5% relative on the Maximum Term Weighted Value (MTWV) measure. It was previously shown, that enhancing detection lists with keyword exemplars given high confidence, improved the algorithm performance. We additionally expanded the detection lists with negative query exemplars and observed further improvements in MTWV",
    "checked": true,
    "id": "b5879bd3e7d06f111f832e69bed8ba395d56fdc8",
    "semantic_title": "rescoring keyword search confidence estimates with graph-based re-ranking using acoustic word embeddings",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/segal19_interspeech.html": {
    "title": "SpeechYOLO: Detection and Localization of Speech Objects",
    "volume": "main",
    "abstract": "In this paper, we propose to apply object detection methods from the vision domain on the speech recognition domain, by treating audio fragments as objects. More specifically, we present SpeechYOLO, which is inspired by the YOLO algorithm [1] for object detection in images. The goal of SpeechYOLO is to localize boundaries of utterances within the input signal, and to correctly classify them. Our system is composed of a convolutional neural network, with a simple least-mean-squares loss function. We evaluated the system on several keyword spotting tasks, that include corpora of read speech and spontaneous speech. Our system compares favorably with other algorithms trained for both localization and classification",
    "checked": true,
    "id": "2efe16ee4b753989d29218b8094da8d5f9bd0aca",
    "semantic_title": "speechyolo: detection and localization of speech objects",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/oktem19_interspeech.html": {
    "title": "Prosodic Phrase Alignment for Machine Dubbing",
    "volume": "main",
    "abstract": "Dubbing is a type of audiovisual translation where dialogues are translated and enacted so that they give the impression that the media is in the target language. It requires a careful alignment of dubbed recordings with the lip movements of performers in order to achieve visual coherence. In this paper, we deal with the specific problem of prosodic phrase synchronization within the framework of machine dubbing. Our methodology exploits the attention mechanism output in neural machine translation to find plausible phrasing for the translated dialogue lines and then uses them to condition their synthesis. Our initial work in this field records comparable speech rate ratio to professional dubbing translation, and improvement in terms of lip-syncing of long dialogue lines",
    "checked": true,
    "id": "f13837b40b2f4ac2ea587995733b80c677545b15",
    "semantic_title": "prosodic phrase alignment for machine dubbing",
    "citation_count": 26
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tannander19_interspeech.html": {
    "title": "Spot the Pleasant People! Navigating the Cocktail Party Buzz",
    "volume": "main",
    "abstract": "We present an experimental platform for making voice likability assessments that are decoupled from individual voices, and instead capture voice characteristics over groups of speakers. We employ methods that we have previously used for other purposes to create the Cocktail platform, where respondents navigate in a voice buzz made up of about 400 voices on a touch screen. They then choose the location where they find the voice buzz most pleasant. Since there is no image or message on the screen, the platform can be used by visually impaired people, who often need to rely on spoken text, on the same premises as seeing people. In this paper, we describe the platform and its motivation along with our analysis method. We conclude by presenting two experiments in which we verify that the platform behaves as expected: one simple sanity test, and one experiment with voices grouped according to their mean pitch variance",
    "checked": true,
    "id": "822defa33f1f18a03eb7992bc18ccc9bba88be01",
    "semantic_title": "spot the pleasant people! navigating the cocktail party buzz",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19o_interspeech.html": {
    "title": "Neural Text Clustering with Document-Level Attention Based on Dynamic Soft Labels",
    "volume": "main",
    "abstract": "In this paper, the deep learning framework is applied in text clustering, an unsupervised task in natural language processing (NLP). Since there are no predefined labels available for text clustering, the deep neural network is trained in a pseudo-supervised fashion with labels generated from pre-clustering step. To address the wrong labelling problem from pre-clustering step, we adopt soft pseudo-labels instead of hard one-hot ones, and these labels are dynamically updated during training. Besides, we build a document-level attention over multiple documents based on dynamic soft pseudo-labels to further reduce the impact of the wrong labels. Experimental results on three public databases show that our model outperforms the state-of-the-art systems",
    "checked": true,
    "id": "e6d81d409c7c78c6e02003d298dbf2e5c2d6063c",
    "semantic_title": "neural text clustering with document-level attention based on dynamic soft labels",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bach19_interspeech.html": {
    "title": "Noisy BiLSTM-Based Models for Disfluency Detection",
    "volume": "main",
    "abstract": "This paper describes BiLSTM-based models to disfluency detection in speech transcripts using residual BiLSTM blocks, self-attention, and noisy training approach. Our best model not only surpasses BERT in 4 non-Switchboard test sets, but also is 20 times smaller than the BERT-based model [1]. Thus, we demonstrate that strong performance can be achieved without extensively use of very large training data. In addition, we show that it is possible to be robust across data sets with noisy training approach in which we found insertion is the most useful noise for augmenting training data",
    "checked": true,
    "id": "7036b6e09b71c08c3ebf5efe753ee672d42d3441",
    "semantic_title": "noisy bilstm-based models for disfluency detection",
    "citation_count": 34
  },
  "https://www.isca-speech.org/archive/interspeech_2019/singh19b_interspeech.html": {
    "title": "Subword RNNLM Approximations for Out-Of-Vocabulary Keyword Search",
    "volume": "main",
    "abstract": "In spoken Keyword Search, the query may contain out-of-vocabulary (OOV) words not observed when training the speech recognition system. Using subword language models (LMs) in the first-pass recognition makes it possible to recognize the OOV words, but even the subword n-gram LMs suffer from data sparsity. Recurrent Neural Network (RNN) LMs alleviate the sparsity problems but are not suitable for first-pass recognition as such. One way to solve this is to approximate the RNNLMs by back-off n-gram models. In this paper, we propose to interpolate the conventional n-gram models and the RNNLM approximation for better OOV recognition. Furthermore, we develop a new RNNLM approximation method suitable for subword units: It produces variable-order n-grams to include long-span approximations and considers also n-grams that were not originally observed in the training corpus. To evaluate these models on OOVs, we setup Arabic and Finnish Keyword Search tasks concentrating only on OOV words. On these tasks, interpolating the baseline RNNLM approximation and a conventional LM outperforms the conventional LM in terms of the Maximum TermWeighted Value for single-character subwords. Moreover, replacing the baseline approximation with the proposed method achieves the best performance on both multi- and single-character subwords",
    "checked": true,
    "id": "24f1ce798948cd6d6878ca2fc411e8e75af834fe",
    "semantic_title": "subword rnnlm approximations for out-of-vocabulary keyword search",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/maekaku19_interspeech.html": {
    "title": "Simultaneous Detection and Localization of a Wake-Up Word Using Multi-Task Learning of the Duration and Endpoint",
    "volume": "main",
    "abstract": "This paper proposes a novel method for simultaneous detection and localization of a wake-up word using multi-task learning of the duration and endpoint. An onset of the wake-up word is estimated by going back in time by an estimated duration of the wake-up word from an estimated endpoint. Accurate endpoint estimation is achieved by training the network to fire only at the endpoint in contrast to the entire wake-up word. The accurate endpoint naturally leads to an accurate onset, when it is used as a basis to calculate an onset with an estimated duration that reflects the whole acoustic information over the entire wake-up word. Experimental results with real-environment data show that a relative improvement in accuracy of 41% for onset estimation and 38% for endpoint estimation are achieved compared to a baseline method",
    "checked": true,
    "id": "e38ea9edc0d1edc406410fe5345f816d5e6ee2e2",
    "semantic_title": "simultaneous detection and localization of a wake-up word using multi-task learning of the duration and endpoint",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lee19e_interspeech.html": {
    "title": "On Mitigating Acoustic Feedback in Hearing Aids with Frequency Warping by All-Pass Networks",
    "volume": "main",
    "abstract": "Acoustic feedback control continues to be a challenging problem due to the emerging form factors in advanced hearing aids (HAs) and hearables. In this paper, we present a novel use of well-known all-pass filters in a network to perform frequency warping that we call \"freping.\" Freping helps in breaking the Nyquist stability criterion and improves adaptive feedback cancellation (AFC). Based on informal subjective assessments, distortions due to freping are fairly benign. While common objective metrics like the perceptual evaluation of speech quality (PESQ) and the hearing-aid speech quality index (HASQI) may not adequately capture distortions due to freping and acoustic feedback artifacts from a perceptual perspective, they are still instructive in assessing the proposed method. We demonstrate quality improvements with freping for a basic AFC (PESQ: 2.56 to 3.52 and HASQI: 0.65 to 0.78) at a gain setting of 20; and an advanced AFC (PESQ: 2.75 to 3.17 and HASQI: 0.66 to 0.73) for a gain of 30. From our investigations, freping provides larger improvement for basic AFC, but still improves overall system performance for many AFC approaches",
    "checked": true,
    "id": "363eaaab2d1f3c8deb522b966c088f5d5dbc05dd",
    "semantic_title": "on mitigating acoustic feedback in hearing aids with frequency warping by all-pass networks",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fazel19_interspeech.html": {
    "title": "Deep Multitask Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "Acoustic echo cancellation or suppression methods aim to suppress the echo originated from acoustic coupling between loudspeakers and microphones. Conventional approaches estimate echo using adaptive filtering. Due to the nonlinearities in the acoustic path of far-end signal, further post-processing is needed to attenuate these nonlinear components. In this paper, we propose a novel architecture based on deep gated recurrent neural networks to estimate the near-end signal from the microphone signal. The proposed architecture is trained using multitask learning to learn the auxiliary task of estimating the echo in order to improve the main task of estimating the clean near-end speech signal. Experimental results show that our proposed deep learning based method outperforms the existing methods for unseen speakers in terms of the echo return loss enhancement (ERLE) for single-talk periods and the perceptual evaluation of speech quality (PESQ) score for double-talk periods",
    "checked": true,
    "id": "a3d6ac19bc4ce6cf4ae4799d5c2f3dd587c47ffb",
    "semantic_title": "deep multitask acoustic echo cancellation",
    "citation_count": 30
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19o_interspeech.html": {
    "title": "Deep Learning for Joint Acoustic Echo and Noise Cancellation with Nonlinear Distortions",
    "volume": "main",
    "abstract": "We formulate acoustic echo and noise cancellation jointly as deep learning based speech separation, where near-end speech is separated from a single microphone recording and sent to the far end. We propose a causal system to address this problem, which incorporates a convolutional recurrent network (CRN) and a recurrent network with long short-term memory (LSTM). The system is trained to estimate the real and imaginary spectrograms of near-end speech and detect the activity of near-end speech from the microphone signal and far-end signal. Subsequently, the estimated real and imaginary spectrograms are used to separate the near-end signal, hence removing echo and noise. The trained near-end speech detector is employed to further suppress residual echo and noise. Evaluation results show that the proposed method effectively removes acoustic echo and background noise in the presence of nonlinear distortions for both simulated and measured room impulse responses (RIRs). Additionally, the proposed method generalizes well to untrained noises, RIRs and speakers",
    "checked": true,
    "id": "be7325c9a9f9be6cd4ce62b0ba53b40a870e38b5",
    "semantic_title": "deep learning for joint acoustic echo and noise cancellation with nonlinear distortions",
    "citation_count": 50
  },
  "https://www.isca-speech.org/archive/interspeech_2019/srensen19_interspeech.html": {
    "title": "Harmonic Beamformers for Non-Intrusive Speech Intelligibility Prediction",
    "volume": "main",
    "abstract": "In recent years, research into objective speech intelligibility measures has gained increased interest as a tool to optimize speech enhancement algorithms. While most intelligibility measures are intrusive, i.e., they require a clean reference signal, this is rarely available in real-time applications. This paper proposes two non-intrusive intelligibility measures, which allow using the intrusive short-time objective intelligibility (STOI) measure without requiring access to the clean signal. Instead, a reference signal is obtained from the degraded signal using either a fixed or an adaptive harmonic spatial filter. This reference signal is then used as input to STOI. The experimental results show a high correlation between both proposed non-intrusive speech intelligibility measures and the original intrusively computed STOI scores",
    "checked": true,
    "id": "72bfb5487f177f0269b2216ded0c8bb2a768a712",
    "semantic_title": "harmonic beamformers for non-intrusive speech intelligibility prediction",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mamun19b_interspeech.html": {
    "title": "Convolutional Neural Network-Based Speech Enhancement for Cochlear Implant Recipients",
    "volume": "main",
    "abstract": "Attempts to develop speech enhancement algorithms with improved speech intelligibility for cochlear implant (CI) users have met with limited success. To improve speech enhancement methods for CI users, we propose to perform speech enhancement in a cochlear filter-bank feature space, a feature-set specifically designed for CI users based on CI auditory stimuli. We leverage a convolutional neural network (CNN) to extract both stationary and non-stationary components of environmental acoustics and speech. We propose three CNN architectures: (1) vanilla CNN that directly generates the enhanced signal; (2) spectral-subtraction-style CNN (SS-CNN) that first predicts noise and then generates the enhanced signal by subtracting noise from the noisy signal; (3) Wiener-style CNN (Wiener-CNN) that generates an optimal mask for suppressing noise. An important problem of the proposed networks is that they introduce considerable delays, which limits their real-time application for CI users. To address this, this study also considers causal variations of these networks. Our experiments show that the proposed networks (both causal and non-causal forms) achieve significant improvement over existing baseline systems. We also found that causal Wiener-CNN outperforms other networks, and leads to the best overall envelope coefficient measure (ECM). The proposed algorithms represent a viable option for implementation on the CCi-MOBILE research platform as a pre-processor for CI users in naturalistic environments",
    "checked": true,
    "id": "ec1cc4ce32e9aa5c66e1190c558646b42b9eee68",
    "semantic_title": "convolutional neural network-based speech enhancement for cochlear implant recipients",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2019/srensen19b_interspeech.html": {
    "title": "Validation of the Non-Intrusive Codebook-Based Short Time Objective Intelligibility Metric for Processed Speech",
    "volume": "main",
    "abstract": "In recent years, objective measures of speech intelligibility have gained increasing interest. However, most speech intelligibility metrics require a clean reference signal, which is often not available in real-life applications. In a recent publication, we proposed a method, the Non-Intrusive Codebook-based Short-Time Objective Intelligibility (NIC-STOI) metric, which allows using an intrusive method without requiring access to the clean signal. The statistics of the reference signal is estimated as a combination of predefined codebooks that best fit the degraded signal by modeling the speech and noisy spectra. In this paper, we perform additional validation of the NIC-STOI in more diverse noise condition as well as for speech processed non-linearly with binary masks, where it is shown to outperform existing non-intrusive metrics",
    "checked": true,
    "id": "caae195932929350bbb56ecd3d203c92fda011e1",
    "semantic_title": "validation of the non-intrusive codebook-based short time objective intelligibility metric for processed speech",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/arai19_interspeech.html": {
    "title": "Predicting Speech Intelligibility of Enhanced Speech Using Phone Accuracy of DNN-Based ASR System",
    "volume": "main",
    "abstract": "The ability of state-of-the-art automatic speech recognition (ASR) systems, which use deep neural networks (DNN), has recently been approaching that of human auditory systems. On the other hand, although measuring the intelligibility of enhanced speech signals is important for developing auditory algorithms and devices, the current measurement methods mainly rely on subjective experiments. Therefore, it would be preferable to employ an ASR system to predict the subjective speech intelligibility (SI) of enhanced speech. In this study, we evaluate the SI prediction performance of DNN-based ASR systems using phone accuracies. We found that an ASR system with multi-condition training achieves the best SI prediction accuracy for enhanced speech when compared with conventional methods (STOI, HASPI) and a recently proposed technique (GEDI). In addition, since our ASR system uses only a phone language model, it offers the advantage of being able to predict intelligibility independently of prior knowledge of words",
    "checked": true,
    "id": "2b58b4946a2f47b4d9c669d0dc73f87cc5897b50",
    "semantic_title": "predicting speech intelligibility of enhanced speech using phone accuracy of dnn-based asr system",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bu19_interspeech.html": {
    "title": "A Novel Method to Correct Steering Vectors in MVDR Beamformer for Noise Robust ASR",
    "volume": "main",
    "abstract": "Accurate steering vectors (SV) are key to many beamformers. However, reliable SV is not easy to obtain. In this work, we investigate a novel method to identify and correct phase errors in SV for MVDR beamforming. Our idea stems from the linear relationship in the phase of a microphone component in narrowband SVs across frequency, as modeled by acoustic transfer function. We utilize this property and feedforward neural nets to make phase prediction for the microphone components in SVs, and use the predicted phase selectively for phase error correction and MVDR beamforming. Our method is robust to large fluctuations in phase spectrum wrapped within [-π, π]. We have evaluated our approach on CHiME-3 and obtained improved performances on both word error rate and short-time objective intelligibility in low reverberant acoustic environments",
    "checked": true,
    "id": "b80f466cbc360011c791a520eb0b44d3e405ed35",
    "semantic_title": "a novel method to correct steering vectors in mvdr beamformer for noise robust asr",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lee19f_interspeech.html": {
    "title": "End-to-End Multi-Channel Speech Enhancement Using Inter-Channel Time-Restricted Attention on Raw Waveform",
    "volume": "main",
    "abstract": "This paper describes a novel waveform-level end-to-end model for multi-channel speech enhancement. The model first extracts sample-level speech embedding using channel-wise convolutional neural network (CNN) and compensates time-delays between the channels based on the embedding, resulting in time-aligned multi-channel signals. Then the signals are given as input of multi-channel enhancement extension of WaveUNet which directly outputs estimated clean speech waveform. The whole model is trained to minimize modified mean squared error (MSE), signal-to-distortion ratio (SDR) cost, and senone cross-entropy of back-end acoustic model at the same time. Evaluated on the CHiME-4 simulated set, the proposed system outperformed state-of-the-art generalized eigenvalue (GEV) beamformer in terms of perceptual evaluation of speech quality (PESQ) and SDR, and showed competitive results in short time objective intelligibility (STOI). Word-error-rates (WERs) of the system's output on simulated sets were comparable to that of bidirectional long short-term memory (BLSTM) GEV beamformer. However, the system showed relatively high WERs on real sets, achieving relative error rate reduction (RERR) of 14.3% over noisy signal on real evaluation set",
    "checked": true,
    "id": "7dacc70a044c93ece0a3e04ddf58b19005b9e185",
    "semantic_title": "end-to-end multi-channel speech enhancement using inter-channel time-restricted attention on raw waveform",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gu19b_interspeech.html": {
    "title": "Neural Spatial Filter: Target Speaker Speech Separation Assisted with Directional Information",
    "volume": "main",
    "abstract": "The recent exploration of deep learning for supervised speech separation has significantly accelerated the progress on the multi-talker speech separation problem. The multi-channel approaches have attracted much research attention due to the benefit of spatial information. In this paper, integrated with the power spectra and inter-channel spatial features at the input level, we explore to leverage directional features, which imply the speaker source from the desired target direction, for target speaker separation. In addition, we incorporate an attention mechanism to dynamically tune the model's attention to the reliable input features to alleviate spatial ambiguity problem when multiple speakers are closely located. We demonstrate, on the far-field WSJ0 2-mix dataset, that our proposed approach significantly improves the performance of speech separation against the baseline single-channel and multi-channel speech separation methods",
    "checked": true,
    "id": "87832bdd2ea7baf3eed1467b0b256918945db8f7",
    "semantic_title": "neural spatial filter: target speaker speech separation assisted with directional information",
    "citation_count": 66
  },
  "https://www.isca-speech.org/archive/interspeech_2019/afouras19_interspeech.html": {
    "title": "My Lips Are Concealed: Audio-Visual Speech Enhancement Through Obstructions",
    "volume": "main",
    "abstract": "Our objective is an audio-visual model for separating a single speaker from a mixture of sounds such as other speakers and background noise. Moreover, we wish to hear the speaker even when the visual cues are temporarily absent due to occlusion To this end we introduce a deep audio-visual speech enhancement network that is able to separate a speaker's voice by conditioning on both the speaker's lip movements and/or a representation of their voice. The voice representation can be obtained by either (i) enrollment, or (ii) by self-enrollment — learning the representation on-the-fly given sufficient unobstructed visual input. The model is trained by blending audios, and by introducing artificial occlusions around the mouth region that prevent the visual modality from dominating The method is speaker-independent, and we demonstrate it on real examples of speakers unheard (and unseen) during training. The method also improves over previous models in particular for cases of occlusion in the visual modality",
    "checked": true,
    "id": "7473594725d668a4cb174a86dbcba7c2148a598e",
    "semantic_title": "my lips are concealed: audio-visual speech enhancement through obstructions",
    "citation_count": 65
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fujita19_interspeech.html": {
    "title": "End-to-End Neural Speaker Diarization with Permutation-Free Objectives",
    "volume": "main",
    "abstract": "In this paper, we propose a novel end-to-end neural-network-based speaker diarization method. Unlike most existing methods, our proposed method does not have separate modules for extraction and clustering of speaker representations. Instead, our model has a single neural network that directly outputs speaker diarization results. To realize such a model, we formulate the speaker diarization problem as a multi-label classification problem, and introduces a permutation-free objective function to directly minimize diarization errors without being suffered from the speaker-label permutation problem. Besides its end-to-end simplicity, the proposed method also benefits from being able to explicitly handle overlapping speech during training and inference. Because of the benefit, our model can be easily trained/adapted with real-recorded multi-speaker conversations just by feeding the corresponding multi-speaker segment labels. We evaluated the proposed method on simulated speech mixtures. The proposed method achieved diarization error rate of 12.28%, while a conventional clustering-based system produced diarization error rate of 28.77%. Furthermore, the domain adaptation with real-recorded speech provided 25.6% relative improvement on the CALLHOME dataset",
    "checked": true,
    "id": "d9dbdd254b02ef1af2769af403cba373c1b1bcb1",
    "semantic_title": "end-to-end neural speaker diarization with permutation-free objectives",
    "citation_count": 151
  },
  "https://www.isca-speech.org/archive/interspeech_2019/india19_interspeech.html": {
    "title": "Self Multi-Head Attention for Speaker Recognition",
    "volume": "main",
    "abstract": "Most state-of-the-art Deep Learning (DL) approaches for speaker recognition work on a short utterance level. Given the speech signal, these algorithms extract a sequence of speaker embeddings from short segments and those are averaged to obtain an utterance level speaker representation. In this work we propose the use of an attention mechanism to obtain a discriminative speaker embedding given non fixed length speech utterances. Our system is based on a Convolutional Neural Network (CNN) that encodes short-term speaker features from the spectrogram and a self multi-head attention model that maps these representations into a long-term speaker embedding. The attention model that we propose produces multiple alignments from different subsegments of the CNN encoded states over the sequence. Hence this mechanism works as a pooling layer which decides the most discriminative features over the sequence to obtain an utterance level representation. We have tested this approach for the verification task for the VoxCeleb1 dataset. The results show that self multi-head attention outperforms both temporal and statistical pooling methods with a 18% of relative EER. Obtained results show a 58% relative improvement in EER compared to i-vector+PLDA",
    "checked": true,
    "id": "ff6107328ea0b890efd8c4a10b16fd40e2511e8d",
    "semantic_title": "self multi-head attention for speaker recognition",
    "citation_count": 88
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vinals19b_interspeech.html": {
    "title": "Phonetically-Aware Embeddings, Wide Residual Networks with Time-Delay Neural Networks and Self Attention Models for the 2018 NIST Speaker Recognition Evaluation",
    "volume": "main",
    "abstract": "Very often, speaker recognition systems do not take into account phonetic information explicitly. In order to gain insight along this line of research, we have studied the use of phonetic information in the embedding extraction process for automatic speaker verification systems in two different ways: on the one hand using the well-known i-vector paradigm and, on the other hand, using Wide Residual Networks (WRN) with Time Delay Neural Networks (TDNN) and Self-Attention Mechanisms. The phonetic information is provided by a WRN with TDNN using 1D convolutional layers specifically trained for this purpose. These two approaches along with the widely used x-vector system based on the Kaldi toolkit were submitted to the 2018 NIST speaker recognition evaluation. As back-end, these representations used a standard PLDA classifier with ad-hoc configurations for each system and in-domain adaptation. The results obtained in the NIST SRE 2018 show that our methods are very promising and it is worth continuing to work on them to improve their performance",
    "checked": true,
    "id": "9bdc4abc55b97c4ee13c280a5c197cf897b900c8",
    "semantic_title": "phonetically-aware embeddings, wide residual networks with time-delay neural networks and self attention models for the 2018 nist speaker recognition evaluation",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tu19_interspeech.html": {
    "title": "Variational Domain Adversarial Learning for Speaker Verification",
    "volume": "main",
    "abstract": "Domain mismatch refers to the problem in which the distribution of training data differs from that of the test data. This paper proposes a variational domain adversarial neural network (VDANN), which consists of a variational autoencoder (VAE) and a domain adversarial neural network (DANN), to reduce domain mismatch. The DANN part aims to retain speaker identity information and learn a feature space that is robust against domain mismatch, while the VAE part is to impose variational regularization on the learned features so that they follow a Gaussian distribution. Thus, the representation produced by VDANN is not only speaker discriminative and domain-invariant but also Gaussian distributed, which is essential for the standard PLDA backend. Experiments on both SRE16 and SRE18-CMN2 show that VDANN outperforms the Kaldi baseline and the standard DANN. The results also suggest that VAE regularization is effective for domain adaptation",
    "checked": true,
    "id": "dbed7c45d99d5e3a6b431b8a48bdb0a850d7bb4e",
    "semantic_title": "variational domain adversarial learning for speaker verification",
    "citation_count": 29
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19m_interspeech.html": {
    "title": "A Unified Framework for Speaker and Utterance Verification",
    "volume": "main",
    "abstract": "Speaker and utterance verification are two tasks that co-exist in text-dependent speaker verification (SV), where a phrase of the same lexical information is spoken during train and test sessions. The conventional approaches mostly verify the speaker and the utterance separately using two models. While there are studies on joint modeling of speaker and utterance, it is always desirable to have a common framework that performs both speaker and utterance verification to access the intended service. To this end, we propose a unified framework that deals with both objectives and the trade-off between the two. The unified framework is based on long short term memory network trained using both speaker and utterance information. We use Part I of RSR2015 database for the studies in this work. We show that the unified framework not only demonstrates competitive SV performance, but also provides a solution for a system to address different levels of security need",
    "checked": true,
    "id": "bbf6089202c0e1d5f8678655d4a7a5a5189ab1b0",
    "semantic_title": "a unified framework for speaker and utterance verification",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nandwana19c_interspeech.html": {
    "title": "Analysis of Critical Metadata Factors for the Calibration of Speaker Recognition Systems",
    "volume": "main",
    "abstract": "In this paper, we analyze and assess the impact of critical metadata factors on the calibration performance of speaker recognition systems. In particular, we study the effect of duration, distance, language, and gender by using a variety of datasets and systematically varying the conditions in the evaluation and calibration sets. For all experiments, the system is based on i-vectors and a probabilistic linear discriminant analysis (PLDA) back-end and linear calibration. We measure system performance in terms of calibration loss. Our experiments reveal (i) a large degradation when the duration used for calibration is significantly different from that in the evaluation set; (ii) no significant degradation when a different gender is used for calibration than for evaluation; (iii) a large degradation when microphone distance is significantly different between the sets; and (iv) a small loss for closely related languages and languages with shared vocabulary. This analysis will be beneficial in the development of speaker recognition systems for use in unseen environments and for forensic speaker recognition analysts when selecting relevant population data",
    "checked": true,
    "id": "80c4173636ad10b7140f4eca170c3c9719e0ea53",
    "semantic_title": "analysis of critical metadata factors for the calibration of speaker recognition systems",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/novotny19_interspeech.html": {
    "title": "Factorization of Discriminatively Trained i-Vector Extractor for Speaker Recognition",
    "volume": "main",
    "abstract": "In this work, we continue in our research on i-vector extractor for speaker verification (SV) and we optimize its architecture for fast and effective discriminative training. We were motivated by computational and memory requirements caused by the large number of parameters of the original generative i-vector model. Our aim is to preserve the power of the original generative model, and at the same time focus the model towards extraction of speaker-related information. We show that it is possible to represent a standard generative i-vector extractor by a model with significantly less parameters and obtain similar performance on SV tasks. We can further refine this compact model by discriminative training and obtain i-vectors that lead to better performance on various SV benchmarks representing different acoustic domains",
    "checked": true,
    "id": "5ad9f76df46dba8a7797825dcd7b3b8aab305461",
    "semantic_title": "factorization of discriminatively trained i-vector extractor for speaker recognition",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/salvati19_interspeech.html": {
    "title": "End-to-End Speaker Identification in Noisy and Reverberant Environments Using Raw Waveform Convolutional Neural Networks",
    "volume": "main",
    "abstract": "Convolutional neural network (CNN) models are being investigated extensively in the field of speech and speaker recognition, and are rapidly gaining appreciation due to their performance robustness and effective training strategies. Recently, they are also providing interesting results in end-to-end configurations using directly raw waveforms for classification, with the drawback however of being more sensible on the amount of training data. We present a raw waveform (RW) end-to-end computational scheme for speaker identification based on CNNs with noise and reverberation data augmentation (DA). The CNN is designed for a frame-to-frame analysis to handle variable-length signals. We analyze the identification performance with simulated experiments in noisy and reverberation conditions comparing the proposed RW-CNN with the mel-frequency cepstral coefficients (MFCCs) features. The results show that the method offers robustness to adverse conditions. The RW-CNN outperforms the MFCC-CNN in noise conditions, and they have similar performance in reverberant environments",
    "checked": true,
    "id": "dd71d9c497209e2f537a880ab1676f8d6f86cb4f",
    "semantic_title": "end-to-end speaker identification in noisy and reverberant environments using raw waveform convolutional neural networks",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/naini19_interspeech.html": {
    "title": "Whisper to Neutral Mapping Using Cosine Similarity Maximization in i-Vector Space for Speaker Verification",
    "volume": "main",
    "abstract": "In this work, we propose a novel feature mapping (FM) from whispered to neutral speech features using a cosine similarity based objective function for speaker verification (SV) using whispered speech. Typically the performance of an SV system enrolled with neutral speech degrades significantly when tested using whispered speech, due to the differences between spectral characteristics of neutral and whispered speech. We hypothesize that FM from whispered Mel frequency cepstral coefficients (MFCC) to neutral MFCC by maximizing cosine similarity between neutral and whisper i-vectors yields better performance than the baseline method, which typically performs a direct FM between MFCC features by minimizing mean squared error (MSE). We also explored an affine transform between MFCC features using the proposed objective function. Whisper SV experiments with 1882 speakers reveal that the equal error rate (EER) using the proposed method is lower than that using the best baseline by ~24% (relative). We show that the proposed FM system maintains the neutral SV performance, while improving the EER of whispered SV unlike baseline methods. We also show that the bias in the learned affine transform is corresponds to the glottal flow information, which is absent in the whispered speech",
    "checked": true,
    "id": "cfd94832c422074cab3c4408cf65ec0e45c6f445",
    "semantic_title": "whisper to neutral mapping using cosine similarity maximization in i-vector space for speaker verification",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhu19b_interspeech.html": {
    "title": "Mixup Learning Strategies for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "Mixup is a learning strategy that constructs additional virtual training samples from existing training samples by linearly interpolating random pairs of them. It has been shown that mixup can help avoid data memorization and thus improve model generalization. This paper investigates the mixup learning strategy in training speaker-discriminative deep neural network (DNN) for better text-independent speaker verification In recent speaker verification systems, a DNN is usually trained to classify speakers in the training set. The DNN, at the same time, learns a low-dimensional embedding of speakers so that speaker embeddings can be generated for any speakers during evaluation. We adapted the mixup strategy to the speaker-discriminative DNN training procedure, and studied different mixup schemes, such as performing mixup on MFCC features or raw audio samples. The mixup learning strategy was evaluated on NIST SRE 2010, 2016 and SITW evaluation sets. Experimental results show consistent performance improvements both in terms of EER and DCF of up to 13% relative. We further find that mixup training also improves the DNN's speaker classification accuracy consistently without requiring any additional data sources",
    "checked": true,
    "id": "0bc3f8c6bc1f3568aac96d3ad0632ebe41134611",
    "semantic_title": "mixup learning strategies for text-independent speaker verification",
    "citation_count": 27
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ferrer19_interspeech.html": {
    "title": "Optimizing a Speaker Embedding Extractor Through Backend-Driven Regularization",
    "volume": "main",
    "abstract": "State-of-the-art speaker verification systems use deep neural networks (DNN) to extract highly discriminant representations of the samples, commonly called speaker embeddings. The networks are trained to maximize the cross-entropy between the estimated posteriors and the speaker labels. The pre-activations from one of the last layers in that network are used as embeddings. These sample-level vectors are then used as input to a backend that generates the final scores. The most successful backend for speaker verification to date is the probabilistic linear discriminant analysis (PLDA) backend. The full process consists of a linear discriminant analysis (LDA) projection of the embeddings, followed by mean and length normalization, ending with PLDA for score computation. While this procedure works very well compared to other approaches, it seems to be inherently suboptimal since the embeddings extractor is not directly trained to optimize the performance of the embeddings when using the PLDA backend for scoring. In this work, we propose one way to encourage the DNN to generate embeddings that are optimized for use in the PLDA backend, by adding a secondary objective designed to measure the performance of a such backend within the network. We show modest but consistent gains across several speaker recognition datasets",
    "checked": true,
    "id": "1a3b5c42e3ff256270082f9b214401413e19b435",
    "semantic_title": "optimizing a speaker embedding extractor through backend-driven regularization",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lee19g_interspeech.html": {
    "title": "The NEC-TT 2018 Speaker Verification System",
    "volume": "main",
    "abstract": "This paper describes the NEC-TT speaker verification system for the 2018 NIST speaker recognition evaluation (SRE'18). We present the details of data partitioning, x-vector speaker embedding, data augmentation, speaker diarization, and domain adaptation techniques used in NEC-TT SRE'18 speaker verification system. For the speaker embedding front-end, we found that the amount and diversity of training data are essential to improve the robustness of the x-vector extractor. This was achieved with data augmentation and mixed-bandwidth training in our submission. For the multi-speaker test scenario, we show that x-vector based speaker diarization is promising and holds potential for future research. For the scoring back-end, we used two variants of probabilistic linear discriminant analysis (PLDA), namely, the Gaussian PLDA and heavy-tailed PLDA. We show that correlation alignment (CORAL) and CORAL+ unsupervised PLDA adaptation are effective to deal with domain mismatch",
    "checked": true,
    "id": "6c72e2e7155e165de9b0e6a91a60d4591cf1f685",
    "semantic_title": "the nec-tt 2018 speaker verification system",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zheng19c_interspeech.html": {
    "title": "Autoencoder-Based Semi-Supervised Curriculum Learning for Out-of-Domain Speaker Verification",
    "volume": "main",
    "abstract": "This study aims to improve the performance of speaker verification system when no labeled out-of-domain data is available. An autoencoder-based semi-supervised curriculum learning scheme is proposed to automatically cluster unlabeled data and iteratively update the corpus during training. This new training scheme allows us to (1) progressively expand the size of training corpus by utilizing unlabeled data and correcting previous labels at run-time; and (2) improve robustness when generalizing to multiple conditions, such as out-of-domain and text-independent speaker verification tasks. It is also discovered that a denoising autoencoder can significantly enhance the clustering accuracy when it is trained on carefully-selected subset of speakers. Our experimental results show a relative reduction of 30%–50% in EER compared to the baseline",
    "checked": true,
    "id": "f9a2f01cb16c92f476636ca51578beda52ef330a",
    "semantic_title": "autoencoder-based semi-supervised curriculum learning for out-of-domain speaker verification",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cai19d_interspeech.html": {
    "title": "Multi-Channel Training for End-to-End Speaker Recognition Under Reverberant and Noisy Environment",
    "volume": "main",
    "abstract": "Despite the significant improvements in speaker recognition enabled by deep neural networks, unsatisfactory performance persists under far-field scenarios due to the effects of the long range fading, room reverberation, and environmental noises. In this study, we focus on far-field speaker recognition with a microphone array. We propose a multi-channel training framework for the deep speaker embedding neural network on noisy and reverberant data. The proposed multi-channel training framework simultaneously processes the time-, frequency- and channel-information to learn a robust deep speaker embedding. Based on the 2-dimensional or 3-dimensional convolution layer, we investigate different multi-channel training schemes. Experiments on the simulated multi-channel reverberant and noisy data show that the proposed method obtains significant improvements over the single-channel trained deep speaker embedding system with front end speech enhancement or multi-channel embedding fusion",
    "checked": true,
    "id": "b4b92cfa6f399b4383e484fc66ec22c9082c53b0",
    "semantic_title": "multi-channel training for end-to-end speaker recognition under reverberant and noisy environment",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cai19e_interspeech.html": {
    "title": "The DKU-SMIIP System for NIST 2018 Speaker Recognition Evaluation",
    "volume": "main",
    "abstract": "In this paper, we present the system submission for the NIST 2018 Speaker Recognition Evaluation by DKU Speech and Multi-Modal Intelligent Information Processing (SMIIP) Lab. We explore various kinds of state-of-the-art front-end extractors as well as back-end modeling for text-independent speaker verifications. Our submitted primary systems employ multiple state-of-the-art front-end extractors, including the MFCC i-vector, the DNN tandem i-vector, the TDNN x-vector, and the deep ResNet. After speaker embedding is extracted, we exploit several kinds of back-end modeling to perform variability compensation and domain adaptation for mismatch training and testing conditions. The final submitted system on the fixed condition obtains actual detection cost of 0.392 and 0.494 on CMN2 and VAST evaluation data respectively. After the official evaluation, we further extend our experiments by investigating multiple encoding layer designs and loss functions for the deep ResNet system",
    "checked": true,
    "id": "1464aa745fb1c46bc82808c12244510b42fba0c4",
    "semantic_title": "the dku-smiip system for nist 2018 speaker recognition evaluation",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wiesner19_interspeech.html": {
    "title": "Pretraining by Backtranslation for End-to-End ASR in Low-Resource Settings",
    "volume": "main",
    "abstract": "We explore training attention-based encoder-decoder ASR in low-resource settings. These models perform poorly when trained on small amounts of transcribed speech, in part because they depend on having sufficient target-side text to train the attention and decoder networks. In this paper we address this shortcoming by pretraining our network parameters using only text-based data and transcribed speech from other languages. We analyze the relative contributions of both sources of data. Across 3 test languages, our text-based approach resulted in a 20% average relative improvement over a text-based augmentation technique without pretraining. Using transcribed speech from nearby languages gives a further 20–30% relative reduction in character error rate",
    "checked": true,
    "id": "bd8922f8cc8284553dc9e6db529af309298451fe",
    "semantic_title": "pretraining by backtranslation for end-to-end asr in low-resource settings",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kim19b_interspeech.html": {
    "title": "Cross-Attention End-to-End ASR for Two-Party Conversations",
    "volume": "main",
    "abstract": "We present an end-to-end speech recognition model that learns interaction between two speakers based on the turn-changing information. Unlike conventional speech recognition models, our model exploits two speakers history of conversational-context information that spans across multiple turns within an end-to-end framework. Specifically, we propose a speaker-specific cross-attention mechanism that can look at the output of the other speaker side as well as the one of the current speaker for better at recognizing long conversations. We evaluated the models on the Switchboard conversational speech corpus and show that our model outperforms standard end-to-end speech recognition models",
    "checked": true,
    "id": "ddf68a01ea4aec3c939ff4a027dc48c59a7b8f64",
    "semantic_title": "cross-attention end-to-end asr for two-party conversations",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chorowski19_interspeech.html": {
    "title": "Towards Using Context-Dependent Symbols in CTC Without State-Tying Decision Trees",
    "volume": "main",
    "abstract": "Deep neural acoustic models benefit from context-dependent (CD) modeling of output symbols. We consider direct training of CTC networks with CD outputs, and identify two issues. The first one is frame-level normalization of probabilities in CTC, which induces strong language modeling behavior that leads to overfitting and interference with external language models. The second one is poor generalization in the presence of numerous lexical units like triphones or tri-chars. We mitigate the former with utterance-level normalization of probabilities. The latter typically requires reducing the CD symbol inventory with state-tying decision trees, which have to be transferred from classical GMM-HMM systems. We replace the trees with a CD symbol embedding network, which saves parameters and ensures generalization to unseen and undersampled CD symbols. The embedding network is trained together with the rest of the acoustic model and removes one of the last cases in which neural systems have to be bootstrapped from GMM-HMM ones",
    "checked": true,
    "id": "dfa4f23107099ad00a0ef4c33453b9b89356bae8",
    "semantic_title": "towards using context-dependent symbols in ctc without state-tying decision trees",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fan19b_interspeech.html": {
    "title": "An Online Attention-Based Model for Speech Recognition",
    "volume": "main",
    "abstract": "Attention-based end-to-end models such as Listen, Attend and Spell (LAS), simplify the whole pipeline of traditional automatic speech recognition (ASR) systems and become popular in the field of speech recognition. In previous work, researchers have shown that such architectures can acquire comparable results to state-of-the-art ASR systems, especially when using a bidirectional encoder and global soft attention (GSA) mechanism. However, bidirectional encoder and GSA are two obstacles for real-time speech recognition. In this work, we aim to stream LAS baseline by removing the above two obstacles. On the encoder side, we use a latency-controlled (LC) bidirectional structure to reduce the delay of forward computation. Meanwhile, an adaptive monotonic chunk-wise attention (AMoChA) mechanism is proposed to replace GSA for the calculation of attention weight distribution. Furthermore, we propose two methods to alleviate the huge performance degradation when combining LC and AMoChA. Finally, we successfully acquire an online LAS model, LC-AMoChA, which has only 3.5% relative performance reduction to LAS baseline on our internal Mandarin corpus",
    "checked": true,
    "id": "0e695684e3b7a14d7b4f68d4c0722a9b5e7591e3",
    "semantic_title": "an online attention-based model for speech recognition",
    "citation_count": 41
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tian19b_interspeech.html": {
    "title": "Self-Attention Transducers for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "Recurrent neural network transducers (RNN-T) have been successfully applied in end-to-end speech recognition. However, the recurrent structure makes it difficult for parallelization. In this paper, we propose a self-attention transducer (SA-T) for speech recognition. RNNs are replaced with self-attention blocks, which are powerful to model long-term dependencies inside sequences and able to be efficiently parallelized. Furthermore, a path-aware regularization is proposed to assist SA-T to learn alignments and improve the performance. Additionally, a chunk-flow mechanism is utilized to achieve online decoding. All experiments are conducted on a Mandarin Chinese dataset AISHELL-1. The results demonstrate that our proposed approach achieves a 21.3% relative reduction in character error rate compared with the baseline RNN-T. In addition, the SA-T with chunk-flow mechanism can perform online decoding with only a little degradation of the performance",
    "checked": true,
    "id": "ed62ba897b7aca6f0a54ed398fe0ac76c525b1b9",
    "semantic_title": "self-attention transducers for end-to-end speech recognition",
    "citation_count": 63
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19u_interspeech.html": {
    "title": "Improving Transformer-Based Speech Recognition Systems with Compressed Structure and Speech Attributes Augmentation",
    "volume": "main",
    "abstract": "The end-to-end (E2E) model allows for training of automatic speech recognition (ASR) systems without having to consider the acoustic model, lexicon, language model and complicated decoding algorithms, which are integral to conventional ASR systems. Recently, the transformer-based E2E ASR model (ASR-Transformer) showed promising results in many speech recognition tasks. The most common practice is to stack a number of feed-forward layers in the encoder and decoder. As a result, the addition of new layers improves speech recognition performance significantly. However, this also leads to a large increase in the number of parameters and severe decoding latency. In this paper, we propose to reduce the model complexity by simply reusing parameters across all stacked layers instead of introducing new parameters per layer. In order to address the slight reduction in recognition quality we propose to augment the speech inputs with bags-of-attributes. As a result we obtain a highly compressed, efficient and high quality ASR model",
    "checked": true,
    "id": "8bfc683672232551c2664233e8e6fae76c2a6260",
    "semantic_title": "improving transformer-based speech recognition systems with compressed structure and speech attributes augmentation",
    "citation_count": 41
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bang19_interspeech.html": {
    "title": "Extending an Acoustic Data-Driven Phone Set for Spontaneous Speech Recognition",
    "volume": "main",
    "abstract": "In this paper, we propose a method to extend a phone set by using a large amount of Korean broadcast data to improve the performance of spontaneous speech recognition. The proposed method first extracts variable-length phoneme-level segments from broadcast data, and then converts them into fixed-length latent vectors based on an LSTM architecture. Then, we used the k-means algorithm to cluster acoustically similar latent vectors and then build a new phone set by gathering the clustered vectors. To update the lexicon of a speech recognizer, we choose the pronunciation sequence of each word with the highest conditional probability. To verify the performance of the proposed unit, we visualize the spectral patterns and segment duration for the new phone set. In both spontaneous and read speech recognition tasks, the proposed unit is shown to produce better performance than the phoneme-based and grapheme-based units",
    "checked": true,
    "id": "f5f1b39f9d570234274b1e5240c7680314988cf0",
    "semantic_title": "extending an acoustic data-driven phone set for spontaneous speech recognition",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/moriya19_interspeech.html": {
    "title": "Joint Maximization Decoder with Neural Converters for Fully Neural Network-Based Japanese Speech Recognition",
    "volume": "main",
    "abstract": "We present a novel fully neural network (FNN) -based automatic speech recognition (ASR) system that addresses the out-of-vocabulary (OOV) problem. The most common approach to the OOV problem is leveraging character/sub-word level units as output symbols. Unfortunately, this approach is not suitable for Japanese and Mandarin Chinese since they have many more grapheme sets than English. Our solution is to develop FNN-based ASR that uses a pronunciation-based unit set with dictionaries, i.e., word-to-pronunciation rules. A previous study proposed, for Mandarin Chinese, a greedy cascading decoder (GCD) that uses two neural converters, acoustic-to-pronunciation (A2P) and pronunciation-to-word (P2W) conversion models. However, to generate optimal word sequences, the previous work considered just optimal pronunciation sequences. In this paper, we propose a joint maximization decoder (JMD) that considers the joint probability of pronunciation and word in beam-search decoding. Moreover, we introduce a neural network based joint source channel model for improving A2P conversion performance. Experiments on Japanese ASR tasks demonstrate that JMD achieves better performance than GCD. Furthermore, we show the effectiveness of using just language resources to retrain the P2W conversion model",
    "checked": true,
    "id": "aefc8a1739618d494184c5b30fc0db3704191eb6",
    "semantic_title": "joint maximization decoder with neural converters for fully neural network-based japanese speech recognition",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/parcollet19b_interspeech.html": {
    "title": "Real to H-Space Encoder for Speech Recognition",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) and more precisely recurrent neural networks (RNNs) are at the core of modern automatic speech recognition systems, due to their efficiency to process input sequences. Recently, it has been shown that different input representations, based on multidimensional algebras, such as complex and quaternion numbers, are able to bring to neural networks a more natural, compressive and powerful representation of the input signal by outperforming common real-valued NNs. Indeed, quaternion-valued neural networks (QNNs) better learn both internal dependencies, such as the relation between the Mel-filter-bank value of a specific time frame and its time derivatives, and global dependencies, describing the relations that exist between time frames. Nonetheless, QNNs are limited to quaternion-valued input signals, and it is difficult to benefit from this powerful representation with real-valued input data. This paper proposes to tackle this weakness by introducing a real-to-quaternion encoder that allows QNNs to process any one dimensional input features, such as traditional Mel-filter-banks for automatic speech recognition",
    "checked": true,
    "id": "51346db5ccdb68beb696211cb63255e7e5bc197a",
    "semantic_title": "real to h-space encoder for speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yi19b_interspeech.html": {
    "title": "Ectc-Docd: An End-to-End Structure with CTC Encoder and OCD Decoder for Speech Recognition",
    "volume": "main",
    "abstract": "Real-time streaming speech recognition is required by most applications for a nice interactive experience. To naturally support online recognition, a common strategy used in recently proposed end-to-end models is to introduce a blank label to the label set and instead output alignments. However, generating the alignment means decoding much longer than the length of the linguistic sequence. Besides, there exist several blank labels between two output units in the alignment, which hinders models from learning the adjacent dependency of units in the target sequence. In this work, we propose an innovative encoder-decoder structure, called Ectc-Docd, for online speech recognition which directly predicts the linguistic sequence without blank labels. Apart from the encoder and decoder structures, Ectc-Docd contains an additional shrinking layer to drop the redundant acoustic information. This layer serves as a bridge connecting acoustic representation and linguistic modelling parts. Through experiments, we confirm that Ectc-Docd can obtain better performance than a strong CTC model in online ASR tasks. We also show that Ectc-Docd can achieve promising results on both Mandarin and English ASR datasets with first and second pass decoding",
    "checked": true,
    "id": "757ef7ff81f5da3d556cb2217280e6183263d13a",
    "semantic_title": "ectc-docd: an end-to-end structure with ctc encoder and ocd decoder for speech recognition",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/denisov19_interspeech.html": {
    "title": "End-to-End Multi-Speaker Speech Recognition Using Speaker Embeddings and Transfer Learning",
    "volume": "main",
    "abstract": "This paper presents our latest investigation on end-to-end automatic speech recognition (ASR) for overlapped speech. We propose to train an end-to-end system conditioned on speaker embeddings and further improved by transfer learning from clean speech. This proposed framework does not require any parallel non-overlapped speech materials and is independent of the number of speakers. Our experimental results on overlapped speech datasets show that joint conditioning on speaker embeddings and transfer learning significantly improves the ASR performance",
    "checked": true,
    "id": "6c6ecb9bb11dc07490d2e579d6c7e7e70622d7e9",
    "semantic_title": "end-to-end multi-speaker speech recognition using speaker embeddings and transfer learning",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hayashi19_interspeech.html": {
    "title": "Pre-Trained Text Embeddings for Enhanced Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "We propose an end-to-end text-to-speech (TTS) synthesis model that explicitly uses information from pre-trained embeddings of the text. Recent work in natural language processing has developed self-supervised representations of text that have proven very effective as pre-training for language understanding tasks. We propose using one such pre-trained representation (BERT) to encode input phrases, as an additional input to a Tacotron2-based sequence-to-sequence TTS model. We hypothesize that the text embeddings contain information about the semantics of the phrase and the importance of each word, which should help TTS systems produce more natural prosody and pronunciation. We conduct subjective listening tests of our proposed models using the 24-hour LJSpeech corpus, finding that they improve mean opinion scores modestly but significantly over a baseline TTS model without pre-trained text embedding input",
    "checked": true,
    "id": "fbf2a6a887ea92311cf207d522c535daf867a6ba",
    "semantic_title": "pre-trained text embeddings for enhanced text-to-speech synthesis",
    "citation_count": 58
  },
  "https://www.isca-speech.org/archive/interspeech_2019/szekely19b_interspeech.html": {
    "title": "Spontaneous Conversational Speech Synthesis from Found Data",
    "volume": "main",
    "abstract": "Synthesising spontaneous speech is a difficult task due to disfluencies, high variability and syntactic conventions different from those of written language. Using found data, as opposed to lab-recorded conversations, for speech synthesis adds to these challenges because of overlapping speech and the lack of control over recording conditions. In this paper we address these challenges by using a speaker-dependent CNN-LSTM breath detector to separate continuous recordings into utterances, which we here apply to extract nine hours of clean single-speaker breath groups from a conversational podcast. The resulting corpus is transcribed automatically (both lexical items and filler tokens) and used to build several voices on a Tacotron 2 architecture. Listening tests show: i) pronunciation accuracy improved with phonetic input and transfer learning; ii) it is possible to create a more fluent conversational voice by training on data without filled pauses; and iii) the presence of filled pauses improved perceived speaker authenticity. Another listening test showed the found podcast voice to be more appropriate for prompts from both public speeches and casual conversations, compared to synthesis from found read speech and from a manually transcribed lab-recorded spontaneous conversation",
    "checked": true,
    "id": "a6a962ba444e7e96e04f730a4f5cc337e48faffa",
    "semantic_title": "spontaneous conversational speech synthesis from found data",
    "citation_count": 44
  },
  "https://www.isca-speech.org/archive/interspeech_2019/klimkov19_interspeech.html": {
    "title": "Fine-Grained Robust Prosody Transfer for Single-Speaker Neural Text-To-Speech",
    "volume": "main",
    "abstract": "We present a neural text-to-speech system for fine-grained prosody transfer from one speaker to another. Conventional approaches for end-to-end prosody transfer typically use either fixed-dimensional or variable-length prosody embedding via a secondary attention to encode the reference signal. However, when trained on a single-speaker dataset, the conventional prosody transfer systems are not robust enough to speaker variability, especially in the case of a reference signal coming from an unseen speaker. Therefore, we propose decoupling of the reference signal alignment from the overall system. For this purpose, we pre-compute phoneme-level time stamps and use them to aggregate prosodic features per phoneme, injecting them into a sequence-to-sequence text-to-speech system. We incorporate a variational auto-encoder to further enhance the latent representation of prosody embeddings. We show that our proposed approach is significantly more stable and achieves reliable prosody transplantation from an unseen speaker. We also propose a solution to the use case in which the transcription of the reference signal is absent. We evaluate all our proposed methods using both objective and subjective listening tests",
    "checked": true,
    "id": "f450d3002a969d60b78b7215c39e3908e8a0a43d",
    "semantic_title": "fine-grained robust prosody transfer for single-speaker neural text-to-speech",
    "citation_count": 64
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hussain19_interspeech.html": {
    "title": "Speech Driven Backchannel Generation Using Deep Q-Network for Enhancing Engagement in Human-Robot Interaction",
    "volume": "main",
    "abstract": "We present a novel method for training a social robot to generate backchannels during human-robot interaction. We address the problem within an off-policy reinforcement learning framework, and show how a robot may learn to produce non-verbal backchannels like laughs, when trained to maximize the engagement and attention of the user. A major contribution of this work is the formulation of the problem as a Markov decision process (MDP) with states defined by the speech activity of the user and rewards generated by quantified engagement levels. The problem that we address falls into the class of applications where unlimited interaction with the environment is not possible (our environment being a human) because it may be time-consuming, costly, impracticable or even dangerous in case a bad policy is executed. Therefore, we introduce deep Q-network (DQN) in a batch reinforcement learning framework, where an optimal policy is learned from a batch data collected using a more controlled policy. We suggest the use of human-to-human dyadic interaction datasets as a batch of trajectories to train an agent for engaging interactions. Our experiments demonstrate the potential of our method to train a robot for engaging behaviors in an offline manner",
    "checked": true,
    "id": "aa892c6ec1b7f62db82681688c57a629c5f0bf85",
    "semantic_title": "speech driven backchannel generation using deep q-network for enhancing engagement in human-robot interaction",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/koriyama19_interspeech.html": {
    "title": "Semi-Supervised Prosody Modeling Using Deep Gaussian Process Latent Variable Model",
    "volume": "main",
    "abstract": "This paper proposes a semi-supervised speech synthesis framework in which prosodic labels of training data are partially annotated. When we construct a text-to-speech (TTS) system, it is crucial to use appropriately annotated prosodic labels. For this purpose, manually annotated ones would provide a good result, but it generally costs much time and patience. Although recent studies report that end-to-end TTS framework can generate natural-sounding prosody without using prosodic labels, this does not always appear in arbitrary languages such as pitch accent ones. Alternatively, we propose an approach to utilizing a latent variable representation of prosodic information. In the latent variable representation, we employ deep Gaussian process (DGP), a deep Bayesian generative model. In the proposed semi-supervised learning framework, the posterior distributions of latent variables are inferred from linguistic and acoustic features, and the inferred latent variables are utilized to train a DGP-based regression model of acoustic features. Experimental results show that the proposed framework can give a comparable performance with the case using fully-annotated speech data in subjective evaluation even if the prosodic information of pitch accent is limited",
    "checked": true,
    "id": "24689e11d19f10eb458521a53d036a4453f8c7d0",
    "semantic_title": "semi-supervised prosody modeling using deep gaussian process latent variable model",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nikulasdottir19_interspeech.html": {
    "title": "Bootstrapping a Text Normalization System for an Inflected Language. Numbers as a Test Case",
    "volume": "main",
    "abstract": "Text normalization is an important part of many natural language applications, in particular for text-to-speech systems. Text normalization poses special challenges for highly inflected languages since the correct morphological form for the normalization is not evident from the non-standard word, e.g. a digit In this paper we report on ongoing work on a text normalization system for Icelandic, a highly inflected North Germanic language. We describe experiments on the normalization of numbers and address the problem of choosing the correct morphological form of number names. We use language models trained on texts containing number names and inspect effects of different LMs on domain specific texts with a high ratio of digits. A partially class based LM, replacing number names with their part-of-speech tags, shows the best results in all domains. We further show that testing normalization on texts where number names have been converted to digits does not show representative results for texts originally containing digits: while a test set similar to the language model training data shows an error rate of 10.1% on inflected cardinals from 1–99, test sets originally containing digits show 45.3% and 55% error rates",
    "checked": true,
    "id": "84148f374f44a9508fc9dfed0b271fbcef34db2a",
    "semantic_title": "bootstrapping a text normalization system for an inflected language. numbers as a test case",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/guo19e_interspeech.html": {
    "title": "Exploiting Syntactic Features in a Parsed Tree to Improve End-to-End TTS",
    "volume": "main",
    "abstract": "The end-to-end TTS, which can predict speech directly from a given sequence of graphemes or phonemes, has shown improved performance over the conventional TTS. However, its predicting capability is still limited by the acoustic/phonetic coverage of the training data, usually constrained by the training set size. To further improve the TTS quality in pronunciation, prosody and perceived naturalness, we propose to exploit the information embedded in a syntactically parse tree where the inter-phrase/word information of a sentence is organized in a multilevel tree structure. Specifically, two key features: phrase structure and relations between adjacent words are investigated. Experimental results in subjective listening, measured on three test sets, show that the proposed approach is effective to improve the pronunciation clarity, prosody and naturalness of the synthesized speech of the baseline system",
    "checked": true,
    "id": "e12dc180f48d9d4ba2ee36982a8e363560fa108e",
    "semantic_title": "exploiting syntactic features in a parsed tree to improve end-to-end tts",
    "citation_count": 26
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ni19_interspeech.html": {
    "title": "Duration Modeling with Global Phoneme-Duration Vectors",
    "volume": "main",
    "abstract": "A duration model is a major component in every parametric speech synthesis system. Conventional methods use full contextual labels as features to predict phoneme durations that require morphological analysis of text. By contrast, advances in bidirectional recurrent neural networks (BRNN) and global space vector models make it possible to perform grapheme-to-phoneme (G2P) conversion from plain text. In this paper, we investigate duration prediction from plain phonemes instead of using their full contextual labels. We propose a new approach that relies on both BRNN and global space vector representations of phonemes (GPV) and durations (GDV). GPVs represent the statistics of phonemes used in a language, whereas GDVs capture duration variations beyond linguistic features. They are essentially learned from a large-scale text corpus in an unsupervised manner where phonemes are converted by G2P We conducted experiments on two speech corpora in Korean and Chinese to train BRNN-based models in a supervised manner. An objective evaluation conducted on a set of test sentences demonstrated that the proposed method leads to more accurate modeling of phoneme durations than the baselines",
    "checked": true,
    "id": "8e635455f2abea0bd9c94defd3f3f6d129bd4455",
    "semantic_title": "duration modeling with global phoneme-duration vectors",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/aubin19_interspeech.html": {
    "title": "Improving Speech Synthesis with Discourse Relations",
    "volume": "main",
    "abstract": "This paper explores whether adding Discourse Relation (DR) features improves the naturalness of neural statistical parametric speech synthesis (SPSS) in English. We hypothesize first — in the light of several previous studies — that DRs have a dedicated prosodic encoding. Secondly, we hypothesize that encoding DRs in a speech synthesizer's input will improve the naturalness of its output. In order to test our hypotheses, we prepare a dataset of DR-annotated transcriptions of audiobooks in English. We then perform an acoustic analysis of the corpus which supports our first hypothesis that DRs are acoustically encoded in speech prosody. The analysis reveals significant correlation between specific DR categories and acoustic features, such as F0 and intensity. Then, we use the corpus to train a neural SPSS system in two configurations: a baseline configuration making use only of conventional linguistic features, and an experimental one where these are supplemented with DRs. Augmenting the inputs with DR features improves objective acoustic scores on a test set and leads to significant preference by listeners in a forced choice AB test for naturalness",
    "checked": true,
    "id": "13ed2054e26e61a6d60a03b93e6a0f31f3bcf550",
    "semantic_title": "improving speech synthesis with discourse relations",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tits19_interspeech.html": {
    "title": "Visualization and Interpretation of Latent Spaces for Controlling Expressive Speech Synthesis Through Audio Analysis",
    "volume": "main",
    "abstract": "The field of Text-to-Speech has experienced huge improvements last years benefiting from deep learning techniques. Producing realistic speech becomes possible now. As a consequence, the research on the control of the expressiveness, allowing to generate speech in different styles or manners, has attracted increasing attention lately. Systems able to control style have been developed and show impressive results. However the control parameters often consist of latent variables and remain complex to interpret In this paper, we analyze and compare different latent spaces and obtain an interpretation of their influence on expressive speech. This will enable the possibility to build controllable speech synthesis systems with an understandable behaviour",
    "checked": true,
    "id": "c2da421b9284f7293750d00092dfc388280caa20",
    "semantic_title": "visualization and interpretation of latent spaces for controlling expressive speech synthesis through audio analysis",
    "citation_count": 29
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yang19h_interspeech.html": {
    "title": "Pre-Trained Text Representations for Improving Front-End Text Processing in Mandarin Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "In this paper, we propose a novel method to improve the performance and robustness of the front-end text processing modules of Mandarin text-to-speech (TTS) synthesis. We use pre-trained text encoding models, such as the encoder of a transformer based NMT model and BERT, to extract the latent semantic representations of words or characters and use them as input features for tasks in the front-end of TTS systems. Our experiments on the tasks of Mandarin polyphone disambiguation and prosodic structure prediction show that the proposed method can significantly improve the performances. Specifically, we get an absolute improvement of 0.013 and 0.027 in F1 score for prosodic word prediction and prosodic phrase prediction respectively, and an absolute improvement of 2.44% in polyphone disambiguation compared to previous methods",
    "checked": true,
    "id": "6abc7dac0bdc50735b6d12f96400f59b5f084759",
    "semantic_title": "pre-trained text representations for improving front-end text processing in mandarin text-to-speech synthesis",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pan19b_interspeech.html": {
    "title": "A Mandarin Prosodic Boundary Prediction Model Based on Multi-Task Learning",
    "volume": "main",
    "abstract": "In this paper, we propose a mandarin prosodic boundary prediction model based on Multi-Task Learning (MTL) architecture. The prosody structure of mandarin is a three-level hierarchical structure, which contains three basic units — Prosodic Word (PW), Prosodic Phrase (PPH) and Intonational Phrase (IPH) [1]. Previous studies usually decompose mandarin prosodic boundary prediction task into three independent tasks on these three unit boundaries [1–4]. In recent years, with the development of deep learning, MTL has achieved state-of-the-art performance on many tasks in Natural Language Processing (NLP) field [5–7]. Inspired by this, this paper implements an MTL framework with Bidirectional Long-Short Term Memory and Conditional Random Field (BLSTM-CRF) as the basic model, and takes three independent tasks of mandarin prosodic boundary prediction as sub-modules for PW, PPH and IPH individually. Under the MTL architecture, the three independent tasks are unified for overall optimization. The experiment results show that our model is effective in solving the task of mandarin prosodic boundary prediction, in which the overall prediction performance is improved by 0.8%, and the model size is reduced by about 55%",
    "checked": true,
    "id": "4d40b8fcd8afc50b491afee945769b9bb2a74080",
    "semantic_title": "a mandarin prosodic boundary prediction model based on multi-task learning",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gokcen19_interspeech.html": {
    "title": "Dual Encoder Classifier Models as Constraints in Neural Text Normalization",
    "volume": "main",
    "abstract": "Neural text normalization systems can achieve low error rates; however, the errors they make include not only ones from which the hearer can recover (such as reading 3 as three dollar) but also unrecoverable errors, such as reading 3 as three euros. FST decoding constraints have proven effective at reducing unrecoverable errors. In this paper we explore an alternative approach to error mitigation: using dual encoder classifiers trained with both positive and negative examples to implement soft constraints on acceptability. Since the error rates are very low, it is difficult to determine when improvement is significant, but qualitative analysis suggests that soft dual encoder constraints can help reduce the number of unrecoverable errors",
    "checked": true,
    "id": "1c07699429cbc5d9e75ba16f60a63a9d92eec3c5",
    "semantic_title": "dual encoder classifier models as constraints in neural text normalization",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19v_interspeech.html": {
    "title": "Knowledge-Based Linguistic Encoding for End-to-End Mandarin Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "Recent researches have shown superior performance of applying end-to-end architecture in text-to-speech (TTS) synthesis. However, considering the complex linguistic structure of Chinese, using Chinese characters directly for Mandarin TTS may suffer from the poor linguistic encoding performance, resulting in improper word tokenization and pronunciation errors. To ensure the naturalness and intelligibility of synthetic speech, state-of-the-art Mandarin TTS systems employ a list of components, such as word tokenization, part-of-speech (POS) tagging and grapheme-to-phoneme (G2P) conversion, to produce knowledge-enhanced inputs to alleviate the problems caused by linguistic encoding. These components are based on linguistic expertise and well-designed, but trained individually, leading to errors compounding for the TTS system. In this paper, to reduce the complexity of Mandarin TTS system and bring further improvement, we proposed a knowledge-based linguistic encoder for the character-based end-to-end Mandarin TTS system. Developed with multi-task learning structure, the proposed encoder can learn from linguistic analysis subtasks, providing robust and discriminative linguistic encodings for the following speech generation decoder. Experimental results demonstrate the effectiveness of the proposed framework, with word tokenization error dropped from 12.81% to 1.58%, syllable pronunciation error dropped from 10.89% to 2.81% compared with state-of-the-art baseline approach, providing mean opinion score (MOS) improvement from 3.76 to 3.87",
    "checked": true,
    "id": "a94a3d53897c0a585d8d7f2d75153f07c00f428c",
    "semantic_title": "knowledge-based linguistic encoding for end-to-end mandarin text-to-speech synthesis",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shankar19c_interspeech.html": {
    "title": "Automated Emotion Morphing in Speech Based on Diffeomorphic Curve Registration and Highway Networks",
    "volume": "main",
    "abstract": "We present a novel approach for emotion conversion that bridges the domains of speech analysis and computer vision. Our strategy is to warp the pitch contour of a source emotional utterance using diffeomorphic curve registration. The associated dynamical process pushes the original source contour towards that of a target emotional utterance. Mathematically, this warping process is completely specified by a set of initial momenta. Therefore, we use parallel data to train a highway neural network (HNet) to predict these initial momenta directly from the signal characteristics. The input features to the HNet include contextual pitch and spectral information. Once trained, the HNet is used to obtain the initial momenta for new utterances. From here, the diffeomorphic process takes over and warps the pitch contour accordingly. We validate our framework on the VESUS repository collected at Johns Hopkins University, which contains parallel emotional utterances from 10 actors. The proposed warping is more accurate that three state-of-the-art baselines for emotion conversion. We also evaluate the quality of our emotion manipulations via crowd sourcing",
    "checked": true,
    "id": "f055b3fe1d2ee979ceb5feb9d249511c76085ea0",
    "semantic_title": "automated emotion morphing in speech based on diffeomorphic curve registration and highway networks",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2019/connaghan19_interspeech.html": {
    "title": "Use of Beiwe Smartphone App to Identify and Track Speech Decline in Amyotrophic Lateral Sclerosis (ALS)",
    "volume": "main",
    "abstract": "The capacity for smartphones to remotely capture speech data affords significant clinical and research opportunities for degenerative neurologic diseases such as amyotrophic lateral sclerosis (ALS). Longitudinal data may inform ALS disease prognosis, facilitate timely intervention, and document response to treatment [1]. A recent study established the feasibility of the Beiwe smartphone-based digital phenotyping to track the clinical progression of ALS across multiple domains [2]. The current investigation extends this work to address the utility of Beiwe to identify and track speech decline in ALS. Twelve participants with ALS used the Beiwe app weekly to record reading passages and self-report (ALSFRS-R) ratings of bulbar (speech) function. Speaking rate and pause variables were automatically extracted from recordings offline [3]. Speech function measures at baseline were significantly different for participants with and without bulbar symptoms. In addition, the rate of decline of all measured speech functions was greater for participants with bulbar symptoms. The successful use of Beiwe for speech function analysis suggests that smartphone-based capture of speech has potential for diagnostic screening and disease progress monitoring in ALS. Further large sample investigation across a comprehensive set of speech variables is warranted",
    "checked": true,
    "id": "a90f50b1402544158119892d10ee06507d09eb4a",
    "semantic_title": "use of beiwe smartphone app to identify and track speech decline in amyotrophic lateral sclerosis (als)",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rowe19_interspeech.html": {
    "title": "Profiling Speech Motor Impairments in Persons with Amyotrophic Lateral Sclerosis: An Acoustic-Based Approach",
    "volume": "main",
    "abstract": "The goal of this study was to profile the speech motor impairments that characterize dysarthria secondary to amyotrophic lateral sclerosis (ALS). This information is important for identifying optimal treatment strategies and guiding speech impairment subtype discovery, which may facilitate ongoing efforts to improve automatic speech recognition (ASR) of dysarthric speech. Speech motor impairments were profiled by introducing a novel framework that assesses four key components of motor control: coordination, consistency, speed, and precision. An individual acoustic feature was selected to represent each component. Specifically, coordination was indexed by the proportion of voice onset time (VOT) to syllable duration, consistency by the coefficient of variation of VOT between repetitions of /pataka/ within each distinct consonant, speed by the slope of the second formant (F2), and precision by the standard deviation of F2 slope between distinct consonants within each repetition of /pataka/. Acoustic measures were extracted from audio recordings of each participant (18 controls and 14 participants with ALS) during a sequential motion rate (SMR) task. Results revealed that the primary underlying speech motor impairments that characterize ALS are in coordination, speed, and precision. Further research is needed to validate the existence of speech-impairment-based subtypes across the continuum of speech motor disorders",
    "checked": true,
    "id": "88af52d17d8eecbf0e508b6fd9dcd626cceaadd0",
    "semantic_title": "profiling speech motor impairments in persons with amyotrophic lateral sclerosis: an acoustic-based approach",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mayle19_interspeech.html": {
    "title": "Diagnosing Dysarthria with Long Short-Term Memory Networks",
    "volume": "main",
    "abstract": "This paper proposes the use of Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units for determining whether Mandarin-speaking individuals are afflicted with a form of Dysarthria based on samples of syllable pronunciations. Several LSTM network architectures are evaluated on this binary classification task, using accuracy and Receiver Operating Characteristic (ROC) curves as metrics. The LSTM models are shown to significantly improve upon a baseline fully connected network, reaching over 90% area under the ROC curve on the task of classifying new speakers, when a sufficient number of cepstrum coefficients are used. The results show that the LSTM's ability to leverage temporal information within its input makes for an effective step in the pursuit of accessible Dysarthria diagnoses",
    "checked": true,
    "id": "8f35374ea5a42005adc79649cf2e0f7361b270e5",
    "semantic_title": "diagnosing dysarthria with long short-term memory networks",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sudro19_interspeech.html": {
    "title": "Modification of Devoicing Error in Cleft Lip and Palate Speech",
    "volume": "main",
    "abstract": "The cleft of the lip and palate (CLP) caused by structural and functional deformation leads to various speech-related disorders, which substantially degrades the speech intelligibility. In this work, devoiced stop consonants in CLP speech are analyzed, and an approach is proposed for its modification in order to enhance the speech intelligibility. The devoicing errors are primarily characterized by the absence of voicebar in the closure interval and relatively longer voice onset time (VOT). The proposed approach first segments the regions corresponding to the closure interval and VOT based on the knowledge of glottal activity, voice onset point, voice offset point, and burst onset point. In the next stage, specific transformations are performed for the modification of closure bar and VOT respectively. For transformation, first different transformation matrices are learned for closure bar and VOT from normal and CLP speakers. The transformation matrix is optimized using nonnegative matrix factorization method. Further, the corresponding transformation matrices are used to modify the closure bar and VOT separately. The subjective evaluation results indicate that the devoiced stop consonants tend to exhibit the characteristics of voiced stop consonants",
    "checked": true,
    "id": "e146a168f99d170db84dcd5334676d9679e74b0f",
    "semantic_title": "modification of devoicing error in cleft lip and palate speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/eshghi19_interspeech.html": {
    "title": "Reduced Task Adaptation in Alternating Motion Rate Tasks as an Early Marker of Bulbar Involvement in Amyotrophic Lateral Sclerosis",
    "volume": "main",
    "abstract": "The identification of robust biomarkers to detect the onset of amyotrophic lateral sclerosis (ALS) has been an ongoing challenge. Recent evidence from multiple studies suggests that speech changes are a reliable early indicator of ALS particularly during physically demanding speaking tasks such as alternating motion rate (AMR). However, it has also been found that individuals make various behavioral adaptations to meet the maximum rate requirement in AMR. In this study, we explored the extent to which persons with early-stage ALS are capable of adapting to challenging speech-like tasks. Speech motor performance of 14 healthy controls was compared to that of 18 patients at the early stage of ALS during standard (unconstrained) and fixed-target (constrained) AMR tasks. Fixed-target tasks were designed to impose high demands on the speech motor system. Although habitual speaking rate was maintained within normal limits, findings revealed that task adaptation was reduced at the early stage of ALS. Furthermore, the difference between the number of cycles in the fixed-target task and standard task showed higher sensitivity than habitual speaking rate to detect early decline in bulbar function. The inability to adapt to the fixed-target task was a good early indicator of bulbar motor involvement due to ALS",
    "checked": true,
    "id": "2e0b8f63ff5adab32c35d96c284ed0911d9ddddb",
    "semantic_title": "reduced task adaptation in alternating motion rate tasks as an early marker of bulbar involvement in amyotrophic lateral sclerosis",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19q_interspeech.html": {
    "title": "Towards the Speech Features of Early-Stage Dementia: Design and Application of the Mandarin Elderly Cognitive Speech Database",
    "volume": "main",
    "abstract": "Speech and language features have been proven to be useful for the detection of neurodegenerative diseases, such as Alzheimer's disease (AD), and its prodromal stage, mild cognitive impairment (MCI). Unfortunately, high-quality speech database remains scarce, which limit its application in automatic screening and assessment of early dementia in clinical practice. To bridge this gap, the present study aimed to design a speech database of Chinese elderly with intact cognition and MCI, named \"Mandarin Elderly Cognitive Speech Database\" (MECSD). The database consists of 110 hours of speech recordings from 85 native speakers of Mandarin Chinese (age range = 55–85 years), including 20 participants with MCI and 65 healthy controls. Manually transcribed materials with temporal information were also included in this database. Nine tasks, involving conventional test batteries and connected speech productions, were used to obtain speech samples, producing a total of 8563 sentences and 49841 words. Details concerning the design of the database, together with our preliminary findings applying automatic speech recognition (ASR), were reported in this study. The MECSD will provide researchers with access to a large shared database that can facilitate hypothesis testing in the study of early-stage dementia",
    "checked": true,
    "id": "1449e0e5a63c2bb31c3aa438fe420312c5cc6ac7",
    "semantic_title": "towards the speech features of early-stage dementia: design and application of the mandarin elderly cognitive speech database",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19p_interspeech.html": {
    "title": "Acoustic Characteristics of Lexical Tone Disruption in Mandarin Speakers After Brain Damage",
    "volume": "main",
    "abstract": "This study identifies the acoustic characteristics of tones produced by Mandarin brain-damaged patients. We investigate the F0 characteristics of the patients' tone productions and compare them with a control group of healthy speakers. The results show tone disruption in patients with brain damage in either the left or the right hemisphere. Even patients' tone productions that were correctly identified by Mandarin native speakers were acoustically different from the ones produced by healthy speakers. The patterns of tone disruption in Mandarin brain-damaged patients might be caused by damage to the motor function in the brain",
    "checked": true,
    "id": "6492764c009a33993f33819de1ca309b3c0ef69f",
    "semantic_title": "acoustic characteristics of lexical tone disruption in mandarin speakers after brain damage",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hermes19_interspeech.html": {
    "title": "Intragestural Variation in Natural Sentence Production: Essential Tremor Patients Treated with DBS",
    "volume": "main",
    "abstract": "In the present study, we investigate intragestural parameters during the production of CV syllables in natural sentence production of Essential Tremor (ET) patients treated with Deep Brain Stimulation (DBS). Within the task dynamic approach, we analyzed temporal and spatial parameters of consonantal and vocalic movements of the respective target syllables. Our analysis revealed that intragestural coordination patterns are affected in the patients' group: While patients with inactivated stimulation (DBS-OFF) already showed signs of dysarthria in terms of longer and less stiff movements, there was an additional slowing down of the speech motor system under activated stimulation (DBS-ON). When comparing CV production in natural sentence to fast syllable repetition tasks (DDK), we find similarities in that there is a slowing down of the system, but also differences in that coordination problems increase in DDK leading to an overmodulation of peak velocities and displacements",
    "checked": true,
    "id": "83b0452d3d128c90e0c14459f6b6c41707f8b2fc",
    "semantic_title": "intragestural variation in natural sentence production: essential tremor patients treated with dbs",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kalita19_interspeech.html": {
    "title": "Nasal Air Emission in Sibilant Fricatives of Cleft Lip and Palate Speech",
    "volume": "main",
    "abstract": "Cleft lip and palate (CLP) is a congenital disorder of the orofacial region. Nasal air emission (NAE) in CLP speech occurs due to the presence of velopharyngeal dysfunction (VPD), and it mostly occurs in the production of fricative sounds. The objective of present work is to study the acoustic characteristics of voiceless sibilant fricatives in Kannada distorted by NAE and develop an SVM-based classification to distinguish normal fricatives from the NAE distorted fricatives. Static spectral measures, such as spectral moments are used to analyze the deviant spectral distribution of NAE distorted fricatives. As the aerodynamic parameters are deviated due to VPD, the temporal variation of spectral characteristics might also get deviated in NAE distorted fricatives. This variation is studied using the peak equivalent rectangular bandwidth (ERB )-number, a psychoacoustic measure to analyze the temporal variation in the spectral properties of fricatives. The analysis of NAE distorted fricatives shows that the maximum spectral density is concentrated in the lower frequency range with steep positive skewness and more variations in the trajectories of peak ERB -number as compared to the normal fricatives. The proposed SVM-based classification achieves good detection rates in discriminating NAE distorted fricatives from normal fricatives",
    "checked": true,
    "id": "fd51fffb54dd6689fceb8246e0055c34a24edeb8",
    "semantic_title": "nasal air emission in sibilant fricatives of cleft lip and palate speech",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/serrano19_interspeech.html": {
    "title": "Parallel vs. Non-Parallel Voice Conversion for Esophageal Speech",
    "volume": "main",
    "abstract": "State of the art systems for voice conversion have been shown to generate highly natural sounding converted speech. Voice conversion techniques have also been applied to alaryngeal speech, with the aim of improving its quality or its intelligibility. In this paper, we present an attempt to apply a voice conversion strategy based on phonetic posteriorgrams (PPGs), which produces very high quality converted speech, to improve the characteristics of esophageal speech. The main advantage of this PPG based architecture lies in the fact that it is able to convert speech from any source, without the need to previously train the system with a parallel corpus. However, our results show that the PPG approach degrades the intelligibility of the converted speech considerably, especially when the input speech is already poorly intelligible. In this paper two systems are compared, an LSTM based one-to-one conversion system, which is referred to as the baseline, and the new system using phonetic posteriorgrams. Both spectral parameters and f are converted using DNN (Deep Neural Network) based architectures. Results from both objective and subjective evaluations are presented, showing that although ASR (Automated Speech Recognition) errors are reduced, original esophageal speech is still preferred by subjects",
    "checked": true,
    "id": "a7bad79f4eec1abc1f892d178d98b2407d47878a",
    "semantic_title": "parallel vs. non-parallel voice conversion for esophageal speech",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dubey19b_interspeech.html": {
    "title": "Hypernasality Severity Detection Using Constant Q Cepstral Coefficients",
    "volume": "main",
    "abstract": "In this work, detection of hypernasality severity in cleft palate speech is attempted using constant Q cepstral coefficients (CQCC) feature. The coupling of nasal tract with the oral tract during the production of hypernasal speech adds nasal formants and anti-formants in low frequency region of vowel spectrum mainly around the first formant. The strength and position of nasal formants and anti-formants along with the oral formants changes as the severity of nasality changes in hypernasal speech. The CQCC feature is extracted from the constant Q transform (CQT) spectrum which employs geometrically spaced frequency bins and maintains a constant Q factor for across the entire spectrum. This results in a higher frequency resolution at lower frequencies and higher temporal resolution at higher frequencies. The CQT spectrum resolves the nasal and oral formants in low frequency and captures the spectral changes due to change in nasality severity. The CQCC feature gives the overall classification accuracy of 83.33% and 78.47% for /i/ and /u/ vowels corresponding to normal, mild and moderate-severe hypernasal speech, respectively using multiclass support vector classifier",
    "checked": true,
    "id": "1b3b27f6c0e3a178c7402b6d57440bb93008ceb5",
    "semantic_title": "hypernasality severity detection using constant q cepstral coefficients",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/niu19_interspeech.html": {
    "title": "Automatic Depression Level Detection via ℓp-Norm Pooling",
    "volume": "main",
    "abstract": "Related physiological studies have shown that Mel-frequency cepstral coefficient (MFCC) is a discriminative acoustic feature for depression detection. This fact has led to some works using MFCCs to identify individual depression degree. However, they rarely adopt neural network to capture high-level feature associated with depression detection. And the suitable feature pooling parameter for depression detection has not been optimized. For these reasons, we propose a hybrid network and ℓ_p-norm pooling combined with least absolute shrinkage and selection operator (LASSO) to improve the accuracy of depression detection. Firstly, the MFCCs of the original speech are divided into many segments. Then, we extract the segment-level feature using the proposed hybrid network, which investigates the depression-related information in the spatial structure, temporal changes and discriminative representation of short-term MFCC segments. Thirdly, ℓ_p-norm pooling combined with LASSO is adopted to find the optimal pooling parameter for depression detection to generate the utterance-level feature. Finally, depression level prediction is accomplished using support vector regression (SVR). Experiments are conducted on AVEC2013 and AVEC2014. The results demonstrate that our proposed method achieves better performance than the previous algorithms",
    "checked": true,
    "id": "551d8b213fc3210e5317277dc58d726871832a0e",
    "semantic_title": "automatic depression level detection via ℓp-norm pooling",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bn19_interspeech.html": {
    "title": "Comparison of Speech Tasks and Recording Devices for Voice Based Automatic Classification of Healthy Subjects and Patients with Amyotrophic Lateral Sclerosis",
    "volume": "main",
    "abstract": "We consider the task of speech based automatic classification of patients with amyotrophic lateral sclerosis (ALS) and healthy subjects. The role of different speech tasks and recording devices on classification accuracy is examined. Sustained phoneme production (PHON), diadochokinetic task (DDK) and spontaneous speech (SPON) have been used as speech tasks. The chosen five recording devices include a high quality microphone and built-in smartphone microphones at various price ranges. Experiments are performed using speech data from 25 ALS patients and 25 healthy subjects using support vector machines and deep neural networks as classifiers and suprasegmental features based on mel frequency cepstral coefficients. Results reveal that DDK consistently performs better than SPON and PHON across all devices for discriminating ALS patients and healthy subjects. Considering DDK, the best classification accuracy of 92.2% is obtained using a high quality microphone but the accuracy drops if there is a mismatch between the microphones for training and test. However, a classifier trained with recordings from all devices together performs more uniformly across all devices. The findings from this study could aid in determining the choice of the task and device in developing an assistive tool for detection and monitoring of ALS",
    "checked": true,
    "id": "ceef24be797f35edacdad71d18e8cf3c4b6eb81d",
    "semantic_title": "comparison of speech tasks and recording devices for voice based automatic classification of healthy subjects and patients with amyotrophic lateral sclerosis",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19r_interspeech.html": {
    "title": "A Modified Algorithm for Multiple Input Spectrogram Inversion",
    "volume": "main",
    "abstract": "We propose a new algorithm to estimate the phase of speech signal in the mixture of audio sources under the assumption that the magnitude spectrum of each source is given. The previous method, multiple input spectrogram inversion algorithm (MISI), often performs poorly when the magnitude spectrograms estimated are not accurate. This may be because it imposes a strict constraint that the summation of source waveforms should be exactly the same as the mixture waveform. Our proposing algorithm employs a new objective function in which this constraint is relaxed. In this objective function, the difference between the summation of source waveforms and the mixture waveform is the target to be minimized. The performance of our method, modified MISI is evaluated on two different experimental settings. In both settings it improves the audio source separation performance compared to MISI",
    "checked": true,
    "id": "0f89f500495335c721ee0fb740337ca9a88d094d",
    "semantic_title": "a modified algorithm for multiple input spectrogram inversion",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bahmaninezhad19_interspeech.html": {
    "title": "A Comprehensive Study of Speech Separation: Spectrogram vs Waveform Separation",
    "volume": "main",
    "abstract": "Speech separation has been studied widely for single-channel close-talk microphone recordings over the past few years; developed solutions are mostly in frequency-domain. Recently, a raw audio waveform separation network (TasNet) is introduced for single-channel data, with achieving high Si-SNR (scale-invariant source-to-noise ratio) and SDR (source-to-distortion ratio) comparing against the state-of-the-art solution in frequency-domain. In this study, we incorporate effective components of the TasNet into a frequency-domain separation method. We compare both for alternative scenarios. We introduce a solution for directly optimizing the separation criterion in frequency-domain networks. In addition to speech separation objective and subjective measurements, we evaluate the separation performance on a speech recognition task as well. We study the speech separation problem for far-field data (more similar to naturalistic audio streams) and develop multi-channel solutions for both frequency and time-domain separators with utilizing spectral, spatial and speaker location information. For our experiments, we simulated multi-channel spatialized reverberate WSJ0-2mix dataset. Our experimental results show that spectrogram separation can achieve competitive performance with better network design. Multi-channel framework as well is shown to improve the single-channel performance relatively up to +35.5% and +46% in terms of WER and SDR, respectively",
    "checked": true,
    "id": "f57459f1bb3d674618678afc392412c481bb7404",
    "semantic_title": "a comprehensive study of speech separation: spectrogram vs waveform separation",
    "citation_count": 66
  },
  "https://www.isca-speech.org/archive/interspeech_2019/inan19_interspeech.html": {
    "title": "Evaluating Audiovisual Source Separation in the Context of Video Conferencing",
    "volume": "main",
    "abstract": "Source separation involving mono-channel audio is a challenging problem, in particular for speech separation where source contributions overlap both in time and frequency. This task is of high interest for applications such as video conferencing. Recent progress in machine learning has shown that the combination of visual cues, coming from the video, can increase the source separation performance. Starting from a recently designed deep neural network, we assess its ability and robustness to separate the visible speakers' speech from other interfering speeches or signals. We test it for different configuration of video recordings where the speaker's face may not be fully visible. We also asses the performance of the network with respect to different sets of visual features from the speakers' faces",
    "checked": true,
    "id": "9e717f6303e3a5ddb1b82dcd45423353b61d3b89",
    "semantic_title": "evaluating audiovisual source separation in the context of video conferencing",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ditter19_interspeech.html": {
    "title": "Influence of Speaker-Specific Parameters on Speech Separation Systems",
    "volume": "main",
    "abstract": "Recent studies have shown that Deep Learning based single-channel speech separation systems perform worse for same-gender mixtures than for different-gender mixtures. In this work, we provide for a more detailed analysis of the respective impact of the fundamental frequency and the vocal tract length on the system performance. While both parameters are correlated with gender, the vocal tract length is a fixed speaker-specific parameter, whereas the fundamental frequency can vary for different speaking styles. We show that the difference of the fundamental frequency medians of two speakers in a mixture is highly correlated with the SDR performance while the difference of the vocal tract lengths is not. Our analysis allows us to do performance predictions for given speakers based on measurements of their fundamental frequency. Furthermore we conclude that current systems separate (short-term) speaking styles rather than (long-term) speaker characteristics",
    "checked": true,
    "id": "abedd779fd83f98748bd9aead1980fa34e5aac92",
    "semantic_title": "influence of speaker-specific parameters on speech separation systems",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zegers19_interspeech.html": {
    "title": "CNN-LSTM Models for Multi-Speaker Source Separation Using Bayesian Hyper Parameter Optimization",
    "volume": "main",
    "abstract": "In recent years there have been many deep learning approaches towards the multi-speaker source separation problem. Most use Long Short-Term Memory - Recurrent Neural Networks (LSTM-RNN) or Convolutional Neural Networks (CNN) to model the sequential behavior of speech. In this paper we propose a novel network for source separation using an encoder-decoder CNN and LSTM in parallel. Hyper parameters have to be chosen for both parts of the network and they are potentially mutually dependent. Since hyper parameter grid search has a high computational burden, random search is often preferred. However, when sampling a new point in the hyper parameter space, it can potentially be very close to a previously evaluated point and thus give little additional information. Furthermore, random sampling is as likely to sample in a promising area as in an hyper space area dominated with poor performing models. Therefore, we use a Bayesian hyper parameter optimization technique and find that the parallel CNN-LSTM outperforms the LSTM-only and CNN-only model",
    "checked": true,
    "id": "c9ca7327374743e77ed488ca7ddf3ef49dc94873",
    "semantic_title": "cnn-lstm models for multi-speaker source separation using bayesian hyper parameter optimization",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bear19_interspeech.html": {
    "title": "Towards Joint Sound Scene and Polyphonic Sound Event Recognition",
    "volume": "main",
    "abstract": "Acoustic Scene Classification (ASC) and Sound Event Detection (SED) are two separate tasks in the field of computational sound scene analysis. In this work, we present a new dataset with both sound scene and sound event labels and use this to demonstrate a novel method for jointly classifying sound scenes and recognizing sound events. We show that by taking a joint approach, learning is more efficient and whilst improvements are still needed for sound event detection, SED results are robust in a dataset where the sample distribution is skewed towards sound scenes",
    "checked": true,
    "id": "816bcbb122ad3126c6ce285f64f56726dd2d1d08",
    "semantic_title": "towards joint sound scene and polyphonic sound event recognition",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fan19c_interspeech.html": {
    "title": "Discriminative Learning for Monaural Speech Separation Using Deep Embedding Features",
    "volume": "main",
    "abstract": "Deep clustering (DC) and utterance-level permutation invariant training (uPIT) have been demonstrated promising for speaker-independent speech separation. DC is usually formulated as two-step processes: embedding learning and embedding clustering, which results in complex separation pipelines and a huge obstacle in directly optimizing the actual separation objectives. As for uPIT, it only minimizes the chosen permutation with the lowest mean square error, doesn't discriminate it with other permutations. In this paper, we propose a discriminative learning method for speaker-independent speech separation using deep embedding features. Firstly, a DC network is trained to extract deep embedding features, which contain each source's information and have an advantage in discriminating each target speakers. Then these features are used as the input for uPIT to directly separate the different sources. Finally, uPIT and DC are jointly trained, which directly optimizes the actual separation objectives. Moreover, in order to maximize the distance of each permutation, the discriminative learning is applied to fine tuning the whole model. Our experiments are conducted on WSJ0-2mix dataset. Experimental results show that the proposed models achieve better performances than DC and uPIT for speaker-independent speech separation",
    "checked": true,
    "id": "bdaf593e762ba5b27ea658f11f946ee3ee9ca546",
    "semantic_title": "discriminative learning for monaural speech separation using deep embedding features",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yousefi19_interspeech.html": {
    "title": "Probabilistic Permutation Invariant Training for Speech Separation",
    "volume": "main",
    "abstract": "Single-microphone, speaker-independent speech separation is normally performed through two steps: (i) separating the specific speech sources, and (ii) determining the best output-label assignment to find the separation error. The second step is the main obstacle in training neural networks for speech separation. Recently proposed Permutation Invariant Training (PIT) addresses this problem by determining the output-label assignment which minimizes the separation error. In this study, we show that a major drawback of this technique is the overconfident choice of the output-label assignment, especially in the initial steps of training when the network generates unreliable outputs. To solve this problem, we propose Probabilistic PIT (Prob-PIT) which considers the output-label permutation as a discrete latent random variable with a uniform prior distribution. Prob-PIT defines a log-likelihood function based on the prior distributions and the separation errors of all permutations; it trains the speech separation networks by maximizing the log-likelihood function. Prob-PIT can be easily implemented by replacing the minimum function of PIT with a soft-minimum function. We evaluate our approach for speech separation on both TIMIT and CHiME datasets. The results show that the proposed method significantly outperforms PIT in terms of Signal to Distortion Ratio and Signal to Interference Ratio",
    "checked": true,
    "id": "a8583f63b06eb862ff7eb287879a2c9d4ee8761a",
    "semantic_title": "probabilistic permutation invariant training for speech separation",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shi19e_interspeech.html": {
    "title": "Which Ones Are Speaking? Speaker-Inferred Model for Multi-Talker Speech Separation",
    "volume": "main",
    "abstract": "Recent deep learning methods have gained noteworthy success in the multi-talker mixed speech separation task, which is also famous known as the Cocktail Party Problem. However, most existing models are well-designed towards some predefined conditions, which make them unable to handle the complex auditory scene automatically, such as a variable and unknown number of speakers in the mixture. In this paper, we propose a speaker-inferred model, based on the flexible and efficient Seq2Seq generation model, to accurately infer the possible speakers and the speech channel of each. Our model is totally end-to-end with several different modules to emphasize and better utilize the information from speakers. Without a priori knowledge about the number of speakers or any additional curriculum training strategy or man-made rules, our method gets comparable performance with those strong baselines",
    "checked": true,
    "id": "11f66646466af83789a66d5a4bc936b44c9b77d5",
    "semantic_title": "which ones are speaking? speaker-inferred model for multi-talker speech separation",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shi19f_interspeech.html": {
    "title": "End-to-End Monaural Speech Separation with Multi-Scale Dynamic Weighted Gated Dilated Convolutional Pyramid Network",
    "volume": "main",
    "abstract": "The monaural speech separation technology is far from satisfactory and has been a challenging task due to the interference of multiple sound sources. While deep dilated temporal convolutional networks (TCN) have been proved to be very effective in sequence modeling, this work investigates how to extend TCN to result in a new state-of-the-art approach for monaural speech separation. First a novel gating mechanisms is introduced and added to result in gated TCN. The gated activation can control the flow of information. Further in order to remedy the temporal scale variation problem caused by word length and pronunciation characteristics of different people, a multi-scale dynamic weighted pyramids gated TCNs is proposed, where a \"weightor\" network is used to determine the weights of different gated TCNs dynamically for each utterance. Since the strengths of different branches with different temporal receipt fields appear complementary, the combination outperforms single branch system. For the objective, we propose to train the network by directly optimizing utterance level signal-to-distortion ratio (SDR) in a permutation invariant training (PIT) style. Our experiments on the the WSJ0-2mix data corpus results in 18.4dB SDR improvement, which shows our proposed networks can leads to performance improvement on the speaker separation task",
    "checked": true,
    "id": "99ae5aaf6bfcf32ec3779f206dec3d099ee8a2c1",
    "semantic_title": "end-to-end monaural speech separation with multi-scale dynamic weighted gated dilated convolutional pyramid network",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lluis19_interspeech.html": {
    "title": "End-to-End Music Source Separation: Is it Possible in the Waveform Domain?",
    "volume": "main",
    "abstract": "Most of the currently successful source separation techniques use the magnitude spectrogram as input, and are therefore by default omitting part of the signal: the phase. To avoid omitting potentially useful information, we study the viability of using end-to-end models for music source separation — which take into account all the information available in the raw audio signal, including the phase. Although during the last decades end-to-end music source separation has been considered almost unattainable, our results confirm that waveform-based models can perform similarly (if not better) than a spectrogram-based deep learning model. Namely: a Wavenet-based model we propose and Wave-U-Net can outperform DeepConvSep, a recent spectrogram-based deep learning model",
    "checked": true,
    "id": "8fbf3fc768664932fc10526c8c620abbebe7e895",
    "semantic_title": "end-to-end music source separation: is it possible in the waveform domain?",
    "citation_count": 62
  },
  "https://www.isca-speech.org/archive/interspeech_2019/foley19_interspeech.html": {
    "title": "Elpis, an Accessible Speech-to-Text Tool",
    "volume": "main",
    "abstract": "Elpis is a speech-to-text tool which has been designed to give language workers, including linguists and speech scientists, access to cutting-edge automatic speech recognition software, without the specialist training typically required to run these systems. Our presentation would demonstrate local and server-based versions of Elpis using sample data from the Abui (ISO 639: abz) language, about 17,000 speakers in Indonesia. Attendees would gain a sense of the benefits that a first-pass ASR transcription can bring to transcription workflows",
    "checked": true,
    "id": "aeda2477b382634483f67721d31c52103557e68b",
    "semantic_title": "elpis, an accessible speech-to-text tool",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gruber19b_interspeech.html": {
    "title": "Framework for Conducting Tasks Requiring Human Assessment",
    "volume": "main",
    "abstract": "This paper presents a web-based framework that improves and simplifies the design and the deployment of tasks that require human input. These tasks may include speech, text or image transcription, annotation and evaluation. The focus is on listening tests for the purpose of a speech synthesis quality assessment. The framework is quite flexible, i.e. many different types of tasks can be prepared and presented to participants. The participants can then work on the tasks via a user-friendly GUI and their responses are recorded in a database. The framework is ready to be integrated as an external task for Amazon Mechanical Turk but it can also be used as a stand-alone platform",
    "checked": true,
    "id": "225e014b966ef3a439ff4433f56785a5d6506d54",
    "semantic_title": "framework for conducting tasks requiring human assessment",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19k_interspeech.html": {
    "title": "Multimedia Simultaneous Translation System for Minority Language Communication with Mandarin",
    "volume": "main",
    "abstract": "Speech recognition for minority language is always behind main stream due to lack of resources. This work presents a system for simultaneous translation between Mandarin and major minority languages such as Uyghur, Tibetan in shape of speech, text and images. The general acoustic model is trained via factorized TDNN with lattice free MMI criteria using mixed-units based lexicon model. For each specific language, acoustic model is trained by multi-task mix-lingual modeling with shared bottleneck layers followed by transfer learning. Besides, the system also supports state-of-the-art OCR, TTS, and machine translation, by which language information will be real-time translated, punctuated and pronounced. The machine translation behind the system gets a high rank in WMT 18 Mandarin-English and CWMT 18 minority language translation task. The system has integrated into a micro-app at WeChat and can facilitate communication between Mandarin and Minority languages",
    "checked": true,
    "id": "843b6a4a9539915d64f120947921b52ea2eeb08a",
    "semantic_title": "multimedia simultaneous translation system for minority language communication with mandarin",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dikici19_interspeech.html": {
    "title": "The SAIL LABS Media Mining Indexer and the CAVA Framework",
    "volume": "main",
    "abstract": "In today's attention-driven news economy, rapid changes of topics and events go hand in hand with rapid changes of vocabulary and of language use. ASR systems aimed at transcribing contents pertaining to this fluid media landscape need to keep up-to-date in a continuous and dynamic manner. Static models, potentially created a long time ago, are hopelessly outdated within a short period of time. The frequent changes in vocabulary and wording need to be reflected in the models employed for optimal performance of transcription if one does not want to risk falling behind. In this demonstration paper we present the audio processing capabilities of the SAIL LABS Media Mining Indexer, and the CAVA Framework allowing semi-automatic and periodic updates of the ASR vocabulary and language model from relevant and new data",
    "checked": true,
    "id": "eff7d9b034bac1efb9ac2218e7b5e5c745e88c83",
    "semantic_title": "the sail labs media mining indexer and the cava framework",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/goel19b_interspeech.html": {
    "title": "CaptionAI: A Real-Time Multilingual Captioning Application",
    "volume": "main",
    "abstract": "We demonstrate CaptionAI, the system that can be used for speech to text transcription, multilingual translation, and real-time closed captioning. It can also broadcast the audio and translated text to personal devices. There are three components of the application, namely, speech to text conversion, machine translation, and real time broadcast of audio and its multilingual text transcription. CaptionAI makes meetings, conference, and events accessible to global audience members with its real-time multilingual captioning and broadcast capabilities, improving comprehension and retention. In this application, we support English and Spanish real-time speech transcription. It also supports seventeen popular languages for real-time Machine Translation of transcribed speech. The front-end is coded on c# and in back-end we use combination of python and c++ based software and packages such as Janus, Gstreamer, and libwebsockets",
    "checked": true,
    "id": "59d831812c387110cceff4396daf7b820109476e",
    "semantic_title": "captionai: a real-time multilingual captioning application",
    "citation_count": 1
  }
}