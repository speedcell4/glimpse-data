{
  "https://www.isca-speech.org/archive/interspeech_2019/tokuda19_interspeech.html": {
    "title": "Statistical Approach to Speech Synthesis: Past, Present and Future",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "fa87441f84e6fe211195648b103707bbe9bd2367",
    "semantic_title": "statistical approach to speech synthesis: past, present and future",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wu19_interspeech.html": {
    "title": "Advances in Automatic Speech Recognition for Child Speech Using Factored Time Delay Neural Network",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) has shown huge advances in adult speech; however, when the models are tested on child speech, the performance does not achieve satisfactory word error rates (WER). This is mainly due to the high variance in acoustic features of child speech and the lack of clean, labeled corpora. We apply the factored time delay neural network (TDNN-F) to the child speech domain, finding that it yields better performance. To enable our models to handle the different noise conditions and extremely small corpora, we augment the original training data by adding noise and reverberation. Compared with conventional GMM-HMM and TDNN systems, TDNN-F does better on two widely accessible corpora: CMU Kids and CSLU Kids, and on the combination of these two. Our system achieves a 26% relative improvement in WER",
    "checked": true,
    "id": "beeaa7417e7818f737c2958550757735982fc49b",
    "semantic_title": "advances in automatic speech recognition for child speech using factored time delay neural network",
    "citation_count": 32
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yeung19_interspeech.html": {
    "title": "A Frequency Normalization Technique for Kindergarten Speech Recognition Inspired by the Role of fo in Vowel Perception",
    "volume": "main",
    "abstract": "Accurate automatic speech recognition (ASR) of kindergarten speech is particularly important as this age group may benefit the most from voice-based educational tools. Due to the lack of young child speech data, kindergarten ASR systems often are trained using older child or adult speech. This study proposes a fundamental frequency (f )-based normalization technique to reduce the spectral mismatch between kindergarten and older child speech. The technique is based on the tonotopic distances between formants and f developed to model vowel perception. This proposed procedure only relies on the computation of median f across an utterance. Tonotopic distances for vowel perception were reformulated as a linear relationship between formants and f to provide an effective approach for frequency normalization. This reformulation was verified by examining the formants and f of child vowel productions. A 208-word ASR experiment using older child speech for training and kindergarten speech for testing was performed to examine the effectiveness of the proposed technique against piecewise vocal tract length, F3-based, and subglottal resonance normalization techniques. Results suggest that the proposed technique either has performance advantages or requires the computation of fewer parameters",
    "checked": true,
    "id": "b521d1c626053e5252bc0fcd3e16d63efde5ba3c",
    "semantic_title": "a frequency normalization technique for kindergarten speech recognition inspired by the role of fo in vowel perception",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gale19_interspeech.html": {
    "title": "Improving ASR Systems for Children with Autism and Language Impairment Using Domain-Focused DNN Transfer Techniques",
    "volume": "main",
    "abstract": "This study explores building and improving an automatic speech recognition (ASR) system for children aged 6–9 years and diagnosed with autism spectrum disorder (ASD), language impairment (LI), or both. Working with only 1.5 hours of target data in which children perform the Clinical Evaluation of Language Fundamentals Recalling Sentences task, we apply deep neural network (DNN) weight transfer techniques to adapt a large DNN model trained on the LibriSpeech corpus of adult speech. To begin, we aim to find the best proportional training rates of the DNN layers. Our best configuration yields a 29.38% word error rate (WER). Using this configuration, we explore the effects of quantity and similarity of data augmentation in transfer learning. We augment our training with portions of the OGI Kids' Corpus, adding 4.6 hours of typically developing speakers aged kindergarten through 3 grade. We find that 2 grade data alone — approximately the mean age of the target data — outperforms other grades and all the sets combined. Doubling the data for 1 , 2 , and 3 grade, we again compare each grade as well as pairs of grades. We find the combination of 1 and 2 grade performs best at a 26.21% WER",
    "checked": true,
    "id": "9826fb6c64b76d5d0809a7c6ae5914342c782930",
    "semantic_title": "improving asr systems for children with autism and language impairment using domain-focused dnn transfer techniques",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ribeiro19_interspeech.html": {
    "title": "Ultrasound Tongue Imaging for Diarization and Alignment of Child Speech Therapy Sessions",
    "volume": "main",
    "abstract": "We investigate the automatic processing of child speech therapy sessions using ultrasound visual biofeedback, with a specific focus on complementing acoustic features with ultrasound images of the tongue for the tasks of speaker diarization and time-alignment of target words. For speaker diarization, we propose an ultrasound-based time-domain signal which we call estimated tongue activity. For word-alignment, we augment an acoustic model with low-dimensional representations of ultrasound images of the tongue, learned by a convolutional neural network. We conduct our experiments using the Ultrasuite repository of ultrasound and speech recordings for child speech therapy sessions. For both tasks, we observe that systems augmented with ultrasound data outperform corresponding systems using only the audio signal",
    "checked": true,
    "id": "e940bdd66421ab3d0d26434798344f846553c35f",
    "semantic_title": "ultrasound tongue imaging for diarization and alignment of child speech therapy sessions",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/loukina19_interspeech.html": {
    "title": "Automated Estimation of Oral Reading Fluency During Summer Camp e-Book Reading with MyTurnToRead",
    "volume": "main",
    "abstract": "Use of speech technologies in the classroom is often limited by the inferior acoustic conditions as well as other factors that might affect the quality of the recordings. We describe MyTurnToRead, an e-book-based app designed to support an interleaved listening and reading experience, where the child takes turns reading aloud with a virtual partner. The child's reading turns are recorded, and processed by an automated speech analysis system in order to provide feedback or track improvement in reading skill. We describe the architecture of the speech processing back-end and evaluate system performance on the data collected in several summer camps where children used the app on consumer-grade devices as part of the camp programming. We show that while the quality of the audio recordings varies greatly, our estimates of student oral reading fluency are very good: for example, the correlation between ASR-based and transcription-based estimates of reading fluency at the speaker level is r=0.93. These are also highly correlated with an external measure of reading comprehension",
    "checked": true,
    "id": "4b6866be7c111007cf4cb949b36c2123cc7f285e",
    "semantic_title": "automated estimation of oral reading fluency during summer camp e-book reading with myturntoread",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lopes19_interspeech.html": {
    "title": "Sustained Vowel Game: A Computer Therapy Game for Children with Dysphonia",
    "volume": "main",
    "abstract": "Problems in vocal quality are common in 4 to 12-year-old children, which may affect their health as well as their social interactions and development process. The sustained vowel exercise is widely used by speech and language pathologists for the child's voice recovery and vocal re-education. Nonetheless, despite being an important voice exercise, it can be a monotonous and tedious activity for children. Here, we propose a computer therapy game that uses the sustained vowel exercise to motivate children on doing this exercise often. In addition, the game gives visual feedback on the child's performance, which helps the child understand how to improve the voice production. The game uses a vowel classification model learned with a support vector machine and Mel frequency cepstral coefficients. A user test with 14 children showed that when using the game, children achieve longer phonation times than without the game. Also, it shows that the visual feedback helps and motivates children on improving their sustained vowel productions",
    "checked": true,
    "id": "594c6449ddd016ae5ac10079546d39b518cee779",
    "semantic_title": "sustained vowel game: a computer therapy game for children with dysphonia",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/esposito19_interspeech.html": {
    "title": "The Dependability of Voice on Elders' Acceptance of Humanoid Agents",
    "volume": "main",
    "abstract": "The research on ambient assistive technology is concerned with features humanoid agents should show in order to gain user acceptance. However, differently aged groups may have different requirements. This paper is particularly focused on agent's voice preferences among elders, young adults, and adolescents To this aim 316 users organized in groups of 45/46 subjects of which 3 groups of elders (65+ years old), 2 of young adults (aged between 22–35 years), and 2 of adolescents (aged between 14–16 years) were recruited and administered the Virtual Agent Acceptance Questionnaire (VAAQ), after watching video-clips of mute and speaking agents, in order to test their preferences in terms of willingness to interact, pragmatic and hedonic qualities, and attractiveness, of proposed speaking and mute agents. In addition, the elders were also tested on listening only the agent's. The results suggest that voice is primary for getting elder's acceptance of virtual humanoid agents in contrast to young adults and adolescents which accept equally well either mute or speaking agents",
    "checked": true,
    "id": "7e956e27098fe50c55e709ed2ba4247ac24f8f5e",
    "semantic_title": "the dependability of voice on elders' acceptance of humanoid agents",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/niebuhr19_interspeech.html": {
    "title": "God as Interlocutor — Real or Imaginary? Prosodic Markers of Dialogue Speech and Expected Efficacy in Spoken Prayer",
    "volume": "main",
    "abstract": "We analyze the phonetic correlates of petitionary prayer in 22 Christian practitioners. Our aim is to examine if praying is characterized by prosodic markers of dialogue speech and expected efficacy. Three similar conditions are compared; 1) requests to God, 2) requests to a human recipient, 3) requests to an imaginary person. We find that making requests to God is clearly distinguishable from making requests to both human and imaginary interlocutors. Requests to God are, unlike requests to an imaginary person, characterized by markers of dialogue speech (as opposed to monologue speech), including, a higher f0 level, a larger f0 range, and a slower speaking rate. In addition, requests to God differ from those made to both human and imaginary persons in markers of expected efficacy on the part of the speaker. These markers are related to a more careful speech production, including almost complete lack of hesitations, more pauses, and a much longer speaking time",
    "checked": false,
    "id": "8608531f9219f0f0b8ed895c26a9dfa3bd95cb17",
    "semantic_title": "god as interlocutor - real or imaginary? prosodic markers of dialogue speech and expected efficacy in spoken prayer",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cohn19_interspeech.html": {
    "title": "Expressiveness Influences Human Vocal Alignment Toward voice-AI",
    "volume": "main",
    "abstract": "This study explores whether people align to expressive speech spoken by a voice-activated artificially intelligent device (voice-AI), specifically Amazon's Alexa. Participants shadowed words produced by the Alexa voice in two acoustically distinct conditions: \"regular\" and \"expressive\", containing more exaggerated pitch contours and longer word durations. Another group of participants rated the shadowed items, in an AXB perceptual similarity task, as an assessment of overall degree of vocal alignment. Results show greater vocal alignment toward expressive speech produced by the Alexa voice and, furthermore, systematic variation based on speaker gender. Overall, these findings have applications to the field of affective computing in understanding human responses to synthesized emotional expressiveness",
    "checked": true,
    "id": "7499570fd0ddc4fe1da060aeb8a5fcec15f33755",
    "semantic_title": "expressiveness influences human vocal alignment toward voice-ai",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lai19_interspeech.html": {
    "title": "Detecting Topic-Oriented Speaker Stance in Conversational Speech",
    "volume": "main",
    "abstract": "Being able to detect topics and speaker stances in conversations is a key requirement for developing spoken language understanding systems that are personalized and adaptive. In this work, we explore how topic-oriented speaker stance is expressed in conversational speech. To do this, we present a new set of topic and stance annotations of the CallHome corpus of spontaneous dialogues. Specifically, we focus on six stances — positivity, certainty, surprise, amusement, interest, and comfort — which are useful for characterizing important aspects of a conversation, such as whether a conversation is going well or not. Based on this, we investigate the use of neural network models for automatically detecting speaker stance from speech in multi-turn, multi-speaker contexts. In particular, we examine how performance changes depending on how input feature representations are constructed and how this is related to dialogue structure. Our experiments show that incorporating both lexical and acoustic features is beneficial for stance detection. However, we observe variation in whether using hierarchical models for encoding lexical and acoustic information improves performance, suggesting that some aspects of speaker stance are expressed more locally than others. Overall, our findings highlight the importance of modelling interaction dynamics and non-lexical content for stance detection",
    "checked": true,
    "id": "ab76b65badae8872ce191ea6173d0fdf1f1c3536",
    "semantic_title": "detecting topic-oriented speaker stance in conversational speech",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sebastian19_interspeech.html": {
    "title": "Fusion Techniques for Utterance-Level Emotion Recognition Combining Speech and Transcripts",
    "volume": "main",
    "abstract": "In human perception and understanding, a number of different and complementary cues are adopted according to different modalities. Various emotional states in communication between humans reflect this variety of cues across modalities. Recent developments in multi-modal emotion recognition utilize deep-learning techniques to achieve remarkable performances, with models based on different features suitable for text, audio and vision. This work focuses on cross-modal fusion techniques over deep learning models for emotion detection from spoken audio and corresponding transcripts We investigate the use of long short-term memory (LSTM) recurrent neural network (RNN) with pre-trained word embedding for text-based emotion recognition and convolutional neural network (CNN) with utterance-level descriptors for emotion recognition from speech. Various fusion strategies are adopted on these models to yield an overall score for each of the emotional categories. Intra-modality dynamics for each emotion is captured in the neural network designed for the specific modality. Fusion techniques are employed to obtain the inter-modality dynamics. Speaker and session-independent experiments on IEMOCAP multi-modal emotion detection dataset show the effectiveness of the proposed approaches. This method yields state-of-the-art results for utterance-level emotion recognition based on speech and text",
    "checked": true,
    "id": "7b43738839277ba7123e8df056df983b79c14530",
    "semantic_title": "fusion techniques for utterance-level emotion recognition combining speech and transcripts",
    "citation_count": 37
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rajwadi19_interspeech.html": {
    "title": "Explaining Sentiment Classification",
    "volume": "main",
    "abstract": "This paper presents a novel 1-D sentiment classifier trained on the benchmark IMDB dataset. The classifier is a 1-D convolutional neural network with repeated convolution and max pooling layers. The main contribution of this work is the demonstration of a deconvolution technique for 1-D convolutional neural networks that is agnostic to specific architecture types. This deconvolution technique enables text classification to be explained, a feature that is important for NLP-based decision support systems, as well as being an invaluable diagnostic tool",
    "checked": true,
    "id": "03849bdecfc720717d3953c8fe9f4ded437f1d1b",
    "semantic_title": "explaining sentiment classification",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kleinlein19_interspeech.html": {
    "title": "Predicting Group-Level Skin Attention to Short Movies from Audio-Based LSTM-Mixture of Experts Models",
    "volume": "main",
    "abstract": "Electrodermal activity (EDA) is a psychophysiological indicator that can be considered a somatic marker of the emotional and attentional reaction of subjects towards stimuli like audiovisual content. EDA measurements are not biased by the cognitive process of giving an opinion or a score to characterize the subjective perception, and group-level EDA recordings integrate the reaction of an audience, thus reducing the signal noise. This paper contributes to the field of audience's attention prediction to video content, extending previous novel work on the use of EDA as ground truth for prediction algorithms. Videos are segmented into shorter clips attending to the audience's increasing or decreasing attention, and we process videos' audio waveform to extract meaningful aural embeddings from a VGGish model pretrained on the Audioset database. While previous similar work on attention level prediction using only audio accomplished 69.83% accuracy, we propose a Mixture of Experts approach to train a binary classifier that outperforms the main existing state-of-the-art approaches predicting increasing and decreasing attention levels with 81.76% accuracy. These results confirm the usefulness of providing acoustic features with a semantic significance, and the convenience of considering experts over partitions of the dataset in order to predict group-level attention from audio",
    "checked": true,
    "id": "13c30a3e0888f4d1af79c05fddaae06895380a4f",
    "semantic_title": "predicting group-level skin attention to short movies from audio-based lstm-mixture of experts models",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schluter19_interspeech.html": {
    "title": "Survey Talk: Modeling in Automatic Speech Recognition: Beyond Hidden Markov Models",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "17e5123e8954a077ab2e2e933b812eec5878a0ce",
    "semantic_title": "survey talk: modeling in automatic speech recognition: beyond hidden markov models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pham19_interspeech.html": {
    "title": "Very Deep Self-Attention Networks for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "Recently, end-to-end sequence-to-sequence models for speech recognition have gained significant interest in the research community. While previous architecture choices revolve around time-delay neural networks (TDNN) and long short-term memory (LSTM) recurrent neural networks, we propose to use self-attention via the Transformer architecture as an alternative. Our analysis shows that deep Transformer networks with high learning capacity are able to exceed performance from previous end-to-end approaches and even match the conventional hybrid systems. Moreover, we trained very deep models with up to 48 Transformer layers for both encoder and decoders combined with stochastic residual connections, which greatly improve generalizability and training efficiency. The resulting models outperform all previous end-to-end ASR approaches on the Switchboard benchmark. An ensemble of these models achieve 9.9% and 17.7% WER on Switchboard and CallHome test sets respectively. This finding brings our end-to-end models to competitive levels with previous hybrid systems. Further, with model ensembling the Transformers can outperform certain hybrid systems, which are more complicated in terms of both structure and training procedure",
    "checked": true,
    "id": "f2bb7e2f5a1afad5370159c15760c44df93c0438",
    "semantic_title": "very deep self-attention networks for end-to-end speech recognition",
    "citation_count": 143
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19_interspeech.html": {
    "title": "Jasper: An End-to-End Convolutional Neural Acoustic Model",
    "volume": "main",
    "abstract": "In this paper we report state-of-the-art results on LibriSpeech among end-to-end speech recognition models without any external training data. Our model, Jasper, uses only 1D convolutions, batch normalization, ReLU, dropout, and residual connections. To improve training, we further introduce a new layer-wise optimizer called NovoGrad. Through experiments, we demonstrate that the proposed deep architecture performs as well or better than more complex choices. Our deepest Jasper variant uses 54 convolutional layers. With this architecture, we achieve 2.95% WER using a beam-search decoder with an external neural language model and 3.86% WER with a greedy decoder on LibriSpeech test-clean. We also report competitive results on Wall Street Journal and the Hub5'00 conversational evaluation datasets",
    "checked": true,
    "id": "d85b2af4f163383bbfa62b73d5f0b179868cc9a8",
    "semantic_title": "jasper: an end-to-end convolutional neural acoustic model",
    "citation_count": 203
  },
  "https://www.isca-speech.org/archive/interspeech_2019/moritz19_interspeech.html": {
    "title": "Unidirectional Neural Network Architectures for End-to-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "In hybrid automatic speech recognition (ASR) systems, neural networks are used as acoustic models (AMs) to recognize phonemes that are composed to words and sentences using pronunciation dictionaries, hidden Markov models, and language models, which can be jointly represented by a weighted finite state transducer (WFST). The importance of capturing temporal context by an AM has been studied and discussed in prior work. In an end-to-end ASR system, however, all components are merged into a single neural network, i.e., the breakdown into an AM and the different parts of the WFST model is no longer possible. This implies that end-to-end neural network architectures have even stronger requirements for processing long contextual information. Bidirectional long short-term memory (BLSTM) neural networks have demonstrated state-of-the-art results in end-to-end ASR but are unsuitable for streaming applications. Latency-controlled BLSTMs account for this by limiting the future context seen by the backward directed recurrence using chunk-wise processing. In this paper, we propose two new unidirectional neural network architectures, the time-delay LSTM (TDLSTM) and the parallel time-delayed LSTM (PTDLSTM) streams, which both limit the processing latency to a fixed size and demonstrate significant improvements compared to prior art on a variety of ASR tasks",
    "checked": true,
    "id": "6aed9eedd5cb712c3d902152cb94f2f18ba4c729",
    "semantic_title": "unidirectional neural network architectures for end-to-end automatic speech recognition",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/belinkov19_interspeech.html": {
    "title": "Analyzing Phonetic and Graphemic Representations in End-to-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end neural network systems for automatic speech recognition (ASR) are trained from acoustic features to text transcriptions. In contrast to modular ASR systems, which contain separately-trained components for acoustic modeling, pronunciation lexicon, and language modeling, the end-to-end paradigm is both conceptually simpler and has the potential benefit of training the entire system on the end task. However, such neural network models are more opaque: it is not clear how to interpret the role of different parts of the network and what information it learns during training. In this paper, we analyze the learned internal representations in an end-to-end ASR model. We evaluate the representation quality in terms of several classification tasks, comparing phonemes and graphemes, as well as different articulatory features. We study two languages (English and Arabic) and three datasets, finding remarkable consistency in how different properties are represented in different layers of the deep neural network",
    "checked": true,
    "id": "facccce4f8d059d6156cf3ce536786eb43016939",
    "semantic_title": "analyzing phonetic and graphemic representations in end-to-end automatic speech recognition",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tawara19_interspeech.html": {
    "title": "Multi-Channel Speech Enhancement Using Time-Domain Convolutional Denoising Autoencoder",
    "volume": "main",
    "abstract": "This paper investigates the use of time-domain convolutional denoising autoencoders (TCDAEs) with multiple channels as a method of speech enhancement. In general, denoising autoencoders (DAEs), deep learning systems that map noise-corrupted into clean waveforms, have been shown to generate high-quality signals while working in the time domain without the intermediate stage of phase modeling. Convolutional DAEs are one of the popular structures which learns a mapping between noise-corrupted and clean waveforms with convolutional denoising autoencoder. Multi-channel signals for TCDAEs are promising because the different times of arrival of a signal can be directly processed with their convolutional structure, Up to this time, TCDAEs have only been applied to single-channel signals. This paper explorers the effectiveness of TCDAEs in a multi-channel configuration. A multi-channel TCDAEs are evaluated on multi-channel speech enhancement experiments, yielding significant improvement over single-channel DAEs in terms of signal-to-distortion ratio, perceptual evaluation of speech quality (PESQ), and word error rate",
    "checked": true,
    "id": "69161fc1b301054f7b4423085530e2dc6083255d",
    "semantic_title": "multi-channel speech enhancement using time-domain convolutional denoising autoencoder",
    "citation_count": 29
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tesch19_interspeech.html": {
    "title": "On Nonlinear Spatial Filtering in Multichannel Speech Enhancement",
    "volume": "main",
    "abstract": "Using multiple microphones for speech enhancement allows for exploiting spatial information for improved performance. In most cases, the spatial filter is selected to be a linear function of the input as, for example, the minimum variance distortionless response (MVDR) beamformer. For non-Gaussian distributed noise, however, the minimum mean square error (MMSE) optimal spatial filter may be nonlinear Potentially, such nonlinear functional relationships could be learned by deep neural networks. However, the performance would depend on many parameters and the architecture of the neural network. Therefore, in this paper, we more generally analyze the potential benefit of nonlinear spatial filters as a function of the multivariate kurtosis of the noise distribution The results imply that using a nonlinear spatial filter is only worth the effort if the noise data follows a distribution with a multivariate kurtosis that is considerably higher than for a Gaussian. In this case, we report a performance difference of up to 2.6 dB segmental signal-to-noise ratio (SNR) improvement for artificial stationary noise. We observe an advantage of 1.2dB for the nonlinear spatial filter over the linear one even for real-world noise data from the CHiME-3 dataset given oracle data for parameter estimation",
    "checked": false,
    "id": "8945e5c963f77d663cf9832c31dcd2da0469ada9",
    "semantic_title": "nonlinear spatial filtering in multichannel speech enhancement",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/martindonas19_interspeech.html": {
    "title": "Multi-Channel Block-Online Source Extraction Based on Utterance Adaptation",
    "volume": "main",
    "abstract": "This paper deals with multi-channel speech recognition in scenarios with multiple speakers. Recently, the spectral characteristics of a target speaker, extracted from an adaptation utterance, have been used to guide a neural network mask estimator to focus on that speaker. In this work we present two variants of speaker-aware neural networks, which exploit both spectral and spatial information to allow better discrimination between target and interfering speakers. Thus, we introduce either a spatial pre-processing prior to the mask estimation or a spatial plus spectral speaker characterization block whose output is directly fed into the neural mask estimator. The target speaker's spectral and spatial signature is extracted from an adaptation utterance recorded at the beginning of a session. We further adapt the architecture for low-latency processing by means of block-online beamforming that recursively updates the signal statistics. Experimental results show that the additional spatial information clearly improves source extraction, in particular in the same-gender case, and that our proposal achieves state-of-the-art performance in terms of distortion reduction and recognition accuracy",
    "checked": true,
    "id": "396c65bbf4098d767586ea0210a8bfa0b829405b",
    "semantic_title": "multi-channel block-online source extraction based on utterance adaptation",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bagheri19_interspeech.html": {
    "title": "Exploiting Multi-Channel Speech Presence Probability in Parametric Multi-Channel Wiener Filter",
    "volume": "main",
    "abstract": "In this paper, we present a practical implementation of the parametric multi-channel Wiener filter (PMWF) noise reduction algorithm. In particular, we extend on methods that incorporate the multi-channel speech presence probability (MC-SPP) in the PMWF derivation and its output. The use of the MC-SPP brings several advantages. Firstly, the MC-SPP allows for better estimates of noise and speech statistics, for which we derive a direct update of the inverse of the noise power spectral density (PSD). Secondly, the MC-SPP is used to control the trade-off parameter in PMWF which, with proper tuning, outperforms the traditional approach with a fixed trade-off parameter. Thirdly, the MC-SPP for each frequency-band is used to obtain the MMSE estimate of the desired speech signal at the output, where we control the maximum amount of noise reduction based on our application. Experimental results on a large number of simulated scenarios show significant benefits of employing MC-SPP in terms of SNR improvements and speech distortion",
    "checked": true,
    "id": "7eb423a18481d37e7d766e9b083a190effecc6a9",
    "semantic_title": "exploiting multi-channel speech presence probability in parametric multi-channel wiener filter",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/togami19_interspeech.html": {
    "title": "Variational Bayesian Multi-Channel Speech Dereverberation Under Noisy Environments with Probabilistic Convolutive Transfer Function",
    "volume": "main",
    "abstract": "In this paper, we propose a multi-channel speech dereverberation method which can reduce reverberation even when acoustic transfer functions (ATFs) are time varying under noisy environments. The microphone input signal is modeled as a convolutive mixture in a time-frequency domain so as to incorporate late reverberation whose tap length is longer than frame size of short term Fourier transform. To reduce reverberation effectively under the time-varying ATF conditions, the proposed method extends the deterministic convolutive transfer function (D-CTF) into a probabilistic convolutive transfer function (P-CTF). A variational Bayesian framework was applied to approximation of a joint posterior probability density functions of a speech source signal and the ATFs. Variational posterior probability density functions and the other parameters are iteratively updated so as to maximize an evidence lower bound (ELBO). Experimental results when the ATFs are time-varying and there is background noise showed that the proposed method can reduce reverberation more accurately than the Weighted Prediction error (WPE) and the Kalman-EM for dereverberation (KEMD)",
    "checked": true,
    "id": "96273401679e1d2da953e3367712c11fd17ee7f1",
    "semantic_title": "variational bayesian multi-channel speech dereverberation under noisy environments with probabilistic convolutive transfer function",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nakatani19_interspeech.html": {
    "title": "Simultaneous Denoising and Dereverberation for Low-Latency Applications Using Frame-by-Frame Online Unified Convolutional Beamformer",
    "volume": "main",
    "abstract": "This article presents frame-by-frame online processing algorithms for a Weighted Power minimization Distortionless response convolutional beamformer (WPD). The WPD unifies widely-used multichannel dereverberation and denoising methods, namely a weighted prediction error based dereverberation method (WPE) and a minimum power distortionless response beamformer (MPDR) into a single convolutional beamformer, and achieves simultaneous dereverberation and denoising based on maximum likelihood estimation. We derive two different online algorithms, one based on frame-by-frame recursive updating of the spatio-temporal covariance matrix of the captured signal, and the other on recursive least square estimation of the convolutional beamformer. In addition, for both algorithms, the desired signal's relative transfer function (RTF) is estimated by online processing using a neural network based online mask estimation. Experiments using the REVERB challenge dataset show the effectiveness of both algorithms in terms of objective speech enhancement measures and automatic speech recognition (ASR) performance",
    "checked": true,
    "id": "460ecb228570a9fbbb12d0cdfa4670a0110e3870",
    "semantic_title": "simultaneous denoising and dereverberation for low-latency applications using frame-by-frame online unified convolutional beamformer",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2019/snyder19_interspeech.html": {
    "title": "Individual Variation in Cognitive Processing Style Predicts Differences in Phonetic Imitation of Device and Human Voices",
    "volume": "main",
    "abstract": "Phonetic imitation, or implicitly matching the acoustic-phonetic patterns of another speaker, has been empirically associated with natural tendencies to promote successful social communication, as well as individual differences in personality and cognitive processing style. The present study explores whether individual differences in cognitive processing style, as indexed by self-reported scored from the Autism-Spectrum Quotient (AQ) questionnaire, are linked to the way people imitate the vocal productions by two digital device voices (i.e., Apple's Siri) and two human voices. Subjects first performed a word shadowing task of human and device voices and then completed the self-administered AQ. We assessed imitation of two acoustic properties: f0 and vowel duration. We find that the attention to detail and the imagination subscale scores on the AQ mediated degree of imitation of f0 and vowel duration, respectively. The findings yield new insight to speech production and perception mechanisms and how it interacts with individual cognitive processing style differences",
    "checked": true,
    "id": "b3516010239a9e4fd3061581793a335fe4615682",
    "semantic_title": "individual variation in cognitive processing style predicts differences in phonetic imitation of device and human voices",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2019/illa19_interspeech.html": {
    "title": "An Investigation on Speaker Specific Articulatory Synthesis with Speaker Independent Articulatory Inversion",
    "volume": "main",
    "abstract": "Estimating speech representations from articulatory movements is known as articulatory-to-acoustic forward (AAF) mapping. Typically this mapping is learned using directly measured articulatory movement in a subject-specific manner. Such AAF mapping has been shown to benefit the speech synthesis applications. In this work, we investigate the speaker similarity and naturalness of utterances generated by AAF which is driven by the articulatory movements from a subject (referred to as cross speaker) different from the speaker (target speaker) used for training AAF mapping. Experiments are performed with directly measured articulatory data from 9 speakers (8 target speakers and 1 cross speaker), which are recorded using Electromagnetic articulograph AG501. Experiments are also performed with articulatory features estimated using speaker independent acoustic-to-articulatory inversion (SI-AAI) model trained on 26 reference speakers. Objective evaluation on target speakers reveal that the articulatory features estimated from SI-AAI result in a lower Mel-cepstrum distortion compared to that using directly measured articulatory features. Further, listening tests reveal that the directly measured articulatory movements preserve the speaker similarity better than estimated ones. Although, for naturalness, articulatory movements predicted by SI-AAI perform better than the direct measurements",
    "checked": true,
    "id": "9e3b686c1443d8e9d263de95cd46fcd9c825edd8",
    "semantic_title": "an investigation on speaker specific articulatory synthesis with speaker independent articulatory inversion",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19_interspeech.html": {
    "title": "Individual Difference of Relative Tongue Size and its Acoustic Effects",
    "volume": "main",
    "abstract": "This study examines how the speaker's tongue size contributes to generating dynamic characteristics of speaker individuality. The relative tongue size (RTS) has been proposed as an index for the tongue area within the oropharyngeal cavity on the midsagittal magnetic resonance imaging (MRI). Our earlier studies have shown that the smaller the RTS, the faster the tongue movement. In this study, acoustic consequences of individual RTS values were analyzed by comparing tongue movement velocity and formant transition rate. The materials used were cine-MRI data and acoustic signals during production of a sentence and two words produced by two female speakers with contrasting RTS values. The results indicate that the speaker with the small RTS value exhibited the faster changes of tongue positions and formant transitions than the speakers with the large RTS values. Since the tongue size is uncontrollable by a speaker's intention, the RTS can be regarded as one of the causal factors of dynamic individual characteristics in the lower frequency region of speech signals",
    "checked": true,
    "id": "68d0c775c0c976117da4b914321e89e5cfa9a717",
    "semantic_title": "individual difference of relative tongue size and its acoustic effects",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yoshinaga19_interspeech.html": {
    "title": "Individual Differences of Airflow and Sound Generation in the Vocal Tract of Sibilant /s/",
    "volume": "main",
    "abstract": "To clarify the individual differences of flow and sound characteristics of sibilant /s/, the large eddy simulation of compressible flow was applied to vocal tract geometries of five subjects pronouncing /s/. The vocal tract geometry was extracted by separately collecting images of digital dental casts and the vocal tract of /s/. The computational grids were constructed for each geometry, and flow and acoustic fields were predicted by the simulation. Results of the simulation showed that jet flow in the vocal tract was disturbed and fluctuated, and the sound source of /s/ was generated in different place for each subject. With an increment of the jet velocity, not only the overall sound amplitude but also the spectral mean was increased, indicating that the increment of the jet velocity contributes to the increase of amplitudes in a higher frequency range among different vocal tract geometries",
    "checked": true,
    "id": "7b273a1c00e585e0677ed99968cf1c18ecbe706f",
    "semantic_title": "individual differences of airflow and sound generation in the vocal tract of sibilant /s/",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/uttam19_interspeech.html": {
    "title": "Hush-Hush Speak: Speech Reconstruction Using Silent Videos",
    "volume": "main",
    "abstract": "Speech Reconstruction is the task of recreation of speech using silent videos as input. In the literature, it is also referred to as lipreading. In this paper, we design an encoder-decoder architecture which takes silent videos as input and outputs an audio spectrogram of the reconstructed speech. The model, despite being a speaker-independent model, achieves comparable results on speech reconstruction to the current state-of-the-art speaker-dependent model. We also perform user studies to infer speech intelligibility. Additionally, we test the usability of the trained model using bilingual speech",
    "checked": true,
    "id": "ec9fda5db1ced6058824ce0c8fe9cb6e1d20c24e",
    "semantic_title": "hush-hush speak: speech reconstruction using silent videos",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/saha19_interspeech.html": {
    "title": "SPEAK YOUR MIND! Towards Imagined Speech Recognition with Hierarchical Deep Learning",
    "volume": "main",
    "abstract": "Speech-related Brain Computer Interface (BCI) technologies provide effective vocal communication strategies for controlling devices through speech commands interpreted from brain signals. In order to infer imagined speech from active thoughts, we propose a novel hierarchical deep learning BCI system for subject-independent classification of 11 speech tokens including phonemes and words. Our novel approach exploits predicted articulatory information of six phonological categories (e.g., nasal, bilabial) as an intermediate step for classifying the phonemes and words, thereby finding discriminative signal responsible for natural speech synthesis. The proposed network is composed of hierarchical combination of spatial and temporal CNN cascaded with a deep autoencoder. Our best models on the KARA database achieve an average accuracy of 83.42% across the six different binary phonological classification tasks, and 53.36% for the individual token identification task, significantly outperforming our baselines. Ultimately, our work suggests the possible existence of a brain imagery footprint for the underlying articulatory movement related to different sounds that can be used to aid imagined speech decoding",
    "checked": true,
    "id": "fb2029d6587f3099b9b62b3abc601c64bd48fefc",
    "semantic_title": "speak your mind! towards imagined speech recognition with hierarchical deep learning",
    "citation_count": 31
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chung19_interspeech.html": {
    "title": "An Unsupervised Autoregressive Model for Speech Representation Learning",
    "volume": "main",
    "abstract": "This paper proposes a novel unsupervised autoregressive neural model for learning generic speech representations. In contrast to other speech representation learning methods that aim to remove noise or speaker variabilities, ours is designed to preserve information for a wide range of downstream tasks. In addition, the proposed model does not require any phonetic or word boundary labels, allowing the model to benefit from large quantities of unlabeled data. Speech representations learned by our model significantly improve performance on both phone classification and speaker verification over the surface features and other supervised and unsupervised approaches. Further analysis shows that different levels of speech information are captured by our model at different layers. In particular, the lower layers tend to be more discriminative for speakers, while the upper layers provide more phonetic content",
    "checked": true,
    "id": "2f803165d054ee89bec2401368ceb9e75bad8b60",
    "semantic_title": "an unsupervised autoregressive model for speech representation learning",
    "citation_count": 322
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19_interspeech.html": {
    "title": "Harmonic-Aligned Frame Mask Based on Non-Stationary Gabor Transform with Application to Content-Dependent Speaker Comparison",
    "volume": "main",
    "abstract": "We propose harmonic-aligned frame mask for speech signals using non-stationary Gabor transform (NSGT). A frame mask operates on the transfer coefficients of a signal and consequently converts the signal into a counterpart signal. It depicts the difference between the two signals. In preceding studies, frame masks based on regular Gabor transform were applied to single-note instrumental sound analysis. This study extends the frame mask approach to speech signals. For voiced speech, the fundamental frequency is usually changing consecutively over time. We employ NSGT with pitch-dependent and therefore time-varying frequency resolution to attain harmonic alignment in the transform domain and hence yield harmonic-aligned frame masks for speech signals. We propose to apply the harmonic-aligned frame mask to content-dependent speaker comparison. Frame masks, computed from voiced signals of a same vowel but from different speakers, were utilized as similarity measures to compare and distinguish the speaker identities (SID). Results obtained with deep neural networks demonstrate that the proposed frame mask is valid in representing speaker characteristics and shows a potential for SID applications in limited data scenarios",
    "checked": true,
    "id": "7cdcd85e2e9e44c5b1040079313d48886ce27553",
    "semantic_title": "harmonic-aligned frame mask based on non-stationary gabor transform with application to content-dependent speaker comparison",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/m19_interspeech.html": {
    "title": "Glottal Closure Instants Detection from Speech Signal by Deep Features Extracted from Raw Speech and Linear Prediction Residual",
    "volume": "main",
    "abstract": "Glottal closure instants (GCI) also called as instants of significant excitation occur during abrupt closure of vocal folds is a well-studied problem for its many potential applications in speech processing. Speech signal or its transformed linear prediction residual (LPR) is the most popular signal representations for GCI detection. In this paper, we propose a supervised classification based GCI detection method, in which, we train multiple convolution neural networks to determine the suitable feature representation for efficient GCI detection. Also, we show that the combined model trained with joint acoustic-residual deep features and the model trained with low pass filtered speech significantly increases the detection accuracy. We have manually annotated the speech signal for ground truth GCI using electroglottograph (EGG) as a reference signal. The evaluation results showed that the proposed model trained with very small and less diverse data performs significantly better than the traditional signal processing and most recent data-driven approaches",
    "checked": true,
    "id": "bf70044aefbf7c8433e93116f1ed9a4e85ca669c",
    "semantic_title": "glottal closure instants detection from speech signal by deep features extracted from raw speech and linear prediction residual",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pascual19_interspeech.html": {
    "title": "Learning Problem-Agnostic Speech Representations from Multiple Self-Supervised Tasks",
    "volume": "main",
    "abstract": "Learning good representations without supervision is still an open issue in machine learning, and is particularly challenging for speech signals, which are often characterized by long sequences with a complex hierarchical structure. Some recent works, however, have shown that it is possible to derive useful speech representations by employing a self-supervised encoder-discriminator approach. This paper proposes an improved self-supervised method, where a single neural encoder is followed by multiple workers that jointly solve different self-supervised tasks. The needed consensus across different tasks naturally imposes meaningful constraints to the encoder, contributing to discover general representations and to minimize the risk of learning superficial ones. Experiments show that the proposed approach can learn transferable, robust, and problem-agnostic features that carry on relevant information from the speech signal, such as speaker identity, phonemes, and even higher-level features such as emotional cues. In addition, a number of design choices make the encoder easily exportable, facilitating its direct usage or adaptation to different problems",
    "checked": true,
    "id": "b3beb9bc7395a8a489b9c64c46329a84d45968bd",
    "semantic_title": "learning problem-agnostic speech representations from multiple self-supervised tasks",
    "citation_count": 197
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nellore19_interspeech.html": {
    "title": "Excitation Source and Vocal Tract System Based Acoustic Features for Detection of Nasals in Continuous Speech",
    "volume": "main",
    "abstract": "The aim of the current study is to propose acoustic features for detection of nasals in continuous speech. Acoustic features that represent certain characteristics of speech production are extracted. Features representing excitation source characteristics are extracted using zero frequency filtering method. Features representing vocal tract system characteristics are extracted using zero time windowing method Feature sets are formed by combining certain subsets of the features mentioned above. These feature sets are evaluated for their representativeness of nasals in continuous speech in three different languages, namely, English, Hindi and Telugu. Results show that nasal detection is reliable and consistent across all the languages mentioned above",
    "checked": true,
    "id": "997642aa05d29b542130ef90653e306fae7c8260",
    "semantic_title": "excitation source and vocal tract system based acoustic features for detection of nasals in continuous speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chatziagapi19_interspeech.html": {
    "title": "Data Augmentation Using GANs for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "In this work, we address the problem of data imbalance for the task of Speech Emotion Recognition (SER). We investigate conditioned data augmentation using Generative Adversarial Networks (GANs), in order to generate samples for underrepresented emotions. We adapt and improve a conditional GAN architecture to generate synthetic spectrograms for the minority class. For comparison purposes, we implement a series of signal-based data augmentation methods. The proposed GAN-based approach is evaluated on two datasets, namely IEMOCAP and FEEL-25k, a large multi-domain dataset. Results demonstrate a 10% relative performance improvement in IEMOCAP and 5% in FEEL-25k, when augmenting the minority classes",
    "checked": true,
    "id": "f3c25b8aa1f7e2a00b947c38d52dcaa6b3da31bd",
    "semantic_title": "data augmentation using gans for speech emotion recognition",
    "citation_count": 82
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kons19_interspeech.html": {
    "title": "High Quality, Lightweight and Adaptable TTS Using LPCNet",
    "volume": "main",
    "abstract": "We present a lightweight adaptable neural TTS system with high quality output. The system is composed of three separate neural network blocks: prosody prediction, acoustic feature prediction and Linear Prediction Coding Net as a neural vocoder. This system can synthesize speech with close to natural quality while running 3 times faster than real-time on a standard CPU The modular setup of the system allows for simple adaptation to new voices with a small amount of data We first demonstrate the ability of the system to produce high quality speech when trained on large, high quality datasets. Following that, we demonstrate its adaptability by mimicking unseen voices using 5 to 20 minutes long datasets with lower recording quality. Large scale Mean Opinion Score quality and similarity tests are presented, showing that the system can adapt to unseen voices with quality gap of 0.12 and similarity gap of 3% compared to natural speech for male voices and quality gap of 0.35 and similarity of gap of 9% for female voices",
    "checked": true,
    "id": "52daae0ff7d09d4b11ab447fa0cb57e2ba1d12b6",
    "semantic_title": "high quality, lightweight and adaptable tts using lpcnet",
    "citation_count": 46
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lorenzotrueba19_interspeech.html": {
    "title": "Towards Achieving Robust Universal Neural Vocoding",
    "volume": "main",
    "abstract": "This paper explores the potential universality of neural vocoders. We train a WaveRNN-based vocoder on 74 speakers coming from 17 languages. This vocoder is shown to be capable of generating speech of consistently good quality (98% relative mean MUSHRA when compared to natural speech) regardless of whether the input spectrogram comes from a speaker or style seen during training or from an out-of-domain scenario when the recording conditions are studio-quality. When the recordings show significant changes in quality, or when moving towards non-speech vocalizations or singing, the vocoder still significantly outperforms speaker-dependent vocoders, but operates at a lower average relative MUSHRA of 75%. These results are shown to be consistent across languages, regardless of them being seen during training (e.g. English or Japanese) or unseen (e.g. Wolof, Swahili, Ahmaric)",
    "checked": true,
    "id": "93d857da76fdeeece7ed641f4d48e1e9770e8315",
    "semantic_title": "towards achieving robust universal neural vocoding",
    "citation_count": 78
  },
  "https://www.isca-speech.org/archive/interspeech_2019/neekhara19_interspeech.html": {
    "title": "Expediting TTS Synthesis with Adversarial Vocoding",
    "volume": "main",
    "abstract": "Recent approaches in text-to-speech (TTS) synthesis employ neural network strategies to vocode perceptually-informed spectrogram representations directly into listenable waveforms. Such vocoding procedures create a computational bottleneck in modern TTS pipelines. We propose an alternative approach which utilizes generative adversarial networks (GANs) to learn mappings from perceptually-informed spectrograms to simple magnitude spectrograms which can be heuristically vocoded. Through a user study, we show that our approach significantly outperforms naïve vocoding strategies while being hundreds of times faster than neural network vocoders used in state-of-the-art TTS systems. We also show that our method can be used to achieve state-of-the-art results in unsupervised synthesis of individual words of speech",
    "checked": true,
    "id": "3b978703968c2e3f8a41b0d34f870bfc2228677f",
    "semantic_title": "expediting tts synthesis with adversarial vocoding",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mustafa19_interspeech.html": {
    "title": "Analysis by Adversarial Synthesis — A Novel Approach for Speech Vocoding",
    "volume": "main",
    "abstract": "Classical parametric speech coding techniques provide a compact representation for speech signals. This affords a very low transmission rate but with a reduced perceptual quality of the reconstructed signals. Recently, autoregressive deep generative models such as WaveNet and SampleRNN have been used as speech vocoders to scale up the perceptual quality of the reconstructed signals without increasing the coding rate. However, such models suffer from a very slow signal generation mechanism due to their sample-by-sample modelling approach. In this work, we introduce a new methodology for neural speech vocoding based on generative adversarial networks (GANs). A fake speech signal is generated from a very compressed representation of the glottal excitation using conditional GANs as a deep generative model. This fake speech is then refined using the LPC parameters of the original speech signal to obtain a natural reconstruction. The reconstructed speech waveforms based on this approach show a higher perceptual quality than the classical vocoder counterparts according to subjective and objective evaluation scores for a dataset of 30 male and female speakers. Moreover, the usage of GANs enables to generate signals in one-shot compared to autoregressive generative models. This makes GANs promising for exploration to implement high-quality neural vocoders",
    "checked": false,
    "id": "07a9dc45559f8c4cc2a8740e36e18f29db8e7a3d",
    "semantic_title": "analysis by adversarial synthesis - a novel approach for speech vocoding",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wu19b_interspeech.html": {
    "title": "Quasi-Periodic WaveNet Vocoder: A Pitch Dependent Dilated Convolution Model for Parametric Speech Generation",
    "volume": "main",
    "abstract": "In this paper, we propose a quasi-periodic neural network (QPNet) vocoder with a novel network architecture named pitch-dependent dilated convolution (PDCNN) to improve the pitch controllability of WaveNet (WN) vocoder. The effectiveness of the WN vocoder to generate high-fidelity speech samples from given acoustic features has been proved recently. However, because of the fixed dilated convolution and generic network architecture, the WN vocoder hardly generates speech with given F values which are outside the range observed in training data. Consequently, the WN vocoder lacks the pitch controllability which is one of the essential capabilities of conventional vocoders. To address this limitation, we propose the PDCNN component which has the time-variant adaptive dilation size related to the given F values and a cascade network structure of the QPNet vocoder to generate quasi-periodic signals such as speech. Both objective and subjective tests are conducted, and the experimental results demonstrate the better pitch controllability of the QPNet vocoder compared to the same and double sized WN vocoders while attaining comparable speech qualities",
    "checked": true,
    "id": "ff01789535aa1535f610739176083f17c6239d2f",
    "semantic_title": "quasi-periodic wavenet vocoder: a pitch dependent dilated convolution model for parametric speech generation",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tian19_interspeech.html": {
    "title": "A Speaker-Dependent WaveNet for Voice Conversion with Non-Parallel Data",
    "volume": "main",
    "abstract": "In a typical voice conversion system, vocoder is commonly used for speech-to-features analysis and features-to-speech synthesis. However, vocoder can be a source of speech quality degradation. This paper presents a novel approach to voice conversion using WaveNet for non-parallel training data. Instead of reconstructing speech with intermediate features, the proposed approach utilizes the WaveNet to map the Phonetic PosteriorGrams (PPGs) to the waveform samples directly. In this way, we avoid the estimation errors arising from vocoding and feature conversion. Additionally, as PPG is assumed to be speaker independent, the proposed approach also reduces the feature mismatch problem in WaveNet vocoder based solutions. Experimental results conducted on the CMU-ARCTIC database show that the proposed approach significantly outperforms the traditional vocoder and WaveNet Vocoder baselines in terms of speech quality",
    "checked": true,
    "id": "b8360a4ec8d54f5bbc4b8081730a47b9c7fe026e",
    "semantic_title": "a speaker-dependent wavenet for voice conversion with non-parallel data",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2019/han19_interspeech.html": {
    "title": "Survey Talk: When Attention Meets Speech Applications: Speech & Speaker Recognition Perspective",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "984a38f397b3211963b36e2049b945afaadaebbf",
    "semantic_title": "survey talk: when attention meets speech applications: speech & speaker recognition perspective",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhao19_interspeech.html": {
    "title": "Attention-Enhanced Connectionist Temporal Classification for Discrete Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Discrete speech emotion recognition (SER), the assignment of a single emotion label to an entire speech utterance, is typically performed as a sequence-to-label task. This approach, however, is limited, in that it can result in models that do not capture temporal changes in the speech signal, including those indicative of a particular emotion. One potential solution to overcome this limitation is to model SER as a sequence-to-sequence task instead. In this regard, we have developed an attention-based bidirectional long short-term memory (BLSTM) neural network in combination with a connectionist temporal classification (CTC) objective function (Attention-BLSTM-CTC) for SER. We also assessed the benefits of incorporating two contemporary attention mechanisms, namely component attention and quantum attention, into the CTC framework. To the best of the authors' knowledge, this is the first time that such a hybrid architecture has been employed for SER.We demonstrated the effectiveness of our approach on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) and FAU-Aibo Emotion corpora. The experimental results demonstrate that our proposed model outperforms current state-of-the-art approaches",
    "checked": true,
    "id": "920f779bef257922d7685244f1a7afc1e4d6ad86",
    "semantic_title": "attention-enhanced connectionist temporal classification for discrete speech emotion recognition",
    "citation_count": 45
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19b_interspeech.html": {
    "title": "Attentive to Individual: A Multimodal Emotion Recognition Network with Personalized Attention Profile",
    "volume": "main",
    "abstract": "A growing number of human-centered applications benefit from continuous advancements in the emotion recognition technology. Many emotion recognition algorithms have been designed to model multimodal behavior cues to achieve high performances. However, most of them do not consider the modulating factors of an individual's personal attributes in his/her expressive behaviors. In this work, we propose a Personalized Attributes-Aware Attention Network (PAaAN) with a novel personalized attention mechanism to perform emotion recognition using speech and language cues. The attention profile is learned from embeddings of an individual's profile, acoustic, and lexical behavior data. The profile embedding is derived using linguistics inquiry word count computed between the target speaker and a large set of movie scripts. Our method achieves the state-of-the-art 70.3% unweighted accuracy in a four class emotion recognition task on the IEMOCAP. Further analysis reveals that affect-related semantic categories are emphasized differently for each speaker in the corpus showing the effectiveness of our attention mechanism for personalization",
    "checked": true,
    "id": "1515d8597ef03ff52cda5bc3551fbdee6350c2bd",
    "semantic_title": "attentive to individual: a multimodal emotion recognition network with personalized attention profile",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gallardoantolin19_interspeech.html": {
    "title": "A Saliency-Based Attention LSTM Model for Cognitive Load Classification from Speech",
    "volume": "main",
    "abstract": "Cognitive Load (CL) refers to the amount of mental demand that a given task imposes on an individual's cognitive system and it can affect his/her productivity in very high load situations. In this paper, we propose an automatic system capable of classifying the CL level of a speaker by analyzing his/her voice. Our research on this topic goes into two main directions. In the first one, we focus on the use of Long Short-Term Memory (LSTM) networks with different weighted pooling strategies for CL level classification. In the second contribution, for overcoming the need of a large amount of training data, we propose a novel attention mechanism that uses the Kalinli's auditory saliency model. Experiments show that our proposal outperforms significantly both, a baseline system based on Support Vector Machines (SVM) and a LSTM-based system with logistic regression attention model",
    "checked": true,
    "id": "af4f0d9acf3841703855891c12ce24f6b41da608",
    "semantic_title": "a saliency-based attention lstm model for cognitive load classification from speech",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mallolragolta19_interspeech.html": {
    "title": "A Hierarchical Attention Network-Based Approach for Depression Detection from Transcribed Clinical Interviews",
    "volume": "main",
    "abstract": "The high prevalence of depression in society has given rise to a need for new digital tools that can aid its early detection. Among other effects, depression impacts the use of language. Seeking to exploit this, this work focuses on the detection of depressed and non-depressed individuals through the analysis of linguistic information extracted from transcripts of clinical interviews with a virtual agent. Specifically, we investigated the advantages of employing hierarchical attention-based networks for this task. Using Global Vectors (GloVe) pretrained word embedding models to extract low-level representations of the words, we compared hierarchical local-global attention networks and hierarchical contextual attention networks. We performed our experiments on the Distress Analysis Interview Corpus - Wizard of Oz (DAIC-WoZ) dataset, which contains audio, visual, and linguistic information acquired from participants during a clinical session. Our results using the DAIC-WoZ test set indicate that hierarchical contextual attention networks are the most suitable configuration to detect depression from transcripts. The configuration achieves an Unweighted Average Recall (UAR) of .66 using the test set, surpassing our baseline, a Recurrent Neural Network that does not use attention",
    "checked": true,
    "id": "8db761dc173e30b0882390892fe92af7acd11208",
    "semantic_title": "a hierarchical attention network-based approach for depression detection from transcribed clinical interviews",
    "citation_count": 30
  },
  "https://www.isca-speech.org/archive/interspeech_2019/carmantini19_interspeech.html": {
    "title": "Untranscribed Web Audio for Low Resource Speech Recognition",
    "volume": "main",
    "abstract": "Speech recognition models are highly susceptible to mismatch in the acoustic and language domains between the training and the evaluation data. For low resource languages, it is difficult to obtain transcribed speech for target domains, while untranscribed data can be collected with minimal effort. Recently, a method applying lattice-free maximum mutual information (LF-MMI) to untranscribed data has been found to be effective for semi-supervised training. However, weaker initial models and domain mismatch can result in high deletion rates for the semi-supervised model. Therefore, we propose a method to force the base model to overgenerate possible transcriptions, relying on the ability of LF-MMI to deal with uncertainty. On data from the IARPA MATERIAL programme, our new semi-supervised method outperforms the standard semi-supervised method, yielding significant gains when adapting for mismatched bandwidth and domain",
    "checked": true,
    "id": "07a873b8a6c00dc72978ef7e98160b3e245c4bca",
    "semantic_title": "untranscribed web audio for low resource speech recognition",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/luscher19_interspeech.html": {
    "title": "RWTH ASR Systems for LibriSpeech: Hybrid vs Attention",
    "volume": "main",
    "abstract": "We present state-of-the-art automatic speech recognition (ASR) systems employing a standard hybrid DNN/HMM architecture compared to an attention-based encoder-decoder design for the LibriSpeech task. Detailed descriptions of the system development, including model design, pretraining schemes, training schedules, and optimization approaches are provided for both system architectures. Both hybrid DNN/HMM and attention-based systems employ bi-directional LSTMs for acoustic modeling/encoding. For language modeling, we employ both LSTM and Transformer based architectures. All our systems are built using RWTH's open-source toolkits RASR and RETURNN. To the best knowledge of the authors, the results obtained when training on the full LibriSpeech training set, are the best published currently, both for the hybrid DNN/HMM and the attention-based systems. Our single hybrid system even outperforms previous results obtained from combining eight single systems. Our comparison shows that on the LibriSpeech 960h task, the hybrid DNN/HMM system outperforms the attention-based system by 15% relative on the clean and 40% relative on the other test sets in terms of word error rate. Moreover, experiments on a reduced 100h-subset of the LibriSpeech training corpus even show a more pronounced margin between the hybrid DNN/HMM and attention-based architectures",
    "checked": false,
    "id": "744196b6cb5091c0760d05ef068a92a6cd531587",
    "semantic_title": "rwth asr systems for librispeech: hybrid vs attention - w/o data augmentation",
    "citation_count": 211
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kanda19_interspeech.html": {
    "title": "Auxiliary Interference Speaker Loss for Target-Speaker Speech Recognition",
    "volume": "main",
    "abstract": "In this paper, we propose a novel auxiliary loss function for target-speaker automatic speech recognition (ASR). Our method automatically extracts and transcribes target speaker's utterances from a monaural mixture of multiple speakers speech given a short sample of the target speaker. The proposed auxiliary loss function attempts to additionally maximize interference speaker ASR accuracy during training. This will regularize the network to achieve a better representation for speaker separation, thus achieving better accuracy on the target-speaker ASR. We evaluated our proposed method using two-speaker-mixed speech in various signal-to-interference-ratio conditions. We first built a strong target-speaker ASR baseline based on the state-of-the-art lattice-free maximum mutual information. This baseline achieved a word error rate (WER) of 18.06% on the test set while a normal ASR trained with clean data produced a completely corrupted result (WER of 84.71%). Then, our proposed loss further reduced the WER by 6.6% relative to this strong baseline, achieving a WER of 16.87%. In addition to the accuracy improvement, we also showed that the auxiliary output branch for the proposed loss can even be used for a secondary ASR for interference speakers' speech",
    "checked": true,
    "id": "388d41b99c9c0867301f345c65877a2796225ead",
    "semantic_title": "auxiliary interference speaker loss for target-speaker speech recognition",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2019/meng19_interspeech.html": {
    "title": "Speaker Adaptation for Attention-Based End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "We propose three regularization-based speaker adaptation approaches to adapt the attention-based encoder-decoder (AED) model with very limited adaptation data from target speakers for end-to-end automatic speech recognition. The first method is Kullback-Leibler divergence (KLD) regularization, in which the output distribution of a speaker-dependent (SD) AED is forced to be close to that of the speaker-independent (SI) model by adding a KLD regularization to the adaptation criterion. To compensate for the asymmetric deficiency in KLD regularization, an adversarial speaker adaptation (ASA) method is proposed to regularize the deep-feature distribution of the SD AED through the adversarial learning of an auxiliary discriminator and the SD AED. The third approach is the multi-task learning, in which an SD AED is trained to jointly perform the primary task of predicting a large number of output units and an auxiliary task of predicting a small number of output units to alleviate the target sparsity issue. Evaluated on a Microsoft short message dictation task, all three methods are highly effective in adapting the AED model, achieving up to 12.2% and 3.0% word error rate improvement over an SI AED trained from 3400 hours data for supervised and unsupervised adaptation, respectively",
    "checked": true,
    "id": "ce77b9212751e1afd10c7fccd5271a806dfbb445",
    "semantic_title": "speaker adaptation for attention-based end-to-end speech recognition",
    "citation_count": 29
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19_interspeech.html": {
    "title": "Large Margin Training for Attention Based End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end speech recognition systems are typically evaluated using the maximum a posterior criterion. Since only one hypothesis is involved during evaluation, the ideal number of hypotheses for training should also be one. In this study, we propose a large margin training scheme for attention based end-to-end speech recognition. Using only one training hypothesis, the large margin training strategy achieves the same performance as the minimum word error rate criterion using four hypotheses. The theoretical derivation in this study is widely applicable to other sequence discriminative criteria such as maximum mutual information. In addition, this paper provides a more succinct formulation of the large margin concept, paving the road towards a better combination of support vector machine and deep neural network",
    "checked": true,
    "id": "72abad6cd58731dafa8e7e35b2ce7192f8f6fc72",
    "semantic_title": "large margin training for attention based end-to-end speech recognition",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mac19_interspeech.html": {
    "title": "Large-Scale Mixed-Bandwidth Deep Neural Network Acoustic Modeling for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "In automatic speech recognition (ASR), wideband (WB) and narrowband (NB) speech signals with different sampling rates typically use separate acoustic models. Therefore mixed-bandwidth (MB) acoustic modeling has important practical values for ASR system deployment. In this paper, we extensively investigate large-scale MB deep neural network acoustic modeling for ASR using 1,150 hours of WB data and 2,300 hours of NB data. We study various MB strategies including downsampling, upsampling and bandwidth extension for MB acoustic modeling and evaluate their performance on 8 diverse WB and NB test sets from various application domains. To deal with the large amounts of training data, distributed training is carried out on multiple GPUs using synchronous data parallelism",
    "checked": true,
    "id": "2804c86dc045206d9759f4f9813e5b66cdbb2771",
    "semantic_title": "large-scale mixed-bandwidth deep neural network acoustic modeling for automatic speech recognition",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/milde19_interspeech.html": {
    "title": "SparseSpeech: Unsupervised Acoustic Unit Discovery with Memory-Augmented Sequence Autoencoders",
    "volume": "main",
    "abstract": "We propose a sparse sequence autoencoder model for unsupervised acoustic unit discovery, based on bidirectional LSTM encoders/decoders with a sparsity-inducing bottleneck. The sparsity layer is based on memory-augmented neural networks, with a differentiable embedding memory bank addressed from the encoder. The decoder reconstructs the encoded input feature sequence from an utterance-level context embedding and the bottleneck representation. At some time steps, the input to the decoder is randomly omitted by applying sequence dropout, forcing the decoder to learn about the temporal structure of the sequence. We propose a bootstrapping training procedure, after which the network can be trained end-to-end with standard back-propagation. Sparsity of the generated representation can be controlled with a parameter in the proposed loss function. We evaluate the units with the ABX discriminability on minimal triphone pairs and also on entire words. Forcing the network to favor highly sparse memory addressings in the memory component yields symbolic-like representations of speech that are very compact and still offer better ABX discriminability than MFCC",
    "checked": true,
    "id": "a98badb3e7503d7be09c01a43f85429505d4c907",
    "semantic_title": "sparsespeech: unsupervised acoustic unit discovery with memory-augmented sequence autoencoders",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ondel19_interspeech.html": {
    "title": "Bayesian Subspace Hidden Markov Model for Acoustic Unit Discovery",
    "volume": "main",
    "abstract": "This work tackles the problem of learning a set of language specific acoustic units from unlabeled speech recordings given a set of labeled recordings from other languages. Our approach may be described by the following two steps procedure: first the model learns the notion of acoustic units from the labelled data and then the model uses its knowledge to find new acoustic units on the target language. We implement this process with the Bayesian Subspace Hidden Markov Model (SHMM), a model akin to the Subspace Gaussian Mixture Model (SGMM) where each low dimensional embedding represents an acoustic unit rather than just a HMM's state. The subspace is trained on 3 languages from the GlobalPhone corpus (German, Polish and Spanish) and the AUs are discovered on the TIMIT corpus. Results, measured in equivalent Phone Error Rate, show that this approach significantly outperforms previous HMM based acoustic units discovery systems and compares favorably with the Variational Auto Encoder-HMM",
    "checked": true,
    "id": "57d1734db27c6ce1aae420d56182a4dab9f4fa5c",
    "semantic_title": "bayesian subspace hidden markov model for acoustic unit discovery",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2019/higuchi19_interspeech.html": {
    "title": "Speaker Adversarial Training of DPGMM-Based Feature Extractor for Zero-Resource Languages",
    "volume": "main",
    "abstract": "We propose a novel framework for extracting speaker-invariant features for zero-resource languages. A deep neural network (DNN)-based acoustic model is normalized against speakers via adversarial training: a multi-task learning process trains a shared bottleneck feature to be discriminative to phonemes and independent of speakers. However, owing to the absence of phoneme labels, zero-resource languages cannot employ adversarial multi-task (AMT) learning for speaker normalization. In this work, we obtain a posteriorgram from a Dirichlet process Gaussian mixture model (DPGMM) and utilize the posterior vector for supervision of the phoneme estimation in the AMT training. The AMT network is designed so that the DPGMM posteriorgram itself is embedded in a speaker-invariant feature space. The proposed network is expected to resolve the potential problem that the posteriorgram may lack reliability as a phoneme representation if the DPGMM components are intermingled with phoneme and speaker information. Based on the Zero Resource Speech Challenges, we conduct phoneme discriminant experiments on the extracted features. The results of the experiments show that the proposed framework extracts discriminative features, suppressing the variety in speakers",
    "checked": true,
    "id": "a62a5322c25a9074fd2499c88a46690afd176755",
    "semantic_title": "speaker adversarial training of dpgmm-based feature extractor for zero-resource languages",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/prasad19_interspeech.html": {
    "title": "Building Large-Vocabulary ASR Systems for Languages Without Any Audio Training Data",
    "volume": "main",
    "abstract": "When building automatic speech recognition (ASR) systems, typically some amount of audio and text data in the target language is needed. While text data can be obtained relatively easily across many languages, transcribed audio data is challenging to obtain. This presents a barrier to making voice technologies available in more languages of the world. In this paper, we present a way to build an ASR system system for a language even in the absence of any audio training data in that language at all. We do this by simply re-using an existing acoustic model from a phonologically similar language, without any kind of modification or adaptation towards the target language. The basic insight is that, if two languages are sufficiently similar in terms of their phonological system, an acoustic model should hold up relatively well when used for another language. We describe how we tailor our pronunciation models to enable such re-use, and show experimental results across a number of languages from various language families. We also provide a theoretical analysis of situations in which this approach is likely to work. Our results show that it is possible to achieve less than 20% word error rate (WER) using this method",
    "checked": true,
    "id": "6a3ea42e8cd381e3ed95a0d4d965409172728aa1",
    "semantic_title": "building large-vocabulary asr systems for languages without any audio training data",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2019/azuh19_interspeech.html": {
    "title": "Towards Bilingual Lexicon Discovery From Visually Grounded Speech Audio",
    "volume": "main",
    "abstract": "In this paper, we present a method for the discovery of word-like units and their approximate translations from visually grounded speech across multiple languages. We first train a neural network model to map images and their spoken audio captions in both English and Hindi to a shared, multimodal embedding space. Next, we use this model to segment and cluster regions of the spoken captions which approximately correspond to words. Finally, we exploit between-cluster similarities in the embedding space to associate English pseudo-word clusters with Hindi pseudo-word clusters, and show that many of these cluster pairings capture semantic translations between English and Hindi words. We present quantitative cross-lingual clustering results, as well as qualitative results in the form of a bilingual picture dictionary",
    "checked": true,
    "id": "7ab9392167bdbaa272c95178d50fb08bcfba7148",
    "semantic_title": "towards bilingual lexicon discovery from visually grounded speech audio",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/feng19_interspeech.html": {
    "title": "Improving Unsupervised Subword Modeling via Disentangled Speech Representation Learning and Transformation",
    "volume": "main",
    "abstract": "This study tackles unsupervised subword modeling in the zero-resource scenario, learning frame-level speech representation that is phonetically discriminative and speaker-invariant, using only untranscribed speech for target languages. Frame label acquisition is an essential step in solving this problem. High quality frame labels should be in good consistency with golden transcriptions and robust to speaker variation. We propose to improve frame label acquisition in our previously adopted deep neural network-bottleneck feature (DNN-BNF) architecture by applying the factorized hierarchical variational autoencoder (FHVAE). FHVAEs learn to disentangle linguistic content and speaker identity information encoded in speech. By discarding or unifying speaker information, speaker-invariant features are learned and fed as inputs to DPGMM frame clustering and DNN-BNF training. Experiments conducted on ZeroSpeech 2017 show that our proposed approaches achieve 2.4% and 0.6% absolute ABX error rate reductions in across- and within-speaker conditions, comparing to the baseline DNN-BNF system without applying FHVAEs. Our proposed approaches significantly outperform vocal tract length normalization in improving frame labeling and subword modeling",
    "checked": true,
    "id": "3bad6b7a7d17eaa0027bee97d6b92cbcb40a33d1",
    "semantic_title": "improving unsupervised subword modeling via disentangled speech representation learning and transformation",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nissen19_interspeech.html": {
    "title": "Listeners' Ability to Identify the Gender of Preadolescent Children in Different Linguistic Contexts",
    "volume": "main",
    "abstract": "This study evaluated listeners' ability to identify the gender of preadolescent children from speech samples of varying length and linguistic context. The listeners were presented with a total of 190 speech samples in four different categories of linguistic context: segments, words, sentences, and discourse. The listeners were instructed to evaluate each speech sample and decide whether the speaker was a male or female and rate their level of confidence in their decision. Results showed listeners identified the gender of the speakers with a high degree of accuracy, ranging from 86% to 95%. Significant differences in listener judgments were found across the four levels of linguistic context, with segments having the lowest accuracy (83%) and discourse the highest accuracy (99%). At the segmental level, the listeners' identification of each speaker's gender was greater for vowels than for fricatives, with both types of phoneme being identified at a rate well above chance. Significant differences in identification were found between the /s/ and /ʃ/ fricatives, but not between the four corner vowels. The perception of gender is likely multifactorial, with listeners possibly using phonetic, prosodic, or stylistic speech cues to determine a speaker's gender",
    "checked": true,
    "id": "f32d178eaffa86fefa8ea014fb67e5ca1b521184",
    "semantic_title": "listeners' ability to identify the gender of preadolescent children in different linguistic contexts",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ahlers19_interspeech.html": {
    "title": "Sibilant Variation in New Englishes: A Comparative Sociophonetic Study of Trinidadian and American English /s(tr)/-Retraction",
    "volume": "main",
    "abstract": "The retraction of /s/, particularly in /str/ clusters, toward [ʃ] has been investigated in British, Australian, and American English and shown to be conditioned phonetically and sociolinguistically. To date, however, no research exists on the retraction of /s/ in New Englishes, the nativized Englishes spoken in postcolonial territories like the Caribbean. We take up this research gap and present the results of a large-scale comparative acoustic analysis of /s/-retraction in Trinidadian English (TrinE) and American English (AmE), using Center of Gravity measurements of more than 23,500 sibilants produced by 181 speakers from two speech corpora The results show that, in TrinE, /str/ is considerably retracted toward [ʃtɹ], while all other /sC(r)/ clusters are non-retracted and acoustically close to singleton /s/; less retracted realizations of /str/ occur across word boundaries. Although a statistically significant contrast is overall maintained between /ʃ/ and the sibilant in /str/, there is considerable overlap across many speakers. The comparison between TrinE and AmE indicates that, while sibilants in TrinE overall show acoustically lower values, both varieties have in common that retraction is limited to /str/ contexts and significantly larger in younger speakers. The degree of /str/-retraction, however, is overall larger in TrinE than AmE",
    "checked": true,
    "id": "2ff5f37edd3cec2c2ec4405cb345ee89bf34027a",
    "semantic_title": "sibilant variation in new englishes: a comparative sociophonetic study of trinidadian and american english /s(tr)/-retraction",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gubian19_interspeech.html": {
    "title": "Tracking the New Zealand English NEAR/SQUARE Merger Using Functional Principal Components Analysis",
    "volume": "main",
    "abstract": "The focus of the study is the application of functional principal components analysis (FPCA) to a sound change in progress in which the square and near falling diphthongs are merging in New Zealand English. FPCA approximated the trajectory shapes of the first two formant frequencies (F1/F2) in a large acoustic database of read New Zealand English speech spanning three different age groups and two regions. The derived FPCA parameters showed a greater degree of centralisation and monophthongisation in square than in near. Compatibly with the evidence of an ongoing sound change in which square is shifting towards near, these shape differences were more marked for older than for younger/mid-age speakers. There was no effect of region nor of place of articulation of the preceding consonant; there was a trend for the merger to be more advanced in low frequency words. The study underlines the benefits of FPCA for quantifying the many types of sound changes involving subtle shifts in speech dynamics. In particular, multi-dimensional trajectory shape differences can be quantified without the need for vowel targets nor for determining the influence of the parameters — in this case of the first two formant frequencies — independently of each other",
    "checked": true,
    "id": "731f20b2a2efc4f4ea684a387f972a33390237d4",
    "semantic_title": "tracking the new zealand english near/square merger using functional principal components analysis",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gessinger19_interspeech.html": {
    "title": "Phonetic Accommodation in a Wizard-of-Oz Experiment: Intonation and Segments",
    "volume": "main",
    "abstract": "This paper discusses phonetic accommodation of 20 native German speakers interacting with the simulated spoken dialogue system Mirabella in a Wizard-of-Oz experiment. The study examines intonation of wh-questions and pronunciation of allophonic contrasts in German. In a question-and-answer exchange with the system, the users produce predominantly falling intonation patterns for wh-questions when the system does so as well. The number of rising patterns on the part of the users increases significantly when Mirabella produces questions with rising intonation. In a map task, Mirabella provides information about hidden items while producing variants of two allophonic contrasts which are dispreferred by the users. For the [ɪç] vs. [ɪk] contrast in the suffix ⟨-ig⟩, the number of dispreferred variants on the part of the users increases significantly during the map task. For the [εː] vs. [eː] contrast as a realization of stressed ⟨-ä-⟩, such a convergence effect is not found on the group level, yet still occurs for some individual users. Almost every user converges to the system to a substantial degree for a subset of the examined features, but we also find maintenance of preferred variants and even occasional divergence. This individual variation is in line with previous findings in accommodation research",
    "checked": true,
    "id": "3cc2acd1c36c2a4a4482cfb215f63a3cbfd374cc",
    "semantic_title": "phonetic accommodation in a wizard-of-oz experiment: intonation and segments",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/niebuhr19b_interspeech.html": {
    "title": "PASCAL and DPA: A Pilot Study on Using Prosodic Competence Scores to Predict Communicative Skills for Team Working and Public Speaking",
    "volume": "main",
    "abstract": "Strong communication skills in public-speaking and team-working exercises are associated with specific acoustic-prosodic profiles and strategies. We hypothesize that analyzing and assessing these profiles and strategies allows us to predict communicative skills. To that end, we used two analysis methods, one for charismatic and persuasive public speaking (PASCAL), and one for cooperative communication (DPA). PASCAL and DPA competency scores are determined on an acoustic basis for speech recordings of 21 students whose task was to co-create, in 7 teams of 3 students, a fully functioning weather station over 14 weeks in an Electrical Engineering project course — and to jointly write a development report about it afterwards. Results show that the students' PASCAL scores are significantly correlated with both the grade in their final oral project presentation and the grade of their written report as assessed by an independent lecturer group. The DPA scores correlate with better time-management and team working as well as with the quality and functionality of the designed product. Explanations for the links between student performance and acoustic competence scores are discussed",
    "checked": true,
    "id": "b6db8d80d41454d6a26e0ab861ea960fe73c8f51",
    "semantic_title": "pascal and dpa: a pilot study on using prosodic competence scores to predict communicative skills for team working and public speaking",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/michalsky19_interspeech.html": {
    "title": "Towards the Prosody of Persuasion in Competitive Negotiation. The Relationship Between f0 and Negotiation Success in Same Sex Sales Tasks",
    "volume": "main",
    "abstract": "Prosodic features play a key role in a speaker's persuasive power. However, previous studies on persuasion have been focused on public speaking and the signaling of leadership, while acoustic studies on negotiation have been primarily concerned with cooperative interactions. In this study we are taking a first step into investigating the role of acoustic-prosodic cues in competitive negotiation, focusing on f0 in same-sex negotiations. Specifically, we ask whether the prosodic correlates of persuasive speech are comparable for public speaking and negotiation. Sixty-two speakers (44f/18m) in 31 same-sex pairs participated in a competitive task to bargain over the selling price of a fictional company. We find a significant correlation between a speaker's f0 features and his/her interlocutor's concession range. In line with findings from public speaking, greater f0 excursions and higher f0 minima correlate with negotiation success. However, while the female speakers also show an expected elevated f0 mean, the opposite is the case for male speakers. We propose that in competitive negotiation, displaying dominance may overrule showing passion in contrast to public speaking, but only for male speakers",
    "checked": true,
    "id": "2a75a198f1b396effd78e8bcccd78c75dc62c5c4",
    "semantic_title": "towards the prosody of persuasion in competitive negotiation. the relationship between f0 and negotiation success in same sex sales tasks",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sager19_interspeech.html": {
    "title": "VESUS: A Crowd-Annotated Database to Study Emotion Production and Perception in Spoken English",
    "volume": "main",
    "abstract": "We introduce the Varied Emotion in Syntactically Uniform Speech (VESUS) repository as a new resource for the speech community. VESUS is a lexically controlled database, in which a semantically neutral script is portrayed with different emotional inflections. In total, VESUS contains over 250 distinct phrases, each read by ten actors in five emotional states. We use crowd sourcing to obtain ten human ratings for the perceived emotional content of each utterance. Our unique database construction enables a multitude of scientific and technical explorations. To jumpstart this effort, we provide benchmark performance on three distinct emotion recognition tasks using VESUS: longitudinal speaker analysis, extrapolating across syntactical complexity, and generalization to a new speaker",
    "checked": true,
    "id": "88fbd781267892f6c50d9bcdb5cfdfe7558fac9f",
    "semantic_title": "vesus: a crowd-annotated database to study emotion production and perception in spoken english",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2019/koh19_interspeech.html": {
    "title": "Building the Singapore English National Speech Corpus",
    "volume": "main",
    "abstract": "The National Speech Corpus (NSC) is the first large-scale Singapore English corpus spearheaded by the Info-communications and Media Development Authority of Singapore. It aims to become an important source of open speech data for automatic speech recognition (ASR) research and speech-related applications. The first release of the corpus features more than 2000 hours of orthographically transcribed read speech data designed with the inclusion of locally relevant words. It is available for public and commercial use upon request at \"www.imda.gov.sg/nationalspeechcorpus\", under the Singapore Open Data License. An accompanying lexicon is currently in the works and will be published soon. In addition, another 1000 hours of conversational speech data will be made available in the near future under the second release of NSC. This paper reports on the development and collection process of the read speech and conversational speech corpora",
    "checked": true,
    "id": "5d86886822404c7398edfad18b01b922eabccec2",
    "semantic_title": "building the singapore english national speech corpus",
    "citation_count": 28
  },
  "https://www.isca-speech.org/archive/interspeech_2019/picheny19_interspeech.html": {
    "title": "Challenging the Boundaries of Speech Recognition: The MALACH Corpus",
    "volume": "main",
    "abstract": "There has been huge progress in speech recognition over the last several years. Tasks once thought extremely difficult, such as SWITCHBOARD, now approach levels of human performance. The MALACH corpus (LDC catalog LDC2012S05), a 375-Hour subset of a large archive of Holocaust testimonies collected by the Survivors of the Shoah Visual History Foundation, presents significant challenges to the speech community. The collection consists of unconstrained, natural speech filled with disfluencies, heavy accents, age-related coarticulations, un-cued speaker and language switching, and emotional speech - all still open problems for speech recognition systems. Transcription is challenging even for skilled human annotators. This paper proposes that the community place focus on the MALACH corpus to develop speech recognition systems that are more robust with respect to accents, disfluencies and emotional speech. To reduce the barrier for entry, a lexicon and training and testing setups have been created and baseline results using current deep learning technologies are presented. The metadata has just been released by LDC (LDC2019S11). It is hoped that this resource will enable the community to build on top of these baselines so that the extremely important information in these and related oral histories becomes accessible to a wider audience",
    "checked": true,
    "id": "8bcbcaa3f507d1d3c26f9a378a7c1d488caacf11",
    "semantic_title": "challenging the boundaries of speech recognition: the malach corpus",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ramteke19_interspeech.html": {
    "title": "NITK Kids' Speech Corpus",
    "volume": "main",
    "abstract": "This paper introduces speech database for analyzing children's speech. The proposed database of children is recorded in Kannada language (one of the South Indian languages) from children between age 2.5 to 6.5 years. The database is named as National Institute of Technology Karnataka Kids' Speech Corpus (NITK Kids' Speech Corpus). The relevant design considerations for the database collection are discussed in detail. It is divided into four age groups with an interval of 1 year between each age group. The speech corpus includes nearly 10 hours of speech recordings from 160 children. For each age range, the data is recorded from 40 children (20 male and 20 female). Further, the effect of developmental changes on the speech from 2.5 to 6.5 years are analyzed using pitch and formant analysis. Some of the potential applications, of the NITK Kids' Speech Corpus, such as, systematic study on the language learning ability of children, phonological process analysis and children speech recognition are discussed",
    "checked": true,
    "id": "057e9df4772bc756051edaad4bcfa1068908e493",
    "semantic_title": "nitk kids' speech corpus",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ali19_interspeech.html": {
    "title": "Towards Variability Resistant Dialectal Speech Evaluation",
    "volume": "main",
    "abstract": "We study the problem of evaluating automatic speech recognition (ASR) systems that target dialectal speech input. A major challenge in this case is that the orthography of dialects is typically not standardized. From an ASR evaluation perspective, this means that there is no clear gold standard for the expected output, and several possible outputs could be considered correct according to different human annotators, which makes standard word error rate (WER) inadequate as an evaluation metric. Specifically targeting the case of Arabic dialects, which are also morphologically rich and complex, we propose a number of alternative WER-based metrics that vary in terms of text representation, including different degrees of morphological abstraction and spelling normalization.We evaluate the efficacy of these metrics by comparing their correlation with human judgments on a validation set of 1,000 utterances. Our results show that the use of morphological abstractions and spelling normalization produces systems with higher correlation with human judgment. We released the code and the datasets to the research community",
    "checked": true,
    "id": "438a0502b893be669eb80e4043bbed52835a0843",
    "semantic_title": "towards variability resistant dialectal speech evaluation",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fallgren19_interspeech.html": {
    "title": "How to Annotate 100 Hours in 45 Minutes",
    "volume": "main",
    "abstract": "Speech data found in the wild hold many advantages over artificially constructed speech corpora in terms of ecological validity and cultural worth. Perhaps most importantly, there is a lot of it. However, the combination of great quantity, noisiness and variation poses a challenge for its access and processing. Generally speaking, automatic approaches to tackle the problem require good labels for training, while manual approaches require time. In this study, we provide further evidence for a semi-supervised, human-in-the-loop framework that previously has shown promising results for browsing and annotating large quantities of found audio data quickly. The findings of this study show that a 100-hour long subset of the Fearless Steps corpus can be annotated for speech activity in less than 45 minutes, a fraction of the time it would take traditional annotation methods, without a loss in performance",
    "checked": true,
    "id": "7eae889eed59ac7813412a96b00e6a9990c8c47d",
    "semantic_title": "how to annotate 100 hours in 45 minutes",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/diez19_interspeech.html": {
    "title": "Bayesian HMM Based x-Vector Clustering for Speaker Diarization",
    "volume": "main",
    "abstract": "This paper presents a simplified version of the previously proposed diarization algorithm based on Bayesian Hidden Markov Models, which uses Variational Bayesian inference for very fast and robust clustering of x-vector (neural network based speaker embeddings). The presented results show that this clustering algorithm provides significant improvements in diarization performance as compared to the previously used Agglomerative Hierarchical Clustering. The output of this system can be further employed as an initialization for a second stage VB diarization system, using frame-wise MFCC features as input, to obtain optimal results",
    "checked": true,
    "id": "55277df8e04cc75d46470318d9ffbffe365527ee",
    "semantic_title": "bayesian hmm based x-vector clustering for speaker diarization",
    "citation_count": 60
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vestman19_interspeech.html": {
    "title": "Unleashing the Unused Potential of i-Vectors Enabled by GPU Acceleration",
    "volume": "main",
    "abstract": "Speaker embeddings are continuous-value vector representations that allow easy comparison between voices of speakers with simple geometric operations. Among others, i-vector and x-vector have emerged as the mainstream methods for speaker embedding. In this paper, we illustrate the use of modern computation platform to harness the benefit of GPU acceleration for i-vector extraction. In particular, we achieve an acceleration of 3000 times in frame posterior computation compared to real time and 25 times in training the i-vector extractor compared to the CPU baseline from Kaldi toolkit. This significant speed-up allows the exploration of ideas that were hitherto impossible. In particular, we show that it is beneficial to update the universal background model (UBM) and re-compute frame alignments while training the i-vector extractor. Additionally, we are able to study different variations of i-vector extractors more rigorously than before. In this process, we reveal some undocumented details of Kaldi's i-vector extractor and show that it outperforms the standard formulation by a margin of 1 to 2% when tested with VoxCeleb speaker verification protocol. All of our findings are asserted by ensemble averaging the results from multiple runs with random start",
    "checked": true,
    "id": "cccc0dc0167dc5a919922d5fb44c431c545e9e1f",
    "semantic_title": "unleashing the unused potential of i-vectors enabled by gpu acceleration",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shon19_interspeech.html": {
    "title": "MCE 2018: The 1st Multi-Target Speaker Detection and Identification Challenge Evaluation",
    "volume": "main",
    "abstract": "The Multi-target Challenge aims to assess how well current speech technology is able to determine whether or not a recorded utterance was spoken by one of a large number of blacklisted speakers. It is a form of multi-target speaker detection based on real-world telephone conversations. Data recordings are generated from call center customer-agent conversations. The task is to measure how accurately one can detect 1) whether a test recording is spoken by a blacklisted speaker, and 2) which specific blacklisted speaker was talking. This paper outlines the challenge and provides its baselines, results, and discussions",
    "checked": true,
    "id": "b3918fab36f106e83e016a3e33d260ad656191c4",
    "semantic_title": "mce 2018: the 1st multi-target speaker detection and identification challenge evaluation",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gao19_interspeech.html": {
    "title": "Improving Aggregation and Loss Function for Better Embedding Learning in End-to-End Speaker Verification System",
    "volume": "main",
    "abstract": "Deep embedding learning based speaker verification (SV) methods have recently achieved significant performance improvement over traditional i-vector systems, especially for short duration utterances. Embedding learning commonly consists of three components: frame-level feature processing, utterance-level embedding learning, and loss function to discriminate between speakers. For the learned embeddings, a back-end model (i.e., Linear Discriminant Analysis followed by Probabilistic Linear Discriminant Analysis (LDA-PLDA)) is generally applied as a similarity measure. In this paper, we propose to further improve the effectiveness of deep embedding learning methods in the following components: (1) A multi-stage aggregation strategy, exploited to hierarchically fuse time-frequency context information for effective frame-level feature processing. (2) A discriminant analysis loss is designed for end-to-end training, which aims to explicitly learn the discriminative embeddings, i.e. with small intra-speaker and large inter-speaker variances. To evaluate the effectiveness of the proposed improvements, we conduct extensive experiments on the VoxCeleb1 dataset. The results outperform state-of-the-art systems by a significant margin. It is also worth noting that the results are obtained using a simple cosine metric instead of the more complex LDA-PLDA backend scoring",
    "checked": true,
    "id": "547e7c3a786e4880f50f4650f1c9b08c69b08253",
    "semantic_title": "improving aggregation and loss function for better embedding learning in end-to-end speaker verification system",
    "citation_count": 54
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lin19_interspeech.html": {
    "title": "LSTM Based Similarity Measurement with Spectral Clustering for Speaker Diarization",
    "volume": "main",
    "abstract": "More and more neural network approaches have achieved considerable improvement upon submodules of speaker diarization system, including speaker change detection and segment-wise speaker embedding extraction. Still, in the clustering stage, traditional algorithms like probabilistic linear discriminant analysis (PLDA) are widely used for scoring the similarity between two speech segments. In this paper, we propose a supervised method to measure the similarity matrix between all segments of an audio recording with sequential bidirectional long short-term memory networks (Bi-LSTM). Spectral clustering is applied on top of the similarity matrix to further improve the performance. Experimental results show that our system significantly outperforms the state-of-the-art methods and achieves a diarization error rate of 6.63% on the NIST SRE 2000 CALLHOME database",
    "checked": true,
    "id": "d900a0d828cef4ea7da7082e970a7fd4119f86aa",
    "semantic_title": "lstm based similarity measurement with spectral clustering for speaker diarization",
    "citation_count": 68
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chung19b_interspeech.html": {
    "title": "Who Said That?: Audio-Visual Speaker Diarisation of Real-World Meetings",
    "volume": "main",
    "abstract": "The goal of this work is to determine ‘who spoke when' in real-world meetings. The method takes surround-view video and single or multi-channel audio as inputs, and generates robust diarisation outputs To achieve this, we propose a novel iterative approach that first enrolls speaker models using audio-visual correspondence, then uses the enrolled models together with the visual information to determine the active speaker We show strong quantitative and qualitative performance on a dataset of real-world meetings. The method is also evaluated on the public AMI meeting corpus, on which we demonstrate results that exceed all comparable methods. We also show that beamforming can be used together with the video to further improve the performance when multi-channel audio is available",
    "checked": true,
    "id": "0b04f4b0a6caeca85282c5e3baa5f24706c0cbe3",
    "semantic_title": "who said that?: audio-visual speaker diarisation of real-world meetings",
    "citation_count": 27
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xie19_interspeech.html": {
    "title": "Multi-PLDA Diarization on Children's Speech",
    "volume": "main",
    "abstract": "Children's speech and other vocalizations pose challenges for speaker diarization. The spontaneity of kids causes rapid or delayed phonetic variations in an utterance, which makes speaker's information difficult to extract. Fast speaker turns and long overlap in conversations between children and their guardians makes correct segmentation even harder compared to, say a business meeting. In this work, we explore diarization of child-guardian interactions. We investigate the effectiveness of adding children's speech to adult data in Probabilistic Linear Discriminant Analysis (PLDA) training. We also train each of two PLDAs with separate objective to a coarse or fine classification of speakers. A fusion of the two PLDAs is examined. By performing this fusion, we expect to improve on children's speech while preserving adult segmentations. Our experimental results show that including children's speech helps reduce DER by 2.7%, achieving a best overall DER of 33.1% with the x-vector system. A fusion system yields a reasonable 33.3% DER that validates our concept",
    "checked": true,
    "id": "c04f91ed76aa8f6410312229520383db361b2a4f",
    "semantic_title": "multi-plda diarization on children's speech",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mccree19_interspeech.html": {
    "title": "Speaker Diarization Using Leave-One-Out Gaussian PLDA Clustering of DNN Embeddings",
    "volume": "main",
    "abstract": "Many modern systems for speaker diarization, such as the top-performing JHU system in the DIHARD 2018 challenge, rely on clustering of DNN speaker embeddings followed by HMM resegmentation. Two problems with this approach are that parameters need significant retuning for different applications, and that the DNN contributes only to the clustering task and not the resegmentation. This paper presents two contributions: an improved HMM segment assignment algorithm using leave-one-out Gaussian PLDA scoring, and an approach to training the DNN such that embeddings directly optimize performance of this scoring method with generatively updated PLDA parameters. Initial experiments with this new system are very promising, achieving state-of-the-art performance for two separate tasks (Callhome and DIHARD18) without any task-dependent parameter tuning",
    "checked": true,
    "id": "224d6aef7d6522f9c97bd8cea764704653df0192",
    "semantic_title": "speaker diarization using leave-one-out gaussian plda clustering of dnn embeddings",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ghahabi19_interspeech.html": {
    "title": "Speaker-Corrupted Embeddings for Online Speaker Diarization",
    "volume": "main",
    "abstract": "Speaker diarization is more challenging in presence of background noise or music, frequent speaker changes, and cross talks. In an online scenario, the decision should be made at time, given only the current short segment and the speakers detected in the past, which makes the task even harder. In this work, an online robust speaker diarization algorithm is proposed in which speech segments are represented by low dimensional vectors referred to as speaker-corrupted embeddings. The proposed speaker embedding network is a deep neural network which takes speaker-corrupted supervectors as input, uses variable ReLU (VReLU) as an activation function, and tries to discriminate the background speakers. Speaker corruption is performed by adding supervectors built by 20 speech frames from other speakers to the supervectors of a given speaker. It is shown that speaker corruption, VReLU, and input dropout increase the generalization power of the proposed network. To increase the robustness, the proposed embeddings are concatenated with LDA transformed supervectors. Experimental results on the Albayzin 2018 evaluation set show a competitive accuracy, more robustness, and much lower computational cost compared to typical offline algorithms",
    "checked": true,
    "id": "f110ea60287aaa90a86c7a256361bad209b44416",
    "semantic_title": "speaker-corrupted embeddings for online speaker diarization",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/park19_interspeech.html": {
    "title": "Speaker Diarization with Lexical Information",
    "volume": "main",
    "abstract": "This work presents a novel approach for speaker diarization to leverage lexical information provided by automatic speech recognition. We propose a speaker diarization system that can incorporate word-level speaker turn probabilities with speaker embeddings into a speaker clustering process to improve the overall diarization accuracy. To integrate lexical and acoustic information in a comprehensive way during clustering, we introduce an adjacency matrix integration for spectral clustering. Since words and word boundary information for word-level speaker turn probability estimation are provided by a speech recognition system, our proposed method works without any human intervention for manual transcriptions. We show that the proposed method improves diarization performance on various evaluation datasets compared to the baseline diarization system using acoustic information only in speaker embeddings",
    "checked": true,
    "id": "828444dfe221ac7ce5c0d1a5d7a7db7d1d78b7ce",
    "semantic_title": "speaker diarization with lexical information",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shafey19_interspeech.html": {
    "title": "Joint Speech Recognition and Speaker Diarization via Sequence Transduction",
    "volume": "main",
    "abstract": "Speech applications dealing with conversations require not only recognizing the spoken words, but also determining who spoke when. The task of assigning words to speakers is typically addressed by merging the outputs of two separate systems, namely, an automatic speech recognition (ASR) system and a speaker diarization (SD) system. The two systems are trained independently with different objective functions. Often the SD systems operate directly on the acoustics and are not constrained to respect word boundaries and this deficiency is overcome in an ad hoc manner. Motivated by recent advances in sequence to sequence learning, we propose a novel approach to tackle the two tasks by a joint ASR and SD system using a recurrent neural network transducer. Our approach utilizes both linguistic and acoustic cues to infer speaker roles, as opposed to typical SD systems, which only use acoustic cues. We evaluated the performance of our approach on a large corpus of medical conversations between physicians and patients. Compared to a competitive conventional baseline, our approach improves word-level diarization error rate from 15.8% to 2.2%",
    "checked": true,
    "id": "6632853535bd7f7f9c438d19467341f6e46a63e5",
    "semantic_title": "joint speech recognition and speaker diarization via sequence transduction",
    "citation_count": 71
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cumani19_interspeech.html": {
    "title": "Normal Variance-Mean Mixtures for Unsupervised Score Calibration",
    "volume": "main",
    "abstract": "Generative calibration models have shown to be an effective alternative to traditional discriminative score calibration techniques, such as Logistic Regression (LogReg). Provided that the score distribution assumptions are sufficiently accurate, generative approaches not only have similar or better performance with respect to LogReg, but also allow for unsupervised or semi-supervised training Recently, we have proposed non-Gaussian linear calibration models able to overcome the limitations of Gaussian approaches. Although these models allow for better characterization of score distributions, they still require the target and non-target distributions to be reciprocally symmetric In this work we further extend these models to cover asymmetric score distributions, as to improve calibration for both supervised and unsupervised scenarios. The improvements have been assessed on NIST SRE 2010 telephone data",
    "checked": true,
    "id": "364ce471c1cddcc930c6e1a88696851ae05db1c0",
    "semantic_title": "normal variance-mean mixtures for unsupervised score calibration",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yamamoto19_interspeech.html": {
    "title": "Speaker Augmentation and Bandwidth Extension for Deep Speaker Embedding",
    "volume": "main",
    "abstract": "This paper investigates a novel data augmentation approach to train deep neural networks (DNNs) used for speaker embedding, i.e. to extract representation that allows easy comparison between speaker voices with a simple geometric operation. Data augmentation is used to create new examples from an existing training set, thereby increasing the quantity of training data improves the robustness of the model. We attempt to increase the number of speakers in the training set by generating new speakers via voice conversion. This speaker augmentation expands the coverage of speakers in the embedding space in contrast to conventional audio augmentation methods which focus on within-speaker variability. With an increased number of speakers in the training set, the DNN is trained to produce a better speaker-discriminative embedding. We also advocate using bandwidth extension to augment narrowband speech for a wideband application. Text-independent speaker recognition experiments in Speakers in the Wild (SITW) demonstrate a 17.9% reduction in minimum detection cost with speaker augmentation. The combined use of the two techniques provides further improvement",
    "checked": true,
    "id": "2bda55920cdef57fa4cd3829a98d178c611a8871",
    "semantic_title": "speaker augmentation and bandwidth extension for deep speaker embedding",
    "citation_count": 49
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ylmaz19_interspeech.html": {
    "title": "Large-Scale Speaker Diarization of Radio Broadcast Archives",
    "volume": "main",
    "abstract": "This paper describes our initial efforts to build a large-scale speaker diarization (SD) and identification system on a recently digitized radio broadcast archive from the Netherlands which has more than 6500 audio tapes with 3000 hours of Frisian-Dutch speech recorded between 1950–2016. The employed large-scale diarization scheme involves two stages: (1) tape-level speaker diarization providing pseudo-speaker identities and (2) speaker linking to relate pseudo-speakers appearing in multiple tapes. Having access to the speaker models of several frequently appearing speakers from the previously collected FAME! speech corpus, we further perform speaker identification by linking these known speakers to the pseudo-speakers identified at the first stage. In this work, we present a recently created longitudinal and multilingual SD corpus designed for large-scale SD research and evaluate the performance of a new speaker linking system using x-vectors with PLDA to quantify cross-tape speaker similarity on this corpus. The performance of this speaker linking system is evaluated on a small subset of the archive which is manually annotated with speaker information. The speaker linking performance reported on this subset (53 hours) and the whole archive (3000 hours) is compared to quantify the impact of scaling up in the amount of speech data",
    "checked": true,
    "id": "8518d62a10a49cb180a1586d13c3973a36bdf81b",
    "semantic_title": "large-scale speaker diarization of radio broadcast archives",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dubey19_interspeech.html": {
    "title": "Toeplitz Inverse Covariance Based Robust Speaker Clustering for Naturalistic Audio Streams",
    "volume": "main",
    "abstract": "Speaker diarization determines who spoke and when? in an audio stream. In this study, we propose a model-based approach for robust speaker clustering using i-vectors. The i-vectors extracted from different segments of same speaker are correlated. We model this correlation with a Markov Random Field (MRF) network. Leveraging the advancements in MRF modeling, we used Toeplitz Inverse Covariance (TIC) matrix to represent the MRF correlation network for each speaker. This approaches captures the sequential structure of i-vectors (or equivalent speaker turns) belonging to same speaker in an audio stream. A variant of standard Expectation Maximization (EM) algorithm is adopted for deriving closed-form solution using dynamic programming (DP) and the alternating direction method of multiplier (ADMM). Our diarization system has four steps: (1) ground-truth segmentation; (2) i-vector extraction; (3) post-processing (mean subtraction, principal component analysis, and length-normalization) ; and (4) proposed speaker clustering. We employ cosine K-means and movMF speaker clustering as baseline approaches. Our evaluation data is derived from: (i) CRSS-PLTL corpus, and (ii) two meetings subset of the AMI corpus. Relative reduction in diarization error rate (DER) for CRSS-PLTL corpus is 43.22% using the proposed advancements as compared to baseline. For AMI meetings IS1000a and IS1003b, relative DER reduction is 29.37% and 9.21%, respectively",
    "checked": true,
    "id": "7342cbe19f130ca0de319dca36d75648cec2ad70",
    "semantic_title": "toeplitz inverse covariance based robust speaker clustering for naturalistic audio streams",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kovacs19_interspeech.html": {
    "title": "Examining the Combination of Multi-Band Processing and Channel Dropout for Robust Speech Recognition",
    "volume": "main",
    "abstract": "A pivotal question in Automatic Speech Recognition (ASR) is the robustness of the trained models. In this study, we investigate the combination of two methods commonly applied to increase the robustness of ASR systems. On the one hand, inspired by auditory experiments and signal processing considerations, multi-band band processing has been used for decades to improve the noise robustness of speech recognition. On the other hand, dropout is a commonly used regularization technique to prevent overfitting by keeping the model from becoming over-reliant on a small set of neurons. We hypothesize that the careful combination of the two approaches would lead to increased robustness, by preventing the resulting model from over-rely on any given band To verify our hypothesis, we investigate various approaches for the combination of the two methods using the Aurora-4 corpus. The results obtained corroborate our initial assumption, and show that the proper combination of the two techniques leads to increased robustness, and to significantly lower word error rates (WERs). Furthermore, we find that the accuracy scores attained here compare favourably to those reported recently on the clean training scenario of the Aurora-4 corpus",
    "checked": true,
    "id": "9f5b8831cd40e6987a70016d107c928ba508dc07",
    "semantic_title": "examining the combination of multi-band processing and channel dropout for robust speech recognition",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/soni19_interspeech.html": {
    "title": "Label Driven Time-Frequency Masking for Robust Continuous Speech Recognition",
    "volume": "main",
    "abstract": "The application of Time-Frequency (T-F) masking based approaches for Automatic Speech Recognition has been shown to provide significant gains in system performance in the presence of additive noise. Such approaches give performance improvement when the T-F masking front-end is trained jointly with the acoustic model. However, such systems still rely on a pre-trained T-F masking enhancement block, trained using pairs of clean and noisy speech signals. Pre-training is necessary due to large number of parameters associated with the enhancement network. In this paper, we propose a flat-start joint training of a network that has both a T-F masking based enhancement block and a phoneme classification block. In particular, we use fully convolutional network as an enhancement front-end to reduce the number of parameters. We train the network by jointly updating the parameters of both these blocks using tied Context-Dependent phoneme states as targets. We observe that pretraining of the proposed enhancement block is not necessary for the convergence. In fact, the proposed flat-start joint training converges faster than the baseline multi-condition trained model. The experiments performed on Aurora-4 database show 7.06% relative improvement over multi-conditioned baseline. We get similar improvements for unseen test conditions as well",
    "checked": true,
    "id": "8bfe5bb970fc99148f1c8c6b666312fd05a12f21",
    "semantic_title": "label driven time-frequency masking for robust continuous speech recognition",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wu19c_interspeech.html": {
    "title": "Speaker-Invariant Feature-Mapping for Distant Speech Recognition via Adversarial Teacher-Student Learning",
    "volume": "main",
    "abstract": "Feature mapping (FM) jointly trained with acoustic model (AFM) is commonly used for single-channel speech enhancement. However, the performance is affected by the inter-speaker variability. In this paper, we propose speaker-invariant AFM (SIAFM) aiming at curtailing the inter-talker variability while achieving speech enhancement. In SIAFM, a feature-mapping network, an acoustic model and a speaker classifier network are jointly optimized to minimize the feature-mapping loss and the senone classification loss, and simultaneously min-maximize the speaker classification loss. Evaluated on AMI dataset, the proposed SIAFM achieves 4.8% and 7.0% relative word error rate (WER) reduction on the overlapped and non-overlapped condition over the baseline acoustic model trained with single distant microphone (SDM) data. Additionally, the SIAFM obtains 3.0% relative overlapped WER and 4.2% relative non-overlapped WER decrease over the multi-conditional (MCT) acoustic model. To further promote the performance of SIAFM, we employ teacher-student learning (TS), in which the posterior probabilities generated by the individual headset microphone (IHM) data can be used in lieu of labels to train the SIAFM model. The experiments show that compared with MCT model, SIAFM with TS (SIAFM-TS) can reach 4.2% relative overlapped WER and 6.3% relative non-overlapped WER decrease respectively",
    "checked": true,
    "id": "a84b392aeb4bd31da086baf4a88484d550e1023c",
    "semantic_title": "speaker-invariant feature-mapping for distant speech recognition via adversarial teacher-student learning",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ming19_interspeech.html": {
    "title": "Full-Sentence Correlation: A Method to Handle Unpredictable Noise for Robust Speech Recognition",
    "volume": "main",
    "abstract": "We describe the theory and implementation of full-sentence speech correlation for speech recognition, and demonstrate its superior robustness to unseen/untrained noise. For the Aurora 2 data, trained with only clean speech, the new method performs competitively against the state-of-the-art with multicondition training and adaptation, and achieves the lowest word error rate in very low SNR (-5 dB). Further experiments with highly nonstationary noise (pop song, broadcast news, etc.) show the surprising ability of the new method to handle unpredictable noise. The new method adds several novel developments to our previous research, including the modeling of the speaker characteristics along with other acoustic and semantic features of speech for separating speech from noise, and a novel Viterbi algorithm to implement full-sentence correlation for speech recognition",
    "checked": true,
    "id": "a34eae29fb8a199ba961d39fdb6b64c154df27bb",
    "semantic_title": "full-sentence correlation: a method to handle unpredictable noise for robust speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/soni19b_interspeech.html": {
    "title": "Generative Noise Modeling and Channel Simulation for Robust Speech Recognition in Unseen Conditions",
    "volume": "main",
    "abstract": "Multi-conditioned training is a state-of-the-art approach to achieve robustness in Automatic Speech Recognition (ASR) systems. This approach works well in practice for seen degradation conditions. However, the performance of such system is still an issue for unseen degradation conditions. In this work we consider distortions due to additive noise and channel mismatch. To achieve the robustness to additive noise, we propose a parametric generative model for noise signals. By changing the parameters of the proposed generative model, various noise signals can be generated and used to develop a multi-conditioned dataset for ASR system training. The generative model is designed to span the feature space of Mel Filterbank Energies by using band-limited white noise signals as basis. To simulate channel distortions, we propose to shift the mean of log spectral magnitude using utterances with estimated channel distortions. Experiments performed on the Aurora 4 noisy speech database show that using noise types generated from the proposed generative model for multi-conditioned training provides significant performance gain for additive noise in unseen conditions. We compare our results with those from multi-conditioning by various real noise databases including environmental and other real life noises",
    "checked": true,
    "id": "4ca5ede1da48cf0649a8cef7dc2269812177cb3e",
    "semantic_title": "generative noise modeling and channel simulation for robust speech recognition in unseen conditions",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kumar19_interspeech.html": {
    "title": "Far-Field Speech Enhancement Using Heteroscedastic Autoencoder for Improved Speech Recognition",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) systems trained on clean speech do not perform well in far-field scenario. Degradation in word error rate (WER) can be as large as 40% in this mismatched scenario. Typically, speech enhancement is applied to map speech from far-field condition to clean condition using a neural network, commonly known as denoising autoencoder (DA). Such speech enhancement technique has shown significant improvement in ASR accuracy. It is a common practice to use mean-square error (MSE) loss to train DA which is based on regression model with residual noise modeled by zero-mean and constant co-variance Gaussian distribution. However, both these assumptions are not optimal, especially in highly non-stationary noisy and far-field scenario. Here, we propose a more generalized loss based on non-zero mean and heteroscedastic co-variance distribution for the residual variables. On the top, we present several novel DA architectures that are more suitable for the heteroscedastic loss. It is shown that the proposed methods outperform the conventional DA and MSE loss by a large margin. We observe relative improvement of 7.31% in WER compared to conventional DA and overall, a relative improvement of 14.4% compared to mismatched train and test scenario",
    "checked": true,
    "id": "8827899043786dee7563d4f93005c2b519fee75e",
    "semantic_title": "far-field speech enhancement using heteroscedastic autoencoder for improved speech recognition",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/delcroix19_interspeech.html": {
    "title": "End-to-End SpeakerBeam for Single Channel Target Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end (E2E) automatic speech recognition (ASR) that directly maps a sequence of speech features into a sequence of characters using a single neural network has received a lot of attention as it greatly simplifies the training and decoding pipelines and enables optimizing the whole system E2E. Recently, such systems have been extended to recognize speech mixtures by inserting a speech separation mechanism into the neural network, allowing to output recognition results for each speaker in the mixture. However, speech separation suffers from a global permutation ambiguity issue, i.e. arbitrary mapping between source speakers and outputs. We argue that this ambiguity would seriously limit the practical use of E2E separation systems. SpeakerBeam has been proposed as an alternative to speech separation to mitigate the global permutation ambiguity. SpeakerBeam aims at extracting only a target speaker in a mixture based on his/her speech characteristics, thus avoiding the global permutation problem. In this paper, we combine SpeakerBeam and an E2E ASR system to allow E2E training of a target speech recognition system. We show promising target speech recognition results in mixtures of two speakers, and discuss interesting properties of the proposed system in terms of speech enhancement and diarization ability",
    "checked": true,
    "id": "931cbd9d689e9fd6bd91f4e8e1dbdd7fbb6df9de",
    "semantic_title": "end-to-end speakerbeam for single channel target speech recognition",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hsu19_interspeech.html": {
    "title": "NIESR: Nuisance Invariant End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "Deep neural network models for speech recognition have achieved great success recently, but they can learn incorrect associations between the target and nuisance factors of speech (e.g., speaker identities, background noise, etc.), which can lead to overfitting. While several methods have been proposed to tackle this problem, existing methods incorporate additional information about nuisance factors during training to develop invariant models. However, enumeration of all possible nuisance factors in speech data and the collection of their annotations is difficult and expensive. We present a robust training scheme for end-to-end speech recognition that adopts an unsupervised adversarial invariance induction framework to separate out essential factors for speech-recognition from nuisances without using any supplementary labels besides the transcriptions. Experiments show that the speech recognition model trained with the proposed training scheme achieves relative improvements of 5.48% on WSJ0, 6.16% on CHiME3, and 6.61% on TIMIT dataset over the base model. Additionally, the proposed method achieves a relative improvement of 14.44% on the combined WSJ0+CHiME3 dataset",
    "checked": true,
    "id": "b510695b9c324eec07576346ceeefcc4cf35387f",
    "semantic_title": "niesr: nuisance invariant end-to-end speech recognition",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/suzuki19_interspeech.html": {
    "title": "Knowledge Distillation for Throat Microphone Speech Recognition",
    "volume": "main",
    "abstract": "Throat microphones are robust against external noise because they receive vibrations directly from the skin, however, their available speech data is limited. This work aims to improve the speech recognition accuracy of throat microphones, and we propose a knowledge distillation method of hybrid DNN-HMM acoustic model. This method distills the knowledge from acoustic model trained with a large amount of close-talk microphone speech data (teacher model) to acoustic model for throat microphones (student model) using a small amount of parallel data of throat and close-talk microphones. The frontend network of the student model contains a feature mapping network from throat microphone acoustic features to close-talk microphone bottleneck features, and the back-end network is a phonetic discrimination network from close-talk microphone bottleneck features. We attempted to improve recognition accuracy further by initializing student model parameters using pretrained front-end and back-end networks. Experimental results using Japanese read speech data showed that the proposed approach achieved 9.8% relative improvement of character error rate (14.3% → 12.9%) compared to the hybrid acoustic model trained only with throat microphone speech data. Furthermore, under noise environments of approximately 70 dBA or higher, the throat microphone system with our approach outperformed the close-talk microphone system",
    "checked": true,
    "id": "fe6fc80fa0aa98ae83fbbc1acc085c774abdc6f5",
    "semantic_title": "knowledge distillation for throat microphone speech recognition",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wu19d_interspeech.html": {
    "title": "Improved Speaker-Dependent Separation for CHiME-5 Challenge",
    "volume": "main",
    "abstract": "This paper summarizes several contributions for improving the speaker-dependent separation system for CHiME-5 challenge, which aims to solve the problem of multi-channel, highly-overlapped conversational speech recognition in a dinner party scenario with reverberations and non-stationary noises. Specifically, we adopt a speaker-aware training method by using i-vector as the target speaker information for multi-talker speech separation. With only one unified separation model for all speakers, we achieve a 10% absolute improvement in terms of word error rate (WER) over the previous baseline of 80.28% on the development set by leveraging our newly proposed data processing techniques and beamforming approach. With our improved back-end acoustic model, we further reduce WER to 60.15% which surpasses the result of our submitted CHiME-5 challenge system without applying any fusion techniques",
    "checked": true,
    "id": "a89322e687af8bf64bcd1c0ee22482b4070c12b2",
    "semantic_title": "improved speaker-dependent separation for chime-5 challenge",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19b_interspeech.html": {
    "title": "Bridging the Gap Between Monaural Speech Enhancement and Recognition with Distortion-Independent Acoustic Modeling",
    "volume": "main",
    "abstract": "Monaural speech enhancement has made dramatic advances in recent years. Although enhanced speech has been demonstrated to have better intelligibility and quality for human listeners, feeding it directly to automatic speech recognition (ASR) systems trained with noisy speech has not produced expected improvements in ASR performance. The lack of an enhancement benefit on recognition, or the gap between monaural speech enhancement and recognition, is often attributed to speech distortions introduced in the enhancement process. In this study, we analyze the distortion problem and propose a distortion-independent acoustic modeling scheme. Experimental results show that the distortion-independent acoustic model is able to overcome the distortion problem. Moreover, it can be used with various speech enhancement models. Both the distortion-independent and a noise-dependent acoustic model perform better than the previous best system on the CHiME-2 corpus. The noise-dependent acoustic model achieves a word error rate of 8.7%, outperforming the previous best result by 6.5% relatively",
    "checked": true,
    "id": "1e7b0f5d6745cd3e36638b5adeabcd42becbebcd",
    "semantic_title": "bridging the gap between monaural speech enhancement and recognition with distortion-independent acoustic modeling",
    "citation_count": 33
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19c_interspeech.html": {
    "title": "Enhanced Spectral Features for Distortion-Independent Acoustic Modeling",
    "volume": "main",
    "abstract": "It has recently been shown that a distortion-independent acoustic modeling method is able to overcome the distortion problem caused by speech enhancement. In this study, we improve the distortion-independent acoustic model by feeding it with enhanced spectral features. Using enhanced magnitude spectra, the automatic speech recognition (ASR) system achieves a word error rate of 7.8% on the CHiME-2 corpus, outperforming our previous best system by more than 10% relatively. Compared with the corresponding enhanced waveform signal based system, systems using enhanced spectral features obtain up to 24% relative improvement. These comparisons show that speech enhancement is helpful for robust ASR and that enhanced spectral features are more suitable for ASR tasks than enhanced waveform signals",
    "checked": true,
    "id": "1a18b768e870d8dc9ab81dc17f6732b664172736",
    "semantic_title": "enhanced spectral features for distortion-independent acoustic modeling",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/neekhara19b_interspeech.html": {
    "title": "Universal Adversarial Perturbations for Speech Recognition Systems",
    "volume": "main",
    "abstract": "In this work, we demonstrate the existence of universal adversarial audio perturbations that cause mis-transcription of audio signals by automatic speech recognition (ASR) systems. We propose an algorithm to find a single quasi-imperceptible perturbation, which when added to any arbitrary speech signal, will most likely fool the victim speech recognition model. Our experiments demonstrate the application of our proposed technique by crafting audio-agnostic universal perturbations for the state-of-the-art ASR system — Mozilla DeepSpeech. Additionally, we show that such perturbations generalize to a significant extent across models that are not available during training, by performing a transferability test on a WaveNet based ASR system",
    "checked": true,
    "id": "1a6446a451472b1c5815ce3e00cf84eb7e641a2a",
    "semantic_title": "universal adversarial perturbations for speech recognition systems",
    "citation_count": 82
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fujimoto19_interspeech.html": {
    "title": "One-Pass Single-Channel Noisy Speech Recognition Using a Combination of Noisy and Enhanced Features",
    "volume": "main",
    "abstract": "This paper introduces a method of noise-robust automatic speech recognition (ASR) that remains effective under one-pass single-channel processing. Under these constraints, the use of single-channel speech enhancement seems to be a reasonable noise-robust approach to ASR, because complicated techniques requiring multi-pass processing cannot be used. However, in many cases, single-channel speech enhancement seriously deteriorates the accuracy of ASR because of speech distortion. In addition, the advanced acoustic modeling framework (joint training) is relatively ineffective in the case of single-channel processing. To overcome these problems, we propose a noise-robust acoustic modeling framework based on a feature-level combination of noisy speech and enhanced speech. To obtain further improvements, we also adopt a sub-network-level combination of noisy and enhanced speech, and a gating mechanism that can dynamically select appropriate speech features. Through comparative evaluations, we confirm that the proposed method successfully improves the accuracy of ASR in noisy environments under strong constraints",
    "checked": true,
    "id": "b1ae7cda95d5ee2da1f33521954b742d4a68e4db",
    "semantic_title": "one-pass single-channel noisy speech recognition using a combination of noisy and enhanced features",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19_interspeech.html": {
    "title": "Jointly Adversarial Enhancement Training for Robust End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "Recently, the end-to-end system has made significant breakthroughs in the field of speech recognition. However, this single end-to-end architecture is not especially robust to the input variations interfered of noises and reverberations, resulting in performance degradation dramatically in reality. To alleviate this issue, the mainstream approach is to use a well-designed speech enhancement module as the front-end of ASR. However, enhancement modules would result in speech distortions and mismatches to training, which sometimes degrades the ASR performance. In this paper, we propose a jointly adversarial enhancement training to boost robustness of end-to-end systems. Specifically, we use a jointly compositional scheme of mask-based enhancement network, attention-based encoder-decoder network and discriminant network during training. The discriminator is used to distinguish between the enhanced features from enhancement network and clean features, which could guide enhancement network to output towards the realistic distribution. With the joint optimization of the recognition, enhancement and adversarial loss, the compositional scheme is expected to learn more robust representations for the recognition task automatically. Systematic experiments on AISHELL-1 show that the proposed method improves the noise robustness of end-to-end systems and achieves the relative error rate reduction of 4.6% over the multi-condition training",
    "checked": true,
    "id": "e564f49e872dab8a030f705434d9c5c195900d78",
    "semantic_title": "jointly adversarial enhancement training for robust end-to-end speech recognition",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yang19_interspeech.html": {
    "title": "Predicting Humor by Learning from Time-Aligned Comments",
    "volume": "main",
    "abstract": "In this paper, we describe a novel approach for generating unsupervised humor labels using time-aligned user comments, and predicting humor using audio information alone. We collected 241 videos of comedy movies and gameplay videos from one of the largest Chinese video-sharing websites. We generate unsupervised humor labels from laughing comments, and find high agreement between these labels and human annotations. From these unsupervised labels, we build deep learning models using speech and text features, which obtain an AUC of 0.751 in predicting humor on a manually annotated test set. To our knowledge, this is the first study predicting perceived humor in large-scale audio data",
    "checked": true,
    "id": "5a414cd30caa50f26136f2fc351cb0abf7c2897f",
    "semantic_title": "predicting humor by learning from time-aligned comments",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dinkov19_interspeech.html": {
    "title": "Predicting the Leading Political Ideology of YouTube Channels Using Acoustic, Textual, and Metadata Information",
    "volume": "main",
    "abstract": "We address the problem of predicting the leading political ideology, i.e., left-center-right bias, for YouTube channels of news media. Previous work on the problem has focused exclusively on text and on analysis of the language used, topics discussed, sentiment, and the like. In contrast, here we study videos, which yields an interesting multimodal setup. Starting with gold annotations about the leading political ideology of major world news media from Media Bias/Fact Check, we searched on YouTube to find their corresponding channels, and we downloaded a recent sample of videos from each channel. We crawled more than 1,000 YouTube hours along with the corresponding subtitles and metadata, thus producing a new multimodal dataset. We further developed a multimodal deep-learning architecture for the task. Our analysis shows that the use of acoustic signal helped to improve bias detection by more than 6% absolute over using text and metadata only. We release the dataset to the research community, hoping to help advance the field of multi-modal political bias detection",
    "checked": true,
    "id": "afa0f48dab3884c0a6e309699bf67efa2f8d5b61",
    "semantic_title": "predicting the leading political ideology of youtube channels using acoustic, textual, and metadata information",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2019/an19_interspeech.html": {
    "title": "Mitigating Gender and L1 Differences to Improve State and Trait Recognition",
    "volume": "main",
    "abstract": "Automatic detection of speaker states and traits is made more difficult by intergroup differences in how they are distributed and expressed in speech and language. In this study, we explore various deep learning architectures for incorporating demographic information into the classification task. We find that early and late fusion of demographic information both improve performance on the task of personality recognition, and a multitask learning model, which performs best, also significantly improves deception detection accuracy. Our findings establish a new state-of-the-art for personality recognition and deception detection on the CXD corpus, and suggest new best practices for mitigating intergroup differences to improve speaker state and trait recognition",
    "checked": true,
    "id": "ea64d4e06812956961f34ce8c47a5e084cf57712",
    "semantic_title": "mitigating gender and l1 differences to improve state and trait recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/weninger19_interspeech.html": {
    "title": "Deep Learning Based Mandarin Accent Identification for Accent Robust ASR",
    "volume": "main",
    "abstract": "In this paper, we present an in-depth study on the classification of regional accents in Mandarin speech. Experiments are carried out on Mandarin speech data systematically collected from 15 different geographical regions in China for broad coverage. We explore bidirectional Long Short-Term Memory (bLSTM) networks and i-vectors to model longer-term acoustic context. Starting from the classification of the collected data into the 15 regional accents, we derive a three-class grouping via non-metric dimensional scaling (NMDS), for which 68.4% average recall can be obtained. Furthermore, we evaluate a state-of-the-art ASR system on the accented data and demonstrate that the character error rate (CER) strongly varies among these accent groups, even if i-vector speaker adaptation is used. Finally, we show that model selection based on the prediction of our bLSTM accent classifier can yield up to 7.6% CER reduction for accented speech",
    "checked": true,
    "id": "862ecbbcd9f57145fc49b01ca4240d7559c063df",
    "semantic_title": "deep learning based mandarin accent identification for accent robust asr",
    "citation_count": 25
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gosztolya19_interspeech.html": {
    "title": "Calibrating DNN Posterior Probability Estimates of HMM/DNN Models to Improve Social Signal Detection from Audio Data",
    "volume": "main",
    "abstract": "To detect social signals such as laughter or filler events from audio data, a straightforward choice is to apply a Hidden Markov Model (HMM) in combination with a Deep Neural Network (DNN) that supplies the local class posterior estimates ( HMM/DNN hybrid model). However, the posterior estimates of the DNN may be suboptimal due to a mismatch between the cost function used during training (e.g. frame-level cross-entropy) and the actual evaluation metric (e.g. segment-level F score). In this study, we show experimentally that by employing a simple posterior probability calibration technique on the DNN outputs, the performance of the HMM/DNN workflow can be significantly improved. Specifically, we apply a linear transformation on the activations of the output layer right before using the softmax function, and fine-tune the parameters of this transformation. Out of the calibration approaches tested, we got the best F scores when the posterior calibration process was adjusted so as to maximize the actual HMM-based evaluation metric",
    "checked": true,
    "id": "ef3071f4e011c444fd49968d6ff7bc8129dbc9e7",
    "semantic_title": "calibrating dnn posterior probability estimates of hmm/dnn models to improve social signal detection from audio data",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mori19_interspeech.html": {
    "title": "Conversational and Social Laughter Synthesis with WaveNet",
    "volume": "main",
    "abstract": "The studies of laughter synthesis are relatively few, and they are still in a preliminary stage. We explored the possibility of applying WaveNet to laughter synthesis. WaveNet is potentially more suitable to model laughter waveforms that do not have a well-established theory of production like speech signals. Conversational laughter was modelled with a spontaneous dialogue speech corpus based on WaveNet. To obtain more stable laughter generation, conditioning WaveNet by power contour was proposed. Experimental results showed that the synthesized laughter by WaveNet was perceived as closer to natural laughter than HMM-based synthesized laughter",
    "checked": true,
    "id": "d764e4687e398b33a32f9da6fff391703f34cd22",
    "semantic_title": "conversational and social laughter synthesis with wavenet",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ludusan19_interspeech.html": {
    "title": "Laughter Dynamics in Dyadic Conversations",
    "volume": "main",
    "abstract": "Human verbal communication is a complex phenomenon involving dynamics that normally result in the alignment of participants on several modalities, and across various linguistic domains. We examined here whether such dynamics occur also for paralinguistic events, in particular, in the case of laughter. Using a conversational corpus containing dyadic interactions in three languages (French, German and Mandarin Chinese), we investigated three measures of alignment: convergence, synchrony and agreement. Support for convergence and synchrony was found in all three languages, although the level of support varied with the language, while the agreement in laughter type was found to be significant for the German data. The implications of these findings towards a better understanding of the role of laughter in human communication are discussed",
    "checked": true,
    "id": "3393da047f281383f283258bbc5ba6240bbd607c",
    "semantic_title": "laughter dynamics in dyadic conversations",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/truong19_interspeech.html": {
    "title": "Towards an Annotation Scheme for Complex Laughter in Speech Corpora",
    "volume": "main",
    "abstract": "Although laughter research has gained quite some interest over the past few years, a shared description of how to annotate laughter and its sub-units is still missing. We present a first attempt towards an annotation scheme that contributes to improving the homogeneity and transparency with which laughter is annotated. This includes the integration of respiratory noises as well as stretches of speech-laughs, and to a limited extend to smiled speech and short silent intervals. Inter-annotator agreement is assessed while applying the scheme to different corpora where laughter is evoked through different methods and varying settings. Annotating laughter becomes more complex when the situation in which laughter occurs becomes more spontaneous and social. There is a substantial disagreement among the annotators with respect to temporal alignment (when does a unit start and when does it end) and unit classification, particularly the determination of starts/ends of laughter episodes. In summary, this detailed laughter annotation study reflects the need for better investigations of the various components of laughter",
    "checked": true,
    "id": "3a3d936f196c010dcbdfa0858e63f2cf08a5e15e",
    "semantic_title": "towards an annotation scheme for complex laughter in speech corpora",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/baird19_interspeech.html": {
    "title": "Using Speech to Predict Sequentially Measured Cortisol Levels During a Trier Social Stress Test",
    "volume": "main",
    "abstract": "The effect of stress on the human body is substantial, potentially resulting in serious health implications. Furthermore, with modern stressors seemingly on the increase, there is an abundance of contributing factors which lead to a diagnosis of acute stress. However, observing biological stress reactions usually includes costly and time consuming sequential fluid-based samples to determine the degree of biological stress. On the contrary, a speech monitoring approach would allow for a non-invasive indication of stress. To evaluate the efficacy of the speech signal as a marker of stress, we explored, for the first time, the relationship between sequential cortisol samples and speech-based features. Utilising a novel corpus of 43 individuals undergoing a standardised Trier Social Stress Test (TSST), we extract a variety of feature sets and observe a correlation between speech and sequential cortisol measurements. For prediction of mean cortisol levels from speech, results show that for the entire TSST oral presentation, handcrafted COMPARE features achieve best results of 0.244 root mean square error [0 ;1] for the sample 20 minutes after the TSST. Correlation also increases at minute 20, with a Spearman's correlation coefficient of 0.421, and Cohen's d of 0.883 between the baseline and minute 20 cortisol predictions",
    "checked": true,
    "id": "7b5204d925ea5b5b88c257f75088145a47dc749d",
    "semantic_title": "using speech to predict sequentially measured cortisol levels during a trier social stress test",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2019/baird19b_interspeech.html": {
    "title": "Sincerity in Acted Speech: Presenting the Sincere Apology Corpus and Results",
    "volume": "main",
    "abstract": "The ability to discern an individual's level of sincerity varies from person to person and across cultures. Sincerity is typically a key indication of personality traits such as trustworthiness, and portraying sincerity can be integral to an abundance of scenarios, e. g. , when apologising. Speech signals are one important factor when discerning sincerity and, with more modern interactions occurring remotely, automatic approaches for the recognition of sincerity from speech are beneficial during both interpersonal and professional scenarios. In this study we present details of the Sincere Apology Corpus ( Sina-C). Annotated by 22 individuals for their perception of sincerity, Sina-C is an English acted-speech corpus of 32 speakers, apologising in multiple ways. To provide an updated baseline for the corpus, various machine learning experiments are conducted. Finding that extracting deep data-representations (utilising the Deep Spectrum toolkit) from the speech signals is best suited. Classification results on the binary (sincere / not sincere) task are at best 79.2% Unweighted Average Recall and for regression, in regards to the degree of sincerity, a Root Mean Square Error of 0.395 from the standardised range [-1.51; 1.72] is obtained",
    "checked": true,
    "id": "821ebbd0dceada9f7182d7ba416b7a0fcca365b4",
    "semantic_title": "sincerity in acted speech: presenting the sincere apology corpus and results",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/niebuhr19c_interspeech.html": {
    "title": "Do not Hesitate! — Unless You Do it Shortly or Nasally: How the Phonetics of Filled Pauses Determine Their Subjective Frequency and Perceived Speaker Performance",
    "volume": "main",
    "abstract": "In this paper, we test whether the perception of filled-pause (FP) frequency and public-speaking performance are mediated by the phonetic characteristics of FPs. In particular, total duration, vowel-formant pattern (if present), and nasal-segment proportion of FPs were correlated with perceptual data of 29 German listeners who rated excerpts of business presentations given by 68 German-speaking managers. Results show strong inter-speaker differences in how and how often FPs are realized. Moreover, differences in FP duration and nasal proportion are significantly correlated with estimated (i.e. subjective) FP frequency and perceived speaker performance. The shorter and more nasal a speaker's FPs are, the more do listeners underestimate the speaker's actual FP frequency and the higher they rate the speaker's public-speaking performance. The results are discussed in terms of their implications for FP saliency and rhetorical training",
    "checked": false,
    "id": "5efda0c06a3679af74366a1f43738c56c1ee0649",
    "semantic_title": "do not hesitate! - unless you do it shortly or nasally: how the phonetics of filled pauses determine their subjective frequency and perceived speaker performance",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vasquezcorrea19_interspeech.html": {
    "title": "Phonet: A Tool Based on Gated Recurrent Neural Networks to Extract Phonological Posteriors from Speech",
    "volume": "main",
    "abstract": "There are a lot of features that can be extracted from speech signals for different applications such as automatic speech recognition or speaker verification. However, for pathological speech processing there is a need to extract features about the presence of the disease or the state of the patients that are comprehensible for clinical experts. Phonological posteriors are a group of features that can be interpretable by the clinicians and at the same time carry suitable information about the patient's speech. This paper presents a tool to extract phonological posteriors directly from speech signals. The proposed method consists of a bank of parallel bidirectional recurrent neural networks to estimate the posterior probabilities of the occurrence of different phonological classes. The proposed models are able to detect the phonological classes with accuracies over 90%. In addition, the trained models are available to be used by the research community interested in the topic",
    "checked": true,
    "id": "5ea63993284d8efbaf063294b72955b41d1a73a1",
    "semantic_title": "phonet: a tool based on gated recurrent neural networks to extract phonological posteriors from speech",
    "citation_count": 26
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chang19_interspeech.html": {
    "title": "Code-Switching Sentence Generation by Generative Adversarial Networks and its Application to Data Augmentation",
    "volume": "main",
    "abstract": "Code-switching is about dealing with alternative languages in speech or text. It is partially speaker-dependent and domain-related, so completely explaining the phenomenon by linguistic rules is challenging. Compared to most monolingual tasks, insufficient data is an issue for code-switching. To mitigate the issue without expensive human annotation, we proposed an unsupervised method for code-switching data augmentation. By utilizing a generative adversarial network, we can generate intra-sentential code-switching sentences from monolingual sentences. We applied the proposed method on two corpora, and the result shows that the generated code-switching sentences improve the performance of code-switching language models",
    "checked": true,
    "id": "cbfac953843d16a6447c7547ab7048eccfa142c9",
    "semantic_title": "code-switching sentence generation by generative adversarial networks and its application to data augmentation",
    "citation_count": 54
  },
  "https://www.isca-speech.org/archive/interspeech_2019/meier19_interspeech.html": {
    "title": "Comparative Analysis of Think-Aloud Methods for Everyday Activities in the Context of Cognitive Robotics",
    "volume": "main",
    "abstract": "We describe our efforts to compare data collection methods using two think-aloud protocols in preparation to be used as a basis for automatic structuring and labeling of a large database of high-dimensional human activities data into a valuable resource for research in cognitive robotics. The envisioned dataset, currently in development, will contain synchronously recorded multimodal data, including audio, video, and biosignals (eye-tracking, motion-tracking, muscle and brain activity) from about 100 participants performing everyday activities while describing their task through use of think-aloud protocols. This paper provides details of our pilot recordings in the well-established and scalable \"table setting scenario,\" describes the concurrent and retrospective think-aloud protocols used, the methods used to analyze them, and compares their potential impact on the data collected as well as the automatic data segmentation and structuring process",
    "checked": true,
    "id": "5122e8acc32d7089f508fb9c11e33712c033e85d",
    "semantic_title": "comparative analysis of think-aloud methods for everyday activities in the context of cognitive robotics",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/beeferman19_interspeech.html": {
    "title": "RadioTalk: A Large-Scale Corpus of Talk Radio Transcripts",
    "volume": "main",
    "abstract": "We introduce RadioTalk, a corpus of speech recognition transcripts sampled from talk radio broadcasts in the United States between October of 2018 and March of 2019. The corpus is intended for use by researchers in the fields of natural language processing, conversational analysis, and the social sciences. The corpus encompasses approximately 2.8 billion words of automatically transcribed speech from 284,000 hours of radio, together with metadata about the speech, such as geographical location, speaker turn boundaries, gender, and radio program information. In this paper we summarize why and how we prepared the corpus, give some descriptive statistics on stations, shows and speakers, and carry out a few high-level analyses",
    "checked": true,
    "id": "455c31d401981a49b08d7f6fefd62101734b925d",
    "semantic_title": "radiotalk: a large-scale corpus of talk radio transcripts",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mdhaffar19_interspeech.html": {
    "title": "Qualitative Evaluation of ASR Adaptation in a Lecture Context: Application to the PASTEL Corpus",
    "volume": "main",
    "abstract": "Lectures are usually known to be highly specialised in that they deal with multiple and domain specific topics. This context is challenging for Automatic Speech Recognition (ASR) systems since they are sensitive to topic variability. Language Model (LM) adaptation is a commonly used technique to address the mismatch problem between training and test data. In this paper, we are interested in a qualitative analysis in order to relevantly compare the accuracy of the LM adaptation. While word error rate is the most common metric used to evaluate ASR systems, we consider that this metric cannot provide accurate information. Consequently, we explore the use of other metrics based on individual word error rate, indexability, and capability of building relevant requests for information retrieval from the ASR outputs. Experiments are carried out on the PASTEL corpus, a new dataset in French language, composed of lecture recordings, manual chaptering, manual transcriptions, and slides. While an adapted LM allows us to reduce the global classical word error rate by 15.62% in relative, we show that this reduction reaches 44.2% when computed on relevant words only. These observations are confirmed with the high LM adaptation gains obtained with indexability and information retrieval metrics",
    "checked": true,
    "id": "e2d6614b1fc90fce21d47f2ceced68690222d042",
    "semantic_title": "qualitative evaluation of asr adaptation in a lecture context: application to the pastel corpus",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/marinelli19_interspeech.html": {
    "title": "Active Annotation: Bootstrapping Annotation Lexicon and Guidelines for Supervised NLU Learning",
    "volume": "main",
    "abstract": "Natural Language Understanding (NLU) models are typically trained in a supervised learning framework. In the case of intent classification, the predicted labels are predefined and based on the designed annotation schema while the labeling process is based on a laborious task where annotators manually inspect each utterance and assign the corresponding label. We propose an Active Annotation (AA) approach where we combine an unsupervised learning method in the embedding space, a human-in-the-loop verification process, and linguistic insights to create lexicons that can be open categories and adapted over time. In particular, annotators define the y-label space on-the-fly during the annotation using an iterative process and without the need for prior knowledge about the input data. We evaluate the proposed annotation paradigm in a real use-case NLU scenario. Results show that our Active Annotation paradigm achieves accurate and higher quality training data, with an annotation speed of an order of magnitude higher with respect to the traditional human-only driven baseline annotation methodology",
    "checked": true,
    "id": "6eca4a8bada8c59373366241e0f474e18ab28194",
    "semantic_title": "active annotation: bootstrapping annotation lexicon and guidelines for supervised nlu learning",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dabike19_interspeech.html": {
    "title": "Automatic Lyric Transcription from Karaoke Vocal Tracks: Resources and a Baseline System",
    "volume": "main",
    "abstract": "Automatic sung speech recognition is a relatively understudied topic that has been held back by a lack of large and freely available datasets. This has recently changed thanks to the release of the DAMP Sing! dataset, a 1100 hour karaoke dataset originating from the social music-making company, Smule. This paper presents work undertaken to define an easily replicable, automatic speech recognition benchmark for this data. In particular, we describe how transcripts and alignments have been recovered from Karaoke prompts and timings; how suitable training, development and test sets have been defined with varying degrees of accent variability; and how language models have been developed using lyric data from the LyricWikia website. Initial recognition experiments have been performed using factored-layer TDNN acoustic models with lattice-free MMI training using Kaldi. The best WER is 19.60% — a new state-of-the-art for this type of data. The paper concludes with a discussion of the many challenging problems that remain to be solved. Dataset definitions and Kaldi scripts have been made available so that the benchmark is easily replicable",
    "checked": true,
    "id": "97c3344232933d29bc4f9e5f548a3cbebc3ac72d",
    "semantic_title": "automatic lyric transcription from karaoke vocal tracks: resources and a baseline system",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19b_interspeech.html": {
    "title": "Detecting Mismatch Between Speech and Transcription Using Cross-Modal Attention",
    "volume": "main",
    "abstract": "In this paper, we propose to detect mismatches between speech and transcriptions using deep neural networks. Although it is generally assumed there are no mismatches in some speech related applications, it is hard to avoid the errors due to one reason or another. Moreover, the use of mismatched data probably leads to performance reduction when training a model. In our work, instead of detecting the errors by computing the distance between manual transcriptions and text strings obtained using a speech recogniser, we view mismatch detection as a classification task and merge speech and transcription features using deep neural networks. To enhance detection ability, we use cross-modal attention mechanism in our approach by learning the relevance between the features obtained from the two modalities. To evaluate the effectiveness of our approach, we test it on Factored WSJCAM0 by randomly setting three kinds of mismatch, word deletion, insertion or substitution. To test its robustness, we train our models using a small number of samples and detect mismatch with different number of words being removed, inserted, and substituted. In our experiments, the results show the use of our approach for mismatch detection is close to 80% on insertion and deletion and outperforms the baseline",
    "checked": true,
    "id": "663e927200e8fe8f3e2c443bc19e108e5c7388dd",
    "semantic_title": "detecting mismatch between speech and transcription using cross-modal attention",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vidal19_interspeech.html": {
    "title": "EpaDB: A Database for Development of Pronunciation Assessment Systems",
    "volume": "main",
    "abstract": "In this paper, we describe the methodology for collecting and annotating a new database designed for conducting research and development on pronunciation assessment. While a significant amount of research has been done in the area of pronunciation assessment, to our knowledge, no database is available for public use for research in the field. Considering this need, we created EpaDB (English Pronunciation by Argentinians Database), which is composed of English phrases read by native Spanish speakers with different levels of English proficiency. The recordings are annotated with ratings of pronunciation quality at phrase-level and detailed phonetic alignments and transcriptions indicating which phones were actually pronounced by the speakers. We present inter-rater agreement, the effect of each phone on overall perceived non-nativeness, and the frequency of specific pronunciation errors",
    "checked": true,
    "id": "13b584e57b52a93af5352a6ac0480dabd15a5813",
    "semantic_title": "epadb: a database for development of pronunciation assessment systems",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/angerbauer19_interspeech.html": {
    "title": "Automatic Compression of Subtitles with Neural Networks and its Effect on User Experience",
    "volume": "main",
    "abstract": "Understanding spoken language can be impeded through factors like noisy environments, hearing impairments or lack of proficiency. Subtitles can help in those cases. However, for fast speech or limited screen size, it might be advantageous to compress the subtitles to their most relevant content. Therefore, we address automatic sentence compression in this paper. We propose a neural network model based on an encoder-decoder approach with the possibility of integrating the desired compression ratio. Using this model, we conduct a user study to investigate the effects of compressed subtitles on user experience. Our results show that compressed subtitles can suffice for comprehension but may pose additional cognitive load",
    "checked": true,
    "id": "297bac44f284a0b62add3223158cef33e8452d98",
    "semantic_title": "automatic compression of subtitles with neural networks and its effect on user experience",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/luo19_interspeech.html": {
    "title": "Integrating Video Retrieval and Moment Detection in a Unified Corpus for Video Question Answering",
    "volume": "main",
    "abstract": "Traditional video question answering models have been designed to retrieve videos to answer input questions. A drawback of this scenario is that users have to watch the entire video to find their desired answer. Recent work presented unsupervised neural models with attention mechanisms to find moments or segments from retrieved videos to provide accurate answers to input questions. Although these two tasks look similar, the latter is more challenging because the former task only needs to judge whether the question is answered in a video and returns the entire video, while the latter is expected to judge which moment within a video matches the question and accurately returns a segment of the video. Moreover, there is a lack of labeled data for training moment detection models. In this paper, we focus on integrating video retrieval and moment detection in a unified corpus. We further develop two models — a self-attention convolutional network and a memory network — for the tasks. Experimental results on our corpus show that the neural models can accurately detect and retrieve moments in supervised settings",
    "checked": true,
    "id": "a58cd5975fe958e11642edfc63e479c9c066b57d",
    "semantic_title": "integrating video retrieval and moment detection in a unified corpus for video question answering",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gutz19_interspeech.html": {
    "title": "Early Identification of Speech Changes Due to Amyotrophic Lateral Sclerosis Using Machine Classification",
    "volume": "main",
    "abstract": "We used a machine learning (ML) approach to detect bulbar amyotrophic lateral sclerosis (ALS) prior to the onset of overt speech symptoms. The dataset included speech samples from 123 participants who were stratified by sex and into three groups: healthy controls, ALS symptomatic, and ALS presymptomatic. We compared models trained on three group pairs (symptomatic-control, presymptomatic-control, and all ALS-control participants). Using acoustic features obtained with the OpenSMILE ComParE13 configuration, we tested several feature filtering techniques. ML classification was achieved using an SVM model and leave-one-out cross-validation. The most successful model, which was trained on symptomatic-control data, yielded an AUC=0.99 for females and AUC=0.91 for males. Models trained on all ALS-control participants had high diagnostic accuracy for classifying symptomatic and presymptomatic ALS participants (females: AUC=0.85; males: AUC=0.91). Additionally, probabilities from these models correlated with speaking rate (females: Spearman coefficient=-0.60, p<0.001; males: Spearman coefficient=-0.43, p<0.001) and intelligible speaking rate (females: Spearman coefficient=-0.65, p<0.001; males: Spearman coefficient=-0.40, p<0.01), indicating their possible use as a severity index of bulbar motor involvement in ALS. These results highlight the importance of stratifying patients by speech severity when testing diagnostic models and demonstrate the potential of ML classification in early detection and progress monitoring of ALS",
    "checked": true,
    "id": "b90d5faeaf7a610b722b66abe5a3ca9ded010d2b",
    "semantic_title": "early identification of speech changes due to amyotrophic lateral sclerosis using machine classification",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/k19_interspeech.html": {
    "title": "Automatic Detection of Breath Using Voice Activity Detection and SVM Classifier with Application on News Reports",
    "volume": "main",
    "abstract": "Breath detection during speech has broad applications ranging from emotion recognition to detection of diseases. Most of the breath detection equipment are contact based. In the proposed method, we use a voice activity detector (VAD) to find the non-speech region and searches the breath only in this region since breath is a non-speech activity. This reduces the execution time. A support vector machine (SVM) classifier is used with radial basis function (RBF) kernel trained on the cepstrogram feature to detect the breaths in the non-speech regions. The classifier output is post-processed to join breathing segments which are closely spaced and remove small duration breaths. Speech breathing rate is calculated as the ratio of the number of breaths to the time between the first and last breath. The algorithm is tested on a student evaluation database. The algorithm yields an F1 Score of 94% and root mean square error (RMSE) of 7.08 breaths/min for the speech-breathing rate. The output has been validated using thermal videos. The breaths have been classified as full and partial detection based on the Intersection over Union (IOU). The algorithm is also tested on some news channel reports which gave a minimum F1 Score of 73%",
    "checked": true,
    "id": "470da741e8afd6e2ab4eabcb7791f92de754da78",
    "semantic_title": "automatic detection of breath using voice activity detection and svm classifier with application on news reports",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/heo19_interspeech.html": {
    "title": "Acoustic Scene Classification Using Teacher-Student Learning with Soft-Labels",
    "volume": "main",
    "abstract": "Acoustic scene classification identifies an input segment into one of the pre-defined classes using spectral information. The spectral information of acoustic scenes may not be mutually exclusive due to common acoustic properties across different classes, such as babble noises included in both airports and shopping malls. However, conventional training procedure based on one-hot labels does not consider the similarities between different acoustic scenes. We exploit teacher-student learning with the purpose to derive soft-labels that consider common acoustic properties among different acoustic scenes. In teacher-student learning, the teacher network produces soft-labels, based on which the student network is trained. We investigate various methods to extract soft-labels that better represent similarities across different scenes. Such attempts include extracting soft-labels from multiple audio segments that are defined as an identical acoustic scene. Experimental results demonstrate the potential of our approach, showing a classification accuracy of 77.36% on the DCASE 2018 task 1 validation set",
    "checked": true,
    "id": "0f7016fca9cf4f3c89683d94530f0eb6a66af8de",
    "semantic_title": "acoustic scene classification using teacher-student learning with soft-labels",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19_interspeech.html": {
    "title": "Rare Sound Event Detection Using Deep Learning and Data Augmentation",
    "volume": "main",
    "abstract": "There is an increasing interest in smart environment and a growing adoption of smart devices. Smart assistants such as Google Home and Amazon Alexa, although focus on speech, could be extended to identify domestic events in real-time to provide more and better smart functions. Sound event detection aims to detect multiple target sound events that may happen simultaneously. The task is challenging due to the overlapping of sound events, the highly imbalanced nature of target and non-target data, and the complicated real-world background noise. In this paper, we proposed a unified approach that takes advantages of both the deep learning and data augmentation. A convolutional neural network (CNN) was combined with a feed-forward neural network (FNN) to improve the detection performance, and a dynamic time warping based data augmentation (DA) method was proposed to address the data imbalance problem. Experiments on several datasets showed a more than 7% increase in accuracy compared to the state-of-the-art approaches",
    "checked": true,
    "id": "4c9e18ce3877a4d90bc0848b57a42b4e8809bba7",
    "semantic_title": "rare sound event detection using deep learning and data augmentation",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sharma19_interspeech.html": {
    "title": "A Combination of Model-Based and Feature-Based Strategy for Speech-to-Singing Alignment",
    "volume": "main",
    "abstract": "Speech and singing are different in many ways. In this work, we propose a novel method to align phonetically identical spoken lyric with a singing vocal in a speech-singing parallel corpus, that is needed in speech-to-singing conversion. We attempt to align speech to singing vocal using a combination of model-based forced alignment and feature-based dynamic time warping (DTW). We first obtain the word boundaries of speech and singing vocals with forced alignment using speech and singing adapted acoustic models, respectively. We consider that speech acoustic models are more accurate than singing acoustic models, therefore, boundaries of spoken words are more accurate than sung words. By searching in the neighborhood of the sung word boundaries in the singing vocal, we hope to improve the alignment between spoken words and sung words. Considering the word boundaries as landmark, we perform speech-to-singing alignment at frame-level using DTW. The proposed method is able to achieve a 47.5% reduction in terms of word boundary error over the baseline, and subsequent improvement of singing quality in a speech-to-singing conversion system",
    "checked": true,
    "id": "07b79f26a8a84f7ddbd81d8222ffd1e55e9ea4bd",
    "semantic_title": "a combination of model-based and feature-based strategy for speech-to-singing alignment",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shrem19_interspeech.html": {
    "title": "Dr.VOT: Measuring Positive and Negative Voice Onset Time in the Wild",
    "volume": "main",
    "abstract": "Voice Onset Time (VOT), a key measurement of speech for basic research and applied medical studies, is the time between the onset of a stop burst and the onset of voicing. When the voicing onset precedes burst onset the VOT is negative; if voicing onset follows the burst, it is positive. In this work, we present a deep-learning model for accurate and reliable measurement of VOT in naturalistic speech. The proposed system addresses two critical issues: it can measure positive and negative VOT equally well, and it is trained to be robust to variation across annotations. Our approach is based on the structured prediction framework, where the feature functions are defined to be RNNs. These learn to capture segmental variation in the signal. Results suggest that our method substantially improves over the current state-of-the-art. In contrast to previous work, our Deep and Robust VOT annotator, Dr.VOT, can successfully estimate negative VOTs while maintaining state-of-the-art performance on positive VOTs. This high level of performance generalizes to new corpora without further retraining",
    "checked": false,
    "id": "90d7175eb18eab5bb969e6e35bd8e2cc6881e43c",
    "semantic_title": "dr.vot : measuring positive and negative voice onset time in the wild",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hui19_interspeech.html": {
    "title": "Effects of Base-Frequency and Spectral Envelope on Deep-Learning Speech Separation and Recognition Models",
    "volume": "main",
    "abstract": "Base-frequencies (F0) and spectral envelopes play an important role in speech separation and recognition by humans. Two experiments were conducted to study how trained networks for multi-speaker speech separation/recognition are affected by difference of F0 and spectral envelopes between source signals. The first experiment examined the effects of natural F0/envelope on the performance of speech separation. Results showed that when the two target signals differed in F0 by ±3 semitones or more or differed in the envelope by a scaling factor larger than 1.08 or less than 0.92, separation performance improved significantly. This is consistent with human listeners and is the first finding for deep learning-network (DNN) models. The second experiment tested the effect of F0/envelope difference on multi-speaker automatic speech recognition(ASR) system's performance. Results showed that multi-speaker recognition result also significantly rely on F0/envelope differences. The overall results indicated that the dependency of the existing automatic systems on monaural cues is similar to that of human, while automatic systems still perform inferior than human on same tasks",
    "checked": true,
    "id": "342580d32d493b25fbc0c52f87a58091f54e4e5f",
    "semantic_title": "effects of base-frequency and spectral envelope on deep-learning speech separation and recognition models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shah19_interspeech.html": {
    "title": "Phone Aware Nearest Neighbor Technique Using Spectral Transition Measure for Non-Parallel Voice Conversion",
    "volume": "main",
    "abstract": "Nearest Neighbor (NN)-based alignment techniques are popular in non-parallel Voice Conversion (VC). The performance of NN-based alignment improves with the information about phone boundary. However, estimating the exact phone boundary is a challenging task. If text corresponding to the utterance is available, the Hidden Markov Model (HMM) can be used to identify the phone boundaries. However, it requires a large amount of training data that is difficult to collect in realistic VC scenarios. Hence, we propose to exploit a Spectral Transition Measure (STM)-based alignment technique that does not require apriori training data. The idea behind STM is that neurons in the auditory or visual cortex respond strongly to the transitional stimuli compared to the steady-state stimuli. The phone boundaries estimated using the STM algorithm are then applied to the NN technique to obtain the aligned spectral features of the source and target speakers. Proposed STM+NN alignment technique is giving on an average 13.67% relative improvement in phonetic accuracy (PA) compared to the NN-based alignment technique. The improvement in %PA after alignment has positively reflected in the better performance in terms of speech quality and speaker similarity (in particular, a relative improvement of 13.63% and 13.26% , respectively) of the converted voice",
    "checked": true,
    "id": "13f8cd2bc61dfdf514172e27441f075285a8ee91",
    "semantic_title": "phone aware nearest neighbor technique using spectral transition measure for non-parallel voice conversion",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shankar19_interspeech.html": {
    "title": "Weakly Supervised Syllable Segmentation by Vowel-Consonant Peak Classification",
    "volume": "main",
    "abstract": "We present a novel approach for blind syllable segmentation that combines model-based feature selection with data-driven classification. In particular, we learn a function that maps short-term energy peaks of a speech utterance onto either the vowel or consonant class. The features used for classification capture spectral and energy signatures which are characteristic of the phonetic properties of the English language. The identified vowel peaks subsequently act as the nucleus of our syllable segments. We demonstrate the effectiveness of our proposed method using nested cross validation on 400 unique test utterances taken randomly from the TIMIT dataset containing over 5000 syllables in total. Our hybrid approach achieves lower insertion rate than the state-of-the-art segmentation methods and a lower deletion rate than all the baseline comparisons",
    "checked": true,
    "id": "3faa3dcb4fc2ac82e54cd021251743b35419bf15",
    "semantic_title": "weakly supervised syllable segmentation by vowel-consonant peak classification",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mateju19_interspeech.html": {
    "title": "An Approach to Online Speaker Change Point Detection Using DNNs and WFSTs",
    "volume": "main",
    "abstract": "In this paper, a new approach to speaker change point (SCP) detection is presented. This method is suitable for online applications (e.g., real-time broadcast monitoring). It is designed in a series of consecutive experiments, aiming at quality of detection as well as low latency. The resulting scheme utilizes a convolution neural network (CNN), whose output is smoothed by a decoder. The CNN is trained using data complemented by artificial examples to reduce different types of errors, and the decoder is based on a weighted finite state transducer (WFST) with the forced length of the transition model. Results obtained on data taken from the COST278 database show that our online approach yields results comparable with an offline multi-pass LIUM toolkit while operating online with a low latency",
    "checked": true,
    "id": "d6f8ef95caef1ebcf2e12fea23a5e76e39f5f5a0",
    "semantic_title": "an approach to online speaker change point detection using dnns and wfsts",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tang19_interspeech.html": {
    "title": "Regression and Classification for Direction-of-Arrival Estimation with Convolutional Recurrent Neural Networks",
    "volume": "main",
    "abstract": "We present a novel learning-based approach to estimate the direction-of-arrival (DOA) of a sound source using a convolutional recurrent neural network (CRNN) trained via regression on synthetic data and Cartesian labels. We also describe an improved method to generate synthetic data to train the neural network using state-of-the-art sound propagation algorithms that model specular as well as diffuse reflections of sound. We compare our model against three other CRNNs trained using different formulations of the same problem: classification on categorical labels, and regression on spherical coordinate labels. In practice, our model achieves up to 43% decrease in angular error over prior methods. The use of diffuse reflection results in 34% and 41% reduction in angular prediction errors on LOCATA and SOFA datasets, respectively, over prior methods based on image-source methods. Our method results in an additional 3% error reduction over prior schemes that use classification networks, and we use 36% fewer network parameters",
    "checked": true,
    "id": "f282a6e0ca8f3f6bdc32fd471094145bd847f319",
    "semantic_title": "regression and classification for direction-of-arrival estimation with convolutional recurrent neural networks",
    "citation_count": 28
  },
  "https://www.isca-speech.org/archive/interspeech_2019/paul19_interspeech.html": {
    "title": "Non-Parallel Voice Conversion Using Weighted Generative Adversarial Networks",
    "volume": "main",
    "abstract": "In this paper, we suggest a novel way to train Generative Adversarial Network (GAN) for the purpose of non-parallel, many-to-many voice conversion. The goal of voice conversion (VC) is to transform speech from a source speaker to that of a target speaker without changing the phonetic contents. Based on ideas from Game Theory, we suggest to multiply the gradient of the Generator with suitable weights. Weights are calculated so that they increase the power of fake samples that fool the Discriminator resulting in a stronger Generator. Motivated by a recently presented GAN based approach for VC, StarGAN-VC, we suggest a variation to StarGAN, referred to as Weighted StarGAN (WeStarGAN). The experiments are conducted on standard CMU ARCTIC database. WeStarGAN-VC approach achieves significantly better relative performance and is clearly preferred over recently proposed StarGAN-VC method in terms of speech subjective quality and speaker similarity with 75% and 65% preference scores, respectively",
    "checked": true,
    "id": "f87e009ed2c0010b40894bee954d2fae8ad81c5a",
    "semantic_title": "non-parallel voice conversion using weighted generative adversarial networks",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chou19_interspeech.html": {
    "title": "One-Shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization",
    "volume": "main",
    "abstract": "Recently, voice conversion (VC) without parallel data has been successfully adapted to multi-target scenario in which a single model is trained to convert the input voice to many different speakers. However, such model suffers from the limitation that it can only convert the voice to the speakers in the training data, which narrows down the applicable scenario of VC. In this paper, we proposed a novel one-shot VC approach which is able to perform VC by only an example utterance from source and target speaker respectively, and the source and target speaker do not even need to be seen during training. This is achieved by disentangling speaker and content representations with instance normalization (IN). Objective and subjective evaluation shows that our model is able to generate the voice similar to target speaker. In addition to the performance measurement, we also demonstrate that this model is able to learn meaningful speaker representations without any supervision",
    "checked": true,
    "id": "c77fa76a857051a6c7deb135a45af8d4a5f32f0f",
    "semantic_title": "one-shot voice conversion by separating speaker and content representations with instance normalization",
    "citation_count": 162
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lu19_interspeech.html": {
    "title": "One-Shot Voice Conversion with Global Speaker Embeddings",
    "volume": "main",
    "abstract": "Building a voice conversion (VC) system for a new target speaker typically requires a large amount of speech data from the target speaker. This paper investigates a method to build a VC system for arbitrary target speaker using one given utterance without any adaptation training process. Inspired by global style tokens (GSTs), which recently has been shown to be effective in controlling the style of synthetic speech, we propose the use of global speaker embeddings (GSEs) to control the conversion target of the VC system. Speaker-independent phonetic posteriorgrams (PPGs) are employed as the local condition input to a conditional WaveNet synthesizer for waveform generation of the target speaker. Meanwhile, spectrograms are extracted from the given utterance and fed into a reference encoder, the generated reference embedding is then employed as attention query to the GSEs to produce the speaker embedding, which is employed as the global condition input to the WaveNet synthesizer to control the generated waveform's speaker identity. In experiments, when compared with an adaptation training based any-to-any VC system, the proposed GSEs based VC approach performs equally well or better in both speech naturalness and speaker similarity, with apparently higher flexibility to the comparison",
    "checked": true,
    "id": "f411384d0fc902e6492ea0863aa6f7d910b4abeb",
    "semantic_title": "one-shot voice conversion with global speaker embeddings",
    "citation_count": 32
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tobing19_interspeech.html": {
    "title": "Non-Parallel Voice Conversion with Cyclic Variational Autoencoder",
    "volume": "main",
    "abstract": "In this paper, we present a novel technique for a non-parallel voice conversion (VC) with the use of cyclic variational auto-encoder (CycleVAE)-based spectral modeling. In a variational autoencoder (VAE) framework, a latent space, usually with a Gaussian prior, is used to encode a set of input features. In a VAE-based VC, the encoded latent features are fed into a decoder, along with speaker-coding features, to generate estimated spectra with either the original speaker identity (reconstructed) or another speaker identity (converted). Due to the non-parallel modeling condition, the converted spectra can not be directly optimized, which heavily degrades the performance of a VAE-based VC. In this work, to overcome this problem, we propose to use CycleVAE-based spectral model that indirectly optimizes the conversion flow by recycling the converted features back into the system to obtain corresponding cyclic reconstructed spectra that can be directly optimized. The cyclic flow can be continued by using the cyclic reconstructed features as input for the next cycle. The experimental results demonstrate the effectiveness of the proposed CycleVAE-based VC, which yields higher accuracy of converted spectra, generates latent features with higher correlation degree, and significantly improves the quality and conversion accuracy of the converted speech",
    "checked": true,
    "id": "a63ad584efd71d9ab0700f083b6fa7b516c8e1d8",
    "semantic_title": "non-parallel voice conversion with cyclic variational autoencoder",
    "citation_count": 54
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kaneko19_interspeech.html": {
    "title": "StarGAN-VC2: Rethinking Conditional Methods for StarGAN-Based Voice Conversion",
    "volume": "main",
    "abstract": "Non-parallel multi-domain voice conversion (VC) is a technique for learning mappings among multiple domains without relying on parallel data. This is important but challenging owing to the requirement of learning multiple mappings and the non-availability of explicit supervision. Recently, StarGAN-VC has garnered attention owing to its ability to solve this problem only using a single generator. However, there is still a gap between real and converted speech. To bridge this gap, we rethink conditional methods of StarGAN-VC, which are key components for achieving non-parallel multi-domain VC in a single model, and propose an improved variant called StarGAN-VC2. Particularly, we rethink conditional methods in two aspects: training objectives and network architectures. For the former, we propose a source-and-target conditional adversarial loss that allows all source domain data to be convertible to the target domain data. For the latter, we introduce a modulation-based conditional method that can transform the modulation of the acoustic feature in a domain-specific manner. We evaluated our methods on non-parallel multi-speaker VC. An objective evaluation demonstrates that our proposed methods improve speech quality in terms of both global and local structure measures. Furthermore, a subjective evaluation shows that StarGAN-VC2 outperforms StarGAN-VC in terms of naturalness and speaker similarity",
    "checked": true,
    "id": "3830bb029e2abd6240ecacf74ec95f584aa2c167",
    "semantic_title": "stargan-vc2: rethinking conditional methods for stargan-based voice conversion",
    "citation_count": 103
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kurita19_interspeech.html": {
    "title": "Robustness of Statistical Voice Conversion Based on Direct Waveform Modification Against Background Sounds",
    "volume": "main",
    "abstract": "This paper presents an investigation of the robustness of statistical voice conversion (VC) under noisy environments. To develop various VC applications, such as augmented vocal production and augmented speech production, it is necessary to handle noisy input speech because some background sounds, such as external noise and an accompanying sound, usually exist in a real environment. In this paper, we investigate an impact of the background sounds on the conversion performance in singing voice conversion focusing on two main VC frameworks, 1) vocoder-based VC and 2) vocoder-free VC based on direct waveform modification. We conduct a subjective evaluation on the converted singing voice quality under noisy conditions and reveal that the vocoder-free VC is more robust against background sounds compared with the vocoder-based VC. We also analyze the robustness of statistical VC and show that a kurtosis ratio of power spectral components before and after conversion is useful as an objective metric to evaluate it without using any target reference signals",
    "checked": true,
    "id": "97e142eee1bc2e8bad7b3a1b11b595750bacbdff",
    "semantic_title": "robustness of statistical voice conversion based on direct waveform modification against background sounds",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhao19b_interspeech.html": {
    "title": "Fast Learning for Non-Parallel Many-to-Many Voice Conversion with Residual Star Generative Adversarial Networks",
    "volume": "main",
    "abstract": "This paper proposes a fast learning framework for non-parallel many-to-many voice conversion with residual Star Generative Adversarial Networks (StarGAN). In addition to the state-of-the-art StarGAN-VC approach that learns an unreferenced mapping between a group of speakers' acoustic features for nonparallel many-to-many voice conversion, our method, which we call Res-StarGAN-VC, presents an enhancement by incorporating a residual mapping. The idea is to leverage on the shared linguistic content between source and target features during conversion. The residual mapping is realized by using identity shortcut connections from the input to the output of the generator in Res-StarGAN-VC. Such shortcut connections accelerate the learning process of the network with no increase of parameters and computational complexity. They also help generate high-quality fake samples at the very beginning of the adversarial training. Experiments and subjective evaluations show that the proposed method offers (1) significantly faster convergence in adversarial training and (2) clearer pronunciations and better speaker similarity of converted speech, compared to the StarGAN-VC baseline on both mono-lingual and cross-lingual many-to-many voice conversion tasks",
    "checked": true,
    "id": "03a401ff5d20cf8b3cbc29ebcd517662e403106e",
    "semantic_title": "fast learning for non-parallel many-to-many voice conversion with residual star generative adversarial networks",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/juvela19_interspeech.html": {
    "title": "GELP: GAN-Excited Linear Prediction for Speech Synthesis from Mel-Spectrogram",
    "volume": "main",
    "abstract": "Recent advances in neural network -based text-to-speech have reached human level naturalness in synthetic speech. The present sequence-to-sequence models can directly map text to mel-spectrogram acoustic features, which are convenient for modeling, but present additional challenges for vocoding (i.e., waveform generation from the acoustic features). High-quality synthesis can be achieved with neural vocoders, such as WaveNet, but such autoregressive models suffer from slow sequential inference. Meanwhile, their existing parallel inference counterparts are difficult to train and require increasingly large model sizes. In this paper, we propose an alternative training strategy for a parallel neural vocoder utilizing generative adversarial networks, and integrate a linear predictive synthesis filter into the model. Results show that the proposed model achieves significant improvement in inference speed, while outperforming a WaveNet in copy-synthesis quality",
    "checked": false,
    "id": "77634fb70549ebff71e4a39936ae351f630812b5",
    "semantic_title": "gelp: gan-excited liner prediction for speech synthesis from mel-spectrogram",
    "citation_count": 26
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yamamoto19b_interspeech.html": {
    "title": "Probability Density Distillation with Generative Adversarial Networks for High-Quality Parallel Waveform Generation",
    "volume": "main",
    "abstract": "This paper proposes an effective probability density distillation (PDD) algorithm for WaveNet-based parallel waveform generation (PWG) systems. Recently proposed teacher-student frameworks in the PWG system have successfully achieved a real-time generation of speech signals. However, the difficulties optimizing the PDD criteria without auxiliary losses result in quality degradation of synthesized speech. To generate more natural speech signals within the teacher-student framework, we propose a novel optimization criterion based on generative adversarial networks (GANs). In the proposed method, the inverse autoregressive flow-based student model is incorporated as a generator in the GAN framework, and jointly optimized by the PDD mechanism with the proposed adversarial learning method. As this process encourages the student to model the distribution of realistic speech waveform, the perceptual quality of the synthesized speech becomes much more natural. Our experimental results verify that the PWG systems with the proposed method outperform both those using conventional approaches, and also autoregressive generation systems with a well-trained teacher WaveNet",
    "checked": true,
    "id": "2f6e03d60bcd38c76811463eb653e0d5012b9480",
    "semantic_title": "probability density distillation with generative adversarial networks for high-quality parallel waveform generation",
    "citation_count": 39
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mohammadi19_interspeech.html": {
    "title": "One-Shot Voice Conversion with Disentangled Representations by Leveraging Phonetic Posteriorgrams",
    "volume": "main",
    "abstract": "We propose voice conversion model from arbitrary source speaker to arbitrary target speaker with disentangled representations. Voice conversion is a task to convert the voice of spoken utterance of source speaker to that of target speaker. Most prior work require to know either source speaker or target speaker or both in training, with either parallel or non-parallel corpus. Instead, we study the problem of voice conversion in nonparallel speech corpora and one-shot learning setting. We convert an arbitrary sentences of an arbitrary source speaker to target speakers given only one or few target speaker training utterances. To achieve this, we propose to use disentangled representations of speaker identity and linguistic context. We use a recurrent neural network (RNN) encoder for speaker embedding and phonetic posteriorgram as linguistic context encoding, along with a RNN decoder to generate converted utterances. Ours is a simpler model without adversarial training or hierarchical model design and thus more efficient. In the subjective tests, our approach achieved significantly better results compared to baseline regarding similarity",
    "checked": true,
    "id": "420b0c4773068b4507684cc361b4ccb9ea811bc1",
    "semantic_title": "one-shot voice conversion with disentangled representations by leveraging phonetic posteriorgrams",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19c_interspeech.html": {
    "title": "Investigation of F0 Conditioning and Fully Convolutional Networks in Variational Autoencoder Based Voice Conversion",
    "volume": "main",
    "abstract": "In this work, we investigate the effectiveness of two techniques for improving variational autoencoder (VAE) based voice conversion (VC). First, we reconsider the relationship between vocoder features extracted using the high quality vocoders adopted in conventional VC systems, and hypothesize that the spectral features are in fact F0 dependent. Such hypothesis implies that during the conversion phase, the latent codes and the converted features in VAE based VC are in fact source F0 dependent. To this end, we propose to utilize the F0 as an additional input of the decoder. The model can learn to disentangle the latent code from the F0 and thus generates converted F0 dependent converted features. Second, to better capture temporal dependencies of the spectral features and the F0 pattern, we replace the frame wise conversion structure in the original VAE based VC framework with a fully convolutional network structure. Our experiments demonstrate that the degree of disentanglement as well as the naturalness of the converted speech are indeed improved",
    "checked": true,
    "id": "b4dd9991a937cf7a7a542f527fc38397c2752dd9",
    "semantic_title": "investigation of f0 conditioning and fully convolutional networks in variational autoencoder based voice conversion",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19b_interspeech.html": {
    "title": "Jointly Trained Conversion Model and WaveNet Vocoder for Non-Parallel Voice Conversion Using Mel-Spectrograms and Phonetic Posteriorgrams",
    "volume": "main",
    "abstract": "The N10 system in the Voice Conversion Challenge 2018 (VCC 2018) has achieved high voice conversion (VC) performance in terms of speech naturalness and speaker similarity. We believe that further improvements can be gained from joint optimization (instead of separate optimization) of the conversion model and WaveNet vocoder, as well as leveraging information from the acoustic representation of the speech waveform, e.g. from Mel-spectrograms. In this paper, we propose a VC architecture to jointly train a conversion model that maps phonetic posteriorgrams (PPGs) to Mel-spectrograms and a WaveNet vocoder. The conversion model has a bottle-neck layer, whose outputs are concatenated with PPGs before being fed into the WaveNet vocoder as local conditioning. A weighted sum of a Mel-spectrogram prediction loss and a WaveNet loss is used as the objective function to jointly optimize parameters of the conversion model and the WaveNet vocoder. Objective and subjective evaluation results show that the proposed approach is capable of achieving significantly improved quality in voice conversion in terms of speech naturalness and speaker similarity of the converted speech for both cross-gender and intra-gender conversions",
    "checked": true,
    "id": "4012133ae119d3f3b556c0b1c66da9eda961b7a7",
    "semantic_title": "jointly trained conversion model and wavenet vocoder for non-parallel voice conversion using mel-spectrograms and phonetic posteriorgrams",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19b_interspeech.html": {
    "title": "Generative Adversarial Networks for Unpaired Voice Transformation on Impaired Speech",
    "volume": "main",
    "abstract": "This paper focuses on using voice conversion (VC) to improve the speech intelligibility of surgical patients who have had parts of their articulators removed. Due to the difficulty of data collection, VC without parallel data is highly desired. Although techniques for unparallel VC — for example, CycleGAN — have been developed, they usually focus on transforming the speaker identity, and directly transforming the speech of one speaker to that of another speaker and as such do not address the task here. In this paper, we propose a new approach for unparallel VC. The proposed approach transforms impaired speech to normal speech while preserving the linguistic content and speaker characteristics. To our knowledge, this is the first end-to-end GAN-based unsupervised VC model applied to impaired speech. The experimental results show that the proposed approach outperforms CycleGAN",
    "checked": true,
    "id": "7a9dd5562ddd3943925a7b06aecb1268d266dfb1",
    "semantic_title": "generative adversarial networks for unpaired voice transformation on impaired speech",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ding19_interspeech.html": {
    "title": "Group Latent Embedding for Vector Quantized Variational Autoencoder in Non-Parallel Voice Conversion",
    "volume": "main",
    "abstract": "This paper proposes a Group Latent Embedding for Vector Quantized Variational Autoencoders (VQ-VAE) used in nonparallel Voice Conversion (VC). Previous studies have shown that VQ-VAE can generate high-quality VC syntheses when it is paired with a powerful decoder. However, in a conventional VQ-VAE, adjacent atoms in the embedding dictionary can represent entirely different phonetic content. Therefore, the VC syntheses can have mispronunciations and distortions whenever the output of the encoder is quantized to an atom representing entirely different phonetic content. To address this issue, we propose an approach that divides the embedding dictionary into groups and uses the weighted average of atoms in the nearest group as the latent embedding. We conducted both objective and subjective experiments on the non-parallel CSTR VCTK corpus. Results show that the proposed approach significantly improves the acoustic quality of the VC syntheses compared to the traditional VQ-VAE (13.7% relative improvement) while retaining the voice identity of the target speaker",
    "checked": true,
    "id": "d2f7e74336baddc6e5e1ac607ad3e33cb24068bd",
    "semantic_title": "group latent embedding for vector quantized variational autoencoder in non-parallel voice conversion",
    "citation_count": 41
  },
  "https://www.isca-speech.org/archive/interspeech_2019/stephenson19_interspeech.html": {
    "title": "Semi-Supervised Voice Conversion with Amortized Variational Inference",
    "volume": "main",
    "abstract": "In this work we introduce a semi-supervised approach to the voice conversion problem, in which speech from a source speaker is converted into speech of a target speaker. The proposed method makes use of both parallel and non-parallel utterances from the source and target simultaneously during training. This approach can be used to extend existing parallel data voice conversion systems such that they can be trained with semi-supervision. We show that incorporating semi-supervision improves the voice conversion performance compared to fully supervised training when the number of parallel utterances is limited as in many practical applications. Additionally, we find that increasing the number non-parallel utterances used in training continues to improve performance when the amount of parallel training data is held constant",
    "checked": true,
    "id": "1f356df1333122c4e90b13681e57fb4d1dd71715",
    "semantic_title": "semi-supervised voice conversion with amortized variational inference",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dey19_interspeech.html": {
    "title": "Exploiting Semi-Supervised Training Through a Dropout Regularization in End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "In this paper, we explore various approaches for semi-supervised learning in an end-to-end automatic speech recognition (ASR) framework. The first step in our approach involves training a seed model on the limited amount of labelled data. Additional unlabelled speech data is employed through a data-selection mechanism to obtain the best hypothesized output, further used to retrain the seed model. However, uncertainties of the model may not be well captured with a single hypothesis. As opposed to this technique, we apply a dropout mechanism to capture the uncertainty by obtaining multiple hypothesized text transcripts of an speech recording. We assume that the diversity of automatically generated transcripts for an utterance will implicitly increase the reliability of the model. Finally, the data-selection process is also applied on these hypothesized transcripts to reduce the uncertainty. Experiments on freely-available TEDLIUM corpus and proprietary Adobe's internal dataset show that the proposed approach significantly reduces ASR errors, compared to the baseline model",
    "checked": true,
    "id": "8cfbaed6f1edecd51226f5a9e3c4645588e24746",
    "semantic_title": "exploiting semi-supervised training through a dropout regularization in end-to-end speech recognition",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kim19_interspeech.html": {
    "title": "Improved Vocal Tract Length Perturbation for a State-of-the-Art End-to-End Speech Recognition System",
    "volume": "main",
    "abstract": "In this paper, we present an improved vocal tract length perturbation (VTLP) algorithm as a data augmentation technique. VTLP is usually accomplished by adjusting the center frequencies of mel filterbank in [1]. Compared to the conventional approach, we re-synthesize waveforms from the frequency-warped spectra using overlap and addition (OLA). This approach had two advantages: First, we can apply an \"acoustic simulator\" [2, 3] after performing the VTLP-based frequency warping. Second, we may use a different window length for frequency warping from that used in feature processing. We observe that the best performance was obtained when the warping coefficient distribution is between 0.8 and 1.2, and the window length is 50 ms. We obtained 3.66% WER and 12.39% WER on the Librispeech test-clean and test-other using an attention-based end-to-end speech recognition system without using any Language Models (LMs). Using the shallow-fusion technique with a Transformer LM, we achieved 2.44% WER and 8.29% WER on the Librispeech test-clean and test-other sets. To the best of our knowledge, the 2.44% WER on the test-clean is the best result ever reported on this test set",
    "checked": true,
    "id": "82a51742f7693a3a0df7096560f0534037ebbccf",
    "semantic_title": "improved vocal tract length perturbation for a state-of-the-art end-to-end speech recognition system",
    "citation_count": 28
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhu19_interspeech.html": {
    "title": "Multi-Accent Adaptation Based on Gate Mechanism",
    "volume": "main",
    "abstract": "When only a limited amount of accented speech data is available, to promote multi-accent speech recognition performance, the conventional approach is accent-specific adaptation, which adapts the baseline model to multiple target accents independently. To simplify the adaptation procedure, we explore adapting the baseline model to multiple target accents simultaneously with multi-accent mixed data. Thus, we propose using accent-specific top layer with gate mechanism (AST-G) to realize multi-accent adaptation. Compared with the baseline model and accent-specific adaptation, AST-G achieves 9.8% and 1.9% average relative WER reduction respectively. However, in real-world applications, we can't obtain the accent category label for inference in advance. Therefore, we apply using an accent classifier to predict the accent label. To jointly train the acoustic model and the accent classifier, we propose the multi-task learning with gate mechanism (MTL-G). As the accent label prediction could be inaccurate, it performs worse than the accent-specific adaptation. Yet, in comparison with the baseline model, MTL-G achieves 5.1% average relative WER reduction",
    "checked": true,
    "id": "b194db16b1fdddd5b1b087429a754a625f7215c6",
    "semantic_title": "multi-accent adaptation based on gate mechanism",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2019/guo19_interspeech.html": {
    "title": "Unsupervised Adaptation with Adversarial Dropout Regularization for Robust Speech Recognition",
    "volume": "main",
    "abstract": "Recent adversarial methods proposed for unsupervised domain adaptation of acoustic models try to fool a specific domain discriminator and learn both senone-discriminative and domain-invariant hidden feature representations. However, a drawback of these approaches is that the feature generator simply aligns different features into the same distribution without considering the class boundaries of the target domain data. Thus, ambiguous target domain features can be generated near the decision boundaries, decreasing speech recognition performance. In this study, we propose to use Adversarial Dropout Regularization (ADR) in acoustic modeling to overcome the foregoing issue. Specifically, we optimize the senone classifier to make its decision boundaries lie in the class boundaries of unlabeled target data. Then, the feature generator learns to create features far away from the decision boundaries, which are more discriminative. We apply the ADR approach on the CHiME-3 corpus and the proposed method yields up to 12.9% relative WER reductions compared with the baseline trained on source domain data only and further improvement over the widely used gradient reversal layer method",
    "checked": true,
    "id": "bbe07b9c03cb3b11521876b2216faf151f6002db",
    "semantic_title": "unsupervised adaptation with adversarial dropout regularization for robust speech recognition",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kitza19_interspeech.html": {
    "title": "Cumulative Adaptation for BLSTM Acoustic Models",
    "volume": "main",
    "abstract": "This paper addresses the robust speech recognition problem as an adaptation task. Specifically, we investigate the cumulative application of adaptation methods. A bidirectional Long Short-Term Memory (BLSTM) based neural network, capable of learning temporal relationships and translation invariant representations, is used for robust acoustic modeling. Further, i-vectors were used as an input to the neural network to perform instantaneous speaker and environment adaptation, providing 8% relative improvement in word error rate on the NIST Hub5 2000 evaluation testset. By enhancing the first-pass i-vector based adaptation with a second-pass adaptation using speaker and environment dependent transformations within the network, a further relative improvement of 5% in word error rate was achieved. We have reevaluated the features used to estimate i-vectors and their normalization to achieve the best performance in a modern large scale automatic speech recognition system",
    "checked": true,
    "id": "8a568abeba0c2668cbbd58ccc094b8257c9ad4d1",
    "semantic_title": "cumulative adaptation for blstm acoustic models",
    "citation_count": 31
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xie19b_interspeech.html": {
    "title": "Fast DNN Acoustic Model Speaker Adaptation by Learning Hidden Unit Contribution Features",
    "volume": "main",
    "abstract": "Speaker adaptation techniques play a key role in reducing the mismatch between automatic speech recognition (ASR) systems and target users. Deep neural network (DNN) acoustic model adaptation by learning speaker-dependent hidden unit contributions (LHUC) scaling vectors has been widely used. The standard LHUC method not only requires multiple decoding passes in test time but also a substantial amount of adaptation data for robust parameter estimation. In order to address the issues, an efficient method of predicting and compressing the LHUC scaling vectors directly from acoustic features using a time-delay DNN (TDNN) and an online averaging layer is proposed in this paper. The resulting LHUC vectors are then used as auxiliary features to adapt DNN acoustic models. Experiments conducted on a 300-hour Switchboard corpus showed that the DNN and TDNN systems using the proposed predicted LHUC features consistently outperformed the corresponding baseline systems by up to about 9% relative reductions of word error rate. Being combined with i-Vector based adaptation, the LHUC feature adapted TDNN systems demonstrated consistent improvement over comparable i-Vector adapted TDNN system",
    "checked": true,
    "id": "1d79786bd857768a2dad047836d54b067807a719",
    "semantic_title": "fast dnn acoustic model speaker adaptation by learning hidden unit contribution features",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tsunoo19_interspeech.html": {
    "title": "End-to-End Adaptation with Backpropagation Through WFST for On-Device Speech Recognition System",
    "volume": "main",
    "abstract": "An on-device DNN-HMM speech recognition system efficiently works with a limited vocabulary in the presence of a variety of predictable noise. In such a case, vocabulary and environment adaptation is highly effective. In this paper, we propose a novel method of end-to-end (E2E) adaptation, which adjusts not only an acoustic model (AM) but also a weighted finite-state transducer (WFST). We convert a pretrained WFST to a trainable neural network and adapt the system to target environments/vocabulary by E2E joint training with an AM. We replicate Viterbi decoding with forward-backward neural network computation, which is similar to recurrent neural networks (RNNs). By pooling output score sequences, a vocabulary posterior for each utterance is obtained and used for discriminative loss computation. Experiments using 2–10 hours of English/Japanese adaptation datasets indicate that the fine-tuning of only WFSTs and that of only AMs are both comparable to a state-of-the-art adaptation method, and E2E joint training of the two components achieves the best recognition performance. We also adapt each language system to the other language using the adaptation data, and the results show that the proposed method also works well for language adaptations",
    "checked": true,
    "id": "e4b741df848e29236139abb6d9f033b496eb619c",
    "semantic_title": "end-to-end adaptation with backpropagation through wfst for on-device speech recognition system",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sar19_interspeech.html": {
    "title": "Learning Speaker Aware Offsets for Speaker Adaptation of Neural Networks",
    "volume": "main",
    "abstract": "In this work, we present an unsupervised long short-term memory (LSTM) layer normalization technique that we call adaptation by speaker aware offsets (ASAO). These offsets are learned using an auxiliary network attached to the main senone classifier. The auxiliary network takes main network LSTM activations as input and tries to reconstruct speaker, (speaker,phone) and (speaker,senone)-level averages of the activations by minimizing the mean-squared error. Once the auxiliary network is jointly trained with the main network, during test time we do not need additional information for the test data as the network will generate the offset itself. Unlike many speaker adaptation studies which only adapt fully connected layers, our method is applicable to LSTM layers in addition to fully-connected layers. In our experiments, we investigate the effect of ASAO of LSTM layers at different depths. We also show its performance when the inputs are already speaker adapted by feature space maximum likelihood linear regression (fMLLR). In addition, we compare ASAO with a speaker adversarial training framework. ASAO achieves higher senone classification accuracy and lower word error rate (WER) than both the unadapted models and the adversarial model on the HUB4 dataset, with an absolute WER reduction of up to 2%",
    "checked": true,
    "id": "7640a918f798fbbf8034a72a13bf30029954d1f8",
    "semantic_title": "learning speaker aware offsets for speaker adaptation of neural networks",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sim19_interspeech.html": {
    "title": "An Investigation into On-Device Personalization of End-to-End Automatic Speech Recognition Models",
    "volume": "main",
    "abstract": "Speaker-independent speech recognition systems trained with data from many users are generally robust against speaker variability and work well for a large population of speakers. However, these systems do not always generalize well for users with very different speech characteristics. This issue can be addressed by building personalized systems that are designed to work well for each specific user. In this paper, we investigate the idea of securely training personalized end-to-end speech recognition models on mobile devices so that user data and models never leave the device and are never stored on a server. We study how the mobile training environment impacts performance by simulating on-device data consumption. We conduct experiments using data collected from speech impaired users for personalization. Our results show that personalization achieved 63.7% relative word error rate reduction when trained in a server environment and 58.1% in a mobile environment. Moving to on-device personalization resulted in 18.7% performance degradation, in exchange for improved scalability and data privacy. To train the model on device, we split the gradient computation into two and achieved 45% memory reduction at the expense of 42% increase in training time",
    "checked": true,
    "id": "aec9df8489c58cb822f911f184ce3b0c61363dda",
    "semantic_title": "an investigation into on-device personalization of end-to-end automatic speech recognition models",
    "citation_count": 50
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jain19_interspeech.html": {
    "title": "A Multi-Accent Acoustic Model Using Mixture of Experts for Speech Recognition",
    "volume": "main",
    "abstract": "A major challenge in Automatic Speech Recognition(ASR) systems is to handle speech from a diverse set of accents. A model trained using a single accent performs rather poorly when confronted with different accents. One of the solutions is a multi-condition model trained on all the accents. However the performance improvement in this approach might be rather limited. Otherwise, accent-specific models might be trained but they become impractical as number of accents increases. In this paper, we propose a novel acoustic model architecture based on Mixture of Experts (MoE) which works well on multiple accents without having the overhead of training separate models for separate accents. The work is based on our earlier work, termed as MixNet, where we showed performance improvement by separation of phonetic class distributions in the feature space. In this paper, we propose an architecture that helps to compensate phonetic and accent variabilities which helps in even better discrimination among the classes. These variabilities are learned in a joint frame-work, and produce consistent improvements over all the individual accents, amounting to an overall 18% relative improvement in accuracy compared to baseline trained in multi-condition style",
    "checked": true,
    "id": "834c5744ccaabbf1930c96f781ebcd4ee0ef76bc",
    "semantic_title": "a multi-accent acoustic model using mixture of experts for speech recognition",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shor19_interspeech.html": {
    "title": "Personalizing ASR for Dysarthric and Accented Speech with Limited Data",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) systems have dramatically improved over the last few years. ASR systems are most often trained from ‘typical' speech, which means that underrepresented groups don't experience the same level of improvement. In this paper, we present and evaluate finetuning techniques to improve ASR for users with non-standard speech. We focus on two types of non-standard speech: speech from people with amyotrophic lateral sclerosis (ALS) and accented speech. We train personalized models that achieve 62% and 35% relative WER improvement on these two groups, bringing the absolute WER for ALS speakers, on a test set of message bank phrases, down to 10% for mild dysarthria and 20% for more serious dysarthria. We show that 71% of the improvement comes from only 5 minutes of training data. Finetuning a particular subset of layers (with many fewer parameters) often gives better results than finetuning the entire model. This is the first step towards building state of the art ASR models for dysarthric speech",
    "checked": true,
    "id": "df3f3c908be233e8aee9872c28258741f4b5ba0a",
    "semantic_title": "personalizing asr for dysarthric and accented speech with limited data",
    "citation_count": 76
  },
  "https://www.isca-speech.org/archive/interspeech_2019/peskov19_interspeech.html": {
    "title": "Mitigating Noisy Inputs for Question Answering",
    "volume": "main",
    "abstract": "Natural language processing systems are often downstream of unreliable inputs: machine translation, optical character recognition, or speech recognition. For instance, virtual assistants can only answer your questions after understanding your speech. We investigate and mitigate the effects of noise from Automatic Speech Recognition systems on two factoid Question Answering ( qa) tasks. Integrating confidences into the model and forced decoding of unknown words are empirically shown to improve the accuracy of downstream neural qa systems. We create and train models on a synthetic corpus of over 500,000 noisy sentences and evaluate on two human corpora from Quizbowl and Jeopardy! competitions",
    "checked": true,
    "id": "970383c0a41d7ae1ec4b8abaa3033778203377b9",
    "semantic_title": "mitigating noisy inputs for question answering",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gupta19_interspeech.html": {
    "title": "One-vs-All Models for Asynchronous Training: An Empirical Analysis",
    "volume": "main",
    "abstract": "Any given classification problem can be modeled using multiclass or One-vs-All (OVA) architecture. An OVA system consists of as many OVA models as the number of classes, providing the advantage of asynchrony, where each OVA model can be re-trained independent of other models. This is particularly advantageous in settings where scalable model training is a consideration (for instance in an industrial environment where multiple and frequent updates need to be made to the classification system). In this paper, we conduct empirical analysis on realizing independent updates to OVA models and its impact on the accuracy of the overall OVA system. Given that asynchronous updates lead to differences in training datasets for OVA models, we first define a metric to quantify the differences in datasets. Thereafter, using Natural Language Understanding as a task of interest, we estimate the impact of three factors: (i) number of classes, (ii) number of data points and, (iii) divergences in training datasets across OVA models; on the OVA system accuracy. Finally, we observe the accuracy impact of increased asynchrony in a Spoken Language Understanding system. We analyze the results and establish that the proposed metric correlates strongly with the model performances in both the experimental settings",
    "checked": true,
    "id": "63e190652d91b23b60a9b6bcc5c706f56eebbf88",
    "semantic_title": "one-vs-all models for asynchronous training: an empirical analysis",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/marzinotto19_interspeech.html": {
    "title": "Adapting a FrameNet Semantic Parser for Spoken Language Understanding Using Adversarial Learning",
    "volume": "main",
    "abstract": "This paper presents a new semantic frame parsing model, based on Berkeley FrameNet, adapted to process spoken documents in order to perform information extraction from broadcast contents. Building upon previous work that had shown the effectiveness of adversarial learning for domain generalization in the context of semantic parsing of encyclopedic written documents, we propose to extend this approach to elocutionary style generalization. The underlying question throughout this study is whether adversarial learning can be used to combine data from different sources and train models on a higher level of abstraction in order to increase their robustness to lexical and stylistic variations as well as automatic speech recognition errors. The proposed strategy is evaluated on a French corpus of encyclopedic written documents and a smaller corpus of radio podcast transcriptions, both annotated with a FrameNet paradigm. We show that adversarial learning increases all models generalization capabilities both on manual and automatic speech transcription as well as on encyclopedic data",
    "checked": true,
    "id": "f6a520994035941a2a2e301003643fefbe0e0914",
    "semantic_title": "adapting a framenet semantic parser for spoken language understanding using adversarial learning",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/parcollet19_interspeech.html": {
    "title": "M2H-GAN: A GAN-Based Mapping from Machine to Human Transcripts for Speech Understanding",
    "volume": "main",
    "abstract": "Deep learning is at the core of recent spoken language understanding (SLU) related tasks. More precisely, deep neural networks (DNNs) drastically increased the performances of SLU systems, and numerous architectures have been proposed. In the real-life context of theme identification of telephone conversations, it is common to hold both a human, manual (TRS) and an automatically transcribed (ASR) versions of the conversations. Nonetheless, and due to production constraints, only the ASR transcripts are considered to build automatic classifiers. TRS transcripts are only used to measure the performances of ASR systems. Moreover, the recent performances in term of classification accuracy, obtained by DNN related systems are close to the performances reached by humans, and it becomes difficult to further increase the performances by only considering the ASR transcripts. This paper proposes to distillates the TRS knowledge available during the training phase within the ASR representation, by using a new generative adversarial network called M2H-GAN to generate a TRS-like version of an ASR document, to improve the theme identification performances",
    "checked": true,
    "id": "4f26b0e04985a7014e8e374d4832c9e556ec8cf8",
    "semantic_title": "m2h-gan: a gan-based mapping from machine to human transcripts for speech understanding",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/georges19_interspeech.html": {
    "title": "Ultra-Compact NLU: Neuronal Network Binarization as Regularization",
    "volume": "main",
    "abstract": "This paper describes an approach for intent classification and tagging on embedded devices, such as smart watches. We describe a technique to train neuronal networks where the final neuronal network weights are binary. This enables memory bandwidth optimized inference and efficient computation even on constrained/embedded platforms The flow of the approach is as follows: tf-idf word selection method reduces the number of overall weights. Bag-of-Words features are used with a feedforward and recurrent neuronal network for intent classification and tagging, respectively. A novel double Gaussian based regularization term is used to train the network. Finally, the weights are almost clipped lossless to -1 or 1 which results in a tiny binary neuronal network for intent classification and tagging Our technique is evaluated using a text corpus of transcribed and annotated voice queries. The test domain is \"lights control\". We compare the intent and tagging accuracy of the ultra-compact binary neuronal network with our baseline system. The novel approach yields comparable accuracy but reduces the model size by a factor of 16: from 160kB to 10kB",
    "checked": true,
    "id": "8f967d0d3f43ecfdfa847775e24e8a5dab6b239c",
    "semantic_title": "ultra-compact nlu: neuronal network binarization as regularization",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lugosch19_interspeech.html": {
    "title": "Speech Model Pre-Training for End-to-End Spoken Language Understanding",
    "volume": "main",
    "abstract": "Whereas conventional spoken language understanding (SLU) systems map speech to text, and then text to intent, end-to-end SLU systems map speech directly to intent through a single trainable model. Achieving high accuracy with these end-to-end models without a large amount of training data is difficult. We propose a method to reduce the data requirements of end-to-end SLU in which the model is first pre-trained to predict words and phonemes, thus learning good features for SLU. We introduce a new SLU dataset, Fluent Speech Commands, and show that our method improves performance both when the full dataset is used for training and when only a small subset is used. We also describe preliminary experiments to gauge the model's ability to generalize to new phrases not heard during training",
    "checked": true,
    "id": "24d7b1487202e3aaf329df3d8135ae6eabefaa45",
    "semantic_title": "speech model pre-training for end-to-end spoken language understanding",
    "citation_count": 244
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shivakumar19_interspeech.html": {
    "title": "Spoken Language Intent Detection Using Confusion2Vec",
    "volume": "main",
    "abstract": "Decoding speaker's intent is a crucial part of spoken language understanding (SLU). The presence of noise or errors in the text transcriptions, in real life scenarios make the task more challenging. In this paper, we address the spoken language intent detection under noisy conditions imposed by automatic speech recognition (ASR) systems. We propose to employ confusion2vec word feature representation to compensate for the errors made by ASR and to increase the robustness of the SLU system. The confusion2vec, motivated from human speech production and perception, models acoustic relationships between words in addition to the semantic and syntactic relations of words in human language. We hypothesize that ASR often makes errors relating to acoustically similar words, and the confusion2vec with inherent model of acoustic relationships between words is able to compensate for the errors. We demonstrate through experiments on the ATIS benchmark dataset, the robustness of the proposed model to achieve state-of-the-art results under noisy ASR conditions. Our system reduces classification error rate (CER) by 20.84% and improves robustness by 37.48% (lower CER degradation) relative to the previous state-of-the-art going from clean to noisy transcripts. Improvements are also demonstrated when training the intent detection models on noisy transcripts",
    "checked": true,
    "id": "944c90a271afad85d4443f8a63284b708e38913a",
    "semantic_title": "spoken language intent detection using confusion2vec",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tomashenko19_interspeech.html": {
    "title": "Investigating Adaptation and Transfer Learning for End-to-End Spoken Language Understanding from Speech",
    "volume": "main",
    "abstract": "This work investigates speaker adaptation and transfer learning for spoken language understanding (SLU). We focus on the direct extraction of semantic tags from the audio signal using an end-to-end neural network approach. We demonstrate that the learning performance of the target predictive function for the semantic slot filling task can be substantially improved by speaker adaptation and by various knowledge transfer approaches. First, we explore speaker adaptive training (SAT) for end-to-end SLU models and propose to use zero pseudo i-vectors for more efficient model initialization and pretraining in SAT. Second, in order to improve the learning convergence for the target semantic slot filling (SF) task, models trained for different tasks, such as automatic speech recognition and named entity extraction are used to initialize neural end-to-end models trained for the target task. In addition, we explore the impact of the knowledge transfer for SLU from a speech recognition task trained in a different language. These approaches allow to develop end-to-end SLU systems in low-resource data scenarios when there is no enough in-domain semantically labeled data, but other resources, such as word transcriptions for the same or another language or named entity annotation, are available",
    "checked": true,
    "id": "60db06f7a29fa9281cabc15f3d1ef8036abe96f5",
    "semantic_title": "investigating adaptation and transfer learning for end-to-end spoken language understanding from speech",
    "citation_count": 25
  },
  "https://www.isca-speech.org/archive/interspeech_2019/song19_interspeech.html": {
    "title": "Topic-Aware Dialogue Speech Recognition with Transfer Learning",
    "volume": "main",
    "abstract": "Dialogue speech widely exists in scenarios such as chitchat, meeting and customer service. General-purpose speech recognition systems usually neglect the topic information in the context of dialogue speech, which has great potential for improving the performance of speech recognition. In this paper, we propose a transfer learning mechanism to conduct topic-aware recognition for dialogue speech. We first propose a new probabilistic topic model named Dialogue Speech Topic Model (DSTM) that is specialized for modeling the context of dialogue speech. We further propose a novel transfer learning mechanism for DSTM to significantly reduce its training cost while preserving its effectiveness for accurate topic inference. The experiment results demonstrate that proposed techniques in language model adaptation effectively improve the performance of the state-of-the-art Automatic Speech Recognition (ASR) system",
    "checked": true,
    "id": "1e564639cd2a37c97afc9959c14aec529adc7de7",
    "semantic_title": "topic-aware dialogue speech recognition with transfer learning",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/masumura19_interspeech.html": {
    "title": "Improving Conversation-Context Language Models with Multiple Spoken Language Understanding Models",
    "volume": "main",
    "abstract": "In this paper, we integrate fully neural network based conversation-context language models (CCLMs) that are suitable for handling multi-turn conversational automatic speech recognition (ASR) tasks, with multiple neural spoken language understanding (SLU) models. A main strength of CCLMs is their capacity to take long-range interactive contexts beyond utterance boundaries into consideration. However, it is hard to optimize the CCLMs so as to fully exploit the long-range interactive contexts because conversation-level training datasets are often limited. In order to mitigate this problem, our key idea is to introduce various SLU models that are developed for spoken dialogue systems into the CCLMs. In our proposed method (which we call \"SLU-assisted CCLM\"), hierarchical recurrent encoder-decoder based language modeling is extended so as to handle various utterance-level SLU results of preceding utterances in a continuous space. We expect that the SLU models will help the CCLMs to properly understand semantic meanings of long-range interactive contexts and to fully leverage them for estimating a next utterance. Our experiments on contact center dialogue ASR tasks demonstrate that SLU-assisted CCLMs combined with three types of SLU models can yield ASR performance improvements",
    "checked": true,
    "id": "3e08eb1fbc4617976173f97db03b49ea88014241",
    "semantic_title": "improving conversation-context language models with multiple spoken language understanding models",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chien19_interspeech.html": {
    "title": "Meta Learning for Hyperparameter Optimization in Dialogue System",
    "volume": "main",
    "abstract": "The performance of dialogue system based on deep reinforcement learning (DRL) highly depends on the selected hyperparameters in DRL algorithms. Traditionally, Gaussian process (GP) provides a probabilistic approach to Bayesian optimization for sequential search which is beneficial to select optimal hyperparameter. However, GP suffers from the expanding computation when the dimension of hyperparameters and the number of search points are increased. This paper presents a meta learning approach to carry out multifidelity Bayesian optimization where a two-level recurrent neural network (RNN) is developed for sequential learning and optimization. The search space is explored via the first-level RNN with cheap and low fidelity over a global region of hyperparameters. The optimization is then exploited and leveraged by the second-level RNN with a high fidelity on the successively small regions. The experiments on the hyperparameter optimization for dialogue system based on the deep Q network show the effectiveness and efficiency by using the proposed multifidelity Bayesian optimization",
    "checked": true,
    "id": "93e4b6acda7e725ffa4104e270273d96ba97e816",
    "semantic_title": "meta learning for hyperparameter optimization in dialogue system",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2019/williams19_interspeech.html": {
    "title": "Zero Shot Intent Classification Using Long-Short Term Memory Networks",
    "volume": "main",
    "abstract": "We describe a zero shot approach to intent classification that allows for the identification of intents that were not present during training. Our approach makes use of a Long-short Term Memory neural network to encode user queries and intents and uses these encodings to score previously unseen intents based on their semantic similarity to the queries. We test our model on intent classification in a personal digital assistant and show an improvement of 15% over a strong baseline. We also investigate the effect of adding a few training samples for the previously unseen intents in a few shot learning setting and show improvements of up to 16% over the baseline method",
    "checked": true,
    "id": "a40b648c13158b6d87744dcf94d40834653f56c8",
    "semantic_title": "zero shot intent classification using long-short term memory networks",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/korpusik19_interspeech.html": {
    "title": "A Comparison of Deep Learning Methods for Language Understanding",
    "volume": "main",
    "abstract": "In this paper, we compare a suite of neural networks (recurrent, convolutional, and the recently proposed BERT model) to a CRF with hand-crafted features on three semantic tagging corpora: the Air Travel Information System (ATIS) benchmark, restaurant queries, and written and spoken meal descriptions. Our motivation is to investigate pre-trained BERT's transferability to the domains we are interested in. We demonstrate that neural networks without feature engineering outperform state-of-the-art statistical and deep learning approaches on all three tasks (except written meal descriptions, where the CRF is slightly better) and that deep, attention-based BERT, in particular, surpasses state-of-the-art results on these tasks. Error analysis shows the models are less confident when making errors, enabling the system to follow up with the user when uncertain",
    "checked": true,
    "id": "684597268c6ac614439d4f0d5de1756a668fdb8a",
    "semantic_title": "a comparison of deep learning methods for language understanding",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kobayashi19_interspeech.html": {
    "title": "Slot Filling with Weighted Multi-Encoders for Out-of-Domain Values",
    "volume": "main",
    "abstract": "This paper proposes a new method for slot filling of out-of-domain (OOD) slot values, which are not included in the training data, in spoken dialogue systems. Word embeddings have been proposed to estimate the OOD slot values included in the word embedding model from keyword information. At the same time, context information is an important clue for estimation because the values in a given slot tend to appear in similar contexts. The proper use of either or both keyword and context information depends on the sentence. Conventional methods input a whole sentence into an encoder and extract important clues by the attention mechanism. However, it is difficult to properly distinguish context and keyword information from the encoder outputs because these two features are already mixed. Our proposed method uses two encoders, which distinctly encode contexts and keywords, respectively. The model calculates weights for the two encoders based on a user utterance and estimates a slot with weighted outputs from the two encoders. Experimental results show that the proposed method achieves a 50% relative improvement in F1 score compared with a baseline model, which detects slot values from user utterances and estimates slots at once with a single encoder",
    "checked": true,
    "id": "179d8f0960980e48baa50e9094878eccbf2908e4",
    "semantic_title": "slot filling with weighted multi-encoders for out-of-domain values",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/seneviratne19_interspeech.html": {
    "title": "Multi-Corpus Acoustic-to-Articulatory Speech Inversion",
    "volume": "main",
    "abstract": "There are several technologies like Electromagnetic articulometry (EMA), ultrasound, real-time Magnetic Resonance Imaging (MRI), and X-ray microbeam that are used to measure speech articulatory movements. Each of these techniques provides a different view of the vocal tract. The measurements performed using the similar techniques also differ greatly due to differences in the placement of sensors, and the anatomy of speakers. This limits most articulatory studies to single datasets. However to yield better results in its applications, the speech inversion systems should be more generalized, which requires the combination of data from multiple sources. This paper proposes a multi-task learning based deep neural network architecture for acoustic-to-articulatory speech inversion trained using three different articulatory datasets — two of them were measured using EMA, and one using X-ray microbeam. Experiments show improved accuracy of the proposed acoustic-to-articulatory mapping compared to the systems trained using single datasets",
    "checked": true,
    "id": "cc79683297c657277b5b82084e0e36134d03bac8",
    "semantic_title": "multi-corpus acoustic-to-articulatory speech inversion",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dash19_interspeech.html": {
    "title": "Towards a Speaker Independent Speech-BCI Using Speaker Adaptation",
    "volume": "main",
    "abstract": "Neurodegenerative diseases such as amyotrophic lateral sclerosis (ALS) can cause locked-in-syndrome (fully paralyzed but aware). Brain-computer interface (BCI) may be the only option to restore their communication. Current BCIs typically use visual or attention correlates in neural activities to select letters randomly displayed on a screen, which are extremely slow (a few words per minute). Speech-BCIs, which aim to convert the brain activity patterns to speech (neural speech decoding), hold the potential to enable faster communication. Although a few recent studies have shown the potential of neural speech decoding, those are focused on speaker-dependent models. In this study, we investigated speaker-independent neural speech decoding of five continuous phrases from Magnetoencephalography (MEG) signals while 8 subjects produced speech covertly (imagination) or overtly (articulation). We have used both supervised and unsupervised speaker adaptation strategies for implementing a speaker independent model. Experimental results demonstrated that the proposed adaptation-based speaker-independent model has significantly improved decoding performance. To our knowledge, this is the first demonstration of the possibility of speaker-independent neural speech decoding",
    "checked": true,
    "id": "f6e52681fb672dfc7f7ce422395d5b67d2687651",
    "semantic_title": "towards a speaker independent speech-bci using speaker adaptation",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sheth19_interspeech.html": {
    "title": "Identifying Input Features for Development of Real-Time Translation of Neural Signals to Text",
    "volume": "main",
    "abstract": "One of the main goals in Brain-Computer Interface (BCI) research is to help patients with faltering communication abilities due to neurodegenerative diseases produce text or speech output using their neural recordings. However, practical implementation of such a system has proven difficult due to limitations in the speed, accuracy, and training time of existing interfaces. In this paper, we contribute to this endeavour by isolating appropriate input features from speech-producing neural signals that will feed into a machine learning classifier to identify target phonemes. Analysing data from six subjects, we discern frequency bands that encapsulate differential information regarding production of vowels and consonants broadly, and more specifically nasals and semivowels. Subsequent spatial localization analysis reveals the underlying cortical regions responsible for different phoneme categories. Anatomical locations along with their respective frequency bands act as prospective feature sets for machine learning classifiers. We demonstrate this classification ability in a preliminary language reconstruction task and show an average word classification accuracy of 30.6% (p<0.001)",
    "checked": true,
    "id": "2e9dab07ec25a3b5bf032c170c99eccfa432dd74",
    "semantic_title": "identifying input features for development of real-time translation of neural signals to text",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/silva19_interspeech.html": {
    "title": "Exploring Critical Articulator Identification from 50Hz RT-MRI Data of the Vocal Tract",
    "volume": "main",
    "abstract": "The study of the static and dynamic aspects of speech production can profit from technologies such as electromagnetic midsagittal articulography (EMA) and real-time magnetic resonance (RTMRI). These can improve our knowledge on which articulators and gestures are involved in producing specific sounds and foster improved speech production models, paramount to advance, e.g., articulatory speech synthesis. Previous work, by the authors, has shown that critical articulator identification could be performed from RTMRI data of the vocal tract, with encouraging results, by extending the applicability of an unsupervised statistical identification method previously proposed for EMA data. Nevertheless, the slower time resolution of the considered RT-MRI corpus (14 Hz), when compared to EMA, potentially influencing the ability to select the most suitable representative configuration for each phone — paramount for strongly dynamic phones, e.g., nasal vowels —, and the lack of a richer set of contexts — relevant for observing coarticulation effects —, were identified as limitations. This article addresses these limitations by exploring critical articulator identification from a faster RTMRI corpus (50 Hz), for European Portuguese, providing a richer set of contexts, and testing how fusing the articulatory data of two speakers might influence critical articulator determination",
    "checked": true,
    "id": "a0b5fbc2de84d0050a8bbea9b120e37c131a4cb0",
    "semantic_title": "exploring critical articulator identification from 50hz rt-mri data of the vocal tract",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/douros19_interspeech.html": {
    "title": "Towards a Method of Dynamic Vocal Tract Shapes Generation by Combining Static 3D and Dynamic 2D MRI Speech Data",
    "volume": "main",
    "abstract": "We present an algorithm for augmenting the shape of the vocal tract using 3D static and 2D dynamic speech MRI data. While static 3D images have better resolution and provide spatial information, 2D dynamic images capture the transitions. The aim of this work is to combine strong points of these two types of data to obtain better image quality of 2D dynamic images and extend the 2D dynamic images to the 3D domain To produce a 3D dynamic consonant-vowel (CV) sequence, our algorithm takes as input the 2D CV transition and the static 3D targets for C and V. To obtain the enhanced sequence of images, the first step is to find a transformation between the 2D images and the mid-sagittal slice of the acoustically corresponding 3D image stack, and then find a transformation between neighbouring sagittal slices in the 3D static image stack. Combination of these transformations allows producing the final set of images. In the present study we first examined the transformation from the 3D mid-sagittal frame to the 2D video in order to improve image quality and then we examined the extension of the 2D video to the 3rd dimension with the aim to enrich spatial information",
    "checked": true,
    "id": "d67862fc963561190e929e778132e0e62c311903",
    "semantic_title": "towards a method of dynamic vocal tract shapes generation by combining static 3d and dynamic 2d mri speech data",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rasskazova19_interspeech.html": {
    "title": "Temporal Coordination of Articulatory and Respiratory Events Prior to Speech Initiation",
    "volume": "main",
    "abstract": "The investigation of the speech planning processes, in particular the timing between acoustic and articulatory onset, has recently received a lot of attention. Respiration has not been considered in this process so far, although it is involved and may be well coordinated with the oral articulators prior and at the onset of the utterance. In light of these considerations, we investigated the temporal coordination between acoustic, respiratory and articulatory events prior to utterance onset. For this purpose 12 native speakers of German have been recorded with Electromagnetic Articulography and Inductance Plethysmography reading sentences that were controlled for length and stress of the first word. The initial segment of the utterance was either /t/ or /n/. The results for six speakers so far indicate that early speech preparation consists of mouth opening during the inhalation phase. The onset of expiration seems to be tightly coupled with the acoustic and the articulatory onset, particularly with the constriction interval of the tongue tip gesture in the first segment. Manner of articulation of the initial segment seems to affect the temporal fine-tuning of preparatory events",
    "checked": true,
    "id": "bb5dbb9b1f2bd931dead024c171b014771e4f542",
    "semantic_title": "temporal coordination of articulatory and respiratory events prior to speech initiation",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gubian19b_interspeech.html": {
    "title": "Zooming in on Spatiotemporal V-to-C Coarticulation with Functional PCA",
    "volume": "main",
    "abstract": "It has long been proposed in speech production research that in CV sequences, the movement for consonant and vowel are initiated synchronously. However, mostly due to limitations on the statistical analysis of articulator motion over time, this could only be shown in a limited fashion, based on positional differences at a single time point during consonantal constriction formation. It is unknown to which extent this observation generalizes to earlier timepoints. In this paper, we illustrate the use of functional principal component analysis (FPCA) for the statistical analysis of articulator motion over time. Using articulography data, we quantify CV coarticulation during constriction formation of [k] in two vowel contexts. We show how FPCA enables us to analyse both horizontal and vertical movement components over time in a single model while preserving information on temporal variability. We combine FPCA with linear mixed modelling to obtain estimated mean trajectories and confidence bands for [k] in the two vowel contexts. Results show that well before the timepoint of peak velocity the vowel causes a substantial spatial separation of the consonantal trajectories, estimated to be at least 3 mm at peak velocity. This lends support to the hypothesis that vowel and consonant are initiated synchronously",
    "checked": true,
    "id": "80d9da86e9215c288cc900043bb1e61aee4e2bff",
    "semantic_title": "zooming in on spatiotemporal v-to-c coarticulation with functional pca",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/csapo19_interspeech.html": {
    "title": "Ultrasound-Based Silent Speech Interface Built on a Continuous Vocoder",
    "volume": "main",
    "abstract": "Recently it was shown that within the Silent Speech Interface (SSI) field, the prediction of F0 is possible from Ultrasound Tongue Images (UTI) as the articulatory input, using Deep Neural Networks for articulatory-to-acoustic mapping. Moreover, text-to-speech synthesizers were shown to produce higher quality speech when using a continuous pitch estimate, which takes non-zero pitch values even when voicing is not present. Therefore, in this paper on UTI-based SSI, we use a simple continuous F0 tracker which does not apply a strict voiced /unvoiced decision. Continuous vocoder parameters (ContF0, Maximum Voiced Frequency and Mel-Generalized Cepstrum) are predicted using a convolutional neural network, with UTI as input. The results demonstrate that during the articulatory-to-acoustic mapping experiments, the continuous F0 is predicted with lower error, and the continuous vocoder produces slightly more natural synthesized speech than the baseline vocoder using standard discontinuous F0",
    "checked": true,
    "id": "586d74f48dc077b9b4a5c538a6efb269fc153d99",
    "semantic_title": "ultrasound-based silent speech interface built on a continuous vocoder",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2019/klein19_interspeech.html": {
    "title": "Assessing Acoustic and Articulatory Dimensions of Speech Motor Adaptation with Random Forests",
    "volume": "main",
    "abstract": "Although most modern theories of speech production assume that representations of speech sounds are multidimensional encompassing acoustic and articulatory information, speech motor learning studies which assess the degree of adaptation in both dimensions are few and far between. In the current paper, we present an auditory perturbation study of German sibilant [s] in which speakers' audio and articulatory movements were recorded by means of electromagnetic articulography. Random Forest, a supervised learning algorithm, was employed to classify speakers' responses produced under unaltered or perturbed feedback based either on acoustic or articulatory parameters. Preliminary results demonstrate that while classification accuracy increases in the acoustic dimension as the perturbation session goes on, the classification accuracy in the articulatory dimension, although overall higher, remains approximately at the same level. This suggests that the adaptation process is characterized by active exploration of the articulatory space which is guided by speakers' auditory feedback",
    "checked": true,
    "id": "5a453c06c86ceb6cc71da04846ac5472f0a8f22f",
    "semantic_title": "assessing acoustic and articulatory dimensions of speech motor adaptation with random forests",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/takemoto19_interspeech.html": {
    "title": "Speech Organ Contour Extraction Using Real-Time MRI and Machine Learning Method",
    "volume": "main",
    "abstract": "Real-time MRI can be used to obtain videos that describe articulatory movements during running speech. For detailed analysis based on a large number of video frames, it is necessary to extract the contours of speech organs, such as the tongue, semi-automatically. The present study attempted to extract the contours of speech organs from videos using a machine learning method. First, an expert operator manually extracted the contours from the frames of a video to build training data sets. The learning operators, or learners, then extracted the contours from each frame of the video. Finally, the errors representing the geometrical distance between the extracted contours and the ground truth, which were the contours excluded from the training data sets, were examined. The results showed that the contours extracted using machine learning were closer to the ground truth than the contours traced by other expert and non-expert operators. In addition, using the same learners, the contours were extracted from other naive videos obtained during different speech tasks of the same subject. As a result, the errors in those videos were similar to those in the video in which the learners were trained",
    "checked": true,
    "id": "d473df3cdaab94de1083256f4d764e77439b05e8",
    "semantic_title": "speech organ contour extraction using real-time mri and machine learning method",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/leeuwen19_interspeech.html": {
    "title": "CNN-Based Phoneme Classifier from Vocal Tract MRI Learns Embedding Consistent with Articulatory Topology",
    "volume": "main",
    "abstract": "Recent advances in real-time magnetic resonance imaging (rtMRI) of the vocal tract provides opportunities for studying human speech. This modality together with acquired speech may enable the mapping of articulatory configurations to acoustic features. In this study, we take the first step by training a deep learning model to classify 27 different phonemes from midsagittal MR images of the vocal tract An American English database was used to train a convolutional neural network for classifying vowels (13 classes), consonants (14 classes) and all phonemes (27 classes) of 17 subjects. Classification top-1 accuracy of the test set for all phonemes was 57%. Error analysis showed voiced and unvoiced sounds often being confused. Moreover, we performed principal component analysis on the network's embedding and observed topological similarities between the network learned representation and the vowel diagram. Saliency maps gave insight into the anatomical regions most important for classification and show congruence with known regions of articulatory importance We demonstrate the feasibility for deep learning to distinguish between phonemes from MRI. Network analysis can be used to improve understanding of normal articulation and speech and, in the future, impaired speech. This study brings us a step closer to the articulatory-to-acoustic mapping from rtMRI",
    "checked": true,
    "id": "fe8c88ce388e17d9dd2bc3485437464dc7f631ea",
    "semantic_title": "cnn-based phoneme classifier from vocal tract mri learns embedding consistent with articulatory topology",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mucke19_interspeech.html": {
    "title": "Strength and Structure: Coupling Tones with Oral Constriction Gestures",
    "volume": "main",
    "abstract": "According to the segmental anchor hypothesis within the Autosegmental-Metrical approach, tones are aligned with segmental boundaries of consonant and vowels in the acoustic domain. In prenuclear rising pitch accents (LH*), the rise is assumed to occur in the vicinity of the accented syllable it is phonologically associated with. However, there are differences in the alignment patterns within and across languages that cannot be captured within the AM approach. In the present study, we investigate the coordination of tonal and oral constriction gestures within Articulatory Phonology. Therefore, we model the coordination of prenuclear LH* pitch accents in Catalan, Northern and Southern German with respect to syllable production on the basis of recordings with a 2D electromagnetic articulography. We provide an extended coupled oscillators model that allows for balanced and imbalanced coupling strengths. Based on examples, we show that the observed differences in alignment patterns for prenuclear rising pitch accents can be modelled with the same underlying coordinative structures/coupling modes for vocalic and tonal gestures and that surface differences arise from gradient variation in coupling strengths",
    "checked": true,
    "id": "3ebf743f94ad9537ec8d7d8db1474bed5a797b33",
    "semantic_title": "strength and structure: coupling tones with oral constriction gestures",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kleijn19_interspeech.html": {
    "title": "Salient Speech Representations Based on Cloned Networks",
    "volume": "main",
    "abstract": "We define salient features as features that are shared by signals that are defined as being equivalent by a system designer. The definition allows the designer to contribute qualitative information. We aim to find salient features that are useful as conditioning for generative networks. We extract salient features by jointly training a set of clones of an encoder network. Each network clone receives as input a different signal from a set of equivalent signals. The objective function encourages the network clones to map their input into a set of features that is identical across the clones. It additionally encourages feature independence and, optionally, reconstruction of a desired target signal by a decoder. As an application, we train a system that extracts a time-sequence of feature vectors of speech and uses it as a conditioning of a WaveNet generative system, facilitating both coding and enhancement",
    "checked": true,
    "id": "fa5c2be217b6592aebbc7726014692434d0d58dc",
    "semantic_title": "salient speech representations based on cloned networks",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ramanathi19_interspeech.html": {
    "title": "ASR Inspired Syllable Stress Detection for Pronunciation Evaluation Without Using a Supervised Classifier and Syllable Level Features",
    "volume": "main",
    "abstract": "Automatic syllable stress detection is typically performed with a supervised classifier considering manually annotated stress markings and features computed within the syllable segments derived from phoneme transcriptions and their time-aligned boundaries. However, the manual annotation is tedious and the errors in estimating segmental information could degrade stress detection accuracy. In order to circumvent these, we propose to estimate stress markings in automatic speech recognition (ASR) framework involving finite-state-transducer (FST) without using annotated stress markings and segmental information. For this, we train the ASR system with native English data along with pronunciation lexicon containing canonical stress markings and decode non-native utterances as pronunciations embedded with stress markings. In the decoding, we use an FST encoded with the pronunciations derived using phoneme transcriptions and the instructions involved in a typical manual annotation. Experiments are conducted on polysyllabic words taken from ISLE corpus containing utterances spoken by Italian and German speakers and using the ASR models trained with three corpora. Among all the three models, the highest stress detection accuracies with the proposed approach respectively on Italian & German speakers are found to be 2.07% & 1.19% higher than and comparable to those with the two supervised classification approaches used as baselines",
    "checked": true,
    "id": "ead6031f9b40ec5d259b5ec7cfac8ee6f9ea0a29",
    "semantic_title": "asr inspired syllable stress detection for pronunciation evaluation without using a supervised classifier and syllable level features",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mannem19_interspeech.html": {
    "title": "Acoustic and Articulatory Feature Based Speech Rate Estimation Using a Convolutional Dense Neural Network",
    "volume": "main",
    "abstract": "In this paper, we propose a speech rate estimation approach using a convolutional dense neural network (CDNN). The CDNN based approach uses the acoustic and articulatory features for speech rate estimation. The Mel Frequency Cepstral Coefficients (MFCCs) are used as acoustic features and the articulograms representing time-varying vocal tract profile are used as articulatory features. The articulogram is computed from a real-time magnetic resonance imaging (rtMRI) video in the midsagittal plane of a subject while speaking. However, in practice, the articulogram features are not directly available, unlike acoustic features from speech recording. Thus, we use an Acoustic-to-Articulatory Inversion method using a bidirectional long-short-term memory network which estimates the articulogram features from the acoustics. The proposed CDNN based approach using estimated articulatory features requires both acoustic and articulatory features during training but it requires only acoustic data during testing. Experiments are conducted using rtMRI videos from four subjects each speaking 460 sentences. The Pearson correlation coefficient is used to evaluate the speech rate estimation. It is found that the CDNN based approach gives a better correlation coefficient than the temporal and selected sub-band correlation (TCSSBC) based baseline scheme by 81.58% and 73.68% (relative) in seen and unseen subject conditions respectively",
    "checked": true,
    "id": "d78b4ebd047a0d38dfbcfadb53d9666e08dd6a47",
    "semantic_title": "acoustic and articulatory feature based speech rate estimation using a convolutional dense neural network",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/springenberg19_interspeech.html": {
    "title": "Predictive Auxiliary Variational Autoencoder for Representation Learning of Global Speech Characteristics",
    "volume": "main",
    "abstract": "Unsupervised learning represents an important opportunity for obtaining useful speech representations. Recently, variational autoencoders (VAEs) have been shown to extract useful representations in an unsupervised manner. These models are usually not designed to explicitly disentangle specific sources of information. When processing data of sequential nature which involves multi-timescale information, disentanglement can however be beneficial. In this paper we address this issue by developing a predictive auxiliary variational autoencoder to obtain speech representations at different timescales. We will present an auxiliary lower bound which is used to develop a model that we call the Predictive Aux-VAE. The model is designed to disentangle global from local information into a dedicated auxiliary variable. Learned representations are analysed with respect to their ability to capture global speech characteristics. We observe that representations of individual speakers are separated well in the latent space and can successfully be used in a subsequent speaker identification task where they achieve high classification accuracy, comparable to a fully supervised model. Moreover, manipulating the global variable allows to change global characteristics while retaining the local content during generation which demonstrates the success of our model to disentangle global from local information",
    "checked": true,
    "id": "bc70d4c7d375986e79875cae2d714ea0521fbd5a",
    "semantic_title": "predictive auxiliary variational autoencoder for representation learning of global speech characteristics",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/paraskevopoulos19_interspeech.html": {
    "title": "Unsupervised Low-Rank Representations for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "We examine the use of linear and non-linear dimensionality reduction algorithms for extracting low-rank feature representations for speech emotion recognition. Two feature sets are used, one based on low-level descriptors and their aggregations (IS10) and one modeling recurrence dynamics of speech (RQA), as well as their fusion. We report speech emotion recognition (SER) results for learned representations on two databases using different classification methods. Classification with low-dimensional representations yields performance improvement in a variety of settings. This indicates that dimensionality reduction is an effective way to combat the curse of dimensionality for SER. Visualization of features in two dimensions provides insight into discriminatory abilities of reduced feature sets",
    "checked": true,
    "id": "eebd7e974f84c49edbee62b1c1c8b2557d254b9e",
    "semantic_title": "unsupervised low-rank representations for speech emotion recognition",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dhiman19_interspeech.html": {
    "title": "On the Suitability of the Riesz Spectro-Temporal Envelope for WaveNet Based Speech Synthesis",
    "volume": "main",
    "abstract": "We address the problem of estimating the time-varying spectral envelope of a speech signal using a spectro-temporal demodulation technique. Unlike the conventional spectrogram, we consider a pitch-adaptive spectrogram and model a spectro-temporal patch using an amplitude- and frequency-modulated two-dimensional (2-D) cosine signal. We employ a demodulation technique based on the Riesz transform that we proposed recently to estimate the amplitude and frequency modulations. The amplitude modulation (AM) corresponds to the vocal-tract filter magnitude response (or envelope) and the frequency modulation (FM) corresponds to the excitation. We consider the AM and demonstrate its effectiveness by incorporating it as an acoustic feature for local conditioning in the statistical WaveNet vocoder for the task of speech synthesis. The quality of the synthesized speech obtained with the Riesz envelope is compared with that obtained using the envelope estimated by the WORLD vocoder. Objective measures and subjective listening tests on the CMU-Arctic database show that the quality of synthesis is superior to that obtained using the WORLD envelope. This study thus establishes the Riesz envelope as an efficient alternative to the WORLD envelope",
    "checked": true,
    "id": "543da25d1d63ac5d607181021d2b2d5ba35b660e",
    "semantic_title": "on the suitability of the riesz spectro-temporal envelope for wavenet based speech synthesis",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xu19_interspeech.html": {
    "title": "Autonomous Emotion Learning in Speech: A View of Zero-Shot Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Conventionally, speech emotion recognition is achieved using passive learning approaches. Differing from such approaches, we herein propose and develop a dynamic method of autonomous emotion learning based on zero-shot learning. The proposed methodology employs emotional dimensions as the attributes in the zero-shot learning paradigm, resulting in two phases of learning, namely attribute learning and label learning. Attribute learning connects the paralinguistic features and attributes utilising speech with known emotional labels, while label learning aims at defining unseen emotions through the attributes. The experimental results achieved on the CINEMO corpus indicate that zero-shot learning is a useful technique for autonomous speech-based emotion learning, achieving accuracies considerably better than chance level and an attribute-based gold-standard setup. Furthermore, different emotion recognition tasks, emotional attributes, and employed approaches strongly influence system performance",
    "checked": true,
    "id": "98960684fc96c0f9c2594bec3b69e59bda567713",
    "semantic_title": "autonomous emotion learning in speech: a view of zero-shot speech emotion recognition",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sudhakara19_interspeech.html": {
    "title": "An Improved Goodness of Pronunciation (GoP) Measure for Pronunciation Evaluation with DNN-HMM System Considering HMM Transition Probabilities",
    "volume": "main",
    "abstract": "Goodness of pronunciation (GoP) is typically formulated with Gaussian mixture model-hidden Markov model (GMM-HMM) based acoustic models considering HMM state transition probabilities (STPs) and GMM likelihoods of context dependent phonemes. On the other hand, deep neural network (DNN)-HMM based acoustic models employed sub-phonemic (senone) posteriors instead of GMM likelihoods along with STPs. However, each senone is shared across many states; thus, there is no one-to-one correspondence between them. In order to circumvent this, most of the existing works have proposed modifications to the GoP formulation considering only posteriors neglecting the STPs. In this work, we derive a formulation for the GoP and it results in the formulation involving both senone posteriors and STPs. Further, we illustrate the steps to implement the proposed GoP formulation in Kaldi, a state-of-the-art automatic speech recognition toolkit. Experiments are conducted on English data collected from Indian speakers using acoustic models trained with native English data from LibriSpeech and Fisher-English corpora. The highest improvement in the correlation coefficient between the scores from the formulations and the expert ratings is found to be 14.89% (relative) better with the proposed approach compared to the best of the existing formulations that don't include STPs",
    "checked": true,
    "id": "aa07cb8a956ba9f7bf7a425997bfcb37a0e4b6cc",
    "semantic_title": "an improved goodness of pronunciation (gop) measure for pronunciation evaluation with dnn-hmm system considering hmm transition probabilities",
    "citation_count": 41
  },
  "https://www.isca-speech.org/archive/interspeech_2019/saha19b_interspeech.html": {
    "title": "Low Resource Automatic Intonation Classification Using Gated Recurrent Unit (GRU) Networks Pre-Trained with Synthesized Pitch Patterns",
    "volume": "main",
    "abstract": "Second language learners of British English (BE) are typically trained to learn four intonation classes — Glide-up, Glide-down, Dive and Take-off. We predict the intonation class in a learner's utterance by modeling the temporal dependencies in the pitch patterns with gated recurrent unit (GRU) networks. For these, we pre-train the GRU network using a set of synthesized pitch patterns representing each intonation class. For the synthesis, we propose to obtain pitch patterns from the tone sequences representing each intonation class obtained from domain knowledge. Experiments are conducted on speech data collected from experts in a spoken English training material for teaching BE intonation. The absolute improvements in the unweighted average recall (UAR) using the proposed scheme with pre-training are found to be 4.14% and 6.01% respectively over the proposed approach without pre-training and the baseline scheme that uses hidden Markov models (HMMs)",
    "checked": true,
    "id": "acee10c113dacb578d603a69f091dcb688886482",
    "semantic_title": "low resource automatic intonation classification using gated recurrent unit (gru) networks pre-trained with synthesized pitch patterns",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vasquezcorrea19b_interspeech.html": {
    "title": "Apkinson: A Mobile Solution for Multimodal Assessment of Patients with Parkinson's Disease",
    "volume": "main",
    "abstract": "Parkinson's disease is a neurological disorder that produces different motor impairments in the patients. The longitudinal assessment of the neurological state of patients is important to improve their quality of life. We introduced Apkinson, a smartphone application to evaluate continuously the speech and movement deficits of Parkinson's patients, who receive feedback about their current state after performing different exercises. The speech assessment considers phonation, articulation, and prosody capabilities of the patients. Movement exercises captured with the inertial sensors of the smartphone evaluated symptoms in the upper and lower limbs",
    "checked": true,
    "id": "dea4695e88ce239222a76605e6db34cd790b5459",
    "semantic_title": "apkinson: a mobile solution for multimodal assessment of patients with parkinson's disease",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kiss19_interspeech.html": {
    "title": "Depression State Assessment: Application for Detection of Depression by Speech",
    "volume": "main",
    "abstract": "We present an application that detects depression by speech based on a speech feature extraction engine. The input of the application is a read speech sample and the output is predicted depression severity level (Beck Depression Inventory). The application analyses the speech sample and evaluates it using support vector regression (SVR). The developed system could assist general medical staff if no specialist is present to aid the diagnosis. If there is a suspicion that the speaker is suffering from depression, it is inevitable to seek special medical assistance. The application supports five native languages: English, French, German, Hungarian and Italian",
    "checked": true,
    "id": "899c31418f8ba698e97bd97a2a113300327942e7",
    "semantic_title": "depression state assessment: application for detection of depression by speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yarra19_interspeech.html": {
    "title": "SPIRE-fluent: A Self-Learning App for Tutoring Oral Fluency to Second Language English Learners",
    "volume": "main",
    "abstract": "Second language (L2) learners often achieve oral fluency by correct pronunciation of words with appropriate pauses. It has been shown that the L2 learners improve their language skills using mobile apps in a self-learning manner. Effective learning is possible with apps that provide detailed feedback. However, apps that train oral fluency in an automatic way are not available. In this work, we present SPIRE-fluent app, which provides an automatic feedback with scores representing learner's pronunciation quality, for each word in a sentence and for the entire sentence. The word specific scores are computed based on the correctness of pronunciation with respect to the expert's audio. Further, the app displays the syllables uttered and a set of two types of pauses produced by the learners and the expert while speaking the sentence. Considering this as a feedback, the learner can correct their mistakes based on the mismatches between those utterances. In addition, it also estimates any pause made by the learners within a word and highlights the syllable containing the phoneme preceding the pause",
    "checked": true,
    "id": "f8798fa1886254d9a549968c9eb0f3b6d0a0ca79",
    "semantic_title": "spire-fluent: a self-learning app for tutoring oral fluency to second language english learners",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nissen19b_interspeech.html": {
    "title": "Using Real-Time Visual Biofeedback for Second Language Instruction",
    "volume": "main",
    "abstract": "This demonstration will illustrate how using real-time visual biofeedback, through a relatively new type of electropalatographic (EPG) sensor, might facilitate improved pronunciation for learners of a second language (L2). The manner in which the EPG sensor is created and its use to track lingua-palatal articulation patterns will be described to individuals. This presentation will also include an explanation of how a student can visualize the contact patterns of their speech using the associated instructional software. A brief tutorial on the features of the instructional software will also be explained during the \"show and tell\" presentation",
    "checked": true,
    "id": "3016e98fff247bcba150ffa3aa326a39ea1dd0f8",
    "semantic_title": "using real-time visual biofeedback for second language instruction",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/miwardelli19_interspeech.html": {
    "title": "Splash: Speech and Language Assessment in Schools and Homes",
    "volume": "main",
    "abstract": "This paper presents a tablet-based app for Speech and Language Assessment in Schools and Homes ( Splash) to provide a first screening for young children aged 4–6 years to assess their speech and language skills. The app aims to be easy-to-administer with an adult, such as a teacher or parent, directing the child through the tasks. Three fun games have been developed to assess receptive language, expressive language and connected speech, respectively. Currently in proof-of-concept mode, when complete Splash will use automatic spoken language processing to give an instant estimate of a child's communication ability and provide guidance on whether to speak specialist support. While not a diagnostic tool, the aim is for Splash to be used to provide immediate reassurance or direction to concerned parents, guardians or teachers as it can be administered by anyone, anywhere",
    "checked": true,
    "id": "f45b3de380ebedd00262246e3c53b136e6891fae",
    "semantic_title": "splash: speech and language assessment in schools and homes",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/annand19_interspeech.html": {
    "title": "Using Ultrasound Imaging to Create Augmented Visual Biofeedback for Articulatory Practice",
    "volume": "main",
    "abstract": "Ultrasound images of the tongue surface can be used to provide real-time visual feedback for clinical practitioners and speakers adjusting pronunciation patterns. However, rapid and complex movements of the tongue can be difficult to interpret and directly relate to desired changes. We are developing a method for simplified visual feedback controlled by efficient, real-time tracking of tongue contours in ultrasound images. Our feedback and control paradigm are briefly discussed, and video of a potential game-like biofeedback stimulus is demonstrated",
    "checked": true,
    "id": "829cfd92b8745b3a69467a6a1327a77fd774e46e",
    "semantic_title": "using ultrasound imaging to create augmented visual biofeedback for articulatory practice",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/radostev19_interspeech.html": {
    "title": "Speech-Based Web Navigation for Limited Mobility Users",
    "volume": "main",
    "abstract": "We present a novel approach that introduces the strengths of voice assistants into a web browser that makes the task of web navigation a lot more accessible to all users, especially under limited mobility circumstances. Voice assistants have now been widely adopted and is providing great user experience for getting simple actions done quickly or getting a quick answer to a question. On the other hand, the benefits of voice assistants have not yet penetrated to the scenarios such as web navigation, which has so far been driven by mouse, keyboard and touch-based input only. In this paper, we demonstrate our speech-based web navigation system, and show that our system improves the completion of the web navigation task on both PC and mobile phone significantly as compared with an out-of-the-box voice assistants on this task",
    "checked": true,
    "id": "6d00006d18a1a9348b08434440e90a9b26c577c8",
    "semantic_title": "speech-based web navigation for limited mobility users",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schultz19_interspeech.html": {
    "title": "Biosignal Processing for Human-Machine Interaction",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "ed97347c1be5df907a2f6a950fead0776830b9d5",
    "semantic_title": "biosignal processing for human-machine interaction",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ryant19_interspeech.html": {
    "title": "The Second DIHARD Diarization Challenge: Dataset, Task, and Baselines",
    "volume": "main",
    "abstract": "This paper introduces the second DIHARD challenge, the second in a series of speaker diarization challenges intended to improve the robustness of diarization systems to variation in recording equipment, noise conditions, and conversational domain. The challenge comprises four tracks evaluating diarization performance under two input conditions (single channel vs. multi-channel) and two segmentation conditions (diarization from a reference speech segmentation vs. diarization from scratch). In order to prevent participants from overtuning to a particular combination of recording conditions and conversational domain, recordings are drawn from a variety of sources ranging from read audiobooks to meeting speech, to child language acquisition recordings, to dinner parties, to web video. We describe the task and metrics, challenge design, datasets, and baseline systems for speech enhancement, speech activity detection, and diarization",
    "checked": true,
    "id": "ff88699c6bac1b289272c445581541ad66848044",
    "semantic_title": "the second dihard diarization challenge: dataset, task, and baselines",
    "citation_count": 147
  },
  "https://www.isca-speech.org/archive/interspeech_2019/singh19_interspeech.html": {
    "title": "LEAP Diarization System for the Second DIHARD Challenge",
    "volume": "main",
    "abstract": "This paper presents the LEAP System, developed for the Second DIHARD diarization Challenge. The evaluation data in the challenge is composed of multi-talker speech in restaurants, doctor-patient conversations, child language acquisition recordings in home environments and audio extracted YouTube videos. The LEAP system is developed using two types of embeddings, one based on i-vector representations and the other one based on x-vector representations. The initial diarization output obtained using agglomerative hierarchical clustering (AHC) done on the probabilistic linear discriminant analysis (PLDA) scores is refined using the Variational-Bayes hidden Markov model (VB-HMM) model. We propose a modified VB-HMM model with posterior scaling which provides significant improvements in the final diarization error rate (DER). We also use a domain compensation on the i-vector features to reduce the mis-match between training and evaluation conditions. N(s)TN(s)TN(s)T Using the proposed approaches, we obtain relative improvements in DER of about 7.1% relative for the best individual system over the DIHARD baseline system and about 13.7% relative for the final system combination on evaluation set. An analysis performed using the proposed posterior scaling method shows that scaling results in improved discrimination among the HMM states in the VB-HMM",
    "checked": true,
    "id": "5bd8938366fcc2cfa6faaf99291873a3a912b650",
    "semantic_title": "leap diarization system for the second dihard challenge",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vinals19_interspeech.html": {
    "title": "ViVoLAB Speaker Diarization System for the DIHARD 2019 Challenge",
    "volume": "main",
    "abstract": "This paper presents the latest improvements in Speaker Diarization obtained by ViVoLAB research group for the 2019 DIHARD Diarization Challenge. This evaluation seeks the improvement of the diarization task in adverse conditions. For this purpose, the audio recordings involve multiple scenarios with no restrictions in terms of speakers, overlapped speech nor quality of the audio. Our submission follows the traditional segmentation-clustering-resegmentation pipeline: Speaker embeddings are extracted from acoustic segments with a single speaker on them, later clustered by means of a PLDA. Our contribution in this work is focused on the clustering step. We present results with our Variational Bayes PLDA clustering and our tree-based clustering strategy, which sequentially assigns the different embeddings to its corresponding speaker according to a PLDA model. Both strategies compare multiple diarization hypotheses and choose their candidate one according to a generative criterion. We also analyze the impact of the different available embeddings in the state-of-the-art with both clustering approaches",
    "checked": true,
    "id": "81e0efe29fe884b81e9d3e956dd31b97ba258d34",
    "semantic_title": "vivolab speaker diarization system for the dihard 2019 challenge",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zajic19_interspeech.html": {
    "title": "UWB-NTIS Speaker Diarization System for the DIHARD II 2019 Challenge",
    "volume": "main",
    "abstract": "In this paper, we present our system developed by the team from the New Technologies for the Information Society (NTIS) research center of the University of West Bohemia in Pilsen, for the Second DIHARD Speech Diarization Challenge. The base of our system follows the currently-standard approach of segmentation, i/x-vector extraction, clustering, and resegmentation. The hyperparameters for each of the subsystems were selected according to the domain classifier trained on the development set of DIHARD II. We compared our system with results from the Kaldi diarization (with i/x-vectors) and combined these systems. At the time of writing of this abstract, our best submission achieved a DER of 23.47% and a JER of 48.99% on the evaluation set (in Track 1 using reference SAD)",
    "checked": true,
    "id": "5b56e4cbb248c94177b01ad993f6742ee7c72ab4",
    "semantic_title": "uwb-ntis speaker diarization system for the dihard ii 2019 challenge",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/park19b_interspeech.html": {
    "title": "The Second DIHARD Challenge: System Description for USC-SAIL Team",
    "volume": "main",
    "abstract": "In this paper, we describe components that form a part of USC-SAIL team's submissions to Track 1 and Track 2 of the second DIHARD speaker diarization challenge. We describe each module in our speaker diarization pipeline and explain the rationale behind our choice of algorithms for each module, while comparing the Diarization Error Rate (DER) against different module combinations. We propose a clustering scheme based on spectral clustering that yields competitive performance. Moreover, we introduce an overlap detection scheme and a re-segmentation system for speaker diarization and investigate their performances using controlled and in-the-wild conditions. In addition, we describe the additional components that will be integrated to our speaker diarization system. To pursue the best performance, we compare our system with the state-of-the-art methods that are presented in the previous challenge and literature. We include preliminary results of our speaker diarization system on the evaluation data from the second DIHARD challenge",
    "checked": true,
    "id": "9a81280f8e8982f39fffdd090d04e3ca5138e6ce",
    "semantic_title": "the second dihard challenge: system description for usc-sail team",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/novoselov19_interspeech.html": {
    "title": "Speaker Diarization with Deep Speaker Embeddings for DIHARD Challenge II",
    "volume": "main",
    "abstract": "This paper describes the ITMO University (DI-IT team) speaker diarization systems submitted to DIHARD Challenge II. As with DIHARD I, this challenge is focused on diarization task for microphone recordings in varying difficult conditions. According to the results of the previous DIHARD I Challenge state-of-the-art diarization systems are based on x-vector embeddings. Such embeddings are clustered using agglomerative hierarchical clustering (AHC) algorithm by means of PLDA scoring. Current research continues the investigation of deep speaker embedding efficiency for the speaker diarization task. This paper explores new types of embedding extractors with different deep neural network architectures and training strategies. We also used AHC to perform embeddings clustering. Alternatively to the PLDA scoring in our AHC procedure we used discriminatively trained cosine similarity metric learning (CSML) model for scoring. Moreover we focused on the optimal AHC threshold tuning according to the specific speech quality. Environment classifier was preliminary trained on development set to predict acoustic conditions for this purpose. We show that such threshold adaptation scheme allows to reduce diarization error rate compared to common AHC threshold for all conditions",
    "checked": true,
    "id": "627a538c1f1a4ebbb778287d80b725fc491e76b6",
    "semantic_title": "speaker diarization with deep speaker embeddings for dihard challenge ii",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/todisco19_interspeech.html": {
    "title": "ASVspoof 2019: Future Horizons in Spoofed and Fake Audio Detection",
    "volume": "main",
    "abstract": "ASVspoof, now in its third edition, is a series of community-led challenges which promote the development of countermeasures to protect automatic speaker verification (ASV) from the threat of spoofing. Advances in the 2019 edition include: (i) a consideration of both logical access (LA) and physical access (PA) scenarios and the three major forms of spoofing attack, namely synthetic, converted and replayed speech; (ii) spoofing attacks generated with state-of-the-art neural acoustic and waveform models; (iii) an improved, controlled simulation of replay attacks; (iv) use of the tandem detection cost function (t-DCF) that reflects the impact of both spoofing and countermeasures upon ASV reliability. Even if ASV remains the core focus, in retaining the equal error rate (EER) as a secondary metric, ASVspoof also embraces the growing importance of fake audio detection. ASVspoof 2019 attracted the participation of 63 research teams, with more than half of these reporting systems that improve upon the performance of two baseline spoofing countermeasures. This paper describes the 2019 database, protocols and challenge results. It also outlines major findings which demonstrate the real progress made in protecting against the threat of spoofing and fake audio",
    "checked": true,
    "id": "396af8a1241d5689a2d730cc76e947f78fe998dd",
    "semantic_title": "asvspoof 2019: future horizons in spoofed and fake audio detection",
    "citation_count": 369
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lai19b_interspeech.html": {
    "title": "ASSERT: Anti-Spoofing with Squeeze-Excitation and Residual Networks",
    "volume": "main",
    "abstract": "We present JHU's system submission to the ASVspoof 2019 Challenge: Anti-Spoofing with Squeeze-Excitation and Residual neTworks (ASSERT). Anti-spoofing has gathered more and more attention since the inauguration of the ASVspoof Challenges, and ASVspoof 2019 dedicates to address attacks from all three major types: text-to-speech, voice conversion, and replay. Built upon previous research work on Deep Neural Network (DNN), ASSERT is a pipeline for DNN-based approach to anti-spoofing. ASSERT has four components: feature engineering, DNN models, network optimization and system combination, where the DNN models are variants of squeeze-excitation and residual networks. We conducted an ablation study of the effectiveness of each component on the ASVspoof 2019 corpus, and experimental results showed that ASSERT obtained more than 93% and 17% relative improvements over the baseline systems in the two sub-challenges in ASVspoof 2019, ranking ASSERT one of the top performing systems. Code and pretrained models are made publicly available",
    "checked": true,
    "id": "48ae745189239c05b41cceccfbecca138e4c2980",
    "semantic_title": "assert: anti-spoofing with squeeze-excitation and residual networks",
    "citation_count": 109
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chettri19_interspeech.html": {
    "title": "Ensemble Models for Spoofing Detection in Automatic Speaker Verification",
    "volume": "main",
    "abstract": "Detecting spoofing attempts of automatic speaker verification (ASV) systems is challenging, especially when using only one modelling approach. For robustness, we use both deep neural networks and traditional machine learning models and combine them as ensemble models through logistic regression. They are trained to detect logical access (LA) and physical access (PA) attacks on the dataset released as part of the ASV Spoofing and Countermeasures Challenge 2019. We propose dataset partitions that ensure different attack types are present during training and validation to improve system robustness. Our ensemble model outperforms all our single models and the baselines from the challenge for both attack types. We investigate why some models on the PA dataset strongly outperform others and find that spoofed recordings in the dataset tend to have longer silences at the end than genuine ones. By removing them, the PA task becomes much more challenging, with the tandem detection cost function (t-DCF) of our best single model rising from 0.1672 to 0.5018 and equal error rate (EER) increasing from 5.98% to 19.8% on the development set",
    "checked": true,
    "id": "bf0cc0febd0e181f1407acf83f7d3104257592cf",
    "semantic_title": "ensemble models for spoofing detection in automatic speaker verification",
    "citation_count": 72
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cai19_interspeech.html": {
    "title": "The DKU Replay Detection System for the ASVspoof 2019 Challenge: On Data Augmentation, Feature Representation, Classification, and Fusion",
    "volume": "main",
    "abstract": "This paper describes our DKU replay detection system for the ASVspoof 2019 challenge. The goal is to develop spoofing countermeasure for automatic speaker recognition in physical access scenario. We leverage the countermeasure system pipeline from four aspects, including the data augmentation, feature representation, classification, and fusion. First, we introduce an utterance-level deep learning framework for anti-spoofing. It receives the variable-length feature sequence and outputs the utterance-level scores directly. Based on the framework, we try out various kinds of input feature representations extracted from either the magnitude spectrum or phase spectrum. Besides, we also perform the data augmentation strategy by applying the speed perturbation on the raw waveform. Our best single system employs a residual neural network trained by the speed-perturbed group delay gram. It achieves EER of 1.04% on the development set, as well as EER of 1.08% on the evaluation set. Finally, using the simple average score from several single systems can further improve the performance. EER of 0.24% on the development set and 0.66% on the evaluation set is obtained for our primary system",
    "checked": true,
    "id": "3f90b660d49447c778dd1b5f115de31ea881e781",
    "semantic_title": "the dku replay detection system for the asvspoof 2019 challenge: on data augmentation, feature representation, classification, and fusion",
    "citation_count": 47
  },
  "https://www.isca-speech.org/archive/interspeech_2019/biaobrzeski19_interspeech.html": {
    "title": "Robust Bayesian and Light Neural Networks for Voice Spoofing Detection",
    "volume": "main",
    "abstract": "We present a replay attack detection system consisting of two convolutional neural network models. The first model consists of a small Bayesian neural network, motivated by the hypothesis that Bayesian models are robust to overfitting. The second one uses a bigger architecture, LCNN, extended with several regularization techniques to improve generalization. Our experiments, considering both size of the networks and use of the Bayesian approach, indicated that smaller networks are sufficient to achieve competitive results. To better estimate the performance against unseen spoofing methods, the final models were selected using novel Attack-Out Cross-Validation. In this procedure each model was tested on a subset of data containing not only previously unseen speakers, but also unseen spoofing attacks. The system was submitted to ASVspoof 2019 challenge's PA condition and achieved a t-DCF score of 0.0219 and EER of 0.88% on the evaluation dataset, which is a 10 times relative improvement over the baseline",
    "checked": true,
    "id": "0820c4caf3f3988496285e12da165efd482e6dbe",
    "semantic_title": "robust bayesian and light neural networks for voice spoofing detection",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lavrentyeva19_interspeech.html": {
    "title": "STC Antispoofing Systems for the ASVspoof2019 Challenge",
    "volume": "main",
    "abstract": "This paper describes the Speech Technology Center (STC) antispoofing systems submitted to the ASVspoof 2019 challenge. The ASVspoof2019 is the extended version of the previous challenges and includes 2 evaluation conditions: logical access use-case scenario with speech synthesis and voice conversion attack types and physical access use-case scenario with replay attacks. During the challenge we developed anti-spoofing solutions for both scenarios. The proposed systems are implemented using deep learning approach and are based on different types of acoustic features. We enhanced Light CNN architecture previously considered by the authors for replay attacks detection and which performed high spoofing detection quality during the ASVspoof2017 challenge. In particular here we investigate the efficiency of angular margin based softmax activation for training robust deep Light CNN classifier to solve the mentioned-above tasks. Submitted systems achieved EER of 1.86% in logical access scenario and 0.54% in physical access scenario on the evaluation part of the Challenge corpora. High performance obtained for the unknown types of spoofing attacks demonstrates the stability of the offered approach in both evaluation conditions",
    "checked": true,
    "id": "12c3c3ef97ed245268a98a7eb273314c59c3c054",
    "semantic_title": "stc antispoofing systems for the asvspoof2019 challenge",
    "citation_count": 165
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yang19b_interspeech.html": {
    "title": "The SJTU Robust Anti-Spoofing System for the ASVspoof 2019 Challenge",
    "volume": "main",
    "abstract": "The robustness of an anti-spoofing system is progressively more important in order to develop a reliable speaker verification system. Previous challenges and datasets mainly focus on a specific type of spoofing attacks. The ASVspoof 2019 edition is the first challenge to address two major spoofing types — logical and physical access. This paper presents the SJTU's submitted anti-spoofing system to the ASVspoof 2019 challenge. Log-CQT features are developed in conjunction with multi-layer convolutional neural networks for robust performance across both subtasks. CNNs with gradient linear units (GLU) activations are utilized for spoofing detection. The proposed system shows consistent performance improvement over all types of spoofing attacks. Our primary submissions achieve the 5 and 8 positions for the logical and physical access respectively. Moreover, our contrastive submission to the PA task exhibits better generalization compared to our primary submission, and achieves a comparable performance to the 3 position of the challenge",
    "checked": true,
    "id": "ace08cdd71a76b95c29cee1a90f671cb5e123235",
    "semantic_title": "the sjtu robust anti-spoofing system for the asvspoof 2019 challenge",
    "citation_count": 37
  },
  "https://www.isca-speech.org/archive/interspeech_2019/alluri19_interspeech.html": {
    "title": "IIIT-H Spoofing Countermeasures for Automatic Speaker Verification Spoofing and Countermeasures Challenge 2019",
    "volume": "main",
    "abstract": "The ASVspoof 2019 challenge focuses on countermeasures for all major spoofing attacks, namely speech synthesis (SS), voice conversion (VC), and replay spoofing attacks. This paper describes the IIIT-H spoofing countermeasures developed for ASVspoof 2019 challenge. In this study, three instantaneous cepstral features namely, single frequency cepstral coefficients, zero time windowing cepstral coefficients, and instantaneous frequency cepstral coefficients are used as front-end features. A Gaussian mixture model is used as back-end classifier. The experimental results on ASVspoof 2019 dataset reveal that the proposed instantaneous features are efficient in detecting VC and SS based attacks. In detecting replay attacks, proposed features are comparable with baseline systems. Further analysis is carried out using metadata to assess the impact of proposed countermeasures on different synthetic speech generating algorithm/replay configurations",
    "checked": true,
    "id": "6c3d696a643d1791a88b87cf58ff8a90a5439b93",
    "semantic_title": "iiit-h spoofing countermeasures for automatic speaker verification spoofing and countermeasures challenge 2019",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19c_interspeech.html": {
    "title": "Anti-Spoofing Speaker Verification System with Multi-Feature Integration and Multi-Task Learning",
    "volume": "main",
    "abstract": "Speaker anti-spoofing is crucial to prevent security breaches when the speaker verification systems encounter the spoofed attacks from the advanced speech synthesis algorithms and high fidelity replay devices. In this paper, we propose a framework based on multiple features integration and multi-task learning (MFMT) for improving anti-spoofing performance. It is important to integrate the complementary information of multiple spectral features within the network, such as MFCC, CQCC, Fbank, etc., as often a single kind of feature is not enough to grasp the global spoofing cues and it generalizes poorly. Furthermore, we propose a helpful butterfly unit (BU) for multi-task learning to propagate the shared representations between the binary decision task and the other auxiliary task. The BU can obtain task representations of other branch during forward propagation and prevent the gradient from assimilating the branch during back propagation. Our proposed system yielded an EER of 9.01% on ASVspoof 2017, while the best single system and the average scores fusion obtained the evaluation EER of 2.39% and 0.96% on ASVspoof 2019 PA, respectively",
    "checked": true,
    "id": "c6657d083b4343144e51f1ec70b72f117b2e8f9b",
    "semantic_title": "anti-spoofing speaker verification system with multi-feature integration and multi-task learning",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/williams19b_interspeech.html": {
    "title": "Speech Replay Detection with x-Vector Attack Embeddings and Spectral Features",
    "volume": "main",
    "abstract": "We present our system submission to the ASVspoof 2019 Challenge Physical Access (PA) task. The objective for this challenge was to develop a countermeasure that identifies speech audio as either bona fide or intercepted and replayed. The target prediction was a value indicating that a speech segment was bona fide (positive values) or \"spoofed\" (negative values). Our system used convolutional neural networks (CNNs) and a representation of the speech audio that combined x-vector attack embeddings with signal processing features. The x-vector attack embeddings were created from mel-frequency cepstral coefficients (MFCCs) using a time-delay neural network (TDNN). These embeddings jointly modeled 27 different environments and 9 types of attacks from the labeled data. We also used sub-band spectral centroid magnitude coefficients (SCMCs) as features. We included an additive Gaussian noise layer during training as a way to augment the data to make our system more robust to previously unseen attack examples. We report system performance using the tandem detection cost function (tDCF) and equal error rate (EER). Our approach performed better that both of the challenge baselines. Our technique suggests that our x-vector attack embeddings can help regularize the CNN predictions even when environments or attacks are more challenging",
    "checked": true,
    "id": "87d2ff88109add62b4b3fa82b0e13a2e77827e8e",
    "semantic_title": "speech replay detection with x-vector attack embeddings and spectral features",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2019/das19_interspeech.html": {
    "title": "Long Range Acoustic Features for Spoofed Speech Detection",
    "volume": "main",
    "abstract": "Speaker verification systems in practice are vulnerable to spoofing attacks. The high quality recording and playback devices make replay attack a real threat to speaker verification. Additionally, the furtherance in voice conversion and speech synthesis has produced perceptually natural sounding speech. The ASVspoof 2019 challenge is organized to study the robustness of countermeasures against such attacks, which cover two common modes of attacks, logical and physical access. The former deals with synthetic attacks arising from voice conversion and text-to-speech techniques, whereas the latter deals with replay attacks. In this work, we explore several novel countermeasures based on long range acoustic features that are found to be effective for spoofing attack detection. The long range features capture different aspects of long range information as they are computed from subbands and octave power spectrum in contrast to the conventional way from linear power spectrum. These novel features are combined with the other known features for improved detection of spoofing attacks. We obtain a tandem detection cost function of 0.1264 and 0.1381 (equal error rate 4.13% and 5.95%) for logical and physical access on the best combined system submitted to the challenge",
    "checked": true,
    "id": "85b5ef05d49c16d78dcbfbc5f65118b272de46ba",
    "semantic_title": "long range acoustic features for spoofed speech detection",
    "citation_count": 33
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chang19b_interspeech.html": {
    "title": "Transfer-Representation Learning for Detecting Spoofing Attacks with Converted and Synthesized Speech in Automatic Speaker Verification System",
    "volume": "main",
    "abstract": "In this paper, we study a countermeasure module to detect spoofing attacks with converted or synthesized speech in tandem automatic speaker verification (ASV). Our approach integrates representation learning and transfer learning methods. For representation learning, good embedding network functions are learned from audio signals with the goal to distinguish different types of spoofing attacks. For transfer learning, the embedding network functions are used to initialize fine-tuning networks. We experiment well-known neural network architectures and front-end raw features to diversify and strengthen the information source for embedding. We participate in the 2019 Automatic Speaker Verification Spoofing and Countermeasures Challenge (ASVspoof 2019) and evaluate the proposed methods with the logical access condition tasks for detecting converted speech and synthesized speech. On the ASVspoof 2019 development set, our best single system achieves a minimum tandem decision cost function of nearly 0 during system development. On the ASVspoof 2019 evaluation set, our primary system achieves a minimum tandem decision cost of 0.1791, and an equal error rate (EER) of 9.08%. Our system does not have over-training issue as it achieves decent performance with unseen test data of the types presented in training, yet the generalization gap is not small with mismatched test data types",
    "checked": true,
    "id": "370307aecb7d0c627014159cf852346db6a225f7",
    "semantic_title": "transfer-representation learning for detecting spoofing attacks with converted and synthesized speech in automatic speaker verification system",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gomezalanis19_interspeech.html": {
    "title": "A Light Convolutional GRU-RNN Deep Feature Extractor for ASV Spoofing Detection",
    "volume": "main",
    "abstract": "The aim of this work is to develop a single anti-spoofing system which can be applied to effectively detect all the types of spoofing attacks considered in the ASVspoof 2019 Challenge: text-to-speech, voice conversion and replay based attacks. To achieve this, we propose the use of a Light Convolutional Gated Recurrent Neural Network (LC-GRNN) as a deep feature extractor to robustly represent speech signals as utterance-level embeddings, which are later used by a back-end recognizer which performs the final genuine/spoofed classification. This novel architecture combines the ability of light convolutional layers for extracting discriminative features at frame level with the capacity of gated recurrent unit based RNNs for learning long-term dependencies of the subsequent deep features. The proposed system has been presented as a contribution to the ASVspoof 2019 Challenge, and the results show a significant improvement in comparison with the baseline systems. Moreover, experiments were also carried out on the ASVspoof 2015 and 2017 corpora, and the results indicate that our proposal clearly outperforms other popular methods recently proposed and other similar deep feature based systems",
    "checked": true,
    "id": "a692a7c971238a24b2ae882a1b6925946ea5e498",
    "semantic_title": "a light convolutional gru-rnn deep feature extractor for asv spoofing detection",
    "citation_count": 54
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zeinali19_interspeech.html": {
    "title": "Detecting Spoofing Attacks Using VGG and SincNet: BUT-Omilia Submission to ASVspoof 2019 Challenge",
    "volume": "main",
    "abstract": "In this paper, we present the system description of the joint efforts of Brno University of Technology (BUT) and Omilia — Conversational Intelligence for the ASVSpoof2019 Spoofing and Countermeasures Challenge. The primary submission for Physical access (PA) is a fusion of two VGG networks, trained on single and two-channels features. For Logical access (LA), our primary system is a fusion of VGG and the recently introduced SincNet architecture. The results on PA show that the proposed networks yield very competitive performance in all conditions and achieved 86% relative improvement compared to the official baseline. On the other hand, the results on LA showed that although the proposed architecture and training strategy performs very well on certain spoofing attacks, it fails to generalize to certain attacks that are unseen during training",
    "checked": true,
    "id": "de0a7b8fbb3edef9114640889086f63415c9a25a",
    "semantic_title": "detecting spoofing attacks using vgg and sincnet: but-omilia submission to asvspoof 2019 challenge",
    "citation_count": 49
  },
  "https://www.isca-speech.org/archive/interspeech_2019/alzantot19_interspeech.html": {
    "title": "Deep Residual Neural Networks for Audio Spoofing Detection",
    "volume": "main",
    "abstract": "The state-of-art models for speech synthesis and voice conversion are capable of generating synthetic speech that is perceptually indistinguishable from bonafide human speech. These methods represent a threat to the automatic speaker verification (ASV) systems. Additionally, replay attacks where the attacker uses a speaker to replay a previously recorded genuine human speech are also possible. In this paper, we present our solution for the ASVSpoof2019 competition, which aims to develop countermeasure systems that distinguish between spoofing attacks and genuine speeches. Our model is inspired by the success of residual convolutional networks in many classification tasks. We build three variants of a residual convolutional neural network that accept different feature representations (MFCC, log-magnitude STFT, and CQCC) of input. We compare the performance achieved by our model variants and the competition baseline models. In the logical access scenario, the fusion of our models has zero t-DCF cost and zero equal error rate (EER), as evaluated on the development set. On the evaluation set, our model fusion improves the t-DCF and EER by 25% compared to the baseline algorithms. Against physical access replay attacks, our model fusion improves the baseline algorithms t-DCF and EER scores by 71% and 75% on the evaluation set, respectively",
    "checked": true,
    "id": "deb8ee23745d3f11b36321324ce2bf1046459b49",
    "semantic_title": "deep residual neural networks for audio spoofing detection",
    "citation_count": 88
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jung19_interspeech.html": {
    "title": "Replay Attack Detection with Complementary High-Resolution Information Using End-to-End DNN for the ASVspoof 2019 Challenge",
    "volume": "main",
    "abstract": "In this study, we concentrate on replacing the process of extracting hand-crafted acoustic feature with end-to-end DNN using complementary high-resolution spectrograms. As a result of advance in audio devices, typical characteristics of a replayed speech based on conventional knowledge alter or diminish in unknown replay configurations. Thus, it has become increasingly difficult to detect spoofed speech with a conventional knowledge-based approach. To detect unrevealed characteristics that reside in a replayed speech, we directly input spectrograms into an end-to-end DNN without knowledge-based intervention. Explorations dealt in this study that differentiates from existing spectrogram-based systems are twofold: complementary information and high-resolution. Spectrograms with different information are explored, and it is shown that additional information such as the phase information can be complementary. High-resolution spectrograms are employed with the assumption that the difference between a bona-fide and a replayed speech exists in the details. Additionally, to verify whether other features are complementary to spectrograms, we also examine raw waveform and an i-vector based system. Experiments conducted on the ASVspoof 2019 physical access challenge show promising results, where t-DCF and equal error rates are 0.0570 and 2.45% for the evaluation set, respectively",
    "checked": true,
    "id": "d53771e89b31c9663f29a5231b57ef2ed9aa17ec",
    "semantic_title": "replay attack detection with complementary high-resolution information using end-to-end dnn for the asvspoof 2019 challenge",
    "citation_count": 39
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dunbar19_interspeech.html": {
    "title": "The Zero Resource Speech Challenge 2019: TTS Without T",
    "volume": "main",
    "abstract": "We present the Zero Resource Speech Challenge 2019, which proposes to build a speech synthesizer without any text or phonetic labels: hence, TTS without T (text-to-speech without text). We provide raw audio for a target voice in an unknown language (the Voice dataset), but no alignment, text or labels. Participants must discover subword units in an unsupervised way (using the Unit Discovery dataset) and align them to the voice recordings in a way that works best for the purpose of synthesizing novel utterances from novel speakers, similar to the target speaker's voice. We describe the metrics used for evaluation, a baseline system consisting of unsupervised subword unit discovery plus a standard TTS system, and a topline TTS using gold phoneme transcriptions. We present an overview of the 19 submitted systems from 10 teams and discuss the main results",
    "checked": true,
    "id": "fd80c74299a0ba9ea6b582e1ad8397e5e2db82ed",
    "semantic_title": "the zero resource speech challenge 2019: tts without t",
    "citation_count": 106
  },
  "https://www.isca-speech.org/archive/interspeech_2019/feng19b_interspeech.html": {
    "title": "Combining Adversarial Training and Disentangled Speech Representation for Robust Zero-Resource Subword Modeling",
    "volume": "main",
    "abstract": "This study addresses the problem of unsupervised subword unit discovery from untranscribed speech. It forms the basis of the ultimate goal of ZeroSpeech 2019, building text-to-speech systems without text labels. In this work, unit discovery is formulated as a pipeline of phonetically discriminative feature learning and unit inference. One major difficulty in robust unsupervised feature learning is dealing with speaker variation. Here the robustness towards speaker variation is achieved by applying adversarial training and FHVAE based disentangled speech representation learning. A comparison of the two approaches as well as their combination is studied in a DNN-bottleneck feature (DNN-BNF) architecture. Experiments are conducted on ZeroSpeech 2019 and 2017. Experimental results on ZeroSpeech 2017 show that both approaches are effective while the latter is more prominent, and that their combination brings further marginal improvement in across-speaker condition. Results on ZeroSpeech 2019 show that in the ABX discriminability task, our approaches significantly outperform the official baseline, and are competitive to or even outperform the official topline. The proposed unit sequence smoothing algorithm improves synthesis quality, at a cost of slight decrease in ABX discriminability",
    "checked": true,
    "id": "6c67f7439b246cf2866cd89824aa2fdc90107a52",
    "semantic_title": "combining adversarial training and disentangled speech representation for robust zero-resource subword modeling",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yusuf19_interspeech.html": {
    "title": "Temporally-Aware Acoustic Unit Discovery for Zerospeech 2019 Challenge",
    "volume": "main",
    "abstract": "Zero-resource speech processing efforts focus on unsupervised discovery of sub-word acoustic units. Common approaches work with spatial similarities between the acoustic frame representations within Bayesian or neural network-based frameworks. We propose two methods that utilize the temporal proximity information in addition to the acoustic similarity for clustering frames into acoustic units. The first approach uses a temporally biased self-organizing map (SOM) to discover such units. Since the SOM unit indices are correlated with (vector) spatial distance, we pool neighboring units and then train a recurrent neural network to predict each pooled unit. The second approach incorporates temporal awareness by training a recurrent sparse autoencoder, in which unsupervised clustering is done on the intermediate softmax layer. This network is then fine-tuned using aligned pairs of acoustically similar sequences obtained via unsupervised term discovery. Our approaches outperform the provided baseline system on two main metrics of the Zerospeech 2019 challenge, ABX-discriminability and bitrate of the quantized embeddings, both for English and the surprise language. Furthermore, the temporal-awareness and the post-filtering techniques adopted in this work resulted in an enhanced continuity of the decoding, yielding low bitrates",
    "checked": true,
    "id": "10ac90538548a3ac23c524e50854a34ee01c4aaa",
    "semantic_title": "temporally-aware acoustic unit discovery for zerospeech 2019 challenge",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/eloff19_interspeech.html": {
    "title": "Unsupervised Acoustic Unit Discovery for Speech Synthesis Using Discrete Latent-Variable Neural Networks",
    "volume": "main",
    "abstract": "For our submission to the ZeroSpeech 2019 challenge, we apply discrete latent-variable neural networks to unlabelled speech and use the discovered units for speech synthesis. Unsupervised discrete subword modelling could be useful for studies of phonetic category learning in infants or in low-resource speech technology requiring symbolic input. We use an autoencoder (AE) architecture with intermediate discretisation. We decouple acoustic unit discovery from speaker modelling by conditioning the AE's decoder on the training speaker identity. At test time, unit discovery is performed on speech from an unseen speaker, followed by unit decoding conditioned on a known target speaker to obtain reconstructed filterbanks. This output is fed to a neural vocoder to synthesise speech in the target speaker's voice. For discretisation, categorical variational autoencoders (CatVAEs), vector-quantised VAEs (VQ-VAEs) and straight-through estimation are compared at different compression levels on two languages. Our final model uses convolutional encoding, VQ-VAE discretisation, deconvolutional decoding and an FFTNet vocoder. We show that decoupled speaker conditioning intrinsically improves discrete acoustic representations, yielding competitive synthesis quality compared to the challenge baseline",
    "checked": true,
    "id": "2130cb5ddde3efe79ace0246203c4e81ad495809",
    "semantic_title": "unsupervised acoustic unit discovery for speech synthesis using discrete latent-variable neural networks",
    "citation_count": 52
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19c_interspeech.html": {
    "title": "Unsupervised End-to-End Learning of Discrete Linguistic Units for Voice Conversion",
    "volume": "main",
    "abstract": "We present an unsupervised end-to-end training scheme where we discover discrete subword units from speech without using any labels. The discrete subword units are learned under an ASR-TTS autoencoder reconstruction setting, where an ASR-Encoder is trained to discover a set of common linguistic units given a variety of speakers, and a TTS-Decoder trained to project the discovered units back to the designated speech. We propose a discrete encoding method, Multilabel-Binary Vectors (MBV), to make the ASR-TTS autoencoder differentiable. We found that the proposed encoding method offers automatic extraction of speech content from speaker style, and is sufficient to cover full linguistic content in a given language. Therefore, the TTS-Decoder can synthesize speech with the same content as the input of ASR-Encoder but with different speaker characteristics, which achieves voice conversion (VC). We further improve the quality of VC using adversarial training, where we train a TTS-Patcher that augments the output of TTS-Decoder. Objective and subjective evaluations show that the proposed approach offers strong VC results as it eliminates speaker identity while preserving content within speech. In the ZeroSpeech 2019 Challenge, we achieved outstanding performance in terms of low bitrate",
    "checked": true,
    "id": "add193d1d6a70bc50768702daa020d45d4b78f9f",
    "semantic_title": "unsupervised end-to-end learning of discrete linguistic units for voice conversion",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2019/s19_interspeech.html": {
    "title": "Zero Resource Speech Synthesis Using Transcripts Derived from Perceptual Acoustic Units",
    "volume": "main",
    "abstract": "Zerospeech synthesis is the task of building vocabulary independent speech synthesis systems, where transcriptions are unavailable for training data. It is, therefore, necessary to convert training data into a sequence of fundamental acoustic units that can be used for synthesis during the test. This paper attempts to discover, and model perceptual acoustic units consisting of steady state, and transient regions in speech. The transients roughly correspond to CV, VC units, while the steady-state corresponds to sonorants and fricatives. The speech signal is first preprocessed by segmenting the same into CVC-like units using a short-term energy-like contour. These CVC segments are clustered using a connected components-based graph clustering technique. The clustered CVC segments are initialized such that the onset (CV) and decays (VC) correspond to transients, and the rhyme corresponds to steady-states. Following this initialization, the units are allowed to re-organise on the continuous speech into a final set of AUs in an HMM-GMM framework. AU sequences thus obtained are used to train synthesis models. The performance of the proposed approach is evaluated on the Zerospeech 2019 challenge database. Subjective and objective scores show that reasonably good quality synthesis with low bit rate encoding can be achieved using the proposed AUs",
    "checked": true,
    "id": "7fe5fab34291f96008e908ca7942c3141851a00b",
    "semantic_title": "zero resource speech synthesis using transcripts derived from perceptual acoustic units",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tjandra19_interspeech.html": {
    "title": "VQVAE Unsupervised Unit Discovery and Multi-Scale Code2Spec Inverter for Zerospeech Challenge 2019",
    "volume": "main",
    "abstract": "We describe our submitted system for the ZeroSpeech Challenge 2019. The current challenge theme addresses the difficulty of constructing a speech synthesizer without any text or phonetic labels and requires a system that can (1) discover subword units in an unsupervised way, and (2) synthesize the speech with a target speaker's voice. Moreover, the system should also balance the discrimination score ABX, the bit-rate compression rate, and the naturalness and the intelligibility of the constructed voice. To tackle these problems and achieve the best trade-off, we utilize a vector quantized variational autoencoder (VQ-VAE) and a multi-scale codebook-to-spectrogram (Code2Spec) inverter trained by mean square error and adversarial loss. The VQ-VAE extracts the speech to a latent space, forces itself to map it into the nearest codebook and produces compressed representation. Next, the inverter generates a magnitude spectrogram to the target voice, given the codebook vectors from VQ-VAE. In our experiments, we also investigated several other clustering algorithms, including K-Means and GMM, and compared them with the VQ-VAE result on ABX scores and bit rates. Our proposed approach significantly improved the intelligibility (in CER), the MOS, and discrimination ABX scores compared to the official ZeroSpeech 2019 baseline or even the topline",
    "checked": true,
    "id": "50970f392a76ced3054703a21d581377f1cc1086",
    "semantic_title": "vqvae unsupervised unit discovery and multi-scale code2spec inverter for zerospeech challenge 2019",
    "citation_count": 60
  },
  "https://www.isca-speech.org/archive/interspeech_2019/niehues19_interspeech.html": {
    "title": "Survey Talk: A Survey on Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "d1a624b3dd6413cb3279230a5ad4ca75feb218ff",
    "semantic_title": "survey talk: a survey on speech translation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jia19_interspeech.html": {
    "title": "Direct Speech-to-Speech Translation with a Sequence-to-Sequence Model",
    "volume": "main",
    "abstract": "We present an attention-based sequence-to-sequence neural network which can directly translate speech from one language into speech in another language, without relying on an intermediate text representation. The network is trained end-to-end, learning to map speech spectrograms into target spectrograms in another language, corresponding to the translated content (in a different canonical voice). We further demonstrate the ability to synthesize translated speech using the voice of the source speaker. We conduct experiments on two Spanish-to-English speech translation datasets, and find that the proposed model slightly underperforms a baseline cascade of a direct speech-to-text translation model and a text-to-speech synthesis model, demonstrating the feasibility of the approach on this very challenging task",
    "checked": true,
    "id": "f93b523fe44e2ad2deaa1c6dac2da878a17eeeaf",
    "semantic_title": "direct speech-to-speech translation with a sequence-to-sequence model",
    "citation_count": 154
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19d_interspeech.html": {
    "title": "End-to-End Speech Translation with Knowledge Distillation",
    "volume": "main",
    "abstract": "End-to-end speech translation (ST), which directly translates from source language speech into target language text, has attracted intensive attentions in recent years. Compared to conventional pipeline systems, end-to-end ST model has potential benefits of lower latency, smaller model size and less error propagation. However, it is notoriously difficult to implement such model which combines automatic speech recognition (ASR) and machine translation (MT) together. In this paper, we propose a knowledge distillation approach to improve ST by transferring the knowledge from text translation. Specifically, we first train a text translation model, regarded as the teacher model, and then ST model is trained to learn the output probabilities of teacher model through knowledge distillation. Experiments on English-French Augmented LibriSpeech and English-Chinese TED corpus show that end-to-end ST is possible to implement on both similar and dissimilar language pairs. In addition, with the instruction of the teacher model, end-to-end ST model can gain significant improvements by over 3.5 BLEU points",
    "checked": true,
    "id": "e4d99f390901df5caac0b587ff685f9cde100342",
    "semantic_title": "end-to-end speech translation with knowledge distillation",
    "citation_count": 133
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gangi19_interspeech.html": {
    "title": "Adapting Transformer to End-to-End Spoken Language Translation",
    "volume": "main",
    "abstract": "Neural end-to-end architectures for sequence-to-sequence learning represent the state of the art in machine translation (MT) and speech recognition (ASR). Their use is also promising for end-to-end spoken language translation (SLT), which combines the main challenges of ASR and MT. Exploiting existing neural architectures, however, requires task-specific adaptations. A network that has obtained state-of-the-art results in MT with reduced training time is Transformer. However, its direct application to speech input is hindered by two limitations of the self-attention network on which it is based: quadratic memory complexity and no explicit modeling of short-range dependencies between input features. High memory complexity poses constraints to the size of models trainable with a GPU, while the inadequate modeling of local dependencies harms final translation quality. This paper presents an adaptation of Transformer to end-to-end SLT that consists in: i) downsampling the input with convolutional neural networks to make the training process feasible on GPUs, ii) modeling the bidimensional nature of a spectrogram, and iii) adding a distance penalty to the attention, so to bias it towards local context. SLT experiments on 8 language directions show that, with our adaptation, Transformer outperforms a strong RNN-based baseline with a significant reduction in training time",
    "checked": true,
    "id": "d0a313a557bd43a7cacb3e5479cd7c491f7faa5c",
    "semantic_title": "adapting transformer to end-to-end spoken language translation",
    "citation_count": 105
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hillis19_interspeech.html": {
    "title": "Unsupervised Phonetic and Word Level Discovery for Speech to Speech Translation for Unwritten Languages",
    "volume": "main",
    "abstract": "We experiment with unsupervised methods for deriving and clustering symbolic representations of speech, working towards speech-to-speech translation for languages without regular (or any) written representations. We consider five low-resource African languages, and we produce three different segmental representations of text data for comparisons against four different segmental representations derived solely from acoustic data for each language. The text and speech data for each language comes from the CMU Wilderness dataset introduced in [1], where speakers read a version of the New Testament in their language. Our goal is to evaluate the translation performance not only of acoustically derived units but also of discovered sequences or \"words\" made from these units, with the intuition that such representations will encode more meaning than phones alone. We train statistical machine translation models for each representation and evaluate their outputs on the basis of BLEU-1 scores to determine their efficacy. Our experiments produce encouraging results: as we cluster our atomic phonetic representations into more word-like units, the amount information retained generally approaches that of the actual words themselves",
    "checked": true,
    "id": "2b34391dc7ca13be1400ac205c95e4aed7325425",
    "semantic_title": "unsupervised phonetic and word level discovery for speech to speech translation for unwritten languages",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bhattacharya19_interspeech.html": {
    "title": "Deep Speaker Recognition: Modular or Monolithic?",
    "volume": "main",
    "abstract": "Speaker recognition has made extraordinary progress with the advent of deep neural networks. In this work, we analyze the performance of end-to-end deep speaker recognizers on two popular text-independent tasks - NIST-SRE 2016 and VoxCeleb. Through a combination of a deep convolutional feature extractor, self-attentive pooling and large-margin loss functions, we achieve state-of-the-art performance on VoxCeleb. Our best individual and ensemble models show a relative improvement of 70% an 82% respectively over the best reported results on this task On the challenging NIST-SRE 2016 task, our proposed end-to-end models show good performance but are unable to match a strong i-vector baseline. State-of-the-art systems for this task use a modular framework that combines neural network embeddings with a probabilistic linear discriminant analysis (PLDA) classifier. Drawing inspiration from this approach we propose to replace the PLDA classifier with a neural network. Our modular neural network approach is able to outperform the i-vector baseline using cosine distance to score verification trials",
    "checked": true,
    "id": "4ebb8e13b68c1676d197b0ebde7c5d63fbfcd11a",
    "semantic_title": "deep speaker recognition: modular or monolithic?",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19d_interspeech.html": {
    "title": "On the Usage of Phonetic Information for Text-Independent Speaker Embedding Extraction",
    "volume": "main",
    "abstract": "Embeddings extracted by deep neural networks have become the state-of-the-art utterance representation in speaker recognition systems. It has recently been shown that incorporating frame-level phonetic information in the embedding extractor can improve the speaker recognition performance. On the other hand, in the final embedding, phonetic information is just an additional source of session variability which may be harmful to the text-independent speaker recognition task. This suggests that at the embedding level phonetic information should be suppressed rather than encouraged. To verify this hypothesis, we perform several experiments that encourage or/and suppress phonetic information at various stages in the network. Our experiments confirm that multitask learning is beneficial if it is applied at the frame-level stage of the network, whereas adversarial training is beneficial if it is used at the segment-level stage of the network. Additionally, the combination of these two approaches improves the performance further, resulting in an equal error rate of 3.17% on the VoxCeleb dataset",
    "checked": true,
    "id": "984263114a79a53699e26e431bf0ef545b47a42c",
    "semantic_title": "on the usage of phonetic information for text-independent speaker embedding extraction",
    "citation_count": 45
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ravanelli19_interspeech.html": {
    "title": "Learning Speaker Representations with Mutual Information",
    "volume": "main",
    "abstract": "Learning good representations is of crucial importance in deep learning. Mutual Information (MI) or similar measures of statistical dependence are promising tools for learning these representations in an unsupervised way. Even though the mutual information between two random variables is hard to measure directly in high dimensional spaces, some recent studies have shown that an implicit optimization of MI can be achieved with an encoder-discriminator architecture similar to that of Generative Adversarial Networks (GANs) In this work, we learn representations that capture speaker identities by maximizing the mutual information between the encoded representations of chunks of speech randomly sampled from the same sentence. The proposed encoder relies on the SincNet architecture and transforms raw speech waveform into a compact feature vector. The discriminator is fed by either positive samples (of the joint distribution of encoded chunks) or negative samples (from the product of the marginals) and is trained to separate them We report experiments showing that this approach effectively learns useful speaker representations, leading to promising results on speaker identification and verification tasks. Our experiments consider both unsupervised and semi-supervised settings and compare the performance achieved with different objective functions",
    "checked": true,
    "id": "a5b94d6be7cd206d88218ba3a3ea4f3c3e8522de",
    "semantic_title": "learning speaker representations with mutual information",
    "citation_count": 75
  },
  "https://www.isca-speech.org/archive/interspeech_2019/you19_interspeech.html": {
    "title": "Multi-Task Learning with High-Order Statistics for x-Vector Based Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "The x-vector based deep neural network (DNN) embedding systems have demonstrated effectiveness for text-independent speaker verification. This paper presents a multi-task learning architecture for training the speaker embedding DNN with the primary task of classifying the target speakers, and the auxiliary task of reconstructing the first- and higher-order statistics of the original input utterance. The proposed training strategy aggregates both the supervised and unsupervised learning into one framework to make the speaker embeddings more discriminative and robust. Experiments are carried out using the NIST SRE16 evaluation dataset and the VOiCES dataset. The results demonstrate that our proposed method outperforms the original x-vector approach with very low additional complexity added",
    "checked": true,
    "id": "70cfbf82cee2f007b3c65cf97373f66f13005b42",
    "semantic_title": "multi-task learning with high-order statistics for x-vector based text-independent speaker verification",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wu19e_interspeech.html": {
    "title": "Data Augmentation Using Variational Autoencoder for Embedding Based Speaker Verification",
    "volume": "main",
    "abstract": "Domain or environment mismatch between training and testing, such as various noises and channels, is a major challenge for speaker verification. In this paper, a variational autoencoder (VAE) is designed to learn the patterns of speaker embeddings extracted from noisy speech segments, including i-vector and x-vector, and generate embeddings with more diversity to improve the robustness of speaker verification systems with probabilistic linear discriminant analysis (PLDA) back-end. The approach is evaluated on the standard NIST SRE 2016 dataset. Compared to manual and generative adversarial network (GAN) based augmentation approaches, the proposed VAE based augmentation achieves a slightly better performance for i-vector on Tagalog and Cantonese with EERs of 15.54% and 7.84%, and a more significant improvement for x-vector on those two languages with EERs of 11.86% and 4.20%",
    "checked": true,
    "id": "c85dd2837138fb1f91758e964b82bc3d7eba659c",
    "semantic_title": "data augmentation using variational autoencoder for embedding based speaker verification",
    "citation_count": 27
  },
  "https://www.isca-speech.org/archive/interspeech_2019/you19b_interspeech.html": {
    "title": "Deep Neural Network Embeddings with Gating Mechanisms for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "In this paper, gating mechanisms are applied in deep neural network (DNN) training for x-vector-based text-independent speaker verification. First, a gated convolution neural network (GCNN) is employed for modeling the frame-level embedding layers. Compared with the time-delay DNN (TDNN), the GCNN can obtain more expressive frame-level representations through carefully designed memory cell and gating mechanisms. Moreover, we propose a novel gated-attention statistics pooling strategy in which the attention scores are shared with the output gate. The gated-attention statistics pooling combines both gating and attention mechanisms into one framework; therefore, we can capture more useful information in the temporal pooling layer. Experiments are carried out using the NIST SRE16 and SRE18 evaluation datasets. The results demonstrate the effectiveness of the GCNN and show that the proposed gated-attention statistics pooling can further improve the performance",
    "checked": true,
    "id": "9f5f12800d0678362d009544b97c7b7c823782f1",
    "semantic_title": "deep neural network embeddings with gating mechanisms for text-independent speaker verification",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bhat19_interspeech.html": {
    "title": "Neural Transition Systems for Modeling Hierarchical Semantic Representations",
    "volume": "main",
    "abstract": "While virtual agents are becoming ubiquitous in our daily life, their functionality is limited to simple commands which involve a single intent and an unstructured set of entities. Typically, in such systems, the natural language understanding (NLU) component uses a sequence tagging model to extract a flat meaning representation. However, in order to support complex user requests with multiple intents with their associated entities, such as those in a product ordering domain, a structured semantic representation is necessary. In this paper, we present hierarchical semantic representations for product ordering in the food services domain and two NLU models that produce such representations efficiently using deep neural networks. The models are based on transition-based algorithms which have been proven to be effective and scalable for multiple NLP tasks such as syntactic parsing and slot filling. The first model uses a multitasking architecture containing multiple transition systems with tree constraints to model the hierarchical annotations, while the second model treats the task as a constituency parsing problem by mapping the target domain annotations to a constituency tree. We demonstrate that both multi-task and constituency-based transition systems achieve competitive results and even show improvements over sequential models, showing their effectiveness in modeling hierarchical structure",
    "checked": true,
    "id": "1518edb6297ce3a89c660a75efef63ac685855d4",
    "semantic_title": "neural transition systems for modeling hierarchical semantic representations",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vukotic19_interspeech.html": {
    "title": "Mining Polysemous Triplets with Recurrent Neural Networks for Spoken Language Understanding",
    "volume": "main",
    "abstract": "The typical RNN (Recurrent Neural Network) pipeline in SLU (Spoken Language Understanding), and specifically in the slot-filling task, consists of three stages: word embedding, context window representation, and label prediction. Label prediction, as a classification task, is the one that creates a sensible context window representation during learning through back-propagation. However, due to natural variations of the data, differences in two same-labeled samples can lead to dissimilar representations, whereas similarities in two differently-labeled samples can lead to them having close representations. In computer vision applications, specifically in face recognition and person re-identification, this problem has recently been successfully tackled by introducing data triplets and a triplet loss function In SLU, each word can be mapped to one or multiple labels depending on small variations of its context. We exploit this fact to construct data triplets consisting of the same words with different contexts that form a pair of datapoints with matching target labels and an another pair with non-matching labels. By using these triplets and an additional loss function, we update the context window representation in order to improve it, make dissimilar samples more distant and similar samples closer, leading to better classification results and an improved rate of convergence",
    "checked": true,
    "id": "40353cb09b4ecb4cc783fb9dace7c3aa18f9b835",
    "semantic_title": "mining polysemous triplets with recurrent neural networks for spoken language understanding",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ray19_interspeech.html": {
    "title": "Iterative Delexicalization for Improved Spoken Language Understanding",
    "volume": "main",
    "abstract": "Recurrent neural network (RNN) based joint intent classification and slot tagging models have achieved tremendous success in recent years for building spoken language understanding and dialog systems. However, these models suffer from poor performance for slots which often encounter large semantic variability in slot values after deployment (e.g. message texts, partial movie/artist names). While greedy delexicalization of slots in the input utterance via substring matching can partly improve performance, it often produces incorrect input. Moreover, such techniques cannot delexicalize slots with out-of-vocabulary slot values not seen at training. In this paper, we propose a novel iterative delexicalization algorithm, which can accurately delexicalize the input, even with out-of-vocabulary slot values. Based on model confidence of the current delexicalized input, our algorithm improves delexicalization in every iteration to converge to the best input having the highest confidence. We show on benchmark and in-house datasets that our algorithm can greatly improve parsing performance for RNN based models, especially for out-of-distribution slot values",
    "checked": true,
    "id": "0de85ba1b1c8921e41f598a7b5f40920c608013d",
    "semantic_title": "iterative delexicalization for improved spoken language understanding",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bhosale19_interspeech.html": {
    "title": "End-to-End Spoken Language Understanding: Bootstrapping in Low Resource Scenarios",
    "volume": "main",
    "abstract": "End-to-end Spoken Language Understanding (SLU) systems, without speech-to-text conversion, are more promising in low resource scenarios. They can be more effective when there is not enough labeled data to train reliable speech recognition and language understanding systems, or where running SLU on edge is preferred over cloud based services. In this paper, we present an approach for bootstrapping end-to-end SLU in low resource scenarios. We show that incorporating layers extracted from pre-trained acoustic models, instead of using the typical Mel filter bank features, lead to better performing SLU models. Moreover, the layers extracted from a model pre-trained on one language perform well even for (a) SLU tasks on a different language and also (b) on utterances from speakers with speech disorder",
    "checked": true,
    "id": "685d4b63e35a8603791dcf672ea5eff3083c96e0",
    "semantic_title": "end-to-end spoken language understanding: bootstrapping in low resource scenarios",
    "citation_count": 26
  },
  "https://www.isca-speech.org/archive/interspeech_2019/takatsu19_interspeech.html": {
    "title": "Recognition of Intentions of Users' Short Responses for Conversational News Delivery System",
    "volume": "main",
    "abstract": "In human-human conversations, listeners often convey intentions to their speakers through feedbacks comprising reflexive short responses. The speakers then recognize these intentions and dynamically change the conversational plans to transmit information more efficiently. For the design of spoken dialogue systems that deliver a massive amount of information, such as news, it is essential to accurately capture users' intentions from reflexive short responses to efficiently select or eliminate the information to be transmitted depending on the user's needs. However, such short responses from users are normally too short to recognize their actual intentions only from the prosodic and linguistic features of their short responses. In this paper, we propose a user's short-response intention-recognition model that accounts for the previous system's utterances as the context of the conversation in addition to prosodic and linguistic features of user's utterances. To achieve this, we define types of short response intentions in terms of effective information transmission and created new dataset by annotating over the interaction data collected using our spoken dialogue system. Our experimental results demonstrate that the classification accuracy can be improved using the linguistic features of the system's previous utterances encoded by Bidirectional Encoder Representations from Transformers (BERT) as the conversational context",
    "checked": true,
    "id": "ee3a4e09e6d295816d88c0f5594ded7e4922e8b6",
    "semantic_title": "recognition of intentions of users' short responses for conversational news delivery system",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/caubriere19_interspeech.html": {
    "title": "Curriculum-Based Transfer Learning for an Effective End-to-End Spoken Language Understanding and Domain Portability",
    "volume": "main",
    "abstract": "We present an end-to-end approach to extract semantic concepts directly from the speech audio signal. To overcome the lack of data available for this spoken language understanding approach, we investigate the use of a transfer learning strategy based on the principles of curriculum learning. This approach allows us to exploit out-of-domain data that can help to prepare a fully neural architecture. Experiments are carried out on the French MEDIA and PORTMEDIA corpora and show that this end-to-end SLU approach reaches the best results ever published on this task. We compare our approach to a classical pipeline approach that uses ASR, POS tagging, lemmatizer, chunker … and other NLP tools that aim to enrich ASR outputs that feed an SLU text to concepts system. Last, we explore the promising capacity of our end-to-end SLU approach to address the problem of domain portability",
    "checked": true,
    "id": "783fc1d48de76e3750d46023b315c75cab97bf40",
    "semantic_title": "curriculum-based transfer learning for an effective end-to-end spoken language understanding and domain portability",
    "citation_count": 45
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dash19b_interspeech.html": {
    "title": "Spatial and Spectral Fingerprint in the Brain: Speaker Identification from Single Trial MEG Signals",
    "volume": "main",
    "abstract": "Brain activity signals are unique subject-specific biological features that can not be forged or stolen. Recognizing this inherent trait, brain waves are recently being acknowledged as a far more secure, sensitive, and confidential biometric approach for user identification. Yet, current electroencephalography (EEG) based biometric systems are still in infancy considering their requirement of a large number of sensors and lower recognition performance compared to present biometric modalities. In this study, we investigated the spatial and spectral fingerprints in the brain with magnetoencephalography (MEG) for speaker identification during rest (pre-stimuli) and speech production. Experimental results suggested that the frontal and the temporal regions of the brain and higher frequency (gamma and high gamma) neural oscillations are more dominating for speaker identification. Moreover, we also found that two optimally located MEG sensors are sufficient to obtain a high speaker classification accuracy during speech tasks whereas at least eight optimally located sensors are needed to accurately identify these subjects during rest-state (pre-stimuli). These results indicated the unique neural traits of speech production across speakers",
    "checked": true,
    "id": "0afd70a1e8365d83649c3fbf9a0fa4875fe4007e",
    "semantic_title": "spatial and spectral fingerprint in the brain: speaker identification from single trial meg signals",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nijveld19_interspeech.html": {
    "title": "ERP Signal Analysis with Temporal Resolution Using a Time Window Bank",
    "volume": "main",
    "abstract": "In order to study the cognitive processes underlying speech comprehension, neuro-physiological measures (e.g., EEG and MEG), or behavioural measures (e.g., reaction times and response accuracy) can be applied. Compared to behavioural measures, EEG signals can provide a more fine-grained and complementary view of the processes that take place during the unfolding of an auditory stimulus EEG signals are often analysed after having chosen specific time windows, which are usually based on the temporal structure of ERP components expected to be sensitive to the experimental manipulation. However, as the timing of ERP components may vary between experiments, trials, and participants, such a-priori defined analysis time windows may significantly hamper the exploratory power of the analysis of components of interest. In this paper, we explore a wide-window analysis method applied to EEG signals collected in an auditory repetition priming experiment This approach is based on a bank of temporal filters arranged along the time axis in combination with linear mixed effects modelling. Crucially, it permits a temporal decomposition of effects in a single comprehensive statistical model which captures the entire EEG trace",
    "checked": true,
    "id": "51755176b6a0f43e50060411f2640f236bb5e67a",
    "semantic_title": "erp signal analysis with temporal resolution using a time window bank",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bosch19_interspeech.html": {
    "title": "Phase Synchronization Between EEG Signals as a Function of Differences Between Stimuli Characteristics",
    "volume": "main",
    "abstract": "The neural processing of speech leads to specific patterns in the brain which can be measured as, e.g., EEG signals. When properly aligned with the speech input and averaged over many tokens, the Event Related Potential (ERP) signal is able to differentiate specific contrasts between speech signals. Well-known effects relate to the difference between expected and unexpected words, in particular in the N400, while effects in N100 and P200 are related to attention and acoustic onset effects. Most EEG studies deal with the amplitude of EEG signals over time, sidestepping the effect of phase and phase synchronization. This paper investigates the relation between phase in the EEG signals measured in an auditory lexical decision task by Dutch participants listening to full and reduced English word forms. We show that phase synchronization takes place across stimulus conditions, and that the so-called circular variance is narrowly related to the type of contrast between stimuli",
    "checked": true,
    "id": "536db6925669eb585d3360ca80e6ff1ea6ea24e4",
    "semantic_title": "phase synchronization between eeg signals as a function of differences between stimuli characteristics",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kharaman19_interspeech.html": {
    "title": "The Processing of Prosodic Cues to Rhetorical Question Interpretation: Psycholinguistic and Neurolinguistics Evidence",
    "volume": "main",
    "abstract": "In many languages, rhetorical questions (RQs) are produced with different prosodic realizations than string-identical information-seeking questions (ISQs). RQs typically have longer constituent durations and breathier voice quality than ISQs and differ in nuclear accent type. This paper reports on an identification experiment (Experiment 1) and an EEG experiment (Experiment 2) on German wh-questions. In the identification experiment, we manipulated nuclear pitch accent type, voice quality and constituent duration and participants indicated whether they judged the realization as ISQ or RQ. The results showed additive effects of the three factors, with pitch accent as strongest predictor. In the EEG experiment, participants heard the stimuli in two contexts, triggering an ISQ or RQ (blocked). We manipulated pitch accent type and voice quality, resulting in RQ-coherent and ISQ-coherent stimuli, based on the outcome of Experiment 1. Results showed a prosodic expectancy positivity (PEP) for prosodic realizations that were incoherent with ISQ-contexts with an onset of ~120ms after the onset of the word with nuclear accent. This effect might reflect the emotional prosodic aspect of RQs. Taken together, participants use prosody to resolve the ambiguity and event-related potentials (ERPs) react to prosodic realizations that do not match contextually triggered expectations",
    "checked": true,
    "id": "9090bab2255bf4c030de8ed600ff23481e399a86",
    "semantic_title": "the processing of prosodic cues to rhetorical question interpretation: psycholinguistic and neurolinguistics evidence",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2019/scharenborg19_interspeech.html": {
    "title": "The Neural Correlates Underlying Lexically-Guided Perceptual Learning",
    "volume": "main",
    "abstract": "There is ample evidence showing that listeners are able to quickly adapt their phoneme classes to ambiguous sounds using a process called lexically-guided perceptual learning. This paper presents the first attempt to examine the neural correlates underlying this process. Specifically, we compared the brain's responses to ambiguous [f/s] sounds in Dutch non-native listeners of English (N=36) before and after exposure to the ambiguous sound to induce learning, using Event-Related Potentials (ERPs). We identified a group of participants who showed lexically-guided perceptual learning in their phonetic categorization behavior as observed by a significant difference in /s/ responses between pretest and posttest and a group who did not. Moreover, we observed differences in mean ERP amplitude to ambiguous phonemes at pretest and posttest, shown by a reliable reduction in amplitude of a positivity over medial central channels from 250 to 550 ms. However, we observed no significant correlation between the size of behavioral and neural pre/posttest effects. Possibly, the observed behavioral and ERP differences between pretest and posttest link to different aspects of the sound classification task. In follow-up research, these differences will be further investigated by assessing their relationship to neural responses to the ambiguous sounds in the exposure phase",
    "checked": true,
    "id": "6b3e2951c1e6e8e30d25257194e437824ab85ebe",
    "semantic_title": "the neural correlates underlying lexically-guided perceptual learning",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/parmonangan19_interspeech.html": {
    "title": "Speech Quality Evaluation of Synthesized Japanese Speech Using EEG",
    "volume": "main",
    "abstract": "As synthesized speech technology becomes more widely used, the synthesized speech quality must be assessed to ensure that it is acceptable. Subjective evaluation metrics, such as mean opinion score (MOS), can only provide an overall impression without any further detailed information about the speech. Therefore, this study proposes predicting speech quality using electroencephalographs (EEG), which are more objective and have high temporal resolution. In this paper, we use one natural speech and four types of synthesized speech lasting two to six seconds. First, to obtain ground truth of MOS, we gathered ten subjects to give opinion score on a scale of one to five for each recording. Second, another nine subjects were asked to measure how close to natural speech each synthesized speech sounded. The subjects' EEGs were recorded while they were listening to and evaluating the listened speech. The best accuracy achieved for classification was 96.61% using support vector machine, 80.36% using linear discriminant analysis, and 59.9% using logistic regression. For regression, we achieved root mean squared error as low as 1.133 using SVR and 1.353 using linear regression. This study demonstrates that EEG could be used to evaluate the perceived speech quality objectively",
    "checked": true,
    "id": "d8ab5835913a42bb14f27b6802b52e75d14180cb",
    "semantic_title": "speech quality evaluation of synthesized japanese speech using eeg",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19d_interspeech.html": {
    "title": "Multi-Microphone Adaptive Noise Cancellation for Robust Hotword Detection",
    "volume": "main",
    "abstract": "Recently we proposed a dual-microphone adaptive noise cancellation (ANC) algorithm with deferred filter coefficients for robust hotword detection in [1]. It exploits two unique hotword-related features: hotwords are the leading phrase of valid voice queries and they are short. These features allow us not to compute a speech-noise mask that is a common prerequisite for many multichannel speech enhancement approaches. This novel idea was found effective against strong and ambiguous speech-like TV noise. In this paper, we show that it can be generalized to support more than two microphones. The development is validated using re-recorded data with background TV noise from a 3-mic array. By adding one more microphone, the false reject (FR) rate can be further reduced relatively by 33.5%",
    "checked": true,
    "id": "44435a85c2309cec8cd2a673e06baa7a5c186fbe",
    "semantic_title": "multi-microphone adaptive noise cancellation for robust hotword detection",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhao19c_interspeech.html": {
    "title": "Multi-Task Multi-Network Joint-Learning of Deep Residual Networks and Cycle-Consistency Generative Adversarial Networks for Robust Speech Recognition",
    "volume": "main",
    "abstract": "Robustness of automatic speech recognition (ASR) systems is a critical issue due to noise and reverberations. Speech enhancement and model adaptation have been studied for long time to address this issue. Recently, the developments of multi-task joint-learning scheme that addresses noise reduction and ASR criteria in a unified modeling framework show promising improvements, but the model training highly relies on paired clean-noisy data. To overcome this limit, the generative adversarial networks (GANs) and the adversarial training method are deployed, which have greatly simplified the model training process without the requirements of complex front-end design and paired training data. Despite the fast developments of GANs for computer visions, only regular GANs have been adopted for robust ASR. In this work, we adopt a more advanced cycle-consistency GAN (CycleGAN) to address the training failure problem due to mode collapse of regular GANs. Using deep residual networks (ResNets), we further expand the multi-task scheme to a multi-task multi-network joint-learning scheme for more robust noise reduction and model adaptation. Experiment results on CHiME-4 show that our proposed approach significantly improves the noise robustness of the ASR system by achieving much lower word error rates (WERs) than the state-of-the-art joint-learning approaches",
    "checked": true,
    "id": "83c92ba230cac393cec20f3853fa2e46be9e43d9",
    "semantic_title": "multi-task multi-network joint-learning of deep residual networks and cycle-consistency generative adversarial networks for robust speech recognition",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/khokhlov19_interspeech.html": {
    "title": "R-Vectors: New Technique for Adaptation to Room Acoustics",
    "volume": "main",
    "abstract": "Distant speech recognition is an important problem which is far from being solved. Reverberation and noise are in the list of main problems in this area. The most popular methods of dealing with them are data augmentation and speech enhancement. In this paper, we propose a novel approach, inspired by modern methods of speaker adaptation First of all, a feed-forward network is trained to classify room impulse responses (RIRs) from speech recordings. Then this network is used for extracting embeddings, which we call R-vectors. These R-vectors are appended to input features of the acoustic model. Due to the lack of labeled data for RIRs classification task, we propose a self-supervised method of training the network, which consists of using artificial audio generated by room simulator Experimental evaluation was conducted on VOiCES19 and AMI single-channel tasks as well as CHiME5 multi-channel task. It is shown that the R-vector-adapted ASR systems achieve up to 14% relative WER reduction. Furthermore, it is additive with gains from state-of-the-art dereverberation (WPE) and speaker adaptation (x-vector) techniques",
    "checked": true,
    "id": "bfb92a178ad45fb20b5a308b7196028a595b88ba",
    "semantic_title": "r-vectors: new technique for adaptation to room acoustics",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kanda19b_interspeech.html": {
    "title": "Guided Source Separation Meets a Strong ASR Backend: Hitachi/Paderborn University Joint Investigation for Dinner Party ASR",
    "volume": "main",
    "abstract": "In this paper, we present Hitachi and Paderborn University's joint effort for automatic speech recognition (ASR) in a dinner party scenario. The main challenges of ASR systems for dinner party recordings obtained by multiple microphone arrays are (1) heavy speech overlaps, (2) severe noise and reverberation, (3) very natural conversational content, and possibly (4) insufficient training data. As an example of a dinner party scenario, we have chosen the data presented during the CHiME-5 speech recognition challenge, where the baseline ASR had a 73.3% word error rate (WER), and even the best performing system at the CHiME-5 challenge had a 46.1% WER. We extensively investigated a combination of the guided source separation-based speech enhancement technique and an already proposed strong ASR backend and found that a tight combination of these techniques provided substantial accuracy improvements. Our final system achieved WERs of 39.94% and 41.64% for the development and evaluation data, respectively, both of which are the best published results for the dataset. We also investigated with additional training data on the official small data in the CHiME-5 corpus to assess the intrinsic difficulty of this ASR task",
    "checked": true,
    "id": "21b772d55c422e337b2ab86bf5732bcd2ba4f3a3",
    "semantic_title": "guided source separation meets a strong asr backend: hitachi/paderborn university joint investigation for dinner party asr",
    "citation_count": 48
  },
  "https://www.isca-speech.org/archive/interspeech_2019/drude19_interspeech.html": {
    "title": "Unsupervised Training of Neural Mask-Based Beamforming",
    "volume": "main",
    "abstract": "We present an unsupervised training approach for a neural network-based mask estimator in an acoustic beamforming application. The network is trained to maximize a likelihood criterion derived from a spatial mixture model of the observations. It is trained from scratch without requiring any parallel data consisting of degraded input and clean training targets. Thus, training can be carried out on real recordings of noisy speech rather than simulated ones. In contrast to previous work on unsupervised training of neural mask estimators, our approach avoids the need for a possibly pre-trained teacher model entirely. We demonstrate the effectiveness of our approach by speech recognition experiments on two different datasets: one mainly deteriorated by noise (CHiME 4) and one by reverberation (REVERB). The results show that the performance of the proposed system is on par with a supervised system using oracle target masks for training and with a system trained using a model-based teacher",
    "checked": true,
    "id": "ad967431765c8b5a11aa7a78d6b836790c181afa",
    "semantic_title": "unsupervised training of neural mask-based beamforming",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ma19_interspeech.html": {
    "title": "Acoustic Model Ensembling Using Effective Data Augmentation for CHiME-5 Challenge",
    "volume": "main",
    "abstract": "CHiME-5 is a research community challenge targeting the problem of far-field and multi-talker conversational speech recognition in dinner party scenarios involving background noises, reverberations and overlapping speech. In this study, we present five different kinds of robust acoustic models which take advantages from both effective data augmentation and ensemble methods to improve the recognition performance for the CHiME-5 challenge. First, we detail the effective data augmentation for far-field scenarios, especially the far-field data simulation. Different from the conventional data simulation methods, we use a signal processing method originally developed for channel identification to estimate the room impulse responses and then simulate the far-field data. Second, we introduce the five different kinds of robust acoustic models. Finally, the effectiveness of our acoustic model ensembling strategies at the lattice level and the state posterior level are evaluated and demonstrated. Our system achieves the best performance of all four tasks among submitted systems in the CHiME-5 challenge",
    "checked": true,
    "id": "9242cc3228e6d43b5cbe68ada0f05477b3405378",
    "semantic_title": "acoustic model ensembling using effective data augmentation for chime-5 challenge",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19d_interspeech.html": {
    "title": "Survey Talk: End-to-End Deep Neural Network Based Speaker and Language Recognition",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "3a9de52f8f893dad4501d44c9ae03b190dad26dc",
    "semantic_title": "survey talk: end-to-end deep neural network based speaker and language recognition",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/padi19_interspeech.html": {
    "title": "Attention Based Hybrid i-Vector BLSTM Model for Language Recognition",
    "volume": "main",
    "abstract": "In this paper, a hybrid i-vector neural network framework (i-BLSTM) which models the sequence information present in a series of short segment i-vectors for the task of spoken language recognition (LRE) is proposed. A sequence of short segment i-vectors are extracted for every speech utterance and are then modeled using a bidirectional long short-term memory (BLSTM) recurrent neural network (RNN). Attention mechanism inside the neural network relevantly weights segments of the speech utterance and the model learns to give higher weights to parts of speech data which are more helpful to the classification task. The proposed framework performs better in short duration and noisy environments when compared with the conventional i-vector system. Experiments are performed on clean, noisy and multi-speaker speech data from NIST LRE 2017 and RATS language recognition corpus. In these experiments, the proposed approach yields significant improvements (relative improvements of 7.6–13% in terms of accuracy for noisy conditions) over the conventional i-vector based language recognition approach and also over an end-to-end LSTM-RNN based approach",
    "checked": true,
    "id": "bae21c43d41b3d2adb619ed7b92ccb252082cebf",
    "semantic_title": "attention based hybrid i-vector blstm model for language recognition",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jung19b_interspeech.html": {
    "title": "RawNet: Advanced End-to-End Deep Neural Network Using Raw Waveforms for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "Recently, direct modeling of raw waveforms using deep neural networks has been widely studied for a number of tasks in audio domains. In speaker verification, however, utilization of raw waveforms is in its preliminary phase, requiring further investigation. In this study, we explore end-to-end deep neural networks that input raw waveforms to improve various aspects: front-end speaker embedding extraction including model architecture, pre-training scheme, additional objective functions, and back-end classification. Adjustment of model architecture using a pre-training scheme can extract speaker embeddings, giving a significant improvement in performance. Additional objective functions simplify the process of extracting speaker embeddings by merging conventional two-phase processes: extracting utterance-level features such as i-vectors or x-vectors and the feature enhancement phase, e.g., linear discriminant analysis. Effective back-end classification models that suit the proposed speaker embedding are also explored. We propose an end-to-end system that comprises two deep neural networks, one frontend for utterance-level speaker embedding extraction and the other for back-end classification. Experiments conducted on the VoxCeleb1 dataset demonstrate that the proposed model achieves state-of-the-art performance among systems without data augmentation. The proposed system is also comparable to the state-of-the-art x-vector system that adopts heavy data augmentation",
    "checked": true,
    "id": "d71d85c4f6f8c263ceb0750def148c2a3708979f",
    "semantic_title": "rawnet: advanced end-to-end deep neural network using raw waveforms for text-independent speaker verification",
    "citation_count": 94
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rao19_interspeech.html": {
    "title": "Target Speaker Extraction for Multi-Talker Speaker Verification",
    "volume": "main",
    "abstract": "The performance of speaker verification degrades significantly when the test speech is corrupted by interference from non-target speakers. Speaker diarization separates speakers well only if the speakers are not overlapped. However, if multiple talkers speak at the same time, we need a technique to separate the speech in the spectral domain. In this paper, we study a way to extract the target speaker's speech from an overlapped multi-talker speech. Specifically, given some reference speech samples from the target speaker, the target speaker's speech is firstly extracted from the overlapped multi-talker speech, then the extracted speech is processed in the speaker verification system. Experimental results show that the proposed approach significantly improves the performance of overlapped multi-talker speaker verification and achieves 64.4% relative EER reduction over the zero-effort baseline",
    "checked": true,
    "id": "706c44ae6036939f5eb5d2041289105a0aa07492",
    "semantic_title": "target speaker extraction for multi-talker speaker verification",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mazzawi19_interspeech.html": {
    "title": "Improving Keyword Spotting and Language Identification via Neural Architecture Search at Scale",
    "volume": "main",
    "abstract": "In this paper we present a novel Neural Architecture Search (NAS) framework to improve keyword spotting and spoken language identification models. Even with the huge success of deep neural networks (DNNs) in many different domains, finding the best network architecture is still a laborious task and very computationally expensive at best with existing searching approaches. Our search approach efficiently and robustly finds better model sequences with respect to hand-designed systems. We do this by constructing architectures incrementally, using a custom mutation algorithm and leveraging the power of parameter transfer between layers. We demonstrate that our approach can automatically design DNNs with an order of magnitude fewer parameters that achieves better performance than the current best models. It leads to significant performance improvements: up to 4.09% accuracy increase for language identification (6.1% if we allow an increase in the number of parameters) and 0.3% for phoneme classification in keyword spotting with half the size of the model",
    "checked": true,
    "id": "1bcad4cdfbc01fbb60a815660d034e561843d67a",
    "semantic_title": "improving keyword spotting and language identification via neural architecture search at scale",
    "citation_count": 34
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zheng19_interspeech.html": {
    "title": "Forward-Backward Decoding for Regularizing End-to-End TTS",
    "volume": "main",
    "abstract": "Neural end-to-end TTS can generate very high-quality synthesized speech, and even close to human recording within similar domain text. However, it performs unsatisfactory when scaling it to challenging test sets. One concern is that the encoder-decoder with attention-based network adopts autoregressive generative sequence model with the limitation of \"exposure bias\". To address this issue, we propose two novel methods, which learn to predict future by improving agreement between forward and backward decoding sequence. The first one is achieved by introducing divergence regularization terms into model training objective to reduce the mismatch between two directional models, namely L2R and R2L (which generates targets from left-to-right and right-to-left, respectively). While the second one operates on decoder-level and exploits the future information during decoding. In addition, we employ a joint training strategy to allow forward and backward decoding to improve each other in an interactive process. Experimental results show our proposed methods especially the second one (bidirectional decoder regularization), leads a significantly improvement on both robustness and overall naturalness, as outperforming baseline (the revised version of Tacotron2) with a MOS gap of 0.14 in a challenging test, and achieving close to human quality (4.42 vs. 4.49 in MOS) on general test",
    "checked": true,
    "id": "f44b10c9e9845c96cc497559314a9e5741846f7e",
    "semantic_title": "forward-backward decoding for regularizing end-to-end tts",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2019/guo19b_interspeech.html": {
    "title": "A New GAN-Based End-to-End TTS Training Algorithm",
    "volume": "main",
    "abstract": "End-to-end, autoregressive model-based TTS has shown significant performance improvements over the conventional ones. However, the autoregressive module training is affected by the exposure bias, or the mismatch between different distributions of real and predicted data. While real data is provided in training, in testing, predicted data is available only. By introducing both real and generated data sequences in training, we can alleviate the effects of the exposure bias. We propose to use Generative Adversarial Network (GAN) along with the idea of \"Professor Forcing\" in training. A discriminator in GAN is jointly trained to equalize the difference between real and the predicted data. In AB subjective listening test, the results show that the new approach is preferred over the standard transfer learning with a CMOS improvement of 0.1. Sentence level intelligibility tests also show significant improvement in a pathological test set. The GAN-trained new model is shown more stable than the baseline to produce better alignments for the Tacotron output",
    "checked": true,
    "id": "fa081e91d94cefea2a6908b67bf550b3a8ba0f75",
    "semantic_title": "a new gan-based end-to-end tts training algorithm",
    "citation_count": 44
  },
  "https://www.isca-speech.org/archive/interspeech_2019/he19_interspeech.html": {
    "title": "Robust Sequence-to-Sequence Acoustic Modeling with Stepwise Monotonic Attention for Neural TTS",
    "volume": "main",
    "abstract": "Neural TTS has demonstrated strong capabilities to generate human-like speech with high quality and naturalness, while its generalization to out-of-domain texts is still a challenging task, with regard to the design of attention-based sequence-to-sequence acoustic modeling. Various errors occur in those inputs with unseen context, including attention collapse, skipping, repeating, etc., which limits the broader applications. In this paper, we propose a novel stepwise monotonic attention method in sequence-to-sequence acoustic modeling to improve the robustness on out-of-domain inputs. The method utilizes the strict monotonic property in TTS with constraints on monotonic hard attention that the alignments between inputs and outputs sequence must be not only monotonic but allowing no skipping on inputs. Soft attention could be used to evade mismatch between training and inference. The experimental results show that the proposed method could achieve significant improvements in robustness on out-of-domain scenarios for phoneme-based models, without any regression on the in-domain naturalness test",
    "checked": true,
    "id": "051238432e23a865981c95579deb978e815d8f55",
    "semantic_title": "robust sequence-to-sequence acoustic modeling with stepwise monotonic attention for neural tts",
    "citation_count": 72
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19b_interspeech.html": {
    "title": "Joint Training Framework for Text-to-Speech and Voice Conversion Using Multi-Source Tacotron and WaveNet",
    "volume": "main",
    "abstract": "We investigated the training of a shared model for both text-to-speech (TTS) and voice conversion (VC) tasks. We propose using an extended model architecture of Tacotron, that is a multi-source sequence-to-sequence model with a dual attention mechanism as the shared model for both the TTS and VC tasks. This model can accomplish these two different tasks respectively according to the type of input. An end-to-end speech synthesis task is conducted when the model is given text as the input while a sequence-to-sequence voice conversion task is conducted when it is given the speech of a source speaker as the input. Waveform signals are generated by using WaveNet, which is conditioned by using a predicted mel-spectrogram. We propose jointly training a shared model as a decoder for a target speaker that supports multiple sources. Listening experiments show that our proposed multi-source encoder-decoder model can efficiently achieve both the TTS and VC tasks",
    "checked": true,
    "id": "27376bd3085e30d5264c73d5b74a318f9895949d",
    "semantic_title": "joint training framework for text-to-speech and voice conversion using multi-source tacotron and wavenet",
    "citation_count": 46
  },
  "https://www.isca-speech.org/archive/interspeech_2019/luong19_interspeech.html": {
    "title": "Training Multi-Speaker Neural Text-to-Speech Systems Using Speaker-Imbalanced Speech Corpora",
    "volume": "main",
    "abstract": "When the available data of a target speaker is insufficient to train a high quality speaker-dependent neural text-to-speech (TTS) system, we can combine data from multiple speakers and train a multi-speaker TTS model instead. Many studies have shown that neural multi-speaker TTS model trained with a small amount data from multiple speakers combined can generate synthetic speech with better quality and stability than a speaker-dependent one. However when the amount of data from each speaker is highly unbalanced, the best approach to make use of the excessive data remains unknown. Our experiments showed that simply combining all available data from every speaker to train a multi-speaker model produces better than or at least similar performance to its speaker-dependent counterpart. Moreover by using an ensemble multi-speaker model, in which each subsystem is trained on a subset of available data, we can further improve the quality of the synthetic speech especially for underrepresented speakers whose training data is limited",
    "checked": true,
    "id": "3c3f043d96f6e704467d085633a0d522b7984881",
    "semantic_title": "training multi-speaker neural text-to-speech systems using speaker-imbalanced speech corpora",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2019/okamoto19_interspeech.html": {
    "title": "Real-Time Neural Text-to-Speech with Sequence-to-Sequence Acoustic Model and WaveGlow or Single Gaussian WaveRNN Vocoders",
    "volume": "main",
    "abstract": "This paper investigates real-time high-fidelity neural text-to-speech (TTS) systems. For real-time neural vocoders, WaveGlow is introduced and single Gaussian (SG)WaveRNN is proposed. The proposed SG-WaveRNN can predict continuous valued speech waveforms with half the synthesis time compared with vanilla WaveRNN with dual-softmax for 16 bit audio prediction. Additionally, a sequence-to-sequence (seq2seq) acoustic model (AM) for pitch accent languages, such as Japanese, is investigated by introducing Tacotron 2 architecture. In the seq2seq AM, full-context labels extracted from a text analyzer are used as input and they are directly converted into mel-spectrograms. The results of subjective experiment using a Japanese female corpus indicate that the proposed SG-WaveRNN vocoder with noise shaping can synthesize high-quality speech waveforms and real-time high-fidelity neural TTS systems can be realized with the seq2seq AM and WaveGlow or SG-WaveRNN vocoders. Especially, the seq2seq AM and WaveGlow vocoder conditioned on mel-spectrograms with simple PyTorch implementations can be realized with real-time factors 0.06 and 0.10 for inference using a GPU",
    "checked": true,
    "id": "5bb538f50cbbba2570a181e396b102d09d945aee",
    "semantic_title": "real-time neural text-to-speech with sequence-to-sequence acoustic model and waveglow or single gaussian wavernn vocoders",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kafle19_interspeech.html": {
    "title": "Fusion Strategy for Prosodic and Lexical Representations of Word Importance",
    "volume": "main",
    "abstract": "We investigate whether, and if so when, prosodic features in spoken dialogue aid in modeling the importance of words to the overall meaning of a dialogue turn. Starting from the assumption that acoustic-prosodic cues help identify important speech content, we investigate representation architectures that combine lexical and prosodic features and evaluate them for predicting word importance. We propose an attention-based feature fusion strategy and additionally show how the addition of strategic supervision of the attention weights results in especially competitive models. We evaluate our fusion strategy on spoken dialogues and demonstrate performance increases over state-of-the-art models. Specifically, our approach both achieves the lowest root mean square error on test data and generalizes better over out-of-vocabulary words",
    "checked": true,
    "id": "587621e39b609ff75820faf832b0a11bd5e4c1ea",
    "semantic_title": "fusion strategy for prosodic and lexical representations of word importance",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chien19b_interspeech.html": {
    "title": "Self Attention in Variational Sequential Learning for Summarization",
    "volume": "main",
    "abstract": "Attention mechanism plays a crucial role in sequential learning for many speech and language applications. However, it is challenging to develop a stochastic attention in a sequence-to-sequence model which consists of two recurrent neural networks (RNNs) as the encoder and decoder. The problem of posterior collapse happens in variational inference and results in the estimated latent variables close to a standard Gaussian prior so that the information from input sequence is disregarded in learning process. This paper presents a new recurrent autoencoder for sentence representation where a self attention scheme is incorporated to activate the interaction between inference and generation in training procedure. In particular, a stochastic RNN decoder is implemented to provide additional latent variable to fulfill self attention for sentence reconstruction. The posterior collapse is alleviated. The latent information is sufficiently attended in variational sequential learning. During test phase, the estimated prior distribution of decoder is sampled for stochastic attention and generation. Experiments on Penn Treebank and Yelp 2013 show the desirable generation performance in terms of perplexity. The visualization of attention weights also illustrates the usefulness of self attention. The evaluation on DUC 2007 demonstrates the merit of variational recurrent autoencoder for document summarization",
    "checked": true,
    "id": "2cd2140f806539cb4759c62f0bd5f50b3f4fc52a",
    "semantic_title": "self attention in variational sequential learning for summarization",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sun19_interspeech.html": {
    "title": "Multi-Modal Sentiment Analysis Using Deep Canonical Correlation Analysis",
    "volume": "main",
    "abstract": "This paper learns multi-modal embeddings from text, audio, and video views/modes of data in order to improve upon downstream sentiment classification. The experimental framework also allows investigation of the relative contributions of the individual views in the final multi-modal embedding. Individual features derived from the three views are combined into a multi-modal embedding using Deep Canonical Correlation Analysis (DCCA) in two ways i) One-Step DCCA and ii) Two-Step DCCA. This paper learns text embeddings using BERT, the current state-of-the-art in text encoders. We posit that this highly optimized algorithm dominates over the contribution of other views, though each view does contribute to the final result. Classification tasks are carried out on two benchmark data sets and on a new Debate Emotion data set, and together these demonstrate that the one-Step DCCA outperforms the current state-of-the-art in learning multi-modal embeddings",
    "checked": true,
    "id": "902bd44b3d94603d23f493ca1486361bb56a3e2b",
    "semantic_title": "multi-modal sentiment analysis using deep canonical correlation analysis",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shen19_interspeech.html": {
    "title": "Interpreting and Improving Deep Neural SLU Models via Vocabulary Importance",
    "volume": "main",
    "abstract": "Spoken language understanding (SLU) is a crucial component in virtual personal assistants. It consists of two main tasks: intent detection and slot filling. State-of-the-art deep neural SLU models have demonstrated good performance on benchmark datasets. However, these models suffer from the significant performance drop in practice after deployment due to the data distribution discrepancy between training and real user utterances. In this paper, we first propose four research questions that help to understand what the state-of-the-art deep neural SLU models actually learn. To answer them, we study the vocabulary importance using a novel Embedding Sparse Structure Learning (SparseEmb) approach. It can be applied onto various existing deep SLU models to efficiently prune the useless words without any additional manual hyperparameter tuning. We evaluate SparseEmb on benchmark datasets using two existing SLU models and answer the proposed research questions. Then, we utilize SparseEmb to sanitize the training data based on the selected useless words as well as the model re-validation during training. Using both benchmark and our collected testing data, we show that our sanitized training data helps to significantly improve the SLU model performance. Both SparseEmb and training data sanitization approaches can be applied onto any deep learning based SLU models",
    "checked": true,
    "id": "feb79e1ed36ca9f9efa2792bbbeeabf87227c85f",
    "semantic_title": "interpreting and improving deep neural slu models via vocabulary importance",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tundik19_interspeech.html": {
    "title": "Assessing the Semantic Space Bias Caused by ASR Error Propagation and its Effect on Spoken Document Summarization",
    "volume": "main",
    "abstract": "Ambitions in artificial intelligence involve machine understanding of human language. The state-of-the-art approach for Spoken Language Understanding is using an Automatic Speech Recognizer (ASR) to generate transcripts, which are further processed with text-based tools. ASR yields error prone transcripts, these errors then propagate further into the processing pipeline. Subjective tests show on the other hand, that humans understand quite well ASR closed captions despite the word and punctuation errors. Our goal is to assess and quantify the loss in the semantic space resulting from error propagation and also analyze error propagation into speech summarization as a special use-case. We show, that word errors cause a slight shift in the semantic space, which is fairly below the average semantic distance between the sentences within a document. We also show, that punctuation errors have higher impact on summarization performance, which suggests that proper sentence level tokenization is crucial for this task",
    "checked": true,
    "id": "a14bb8f09fac4c0e8e5acc53e43b5986cdf6998d",
    "semantic_title": "assessing the semantic space bias caused by asr error propagation and its effect on spoken document summarization",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19e_interspeech.html": {
    "title": "Latent Topic Attention for Domain Classification",
    "volume": "main",
    "abstract": "Attention-based bidirectional long short-term network (BiLSTM) models have recently shown promising results in text classification tasks. However, when the amount of training data is restricted, or the distribution of the test data is quite different from the training data, some potential informative words maybe hard to be captured in training. In this work, we propose a new method to learn attention mechanism for domain classification. Unlike the past attention mechanisms only guided by domain tags of training data, we explore using the latent topics in the data set to learn topic attention, and employ it for BiLSTM. Experiments on the SMP-ECDT benchmark corpus show that the proposed latent topic attention mechanism outperforms the state-of-the-art soft and hard attention mechanisms in domain classification. Moreover, experiment result shows that the proposed method can be trained with additional unlabeled data and further improve the domain classification performance",
    "checked": true,
    "id": "c27b022e6cab84c20a27368c74d53d0f374f3401",
    "semantic_title": "latent topic attention for domain classification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/narisetty19_interspeech.html": {
    "title": "A Unified Bayesian Source Modelling for Determined Blind Source Separation",
    "volume": "main",
    "abstract": "This paper proposes a determined blind source separation (BSS) method with a Bayesian generalization for unified modelling of multiple audio sources. Our probabilistic framework allows a flexible multi-source modelling where the number of latent features required for the unified model is optimally estimated. When partitioning the latent features of the unified model to represent individual sources, the proposed approach helps to avoid over-fitting or under-fitting the correlations among sources. This adaptability of our Bayesian generalization therefore adds flexibility to conventional BSS approaches, where the number of latent features in the unified model has to be specified in advance. In the task of separating speech mixture signals, we show that our proposed method models diverse sources in a flexible manner and markedly improves the separation performance as compared to the conventional methods",
    "checked": true,
    "id": "547fc654d1c827fd9e5d1abea4308374c2b77254",
    "semantic_title": "a unified bayesian source modelling for determined blind source separation",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/takahashi19_interspeech.html": {
    "title": "Recursive Speech Separation for Unknown Number of Speakers",
    "volume": "main",
    "abstract": "In this paper we propose a method of single-channel speaker-independent multi-speaker speech separation for an unknown number of speakers. As opposed to previous works, in which the number of speakers is assumed to be known in advance and speech separation models are specific for the number of speakers, our proposed method can be applied to cases with different numbers of speakers using a single model by recursively separating a speaker. To make the separation model recursively applicable, we propose one-and-rest permutation invariant training (OR-PIT). Evaluation on WSJ0-2mix and WSJ0-3mix datasets show that our proposed method achieves state-of-the-art results for two- and three-speaker mixtures with a single model. Moreover, the same model can separate four-speaker mixture, which was never seen during the training. We further propose the detection of the number of speakers in a mixture during recursive separation and show that this approach can more accurately estimate the number of speakers than detection in advance by using a deep neural network based classifier",
    "checked": true,
    "id": "1315f5de75a0fd6d679d27ec5f9b545d967c0dde",
    "semantic_title": "recursive speech separation for unknown number of speakers",
    "citation_count": 65
  },
  "https://www.isca-speech.org/archive/interspeech_2019/appeltans19_interspeech.html": {
    "title": "Practical Applicability of Deep Neural Networks for Overlapping Speaker Separation",
    "volume": "main",
    "abstract": "This paper examines the applicability in realistic scenarios of two deep learning based solutions to the overlapping speaker separation problem. Firstly, we present experiments that show that these methods are applicable for a broad range of languages. Further experimentation indicates limited performance loss for untrained languages, when these have common features with the trained language(s). Secondly, it investigates how the methods deal with realistic background noise and proposes some modifications to better cope with these disturbances. The deep learning methods that will be examined are deep clustering and deep attractor networks",
    "checked": true,
    "id": "e8b6b833fdd98032ddca828c6e447d61dd92f018",
    "semantic_title": "practical applicability of deep neural networks for overlapping speaker separation",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gu19_interspeech.html": {
    "title": "Speech Separation Using Independent Vector Analysis with an Amplitude Variable Gaussian Mixture Model",
    "volume": "main",
    "abstract": "Independent vector analysis (IVA) utilizing Gaussian mixture model (GMM) as source priors has been demonstrated as an effective algorithm for joint blind source separation (JBSS). However, an extra pre-training process is required to provide initial parameter values for successful speech separation. In this paper, we introduce a time-varying parameter in the GMM to adapt to the temporal power fluctuation embedded in the nonstationary speech signal so as to avoid the pre-training process. The expectation-maximization (EM) process updating both the demixing matrix and the signal model is altered correspondingly. Experimental results confirm the efficacy of the proposed method under random initialization and further show its advantage in terms of a competitive separation accuracy and a faster convergence speed",
    "checked": true,
    "id": "baaeca2ea8812a0821e6c0baf53e158dfb545f2f",
    "semantic_title": "speech separation using independent vector analysis with an amplitude variable gaussian mixture model",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yang19c_interspeech.html": {
    "title": "Improved Speech Separation with Time-and-Frequency Cross-Domain Joint Embedding and Clustering",
    "volume": "main",
    "abstract": "Speech separation has been very successful with deep learning techniques. Substantial effort has been reported based on approaches over magnitude spectrogram, which is well known as the standard time-and-frequency cross-domain representation for speech signals. It is highly correlated to the phonetic structure of speech, or \"how the speech sounds\" when perceived by human, but primarily frequency domain features carrying temporal behaviour. Very impressive work achieving speech separation over time domain was reported recently, probably because waveforms in time domain may describe the different realizations of speech in a more precise way than magnitude spectrogram lacking phase information. In this paper, we propose a framework properly integrating the above two directions, hoping to achieve both purposes. We construct a time-and-frequency feature map by concatenating 1-dim convolution encoded feature map (for time domain) and magnitude spectrogram (for frequency domain), which was then processed by an embedding network and clustering approaches very similar to those used in time and frequency domain prior works. In this way, the information in time and frequency domains, as well as the interactions between them, can be jointly considered during embedding and clustering. Very encouraging results (state-of-the-art to our knowledge) were obtained with WSJ0-2mix dataset in preliminary experiments",
    "checked": true,
    "id": "687a450ba981ca3f8f52c556b53ec8393faa63c0",
    "semantic_title": "improved speech separation with time-and-frequency cross-domain joint embedding and clustering",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wichern19_interspeech.html": {
    "title": "WHAM!: Extending Speech Separation to Noisy Environments",
    "volume": "main",
    "abstract": "Recent progress in separating the speech signals from multiple overlapping speakers using a single audio channel has brought us closer to solving the cocktail party problem. However, most studies in this area use a constrained problem setup, comparing performance when speakers overlap almost completely, at artificially low sampling rates, and with no external background noise. In this paper, we strive to move the field towards more realistic and challenging scenarios. To that end, we created the WSJ0 Hipster Ambient Mixtures (WHAM!) dataset, consisting of two speaker mixtures from the wsj0-2mix dataset combined with real ambient noise samples. The samples were collected in coffee shops, restaurants, and bars in the San Francisco Bay Area, and are made publicly available. We benchmark various speech separation architectures and objective functions to evaluate their robustness to noise. While separation performance decreases as a result of noise, we still observe substantial gains relative to the noisy signals for most approaches",
    "checked": true,
    "id": "a6aca3527e123849b740d63e064051a1a46ecec6",
    "semantic_title": "wham!: extending speech separation to noisy environments",
    "citation_count": 173
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nautsch19_interspeech.html": {
    "title": "Survey Talk: Preserving Privacy in Speaker and Speech Characterisation",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "9fcd14c6796ccea86c04e6ff1fa8b5d39c98a471",
    "semantic_title": "survey talk: preserving privacy in speaker and speech characterisation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chermaz19_interspeech.html": {
    "title": "Evaluating Near End Listening Enhancement Algorithms in Realistic Environments",
    "volume": "main",
    "abstract": "Speech playback (e.g., TV, radio, public address) becomes harder to understand in the presence of noise and reverberation. NELE (Near End Listening Enhancement) algorithms can improve intelligibility by modifying the signal before it is played back. Substantial intelligibility improvements have been achieved in the lab for both natural and synthetic speech. However, evidence is still scarce on how these algorithms work under conditions of realistic noise and reverberation We present a realistic test platform, featuring two representative everyday scenarios in which speech playback may occur (in the presence of both noise and reverberation): a domestic space (living room) and a public space (cafeteria). The generated stimuli are evaluated by measuring keyword accuracy rates in a listening test with normal hearing subjects We use the new platform to compare three state-of-the-art NELE algorithms, employing either noise-adaptive or non-adaptive strategies, and with or without compensation for reverberation",
    "checked": true,
    "id": "420b3014011bcd426d545b6f5e9fdfd9b033c02f",
    "semantic_title": "evaluating near end listening enhancement algorithms in realistic environments",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/edraki19_interspeech.html": {
    "title": "Improvement and Assessment of Spectro-Temporal Modulation Analysis for Speech Intelligibility Estimation",
    "volume": "main",
    "abstract": "Several recent high-performing intelligibility estimators of acoustically degraded speech signals employ temporal modulation analysis. In this paper, we investigate the utility of using both spectro- and temporal-modulation for estimating speech intelligibility. We modified a pre-existing speech intelligibility estimation scheme (STMI) that was inspired by human auditory spectro-temporal modulation analysis. We produced several variants of the modified STMI and assessed their intelligibility prediction accuracy, in comparison with several high-performing estimators. Among the estimators tested, one of the STMI variants and eSTOI performed consistently well on both noisy and reverberated speech. These results suggest that spectro-temporal modulation analysis is useful for certain degradation conditions such as modulated noise and reverberation",
    "checked": true,
    "id": "90845a4ca26fde66715b5aa39be180e9caaea290",
    "semantic_title": "improvement and assessment of spectro-temporal modulation analysis for speech intelligibility estimation",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19c_interspeech.html": {
    "title": "Listener Preference on the Local Criterion for Ideal Binary-Masked Speech",
    "volume": "main",
    "abstract": "Ideal binary mask (IBM) is a signal-processing technique that retains the time-frequency regions in a mixture of target speech and background noise when the local signal-to-noise ratio (SNR) is higher than a local criterion (LC) and removes the regions otherwise. The intelligibility of IBM-processed speech is typically high and does not depend on the choice of LC for a wide range of LC values. The current study investigates the listeners' preferences on the LC value for IBM processed speech. Concatenated everyday sentences were mixed with three types of background noises (airplane noise, train noise, and multi-talker babble) and were presented continuously to the listeners following the IBM processing. The IBM algorithm was implemented so that the listeners were able to adjust the LC value in real-time using a programmable knob. The listeners were instructed to adjust the LC value until the IBM-processed stimuli reached the most preferable quality. Across 20 listeners, large individual differences were observed for the preferred LC values. A cluster analysis identified that 11 of the 20 listeners exhibited consistent patterns of results. For this main cluster of listeners, the preferred LC value depended on the noise type, overall SNR, and the difficulty of the target sentences",
    "checked": true,
    "id": "700eb8a3e58c972e6378bbb1d40773f6ee823348",
    "semantic_title": "listener preference on the local criterion for ideal binary-masked speech",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dinh19_interspeech.html": {
    "title": "Using a Manifold Vocoder for Spectral Voice and Style Conversion",
    "volume": "main",
    "abstract": "We propose a new type of spectral feature that is both compact and interpolable, and thus ideally suited for regression approaches that involve averaging. The feature is realized by means of a speaker-independent variational autoencoder (VAE), which learns a latent space based on the low-dimensional manifold of high-resolution speech spectra. In vocoding experiments, we showed that using a 12-dimensional VAE feature (VAE-12) resulted in significantly better perceived speech quality compared to a 12-dimensional MCEP feature. In voice conversion experiments, using VAE-12 resulted in significantly better perceived speech quality as compared to 40-dimensional MCEPs, with similar speaker accuracy. In habitual to clear style conversion experiments, we significantly improved the speech intelligibility for one of three speakers, using a custom skip-connection deep neural network, with the average keyword recall accuracy increasing from 24% to 46%",
    "checked": true,
    "id": "781a3242e3ec90828c5d262f0db085cdc63dab4c",
    "semantic_title": "using a manifold vocoder for spectral voice and style conversion",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/platen19_interspeech.html": {
    "title": "Multi-Span Acoustic Modelling Using Raw Waveform Signals",
    "volume": "main",
    "abstract": "Traditional automatic speech recognition (ASR) systems often use an acoustic model (AM) built on handcrafted acoustic features, such as log Mel-filter bank (FBANK) values. Recent studies found that AMs with convolutional neural networks (CNNs) can directly use the raw waveform signal as input. Given sufficient training data, these AMs can yield a competitive word error rate (WER) to those built on FBANK features. This paper proposes a novel multi-span structure for acoustic modelling based on the raw waveform with multiple streams of CNN input layers, each processing a different span of the raw waveform signal. Evaluation on both the single channel CHiME4 and AMI data sets show that multi-span AMs give a lower WER than FBANK AMs by an average of about 5% (relative). Analysis of the trained multi-span model reveals that the CNNs can learn filters that are rather different to the log Mel-filters. Furthermore, the paper shows that a widely used single span raw waveform AM can be improved by using a smaller CNN kernel size and increased stride to yield improved WERs",
    "checked": true,
    "id": "6033241c2c55fed10866dacce60726a780a107f6",
    "semantic_title": "multi-span acoustic modelling using raw waveform signals",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/merboldt19_interspeech.html": {
    "title": "An Analysis of Local Monotonic Attention Variants",
    "volume": "main",
    "abstract": "Speech recognition using attention-based models is an effective approach to transcribing audio directly to text within an integrated end-to-end architecture. Global attention approaches compute a weighting over the complete input sequence, whereas local attention mechanisms are restricted to only a localized window of the sequence. For speech, the latter approach supports the monotonicity property of the speech-text alignment. Therefore, we revise several variants of such models and provide a comprehensive comparison, which has been missing so far in the literature. Additionally, we introduce a simple technique to implement windowed attention. This can be applied on top of an existing global attention model. The goal is to transition into a local attention model, by using a local window for the otherwise unchanged attention mechanism, starting from the temporal position with the most recent most active attention energy. We test this method on Switchboard and LibriSpeech and show that the proposed model can even be trained from random initialization and achieve results comparable to the global attention baseline",
    "checked": true,
    "id": "42b5438a084c69065acc436a4237f6c6369abb72",
    "semantic_title": "an analysis of local monotonic attention variants",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sun19b_interspeech.html": {
    "title": "Layer Trajectory BLSTM",
    "volume": "main",
    "abstract": "Recently, we proposed layer trajectory (LT) LSTM (ltLSTM) which significantly outperforms LSTM by decoupling the functions of senone classification and temporal modeling with separate depth and time LSTMs. We further improved ltLSTM with contextual layer trajectory LSTM (cltLSTM) which uses the future context frames to predict target labels. Given bidirectional LSTM (BLSTM) also uses future context frames to improve its modeling power, in this study we first compare the performance between these two models. Then we apply the layer trajectory idea to further improve BLSTM models, in which BLSTM is in charge of modeling the temporal information while depth-LSTM takes care of senone classification. In addition, we also investigate the model performance among different LT component designs on BLSTM models. Trained with 30 thousand hours of EN-US Microsoft internal data, the proposed layer trajectory BLSTM (ltBLSTM) model improved the baseline BLSTM with up to 14.5% relative word error rate (WER) reduction across different tasks",
    "checked": true,
    "id": "90ff16cbc5a161789f0bbd115924fd1195e59e4b",
    "semantic_title": "layer trajectory blstm",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/karita19_interspeech.html": {
    "title": "Improving Transformer-Based End-to-End Speech Recognition with Connectionist Temporal Classification and Language Model Integration",
    "volume": "main",
    "abstract": "The state-of-the-art neural network architecture named Transformer has been used successfully for many sequence-to-sequence transformation tasks. The advantage of this architecture is that it has a fast iteration speed in the training stage because there is no sequential operation as with recurrent neural networks (RNN). However, an RNN is still the best option for end-to-end automatic speech recognition (ASR) tasks in terms of overall training speed (i.e., convergence) and word error rate (WER) because of effective joint training and decoding methods. To realize a faster and more accurate ASR system, we combine Transformer and the advances in RNN-based ASR. In our experiments, we found that the training of Transformer is slower than that of RNN as regards the learning curve and integration with the naive language model (LM) is difficult. To address these problems, we integrate connectionist temporal classification (CTC) with Transformer for joint training and decoding. This approach makes training faster than with RNNs and assists LM integration. Our proposed ASR system realizes significant improvements in various ASR tasks. For example, it reduced the WERs from 11.1% to 4.5% on the Wall Street Journal and from 16.1% to 11.6% on the TED-LIUM by introducing CTC and LM integration into the Transformer baseline",
    "checked": true,
    "id": "ffe1416bcfde82f567dd280975bebcfeb4892298",
    "semantic_title": "improving transformer-based end-to-end speech recognition with connectionist temporal classification and language model integration",
    "citation_count": 172
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19d_interspeech.html": {
    "title": "Trainable Dynamic Subsampling for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "Jointly optimised attention-based encoder-decoder models have yielded impressive speech recognition results. The recurrent neural network (RNN) encoder is a key component in such models — it learns the hidden representations of the inputs. However, it is difficult for RNNs to model the long sequences characteristic of speech recognition. To address this, subsampling between stacked recurrent layers of the encoder is commonly employed. This method reduces the length of the input sequence and leads to gains in accuracy. However, static subsampling may both include redundant information and miss relevant information We propose using a dynamic subsampling RNN (dsRNN) encoder. Unlike a statically subsampled RNN encoder, the dsRNN encoder can learn to skip redundant frames. Furthermore, the skip ratio may vary at different stages of training, thus allowing the encoder to learn the most relevant information for each epoch. Although the dsRNN is unidirectional, it yields lower phone error rates (PERs) than a bidirectional RNN on TIMIT. The dsRNN encoder has a 16.8% PER on the TIMIT test set, a considerable improvement over static subsampling methods used with unidirectional and bidirectional RNN encoders (23.5% and 20.4% PER respectively)",
    "checked": true,
    "id": "9eb0ea96f501fdbfbbe949680a0a9485df39438d",
    "semantic_title": "trainable dynamic subsampling for end-to-end speech recognition",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhao19d_interspeech.html": {
    "title": "Shallow-Fusion End-to-End Contextual Biasing",
    "volume": "main",
    "abstract": "Contextual biasing to a specific domain, including a user's song names, app names and contact names, is an important component of any production-level automatic speech recognition (ASR) system. Contextual biasing is particularly challenging in end-to-end models because these models keep a small list of candidates during beam search, and also do poorly on proper nouns, which is the main source of biasing phrases. In this paper, we present various algorithmic and training improvements to shallow-fusion-based biasing for end-to-end models. We will show that the proposed approach obtains better performance than a state-of-the-art conventional model across a variety of tasks, the first time this has been demonstrated",
    "checked": true,
    "id": "d6c70a2ce72ccc11461860c3a738a1f7ca8d7309",
    "semantic_title": "shallow-fusion end-to-end contextual biasing",
    "citation_count": 99
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nasir19_interspeech.html": {
    "title": "Modeling Interpersonal Linguistic Coordination in Conversations Using Word Mover's Distance",
    "volume": "main",
    "abstract": "Linguistic coordination is a well-established phenomenon in spoken conversations and often associated with positive social behaviors and outcomes. While there have been many attempts to measure lexical coordination or entrainment in literature, only a few have explored coordination in syntactic or semantic space. In this work, we attempt to combine these different aspects of coordination into a single measure by leveraging distances in a neural word representation space. In particular, we adopt the recently proposed Word Mover's Distance with word2vec embeddings and extend it to measure the dissimilarity in language used in multiple consecutive speaker turns. To validate our approach, we apply this measure for two case studies in the clinical psychology domain. We find that our proposed measure is correlated with the therapist's empathy towards their patient in Motivational Interviewing and with affective behaviors in Couples Therapy. In both case studies, our proposed metric exhibits higher correlation than previously proposed measures. When applied to the couples with relationship improvement, we also notice a significant decrease in the proposed measure over the course of therapy, indicating higher linguistic coordination",
    "checked": true,
    "id": "21aa3b117bbfac0580b6d7b415a9b8beeec85ad3",
    "semantic_title": "modeling interpersonal linguistic coordination in conversations using word mover's distance",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/du19_interspeech.html": {
    "title": "Bag-of-Acoustic-Words for Mental Health Assessment: A Deep Autoencoding Approach",
    "volume": "main",
    "abstract": "Despite the recent success of deep learning, it is generally difficult to apply end-to-end deep neural networks to small datasets, such as those from the health domain, due to the tendency of neural networks to over-fit. In addition, how neural models reach their decisions is not well understood. In this paper, we present a two-stage approach to acoustic-based classification of behavior markers related to mental health disorders: first, a dictionary and the mapping from speech signals to the dictionary are learned jointly by a deep autoencoder, then the bag-of-words representation of speech is used for classification, using classifiers with simple decision boundaries. This deep bag-of-features approach has the advantage of offering more interpretability, while the use of deep autoencoder gains improvements in prediction by learning higher level features with long range dependencies, comparing to previous work using only low-level descriptors. In addition, we demonstrate the use of labeled emotion recognition data from other domains to supervise acoustic word encoding in order to help predict psychological traits. Experiments are conducted on audio recordings of 65 clinically recorded interviews with the self-reported level of post-traumatic stress disorder (PTSD), depression, and rapport with the interviewers",
    "checked": true,
    "id": "a48d363f30a465ec92b13328e5ac63db8741efa0",
    "semantic_title": "bag-of-acoustic-words for mental health assessment: a deep autoencoding approach",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/voleti19_interspeech.html": {
    "title": "Objective Assessment of Social Skills Using Automated Language Analysis for Identification of Schizophrenia and Bipolar Disorder",
    "volume": "main",
    "abstract": "Several studies have shown that speech and language features, automatically extracted from clinical interviews or spontaneous discourse, have diagnostic value for mental disorders such as schizophrenia and bipolar disorder. They typically make use of a large feature set to train a classifier for distinguishing between two groups of interest, i.e. a clinical and control group. However, a purely data-driven approach runs the risk of overfitting to a particular data set, especially when sample sizes are limited. Here, we first down-select the set of language features to a small subset that is related to a well-validated test of functional ability, the Social Skills Performance Assessment (SSPA). This helps establish the concurrent validity of the selected features. We use only these features to train a simple classifier to distinguish between groups of interest. Linear regression reveals that a subset of language features can effectively model the SSPA, with a correlation coefficient of 0.75. Furthermore, the same feature set can be used to build a strong binary classifier to distinguish between healthy controls and a clinical group (AUC = 0.96) and also between patients within the clinical group with schizophrenia and bipolar I disorder (AUC = 0.83)",
    "checked": true,
    "id": "83c4354415811ff2b50a5f11f770730873f13f4e",
    "semantic_title": "objective assessment of social skills using automated language analysis for identification of schizophrenia and bipolar disorder",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2019/matton19_interspeech.html": {
    "title": "Into the Wild: Transitioning from Recognizing Mood in Clinical Interactions to Personal Conversations for Individuals with Bipolar Disorder",
    "volume": "main",
    "abstract": "Bipolar Disorder, a mood disorder with recurrent mania and depression, requires ongoing monitoring and specialty management. Current monitoring strategies are clinically-based, engaging highly specialized medical professionals who are becoming increasingly scarce. Automatic speech-based monitoring via smartphones has the potential to augment clinical monitoring by providing inexpensive and unobtrusive measurements of a patient's daily life. The success of such an approach is contingent on the ability to successfully utilize \"in-the-wild\" data. However, most existing work on automatic mood detection uses datasets collected in clinical or laboratory settings. This study presents experiments in automatically detecting depression severity in individuals with Bipolar Disorder using data derived from clinical interviews and from personal conversations. We find that mood assessment is more accurate using data collected from clinical interactions, in part because of their highly structured nature. We demonstrate that although the features that are most effective in clinical interactions do not extend well to personal conversational data, we can identify alternative features relevant in personal conversational speech to detect mood symptom severity. Our results highlight the challenges unique to working with \"in-the-wild\" data, providing insight into the degree to which the predictive ability of speech features is preserved outside of a clinical interview",
    "checked": true,
    "id": "e16e0e4da49639157a4687694b5a55b0c5d8e6a2",
    "semantic_title": "into the wild: transitioning from recognizing mood in clinical interactions to personal conversations for individuals with bipolar disorder",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rohanian19_interspeech.html": {
    "title": "Detecting Depression with Word-Level Multimodal Fusion",
    "volume": "main",
    "abstract": "Semi-structured clinical interviews are frequently used diagnostic tools for identifying depression during an assessment phase. In addition to the lexical content of a patient's responses, multimodal cues concurrent with the responses are indicators of their motor and cognitive state, including those derivable from their voice quality and gestural behaviour. In this paper, we use information from different modalities in order to train a classifier capable of detecting the binary state of a subject (clinically depressed or not), as well as the level of their depression. We propose a model that is able to perform modality fusion incrementally after each word in an utterance using a time-dependent recurrent approach in a deep learning set-up. To mitigate noisy modalities, we utilize fusion gates that control the degree to which the audio or visual modality contributes to the final prediction. Our results show the effectiveness of word-level multimodal fusion, achieving state-of-the-art results in depression detection and outperforming early feature-level and late fusion techniques",
    "checked": true,
    "id": "023f954745e84062c6190f08c9a250fab446fa35",
    "semantic_title": "detecting depression with word-level multimodal fusion",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/espywilson19_interspeech.html": {
    "title": "Assessing Neuromotor Coordination in Depression Using Inverted Vocal Tract Variables",
    "volume": "main",
    "abstract": "Speech articulation is a complex activity that requires finely timed coordination across articulators, i.e., tongue, jaw, lips, and velum. In a depressed state involving psychomotor retardation, this coordination changes and in turn modifies the perceived speech signal. In previous work, we used the correlation structure of formant trajectories as a proxy for articulatory coordination, from which features were derived for predicting the degree of depression. Ideally, however, we seek coordination of the actual articulators using characteristics such as the degree and place of tongue constriction, often referred to as a tract variable (TV). In this paper, applying a novel articulatory inversion process, we investigate the relation between correlation structure of formant tracks versus that of TVs. We show on a pilot depressed/control dataset that, with the same number of variables, TV coordination-based features, although with some characteristics similar to their counterpart, outperform the corresponding formant track correlation features in detection of the depressed state. We speculate on the latent information being captured by TVs that is not present in formants",
    "checked": true,
    "id": "cb0ddfe586fbde3184bbd02dfcfa1e6f84d97798",
    "semantic_title": "assessing neuromotor coordination in depression using inverted vocal tract variables",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2019/paul19b_interspeech.html": {
    "title": "Towards Universal Dialogue Act Tagging for Task-Oriented Dialogues",
    "volume": "main",
    "abstract": "Machine learning approaches for building task-oriented dialogue systems require large conversational datasets with labels to train on. We are interested in building task-oriented dialogue systems from human-human conversations, which may be available in ample amounts in existing customer care center logs or can be collected from crowd workers. Annotating these datasets can be prohibitively expensive. Recently multiple annotated task-oriented human-machine dialogue datasets have been released, however their annotation schema varies across different collections, even for well-defined categories such as dialogue acts (DAs). We propose a Universal DA schema for task-oriented dialogues and align existing annotated datasets with our schema. Our aim is to train a Universal DA tagger (U-DAT) for task-oriented dialogues and use it for tagging human-human conversations. We investigate multiple datasets, propose manual and automated approaches for aligning the different schema, and present results on a target corpus of human-human dialogues. In unsupervised learning experiments we achieve an F1 score of 54.1% on system turns in human-human dialogues. In a semi-supervised setup, the F1 score increases to 57.7% which would otherwise require at least 1.7K manually annotated turns. For new domains, we show further improvements when unlabeled or labeled target domain data is available",
    "checked": true,
    "id": "58265aeb4fb993cdb479c31bed0d9d31f439eee5",
    "semantic_title": "towards universal dialogue act tagging for task-oriented dialogues",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/goel19_interspeech.html": {
    "title": "HyST: A Hybrid Approach for Flexible and Accurate Dialogue State Tracking",
    "volume": "main",
    "abstract": "Recent works on end-to-end trainable neural network based approaches have demonstrated state-of-the-art results on dialogue state tracking. The best performing approaches estimate a probability distribution over all possible slot values. However, these approaches do not scale for large value sets commonly present in real-life applications and are not ideal for tracking slot values that were not observed in the training set. To tackle these issues, candidate-generation-based approaches have been proposed. These approaches estimate a set of values that are possible at each turn based on the conversation history and/or language understanding outputs, and hence enable state tracking over unseen values and large value sets however, they fall short in terms of performance in comparison to the first group. In this work, we analyze the performance of these two alternative dialogue state tracking methods, and present a hybrid approach (HyST) which learns the appropriate method for each slot type. To demonstrate the effectiveness of HyST on a rich-set of slot types, we experiment with the recently released MultiWOZ-2.0 multi-domain, task-oriented dialogue-dataset. Our experiments show that HyST scales to multi-domain applications. Our best performing model results in a relative improvement of 24% and 10% over the previous SOTA and our best baseline respectively",
    "checked": true,
    "id": "242e9605760be04bd4d411c4320df4e6781dbdd8",
    "semantic_title": "hyst: a hybrid approach for flexible and accurate dialogue state tracking",
    "citation_count": 67
  },
  "https://www.isca-speech.org/archive/interspeech_2019/martinek19_interspeech.html": {
    "title": "Multi-Lingual Dialogue Act Recognition with Deep Learning Methods",
    "volume": "main",
    "abstract": "This paper deals with multi-lingual dialogue act (DA) recognition. The proposed approaches are based on deep neural networks and use word2vec embeddings for word representation. Two multi-lingual models are proposed for this task. The first approach uses one general model trained on the embeddings from all available languages. The second method trains the model on a single pivot language and a linear transformation method is used to project other languages onto the pivot language. The popular convolutional neural network and LSTM architectures with different set-ups are used as classifiers. To the best of our knowledge this is the first attempt at multi-lingual DA recognition using neural networks. The multi-lingual models are validated experimentally on two languages from the Verbmobil corpus",
    "checked": true,
    "id": "0037b62d4dbf2af532d5bb59184f0f4ae38c39a2",
    "semantic_title": "multi-lingual dialogue act recognition with deep learning methods",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chao19_interspeech.html": {
    "title": "BERT-DST: Scalable End-to-End Dialogue State Tracking with Bidirectional Encoder Representations from Transformer",
    "volume": "main",
    "abstract": "An important yet rarely tackled problem in dialogue state tracking (DST) is scalability for dynamic ontology ( e.g., movie, restaurant) and unseen slot values. We focus on a specific condition, where the ontology is unknown to the state tracker, but the target slot value (except for none and dontcare), possibly unseen during training, can be found as word segment in the dialogue context. Prior approaches often rely on candidate generation from n-gram enumeration or slot tagger outputs, which can be inefficient or suffer from error propagation. We propose BERT-DST, an end-to-end dialogue state tracker which directly extracts slot values from the dialogue context. We use BERT as dialogue context encoder whose contextualized language representations are suitable for scalable DST to identify slot values from their semantic context. Furthermore, we employ encoder parameter sharing across all slots with two advantages: (1) Number of parameters does not grow linearly with the ontology. (2) Language representation knowledge can be transferred among slots. Empirical evaluation shows BERT-DST with cross-slot parameter sharing outperforms prior work on the benchmark scalable DST datasets Sim-M and Sim-R, and achieves competitive performance on the standard DSTC2 and WOZ 2.0 datasets",
    "checked": true,
    "id": "b39067d7670b9ffc52ac7cd647f6d95dc946f161",
    "semantic_title": "bert-dst: scalable end-to-end dialogue state tracking with bidirectional encoder representations from transformer",
    "citation_count": 87
  },
  "https://www.isca-speech.org/archive/interspeech_2019/griol19_interspeech.html": {
    "title": "Discovering Dialog Rules by Means of an Evolutionary Approach",
    "volume": "main",
    "abstract": "Designing the rules for the dialog management process is one of the most resources-consuming tasks when developing a dialog system. Although statistical approaches to dialog management are becoming mainstream in research and industrial contexts, still many systems are being developed following the rule-based or hybrid paradigms. For example, when developers require deterministic system responses to keep total control on the decisions made by the system, or because the infrastructure employed is designed for rule-based systems using technologies currently used in commercial platforms. In this paper, we propose the use of evolutionary algorithms to automatically obtain the dialog rules that are implicit in a dialog corpus. Our proposal makes it possible to exploit the benefits of statistical approaches to build rule-based systems. Our proposal has been evaluated with a practical spoken dialog system, for which we have automatically obtained a set of fuzzy rules to successfully manage the dialog",
    "checked": true,
    "id": "e44c9758f069124e4107949c9e67e3d127e17ea0",
    "semantic_title": "discovering dialog rules by means of an evolutionary approach",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19c_interspeech.html": {
    "title": "Active Learning for Domain Classification in a Commercial Spoken Personal Assistant",
    "volume": "main",
    "abstract": "We describe a method for selecting relevant new training data for the LSTM-based domain selection component of our personal assistant system. Adding more annotated training data for any ML system typically improves accuracy, but only if it provides examples not already adequately covered in the existing data. However, obtaining, selecting, and labeling relevant data is expensive. This work presents a simple technique that automatically identifies new helpful examples suitable for human annotation. Our experimental results show that the proposed method, compared with random-selection and entropy-based methods, leads to higher accuracy improvements given a fixed annotation budget. Although developed and tested in the setting of a commercial intelligent assistant, the technique is of wider applicability",
    "checked": true,
    "id": "1a643c6a43341b0480e031193bd57b83a8cb0d47",
    "semantic_title": "active learning for domain classification in a commercial spoken personal assistant",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sadjadi19_interspeech.html": {
    "title": "The 2018 NIST Speaker Recognition Evaluation",
    "volume": "main",
    "abstract": "In 2018, the U.S. National Institute of Standards and Technology (NIST) conducted the most recent in an ongoing series of speaker recognition evaluations (SRE). SRE18 was organized in a similar manner to SRE16, focusing on speaker detection over conversational telephony speech (CTS) collected outside north America. SRE18 also featured several new aspects including: two new data domains, namely voice over internet protocol (VoIP) and audio extracted from amateur online videos (AfV), as well as a new language (Tunisian Arabic). A total of 78 organizations (forming 48 teams) from academia and industry participated in SRE18 and submitted 129 valid system outputs under fixed and open training conditions first introduced in SRE16. This paper presents an overview of the evaluation and several analyses of system performance for all primary conditions in SRE18. The evaluation results suggest that 1) speaker recognition on AfV was more challenging than on telephony data, 2) speaker representations (aka embeddings) extracted using end-to-end neural network frameworks were most effective, 3) top performing systems exhibited similar performance, and 4) greatest performance improvements were largely due to data augmentation, use of extended and more complex models for data representation, as well as effective use of the provided development sets",
    "checked": false,
    "id": "04b3aa2a7a4906a1be08a35b47d21e22beeb24a2",
    "semantic_title": "performance analysis of the 2017 nist language recognition evaluation",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2019/villalba19_interspeech.html": {
    "title": "State-of-the-Art Speaker Recognition for Telephone and Video Speech: The JHU-MIT Submission for NIST SRE18",
    "volume": "main",
    "abstract": "We present a condensed description of the joint effort of JHU-CLSP, JHU-HLTCOE, MIT-LL., MIT CSAIL and LSE-EPITA for NIST SRE18. All the developed systems consisted of x-vector/i-vector embeddings with some flavor of PLDA backend. Very deep x-vector architectures — Extended and Factorized TDNN, and ResNets — clearly outperformed shallower x-vectors and i-vectors. The systems were tailored to the video (VAST) or to the telephone (CMN2) condition. The VAST data was challenging, yielding 4 times worse performance than other video based datasets like Speakers in the Wild. We were able to calibrate the VAST data with very few development trials by using careful adaptation and score normalization methods. The VAST primary fusion yielded EER=10.18% and Cprimary=0.431. By improving calibration in post-eval, we reached Cprimary=0.369. In CMN2, we used unsupervised SPLDA adaptation based on agglomerative clustering and score normalization to correct the domain shift between English and Tunisian Arabic models. The CMN2 primary fusion yielded EER=4.5% and Cprimary=0.313. Extended TDNN x-vector was the best single system obtaining EER=11.1% and Cprimary=0.452 in VAST; and 4.95% and 0.354 in CMN2",
    "checked": true,
    "id": "2d198d5209b9f144378ff1f86ce8bfc36249669e",
    "semantic_title": "state-of-the-art speaker recognition for telephone and video speech: the jhu-mit submission for nist sre18",
    "citation_count": 85
  },
  "https://www.isca-speech.org/archive/interspeech_2019/garciaromero19_interspeech.html": {
    "title": "x-Vector DNN Refinement with Full-Length Recordings for Speaker Recognition",
    "volume": "main",
    "abstract": "State-of-the-art text-independent speaker recognition systems for long recordings (a few minutes) are based on deep neural network (DNN) speaker embeddings. Current implementations of this paradigm use short speech segments (a few seconds) to train the DNN. This introduces a mismatch between training and inference when extracting embeddings for long duration recordings. To address this, we present a DNN refinement approach that updates a subset of the DNN parameters with full recordings to reduce this mismatch. At the same time, we also modify the DNN architecture to produce embeddings optimized for cosine distance scoring. This is accomplished using a large-margin strategy with angular softmax. Experimental validation shows that our approach is capable of producing embeddings that achieve record performance on the SITW benchmark",
    "checked": true,
    "id": "ec7ff1cefcd86523f98652150686de7ae1531287",
    "semantic_title": "x-vector dnn refinement with full-length recordings for speaker recognition",
    "citation_count": 41
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lee19_interspeech.html": {
    "title": "I4U Submission to NIST SRE 2018: Leveraging from a Decade of Shared Experiences",
    "volume": "main",
    "abstract": "The I4U consortium was established to facilitate a joint entry to NIST speaker recognition evaluations (SRE). The latest edition of such joint submission was in SRE 2018, in which the I4U submission was among the best-performing systems. SRE'18 also marks the 10-year anniversary of I4U consortium into NIST SRE series of evaluation. The primary objective of the current paper is to summarize the results and lessons learned based on the twelve sub-systems and their fusion submitted to SRE'18. It is also our intention to present a shared view on the advancements, progresses, and major paradigm shifts that we have witnessed as an SRE participant in the past decade from SRE'08 to SRE'18. In this regard, we have seen, among others, a paradigm shift from supervector representation to deep speaker embedding, and a switch of research challenge from channel compensation to domain adaptation",
    "checked": true,
    "id": "f704413b7eb14acdb0a3b231c40be1352b74f3ee",
    "semantic_title": "i4u submission to nist sre 2018: leveraging from a decade of shared experiences",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2019/khoury19_interspeech.html": {
    "title": "Pindrop Labs' Submission to the First Multi-Target Speaker Detection and Identification Challenge",
    "volume": "main",
    "abstract": "This paper summarizes Pindrop Labs' submission to the multi-target speaker detection and identification challenge evaluation (MCE 2018). The MCE challenge is geared towards detecting blacklisted speakers (fraudsters) in the context of call centers. Particularly, it aims to answer the following two questions: Is the speaker of the test utterance on the blacklist? If so, which speaker is it among the blacklisted speakers? While one single system can answer both questions, this work looks at them as two separate tasks: blacklist detection and closed-set identification. The former is addressed using four different systems including probabilistic linear discriminant analysis (PLDA), two deep neural network (DNN) based systems, and a simple system based on cosine similarity and logistic regression. The latter is addressed by combining PLDA and neural network based systems. The proposed system was the best performing system at the challenge on both tasks, reducing the blacklist detection error (Top-S EER) by 31.9% and the identification error (Top-1 EER) by 46.4% over the MCE baseline on the evaluation data",
    "checked": true,
    "id": "351776bc712a12a19197d62c7e744dc1303216d8",
    "semantic_title": "pindrop labs' submission to the first multi-target speaker detection and identification challenge",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/garciaromero19b_interspeech.html": {
    "title": "Speaker Recognition Benchmark Using the CHiME-5 Corpus",
    "volume": "main",
    "abstract": "In this paper, we introduce a speaker recognition benchmark derived from the publicly-available CHiME-5 corpus. Our goal is to foster research that tackles the challenging artifacts introduced by far-field multi-speaker recordings of naturally occurring spoken interactions. The benchmark comprises four tasks that involve enrollment and test conditions with single-speaker and/or multi-speaker recordings. Additionally, it supports performance comparisons between close-talking vs distant/far-field microphone recordings, and single-microphone vs microphone-array approaches. We validate the evaluation design with a single-microphone state-of-the-art DNN speaker recognition and diarization system (that we are making publicly available). The results show that the proposed tasks are very challenging, and can be used to quantify the performance gap due to the degradations present in far-field multi-speaker recordings",
    "checked": true,
    "id": "7e0570f498a5de4f2a861546d4e67ba208f71d12",
    "semantic_title": "speaker recognition benchmark using the chime-5 corpus",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ayllon19_interspeech.html": {
    "title": "Investigating the Effects of Noisy and Reverberant Speech in Text-to-Speech Systems",
    "volume": "main",
    "abstract": "The quality of the voices synthesized by a Text-to-Speech (TTS) system depends on the quality of the training data. In real case scenario of TTS personalization from user's voice recordings, the latter are usually affected by noise and reverberation. Speech enhancement can be useful to clean the corrupted speech but it is necessary to understand the effects that noise and reverberation have on the different statistical models that compose the TTS system. In this work we perform a thorough study of how noise and reverberation impact the acoustic and duration models of the TTS system. We also evaluate the effectiveness of time-frequency masking for cleaning the training data. Objective and subjective evaluations reveal that under normal recording scenarios noise leads to a higher degradation than reverberation in terms of naturalness of the synthesized speech",
    "checked": true,
    "id": "028e72fc9cf7a97ee12854ca4f84fe20fb911f03",
    "semantic_title": "investigating the effects of noisy and reverberant speech in text-to-speech systems",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kuo19_interspeech.html": {
    "title": "Selection and Training Schemes for Improving TTS Voice Built on Found Data",
    "volume": "main",
    "abstract": "This work investigates different selection and training schemes to improve the naturalness of synthesized text-to-speech voices built on found data. The approach outlined in this paper examines the combinations of different metrics to detect and reject segments of training data that can degrade the performance of the system. We conducted a series of objective and subjective experiments on two 24-hour single-speaker corpuses of found data collected from diverse sources. We show that using an even smaller, yet carefully selected, set of data can lead to a text-to-speech system able to generate more natural speech than a system trained on the complete dataset. Moreover, we show that training the system by fine-tuning from the system trained on the whole dataset leads to additional improvement in naturalness by allowing a more aggressive selection of training data",
    "checked": true,
    "id": "68aeae8bef6ad6baaa006d893742118affdcbbfa",
    "semantic_title": "selection and training schemes for improving tts voice built on found data",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/braude19_interspeech.html": {
    "title": "All Together Now: The Living Audio Dataset",
    "volume": "main",
    "abstract": "The ongoing focus in speech technology research on machine learning based approaches leaves the community hungry for data. However, datasets tend to be recorded once and then released, sometimes behind registration requirements or paywalls. In this paper we describe our Living Audio Dataset. The aim is to provide audio data that is in the public domain, multilingual, and expandable by communities. We discuss the role of linguistic resources, given the success of systems such as Tacotron which use direct text-to-speech mappings, and consider how data provenance could be built into such resources. So far the data has been collected for TTS purposes, however, it is also suitable for ASR. At the time of publication audio resources already exist for Dutch, R.P. English, Irish, and Russian",
    "checked": true,
    "id": "3626c3db896fb70e7812f12c0acbd51aae427156",
    "semantic_title": "all together now: the living audio dataset",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zen19_interspeech.html": {
    "title": "LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech",
    "volume": "main",
    "abstract": "This paper introduces a new speech corpus called \"LibriTTS\" designed for text-to-speech use. It is derived from the original audio and text materials of the LibriSpeech corpus, which has been used for training and evaluating automatic speech recognition systems. The new corpus inherits desired properties of the LibriSpeech corpus while addressing a number of issues which make LibriSpeech less than ideal for text-to-speech work. The released corpus consists of 585 hours of speech data at 24kHz sampling rate from 2,456 speakers and the corresponding texts. Experimental results show that neural end-to-end TTS models trained from the LibriTTS corpus achieved above 4.0 in mean opinion scores in naturalness in five out of six evaluation speakers. The corpus is freely available for download from http://www.openslr.org/60/",
    "checked": true,
    "id": "2789b6c84ba1422746246685001accba5563e7c1",
    "semantic_title": "libritts: a corpus derived from librispeech for text-to-speech",
    "citation_count": 440
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shamsi19_interspeech.html": {
    "title": "Corpus Design Using Convolutional Auto-Encoder Embeddings for Audio-Book Synthesis",
    "volume": "main",
    "abstract": "In this study, we propose an approach for script selection in order to design TTS speech corpora. A Deep Convolutional Neural Network (DCNN) is used to project linguistic information to an embedding space. The embedded representation of the corpus is then fed to a selection process to extract a subset of utterances which offers a good linguistic coverage while tending to limit the linguistic unit repetition. We present two selection processes: a clustering approach based on utterance distance and another method that tends to reach a target distribution of linguistic events. We compare the synthetic signal quality of the proposed methods to state of art methods objectively and subjectively. The subjective and objective measures confirm the performance of the proposed methods in order to design speech corpora with better synthetic speech quality. The perceptual test shows that our TTS global cost can be used as an alternative to synthetic overall quality",
    "checked": true,
    "id": "682b9950dc65ca8aaba7034890ac7e75c8ebdd8c",
    "semantic_title": "corpus design using convolutional auto-encoder embeddings for audio-book synthesis",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hojo19_interspeech.html": {
    "title": "Evaluating Intention Communication by TTS Using Explicit Definitions of Illocutionary Act Performance",
    "volume": "main",
    "abstract": "Text-to-speech (TTS) synthesis systems have been evaluated with respect to attributes such as quality, naturalness and intelligibility. However, an evaluation protocol with respect to communication of intentions has not yet been established. Evaluating this sometimes produce unreliable results because participants can misinterpret definitions of intentions. This misinterpretation is caused by the colloquial and implicit description of intentions. To address this problem, this work explicitly defines each intention following theoretical definitions, \"felicity conditions\", in speech-act theory. We define the communication of each intention with one to four necessary and sufficient conditions to be satisfied. In listening tests, participants rated whether each condition was satisfied or not. We compared the proposed protocol with the conventional baseline using four different voice conditions; neutral TTS, conversational TTS w/ and w/o intention inputs, and recorded speech. The experimental results with 10 participants showed that the proposed protocol produced smaller within-group variation and larger between-group variation. These results indicate that the proposed protocol can be used to evaluate intention communication with higher inter-rater reliability and sensitivity",
    "checked": true,
    "id": "67899ac5f3ab6ca862a09d7588e24d60e895f32a",
    "semantic_title": "evaluating intention communication by tts using explicit definitions of illocutionary act performance",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lo19_interspeech.html": {
    "title": "MOSNet: Deep Learning-Based Objective Assessment for Voice Conversion",
    "volume": "main",
    "abstract": "Existing objective evaluation metrics for voice conversion (VC) are not always correlated with human perception. Therefore, training VC models with such criteria may not effectively improve naturalness and similarity of converted speech. In this paper, we propose deep learning-based assessment models to predict human ratings of converted speech. We adopt the convolutional and recurrent neural network models to build a mean opinion score (MOS) predictor, termed as MOSNet. The proposed models are tested on large-scale listening test results of the Voice Conversion Challenge (VCC) 2018. Experimental results show that the predicted scores of the proposed MOSNet are highly correlated with human MOS ratings at the system level while being fairly correlated with human MOS ratings at the utterance level. Meanwhile, we have modified MOSNet to predict the similarity scores, and the preliminary results show that the predicted scores are also fairly correlated with human ratings. These results confirm that the proposed models could be used as a computational evaluator to measure the MOS of VC systems to reduce the need for expensive human rating",
    "checked": false,
    "id": "f45b6ecb84bcae0aec420f7491578f6297fbc81a",
    "semantic_title": "mosnet: deep learning based objective assessment for voice conversion",
    "citation_count": 155
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fong19_interspeech.html": {
    "title": "Investigating the Robustness of Sequence-to-Sequence Text-to-Speech Models to Imperfectly-Transcribed Training Data",
    "volume": "main",
    "abstract": "Sequence-to-sequence (S2S) text-to-speech (TTS) models can synthesise high quality speech when large amounts of annotated training data are available. Transcription errors exist in all data and are especially prevalent in found data such as audiobooks. In previous generations of TTS technology, alignment using Hidden Markov Models (HMMs) was widely used to identify and eliminate bad data. In S2S models, the use of attention replaces HMM-based alignment, and there is no explicit mechanism for removing bad data. It is not yet understood how such models deal with transcription errors in the training data We evaluate the quality of speech from S2S-TTS models when trained on data with imperfect transcripts, simulated using corruption, or provided by an Automatic Speech Recogniser (ASR).We find that attention can skip over extraneous words in the input sequence, providing robustness to insertion errors. But substitutions and deletions pose a problem because there is no ground truth input available to align to the ground truth acoustics during teacher-forced training. We conclude that S2S-TTS systems are only partially robust to training on imperfectly-transcribed data and further work is needed",
    "checked": true,
    "id": "e1b1d2a54928681d7fc0e1e749e7dab540766101",
    "semantic_title": "investigating the robustness of sequence-to-sequence text-to-speech models to imperfectly-transcribed training data",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/govender19_interspeech.html": {
    "title": "Using Pupil Dilation to Measure Cognitive Load When Listening to Text-to-Speech in Quiet and in Noise",
    "volume": "main",
    "abstract": "With increased use of text-to-speech (TTS) systems in real-world applications, evaluating how such systems influence the human cognitive processing system becomes important. Particularly in situations where cognitive load is high, there may be negative implications such as fatigue. For example, noisy situations generally require the listener to exert increased mental effort. A better understanding of this could eventually suggest new ways of generating synthetic speech that demands low cognitive load. In our previous study, pupil dilation was used as an index of cognitive effort. Pupil dilation was shown to be sensitive to the quality of synthetic speech, but there were some uncertainties regarding exactly what was being measured. The current study resolves some of those uncertainties. Additionally, we investigate how the pupil dilates when listening to synthetic speech in the presence of speech-shaped noise. Our results show that, in quiet listening conditions, pupil dilation does not reflect listening effort but rather attention and engagement. In noisy conditions, increased pupil dilation indicates that listening effort increases as signal-to-noise ratio decreases, under all conditions tested",
    "checked": true,
    "id": "97e3f3a3dfef82a64dad34c85f1399f3ef534c45",
    "semantic_title": "using pupil dilation to measure cognitive load when listening to text-to-speech in quiet and in noise",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/douros19b_interspeech.html": {
    "title": "A Multimodal Real-Time MRI Articulatory Corpus of French for Speech Research",
    "volume": "main",
    "abstract": "In this work we describe the creation of ArtSpeechMRIfr: a real-time as well as static magnetic resonance imaging (rtMRI, 3D MRI) database of the vocal tract. The database contains also processed data: denoised audio, its phonetically aligned annotation, articulatory contours, and vocal tract volume information, which provides a rich resource for speech research. The database is built on data from two male speakers of French It covers a number of phonetic contexts in the controlled part, as well as spontaneous speech, 3D MRI scans of sustained vocalic articulations, and of the dental casts of the subjects. The corpus for rtMRI consists of 79 synthetic sentences constructed from a phonetized dictionary that makes possible to shorten the duration of acquisitions while keeping a very good coverage of the phonetic contexts which exist in French. The 3D MRI includes acquisitions for 12 French vowels and 10 consonants, each of which was pronounced in several vocalic contexts. Articulatory contours (tongue, jaw, epiglottis, larynx, velum, lips) as well as 3D volumes were manually drawn for a part of the images",
    "checked": true,
    "id": "a6504d175847081380e25a488bed86e109053c03",
    "semantic_title": "a multimodal real-time mri articulatory corpus of french for speech research",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19d_interspeech.html": {
    "title": "A Chinese Dataset for Identifying Speakers in Novels",
    "volume": "main",
    "abstract": "Identifying speakers in novels aims at determining who says a quote in a given context by text analysis. This task is important for speech synthesis systems to assign appropriate voices to the quotes when producing audio books. Several English datasets have been constructed for this task. However, the difference between English and Chinese impedes processing Chinese novels using the models built on English datasets directly. Therefore, this paper presents a Chinese dataset, which contains 2,548 quotes from World of Plainness, a famous Chinese novel, with manually labelled speaker identities. Furthermore, two baseline speaker identification methods, i.e., a rule-based one and a classifier-based one, are designed and experimented using this Chinese dataset. These two methods achieve accuracies of 53.77% and 58.66% respectively on the test set",
    "checked": true,
    "id": "1e2a1ebd623eaa0807cd1f82c69d0ce60ee77eec",
    "semantic_title": "a chinese dataset for identifying speakers in novels",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/park19c_interspeech.html": {
    "title": "CSS10: A Collection of Single Speaker Speech Datasets for 10 Languages",
    "volume": "main",
    "abstract": "We describe our development of CSS10, a collection of single speaker speech datasets for ten languages. It is composed of short audio clips from LibriVox audiobooks and their aligned texts. To validate its quality we train two neural text-to-speech models on each dataset. Subsequently, we conduct Mean Opinion Score tests on the synthesized speech samples. We make our datasets, pre-trained models, and test resources publicly available. We hope they will be used for future speech tasks",
    "checked": true,
    "id": "ff7f3e65542c323056febe24bc17ea094fa37e4f",
    "semantic_title": "css10: a collection of single speaker speech datasets for 10 languages",
    "citation_count": 69
  },
  "https://www.isca-speech.org/archive/interspeech_2019/karaulov19_interspeech.html": {
    "title": "Attention Model for Articulatory Features Detection",
    "volume": "main",
    "abstract": "Articulatory distinctive features, as well as phonetic transcription, play important role in speech-related tasks: computer-assisted pronunciation training, text-to-speech conversion (TTS), studying speech production mechanisms, speech recognition for low-resourced languages. End-to-end approaches to speech-related tasks got a lot of traction in recent years. We apply Listen, Attend and Spell (LAS) [1] architecture to phones recognition on a small small training set, like TIMIT [2]. Also, we introduce a novel decoding technique that allows to train manners and places of articulation detectors end-to-end using attention models. We also explore joint phones recognition and articulatory features detection in multitask learning setting",
    "checked": true,
    "id": "2c400ac71a6a3ae182474a0ed1bc4d9585285447",
    "semantic_title": "attention model for articulatory features detection",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tong19_interspeech.html": {
    "title": "Unbiased Semi-Supervised LF-MMI Training Using Dropout",
    "volume": "main",
    "abstract": "The lattice-free MMI objective (LF-MMI) with finite-state transducer (FST) supervision lattice has been used in semi-supervised training of state-of-the-art neural network acoustic models for automatic speech recognition (ASR). However, the FST based supervision lattice does not sample from the posterior predictive distribution of word-sequences but only contains the decoding hypotheses corresponding to the Maximum Likelihood estimate of weights, so that the training might be biased towards incorrect hypotheses in the supervision lattice even if the best path is perfectly correct. In this paper, we propose a novel framework which uses Dropout at the test time to sample from the posterior predictive distribution of word-sequences to produce unbiased supervision lattices for semi-supervised training. We investigate the dropout sampling from both the acoustic model and the language model to generate supervision. Results on Fisher English show that the proposed approach achieves WER recovery of ~51.6% over regular semi-supervised LF-MMI training",
    "checked": true,
    "id": "07aa6200b01cfa6569ed063e653a05fa3d0044fe",
    "semantic_title": "unbiased semi-supervised lf-mmi training using dropout",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cui19_interspeech.html": {
    "title": "Acoustic Model Optimization Based on Evolutionary Stochastic Gradient Descent with Anchors for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Evolutionary stochastic gradient descent (ESGD) was proposed as a population-based approach that combines the merits of gradient-aware and gradient-free optimization algorithms for superior overall optimization performance. In this paper we investigate a variant of ESGD for optimization of acoustic models for automatic speech recognition (ASR). In this variant, we assume the existence of a well-trained acoustic model and use it as an anchor in the parent population whose good \"gene\" will prorogate in the evolution to the offsprings. We propose an ESGD algorithm leveraging the anchor models such that it guarantees the best fitness of the population will never degrade from the anchor model. Experiments on 50-hour Broadcast News (BN50) and 300-hour Switchboard (SWB300) show that the ESGD with anchors can further improve the loss and ASR performance over the existing well-trained acoustic models",
    "checked": true,
    "id": "2db65daf64a9285edc08b7237825c0f4d9c8c772",
    "semantic_title": "acoustic model optimization based on evolutionary stochastic gradient descent with anchors for automatic speech recognition",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shah19b_interspeech.html": {
    "title": "Whether to Pretrain DNN or not?: An Empirical Analysis for Voice Conversion",
    "volume": "main",
    "abstract": "Recently, Deep Neural Network (DNN)-based Voice Conversion (VC) techniques have become popular in the VC literature. These techniques suffer from the issue of overfitting due to less amount of available training data from a target speaker. To alleviate this, pre-training is used for better initialization of the DNN parameters, which leads to faster convergence of parameters. Greedy layerwise pre-training of the stacked Restricted Boltzmann Machine (RBM) or the stacked De-noising AutoEncoder (DAE) is used with extra available speaker-pairs‘ data. This pre-training is time-consuming and requires a separate network to learn the parameters of the network. In this work, we propose to analyze the DNN training strategies for the VC task, specifically with and without pre-training. In particular, we investigate whether an extra pre-training step could be avoided by using recent advances in deep learning. The VC experiments were performed on two VC Challenge (VCC) databases 2016 and 2018. Objective and subjective tests show that DNN trained with Adam optimization and Exponential Linear Unit (ELU) performed comparable or better than the pre-trained DNN without compromising on speech quality and speaker similarity of the converted voices",
    "checked": true,
    "id": "c118d2c5b35025eae7ec72a03f19bb939b385e53",
    "semantic_title": "whether to pretrain dnn or not?: an empirical analysis for voice conversion",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/goyal19_interspeech.html": {
    "title": "Detection of Glottal Closure Instants from Raw Speech Using Convolutional Neural Networks",
    "volume": "main",
    "abstract": "Glottal Closure Instants (GCIs) correspond to the temporal locations of significant excitation to the vocal tract occurring during the production of voiced speech. GCI detection from speech signals is a well-studied problem given its importance in speech processing. Most of the existing approaches for GCI detection adopt a two-stage approach (i) Transformation of speech signal into a representative signal where GCIs are localized better, (ii) extraction of GCIs using the representative signal obtained in first stage. The former stage is accomplished using signal processing techniques based on the principles of speech production and the latter with heuristic-algorithms such as dynamic-programming and peak-picking. These methods are thus task-specific and rely on the methods used for representative signal extraction. However in this paper, we formulate the GCI detection problem from a representation learning perspective where appropriate representation is implicitly learned from the raw-speech data samples. Specifically, GCI detection is cast as a supervised multi-task learning problem solved using a deep convolutional neural network jointly optimizing a classification and regression cost. The learning capability is demonstrated with several experiments on standard datasets. The results compare well with the state-of- the-art algorithms while performing better in the case of presence of real-world non-stationary noise",
    "checked": true,
    "id": "434861fc575dba916e56a719ae977b00dc53a72a",
    "semantic_title": "detection of glottal closure instants from raw speech using convolutional neural networks",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fainberg19_interspeech.html": {
    "title": "Lattice-Based Lightly-Supervised Acoustic Model Training",
    "volume": "main",
    "abstract": "In the broadcast domain there is an abundance of related text data and partial transcriptions, such as closed captions and subtitles. This text data can be used for lightly supervised training, in which text matching the audio is selected using an existing speech recognition model. Current approaches to light supervision typically filter the data based on matching error rates between the transcriptions and biased decoding hypotheses. In contrast, semi-supervised training does not require matching text data, instead generating a hypothesis using a background language model. State-of-the-art semi-supervised training uses lattice-based supervision with the lattice-free MMI (LF-MMI) objective function. We propose a technique to combine inaccurate transcriptions with the lattices generated for semi-supervised training, thus preserving uncertainty in the lattice where appropriate. We demonstrate that this combined approach reduces the expected error rates over the lattices, and reduces the word error rate (WER) on a broadcast task",
    "checked": true,
    "id": "ec1e685d48d337a1f8abe8bbbae5f9da3856d19e",
    "semantic_title": "lattice-based lightly-supervised acoustic model training",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/michel19_interspeech.html": {
    "title": "Comparison of Lattice-Free and Lattice-Based Sequence Discriminative Training Criteria for LVCSR",
    "volume": "main",
    "abstract": "Sequence discriminative training criteria have long been a standard tool in automatic speech recognition for improving the performance of acoustic models over their maximum likelihood / cross entropy trained counterparts. While previously a lattice approximation of the search space has been necessary to reduce computational complexity, recently proposed methods use other approximations to dispense of the need for the computationally expensive step of separate lattice creation In this work we present a memory efficient implementation of the forward-backward computation that allows us to use unigram word-level language models in the denominator calculation while still doing a full summation on GPU. This allows for a direct comparison of lattice-based and lattice-free sequence discriminative training criteria such as MMI and sMBR, both using the same language model during training We compared performance, speed of convergence, and stability on large vocabulary continuous speech recognition tasks like Switchboard and Quaero. We found that silence modeling seriously impacts the performance in the lattice-free case and needs special treatment. In our experiments lattice-free MMI comes on par with its lattice-based counterpart. Lattice-based sMBR still outperforms all lattice-free training criteria",
    "checked": true,
    "id": "11f9664eb42cdbe6f434aea3381458d4055328f6",
    "semantic_title": "comparison of lattice-free and lattice-based sequence discriminative training criteria for lvcsr",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/masumura19b_interspeech.html": {
    "title": "End-to-End Automatic Speech Recognition with a Reconstruction Criterion Using Speech-to-Text and Text-to-Speech Encoder-Decoders",
    "volume": "main",
    "abstract": "In this paper, we present a novel end-to-end automatic speech recognition (ASR) method that considers whether an input speech can be reconstructed from a generated text or not. A speech-to-text encoder-decoder model is one of the most powerful end-to-end ASR methods since it does not make any conditional independence assumptions. However, encoder-decoder models often suffer from a problem that is caused from a gap between the teacher forcing in a training phase and the free running in a testing phase. In fact, there is no guarantee that texts can be generated correctly when some generation errors occur in conditioning contexts. In order to mitigate this problem, our proposed method utilizes not only a generation probability of the text computed from a speech-to-text encoder-decoder but also a reconstruction probability of the speech computed from a text-to-speech encoder-decoder on the basis of a maximum mutual information criterion. We can expect that considering the reconstruction criterion can impose a constraint against generation errors. In addition, in order to compute the reconstruction probability, we introduce a mixture density network into the text-to-speech encoder-decoder. Our experiments on Japanese lecture ASR tasks demonstrate that considering the reconstruction criterion can yield ASR performance improvements",
    "checked": true,
    "id": "6f4bc173bb9e965e592c4df9e911216afb416877",
    "semantic_title": "end-to-end automatic speech recognition with a reconstruction criterion using speech-to-text and text-to-speech encoder-decoders",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/heba19_interspeech.html": {
    "title": "Char+CV-CTC: Combining Graphemes and Consonant/Vowel Units for CTC-Based ASR Using Multitask Learning",
    "volume": "main",
    "abstract": "Previous work has shown that end-to-end neural-based speech recognition systems can be improved by adding auxiliary tasks at intermediate layers. In this paper, we report multitask learning (MTL) experiments in the context of connectionist temporal classification (CTC) based speech recognition at character level. We compare several MTL architectures that jointly learn to predict characters (sometimes called graphemes) and consonant/vowel (CV) binary labels. The best approach, which we call Char+CV-CTC, adds up the character and CV logits to obtain the final character predictions. The idea is to put more weight on the vowel (consonant) characters when the vowel (consonant) symbol ‘V' (‘C') is predicted in the auxiliary-task branch of the network. Experiments were carried out on the Wall Street Journal (WSJ) corpus. Char+CV-CTC achieved the best ASR results with a 2.2% Character Error Rate and a 6.1% Word Error Rate (WER) on the Eval92 evaluation subset. This model outperformed its monotask model counterpart by 0.7% absolute in WER and also achieved almost the same performance of 6.0% as a strong baseline phone-based Time Delay Neural Network (\"TDNN-Phone+TR2\") model",
    "checked": true,
    "id": "94cb5163d3e0c04459d46f56d2e363960987cb88",
    "semantic_title": "char+cv-ctc: combining graphemes and consonant/vowel units for ctc-based asr using multitask learning",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kurata19_interspeech.html": {
    "title": "Guiding CTC Posterior Spike Timings for Improved Posterior Fusion and Knowledge Distillation",
    "volume": "main",
    "abstract": "Conventional automatic speech recognition (ASR) systems trained from frame-level alignments can easily leverage posterior fusion to improve ASR accuracy and build a better single model with knowledge distillation. End-to-end ASR systems trained using the Connectionist Temporal Classification (CTC) loss do not require frame-level alignment and hence simplify model training. However, sparse and arbitrary posterior spike timings from CTC models pose a new set of challenges in posterior fusion from multiple models and knowledge distillation between CTC models. We propose a method to train a CTC model so that its spike timings are guided to align with those of a pre-trained guiding CTC model. As a result, all models that share the same guiding model have aligned spike timings. We show the advantage of our method in various scenarios including posterior fusion of CTC models and knowledge distillation between CTC models with different architectures. With the 300-hour Switchboard training data, the single word CTC model distilled from multiple models improved the word error rates to 13.7%/23.1% from 14.9%/24.1% on the Hub5 2000 Switchboard/CallHome test sets without using any data augmentation, language model, or complex decoder",
    "checked": true,
    "id": "cbfc99c47296ee1f958f5860d07587bf190a9cfb",
    "semantic_title": "guiding ctc posterior spike timings for improved posterior fusion and knowledge distillation",
    "citation_count": 39
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fukuda19_interspeech.html": {
    "title": "Direct Neuron-Wise Fusion of Cognate Neural Networks",
    "volume": "main",
    "abstract": "This paper proposes a method to create a robust acoustic model by directly fusing multiple neural networks that have dissimilar characteristics without any additional layers/nodes involving retraining procedures. The fused neural networks derive from a shared parent neural network and are referred to as cognate (child) neural networks in this paper. The neural networks are fused by interpolating weight and bias parameters associated with each neuron with a different fusion weight, assuming that cognate neural networks to be fused have the same topology. Therefore, no extra computational cost during decoding is required. The fusion weight is determined by considering a cosine similarity estimated from parameters connecting to the neuron and the fusion is performed for every neuron. Experiments were carried out using a test suite consisting of various acoustic conditions with a wide SNR range, speakers including foreign accented speakers, and speaking styles. From the experiments, the network created by fusing cognate neural networks showed consistent improvement on average compared with the commercial-grade domain-free network originating from the parent model. In addition, we demonstrate that the fusion considering input connections to the neuron achieves the highest accuracy in our experiments",
    "checked": true,
    "id": "950621a7899811bf7d1dd167667a5dd87b8db541",
    "semantic_title": "direct neuron-wise fusion of cognate neural networks",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ladkat19_interspeech.html": {
    "title": "Two Tiered Distributed Training Algorithm for Acoustic Modeling",
    "volume": "main",
    "abstract": "We present a hybrid approach for scaling distributed training of neural networks by combining Gradient Threshold Compression (GTC) algorithm — a variant of stochastic gradient descent (SGD) — which compresses gradients with thresholding and quantization techniques and Blockwise Model Update Filtering (BMUF) algorithm — a variant of model averaging (MA). In this proposed method, we divide total number of workers into smaller subgroups in a hierarchical manner and limit frequent communication across subgroups. We update local model using GTC within a subgroup and global model using BMUF across different subgroups. We evaluate this approach in an Automatic Speech Recognition (ASR) task, by training deep long short-term memory (LSTM) acoustic models on 2000 hours of speech. Experiments show that, for a wide range in the number of GPUs used for distributed training, the proposed approach achieves a better trade-off between accuracy and scalability compared to GTC and BMUF",
    "checked": true,
    "id": "00eb80de2f38a6dd9a31e934e82d986d5ea2408a",
    "semantic_title": "two tiered distributed training algorithm for acoustic modeling",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19f_interspeech.html": {
    "title": "Exploring the Encoder Layers of Discriminative Autoencoders for LVCSR",
    "volume": "main",
    "abstract": "Discriminative autoencoders (DcAEs) have been proven to improve generalization of the learned acoustic models by increasing their reconstruction capacity of input features from the frame embeddings. In this paper, we integrate DcAEs into two models, namely TDNNs and LSTMs, which have been commonly adopted in the Kaldi recipes for LVCSR in recent years, using the modified nnet3 neural network library. We also explore two kinds of skip-connection mechanisms for DcAEs, namely concatenation and addition. The results of LVCSR experiments on the MATBN Mandarin Chinese corpus and the WSJ English corpus show that the proposed DcAE-TDNN-based system achieves relative word error rate reductions of 3% and 10% over the TDNN-based baseline system, respectively. The DcAE-TDNN-LSTM-based system also outperforms the TDNN-LSTM-based baseline system. The results imply the flexibility of DcAEs to be integrated with other existing or prospective neural network-based acoustic models",
    "checked": true,
    "id": "2c379326e66abe8879d21a1dd0a1f7d5a2eb8843",
    "semantic_title": "exploring the encoder layers of discriminative autoencoders for lvcsr",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kurata19b_interspeech.html": {
    "title": "Multi-Task CTC Training with Auxiliary Feature Reconstruction for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "We present a multi-task Connectionist Temporal Classification (CTC) training for end-to-end (E2E) automatic speech recognition with input feature reconstruction as an auxiliary task. Whereas the main task of E2E CTC training and the auxiliary reconstruction task share the encoder network, the auxiliary task tries to reconstruct the input feature from the encoded information. In addition to standard feature reconstruction, we distort the input feature only in the auxiliary reconstruction task, such as (1) swapping the former and latter parts of an utterance, or (2) using a part of an utterance by stripping the beginning or end parts. These distortions intentionally suppress long-span dependencies in the time domain, which avoids overfitting to the training data. We trained phone-based CTC and word-based CTC models with the proposed multi-task learning and demonstrated that it improves ASR accuracy on various test sets that are matched and unmatched with the training data",
    "checked": true,
    "id": "b8e3658a4940df5528bda0d3aaa4ad8650184be9",
    "semantic_title": "multi-task ctc training with auxiliary feature reconstruction for end-to-end speech recognition",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19e_interspeech.html": {
    "title": "Framewise Supervised Training Towards End-to-End Speech Recognition Models: First Results",
    "volume": "main",
    "abstract": "Recurrent neural networks (RNNs) trained with connectionist temporal classification (CTC) technique have delivered promising results in many speech recognition tasks. However, the forward-backward algorithm that CTC takes for model optimization requires a huge amount of computation. This paper introduces a new training method towards RNN-based end-to-end models, which significantly saves computing power without losing accuracy. Unlike CTC, the label sequence is aligned to the labelling hypothesis and then to the input sequence by the Weighted Minimum Edit-Distance Aligning (WMEDA) algorithm. Based on the alignment, the framewise supervised training is conducted. Moreover, Pronunciation Embedding (PE), the acoustic representation towards a linguistic target, is proposed in order to calculate the weights in WMEDA algorithm. The model is evaluated on TIMIT and AIShell-1 datasets for English phoneme and Chinese character recognitions. For TIMIT, the model achieves a comparable 18.57% PER to the 18.4% PER of the CTC baseline. As for AIShell-1, a joint Pinyin-character model is trained, giving a 19.38% CER, which is slightly better than the 19.43% CER obtained by the CTC character model, and the training time of this model is only 54.3% of the CTC model's",
    "checked": true,
    "id": "904be254fe5173050260a72b438b2693f475b261",
    "semantic_title": "framewise supervised training towards end-to-end speech recognition models: first results",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/georgiou19_interspeech.html": {
    "title": "Deep Hierarchical Fusion with Application in Sentiment Analysis",
    "volume": "main",
    "abstract": "Recognizing the emotional tone in spoken language is a challenging research problem that requires modeling not only the acoustic and textual modalities separately but also their cross-interactions. In this work, we introduce a hierarchical fusion scheme for sentiment analysis of spoken sentences. Two bidirectional Long-Short-Term-Memory networks (BiLSTM), followed by multiple fully connected layers, are trained in order to extract feature representations for each of the textual and audio modalities. The representations of the unimodal encoders are both fused at each layer and propagated forward, thus achieving fusion at the word, sentence and high/sentiment levels. The proposed approach of deep hierarchical fusion achieves state-of-the-art results for sentiment analysis tasks. Through an ablation study, we show that the proposed fusion method achieves greater performance gains over the unimodal baseline compared to other fusion approaches in the literature",
    "checked": true,
    "id": "9783154a3790a988b50c763af322b5086fc5ae8a",
    "semantic_title": "deep hierarchical fusion with application in sentiment analysis",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mitra19_interspeech.html": {
    "title": "Leveraging Acoustic Cues and Paralinguistic Embeddings to Detect Expression from Voice",
    "volume": "main",
    "abstract": "Millions of people reach out to digital assistants such as Siri every day, asking for information, making phone calls, seeking assistance, and much more. The expectation is that such assistants should understand the intent of the user's query. Detecting the intent of a query from a short, isolated utterance is a difficult task. Intent cannot always be obtained from speech-recognized transcriptions. A transcription-driven approach can interpret what has been said but fails to acknowledge how it has been said, and as a consequence, may ignore the expression present in the voice. Our work investigates whether a system can reliably detect vocal expression in queries using acoustic and paralinguistic embedding. Results show that the proposed method offers a relative equal error rate (EER) decrease of 60% compared to a bag-of-word based system, corroborating that expression is significantly represented by vocal attributes, rather than being purely lexical. Addition of emotion embedding helped to reduce the EER by 30% relative to the acoustic embedding, demonstrating the relevance of emotion in expressive voice",
    "checked": true,
    "id": "4976a2ee115624ca0c06f5bd7b07faa6fbb903fd",
    "semantic_title": "leveraging acoustic cues and paralinguistic embeddings to detect expression from voice",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/parry19_interspeech.html": {
    "title": "Analysis of Deep Learning Architectures for Cross-Corpus Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Speech Emotion Recognition (SER) is an important and challenging task for human-computer interaction. In the literature deep learning architectures have been shown to yield state-of-the-art performance on this task when the model is trained and evaluated on the same corpus. However, prior work has indicated that such systems often yield poor performance on unseen data. To improve the generalisation capabilities of emotion recognition systems one possible approach is cross-corpus training, which consists of training the model on an aggregation of different corpora. In this paper we present an analysis of the generalisation capability of deep learning models using cross-corpus training with six different speech emotion corpora. We evaluate the models on an unseen corpus and analyse the learned representations using the t-SNE algorithm, showing that architectures based on recurrent neural networks are prone to overfit the corpora present in the training set, while architectures based on convolutional neural networks (CNNs) show better generalisation capabilities. These findings indicate that (1) cross-corpus training is a promising approach for improving generalisation and (2) CNNs should be the architecture of choice for this approach",
    "checked": true,
    "id": "001456851ed0a4d10cf0ac71e7b5fb1f2359181c",
    "semantic_title": "analysis of deep learning architectures for cross-corpus speech emotion recognition",
    "citation_count": 44
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19e_interspeech.html": {
    "title": "A Path Signature Approach for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Automatic speech emotion recognition (SER) remains a difficult task within human-computer interaction, despite increasing interest in the research community. One key challenge is how to effectively integrate short-term characterisation of speech segments with long-term information such as temporal variations. Motivated by the numerical approximation theory of stochastic differential equations (SDEs), we propose the novel use of path signatures. The latter provide a pathwise definition to solve SDEs, for the integration of short speech frames. Furthermore we propose a hierarchical tree structure of path signatures, to capture both global and local information. A simple tree-based convolutional neural network (TBCNN) is used for learning the structural information stemming from dyadic path-tree signatures. Our experimental results on a widely used benchmark dataset demonstrate comparable performance to complex neural network based systems",
    "checked": true,
    "id": "cf5305c414b2ec2c1e7651e411a17a31b3c393e2",
    "semantic_title": "a path signature approach for speech emotion recognition",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2019/egorow19_interspeech.html": {
    "title": "Employing Bottleneck and Convolutional Features for Speech-Based Physical Load Detection on Limited Data Amounts",
    "volume": "main",
    "abstract": "The detection of different levels of physical load from speech has many applications: Besides telemedicine, non-contact detection of certain heart rate ranges can be useful for sports and other leisure time devices. Available approaches mainly use a high number of spectral and prosodic features. In this setting of typically small data sets, such as the Talk & Run data set and the Munich Biovoice Corpus, the high-dimensional feature spaces are only sparsely populated. Therefore, we aim at a reduction of the feature number using modern neural net inspired features: Bottleneck layer features, obtained from standard low-level descriptors via a feed-forward neural network, and activation map features, obtained from spectrograms via a convolutional neural network. We use these features for an SVM classification of high and low physical load and compare their performance. We also discuss the possibility of hyperparameter transfer of the extracting networks between different data sets. We show that even for limited amounts of data, deep learning based methods can bring a substantial improvement over \"conventional\" features",
    "checked": true,
    "id": "d6fb2d82fb5327ab47a2595c286c0435e4b64cba",
    "semantic_title": "employing bottleneck and convolutional features for speech-based physical load detection on limited data amounts",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhao19e_interspeech.html": {
    "title": "Speech Emotion Recognition in Dyadic Dialogues with Attentive Interaction Modeling",
    "volume": "main",
    "abstract": "In dyadic human-human interactions, a more complex interaction scenario, a person's emotional state can be influenced by both self emotional evolution and the interlocutor's behaviors. However, previous speech emotion recognition studies infer the speaker's emotional state mainly based on the targeted speech segment without considering the above two contextual factors. In this paper, we propose an Attentive Interaction Model (AIM) to capture both self- and interlocutor-context to enhance the speech emotion recognition in the dyadic dialog. The model learns to dynamically focus on long-term relevant contexts of the speaker and the interlocutor via the self-attention mechanism and fuse the adaptive context with the present behavior to predict the current emotional state. We carry out extensive experiments on the IEMOCAP corpus for dimensional emotion recognition in arousal and valence. Our model achieves on par performance with baselines for arousal recognition and significantly outperforms baselines for valence recognition, which demonstrates the effectiveness of the model to select useful contexts for emotion recognition in dyadic interactions",
    "checked": true,
    "id": "1ad3be3e455559fd4f2f3dc0c489c1b925cbbc3d",
    "semantic_title": "speech emotion recognition in dyadic dialogues with attentive interaction modeling",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhong19_interspeech.html": {
    "title": "Predicting Group Performances Using a Personality Composite-Network Architecture During Collaborative Task",
    "volume": "main",
    "abstract": "Personality has not only been studied at an individual level, its composite effect between team members has also been indicated to be related to the overall group performance. In this work, we propose a Personality Composite-Network (P-CompN) architecture that models the group-level personality composition with its intertwining effect being integrated into the network modeling of team members vocal behaviors in order to predict the group performances during collaborative problem solving tasks. In specific, we evaluate our proposed P-CompN in a large-scale dataset consist of three-person small group interactions. Our framework achieves a promising group performance classification accuracy of 70.0%, which outperforms baseline model of using only vocal behaviors without personality attributes by 14.4% absolutely. Our analysis further indicates that our proposed personality composite network impacts the vocal behavior models more significantly on the high performing groups versus the low performing groups",
    "checked": true,
    "id": "140653a1b1b70dece4259ef9a283152e634f7b18",
    "semantic_title": "predicting group performances using a personality composite-network architecture during collaborative task",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chao19b_interspeech.html": {
    "title": "Enforcing Semantic Consistency for Cross Corpus Valence Regression from Speech Using Adversarial Discrepancy Learning",
    "volume": "main",
    "abstract": "Issues of mismatch between databases remain a major challenge in performing emotion recognition on target unlabeled corpus from labeled source data. While studies have shown that by means of aligning source and target data distribution to learn a common feature space can mitigate these issues partially, they neglect the effect of distortion in emotion semantics across different databases. This distortion is especially crucial when regressing higher level emotion attribute such as valence. In this work, we propose a maximum regression discrepancy (MRD) network, which enforces cross corpus semantic consistency by learning a common acoustic feature space that minimizes discrepancy on those maximally-distorted samples through adversarial training. We evaluate our framework on two large emotion corpus, the USC IEMOCAP and the MSP-IMPROV, for the task of cross corpus valence regression from speech. Our MRD demonstrates a significant 10% and 5% improvement in concordance correlation coefficients (CCC) compared to using baseline source-only methods, and we also show that it outperforms two state-of-art domain adaptation techniques. Further analysis reveals that our model is more effective in reducing semantic distortion on low valence than high valence samples",
    "checked": true,
    "id": "740df49ffcb268a548fdbf67b97a1432ef4f7e46",
    "semantic_title": "enforcing semantic consistency for cross corpus valence regression from speech using adversarial discrepancy learning",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mao19_interspeech.html": {
    "title": "Deep Learning of Segment-Level Feature Representation with Multiple Instance Learning for Utterance-Level Speech Emotion Recognition",
    "volume": "main",
    "abstract": "In this paper, we propose to combine the deep learning of feature representation with multiple instance learning (MIL) to recognize emotion from speech. The key idea of our approach is to first consciously classify the emotional state of each segment. Then the utterance-level classification is constructed as an aggregation of the segment-level decisions. For the segment-level classification, we attempt two different deep neural network (DNN) architectures called SegMLP and SegCNN, respectively. SegMLP is a multilayer perceptron (MLP) that extracts high-level feature representation from the manually designed perceptual features, and SegCNN is a convolutional neural network (CNN) that automatically learn emotion-specific features from the log Mel filterbanks. Extensive emotion recognition experiments are carried out on the CASIA corpus and the IEMOCAP database. We find that: (1) the aggregation of segment-level decisions provides richer information than the statistics over the low-level descriptors (LLDs) across the whole utterance; (2) automatic feature learning outperforms manual features. Our experimental results are also compared with those of state-of-the-art methods, further demonstrating the effectiveness of the proposed approach",
    "checked": true,
    "id": "5a72ea19ac47f5cb3d049cc7356e4f644e46c86c",
    "semantic_title": "deep learning of segment-level feature representation with multiple instance learning for utterance-level speech emotion recognition",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2019/triantafyllopoulos19_interspeech.html": {
    "title": "Towards Robust Speech Emotion Recognition Using Deep Residual Networks for Speech Enhancement",
    "volume": "main",
    "abstract": "The use of deep learning (DL) architectures for speech enhancement has recently improved the robustness of voice applications under diverse noise conditions. These improvements are usually evaluated based on the perceptual quality of the enhanced audio or on the performance of automatic speech recognition (ASR) systems. We are interested instead in the usefulness of these algorithms in the field of speech emotion recognition (SER), and specifically in whether an enhancement architecture can effectively remove noise while preserving enough information for an SER algorithm to accurately identify emotion in speech. We first show how a scalable DL architecture can be trained to enhance audio signals in a large number of unseen environments, and go on to show how that can benefit common SER pipelines in terms of noise robustness. Our results show that incorporating a speech enhancement architecture is beneficial, especially for low signal-to-noise ratio (SNR) conditions",
    "checked": true,
    "id": "a31dd89ac8258b2fc27c82d30dcb5c7beb49dc92",
    "semantic_title": "towards robust speech emotion recognition using deep residual networks for speech enhancement",
    "citation_count": 35
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19f_interspeech.html": {
    "title": "Towards Discriminative Representations and Unbiased Predictions: Class-Specific Angular Softmax for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Speech emotion recognition (SER) is a challenging task: the complex emotional expressions make it difficult to discriminate different emotions; the unbalanced data misleads models to give biased predictions. In this work, we tackle these two problems by the angular softmax loss. First, we replace the vanilla softmax with angular softmax to learn emotional representations with strong discriminant power. Besides, inspired by its novel geometric interpretation, we establish a general calculation model and deduce a concise formula of decision domain. Based on these derivations, we propose our solution to data imbalance: class-specific angular softmax by which we can directly adjust decision domains of different emotion classes. Experimental results on the IEMOCAP corpus indicate significant improvements on two state-of-the-art models therefore demonstrate the effectiveness of our proposed methods",
    "checked": true,
    "id": "e2489151e20164a354952a3f17b266d8b4e13600",
    "semantic_title": "towards discriminative representations and unbiased predictions: class-specific angular softmax for speech emotion recognition",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jalal19_interspeech.html": {
    "title": "Learning Temporal Clusters Using Capsule Routing for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Emotion recognition from speech plays a significant role in adding emotional intelligence to machines and making human-machine interaction more natural. One of the key challenges from machine learning standpoint is to extract patterns which bear maximum correlation with the emotion information encoded in this signal while being as insensitive as possible to other types of information carried by speech. In this paper, we propose a novel temporal modelling framework for robust emotion classification using bidirectional long short-term memory network (BLSTM), CNN and Capsule networks. The BLSTM deals with the temporal dynamics of the speech signal by effectively representing forward/backward contextual information while the CNN along with the dynamic routing of the Capsule net learn temporal clusters which altogether provide a state-of-the-art technique for classifying the extracted patterns. The proposed approach was compared with a wide range of architectures on the FAU-Aibo and RAVDESS corpora and remarkable gain over state-of-the-art systems were obtained. For FAO-Aibo and RAVDESS 77.6% and 56.2% accuracy was achieved, respectively, which is 3% and 14% (absolute) higher than the best-reported result for the respective tasks",
    "checked": true,
    "id": "221f95be1855f01ce00970120d856cbe95a1b491",
    "semantic_title": "learning temporal clusters using capsule routing for speech emotion recognition",
    "citation_count": 39
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dapolito19_interspeech.html": {
    "title": "L2 Pronunciation Accuracy and Context: A Pilot Study on the Realization of Geminates in Italian as L2 by French Learners",
    "volume": "main",
    "abstract": "This paper investigates the interaction between the characteristics of both L1 and L2 phonetic-phonological systems and how the context, in terms of the amount of information available (less vs more information), may influence the accuracy in producing L2 sounds as well as speech fluency. Specifically, it focuses on how French learners of Italian as L2, representing two different competence levels (lower and higher), realize geminates (non-native sounds) in two different contexts (less and more rich). A rich context is expected to induce lower accuracy. Acoustic data of nine subjects (three beginners, three advanced and three natives as control) were collected and analyzed in order to observe: 1) the realization of geminates (duration of the consonant and preceding vowel as an index of accuracy); and 2) the speech fluency (number and duration of disfluencies; speech/articulation rate). Results suggest that learners' productions are affected by L1, above all in the case of beginners, who show a lower degree of accuracy. As regards the accuracy and context interaction, results show that the production of geminates is more accurate (longer duration) in poor than in rich context. Further, a higher number of disfluencies is found in rich than in poor context",
    "checked": true,
    "id": "68db7dba7e641dd80dcc0237734f5a615bca3143",
    "semantic_title": "l2 pronunciation accuracy and context: a pilot study on the realization of geminates in italian as l2 by french learners",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jamakovic19_interspeech.html": {
    "title": "The Monophthongs of Formal Nigerian English: An Acoustic Analysis",
    "volume": "main",
    "abstract": "Postcolonial varieties of English, used in countries such as Nigeria, the Philippines and India, are influenced by local (\"endonormative\") and external (\"exonormative\") forces, the latter often in the form of British/American English. In the ensuing stylistic continuum, informal speech is more endonormatively oriented than formal/educated speech — which is, in turn, clearly distinguishable from British/American English. The formal subvariety is often regarded as the incipient local standard and is commonly less marked by L1 influence than the informal subvariety Nigerian English (NigE) is the most widely spoken African variety of English, but empirical/quantitative descriptions are rare. In this pilot study, we present an acoustic analysis of eleven phonological monophthongs and two phonological diphthongs that are commonly monophthongised. A total of 811 occurrences, produced in formal contexts by nine educated speakers of NigE with L1 Igbo, was extracted from the ICE Nigeria corpus and analysed acoustically (Lobanov-normalised vowel formants at vowel midpoint) Results show that the NigE speakers reduced the thirteen vowel system to a total of nine distinct phonemes that closely resembles the L1 Igbo vowel inventory. This result suggests substantial L1 influence even at the level of Formal NigE",
    "checked": true,
    "id": "b1b84144c9caea63b386ad6d1392dc44d827920b",
    "semantic_title": "the monophthongs of formal nigerian english: an acoustic analysis",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/arantes19_interspeech.html": {
    "title": "Quantifying Fundamental Frequency Modulation as a Function of Language, Speaking Style and Speaker",
    "volume": "main",
    "abstract": "In this study, we outline a methodology to quantify the degree of similarity between pairs of f distributions based on the Anderson-Darling measure that underlies its namesake goodness-of-fit test. The procedure emphasizes differences due to more fine-grained f modulations rather than differences in measures of central tendency, such as the mean and median. In order to assess the procedure's usefulness for speaker comparison, we applied it to a multilingual corpus in which participants contributed speech delivered in three speaking styles. The similarity measure was calculated separately as function of speaking style and speaker. Between-speaker variability (different speakers, same style) in distribution similarity varied significantly between styles — spontaneous interview shows greater variability than read sentences and word list in five languages (English, French, Italian, Portuguese and Swedish); in Estonian and German, read sentences yield more variability. Within-speaker variability (same speaker, different styles) levels are lower than between-speaker in the style that exhibit the greatest variability. The results point to the potential use of the proposed methodology as a way to identify possible idiosyncratic traits in f distributions. Also, they further demonstrate the effect of speaking styles on intonation patterns",
    "checked": true,
    "id": "86458a284a34ad335714d75425e49286b500f5d3",
    "semantic_title": "quantifying fundamental frequency modulation as a function of language, speaking style and speaker",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kelly19_interspeech.html": {
    "title": "The Voicing Contrast in Stops and Affricates in the Western Armenian of Lebanon",
    "volume": "main",
    "abstract": "Research on Western Armenian has described it as having a contrast between voiceless aspirated stops and affricates, and voiced stops and affricates [1, 2]. The variety of Western Armenian spoken by a large population in Lebanon has not yet been examined phonetically, to determine the acoustic correlates of this contrast. The current study examines the alveolar and postalveolar affricates and alveolar stops (voiceless aspirated and voiced) in both word-initial and word-medial position, using nonsense words written in the Armenian script. The results indicate that voiced sounds have prevoicing, voiceless affricates have some aspiration, but voiceless stops have very short VOT, which aligns better with an analysis of them being classified as unaspirated. It was also found that position in the word does not affect VOT, duration of the closure or frication",
    "checked": true,
    "id": "fa966a27f718ffa28c0aefb0a26cd8967c5f4021",
    "semantic_title": "the voicing contrast in stops and affricates in the western armenian of lebanon",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jatteau19_interspeech.html": {
    "title": "Gra[f] e!\" Word-Final Devoicing of Obstruents in Standard French: An Acoustic Study Based on Large Corpora",
    "volume": "main",
    "abstract": "This study investigates the tendency towards word-final devoicing of voiced obstruents in Standard French, and how devoicing is influenced by domain, speech style, manner and place of articulation. Three large corpora with automatic segmentations produced by forced alignment are used: ESTER, ETAPE and NCCFr. A voicing-ratio is established for each obstruent via F0 extraction in Praat, and the percentage of fully voiced segments is computed. We find a salient pattern of devoicing before pause, with no clear effect of speech style. Fricatives devoice more than stops, and posterior fricatives devoice more than anterior ones. Since voicing plays a central role in the cross-linguistic pattern of word-final [voice] neutralisation, this study gives insight into the potential phonetic precursors of this process",
    "checked": true,
    "id": "de779c29686147e7e091cbbf7fbb964d350cad8a",
    "semantic_title": "gra[f] e!\" word-final devoicing of obstruents in standard french: an acoustic study based on large corpora",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19g_interspeech.html": {
    "title": "Acoustic Indicators of Deception in Mandarin Daily Conversations Recorded from an Interactive Game",
    "volume": "main",
    "abstract": "Being able to distinguish the differences between deceptive and truthful statements in a dialogue is an important skill in daily life. Extensive studies on the acoustic features of deceptive English speech have been reported, but such research in Mandarin is relatively scarce. We constructed a Mandarin deception database of daily dialogues from native speakers in Taiwan. College students were recruited to participate in a game in which they were encouraged to lie and convince their opponents of experiences that they did not have. After data collection, acoustic-prosodic features were extracted. The statistics of these features were calculated so that the differences between truthful and deceptive sentences, both as they were intended and perceived, can be compared. Results indicate that different people tend to use different acoustic features when telling a lie; the participants could be put into 10 categories in a dendrogram, with an exception of 31 people from whom no acoustic indicators for deception were found. Without considering interpersonal differences, our best classifier reached an F1 score of 53.37% in distinguishing deceptive and truthful segmentation units. We hope to present this new database as a corpus for future studies on deception in Mandarin conversations",
    "checked": true,
    "id": "48adbfbc6b82bf3ee133f7078b72913951f671bc",
    "semantic_title": "acoustic indicators of deception in mandarin daily conversations recorded from an interactive game",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schuppler19_interspeech.html": {
    "title": "Prosodic Effects on Plosive Duration in German and Austrian German",
    "volume": "main",
    "abstract": "This study investigates the acoustic cues used to mark prosodic boundaries in two varieties of German, with a specific focus on variations in production of fortis and lenis plosives. We extracted prosodic-boundary-adjacent and non-boundary-adjacent plosives from GRASS (Austrian German) and the Kiel Corpus of Read Speech (Northern German), and investigated closure duration, burst features, and duration characteristics of the surrounding segments. We find that closure and burst duration features, as well as duration of a preceding adjacent segment, vary consistently in relationship to the presence or absence of a prosodic boundary, but that the relative weights of these features differ in the two varieties studied",
    "checked": true,
    "id": "88f25fd2b7a570a3f51264e0fb02a9a605f1b551",
    "semantic_title": "prosodic effects on plosive duration in german and austrian german",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/johny19_interspeech.html": {
    "title": "Cross-Lingual Consistency of Phonological Features: An Empirical Study",
    "volume": "main",
    "abstract": "The concept of a phoneme arose historically as a theoretical abstraction that applies language-internally. Using phonemes and phonological features in cross-linguistic settings raises an important question of conceptual validity: Are contrasts that are meaningful within a language also empirically robust across languages? This paper develops a method for assessing the crosslinguistic consistency of phonological features in phoneme inventories. The method involves training separate binary neural classifiers for several phonological contrast in audio spans centered on particular segments within continuous speech. To assess cross-linguistic consistency, these classifiers are evaluated on held-out languages and classification quality is reported. We apply this method to several common phonological contrasts, including vowel height, vowel frontness, and retroflex consonants, in the context of multi-speaker corpora for ten languages from three language families (Indo-Aryan, Dravidian, and Malayo-Polynesian). We empirically evaluate and discuss the consistency of phonological contrasts derived from features found in phonological ontologies such as PanPhon and PHOIBLE",
    "checked": true,
    "id": "f03740d071738682fbc1b9dd46c5748f6ba8697c",
    "semantic_title": "cross-lingual consistency of phonological features: an empirical study",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/guitardivent19_interspeech.html": {
    "title": "Are IP Initial Vowels Acoustically More Distinct? Results from LDA and CNN Classifications",
    "volume": "main",
    "abstract": "Past results have suggested that initial strengthening (IS) effects target the contrastive phonetic properties of segments, with a maximization of acoustic contrasts in initial position of strong prosodic domains. Here, we investigate whether IS effects translate into a better acoustic discriminability within the French oral vowels system. Discriminability is assessed on the basis of classification results of two types of classifiers: a linear discriminant analysis (LDA) based on the four formants frequencies, and a deep convolutional neural network (CNN) based on spectrograms. The test set includes 720 exemplars of /i, y, e, ε, a, x, u, o, ɔ/ (with /x/=/ø, œ/) produced in a labial context, either in intonational phrase initial (IPi) or word initial (Wi) position. Classifiers were trained using a set of 4500 vowels extracted from a large read speech corpus. Results show a better discriminability of vowels (overall better classification rate) in IPi than in Wi with the two methods. Less confusion in IPi is found between rounded and unrounded, and between back and front vowels, but not between the vowels along the four-way height contrast. Less confusion between peripheral and central vowels also expresses a maximization of contrasts within the acoustic space in IPi position",
    "checked": true,
    "id": "a3420b87393a1ed8ada6ba862d9c16b093986cb5",
    "semantic_title": "are ip initial vowels acoustically more distinct? results from lda and cnn classifications",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wei19_interspeech.html": {
    "title": "Neural Network-Based Modeling of Phonetic Durations",
    "volume": "main",
    "abstract": "A deep neural network (DNN)-based model has been developed to predict non-parametric distributions of durations of phonemes in specified phonetic contexts and used to explore which factors influence durations most. Major factors in US English are pre-pausal lengthening, lexical stress, and speaking rate. The model can be used to check that text-to-speech (TTS) training speech follows the script and words are pronounced as expected. Duration prediction is poorer with training speech for automatic speech recognition (ASR) because the training corpus typically consists of single utterances from many speakers and is often noisy or casually spoken. Low probability durations in ASR training material nevertheless mostly correspond to non-standard speech, with some having disfluencies. Children's speech is disproportionately present in these utterances, since children show much more variation in timing",
    "checked": true,
    "id": "480288deec8081b36ed33805b91f0498236ee6ec",
    "semantic_title": "neural network-based modeling of phonetic durations",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/moczanow19_interspeech.html": {
    "title": "An Acoustic Study of Vowel Undershoot in a System with Several Degrees of Prominence",
    "volume": "main",
    "abstract": "The paper presents the results of a pilot study investigating the relationship between vowel quality and duration in Ukrainian. In this language, lexical stress is cued by increased duration; smaller but systematic differences in length occur between unstressed, rhythmic stress-bearing, and pretonic syllables. The presence of several degrees of lengthening within one word makes it possible to test the long-established theories of vowel reduction posing a direct link between decreased duration and vowel undershoot. Overall, the analysis of the aggregated data collected from four native speakers of Ukrainian points to a strong correlation between decreasing duration and the undershoot of F1 targets. However, in separate by-position and by-speaker analyses, no correlation between F1 and duration is observed in the positions of rhythmic and lexical stress. We thus conclude that the stability of the F1 target vis-à-vis temporal parameters may constitute another parameter expressing metrical prominence. In addition, our data suggests that formant undershoot may be affected by an articulatory effort",
    "checked": true,
    "id": "88cb975099f83d4e7af9de1f589085f6c90b40c3",
    "semantic_title": "an acoustic study of vowel undershoot in a system with several degrees of prominence",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/berger19_interspeech.html": {
    "title": "A Preliminary Study of Charismatic Speech on YouTube: Correlating Prosodic Variation with Counts of Subscribers, Views and Likes",
    "volume": "main",
    "abstract": "This paper is a first investigation into the influence of the pitch range and the intensity variation on the number of subscribers, views and likes of YouTube Creators. A total of ten minutes of speech material from five English and five North-American YouTubers was analyzed. The results for pitch range and intensity variation suggest that an increase in both parameters results in higher subscriber counts. For views, there was no influence of pitch range, but an increase in intensity variation results in a lower number of views. Pitch range and intensity variation had no influence on the like count. Furthermore, both origin and gender had an influence on the results. Ultimately, this study will provide further information for the phonetic research of charisma (i.e., the perceived charm, competence, power, and persuasiveness of a speaker), as it is suspected that the acoustic features that have so far been connected to charisma also play an important role in the success of a YouTuber and their channel",
    "checked": true,
    "id": "4f1af0125197ec6bf72a5a85b56a618c7ff0da78",
    "semantic_title": "a preliminary study of charismatic speech on youtube: correlating prosodic variation with counts of subscribers, views and likes",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/luo19b_interspeech.html": {
    "title": "Phonetic Detail Encoding in Explaining the Size of Speech Planning Window",
    "volume": "main",
    "abstract": "With the ultimate goal of understanding the production planning scope, this study manipulates phonetic information (place of articulation and voicing) and measures three acoustic cues to analyze consonant clusters across words produced by English (L1) and Mandarin (L2) speakers. We continue to explore a) how phonetic detail interacts with prosodic boundary in modulating surface realization, and b) the roles of phonetic information in speech planning motor control. The results show that L2 speakers exhibited different acoustic deviations varying with their proficiency level. The group with lower L2 proficiency significantly deviated from the L1 group in release likelihood and closure shortening, while the higher-proficiency group exhibited less nativelike performance in terms of closure durations. The results also discover that all speakers are subject to language-independent articulatory constraint at word boundaries, while language-specific phonetic detail accounts for more nonnative deviations. The core findings highlight a long-distance speech planning scope in native speech, with cross-word phonetic information interacting with prosodic encoding. It is argued that phonology applies blindly across words and is independent of lexical cognitive load",
    "checked": true,
    "id": "7ecea15ab19fe879f9a665b76f74bb4f1fbddecf",
    "semantic_title": "phonetic detail encoding in explaining the size of speech planning window",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zarka19_interspeech.html": {
    "title": "Acoustic Cues to Topic and Narrow Focus in Egyptian Arabic",
    "volume": "main",
    "abstract": "This study investigates acoustic cues (duration, scaling and alignment of peaks and valleys) to the prosodic realization of topics and narrow subject foci in a declarative SVO sentence in Egyptian Arabic. Morpho-syntactically identical sentences were elicited in appropriately designed contexts from 18 native speakers by means of a question-answer paradigm. The results show that the stressed syllable of a focused word is longer than the stressed syllable of the same word in topic condition. Additionally, the peaks of foci are generally scaled higher than those of topics. These differences clearly point to varying degrees of prosodic prominence. Furthermore, the alignment of the F0 peak and the subsequent low endpoint of a rising-falling tonal contour is earlier in foci than in topics, indicating that focus is signaled by an early sharp fall whereas the falling part of the tonal gesture starts later and is shallower in the case of a topic. Overall, our results suggest that narrow subject foci and topics tend to be associated with different pitch events",
    "checked": true,
    "id": "0b7a73c553db407f3fd17475721c82453d9c20d5",
    "semantic_title": "acoustic cues to topic and narrow focus in egyptian arabic",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/alowonou19_interspeech.html": {
    "title": "Acoustic and Articulatory Study of Ewe Vowels: A Comparative Study of Male and Female",
    "volume": "main",
    "abstract": "In order to investigate the difference in Ewe males and Ewe females during the production of Ewe vowels, results from the comparative quantitative and qualitative assessments of tongue shape and movement using ultrasound imaging as well as the comparative evaluation of F1 and F2 frequency values from data collected from 9 Ewe male speakers and 6 Ewe female speakers, were presented in this study. The results showed that vowels are produced with higher formant frequencies by Ewe female speakers compared to Ewe male speakers, except for the vowel /ε/ produced with a lower F1 frequency by Ewe females. The articulatory results showed a higher and more forwarder tongue configuration for Ewe male compared to female counterparts",
    "checked": true,
    "id": "a68ce3e616e1a3d73563657a6d7fbcbd2fcabd35",
    "semantic_title": "acoustic and articulatory study of ewe vowels: a comparative study of male and female",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/guo19c_interspeech.html": {
    "title": "Speech Augmentation via Speaker-Specific Noise in Unseen Environment",
    "volume": "main",
    "abstract": "Speech augmentation is a common and effective strategy to avoid overfitting and improve on the robustness of an emotion recognition model. In this paper, we investigate for the first time the intrinsic attributes in a speech signal using the multi-resolution analysis theory and the Hilbert-Huang Spectrum, with the goal of developing a robust speech augmentation approach from raw speech data. Specifically, speech decomposition in a double tree complex wavelet transform domain is realized, to obtain sub-speech signals; then, the Hilbert Spectrum using Hilbert-Huang Transform is calculated for each sub-band to capture the noise content in unseen environments with the voice restriction to 100–4000 Hz; finally, the speech-specific noise that varies with the speaker individual, scenarios, environment, and voice recording equipment, can be reconstructed from the top two high-frequency sub-bands to enhance the raw signal. Our proposed speech augmentation is demonstrated using five robust machine learning architectures based on the RAVDESS database, achieving up to 9.3% higher accuracy compared to the performance on raw data for an emotion recognition task",
    "checked": true,
    "id": "244fb2ad025aea8a74e6f4899d480be2d10262f7",
    "semantic_title": "speech augmentation via speaker-specific noise in unseen environment",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hao19_interspeech.html": {
    "title": "UNetGAN: A Robust Speech Enhancement Approach in Time Domain for Extremely Low Signal-to-Noise Ratio Condition",
    "volume": "main",
    "abstract": "Speech enhancement at extremely low signal-to-noise ratio (SNR) condition is a very challenging problem and rarely investigated in previous works. This paper proposes a robust speech enhancement approach (UNetGAN) based on U-Net and generative adversarial learning to deal with this problem. This approach consists of a generator network and a discriminator network, which operate directly in the time domain. The generator network adopts a U-Net like structure and employs dilated convolution in the bottleneck of it. We evaluate the performance of the UNetGAN at low SNR conditions (up to -20dB) on the public benchmark. The result demonstrates that it significantly improves the speech quality and substantially outperforms the representative deep learning models, including SEGAN, cGAN fo SE, Bidirectional LSTM using phase-sensitive spectrum approximation cost function (PSA-BLSTM) and Wave-U-Net regarding Short-Time Objective Intelligibility (STOI) and Perceptual evaluation of speech quality (PESQ)",
    "checked": true,
    "id": "7c90f01050817d1f8ddba9359737f12cdb96560c",
    "semantic_title": "unetgan: a robust speech enhancement approach in time domain for extremely low signal-to-noise ratio condition",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pascual19b_interspeech.html": {
    "title": "Towards Generalized Speech Enhancement with Generative Adversarial Networks",
    "volume": "main",
    "abstract": "The speech enhancement task usually consists of removing additive noise or reverberation that partially mask spoken utterances, affecting their intelligibility. However, little attention is drawn to other, perhaps more aggressive signal distortions like clipping, chunk elimination, or frequency-band removal. Such distortions can have a large impact not only on intelligibility, but also on naturalness or even speaker identity, and require of careful signal reconstruction. In this work, we give full consideration to this generalized speech enhancement task, and show it can be tackled with a time-domain generative adversarial network (GAN). In particular, we extend a previous GAN-based speech enhancement system to deal with mixtures of four types of aggressive distortions. Firstly, we propose the addition of an adversarial acoustic regression loss that promotes a richer feature extraction at the discriminator. Secondly, we also make use of a two-step adversarial training schedule, acting as a warm up-and-fine-tune sequence. Both objective and subjective evaluations show that these two additions bring improved speech reconstructions that better match the original speaker identity and naturalness",
    "checked": true,
    "id": "1bc219f54053ba7d97d55db119785f1777af597a",
    "semantic_title": "towards generalized speech enhancement with generative adversarial networks",
    "citation_count": 27
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19g_interspeech.html": {
    "title": "A Convolutional Neural Network with Non-Local Module for Speech Enhancement",
    "volume": "main",
    "abstract": "Convolution neural networks (CNNs) are achieving increasing attention for the speech enhancement task recently. However, the convolutional operations only process a local neighborhood (several nearest neighboring neurons) at a time across either space or time direction. The long-range dependencies can only be captured when the convolutional operations are applied recursively, but the problems of computationally inefficient and optimization difficulties are introduced. Inspired by the recent impressive performance of the non-local module in many computer vision tasks, we propose a convolutional neural network with non-local module for speech enhancement in this paper. The non-local operations are capable of capturing the global information in the frequency domain through passing information between distant time-frequency units. The non-local operations are able to set the dimension of the input as an arbitrary value, which results in the easy integration with our proposed network framework. Experimental results demonstrate that the proposed method not only improves the computational efficiency significantly but also outperforms the competing methods in terms of objective speech intelligibility and quality metrics",
    "checked": true,
    "id": "4a7e7742eaf11beb7dddacc966b0fc3e94c1a277",
    "semantic_title": "a convolutional neural network with non-local module for speech enhancement",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lin19b_interspeech.html": {
    "title": "IA-NET: Acceleration and Compression of Speech Enhancement Using Integer-Adder Deep Neural Network",
    "volume": "main",
    "abstract": "Numerous compression and acceleration techniques achieved state-of-the-art results for classification tasks in speech processing. However, the same techniques produce unsatisfactory performance for regression tasks, because of the different natures of classification and regression tasks. This paper presents a novel integer-adder deep neural network (IA-Net), which compresses model size and accelerates the inference process in speech enhancement, an important task in speech-signal processing, by replacing the floating-point multiplier with an integer-adder. The experimental results show that the inference time of IA-Net can be significantly reduced by 20% and the model size can be compressed by 71.9% without any performance degradation. To the best of our knowledge, this is the first study that decreases the inference time and compresses the model size, simultaneously, while producing good performance for speech enhancement. Based on the promising results, we believe that the proposed framework can be deployed in various mobile and edge-computing devices",
    "checked": true,
    "id": "7c8f6b581d36a99bd323ad540964f15899f7db18",
    "semantic_title": "ia-net: acceleration and compression of speech enhancement using integer-adder deep neural network",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chai19_interspeech.html": {
    "title": "KL-Divergence Regularized Deep Neural Network Adaptation for Low-Resource Speaker-Dependent Speech Enhancement",
    "volume": "main",
    "abstract": "In this paper, we propose a Kullback-Leibler divergence (KLD) regularized approach to adapting speaker-independent (SI) speech enhancement model based on regression deep neural networks (DNNs) to another speaker-dependent (SD) model using a tiny amount of speaker-specific adaptation data. This algorithm adapts the DNN model conservatively by forcing the conditional target distribution estimated from the SD model to be close to that from the SI model. The constraint is realized by adding KLD regularization to our previously proposed maximum likelihood objective function. Experimental results demonstrate that, even with only 10 seconds of SD adaptation data, the proposed framework consistently achieves speech intelligibility improvements under all 15 unseen noise types evaluated and at all signal-to-noise ratio levels for all 8 test speakers from the WSJ0 evaluation set",
    "checked": true,
    "id": "c04558ac3e8558aebbb9dbf6be6154717bfbf821",
    "semantic_title": "kl-divergence regularized deep neural network adaptation for low-resource speaker-dependent speech enhancement",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/llombart19_interspeech.html": {
    "title": "Speech Enhancement with Wide Residual Networks in Reverberant Environments",
    "volume": "main",
    "abstract": "This paper proposes a speech enhancement method which exploits the high potential of residual connections in a Wide Residual Network architecture. This is supported on single dimensional convolutions computed alongside the time domain, which is a powerful approach to process contextually correlated representations through the temporal domain, such as speech feature sequences. We find the residual mechanism extremely useful for the enhancement task since the signal always has a linear shortcut and the non-linear path enhances it in several steps by adding or subtracting corrections. The enhancement capability of the proposal is assessed by objective quality metrics evaluated with simulated and real samples of reverberated speech signals. Results show that the proposal outperforms the state-of-the-art method called WPE, which is known to effectively reduce reverberation and greatly enhance the signal. The proposed model, trained with artificial synthesized reverberation data, was able to generalize to real room impulse responses for a variety of conditions (e.g. different room sizes, RT , near & far field). Furthermore, it achieves accuracy for real speech with reverberation from two different datasets",
    "checked": true,
    "id": "f6753201f2553cad294046046bf4683f45e1019f",
    "semantic_title": "speech enhancement with wide residual networks in reverberant environments",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/reddy19_interspeech.html": {
    "title": "A Scalable Noisy Speech Dataset and Online Subjective Test Framework",
    "volume": "main",
    "abstract": "Background noise is a major source of quality impairments in Voice over Internet Protocol (VoIP) and Public Switched Telephone Network (PSTN) calls. Recent work shows the efficacy of deep learning for noise suppression, but the datasets have been relatively small compared to those used in other domains (e.g., ImageNet) and the associated evaluations have been more focused. In order to better facilitate deep learning research in Speech Enhancement, we present a noisy speech dataset (MS-SNSD) that can scale to arbitrary sizes depending on the number of speakers, noise types, and Speech to Noise Ratio (SNR) levels desired. We show that increasing dataset sizes increases noise suppression performance as expected. In addition, we provide an open-source evaluation methodology to evaluate the results subjectively at scale using crowdsourcing, with a reference algorithm to normalize the results. To demonstrate the dataset and evaluation framework we apply it to several noise suppressors and compare the subjective Mean Opinion Score (MOS) with objective quality measures such as SNR, PESQ, POLQA, and VISQOL and show why MOS is still required. Our subjective MOS evaluation is the first large scale evaluation of Speech Enhancement algorithms that we are aware of",
    "checked": true,
    "id": "cc3b0272547e5d1d7afdb450e9df5d2805f5c42e",
    "semantic_title": "a scalable noisy speech dataset and online subjective test framework",
    "citation_count": 86
  },
  "https://www.isca-speech.org/archive/interspeech_2019/adiga19_interspeech.html": {
    "title": "Speech Enhancement for Noise-Robust Speech Synthesis Using Wasserstein GAN",
    "volume": "main",
    "abstract": "The quality of speech synthesis systems can be significantly deteriorated by the presence of background noise in the recordings. Despite the existence of speech enhancement techniques for effectively suppressing additive noise under low signal-to-noise (SNR) conditions, these techniques have been neither designed nor tested in speech synthesis tasks where background noise has relatively lower energy. In this paper, we propose a speech enhancement technique based on generative adversarial networks (GANs) which acts as a preprocessing step of speech synthesis. Motivated by the speech enhancement generative adversarial network (SEGAN) approach and recent advances in deep learning, we propose to use Wasserstein GAN (WGAN) with gradient penalty and gated activation functions to the autoencoder network of SEGAN. We studied the impact of the proposed method on a data set consisting of 28 speakers and different noise types with 3 different SNR level. The effectiveness of the proposed method in the context of speech synthesis is demonstrated through the training of WaveNet vocoder. We compare our method against SEGAN. Both subjective and objective metrics confirm that the proposed speech enhancement approach outperforms SEGAN in terms of speech synthesis quality",
    "checked": true,
    "id": "5d8811ea4ed096b65572d9192c4e3cc26572523f",
    "semantic_title": "speech enhancement for noise-robust speech synthesis using wasserstein gan",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pv19_interspeech.html": {
    "title": "A Non-Causal FFTNet Architecture for Speech Enhancement",
    "volume": "main",
    "abstract": "In this paper, we suggest a new parallel, non-causal and shallow waveform domain architecture for speech enhancement based on FFTNet, a neural network for generating high quality audio waveform. In contrast to other waveform based approaches like WaveNet, FFTNet uses an initial wide dilation pattern. Such an architecture better represents the long term correlated structure of speech in the time domain, where noise is usually highly non-correlated, and therefore it is suitable for waveform domain based speech enhancement. To further strengthen this feature of FFTNet, we suggest a non-causal FFTNet architecture, where the present sample in each layer is estimated from the past and future samples of the previous layer. By suggesting a shallow network and applying non-causality within certain limits, the suggested FFTNet for speech enhancement (SE-FFTNet) uses much fewer parameters compared to other neural network based approaches for speech enhancement like WaveNet and SEGAN. Specifically, the suggested network has considerably reduced model parameters: 32% fewer compared to WaveNet and 87% fewer compared to SEGAN. Finally, based on subjective and objective metrics, SE-FFTNet outperforms WaveNet in terms of enhanced signal quality, while it provides equally good performance as SEGAN",
    "checked": true,
    "id": "51788627f814026348ba0941ab70b7275edccf0d",
    "semantic_title": "a non-causal fftnet architecture for speech enhancement",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/braithwaite19_interspeech.html": {
    "title": "Speech Enhancement with Variance Constrained Autoencoders",
    "volume": "main",
    "abstract": "Recent machine learning based approaches to speech enhancement operate in the time domain and have been shown to outperform the classical enhancement methods. Two such models are SE-GAN and SE-WaveNet, both of which rely on complex neural network architectures, making them expensive to train. We propose using the Variance Constrained Autoencoder (VCAE) for speech enhancement. Our model uses a more straightforward neural network structure than competing solutions and is a natural model for the task of speech enhancement. We demonstrate experimentally that the proposed enhancement model outperforms SE-GAN and SE-WaveNet in terms of perceptual quality of enhanced signals",
    "checked": true,
    "id": "a5ac6be3ea4543fb0b87e38d9cfeb7f8181880ab",
    "semantic_title": "speech enhancement with variance constrained autoencoders",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kyriakopoulos19_interspeech.html": {
    "title": "A Deep Learning Approach to Automatic Characterisation of Rhythm in Non-Native English Speech",
    "volume": "main",
    "abstract": "A speaker's rhythm contributes to the intelligibility of their speech and can be characteristic of their language and accent. For non-native learners of a language, the extent to which they match its natural rhythm is an important predictor of their proficiency. As a learner improves, their rhythm is expected to become less similar to their L1 and more to the L2. Metrics based on the variability of the durations of vocalic and consonantal intervals have been shown to be effective at detecting language and accent. In this paper, pairwise variability (PVI, CCI) and variance (varcoV, varcoC) metrics are first used to predict proficiency and L1 of non-native speakers taking an English spoken exam. A deep learning alternative to generalise these features is then presented, in the form of a tunable duration embedding, based on attention over an RNN over durations. The RNN allows relationships beyond pairwise to be captured, while attention allows sensitivity to the different relative importance of durations. The system is trained end-to-end for proficiency and L1 prediction and compared to the baseline. The values of both sets of features for different proficiency levels are then visualised and compared to native speech in the L1 and the L2",
    "checked": true,
    "id": "5779cff11e1968416c5914cf228c5ca578363230",
    "semantic_title": "a deep learning approach to automatic characterisation of rhythm in non-native english speech",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/merkx19_interspeech.html": {
    "title": "Language Learning Using Speech to Image Retrieval",
    "volume": "main",
    "abstract": "Humans learn language by interaction with their environment and listening to other humans. It should also be possible for computational models to learn language directly from speech but so far most approaches require text. We improve on existing neural network approaches to create visually grounded embeddings for spoken utterances. Using a combination of a multi-layer GRU, importance sampling, cyclic learning rates, ensembling and vectorial self-attention our results show a remarkable increase in image-caption retrieval performance over previous work. Furthermore, we investigate which layers in the model learn to recognise words in the input. We find that deeper network layers are better at encoding word presence, although the final layer has slightly lower performance. This shows that our visually grounded sentence encoder learns to recognise words from the input even though it is not explicitly trained for word recognition",
    "checked": true,
    "id": "c3465f1ebc248b9c955a1baff8778afe08988b8a",
    "semantic_title": "language learning using speech to image retrieval",
    "citation_count": 40
  },
  "https://www.isca-speech.org/archive/interspeech_2019/skidmore19_interspeech.html": {
    "title": "Using Alexa for Flashcard-Based Learning",
    "volume": "main",
    "abstract": "Despite increasing awareness of Alexa's potential as an educational tool, there remains a limited scope for Alexa skills to accommodate the features required for effective language learning. This paper describes an investigation into implementing ‘spaced-repetition', a non-trivial feature of flashcard-based learning, through the development of an Alexa skill called ‘Japanese Flashcards'. Here we show that existing Alexa development features such as skill persistence allow for the effective implementation of spaced-repetition and suggest a heuristic adaptation of the spaced-repetition model that is appropriate for use with voice assistants (VAs). We also highlight areas of the Alexa development process that limit the facilitation of language learning, namely the lack of multilingual speech recognition, and offer solutions to these current limitations. Overall, the investigation shows that Alexa can successfully facilitate simple L2-L1 flashcard-based language learning and highlights the potential for Alexa to be used as a sophisticated and effective language learning tool",
    "checked": true,
    "id": "00e673536b1948db76cc0560d82b005013e65e2e",
    "semantic_title": "using alexa for flashcard-based learning",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hansen19_interspeech.html": {
    "title": "The 2019 Inaugural Fearless Steps Challenge: A Giant Leap for Naturalistic Audio",
    "volume": "main",
    "abstract": "The 2019 FEARLESS STEPS (FS-1) Challenge is an initial step to motivate a streamlined and collaborative effort from the speech and language community towards addressing massive naturalistic audio, the first of its kind. The Fearless Steps Corpus is a collection of 19,000 hours of multi-channel recordings of spontaneous speech from over 450 speakers under multiple noise conditions. A majority of the Apollo Missions original analog data is unlabeled and has thus far motivated the development of both unsupervised and semi-supervised strategies. This edition of the challenge encourages the development of core speech and language technology systems for data with limited ground-truth / low resource availability and is intended to serve as the \"First Step\" towards extracting high-level information from such massive unlabeled corpora. In conjunction with the Challenge, 11,000 hours of synchronized 30-channel Apollo-11 audio data has also been released to the public by CRSS-UTDallas. We describe in this paper the Fearless Steps Corpus, Challenge Tasks, their associated baseline systems, and results. In conclusion, we also provide insights gained by the CRSS-UTDallas team during the inaugural Fearless Steps Challenge",
    "checked": true,
    "id": "181d41545fbf71953399f2df48bc9f1cc3612743",
    "semantic_title": "the 2019 inaugural fearless steps challenge: a giant leap for naturalistic audio",
    "citation_count": 30
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19e_interspeech.html": {
    "title": "Completely Unsupervised Phoneme Recognition by a Generative Adversarial Network Harmonized with Iteratively Refined Hidden Markov Models",
    "volume": "main",
    "abstract": "Producing a large annotated speech corpus for training ASR systems remains difficult for more than 95% of languages all over the world which are low-resourced, but collecting a relatively big unlabeled data set for such languages is more achievable. This is why some initial effort have been reported on completely unsupervised speech recognition learned from unlabeled data only, although with relatively high error rates. In this paper, we develop a Generative Adversarial Network (GAN) to achieve this purpose, in which a Generator and a Discriminator learn from each other iteratively to improve the performance. We further use a set of Hidden Markov Models (HMMs) iteratively refined from the machine generated labels to work in harmony with the GAN. The initial experiments on TIMIT data set achieve an phone error rate of 33.1%, which is 8.5% lower than the previous state-of-the-art",
    "checked": true,
    "id": "478be23706b0a9de1b4f002f909c211a18ed724d",
    "semantic_title": "completely unsupervised phoneme recognition by a generative adversarial network harmonized with iteratively refined hidden markov models",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/trisitichoke19_interspeech.html": {
    "title": "Analysis of Native Listeners' Facial Microexpressions While Shadowing Non-Native Speech — Potential of Shadowers' Facial Expressions for Comprehensibility Prediction",
    "volume": "main",
    "abstract": "Recently, researchers' attention has been paid to pronunciation assessment not based on comparison between L2 speech and native models, but based on comprehensibility of L2 speech [1, 2, 3]. In our previous studies [4, 5, 6], native listeners' shadowing of L2 speech was examined and it was shown that delay of shadowing and accuracy of articulation in shadowing utterances, both of which were acoustically calculated, are strongly influenced by the amount of cognitive load imposed for understanding L2 speech, especially when it is with strong accents. In this paper, aside from acoustic analysis of shadowings, we focus on shadowers' facial microexpressions and examine how they are correlated with perceived comprehensibility. To extract facial expression features, two methods are tested. One is a computer-vision-based method and recorded videos of shadowers' facial expressions are analyzed. The other is a method using a physiological sensor that can detect subtle movements of facial muscles. In experiments, four shadowers' facial expressions are analyzed, each of whom shadowed approximately 800 L2 utterances. Results show that some of shadowers' facial expressions are highly correlated with perceived comprehensibility, and that those facial expressions are strongly shadower-dependent. These results indicate a high potential of shadowers' facial expressions for comprehensibility prediction",
    "checked": false,
    "id": "3fb2aae7f3584075997668f2425839a48223b076",
    "semantic_title": "analysis of native listeners' facial microexpressions while shadowing non-native speech - potential of shadowers' facial expressions for comprehensibility prediction",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/karhila19_interspeech.html": {
    "title": "Transparent Pronunciation Scoring Using Articulatorily Weighted Phoneme Edit Distance",
    "volume": "main",
    "abstract": "For researching effects of gamification in foreign language learning for children in the \"Say It Again, Kid!\" project we developed a feedback paradigm that can drive gameplay in pronunciation learning games. We describe our scoring system based on the difference between a reference phone sequence and the output of a multilingual CTC phoneme recogniser. We present a white-box scoring model of mapped weighted Levenshtein edit distance between reference and error with error weights for articulatory differences computed from a training set of scored utterances. The system can produce a human-readable list of each detected mispronunciation's contribution to the utterance score. We compare our scoring method to established black box methods",
    "checked": true,
    "id": "01b41bea197ce7abfeb45dba44e5928c313e8807",
    "semantic_title": "transparent pronunciation scoring using articulatorily weighted phoneme edit distance",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yoon19_interspeech.html": {
    "title": "Development of Robust Automated Scoring Models Using Adversarial Input for Oral Proficiency Assessment",
    "volume": "main",
    "abstract": "In this study, we developed an automated scoring model for an oral proficiency test eliciting spontaneous speech from non-native speakers of English. In a large-scale oral proficiency test, a small number of responses may have atypical characteristics that make it difficult even for state-of-the-art automated scoring models to assign fair scores. The oral proficiency test in this study consisted of questions asking about content in materials provided to the test takers, and the atypical responses frequently had serious content abnormalities. In order to develop an automated scoring system that is robust to these atypical responses, we first developed a set of content features to capture content abnormalities. Next, we trained scoring models using the augmented training dataset, including synthetic atypical responses. Compared to the baseline scoring model, the new model showed comparable performance in scoring normal responses, while it assigned fairer scores for authentic atypical responses extracted from operational test administrations",
    "checked": true,
    "id": "acfefebe39c39c9e38363b81c6c729febc360581",
    "semantic_title": "development of robust automated scoring models using adversarial input for oral proficiency assessment",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lu19b_interspeech.html": {
    "title": "Impact of ASR Performance on Spoken Grammatical Error Detection",
    "volume": "main",
    "abstract": "Computer assisted language learning (CALL) systems aid learners to monitor their progress by providing scoring and feedback on language assessment tasks. Free speaking tests allow assessment of what a learner has said, as well as how they said it. For these tasks, Automatic Speech Recognition (ASR) is required to generate transcriptions of a candidate's responses, the quality of these transcriptions is crucial to provide reliable feedback in downstream processes. This paper considers the impact of ASR performance on Grammatical Error Detection (GED) for free speaking tasks, as an example of providing feedback on a learner's use of English. The performance of an advanced deep-learning based GED system, initially trained on written corpora, is used to evaluate the influence of ASR errors. One consequence of these errors is that grammatical errors can result from incorrect transcriptions as well as learner errors, this may yield confusing feedback. To mitigate the effect of these errors, and reduce erroneous feedback, ASR confidence scores are incorporated into the GED system. By additionally adapting the written text GED system to the speech domain, using ASR transcriptions, significant gains in performance can be achieved. Analysis of the GED performance for different grammatical error types and across grade is also presented",
    "checked": true,
    "id": "da7587615403b0d8889eabe1c37f516ff14669c5",
    "semantic_title": "impact of asr performance on spoken grammatical error detection",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yang19d_interspeech.html": {
    "title": "Self-Imitating Feedback Generation Using GAN for Computer-Assisted Pronunciation Training",
    "volume": "main",
    "abstract": "Self-imitating feedback is an effective and learner-friendly method for non-native learners in Computer-Assisted Pronunciation Training. Acoustic characteristics in native utterances are extracted and transplanted onto learner's own speech input, and given back to the learner as a corrective feedback. Previous works focused on speech conversion using prosodic transplantation techniques based on PSOLA algorithm. Motivated by the visual differences found in spectrograms of native and non-native speeches, we investigated applying GAN to generate self-imitating feedback by utilizing generator' s ability through adversarial training. Because this mapping is highly under-constrained, we also adopt cycle consistency loss to encourage the output to preserve the global structure, which is shared by native and non-native utterances. Trained on 97,200 spectrogram images of short utterances produced by native and non-native speakers of Korean, the generator is able to successfully transform the non-native spectrogram input to a spectrogram with properties of self-imitating feedback. Furthermore, the transformed spectrogram shows segmental corrections that cannot be obtained by prosodic transplantation. Perceptual test comparing the self-imitating and correcting abilities of our method with the baseline PSOLA method shows that the generative approach with cycle consistency loss is promising",
    "checked": true,
    "id": "5b378c2a4ae964fb8ac25d7b6ff689fb31880c6c",
    "semantic_title": "self-imitating feedback generation using gan for computer-assisted pronunciation training",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hori19_interspeech.html": {
    "title": "Joint Student-Teacher Learning for Audio-Visual Scene-Aware Dialog",
    "volume": "main",
    "abstract": "Multimodal fusion of audio, vision, and text has demonstrated significant benefits in advancing the performance of several tasks, including machine translation, video captioning, and video summarization. Audio-Visual Scene-aware Dialog (AVSD) is a new and more challenging task, proposed recently, that focuses on generating sentence responses to questions that are asked in a dialog about video content. While prior approaches designed to tackle this task have shown the need for multimodal fusion to improve response quality, the best-performing systems often rely heavily on human-generated summaries of the video content, which are unavailable when such systems are deployed in real-world. This paper investigates how to compensate for such information, which is missing in the inference phase but available during the training phase. To this end, we propose a novel AVSD system using student-teacher learning, in which a student network is (jointly) trained to mimic the teacher's responses. Our experiments demonstrate that in addition to yielding state-of-the-art accuracy against the baseline DSTC7-AVSD system, the proposed approach (which does not use human-generated summaries at test time) performs competitively with methods that do use those summaries",
    "checked": true,
    "id": "f78c136471778771c29fb385d3a8c1a1def28de1",
    "semantic_title": "joint student-teacher learning for audio-visual scene-aware dialog",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gopalakrishnan19_interspeech.html": {
    "title": "Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations",
    "volume": "main",
    "abstract": "Building socialbots that can have deep, engaging open-domain conversations with humans is one of the grand challenges of artificial intelligence (AI). To this end, bots need to be able to leverage world knowledge spanning several domains effectively when conversing with humans who have their own world knowledge. Existing knowledge-grounded conversation datasets are primarily stylized with explicit roles for conversation partners. These datasets also do not explore depth or breadth of topical coverage with transitions in conversations. We introduce Topical-Chat, a knowledge-grounded human-human conversation dataset where the underlying knowledge spans 8 broad topics and conversation partners don't have explicitly defined roles, to help further research in open-domain conversational AI. We also train several state-of-the-art encoder-decoder conversational models on Topical-Chat and perform automated and human evaluation for benchmarking",
    "checked": true,
    "id": "980456f50cd4b6e30649592afb693d5b6af8a703",
    "semantic_title": "topical-chat: towards knowledge-grounded open-domain conversations",
    "citation_count": 225
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kubasova19_interspeech.html": {
    "title": "Analyzing Verbal and Nonverbal Features for Predicting Group Performance",
    "volume": "main",
    "abstract": "This work analyzes the efficacy of verbal and nonverbal features of group conversation for the task of automatic prediction of group task performance. We describe a new publicly available survival task dataset that was collected and annotated to facilitate this prediction task. In these experiments, the new dataset is merged with an existing survival task dataset, allowing us to compare feature sets on a much larger amount of data than has been used in recent related work. This work is also distinct from related research on social signal processing (SSP) in that we compare verbal and nonverbal features, whereas SSP is almost exclusively concerned with nonverbal aspects of social interaction. A key finding is that nonverbal features from the speech signal are extremely effective for this task, even on their own. However, the most effective individual features are verbal features, and we highlight the most important ones",
    "checked": true,
    "id": "93891ca3b4d54ac985b6e1cef329c27ac73b6354",
    "semantic_title": "analyzing verbal and nonverbal features for predicting group performance",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2019/martinez19_interspeech.html": {
    "title": "Identifying Therapist and Client Personae for Therapeutic Alliance Estimation",
    "volume": "main",
    "abstract": "Psychotherapy, from a narrative perspective, is the process in which a client relates an on-going life-story to a therapist. In each session, a client will recount events from their life, some of which stand out as more significant than others. These significant stories can ultimately shape one's identity. In this work we study these narratives in the context of therapeutic alliance — a self-reported measure on the perception of a shared bond between client and therapist. We propose that alliance can be predicted from the interactions between certain types of clients with types of therapists. To validate this method, we obtained 1235 transcribed sessions with client-reported alliance to train an unsupervised approach to discover groups of therapists and clients based on common types of narrative characters, or personae. We measure the strength of the relation between personae and alliance in two experiments. Our results show that (1) alliance can be explained by the interactions between the discovered character types, and (2) models trained on therapist and client personae achieve significant performance gains compared to competitive supervised baselines. Finally, exploratory analysis reveals important character traits that lead to an improved perception of alliance",
    "checked": true,
    "id": "13ba6810b92a262a0af875d325dd79a7b892cd47",
    "semantic_title": "identifying therapist and client personae for therapeutic alliance estimation",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/haake19_interspeech.html": {
    "title": "Do Hesitations Facilitate Processing of Partially Defective System Utterances? An Exploratory Eye Tracking Study",
    "volume": "main",
    "abstract": "Spoken dialogue systems are predominantly evaluated using offline methods such as user ratings or task-oriented measures. Various phenomena in conversational speech, however, are known to affect the way the listener's comprehension unfolds over time, and not necessarily the final result of the comprehension process. For instance, in human reference comprehension, conversational signals like hesitations have been shown to ease processing of expressions referring to difficult-to-describe targets, as can primarily be observed in listeners' anticipatory eye movements rather than in their final reference resolution decision. In this study, we explore eye tracking for testing conversational dialogue systems, looking at how listeners process automatically generated referring expressions containing defective attributes. We investigate whether hesitations facilitate the processing of partially defective system utterances and track the user's eye movements when listening to expressions with: (i) semantically defective but fluently synthesized adjectives, (ii) defective and lengthened adjectives, i.e. containing a conversational uncertainty signal. Our results are encouraging: whereas the offline measure of task success does not show any differences between the two conditions, the listeners' eye movements suggest that processing of partially defective utterances might be facilitated by conversational hesitations",
    "checked": true,
    "id": "b50615d6f20d69ed2ddaacb8910b34e1682e9c8f",
    "semantic_title": "do hesitations facilitate processing of partially defective system utterances? an exploratory eye tracking study",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19h_interspeech.html": {
    "title": "Influence of Contextuality on Prosodic Realization of Information Structure in Chinese Dialogues",
    "volume": "main",
    "abstract": "In this paper, we present a detailed investigation on the influence of contextuality on the prosodic realization of information structure in Chinese dialogues. The materials were selected from the 863 corpus, which contains both isolated sentences and spontaneous dialogues. RefLex was selected as the annotation scheme, which differentiates information structure on the lexical and referential levels. Prosodic data (including duration and pitch range) from 12 groups of spontaneous dialogues were analyzed with the linear mixed effects mode, and each of them consists of 13–22 turns. The isolated sentences corresponding to these dialogues were also analyzed. The analysis results reveal the influence of contextuality. Specifically, the features of prosodic realization of information structure on the lexical and referential levels show a contrary tendency. The statistical analysis indicates that the speakers use duration and pitch ranges as phonetic cues to distinguish information structures on both levels. On the other hand, duration on the referential level is the only phonetic cue affected by contextuality",
    "checked": true,
    "id": "1424284bb70a3417be9b2b41c5566230c02722c1",
    "semantic_title": "influence of contextuality on prosodic realization of information structure in chinese dialogues",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gjoreski19_interspeech.html": {
    "title": "Cross-Lingual Transfer Learning for Affective Spoken Dialogue Systems",
    "volume": "main",
    "abstract": "This paper presents a case study of cross-lingual transfer learning applied for affective computing in the domain of spoken dialogue systems. Prosodic features of correction dialog acts are modeled on a group of languages and compared with languages excluded from the analysis Speech from different languages was recorded in carefully staged Wizard-of-Oz experiments, however, without the possibility to ensure balanced distribution of speakers per language. In order to assess the possibility of cross-lingual transfer learning and to ensure reliable classification of corrections independently of language, we employed different machine learning approaches along with relevant acoustic-prosodic features sets The results of the experiments with mono-lingual corpora (trained and tested on a single language) and cross-lingual (trained on several languages and tested on the rest) were analyzed and compared in the terms of accuracy and F1 score",
    "checked": true,
    "id": "f3b9a79b1b099fa0f2f22512f823e14063b8ae0a",
    "semantic_title": "cross-lingual transfer learning for affective spoken dialogue systems",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yu19_interspeech.html": {
    "title": "Identifying Personality Traits Using Overlap Dynamics in Multiparty Dialogue",
    "volume": "main",
    "abstract": "Research on human spoken language has shown that speech plays an important role in identifying speaker personality traits. In this work, we propose an approach for identifying speaker personality traits using overlap dynamics in multiparty spoken dialogues. We first define a set of novel features representing the overlap dynamics of each speaker. We then investigate the impact of speaker personality traits on these features using ANOVA tests. We find that features of overlap dynamics significantly vary for speakers with different levels of both Extraversion and Conscientiousness. Finally, we find that classifiers using only overlap dynamics features outperform random guessing in identifying Extraversion and Agreeableness, and that the improvements are statistically significant",
    "checked": true,
    "id": "9e515d969535a4b35f02cb4644ac427806c6d113",
    "semantic_title": "identifying personality traits using overlap dynamics in multiparty dialogue",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/aldeneh19_interspeech.html": {
    "title": "Identifying Mood Episodes Using Dialogue Features from Clinical Interviews",
    "volume": "main",
    "abstract": "Bipolar disorder, a severe chronic mental illness characterized by pathological mood swings from depression to mania, requires ongoing symptom severity tracking to both guide and measure treatments that are critical for maintaining long-term health. Mental health professionals assess symptom severity through semi-structured clinical interviews. During these interviews, they observe their patients' spoken behaviors, including both what the patients say and how they say it. In this work, we move beyond acoustic and lexical information, investigating how higher-level interactive patterns also change during mood episodes. We then perform a secondary analysis, asking if these interactive patterns, measured through dialogue features, can be used in conjunction with acoustic features to automatically recognize mood episodes. Our results show that it is beneficial to consider dialogue features when analyzing and building automated systems for predicting and monitoring mood",
    "checked": true,
    "id": "b5b5777a4b2ba959457822412a8e6d5131bca870",
    "semantic_title": "identifying mood episodes using dialogue features from clinical interviews",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lubold19_interspeech.html": {
    "title": "Do Conversational Partners Entrain on Articulatory Precision?",
    "volume": "main",
    "abstract": "The communication phenomenon known as conversational entrainment occurs when dialogue partners align or adapt their behavior to one another while conversing. Associated with rapport, trust, and communicative efficiency, entrainment appears to facilitate conversational success. In this work, we explore how conversational partners entrain or align on articulatory precision or the clarity with which speakers articulate their spoken productions. Articulatory precision also has implications for conversational success as precise articulation can enhance speech understanding and intelligibility. However, in conversational speech, speakers tend to reduce their articulatory precision, preferring low-cost, imprecise speech. Speakers may adapt their articulation and become more precise depending on feedback from their listeners. Given the potential of entrainment, we are interested in how conversational partners adapt or entrain their articulatory precision to one another. We explore this phenomenon in 57 task-based dialogues. Controlling for the influence of speaking rate, we find that speakers entrain on articulatory precision, with significant alignment on articulation of consonants. We discuss the potential applications that speaker alignment on precision might have for modeling conversation and implementing strategies for enhancing communicative success in human-human and human-computer interactions",
    "checked": true,
    "id": "d6a65211033fb4cf62523f4a293c976477dcad8d",
    "semantic_title": "do conversational partners entrain on articulatory precision?",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lian19_interspeech.html": {
    "title": "Conversational Emotion Analysis via Attention Mechanisms",
    "volume": "main",
    "abstract": "Different from the emotion recognition in individual utterances, we propose a multimodal learning framework using relation and dependencies among the utterances for conversational emotion analysis. The attention mechanism is applied to the fusion of the acoustic and lexical features. Then these fusion representations are fed into the self-attention based bi-directional gated recurrent unit (GRU) layer to capture long-term contextual information. To imitate real interaction patterns of different speakers, speaker embeddings are also utilized as additional inputs to distinguish the speaker identities during conversational dialogs. To verify the effectiveness of the proposed method, we conduct experiments on the IEMOCAP database. Experimental results demonstrate that our method shows absolute 2.42% performance improvement over the state-of-the-art strategies",
    "checked": true,
    "id": "7ca22be345ec2b0af792e695dd6846e6218881b1",
    "semantic_title": "conversational emotion analysis via attention mechanisms",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2019/oneill19_interspeech.html": {
    "title": "The Effect of Phoneme Distribution on Perceptual Similarity in English",
    "volume": "main",
    "abstract": "This paper investigates the extent to which native speaker perceptions regarding the similarity between phonemes of English are influenced by their distributional properties. A similarity hierarchy model based on the distribution of consonantal phonemes in the English language was generated by creating phoneme-embeddings from contextual information. We compare this to similarity models based on phonological feature theory and on native speaker perception. Characteristics of the perception-based model are shown to appear in the distribution-based model whilst not being captured by the feature-based model. This not only provides evidence of similarity perceptions being influenced by distributional properties but is an argument for incorporating distributional information alongside phonological features when modelling perceptual similarity",
    "checked": true,
    "id": "ea0f373e9b4a0e7bc67a2525d26ae2e89fc3ded3",
    "semantic_title": "the effect of phoneme distribution on perceptual similarity in english",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kakouros19_interspeech.html": {
    "title": "Prosodic Representations of Prominence Classification Neural Networks and Autoencoders Using Bottleneck Features",
    "volume": "main",
    "abstract": "Prominence perception has been known to correlate with a complex interplay of the acoustic features of energy, fundamental frequency, spectral tilt, and duration. The contribution and importance of each of these features in distinguishing between prominent and non-prominent units in speech is not always easy to determine, and more so, the prosodic representations that humans and automatic classifiers learn have been difficult to interpret. This work focuses on examining the acoustic prosodic representations that binary prominence classification neural networks and autoencoders learn for prominence. We investigate the complex features learned at different layers of the network as well as the 10-dimensional bottleneck features (BNFs), for the standard acoustic prosodic correlates of prominence separately and in combination. We analyze and visualize the BNFs obtained from the prominence classification neural networks as well as their network activations. The experiments are conducted on a corpus of Dutch continuous speech with manually annotated prominence labels. Our results show that the prosodic representations obtained from the BNFs and higher-dimensional non-BNFs provide good separation of the two prominence categories, with, however, different partitioning of the BNF space for the distinct features, and the best overall separation obtained for F0",
    "checked": true,
    "id": "9099691d93d630c23575cd2725d3d33e50c1ee42",
    "semantic_title": "prosodic representations of prominence classification neural networks and autoencoders using bottleneck features",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/peperkamp19_interspeech.html": {
    "title": "Compensation for French Liquid Deletion During Auditory Sentence Processing",
    "volume": "main",
    "abstract": "Phonological rules change the surface realization of words. Listeners undo these changes in order to retrieve the canonical word form. We investigate this so-called compensation for a French deletion rule, i.e. liquid deletion. This rule optionally deletes the final consonant of a word-final obstruent-liquid cluster. It can apply both before consonants and before vowels, but its application is about twice as frequent before consonants. Using a word detection task, we find an overall relatively low rate of compensation, which we argue is due to the relatively high perceptual salience of the rule. We also observe a clear effect of context, though: listeners compensate more than twice as often for a deleted liquid before a consonant than before a vowel. This is evidence that compensation involves fine-grained knowledge about the probability of the rule's application in different contexts",
    "checked": true,
    "id": "5cdb5182871bb33e1b785c5e580eb906d5515163",
    "semantic_title": "compensation for french liquid deletion during auditory sentence processing",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kocharov19_interspeech.html": {
    "title": "Prosodic Factors Influencing Vowel Reduction in Russian",
    "volume": "main",
    "abstract": "Unstressed vowels in Russian are reduced in both duration and quality, but these two manifestations of vowel reduction do not have to be observed simultaneously. In order to investigate this question, we analysed the reduction pattern of words in such contexts where lengthening is induced by prosodic factors: prominence and pre-boundary lengthening. The study is based on a large corpus of read speech. The following results were obtained: (1) as expected, both contexts increase vowel duration; (2) under prosodic prominence vowels undergo less qualitative reduction, while pre-boundary lengthening has no effect on qualitative reduction; (3) additionally, it was shown that prominence mainly affects the pretonic part of the word, while pre-boundary lengthening — the post-tonic part. Thus, an increase in vowel duration does not always cause a decrease in qualitative reduction, which may serve as evidence against the idea that qualitative reduction is caused by quantitative reduction. Additionally, these results may serve as an argument for the idea that the two processes — vowel reduction and temporal organization of utterance — are autonomous",
    "checked": true,
    "id": "b69b002748b77b11000faa32b833f644cc3b6baa",
    "semantic_title": "prosodic factors influencing vowel reduction in russian",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gobl19_interspeech.html": {
    "title": "Time to Frequency Domain Mapping of the Voice Source: The Influence of Open Quotient and Glottal Skew on the Low End of the Source Spectrum",
    "volume": "main",
    "abstract": "This paper explores the mapping of time and frequency domain aspects of the voice source, focussing on the low end of the source spectrum. It refines and extends an earlier study, where the LF model was used to explore the correspondences between the open quotient (O ), glottal skew (R ) and harmonic levels of the source spectrum, including the H1-H2 measure, widely assumed to reflect differences in O Here we use a different model (the F-model) as it better reflects the effective open quotient and glottal skew in certain conditions. As in the earlier study, a series of glottal pulses were generated, keeping peak glottal flow constant, while systematically varying O and R Results suggest that the effects of R on the low harmonics is considerably less than estimated in the earlier study, and its main impact is on the level of H2 (and consequently H1-H2) when O is relatively high. The conclusion remains that the H1-H2 is not simply a direct reflection of O However, for O values of up to about 0.6, it maps closely to H1-H2: beyond this point, H1-H2 reflects a more complex interaction of open quotient and glottal skew",
    "checked": true,
    "id": "a19d235b005978fafec7766d828e09f72beafc17",
    "semantic_title": "time to frequency domain mapping of the voice source: the influence of open quotient and glottal skew on the low end of the source spectrum",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chodroff19_interspeech.html": {
    "title": "Testing the Distinctiveness of Intonational Tunes: Evidence from Imitative Productions in American English",
    "volume": "main",
    "abstract": "Understanding the structure of intonational variation is a longstanding issue in prosodic research. A given utterance can be realized with countless intonational contours, and while variation in prosodic meaning is also large, listeners nevertheless converge on relatively consistent form-function mappings. While this suggests the existence of abstract intonational representations, it has been unclear how exactly these are defined. The present study examines the validity of a well-defined set of phonological representations for the generation of intonation in the nuclear region of an intonational phrase in American English: namely, the combination of binary pitch accents (H*/L*), phrase accents (H-/L-), and boundary tones (H%/L%) proposed in Pierrehumbert (1980). In an exploratory study, we examined whether speakers maintained the eight-way distinction among intonational contours posited to exist in this representational system. We created eight synthesized contours according to Pierrehumbert (1980) and examined whether listeners generalized these contours to novel productions. Speakers largely distinguished rising from non-rising contours in production, but few other distinctions were maintained. While this does not rule out the existence of additional contours in production, these findings do suggest that the representation of rising and non-rising contours may be privileged and more readily accessible in the intonational grammar",
    "checked": true,
    "id": "51fad0610ef0a42738f75d11be2af8313fee3afe",
    "semantic_title": "testing the distinctiveness of intonational tunes: evidence from imitative productions in american english",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/park19d_interspeech.html": {
    "title": "A Study of a Cross-Language Perception Based on Cortical Analysis Using Biomimetic STRFs",
    "volume": "main",
    "abstract": "For those in the early stage of learning a foreign language, they commonly experience difficulties in understanding spoken words in the second language, while they have no problem in recognizing words spoken in their mother tongue. This paper examines this phenomenon using biomimetic receptive fields that can be interpreted as a transfer function between acoustic stimulus and cortical responses in the brain. While receptive fields of individual subjects are often optimized to recognize unique phonemes in their mother language, it is unclear whether challenges associated with acquiring a new language (especially in adulthood) is due to a mismatch between phonemic characteristics in the new language and optimized processing in the system. We explore this question by contrasting biomimetic systems optimized for four different languages with sufficiently different characteristics. We perform English phoneme classification with these language-optimized systems. We observed distinctive characteristics in receptive fields emerging from each language, and the differences of English phoneme recognition performance accordingly",
    "checked": true,
    "id": "90e41f373a7b203267fa3f34896c07ce7a58af0d",
    "semantic_title": "a study of a cross-language perception based on cortical analysis using biomimetic strfs",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sturm19_interspeech.html": {
    "title": "Perceptual Evaluation of Early versus Late F0 Peaks in the Intonation Structure of Czech Question-Word Questions",
    "volume": "main",
    "abstract": "Question-word questions in Czech lexically mark their interrogative function in the initial position: in their standard form, they begin with an interrogative lexeme. For many linguists, this is a sufficient reason for resigning on intonation marking, so they claim that the speech melody in these questions is identical to the melody of statements. A careful observation of the current Czech speech suggests otherwise This paper presents a perceptual experiment in which Czech speakers evaluated two contrastive forms of the interrogative melody, specifically the one with a late peak modelled after statements (as suggested by some authors), and the one with an early peak modelled after our empirical data collected previously. Thirty-two listeners expressed a statistically significant preference for the early peak in a perception test. This outcome resonates with the sample of speech production of the questions. However, the late peak is also possible and acceptable: we assume that it might be a signal of contrastive emphasis or an implicational cue",
    "checked": true,
    "id": "6ad86ae559e4adddc005001d4738edd4a1af6ed9",
    "semantic_title": "perceptual evaluation of early versus late f0 peaks in the intonation structure of czech question-word questions",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kelterer19_interspeech.html": {
    "title": "Acoustic Correlates of Phonation Type in Chichimec",
    "volume": "main",
    "abstract": "Chichimec is an Oto-Manguean language of Mexico with a phonological contrast between modal, breathy and creaky vowels. This study is the first acoustic investigation of this contrast in Chichimec, based on spectral tilt and Cepstral Peak Prominence (CPP) measures. We consider the change of these measures over the course of the vowel and include a high vowel, which was omitted in most phonation studies of other languages. The present study not only contributes to the description of Chichimec with respect to the different portions of the vowel, but also explores the adequacy of the acoustic measures of phonation type for low and high vowels Our results show that phonation changes in the course of the vowel, and that this change is a relevant factor for phonation types in Chichimec. We find that CPP is the best measure to characterize Chichimec phonation contrasts in all vowels. For the vowel /a/, spectral tilt measures are better indicators of phonation type for women than for men. The results for /i/ indicate that spectral tilt distinguishes breathy from modal vowels for men, but that these measures might generally not be appropriate to describe phonation contrasts in women's high vowels",
    "checked": true,
    "id": "ff8bef401b7d23286da6851407d1c1f736f87e88",
    "semantic_title": "acoustic correlates of phonation type in chichimec",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chien19c_interspeech.html": {
    "title": "F0 Variability Measures Based on Glottal Closure Instants",
    "volume": "main",
    "abstract": "The periodicity of a voiced-sound signal can reflect physiological conditions such as identity, age, and voice disorder. One way to look into this periodicity is to measure the temporal variability of vocal fundamental frequency (F0). This paper proposes 2 measures of F0 variability based on glottal closure instant (GCI). GCI is essential to the detection of F0 when the signal waveform varies substantially between adjacent cycles, e.g., in breathy voice. Frequency-selective variability measurements are taken from an interpolated sequence of fundamental-period values based on GCIs, including certain spectral-shape parameters which constitute a multi-variate measure. The utility of these measures was demonstrated in two experiments designed for inter- and intra-speaker comparisons, respectively",
    "checked": true,
    "id": "0fea803b7937fc4c331fc68e15e239367b0eb498",
    "semantic_title": "f0 variability measures based on glottal closure instants",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tavi19_interspeech.html": {
    "title": "Recognition of Creaky Voice from Emergency Calls",
    "volume": "main",
    "abstract": "Although creaky voice, or vocal fry, is widely studied phonation mode, open questions still exist in creak's acoustic characterization and automatic recognition. Many questions are open since creak varies significantly depending on conversational context. In this study, we introduce an exploratory creak recognizer based on convolutional neural network (CNN), which is generated specifically for emergency calls. The study focuses on recognition of creaky voice from authentic emergency calls because creak detection could potentially provide information about the caller's emotional state or attempt of voice disguise. We generated the CNN recognition system using emergency call recordings and other out-of-domain speech recordings and compared the results with an already existing and widely used creaky voice detection system: using poor quality emergency call recordings as test data, this system achieved F1 of 0.41 whereas our CNN system accomplished an F1 of 0.64. The results show that the CNN system can perform moderately well using a limited amount of training data on challenging testing data and has the potential to achieve higher F scores when more emergency calls are used for model training",
    "checked": true,
    "id": "6b3ed4c4a1c35d6b4c33b2aad5051900cc68e42b",
    "semantic_title": "recognition of creaky voice from emergency calls",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xu19b_interspeech.html": {
    "title": "Direct F0 Estimation with Neural-Network-Based Regression",
    "volume": "main",
    "abstract": "Pitch tracking, or the continuous extraction of fundamental frequency from speech waveforms, is of vital importance to many applications in speech analysis and synthesis. Many existing trackers, including conventional ones such as Praat, RAPT and YIN, and newly proposed neural-network-based ones such as DNN-CLS, CREPE and RNN-REG, have conducted an extensive investigation into speech pitch tracking. This work developed a different end-to-end regression model based on neural networks, where a voice detector and a newly proposed value estimator work jointly to highlight the trajectory of fundamental frequency. Experiments on the PTDB-TUG corpus showed that the system surpasses canonical neural networks in terms of gross error rate. It further outperformed conventional trackers under clean condition and neural-network classifiers under noisy condition by the NOISEX-92 corpus",
    "checked": true,
    "id": "8467f264c2b3cda6de7da2371e6bab5e9eee366b",
    "semantic_title": "direct f0 estimation with neural-network-based regression",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sharma19b_interspeech.html": {
    "title": "Real Time Online Visual End Point Detection Using Unidirectional LSTM",
    "volume": "main",
    "abstract": "Visual Voice Activity Detection (V-VAD) involves the detection of speech activity of a speaker using visual features. The V-VAD is useful in detecting the end point of an utterance under noisy acoustic conditions or for maintaining speaker privacy. In this paper, we propose a speaker independent, real-time solution for V-VAD. The focus is on real-time aspect and accuracy as such algorithms will play a key role in detecting end point especially while interacting with speech assistants. We propose two novel methods one using CNN and the other using 2D-DCT features. Unidirectional LSTMs are used in both the methods to make it online and learn temporal dependence. The methods are tested on two publicly available datasets. Additionally the methods are also tested on a locally collected dataset which further validates our hypothesis. Additionally it has been observed through experiments that both the approaches generalize to unseen speakers. It has been shown that our best approach gives substantial improvement over earlier methods done on the same dataset",
    "checked": true,
    "id": "58fce6a18b888d96a41354bfc55b75f7f312ae21",
    "semantic_title": "real time online visual end point detection using unidirectional lstm",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ardaillon19_interspeech.html": {
    "title": "Fully-Convolutional Network for Pitch Estimation of Speech Signals",
    "volume": "main",
    "abstract": "The estimation of fundamental frequency (F ) from audio is a necessary step in many speech processing tasks such as speech synthesis, that require to accurately analyze big datasets, or real-time voice transformations, that require low computation times. New approaches using neural networks have been recently proposed for F estimation, outperforming previous approaches in terms of accuracy. The work presented here aims at bringing some more improvements over such CNN-based state-of-the-art approaches, especially when targeting speech data. More specifically, we first propose to use the recent PaN speech synthesis engine in order to generate a high-quality speech database with a reliable ground truth F annotation. Then, we propose 3 variants of a new fully-convolutional network (FCN) architecture that are shown to perform better than other similar data-driven methods, with a significantly reduced computational load making them more suitable for real-time purposes",
    "checked": true,
    "id": "fd350e10baac1e51f730d548c037e3d02b5ed938",
    "semantic_title": "fully-convolutional network for pitch estimation of speech signals",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dong19_interspeech.html": {
    "title": "Vocal Pitch Extraction in Polyphonic Music Using Convolutional Residual Network",
    "volume": "main",
    "abstract": "Pitch extraction, also known as fundamental frequency estimation, is a long-term task in audio signal processing. Especially, due to the presence of accompaniment, vocal pitch extraction in polyphonic music is more challenging. So far, most of deep learning approaches use log mel spectrogram as input, which neglect the phase information. In addition, shallow networks have been applied on waveform directly, which may not handle contaminated vocal data well. In this paper, a deep convolutional residual network is proposed. It analyzes and extracts effective feature from waveform automatically. Residual learning can reduce model degradation due to the skip connection and residual mapping. In comparison to reported results, the proposed approach shows 5% and 4% improvement on overall accuracy(OA) and raw pitch accuracy(RPA) respectively",
    "checked": true,
    "id": "53823d31235e6e22fb4e8dd9997c182232e99714",
    "semantic_title": "vocal pitch extraction in polyphonic music using convolutional residual network",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sharma19c_interspeech.html": {
    "title": "Multi-Level Adaptive Speech Activity Detector for Speech in Naturalistic Environments",
    "volume": "main",
    "abstract": "Speech activity detection (SAD) is a part of many speech processing applications. The traditional SAD approaches use signal energy as the evidence to identify the speech regions. However, such methods perform poorly under uncontrolled environments. In this work, we propose a novel SAD approach using a multi-level decision with signal knowledge in an adaptive manner. The multi-level evidence considered are modulation spectrum and smoothed Hilbert envelope of linear prediction (LP) residual. Modulation spectrum has compelling parallels to the dynamics of speech production and captures information only for the speech component. Contrarily, Hilbert envelope of LP residual captures excitation source aspect of speech. Under uncontrolled scenario, these evidence are found to be robust towards the signal distortions and thus expected to work well. In view of different levels of interference present in the signal, we propose to use a quality factor to control the speech/non-speech decision in an adaptive manner. We refer this method as multi-level adaptive SAD and evaluate on Fearless Steps corpus that is collected during Apollo-11 Mission in naturalistic environments. We achieve a detection cost function of 7.35% with the proposed multi-level adaptive SAD on the evaluation set of Fearless Steps 2019 challenge corpus",
    "checked": true,
    "id": "359c9784d1eaea21f32f4208b106c8ec1a07e603",
    "semantic_title": "multi-level adaptive speech activity detector for speech in naturalistic environments",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sharma19d_interspeech.html": {
    "title": "On the Importance of Audio-Source Separation for Singer Identification in Polyphonic Music",
    "volume": "main",
    "abstract": "Singer identification is to automatically identify the singer in a music recording, such as a polyphonic song. A song has two major acoustic components that are singing vocals and background accompaniment. Although identifying singers is similar to speaker identification, it is challenging due to the interference of background accompaniment on the singer-specific information in singing vocals. We believe that separating the background accompaniment from the singing vocal will help us to overcome the interference. In this work, we extract the singing vocals from polyphonic songs using Wave-U-Net based audio-source separation approach. The extracted singing vocals are then used in i-vector based singer identification system. Further, we explore different state-of-the-art audio-source separation methods to establish the role of considered method in application to singer identification. The proposed singer identification framework achieves an absolute accuracy improvement of 5.66% over the baseline without audio-source separation",
    "checked": true,
    "id": "05385b7f05b28b4fca0df6e4c6ae5281c92447e0",
    "semantic_title": "on the importance of audio-source separation for singer identification in polyphonic music",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/terasawa19_interspeech.html": {
    "title": "Investigating the Physiological and Acoustic Contrasts Between Choral and Operatic Singing",
    "volume": "main",
    "abstract": "In this study, the difference in glottal vibration and timbre of singing voice in choral and operatic singing was investigated. Eight professional singers with active careers in operatic and choral performances participated in the experiment and sang excerpts from three operatic songs and two choral songs. Audio and electroglottograph signals were simultaneously recorded. The open quotient (O ) and singing power ratio (SPR) of the voices were analyzed, and it was found that the O of choral singing tends to be higher and the SPR of choral singing tends to be lower than those of operatic singing. This suggests that choral singing is conducted with laxer vocal fold coordination, and it has less ringing timbre than operatic singing. However, the O and SPR were not directly correlated: the degree of adjustment of SPR differed across singers, suggesting that the strategy to achieve a desired voice quality is individualistic in nature",
    "checked": true,
    "id": "2f86a382d9ca02cff6c4e01ad6ba424668a65f2e",
    "semantic_title": "investigating the physiological and acoustic contrasts between choral and operatic singing",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lin19c_interspeech.html": {
    "title": "Optimizing Voice Activity Detection for Noisy Conditions",
    "volume": "main",
    "abstract": "In this work, we focus our attention on how to improve Voice Activity Detection (VAD) in noisy conditions. We propose a Convolutional Neural Network (CNN) based model, as well as a Denoising Autoencoder (DAE), and experiment against acoustic features and their delta features in noise levels ranging from SNR 35 dB to 0 dB. The experiments compare and find the best model configuration for robust performance in noisy conditions. We observe that combining more expressive audio features with the use of DAEs improve accuracy, especially as noise increases. At 0 dB, the proposed model trained with the best feature set could achieve a lab test accuracy of 93.2% (averaged across all noise levels) and 88.6% inference accuracy on device. We also compress the neural network and deploy the inference model that is optimized for the app so that the average on-device CPU usage is reduced to 14% from 37%",
    "checked": true,
    "id": "1c36b20a2c11eb7382c766fe9574010c195ca2f1",
    "semantic_title": "optimizing voice activity detection for noisy conditions",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yamamoto19c_interspeech.html": {
    "title": "Small-Footprint Magic Word Detection Method Using Convolutional LSTM Neural Network",
    "volume": "main",
    "abstract": "The number of consumer devices which can be operated by voice is increasing every year. Magic Word Detection (MWD), the detection of an activation keyword in continuous speech, has become an essential technology for the hands-free operation of such devices. Because MWD systems need to run constantly in order to detect Magic Words at any time, many studies have focused on the development of a small-footprint system. In this paper, we propose a novel, small-footprint MWD method which uses a convolutional Long Short-Term Memory (LSTM) neural network to capture frequency and time domain features over time. As a result, the proposed method outperforms the baseline method while reducing the number of parameters by more than 80%. An experiment on a small-scale device demonstrates that our model is efficient enough to function in real time",
    "checked": true,
    "id": "dc3e777f8c13d6aaa750bc368c326aac0602d1f0",
    "semantic_title": "small-footprint magic word detection method using convolutional lstm neural network",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gupta19b_interspeech.html": {
    "title": "Acoustic Modeling for Automatic Lyrics-to-Audio Alignment",
    "volume": "main",
    "abstract": "Automatic lyrics to polyphonic audio alignment is a challenging task not only because the vocals are corrupted by background music, but also there is a lack of annotated polyphonic corpus for effective acoustic modeling. In this work, we propose (1) using additional speech and music-informed features and (2) adapting the acoustic models trained on a large amount of solo singing vocals towards polyphonic music using a small amount of in-domain data. Incorporating additional information such as voicing and auditory features together with conventional acoustic features aims to bring robustness against the increased spectro-temporal variations in singing vocals. By adapting the acoustic model using a small amount of polyphonic audio data, we reduce the domain mismatch between training and testing data. We perform several alignment experiments and present an in-depth alignment error analysis on acoustic features, and model adaptation techniques. The results demonstrate that the proposed strategy provides a significant error reduction of word boundary alignment over comparable existing systems, especially on more challenging polyphonic data with long-duration musical interludes",
    "checked": true,
    "id": "60469bda847b93abe548f85e3ea4a52d8ef0c70b",
    "semantic_title": "acoustic modeling for automatic lyrics-to-audio alignment",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vafeiadis19_interspeech.html": {
    "title": "Two-Dimensional Convolutional Recurrent Neural Networks for Speech Activity Detection",
    "volume": "main",
    "abstract": "Speech Activity Detection (SAD) plays an important role in mobile communications and automatic speech recognition (ASR). Developing efficient SAD systems for real-world applications is a challenging task due to the presence of noise. We propose a new approach to SAD where we treat it as a two-dimensional multilabel image classification problem. To classify the audio segments, we compute their Short-time Fourier Transform spectrograms and classify them with a Convolutional Recurrent Neural Network (CRNN), traditionally used in image recognition. Our CRNN uses a sigmoid activation function, max-pooling in the frequency domain, and a convolutional operation as a moving average filter to remove misclassified spikes. On the development set of Task 1 of the 2019 Fearless Steps Challenge, our system achieved a decision cost function (DCF) of 2.89%, a 66.4% improvement over the baseline. Moreover, it achieved a DCF score of 3.318% on the evaluation dataset of the challenge, ranking first among all submissions",
    "checked": true,
    "id": "e0f90ca1aa83bc5eadfbcc33830811ebe3fef871",
    "semantic_title": "two-dimensional convolutional recurrent neural networks for speech activity detection",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kaburagi19_interspeech.html": {
    "title": "A Study of Soprano Singing in Light of the Source-Filter Interaction",
    "volume": "main",
    "abstract": "We examined the physical interaction between the voice source system in the larynx and the acoustic filter of the vocal tract. The vocal tract of a soprano was first scanned in three dimensions using magnetic resonance imaging (MRI) while she produced four musical notes with different vowels. These images were used to simulate voice production, including the vibratory motion of the vocal folds and the behavior of glottal airflow. Images for the /i/ vowel were used in the simulation, because a good proximity relationship was found between the fundamental frequency and the first impedance peak of the vocal tract. The simulation results revealed that the fundamental frequency (vibration frequency of the vocal folds) was decreased to a large extent by the source-filter interaction especially when their natural frequency was in the proximity of the impedance peak. In a specific case, this frequency lowering had the effect of changing the acoustic load of the vocal tract exerted on the vocal folds so that their vibratory motion was effectively assisted",
    "checked": true,
    "id": "eb55231871f61f36000344b6a2d88936f26fa52e",
    "semantic_title": "a study of soprano singing in light of the source-filter interaction",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zou19_interspeech.html": {
    "title": "Boosting Character-Based Chinese Speech Synthesis via Multi-Task Learning and Dictionary Tutoring",
    "volume": "main",
    "abstract": "Recent character-based end-to-end text-to-speech (TTS) systems have shown promising performance in natural speech generation, especially for English. However, for Chinese TTS, the character-based model is easy to generate speech with wrong pronunciation due to the label sparsity issue. To address this issue, we introduce an additional learning task of character-to-pinyin mapping to boost the pronunciation learning of characters, and leverage a pre-trained dictionary network to correct the pronunciation mistake through joint training. Specifically, our model predicts pinyin labels as an auxiliary task to assist learning better hidden representations of Chinese characters, where pinyin is a standard phonetic representation for Chinese characters. The dictionary network plays a role as a tutor to further help hidden representation learning. Experiments demonstrate that employing the pinyin auxiliary task and an external dictionary network clearly enhances the naturalness and intelligibility of the synthetic speech directly from the Chinese character sequences",
    "checked": true,
    "id": "c82047fcb6f757e3daad1fdacb79553fc706c2c3",
    "semantic_title": "boosting character-based chinese speech synthesis via multi-task learning and dictionary tutoring",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xue19_interspeech.html": {
    "title": "Building a Mixed-Lingual Neural TTS System with Only Monolingual Data",
    "volume": "main",
    "abstract": "When deploying a Chinese neural Text-to-Speech (TTS) system, one of the challenges is to synthesize Chinese utterances with English phrases or words embedded. This paper looks into the problem in the encoder-decoder framework when only monolingual data from a target speaker is available. Specifically, we view the problem from two aspects: speaker consistency within an utterance and naturalness. We start the investigation with an average voice model which is built from multi-speaker monolingual data, i.e., Mandarin and English data. On the basis of that, we look into speaker embedding for speaker consistency within an utterance and phoneme embedding for naturalness and intelligibility, and study the choice of data for model training. We report the findings and discuss the challenges to build a mixed-lingual TTS system with only monolingual data",
    "checked": true,
    "id": "9db5a3d62bf97898ebfae3e5acd946710ad66c65",
    "semantic_title": "building a mixed-lingual neural tts system with only monolingual data",
    "citation_count": 30
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sokolov19_interspeech.html": {
    "title": "Neural Machine Translation for Multilingual Grapheme-to-Phoneme Conversion",
    "volume": "main",
    "abstract": "Grapheme-to-phoneme (G2P) models are a key component in Automatic Speech Recognition (ASR) systems, such as the ASR system in Alexa, as they are used to generate pronunciations for out-of-vocabulary words that do not exist in the pronunciation lexicons (mappings like \"e c h o\" → \"E k oU\") Most G2P systems are monolingual and based on traditional joint-sequence based n-gram models [1, 2]. As an alternative, we present a single end-to-end trained neural G2P model that shares same encoder and decoder across multiple languages. This allows the model to utilize a combination of universal symbol inventories of Latin-like alphabets and cross-linguistically shared feature representations. Such model is especially useful in the scenarios of low resource languages and code switching/ foreign words, where the pronunciations in one language need to be adapted to other locales or accents. We further experiment with word language distribution vector as an additional training target in order to improve system performance by helping the model decouple pronunciations across a variety of languages in the parameter space. We show 7.2% average improvement in phoneme error rate over low resource languages and no degradation over high resource ones compared to monolingual baselines",
    "checked": true,
    "id": "6821e76c20efc7f4d45434dcd92a142a2baab295",
    "semantic_title": "neural machine translation for multilingual grapheme-to-phoneme conversion",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/taylor19_interspeech.html": {
    "title": "Analysis of Pronunciation Learning in End-to-End Speech Synthesis",
    "volume": "main",
    "abstract": "Ensuring correct pronunciation for the widest possible variety of text input is vital for deployed text-to-speech (TTS) systems. For languages such as English that do not have trivial spelling, systems have always relied heavily upon a lexicon, both for pronunciation lookup and for training letter-to-sound (LTS) models as a fall-back to handle out-of-vocabulary words (OOVs). In contrast, recently proposed models that are trained \"end-to-end\" (E2E) aim to avoid linguistic text analysis and any explicit phone representation, instead learning pronunciation implicitly as part of a direct mapping from input characters to speech audio. This might be termed implicit LTS. In this paper, we explore the nature of this approach by training explicit LTS models with datasets commonly used to build E2E systems. We compare their performance with LTS models trained on a high quality English lexicon. We find that LTS errors for words with ambiguous or unpredictable pronunciations are mirrored as mispronunciations by an E2E model. Overall, our analysis suggests that limited and unbalanced lexical coverage in E2E training data may pose significant confounding factors that complicate learning accurate pronunciations in a purely E2E system",
    "checked": true,
    "id": "c9e17537d8b7d1d3fc19d95a77e97f3db2175248",
    "semantic_title": "analysis of pronunciation learning in end-to-end speech synthesis",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19f_interspeech.html": {
    "title": "End-to-End Text-to-Speech for Low-Resource Languages by Cross-Lingual Transfer Learning",
    "volume": "main",
    "abstract": "End-to-end text-to-speech (TTS) has shown great success on large quantities of paired text plus speech data. However, laborious data collection remains difficult for at least 95% of the languages over the world, which hinders the development of TTS in different languages. In this paper, we aim to build TTS systems for such low-resource (target) languages where only very limited paired data are available. We show such TTS can be effectively constructed by transferring knowledge from a high-resource (source) language. Since the model trained on source language cannot be directly applied to target language due to input space mismatch, we propose a method to learn a mapping between source and target linguistic symbols. Benefiting from this learned mapping, pronunciation information can be preserved throughout the transferring procedure. Preliminary experiments show that we only need around 15 minutes of paired data to obtain a relatively good TTS system. Furthermore, analytic studies demonstrated that the automatically discovered mapping correlate well with the phonetic expertise",
    "checked": true,
    "id": "ae2e7e0132019fb3302bab08e8e7248672440cb4",
    "semantic_title": "end-to-end text-to-speech for low-resource languages by cross-lingual transfer learning",
    "citation_count": 67
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19e_interspeech.html": {
    "title": "Learning to Speak Fluently in a Foreign Language: Multilingual Speech Synthesis and Cross-Language Voice Cloning",
    "volume": "main",
    "abstract": "We present a multispeaker, multilingual text-to-speech (TTS) synthesis model based on Tacotron that is able to produce high quality speech in multiple languages. Moreover, the model is able to transfer voices across languages, e.g. synthesize fluent Spanish speech using an English speaker's voice, without training on any bilingual or parallel examples. Such transfer works across distantly related languages, e.g. English and Mandarin Critical to achieving this result are: 1. using a phonemic input representation to encourage sharing of model capacity across languages, and 2. incorporating an adversarial loss term to encourage the model to disentangle its representation of speaker identity (which is perfectly correlated with language in the training data) from the speech content. Further scaling up the model by training on multiple speakers of each language, and incorporating an autoencoding input to help stabilize attention during training, results in a model which can be used to consistently synthesize intelligible speech for training speakers in all languages seen during training, and in native or foreign accents",
    "checked": true,
    "id": "4b45ec780aed51ed3847c6167cab9c4ece1caeec",
    "semantic_title": "learning to speak fluently in a foreign language: multilingual speech synthesis and cross-language voice cloning",
    "citation_count": 138
  },
  "https://www.isca-speech.org/archive/interspeech_2019/juzova19_interspeech.html": {
    "title": "Unified Language-Independent DNN-Based G2P Converter",
    "volume": "main",
    "abstract": "We introduce a unified Grapheme-to-phoneme conversion framework based on the composition of deep neural networks. In contrary to the usual approaches building the G2P frameworks from the dictionary, we use whole phrases, which allows us to capture various language properties, e.g. cross-word assimilation, without the need for any special care or topology adjustments. The evaluation is carried out on three different languages — English, Czech and Russian. Each requires dealing with specific properties, stressing the proposed framework in various ways. The very first results show promising performance of the proposed framework, dealing with all the phenomena specific to the tested languages. Thus, we consider the framework to be language-independent for a wide range of languages",
    "checked": true,
    "id": "d5135a81173b95c5c1b2aa20bf6abd1972e682b2",
    "semantic_title": "unified language-independent dnn-based g2p converter",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dai19_interspeech.html": {
    "title": "Disambiguation of Chinese Polyphones in an End-to-End Framework with Semantic Features Extracted by Pre-Trained BERT",
    "volume": "main",
    "abstract": "Grapheme-to-phoneme (G2P) conversion serves as an essential component in Chinese Mandarin text-to-speech (TTS) system, where polyphone disambiguation is the core issue. In this paper, we propose an end-to-end framework to predict the pronunciation of polyphonic character, which accepts sentence containing polyphonic character as input in the form of Chinese character sequence without the necessity of any preprocessing. The proposed method consists of a pre-trained bidirectional encoder representations from Transformers (BERT) model and a neural network (NN) based classifier. The pre-trained BERT model extracts semantic features from raw Chinese character sequence and the NN based classifier predicts the polyphonic character's pronunciation according to BERT output. To explore the impact of contextual information on polyphone disambiguation, three different classifiers are investigated: a fully-connected network based classifier, a long short-term memory (LSTM) network based classifier and a Transformer block based classifier. Experimental results demonstrate the effectiveness of the proposed end-to-end framework for polyphone disambiguation and the semantic features extracted by BERT can greatly enhance the performance",
    "checked": true,
    "id": "28e9825fd3171f9d50bad544e160f675d0cdcaa5",
    "semantic_title": "disambiguation of chinese polyphones in an end-to-end framework with semantic features extracted by pre-trained bert",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yolchuyeva19_interspeech.html": {
    "title": "Transformer Based Grapheme-to-Phoneme Conversion",
    "volume": "main",
    "abstract": "Attention mechanism is one of the most successful techniques in deep learning based Natural Language Processing (NLP). The transformer network architecture is completely based on attention mechanisms, and it outperforms sequence-to-sequence models in neural machine translation without recurrent and convolutional layers. Grapheme-to-phoneme (G2P) conversion is a task of converting letters (grapheme sequence) to their pronunciations (phoneme sequence). It plays a significant role in text-to-speech (TTS) and automatic speech recognition (ASR) systems. In this paper, we investigate the application of transformer architecture to G2P conversion and compare its performance with recurrent and convolutional neural network based approaches. Phoneme and word error rates are evaluated on the CMUDict dataset for US English and the NetTalk dataset. The results show that transformer based G2P outperforms the convolutional-based approach in terms of word error rate and our results significantly exceeded previous recurrent approaches (without attention) regarding word and phoneme error rates on both datasets. Furthermore, the size of the proposed model is much smaller than the size of the previous approaches",
    "checked": true,
    "id": "4cb319f5c1a7208465d32b4beceb82e958634c1e",
    "semantic_title": "transformer based grapheme-to-phoneme conversion",
    "citation_count": 41
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bleyan19_interspeech.html": {
    "title": "Developing Pronunciation Models in New Languages Faster by Exploiting Common Grapheme-to-Phoneme Correspondences Across Languages",
    "volume": "main",
    "abstract": "We discuss two methods that let us easily create grapheme-to-phoneme (G2P) conversion systems for languages without any human-curated pronunciation lexicons, as long as we know the phoneme inventory of the target language and as long as we have some pronunciation lexicons for other languages written in the same script. We use these resources to infer what grapheme-to-phoneme correspondences we would expect, and predict pronunciations for words in the target language with minimal or no language-specific human work. Our first approach uses finite-state transducers, while our second approach uses a sequence-to-sequence neural network. Our G2P models reach high degrees of accuracy, and can be used for various applications, e.g. in developing an automatic speech recognition system. Our methods greatly simplify a task that has historically required extensive manual labor",
    "checked": true,
    "id": "c72131f83a11af2fec96eb3343d7eed2a7954fbe",
    "semantic_title": "developing pronunciation models in new languages faster by exploiting common grapheme-to-phoneme correspondences across languages",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19g_interspeech.html": {
    "title": "Cross-Lingual, Multi-Speaker Text-To-Speech Synthesis Using Neural Speaker Embedding",
    "volume": "main",
    "abstract": "Neural network-based model for text-to-speech (TTS) synthesis has made significant progress in recent years. In this paper, we present a cross-lingual, multi-speaker neural end-to-end TTS framework which can model speaker characteristics and synthesize speech in different languages. We implement the model by introducing a separately trained neural speaker embedding network, which can represent the latent structure of different speakers and language pronunciations. We train the speech synthesis network bilingually and prove the possibility of synthesizing Chinese speaker's English speech and vice versa. We explore different methods to fit a new speaker using only a few speech samples. The experimental results show that, with only several minutes of audio from a new speaker, the proposed model can synthesize speech bilingually and acquire decent naturalness and similarity for both languages",
    "checked": true,
    "id": "f1705f684a9ad65a7cc7455b9e54c2a69f193ffa",
    "semantic_title": "cross-lingual, multi-speaker text-to-speech synthesis using neural speaker embedding",
    "citation_count": 42
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cai19b_interspeech.html": {
    "title": "Polyphone Disambiguation for Mandarin Chinese Using Conditional Neural Network with Multi-Level Embedding Features",
    "volume": "main",
    "abstract": "This paper describes a conditional neural network architecture for Mandarin Chinese polyphone disambiguation. The system is composed of a bidirectional recurrent neural network component acting as a sentence encoder to accumulate the context correlations, followed by a prediction network that maps the polyphonic character embeddings along with the conditions to corresponding pronunciations. We obtain the word-level condition from a pre-trained word-to-vector lookup table. One goal of polyphone disambiguation is to address the homograph problem existing in the front-end processing of Mandarin Chinese text-to-speech system. Our system achieves an accuracy of 94.69% on a publicly available polyphonic character dataset. To further validate our choices on the conditional feature, we investigate polyphone disambiguation systems with multi-level conditions respectively. The experimental results show that both the sentence-level and the word-level conditional embedding features are able to attain good performance for Mandarin Chinese polyphone disambiguation",
    "checked": true,
    "id": "a7cc5347c4bb84a59b3c8df57657749682c7928d",
    "semantic_title": "polyphone disambiguation for mandarin chinese using conditional neural network with multi-level embedding features",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sun19c_interspeech.html": {
    "title": "Token-Level Ensemble Distillation for Grapheme-to-Phoneme Conversion",
    "volume": "main",
    "abstract": "Grapheme-to-phoneme (G2P) conversion is an important task in automatic speech recognition and text-to-speech systems. Recently, G2P conversion is viewed as a sequence to sequence task and modeled by RNN or CNN based encoder-decoder framework. However, previous works do not consider the practical issues when deploying G2P model in the production system, such as how to leverage additional unlabeled data to boost the accuracy, as well as reduce model size for online deployment. In this work, we propose token-level ensemble distillation for G2P conversion, which can (1) boost the accuracy by distilling the knowledge from additional unlabeled data, and (2) reduce the model size but maintain the high accuracy, both of which are very practical and helpful in the online production system. We use token-level knowledge distillation, which results in better accuracy than the sequence-level counterpart. What is more, we adopt the Transformer instead of RNN or CNN based models to further boost the accuracy of G2P conversion. Experiments on the publicly available CMUDict dataset and an internal English dataset demonstrate the effectiveness of our proposed method. Particularly, our method achieves 19.88% WER on CMUDict dataset, outperforming the previous works by more than 4.22% WER, and setting the new state-of-the-art results",
    "checked": true,
    "id": "04265022434f6c0ea2782952830ea344bff5705a",
    "semantic_title": "token-level ensemble distillation for grapheme-to-phoneme conversion",
    "citation_count": 48
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19i_interspeech.html": {
    "title": "Multilingual Speech Recognition with Corpus Relatedness Sampling",
    "volume": "main",
    "abstract": "Multilingual acoustic models have been successfully applied to low-resource speech recognition. Most existing works have combined many small corpora together, and pretrained a multilingual model by sampling from each corpus uniformly. The model is eventually fine-tuned on each target corpus. This approach, however, fails to exploit the relatedness and similarity among corpora in the training set. For example, the target corpus might benefit more from a corpus in the same domain or a corpus from a close language. In this work, we propose a simple but useful sampling strategy to take advantage of this relatedness. We first compute the corpus-level embeddings and estimate the similarity between each corpus. Next we start training the multilingual model with uniform-sampling from each corpus at first, then we gradually increase the probability to sample from related corpora based on its similarity with the target corpus. Finally the model would be fine-tuned automatically on the target corpus. Our sampling strategy outperforms the baseline multilingual model on 16 low-resource tasks. Additionally, we demonstrate that our corpus embeddings capture the language and domain information of each corpus",
    "checked": true,
    "id": "9e39c4bf71af7b551e3bf747f20e701b77a72f37",
    "semantic_title": "multilingual speech recognition with corpus relatedness sampling",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2019/arsikere19_interspeech.html": {
    "title": "Multi-Dialect Acoustic Modeling Using Phone Mapping and Online i-Vectors",
    "volume": "main",
    "abstract": "This paper proposes a simple phone mapping approach to multi-dialect acoustic modeling. In contrast to the widely used shared hidden layer (SHL) training approach (hidden layers are shared across dialects whereas output layers are kept separate), phone mapping simplifies model training and maintenance by allowing all the network parameters to be shared; it also simplifies online adaptation via HMM-based i-vectors by allowing the same T-matrix to be used for all the dialects. Using the LSTM-HMM framework, we compare phone mapping with transfer learning and SHL training, and we also compare the efficacy of online i-vectors with that of one-hot dialect encoding. Experiments with a 2K hour dataset comprising four English dialects show that (1) phone mapping yields significant WER reductions over dialect-specific training (14%, on average) and transfer learning (5%, on average); (2) SHL training is only slightly better than phone mapping; and (3) i-vectors provide useful additional reductions (3%, on average) while one-hot encoding has little effect. Even with a large 40K hour dataset (comprising the same four English dialects) and fully optimized sequence discriminative training, we show that phone mapping provides healthy WER reductions over dialect-specific models (10%, on average)",
    "checked": true,
    "id": "6d879c254c7725005ef2414589a9ad96c372ceaf",
    "semantic_title": "multi-dialect acoustic modeling using phone mapping and online i-vectors",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kannan19_interspeech.html": {
    "title": "Large-Scale Multilingual Speech Recognition with a Streaming End-to-End Model",
    "volume": "main",
    "abstract": "Multilingual end-to-end (E2E) models have shown great promise in expansion of automatic speech recognition (ASR) coverage of the world's languages. They have shown improvement over monolingual systems, and have simplified training and serving by eliminating language-specific acoustic, pronunciation, and language models. This work presents an E2E multilingual system which is equipped to operate in low-latency interactive applications, as well as handle a key challenge of real world data: the imbalance in training data across languages. Using nine Indic languages, we compare a variety of techniques, and find that a combination of conditioning on a language vector and training language-specific adapter layers produces the best model. The resulting E2E multilingual model achieves a lower word error rate (WER) than both monolingual E2E models (eight of nine languages) and monolingual conventional systems (all nine languages)",
    "checked": true,
    "id": "5cf3e46e6d427a87726c18f22def612519176938",
    "semantic_title": "large-scale multilingual speech recognition with a streaming end-to-end model",
    "citation_count": 135
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mendes19_interspeech.html": {
    "title": "Recognition of Latin American Spanish Using Multi-Task Learning",
    "volume": "main",
    "abstract": "In the broadcast news domain, national wide newscasters typically interact with communities with a diverse set of accents. One of the challenges in speech recognition is the performance degradation in the presence of these diverse conditions. Performance further aggravates when the accents are from other countries that share the same language. Extensive work has been conducted in this topic for languages such as English and Mandarin. Recently, TDNN based multi-task learning has received some attention in this area, with interesting results, typically using models trained with a variety of different accented corpora from a particular language. In this work, we look at the case of LATAM (Latin American) Spanish for its unique and distinctive accent variations. Because LATAM Spanish has historically been influenced by non-Spanish European migrations, we anticipated that LATAM based speech recognition performance can be further improved by including these influential languages, during a TDNN based multi-task training. Experiments show that including such languages in the training setup outperforms the single task acoustic model baseline. We also propose an automatic per-language weight selection strategy to regularize each language contribution during multi-task training",
    "checked": true,
    "id": "4cc4c860b11fefabee4d486fb0fa9d8870fbe248",
    "semantic_title": "recognition of latin american spanish using multi-task learning",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/viglino19_interspeech.html": {
    "title": "End-to-End Accented Speech Recognition",
    "volume": "main",
    "abstract": "Correct pronunciation is known to be the most difficult part to acquire for (native or non-native) language learners. The accented speech is thus more variable, and standard Automatic Speech Recognition (ASR) training approaches that rely on intermediate phone alignment might introduce errors during the ASR training. With end-to-end training we could alleviate this problem. In this work, we explore the use of multi-task training and accent embedding in the context of end-to-end ASR trained with the connectionist temporal classification loss. Comparing to the baseline developed using conventional ASR framework exploiting time-delay neural networks trained on accented English, we show significant relative improvement of about 25% in word error rate. Additional evaluation on unseen accent data yields relative improvements of of 31% and 2% for New Zealand English and Indian English, respectively",
    "checked": true,
    "id": "974c3a3ed4a18990fd7f77edea511b22de16811c",
    "semantic_title": "end-to-end accented speech recognition",
    "citation_count": 47
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19j_interspeech.html": {
    "title": "End-to-End Articulatory Attribute Modeling for Low-Resource Multilingual Speech Recognition",
    "volume": "main",
    "abstract": "The end-to-end (E2E) model allows for training of automatic speech recognition (ASR) systems without the hand-designed language-specific pronunciation lexicons. However, constructing the multilingual low-resource E2E ASR system is still challenging due to the vast number of symbols (e.g., words and characters). In this paper, we investigate an efficient method of encoding multilingual transcriptions for training E2E ASR systems. We directly encode the symbols of multilingual writing systems to universal articulatory representations, which is much more compact than characters and words. Compared with traditional multilingual modeling methods, we directly build a single acoustic-articulatory within recent transformer-based E2E framework for ASR tasks. The speech recognition results of our proposed method significantly outperform the conventional word-based and character-based E2E models",
    "checked": true,
    "id": "c4f2490ce85e1c6834e8c4a54227378822efbf66",
    "semantic_title": "end-to-end articulatory attribute modeling for low-resource multilingual speech recognition",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/taneja19_interspeech.html": {
    "title": "Exploiting Monolingual Speech Corpora for Code-Mixed Speech Recognition",
    "volume": "main",
    "abstract": "One of the main challenges in building code-mixed ASR systems is the lack of annotated speech data. Often, however, monolingual speech corpora are available in abundance for the languages in the code-mixed speech. In this paper, we explore different techniques that use monolingual speech to create synthetic code-mixed speech and examine their effect on training models for code-mixed ASR. We assume access to a small amount of real code-mixed text, from which we extract probability distributions that govern the transition of phones across languages at code-switch boundaries and the span lengths corresponding to a particular language. We extract segments from monolingual data and concatenate them to form code-mixed utterances such that these probability distributions are preserved. Using this synthetic speech, we show significant improvements in Hindi-English code-mixed ASR performance compared to using synthetic speech naively constructed from complete utterances in different languages. We also present language modelling experiments that use synthetically constructed code-mixed text and discuss their benefits",
    "checked": true,
    "id": "4642d0e08964976e2fd960c0360f06ff5dc8e3ca",
    "semantic_title": "exploiting monolingual speech corpora for code-mixed speech recognition",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hu19_interspeech.html": {
    "title": "Phoneme-Based Contextualization for Cross-Lingual Speech Recognition in End-to-End Models",
    "volume": "main",
    "abstract": "Contextual automatic speech recognition, i.e., biasing recognition towards a given context (e.g. user's playlists, or contacts), is challenging in end-to-end (E2E) models. Such models maintain a limited number of candidates during beam-search decoding, and have been found to recognize rare named entities poorly. The problem is exacerbated when biasing towards proper nouns in foreign languages, e.g., geographic location names, which are virtually unseen in training and are thus out-of-vocabulary (OOV). While grapheme or wordpiece E2E models might have a difficult time spelling OOV words, phonemes are more acoustically salient and past work has shown that E2E phoneme models can better predict such words. In this work, we propose an E2E model containing both English wordpieces and phonemes in the modeling space, and perform contextual biasing of foreign words at the phoneme level by mapping pronunciations of foreign words into similar English phonemes. In experimental evaluations, we find that the proposed approach performs 16% better than a grapheme-only biasing model, and 8% better than a wordpiece-only biasing model on a foreign place name recognition task, with only slight degradation on regular English tasks",
    "checked": true,
    "id": "e2f30027b2a2fa6ecc271934cc6de2e4beb10035",
    "semantic_title": "phoneme-based contextualization for cross-lingual speech recognition in end-to-end models",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/khassanov19_interspeech.html": {
    "title": "Constrained Output Embeddings for End-to-End Code-Switching Speech Recognition with Only Monolingual Data",
    "volume": "main",
    "abstract": "The lack of code-switch training data is one of the major concerns in the development of end-to-end code-switching automatic speech recognition (ASR) models. In this work, we propose a method to train an improved end-to-end code-switching ASR using only monolingual data. Our method encourages the distributions of output token embeddings of monolingual languages to be similar, and hence, promotes the ASR model to easily code-switch between languages. Specifically, we propose to use Jensen-Shannon divergence and cosine distance based constraints. The former will enforce output embeddings of monolingual languages to possess similar distributions, while the later simply brings the centroids of two distributions to be close to each other. Experimental results demonstrate high effectiveness of the proposed method, yielding up to 4.5% absolute mixed error rate improvement on Mandarin-English code-switching ASR task",
    "checked": true,
    "id": "298f209baa6c98afd72469f9823bf6a3f39f5f0c",
    "semantic_title": "constrained output embeddings for end-to-end code-switching speech recognition with only monolingual data",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zeng19_interspeech.html": {
    "title": "On the End-to-End Solution to Mandarin-English Code-Switching Speech Recognition",
    "volume": "main",
    "abstract": "Code-switching (CS) refers to a linguistic phenomenon where a speaker uses different languages in an utterance or between alternating utterances. In this work, we study end-to-end (E2E) approaches to the Mandarin-English code-switching speech recognition task. We first examine the effectiveness of using data augmentation and byte-pair encoding (BPE) subword units. More importantly, we propose a multitask learning recipe, where a language identification task is explicitly learned in addition to the E2E speech recognition task. Furthermore, we introduce an efficient word vocabulary expansion method for language modeling to alleviate data sparsity issues under the code-switching scenario. Experimental results on the SEAME data, a Mandarin-English code-switching corpus, demonstrate the effectiveness of the proposed methods",
    "checked": true,
    "id": "0306bd168cb36b22b76e1731702ed0e7e8c3f3d9",
    "semantic_title": "on the end-to-end solution to mandarin-english code-switching speech recognition",
    "citation_count": 72
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19f_interspeech.html": {
    "title": "Towards Language-Universal Mandarin-English Speech Recognition",
    "volume": "main",
    "abstract": "Multilingual and code-switching speech recognition are two challenging tasks that are studied separately in many previous works. In this work, we jointly study multilingual and code-switching problems, and present a language-universal bilingual system for Mandarin-English speech recognition. Specifically, we propose a novel bilingual acoustic model, which consists of two monolingual system initialized subnets and a shared output layer corresponding to the Character-Subword acoustic modeling units. The bilingual acoustic model is trained using a large Mandarin-English corpus with CTC and sMBR criteria. We find that this model, which is not given any information about language identity, can achieve comparable performance in monolingual Mandarin and English test sets compared to the well-trained language-specific Mandarin and English ASR systems, respectively. More importantly, the proposed bilingual model can automatically learn the language switching. Experimental results on a Mandarin-English code-switching test set show that it can achieve 11.8% and 17.9% relative error reduction on Mandarin and English parts, respectively",
    "checked": true,
    "id": "1786d90b757bba799f6ec29485a5d575d1fdef65",
    "semantic_title": "towards language-universal mandarin-english speech recognition",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/swarup19_interspeech.html": {
    "title": "Improving ASR Confidence Scores for Alexa Using Acoustic and Hypothesis Embeddings",
    "volume": "main",
    "abstract": "In automatic speech recognition, confidence measures provide a quantitative representation used to assess whether a generated hypothesis text is correct or not. For personal assistant devices like Alexa, automatic speech recognition (ASR) errors are inevitable due to the imperfection of today's speech recognition technology. Hence, confidence scores provide an important metric to gauge the correctness of ASR hypothesis text and enable downstream consumers to subsequently initiate appropriate actions. In this work, our aim is to improve the correctness of our confidence scores by enhancing our baseline model architecture with learned features, namely acoustic and 1-best hypothesis embeddings. These embeddings are obtained by training separate networks on acoustic features and ASR 1-best hypothesis respectively. We present an experimental evaluation on a large US English data set showing a 6% relative equal error rate reduction and 13% relative normalized cross-entropy improvement over our baseline system by incorporating these embeddings. We also present a deeper analysis of the embeddings revealing that the acoustic embedding results in a better prediction of insertion errors whereas the 1-best hypothesis embedding helps to better predict substitution errors",
    "checked": true,
    "id": "0a0bcac192dd64ed79a95ea7792480b26fc6d604",
    "semantic_title": "improving asr confidence scores for alexa using acoustic and hypothesis embeddings",
    "citation_count": 32
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19g_interspeech.html": {
    "title": "Investigation of Transformer Based Spelling Correction Model for CTC-Based End-to-End Mandarin Speech Recognition",
    "volume": "main",
    "abstract": "Connectionist Temporal Classification (CTC) based end-to-end speech recognition system usually need to incorporate an external language model by using WFST-based decoding in order to achieve promising results. This is more essential to Mandarin speech recognition since it owns a special phenomenon, namely homophone, which causes a lot of substitution errors. The linguistic information introduced by language model is somehow helpful to distinguish these substitution errors. In this work, we propose a transformer based spelling correction model to automatically correct errors, especially the substitution errors, made by CTC-based Mandarin speech recognition system. Specifically, we investigate to use the recognition results generated by CTC-based systems as input and the ground-truth transcriptions as output to train a transformer with encoder-decoder architecture, which is much similar to machine translation. Experimental results in a 20,000 hours Mandarin speech recognition task show that the proposed spelling correction model can achieve a CER of 3.41%, which results in 22.9% and 53.2% relative improvement compared to the baseline CTC-based systems decoded with and without language model, respectively",
    "checked": true,
    "id": "fda800775bc3653b9ea83f66cf4223cb323e2303",
    "semantic_title": "investigation of transformer based spelling correction model for ctc-based end-to-end mandarin speech recognition",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2019/peyser19_interspeech.html": {
    "title": "Improving Performance of End-to-End ASR on Numeric Sequences",
    "volume": "main",
    "abstract": "Recognizing written domain numeric utterances (e.g., I need 1.25.) can be challenging for ASR systems, particularly when numeric sequences are not seen during training. This out-of-vocabulary (OOV) issue is addressed in conventional ASR systems by training part of the model on spoken domain utterances (e.g., I need one dollar and twenty five cents.), for which numeric sequences are composed of in-vocabulary numbers, and then using an FST verbalizer to denormalize the result. Unfortunately, conventional ASR models are not suitable for the low memory setting of on-device speech recognition. E2E models such as RNN-T are attractive for on-device ASR, as they fold the AM, PM and LM of a conventional model into one neural network. However, in the on-device setting the large memory footprint of an FST denormer makes spoken domain training more difficult. In this paper, we investigate techniques to improve E2E model performance on numeric data. We find that using a text-to-speech system to generate additional numeric training data, as well as using a small-footprint neural network to perform spoken-to-written domain denorming, yields improvement in several numeric classes. In the case of the longest numeric sequences, we see reduction of WER by up to a factor of 8",
    "checked": true,
    "id": "a2fef91c2887bcc0520998c71aa3b50e3f7446e8",
    "semantic_title": "improving performance of end-to-end asr on numeric sequences",
    "citation_count": 29
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bai19_interspeech.html": {
    "title": "A Time Delay Neural Network with Shared Weight Self-Attention for Small-Footprint Keyword Spotting",
    "volume": "main",
    "abstract": "Keyword spotting requires a small memory footprint to run on mobile devices. However, previous works still use several hundred thousand parameters to achieve good performance. To address this issue, we propose a time delay neural network with shared weight self-attention for small-footprint keyword spotting. By sharing weights, the parameters of self-attention are reduced but without performance reduction. The publicly available Google Speech Commands dataset is used to evaluate the models. The number of parameters (12K) of our model is 1/20 of state-of-the-art ResNet model (239K). The proposed model achieves an error rate of 4.19% , which is comparable to the ResNet model",
    "checked": true,
    "id": "a76a76cb959c2ca53b8d62bd2baa7db3d50659e6",
    "semantic_title": "a time delay neural network with shared weight self-attention for small-footprint keyword spotting",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kao19_interspeech.html": {
    "title": "Sub-Band Convolutional Neural Networks for Small-Footprint Spoken Term Classification",
    "volume": "main",
    "abstract": "This paper proposes a Sub-band Convolutional Neural Network for spoken term classification. Convolutional neural networks (CNNs) have proven to be very effective in acoustic applications such as spoken term classification, keyword spotting, speaker identification, acoustic event detection, etc. Unlike applications in computer vision, the spatial invariance property of 2D convolutional kernels does not fit acoustic applications well since the meaning of a specific 2D kernel varies a lot along the feature axis in an input feature map. We propose a sub-band CNN architecture to apply different convolutional kernels on each feature sub-band, which makes the overall computation more efficient. Experimental results show that the computational efficiency brought by sub-band CNN is more beneficial for small-footprint models. Compared to a baseline full band CNN for spoken term classification on a publicly available Speech Commands dataset, the proposed sub-band CNN architecture reduces the computation by 39.7% on commands classification, and 49.3% on digits classification with accuracy maintained",
    "checked": true,
    "id": "1f45395944568f7062f8faa8a801a2995e70897e",
    "semantic_title": "sub-band convolutional neural networks for small-footprint spoken term classification",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19k_interspeech.html": {
    "title": "Investigating Radical-Based End-to-End Speech Recognition Systems for Chinese Dialects and Japanese",
    "volume": "main",
    "abstract": "Training automatic speech recognition (ASR) systems for East Asian languages (e.g., Chinese and Japanese) is tough work because of the characters existing in the writing systems of these languages. Traditionally, we first need to get the pronunciation of these characters by morphological analysis. The end-to-end (E2E) model allows for directly using characters or words as the modeling unit. However, since different groups of people (e.g., residents in Chinese mainland, Hong Kong, Taiwan, and Japan) adopts different writing forms for a character, this also leads to a large increase in the number of vocabulary, especially when building ASR systems across languages or dialects. In this paper, we propose a new E2E ASR modeling method by decomposing the characters into a set of radicals. Our experiments demonstrate that it is possible to effectively reduce the vocabulary size by sharing the basic radicals across different dialect of Chinese. Moreover, we also demonstrate this method could also be used to construct a Japanese E2E ASR system. The system modeled with radicals and kana achieved similar performance compared to state-of-the-art E2E system built with word-piece units",
    "checked": true,
    "id": "f069944718f3e29ca4ffdd20ba486828459a6e8b",
    "semantic_title": "investigating radical-based end-to-end speech recognition systems for chinese dialects and japanese",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/guo19d_interspeech.html": {
    "title": "Joint Decoding of CTC Based Systems for Speech Recognition",
    "volume": "main",
    "abstract": "Connectionist temporal classification (CTC) has been successfully used in speech recognition. It learns the alignments between speech frames and label sequences automatically without explicit pre-generated frame-level labels. While this property is convenient for shortening the training pipeline, it may become a potential disadvantage for the frame-level system combination due to inaccurate alignments. In this paper, a novel Dynamic Time Warping (DTW) based position calibration algorithm is proposed for joint decoding on two CTC based acoustic models. Furthermore, joint decoding for CTC and conventional hybrid NN-HMM models is also explored. Experiments on a large vocabulary Mandarin speech recognition task show that the proposed joint decoding of both CTC based and CTC-Hybrid based systems can achieve a significant and consistent character error rate reduction",
    "checked": true,
    "id": "cd15d9939b0c308218a9ce8a6e5037ff0016ac02",
    "semantic_title": "joint decoding of ctc based systems for speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tanaka19_interspeech.html": {
    "title": "A Joint End-to-End and DNN-HMM Hybrid Automatic Speech Recognition System with Transferring Sharable Knowledge",
    "volume": "main",
    "abstract": "This paper presents joint end-to-end and deep neural network-hidden Markov model (DNN-HMM) hybrid automatic speech recognition (ASR) systems that share network components. End-to-end ASR systems have been shown competitive performance compared with the DNN-HMM hybrid ASR systems in recent studies. These systems have different advantages, which are an estimation ability based on the totally optimized model of the end-to-end ASR system and a stable processing based on a frame-by-frame manner of the DNN-HMM hybrid ASR system. In our previous study, we proposed a method to utilize an end-to-end ASR system for rescoring hypotheses generated from a DNN-HMM hybrid ASR system. However, the conventional method cannot efficiently leverage the advantages since network components are independently modeled. In order to tackle this problem, we propose a joint end-to-end and DNN-HMM hybrid ASR systems that share the network to transfer knowledge of the systems. In the proposed method, end-to-end ASR systems utilize the information from an output of an internal layer in a DNN acoustic model in the DNN-HMM hybrid ASR system for enhancing the end-to-end ASR system. This enables us to efficiently leverage sharable information for improving the joint ASR system. Experimental results show that the proposed method outperforms the conventional method",
    "checked": true,
    "id": "98acb379edbf8e7c91223509e8e6f046566ff871",
    "semantic_title": "a joint end-to-end and dnn-hmm hybrid automatic speech recognition system with transferring sharable knowledge",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/malhotra19_interspeech.html": {
    "title": "Active Learning Methods for Low Resource End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "Recently developed end-to-end (E2E) automatic speech recognition (ASR) systems demand abundance of transcribed speech data, there are several scenarios where the labeling of speech data is cumbersome and expensive. For a fixed annotation cost, active learning for speech recognition allows to efficiently train the ASR model. In this work, we advance the most common approach for active learning methods which relies on uncertainty sampling technique. In particular, we explore the use of path probability of the decoded sequence as a confidence measure and select the samples with the least confidence for active learning. In order to reduce the sampling bias in active learning, we propose a regularized uncertainty sampling approach that incorporates an i-vector diversity measure. Thus, the active learning in the proposed framework uses a joint score of uncertainty and i-vector diversity. The benefits of the proposed approach are illustrated for an E2E ASR task performed on CSJ and Librispeech datasets. In these experiments, we show that the proposed approach yields considerable improvements over the baseline model using random sampling",
    "checked": true,
    "id": "0c8c118694b8f4dfcd57d4990a6c80ad8d71238a",
    "semantic_title": "active learning methods for low resource end-to-end speech recognition",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/karafiat19_interspeech.html": {
    "title": "Analysis of Multilingual Sequence-to-Sequence Speech Recognition Systems",
    "volume": "main",
    "abstract": "This paper investigates the applications of various multilingual approaches developed in conventional deep neural network - hidden Markov model (DNN-HMM) systems to sequence-to-sequence (seq2seq) automatic speech recognition (ASR). We employ a joint connectionist temporal classification-attention network as our base model. Our main contribution is separated into two parts. First, we investigate the effectiveness of the seq2seq model with stacked multilingual bottle-neck features obtained from a conventional DNN-HMM system on the Babel multilingual speech corpus. Second, we investigate the effectiveness of transfer learning from a pre-trained multilingual seq2seq model with and without the target language included in the original multilingual training data. In this experiment, we also explore various architectures and training strategies of the multilingual seq2seq model by making use of knowledge obtained in the DNN-HMM based transfer-learning. Although both approaches significantly improved the performance from a monolingual seq2seq baseline, interestingly, we found the multilingual bottle-neck features to be superior to multilingual models with transfer learning. This finding suggests that we can efficiently combine the benefits of the DNN-HMM system with the seq2seq system through multilingual bottle-neck feature techniques",
    "checked": true,
    "id": "aa0b93501f79d57fe8542e72ccc8843ea50443c9",
    "semantic_title": "analysis of multilingual sequence-to-sequence speech recognition systems",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zapotoczny19_interspeech.html": {
    "title": "Lattice Generation in Attention-Based Speech Recognition Models",
    "volume": "main",
    "abstract": "Attention-based neural speech recognition models are frequently decoded with beam search, which produces a tree of hypotheses. In many cases, such as when using external language models, numerous decoding hypotheses need to be considered, requiring large beam sizes during decoding. We demonstrate that it is possible to merge certain nodes in a tree of hypotheses, in order to obtain a decoding lattice, which increases the number of decoding hypotheses without increasing the number of candidates that are scored by the neural network. We propose a convolutional architecture, which facilitates comparing states of the model at different pi The experiments are carried on the Wall Street Journal dataset, where the lattice decoder obtains lower word error rates with smaller beam sizes, than an otherwise similar architecture with regular beam search",
    "checked": true,
    "id": "1ded3d23b5b5a9264fe5e67d07ce217f0adf9765",
    "semantic_title": "lattice generation in attention-based speech recognition models",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jansche19_interspeech.html": {
    "title": "Sampling from Stochastic Finite Automata with Applications to CTC Decoding",
    "volume": "main",
    "abstract": "Stochastic finite automata arise naturally in many language and speech processing tasks. They include stochastic acceptors, which represent certain probability distributions over random strings. We consider the problem of efficient sampling: drawing random string variates from the probability distribution represented by stochastic automata and transformations of those. We show that path-sampling is effective and can be efficient if the epsilon-graph of a finite automaton is acyclic. We provide an algorithm that ensures this by conflating epsilon-cycles within strongly connected components. Sampling is also effective in the presence of non-injective transformations of strings. We illustrate this in the context of decoding for Connectionist Temporal Classification (CTC), where the predictive probabilities yield auxiliary sequences which are transformed into shorter labeling strings. We can sample efficiently from the transformed labeling distribution and use this in two different strategies for finding the most probable CTC labeling",
    "checked": true,
    "id": "cffaf577458b3b5730bcaf35a5a00a1b1e762f2d",
    "semantic_title": "sampling from stochastic finite automata with applications to ctc decoding",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dudziak19_interspeech.html": {
    "title": "ShrinkML: End-to-End ASR Model Compression Using Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gaur19_interspeech.html": {
    "title": "Acoustic-to-Phrase Models for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19l_interspeech.html": {
    "title": "Performance Monitoring for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cohn19b_interspeech.html": {
    "title": "The Role of Musical Experience in the Perceptual Weighting of Acoustic Cues for the Obstruent Coda Voicing Contrast in American English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lewandowski19_interspeech.html": {
    "title": "Individual Differences in Implicit Attention to Phonetic Detail in Speech Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lalonde19_interspeech.html": {
    "title": "Effects of Natural Variability in Cross-Modal Temporal Correlations on Audiovisual Speech Recognition Benefit",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bentum19_interspeech.html": {
    "title": "Listening with Great Expectations: An Investigation of Word Form Anticipations in Naturalistic Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bentum19b_interspeech.html": {
    "title": "Quantifying Expectation Modulation in Human Speech Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/turner19_interspeech.html": {
    "title": "Perception of Pitch Contours in Speech and Nonspeech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bosch19b_interspeech.html": {
    "title": "Analyzing Reaction Time and Error Sequences in Lexical Decision Experiments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19e_interspeech.html": {
    "title": "Automatic Detection of the Temporal Segmentation of Hand Movements in British English Cued Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yokoe19_interspeech.html": {
    "title": "Place Shift as an Autonomous Process: Evidence from Japanese Listeners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/meyer19_interspeech.html": {
    "title": "A Perceptual Study of CV Syllables in Both Spoken and Whistled Speech: A Tashlhiyt Berber Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hsieh19_interspeech.html": {
    "title": "Consonant Classification in Mandarin Based on the Depth Image Feature: A Pilot Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/levari19_interspeech.html": {
    "title": "The Different Roles of Expectations in Phonetic and Lexical Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/segedin19_interspeech.html": {
    "title": "Perceptual Adaptation to Device and Human Voices: Learning and Generalization of a Phonetic Shift Across Real and Voice-AI Talkers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/papadimitriou19_interspeech.html": {
    "title": "End-to-End Convolutional Sequence Learning for ASL Fingerspelling Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/somandepalli19_interspeech.html": {
    "title": "Multiview Shared Subspace Learning Across Speakers and Speech Commands",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/belitz19_interspeech.html": {
    "title": "A Machine Learning Based Clustering Protocol for Determining Hearing Aid Initial Configurations from Pure-Tone Audiograms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nguyen19_interspeech.html": {
    "title": "Acoustic Scene Classification with Mismatched Devices Using CliqueNets and Mixup Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ahmed19_interspeech.html": {
    "title": "DeepLung: Smartphone Convolutional Neural Network-Based Inference of Lung Anomalies for Pulmonary Patients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/moore19_interspeech.html": {
    "title": "On the Use/Misuse of the Term ‘Phoneme",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/muckenhirn19_interspeech.html": {
    "title": "Understanding and Visualizing Raw Waveform-Based CNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kilgour19_interspeech.html": {
    "title": "Fréchet Audio Distance: A Reference-Free Metric for Evaluating Music Enhancement Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gong19_interspeech.html": {
    "title": "ReMASC: Realistic Replay Attack Corpus for Voice Controlled Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bt19_interspeech.html": {
    "title": "Analyzing Intra-Speaker and Inter-Speaker Vocal Tract Impedance Characteristics in a Low-Dimensional Feature Space Using t-SNE",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lee19b_interspeech.html": {
    "title": "Directional Audio Rendering Using a Neural Network Based Personalized HRTF",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pienaar19_interspeech.html": {
    "title": "Online Speech Processing and Analysis Suite",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/maurer19_interspeech.html": {
    "title": "Formant Pattern and Spectral Shape Ambiguity of Vowel Sounds, and Related Phenomena of Vowel Acoustics — Exemplary Evidence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/noll19_interspeech.html": {
    "title": "Sound Tools eXtended (STx) 5.0 — A Powerful Sound Analysis Tool Optimized for Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/eldesouki19_interspeech.html": {
    "title": "FarSpeech: Arabic Natural Language Processing for Live Arabic Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/haider19_interspeech.html": {
    "title": "A System for Real-Time Privacy Preserving Data Collection for Ambient Assisted Living",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gupta19c_interspeech.html": {
    "title": "NUS Speak-to-Sing: A Web Platform for Personalized Speech-to-Singing Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kaltenbacher19_interspeech.html": {
    "title": "Physiology and Physics of Voice Production",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schuller19_interspeech.html": {
    "title": "The INTERSPEECH 2019 Computational Paralinguistics Challenge: Styrian Dialects, Continuous Sleepiness, Baby Sounds & Orca Activity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dubagunta19_interspeech.html": {
    "title": "Using Speech Production Knowledge for Raw Waveform Modelling Based Styrian Dialect Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/elsner19_interspeech.html": {
    "title": "Deep Neural Baselines for Computational Paralinguistics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kisler19_interspeech.html": {
    "title": "Styrian Dialect Classification: Comparing and Fusing Classifiers Based on a Feature Selection Using a Genetic Algorithm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yeh19_interspeech.html": {
    "title": "Using Attention Networks and Adversarial Augmentation for Styrian Dialect Continuous Sleepiness and Baby Sound Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wu19f_interspeech.html": {
    "title": "Ordinal Triplet Loss: Investigating Sleepiness Detection from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ravi19_interspeech.html": {
    "title": "Voice Quality and Between-Frame Entropy for Sleepiness Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gosztolya19b_interspeech.html": {
    "title": "Using Fisher Vector and Bag-of-Audio-Words Representations to Identify Styrian Dialects, Sleepiness, Baby & Orca Sounds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/das19b_interspeech.html": {
    "title": "Instantaneous Phase and Long-Term Acoustic Cues for Orca Activity Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schiller19_interspeech.html": {
    "title": "Relevance-Based Feature Masking: Improving Neural Network Based Whale Classification Through Explainable Artificial Intelligence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/caraty19_interspeech.html": {
    "title": "Spatial, Temporal and Spectral Multiresolution Analysis for the INTERSPEECH 2019 ComParE Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wu19g_interspeech.html": {
    "title": "The DKU-LENOVO Systems for the INTERSPEECH 2019 Computational Paralinguistic Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nandwana19_interspeech.html": {
    "title": "The VOiCES from a Distance Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/novoselov19b_interspeech.html": {
    "title": "STC Speaker Recognition Systems for the VOiCES from a Distance Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/matejka19_interspeech.html": {
    "title": "Analysis of BUT Submission in Far-Field Scenarios of VOiCES 2019 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/medennikov19_interspeech.html": {
    "title": "The STC ASR System for the VOiCES from a Distance Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chong19_interspeech.html": {
    "title": "The I2R's ASR System for the VOiCES from a Distance Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nandwana19b_interspeech.html": {
    "title": "The VOiCES from a Distance Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/novoselov19c_interspeech.html": {
    "title": "STC Speaker Recognition Systems for the VOiCES from a Distance Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/matejka19b_interspeech.html": {
    "title": "Analysis of BUT Submission in Far-Field Scenarios of VOiCES 2019 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/medennikov19b_interspeech.html": {
    "title": "The STC ASR System for the VOiCES from a Distance Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chong19b_interspeech.html": {
    "title": "The I2R's ASR System for the VOiCES from a Distance Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jati19_interspeech.html": {
    "title": "Multi-Task Discriminative Training of Hybrid DNN-TVM Model for Speaker Verification with Noisy and Far-Field Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/snyder19b_interspeech.html": {
    "title": "The JHU Speaker Recognition System for the VOiCES 2019 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19h_interspeech.html": {
    "title": "Intel Far-Field Speaker Recognition System for VOiCES Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sun19d_interspeech.html": {
    "title": "The I2R's Submission to VOiCES Distance Speaker Recognition Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liang19_interspeech.html": {
    "title": "The LeVoice Far-Field Speech Recognition System for VOiCES from a Distance Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19f_interspeech.html": {
    "title": "The JHU ASR System for VOiCES from a Distance Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cai19c_interspeech.html": {
    "title": "The DKU System for the Speaker Recognition Task of the 2019 VOiCES from a Distance Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hauptman19_interspeech.html": {
    "title": "Identifying Distinctive Acoustic and Spectral Features in Parkinson's Disease",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/drioli19_interspeech.html": {
    "title": "Aerodynamics and Lumped-Masses Combined with Delay Lines for Modeling Vertical and Anterior-Posterior Phase Differences in Pathological Vocal Fold Vibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kadiri19_interspeech.html": {
    "title": "Mel-Frequency Cepstral Coefficients of Voice Source Waveforms for Classification of Phonation Types in Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cho19_interspeech.html": {
    "title": "Automatic Detection of Autism Spectrum Disorder in Children Using Acoustic and Text Features from Brief Natural Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schoentgen19_interspeech.html": {
    "title": "Analysis and Synthesis of Vocal Flutter and Vocal Jitter",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schaeffler19_interspeech.html": {
    "title": "Reliability of Clinical Voice Parameters Captured with Smartphones — Measurements of Added Noise and Spectral Tilt",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/moore19b_interspeech.html": {
    "title": "Say What? A Dataset for Exploring the Error Patterns That Two ASR Engines Make",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ward19_interspeech.html": {
    "title": "Survey Talk: Prosody Research and Applications: The State of the Art",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/roessig19_interspeech.html": {
    "title": "Dimensions of Prosodic Prominence in an Attractor Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/suni19_interspeech.html": {
    "title": "Comparative Analysis of Prosodic Characteristics Using WaveNet Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/murphy19_interspeech.html": {
    "title": "The Role of Voice Quality in the Perception of Prominence in Synthetic Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/albar19_interspeech.html": {
    "title": "Phonological Awareness of French Rising Contours in Japanese Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/okawa19_interspeech.html": {
    "title": "Audio Classification of Bit-Representation Waveform",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mulimani19_interspeech.html": {
    "title": "Locality-Constrained Linear Coding Based Fused Visual Features for Robust Acoustic Event Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shen19b_interspeech.html": {
    "title": "Learning How to Listen: A Temporal-Frequential Attention Model for Sound Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ford19_interspeech.html": {
    "title": "A Deep Residual Network for Large-Scale Acoustic Scene Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/reddy19b_interspeech.html": {
    "title": "Supervised Classifiers for Audio Impairments with Noisy Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tarantino19_interspeech.html": {
    "title": "Self-Attention for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nachmani19_interspeech.html": {
    "title": "Unsupervised Singing Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lee19c_interspeech.html": {
    "title": "Adversarially Trained End-to-End Korean Singing Voice Synthesis System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yi19_interspeech.html": {
    "title": "Singing Voice Synthesis Using Deep Autoregressive Neural Networks for Acoustic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dahmani19_interspeech.html": {
    "title": "Conditional Variational Auto-Encoder for Text-Driven Expressive AudioVisual Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ayllon19b_interspeech.html": {
    "title": "A Strategy for Improved Phone-Level Lyrics-to-Audio Alignment for Speech-to-Singing Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/biasuttolervat19_interspeech.html": {
    "title": "Modeling Labial Coarticulation with Bidirectional Gated Recurrent Networks and Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/park19e_interspeech.html": {
    "title": "SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/audhkhasi19_interspeech.html": {
    "title": "Forget a Bit to Learn Better: Soft Forgetting for CTC-Based Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/miao19_interspeech.html": {
    "title": "Online Hybrid CTC/Attention Architecture for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19h_interspeech.html": {
    "title": "A Highly Efficient Distributed Deep Learning System for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19i_interspeech.html": {
    "title": "Knowledge Distillation for End-to-End Monaural Multi-Talker ASR System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/menne19_interspeech.html": {
    "title": "Analysis of Deep Clustering as Preprocessing for Automatic Speech Recognition of Sparsely Overlapping Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bradlow19_interspeech.html": {
    "title": "Survey Talk: Recognition of Foreign-Accented Speech: Challenges and Opportunities for Human and Computer Speech Communication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/novak19_interspeech.html": {
    "title": "The Effects of Time Expansion on English as a Second Language Individuals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shi19_interspeech.html": {
    "title": "Capturing L1 Influence on L2 Pronunciation by Simulating Perceptual Space Using Acoustic Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19h_interspeech.html": {
    "title": "Cognitive Factors in Thai-Naïve Mandarin Speakers' Imitation of Thai Lexical Tones",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tremblay19_interspeech.html": {
    "title": "Foreign-Language Knowledge Enhances Artificial-Language Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/abujabal19_interspeech.html": {
    "title": "Neural Named Entity Recognition from Subword Units",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bhati19_interspeech.html": {
    "title": "Unsupervised Acoustic Segmentation and Clustering Using Siamese Network Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yusuf19b_interspeech.html": {
    "title": "An Empirical Evaluation of DTW Subsampling Methods for Keyword Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yang19e_interspeech.html": {
    "title": "Linguistically-Informed Training of Acoustic Word Embeddings for Low-Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19g_interspeech.html": {
    "title": "Multimodal Word Discovery and Retrieval with Phone Sequence and Image Concepts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/boito19_interspeech.html": {
    "title": "Empirical Evaluation of Sequence-to-Sequence Models for Word Discovery in Low-Resource Settings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xue19b_interspeech.html": {
    "title": "Direct-Path Signal Cross-Correlation Estimation for Sound Source Localization in Reverberation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/grondin19_interspeech.html": {
    "title": "Multiple Sound Source Localization with SVD-PHAT",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19j_interspeech.html": {
    "title": "Robust DOA Estimation Based on Convolutional Neural Network and Time-Frequency Masking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/masuyama19_interspeech.html": {
    "title": "Multichannel Loss Function for Supervised Speech Source Separation by Mask-Based Beamforming",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19m_interspeech.html": {
    "title": "Direction-Aware Speaker Beam for Multi-Channel Speaker Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ochiai19_interspeech.html": {
    "title": "Multimodal SpeakerBeam: Single Channel Target Speech Extraction with Audio-Visual Speaker Clues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/germain19_interspeech.html": {
    "title": "Speech Denoising with Deep Feature Losses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19h_interspeech.html": {
    "title": "VoiceFilter: Targeted Voice Separation by Speaker-Conditioned Spectrogram Masking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liao19_interspeech.html": {
    "title": "Incorporating Symbolic Sequential Modeling for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mowlaee19_interspeech.html": {
    "title": "Maximum a posteriori Speech Enhancement Based on Double Spectrum",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yao19_interspeech.html": {
    "title": "Coarse-to-Fine Optimization for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hui19b_interspeech.html": {
    "title": "Kernel Machines Beat Deep Neural Networks on Mask-Based Single-Channel Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/metze19_interspeech.html": {
    "title": "Survey Talk: Multimodal Processing of Speech and Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shrivastava19_interspeech.html": {
    "title": "MobiVSR : Efficient and Light-Weight Neural Network for Visual Speech Recognition on Mobile Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kandala19_interspeech.html": {
    "title": "Speaker Adaptation for Lip-Reading Using Visual Identity Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/koumparoulis19_interspeech.html": {
    "title": "MobiLipNet: Resource-Efficient Deep Learning Based Lipreading",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/qu19_interspeech.html": {
    "title": "LipSound: Neural Mel-Spectrogram Reconstruction for Lip Reading",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sainath19_interspeech.html": {
    "title": "Two-Pass End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lam19_interspeech.html": {
    "title": "Extract, Adapt and Recognize: An End-to-End Neural Network for Corrupted Monaural Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gowda19_interspeech.html": {
    "title": "Multi-Task Multi-Resolution Char-to-BPE Cross-Attention Decoder for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/han19b_interspeech.html": {
    "title": "Multi-Stride Self-Attention for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hu19b_interspeech.html": {
    "title": "LF-MMI Training of Bayesian and Gaussian Process Time Delay Neural Networks for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lu19c_interspeech.html": {
    "title": "Self-Teaching Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19n_interspeech.html": {
    "title": "Improved End-to-End Speech Emotion Recognition Using Self Attention Mechanism and Multitask Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schmitt19_interspeech.html": {
    "title": "Continuous Emotion Recognition in Speech — Do We Need Recurrence?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ouyang19_interspeech.html": {
    "title": "Speech Based Emotion Prediction: Can a Linear Model Work?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ando19_interspeech.html": {
    "title": "Speech Emotion Recognition Based on Multi-Label Emotion Existence Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gorrostieta19_interspeech.html": {
    "title": "Gender De-Biasing in Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bao19_interspeech.html": {
    "title": "CycleGAN-Based Emotion Style Transfer as Data Augmentation for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bollepalli19_interspeech.html": {
    "title": "Lombard Speech Synthesis Using Transfer Learning in a Tacotron Text-to-Speech System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/seshadri19_interspeech.html": {
    "title": "Augmented CycleGANs for Continuous Scale Normal-to-Lombard Speaking Style Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhao19f_interspeech.html": {
    "title": "Foreign Accent Conversion by Synthesizing Speech from Phonetic Posteriorgrams",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shankar19b_interspeech.html": {
    "title": "A Multi-Speaker Emotion Morphing Model Using Highway Networks and Maximum Likelihood Objective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lapidot19_interspeech.html": {
    "title": "Effects of Waveform PMF on Anti-Spoofing Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gao19b_interspeech.html": {
    "title": "Nonparallel Emotional Speech Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/stafylakis19_interspeech.html": {
    "title": "Self-Supervised Speaker Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nautsch19b_interspeech.html": {
    "title": "Privacy-Preserving Speaker Recognition with Cohort Score Normalisation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19f_interspeech.html": {
    "title": "Large Margin Softmax Loss for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hajavi19_interspeech.html": {
    "title": "A Deep Neural Network for Short-Segment Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhou19_interspeech.html": {
    "title": "Deep Speaker Embedding Extraction with Channel-Wise Feature Responses and Additive Supervision Softmax Loss Function",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shon19b_interspeech.html": {
    "title": "VoiceID Loss: Speech Enhancement for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/avila19_interspeech.html": {
    "title": "Blind Channel Response Estimation for Replay Attack Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/patil19_interspeech.html": {
    "title": "Energy Separation-Based Instantaneous Frequency Estimation for Cochlear Cepstral Feature for Replay Spoof Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mingote19_interspeech.html": {
    "title": "Optimization of False Acceptance/Rejection Rates and Decision Threshold for End-to-End Text-Dependent Speaker Verification Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fan19_interspeech.html": {
    "title": "Deep Hashing for Speaker Identification and Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/marras19_interspeech.html": {
    "title": "Adversarial Optimization for Dictionary Attacks on Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gunendradasan19_interspeech.html": {
    "title": "An Adaptive-Q Cochlear Model for Replay Spoofing Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yun19_interspeech.html": {
    "title": "An End-to-End Text-Independent Speaker Verification Framework with a Keyword Adversarial Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/seo19_interspeech.html": {
    "title": "Shortcut Connections Based Deep Speaker Embeddings for End-to-End Speaker Verification System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/you19c_interspeech.html": {
    "title": "Device Feature Extractor for Replay Spoofing Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19i_interspeech.html": {
    "title": "Cross-Domain Replay Spoofing Attack Detection Using Domain Adversarial Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kanagasundaram19_interspeech.html": {
    "title": "A Study of x-Vector Based Speaker Recognition on Short Utterances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19i_interspeech.html": {
    "title": "Tied Mixture of Factor Analyzers Layer to Combine Frame Level Representations in Neural Speaker Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wickramasinghe19_interspeech.html": {
    "title": "Biologically Inspired Adaptive-Q Filterbanks for Replay Spoofing Attack Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bousquet19_interspeech.html": {
    "title": "On Robustness of Unsupervised Domain Adaptation for Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shon19c_interspeech.html": {
    "title": "Large-Scale Speaker Retrieval on Random Speaker Variability Subspace",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yoshioka19_interspeech.html": {
    "title": "Meeting Transcription Using Asynchronous Distant Microphones",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/thomas19_interspeech.html": {
    "title": "Detection and Recovery of OOVs for Improved English Broadcast News Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/farooq19_interspeech.html": {
    "title": "Improving Large Vocabulary Urdu Speech Recognition System Using Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tang19b_interspeech.html": {
    "title": "Hybrid Arbitration Using Raw ASR String and NLU Information — Taking the Best of Both Embedded World and Cloud World",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/szaszak19_interspeech.html": {
    "title": "Leveraging a Character, Word and Prosody Triplet for an ASR Error Robust and Agglutination Friendly Punctuation Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pellegrini19_interspeech.html": {
    "title": "The Airbus Air Traffic Control Speech Recognition 2018 Challenge: Towards ATC Automatic Transcription and Call Sign Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/oneata19_interspeech.html": {
    "title": "Kite: Automatic Speech Recognition for Unmanned Aerial Vehicles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19j_interspeech.html": {
    "title": "Exploring Methods for the Automatic Detection of Errors in Manual Transcription",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/biswas19_interspeech.html": {
    "title": "Improved Low-Resource Somali Speech Recognition by Semi-Supervised Acoustic and Language Model Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/helgadottir19_interspeech.html": {
    "title": "The Althingi ASR System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gupta19d_interspeech.html": {
    "title": "CRIM's Speech Transcription and Call Sign Detection System for the ATC Airbus Challenge Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rutowski19_interspeech.html": {
    "title": "Optimizing Speech-Input Length for Speaker-Independent Depression Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pietrowicz19_interspeech.html": {
    "title": "A New Approach for Automating Analysis of Responses on Verbal Fluency Tests from Subjects At-Risk for Schizophrenia",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jeancolas19_interspeech.html": {
    "title": "Comparison of Telephone Recordings and Professional Microphone Recordings for Early Detection of Parkinson's Disease, Using Mel-Frequency Cepstral Coefficients with Gaussian Mixture Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/janbakhshi19_interspeech.html": {
    "title": "Spectral Subspace Analysis for Automatic Assessment of Pathological Speech Intelligibility",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pasquale19_interspeech.html": {
    "title": "An Investigation of Therapeutic Rapport Through Prosody in Brief Psychodynamic Psychotherapy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rueda19_interspeech.html": {
    "title": "Feature Representation of Pathophysiology of Parkinsonian Dysarthria",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/onu19_interspeech.html": {
    "title": "Neural Transfer Learning for Cry-Based Diagnosis of Perinatal Asphyxia",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hong19_interspeech.html": {
    "title": "Investigating the Variability of Voice Quality and Pain Levels as a Function of Multiple Clinical Parameters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lopez19_interspeech.html": {
    "title": "Assessing Parkinson's Disease from Speech Using Fisher Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/klumpp19_interspeech.html": {
    "title": "Feature Space Visualization with Spatial Similarity Maps for Pathological Speech Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chakravarthula19_interspeech.html": {
    "title": "Predicting Behavior in Cancer-Afflicted Patient and Spouse Interactions Using Speech and Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/qin19_interspeech.html": {
    "title": "Automatic Assessment of Language Impairment Based on Raw ASR Output",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fu19_interspeech.html": {
    "title": "Effects of Spectral and Temporal Cues to Mandarin Concurrent-Vowels Identification for Normal-Hearing and Hearing-Impaired Listeners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zayats19_interspeech.html": {
    "title": "Disfluencies and Human Speech Transcription Errors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/parhammer19_interspeech.html": {
    "title": "The Influence of Distraction on Speech Processing: How Selective is Selective Attention?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hazan19_interspeech.html": {
    "title": "Subjective Evaluation of Communicative Effort for Younger and Older Adults in Interactive Tasks with Energetic and Informational Masking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/davis19_interspeech.html": {
    "title": "Perceiving Older Adults Producing Clear and Lombard Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ariasvergara19_interspeech.html": {
    "title": "Phone-Attribute Posteriors to Evaluate the Speech of Cochlear Implant Users",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hodoshima19_interspeech.html": {
    "title": "Effects of Urgent Speech and Congruent/Incongruent Text on Speech Intelligibility in Noise and Reverberation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mamun19_interspeech.html": {
    "title": "Quantifying Cochlear Implant Users' Ability for Speaker Identification Using CI Auditory Stimuli",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/felker19_interspeech.html": {
    "title": "Lexically Guided Perceptual Learning of a Vowel Shift in an Interactive L2 Listening Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/paulus19_interspeech.html": {
    "title": "Talker Intelligibility and Listening Effort with Temporally Modified Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ward19b_interspeech.html": {
    "title": "R2SPIN: Re-Recording the Revised Speech Perception in Noise Test",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19j_interspeech.html": {
    "title": "Contributions of Consonant-Vowel Transitions to Mandarin Tone Identification in Simulated Electric-Acoustic Hearing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pirhosseinloo19_interspeech.html": {
    "title": "Monaural Speech Enhancement with Dilated Convolutions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liao19b_interspeech.html": {
    "title": "Noise Adaptive Speech Enhancement Using Domain Adversarial Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ge19_interspeech.html": {
    "title": "Environment-Dependent Attention-Driven Recurrent Convolutional Neural Network for Robust Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pariente19_interspeech.html": {
    "title": "A Statistically Principled and Computationally Efficient Approach to Speech Enhancement Using Variational Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lin19d_interspeech.html": {
    "title": "Speech Enhancement Using Forked Generative Adversarial Networks with Spectral Subtraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zezario19_interspeech.html": {
    "title": "Specialized Speech Enhancement Model Selection Based on Learned Non-Intrusive Quality Assessment Metric",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chuang19_interspeech.html": {
    "title": "Speaker-Aware Deep Denoising Autoencoder with Embedded Speaker Identity for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19g_interspeech.html": {
    "title": "Investigation of Cost Function for Supervised Monaural Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shi19b_interspeech.html": {
    "title": "Deep Attention Gated Dilated Temporal Convolutional Networks with Intra-Parallel Convolutional Modules for End-to-End Monaural Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19k_interspeech.html": {
    "title": "Masking Estimation with Phase Restoration of Clean Speech for Monaural Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/llombart19b_interspeech.html": {
    "title": "Progressive Speech Enhancement with Residual Connections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19k_interspeech.html": {
    "title": "Acoustic Model Bootstrapping Using Semi-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mantena19_interspeech.html": {
    "title": "Bandwidth Embeddings for Mixed-Bandwidth Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/khare19_interspeech.html": {
    "title": "Adversarial Black-Box Attacks on Automatic Speech Recognition Systems Using Multi-Objective Evolutionary Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/soomro19_interspeech.html": {
    "title": "Towards Debugging Deep Neural Networks by Generating Speech Utterances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ding19b_interspeech.html": {
    "title": "Compression of CTC-Trained Acoustic Models by Dynamic Frame-Wise Distillation or Segment-Wise N-Best Hypotheses Imitation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lopezespejo19_interspeech.html": {
    "title": "Keyword Spotting for Hearing Assistive Devices Robust to External Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/doulaty19_interspeech.html": {
    "title": "Latent Dirichlet Allocation Based Acoustic Data Selection for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19o_interspeech.html": {
    "title": "Target Speaker Recovery and Recognition Network with Average x-Vector and Global Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/suzuki19b_interspeech.html": {
    "title": "Lyrics Recognition from Singing Voice Focused on Correspondence Between Voice and Notes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hsu19b_interspeech.html": {
    "title": "Transfer Learning from Audio-Visual Grounding to Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/luo19c_interspeech.html": {
    "title": "Cross-Corpus Speech Emotion Recognition Using Semi-Supervised Transfer Non-Negative Matrix Factorization with Adaptation Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tammewar19_interspeech.html": {
    "title": "Modeling User Context for Valence Prediction from Narratives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chakraborty19_interspeech.html": {
    "title": "Front-End Feature Compensation and Denoising for Noise Robust Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19p_interspeech.html": {
    "title": "The Contribution of Acoustic Features Analysis to Model Emotion Perceptual Process for Language Diversity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rajan19_interspeech.html": {
    "title": "Design and Development of a Multi-Lingual Speech Corpora (TaMaR-EmoDB) for Emotion Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sridhar19_interspeech.html": {
    "title": "Speech Emotion Recognition with a Reject Option",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jin19_interspeech.html": {
    "title": "Development of Emotion Rankers Based on Intended and Perceived Emotion Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gideon19_interspeech.html": {
    "title": "Emotion Recognition from Natural Phone Conversations in Individuals with and without Recent Suicidal Ideation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nazareth19_interspeech.html": {
    "title": "An Acoustic and Lexical Analysis of Emotional Valence in Spontaneous Speech: Autobiographical Memory Recall in Older Adults",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhao19g_interspeech.html": {
    "title": "Does the Lombard Effect Improve Emotional Communication in Noise? — Analysis of Emotional Speech Acted in Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gharsellaoui19_interspeech.html": {
    "title": "Linear Discriminant Differential Evolution for Feature Selection in Emotional Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sahu19_interspeech.html": {
    "title": "Multi-Modal Learning for Speech Emotion Recognition: An Analysis and Comparison of ASR Outputs with Ground Truth Transcription",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/spinu19_interspeech.html": {
    "title": "Articulatory Characteristics of Secondary Palatalization in Romanian Fricatives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ratko19_interspeech.html": {
    "title": "Articulation of Vowel Length Contrasts in Australian English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/deme19_interspeech.html": {
    "title": "V-to-V Coarticulation Induced Acoustic and Articulatory Variability of Vowels: The Effect of Pitch-Accent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/king19_interspeech.html": {
    "title": "The Contribution of Lip Protrusion to Anglo-English /r/: Evidence from Hyper- and Non-Hyperarticulated Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/marko19_interspeech.html": {
    "title": "Articulatory Analysis of Transparent Vowel /iː/ in Harmonic and Antiharmonic Hungarian Stems: Is There a Difference?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cunha19_interspeech.html": {
    "title": "On the Role of Oral Configurations in European Portuguese Nasal Vowels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xiong19_interspeech.html": {
    "title": "Residual + Capsule Networks (ResCap) for Simultaneous Single-Channel Overlapped Keyword Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19i_interspeech.html": {
    "title": "A Study for Improving Device-Directed Speech Detection Toward Frictionless Human-Machine Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/su19_interspeech.html": {
    "title": "Unsupervised Methods for Audio Classification from Lecture Discussion Recordings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ashihara19_interspeech.html": {
    "title": "Neural Whispered Speech Detection with Imbalanced Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bergler19_interspeech.html": {
    "title": "Deep Learning for Orca Call Type Identification — A Fully Unsupervised Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sacchi19_interspeech.html": {
    "title": "Open-Vocabulary Keyword Spotting with Audio and Text Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gao19c_interspeech.html": {
    "title": "ToneNet: A CNN Model of Tone Classification of Mandarin Chinese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/choi19_interspeech.html": {
    "title": "Temporal Convolution for Real-Time Keyword Spotting on Mobile Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19j_interspeech.html": {
    "title": "Audio Tagging with Compact Feedforward Sequential Memory Network and Audio-to-Audio Ratio Based Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yang19f_interspeech.html": {
    "title": "Music Genre Classification Using Duplicated Convolutional Layers in Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/carmi19_interspeech.html": {
    "title": "A Storyteller's Tale: Literature Audiobooks Genre Classification Using CNN and RNN Architectures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hwang19_interspeech.html": {
    "title": "Parameter Enhancement for MELP Speech Codec in Noisy Communication Environment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhen19_interspeech.html": {
    "title": "Cascaded Cross-Module Residual Learning Towards Lightweight End-to-End Speech Coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/backstrom19_interspeech.html": {
    "title": "End-to-End Optimization of Source Models for Speech and Audio Coding Using a Machine Learning Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/valin19_interspeech.html": {
    "title": "A Real-Time Wideband Neural Vocoder at 1.6kb/s Using LPCNet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fuchs19_interspeech.html": {
    "title": "Super-Wideband Spectral Envelope Modeling for Speech Coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19q_interspeech.html": {
    "title": "Speech Audio Super-Resolution for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gupta19e_interspeech.html": {
    "title": "Artificial Bandwidth Extension Using H∞ Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mittag19_interspeech.html": {
    "title": "Quality Degradation Diagnosis for Voice Networks — Estimating the Perceived Noisiness, Coloration, and Discontinuity of Transmitted Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chai19b_interspeech.html": {
    "title": "A Cross-Entropy-Guided (CEG) Measure for Speech Enhancement Front-End Assessing Performances of Back-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/moller19_interspeech.html": {
    "title": "Extending the E-Model Towards Super-Wideband and Fullband Speech Communication Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sadhu19_interspeech.html": {
    "title": "Modulation Vectors as Robust Feature Representation for ASR in Domain Mismatched Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19r_interspeech.html": {
    "title": "Prosody Usage Optimization for Children Speech Recognition with Zero Resource Children Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/agrawal19_interspeech.html": {
    "title": "Unsupervised Raw Waveform Representation Learning for ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ramsay19_interspeech.html": {
    "title": "Low-Dimensional Bottleneck Features for On-Device Continuous Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/riviello19_interspeech.html": {
    "title": "Binary Speech Features for Keyword Spotting Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schneider19_interspeech.html": {
    "title": "wav2vec: Unsupervised Pre-Training for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cho19b_interspeech.html": {
    "title": "Automatic Detection of Prosodic Focus in American English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/menon19_interspeech.html": {
    "title": "Feature Exploration for Almost Zero-Resource ASR-Free Keyword Spotting Using a Multilingual Bottleneck Extractor and Correspondence Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/loweimi19_interspeech.html": {
    "title": "On Learning Interpretable CNNs with Parametric Modulated Kernel-Based Filters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/verwimp19_interspeech.html": {
    "title": "Reverse Transfer Learning: Can Word Embeddings Trained for Different NLP Tasks Improve Neural Language Models?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19l_interspeech.html": {
    "title": "Joint Grapheme and Phoneme Embeddings for Contextual End-to-End ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19h_interspeech.html": {
    "title": "Character-Aware Sub-Word Level Language Modeling for Uyghur and Turkish ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pusateri19_interspeech.html": {
    "title": "Connecting and Comparing Language Model Interpolation Techniques",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/khassanov19b_interspeech.html": {
    "title": "Enriching Rare Word Representations in Neural Language Models by Embedding Matrix Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yu19b_interspeech.html": {
    "title": "Comparative Study of Parametric and Representation Uncertainty Modeling for Recurrent Neural Network Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/agenbag19_interspeech.html": {
    "title": "Improving Automatically Induced Lexicons for Highly Agglutinating Languages Using Data-Driven Morphological Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/coucheirolimeres19_interspeech.html": {
    "title": "Attention-Based Word Vector Prediction with LSTMs and its Application to the OOV Problem in ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gao19d_interspeech.html": {
    "title": "Code-Switching Sentence Generation by Bert and Generative Adversarial Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ritchie19_interspeech.html": {
    "title": "Unified Verbalization for Speech Recognition & Synthesis Across Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sharma19e_interspeech.html": {
    "title": "Better Morphology Prediction for Better Speech Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sennema19_interspeech.html": {
    "title": "Vietnamese Learners Tackling the German /ʃt/ in Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lewis19_interspeech.html": {
    "title": "An Articulatory-Acoustic Investigation into GOOSE-Fronting in German-English Bilinguals Residing in London, UK",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jenne19_interspeech.html": {
    "title": "Multimodal Articulation-Based Pronunciation Error Detection with Spectrogram and Acoustic Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/foltz19_interspeech.html": {
    "title": "Using Prosody to Discover Word Order Alternations in a Novel Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bradlow19b_interspeech.html": {
    "title": "Speaking Rate, Information Density, and Information Rate in First-Language and Second-Language Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/graham19_interspeech.html": {
    "title": "Articulation Rate as a Metric in Spoken Language Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xu19c_interspeech.html": {
    "title": "Learning Alignment for Multimodal Emotion Recognition from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/peperkamp19b_interspeech.html": {
    "title": "Liquid Deletion in French Child-Directed Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/seidl19_interspeech.html": {
    "title": "Towards Detection of Canonical Babbling by Citizen Scientists: Performance as a Function of Clip Length",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ludusan19b_interspeech.html": {
    "title": "Nasal Consonant Discrimination in Infant- and Adult-Directed Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/marklund19_interspeech.html": {
    "title": "No Distributional Learning in Adults from Attended Listening to Non-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rasanen19_interspeech.html": {
    "title": "A Computational Model of Early Language Acquisition from Audiovisual Experiences of Young Infants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/du19b_interspeech.html": {
    "title": "The Production of Chinese Affricates /ts/ and /tsh/ by Native Urdu Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19s_interspeech.html": {
    "title": "Multi-Stream Network with Temporal Attention for Environmental Sound Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cerutti19_interspeech.html": {
    "title": "Neural Network Distillation on IoT Platforms for Sound Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lu19d_interspeech.html": {
    "title": "Class-Wise Centroid Distance Metric Learning for Acoustic Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bai19b_interspeech.html": {
    "title": "A Hybrid Approach to Acoustic Scene Classification Based on Universal Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/he19b_interspeech.html": {
    "title": "Hierarchical Pooling Structure for Weakly Labeled Sound Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xia19_interspeech.html": {
    "title": "Sound Event Detection in Multichannel Audio Using Convolutional Time-Frequency-Channel Squeeze and Excitation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pham19b_interspeech.html": {
    "title": "A Robust Framework for Acoustic Scene Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shi19c_interspeech.html": {
    "title": "Compression of Acoustic Event Detection Models with Quantized Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19m_interspeech.html": {
    "title": "An End-to-End Audio Classification System Based on Raw Waveforms and Mix-Training Strategy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19k_interspeech.html": {
    "title": "Few-Shot Audio Classification with Attentional Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lu19e_interspeech.html": {
    "title": "Semi-Supervised Audio Classification with Consistency-Based Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mizgajski19_interspeech.html": {
    "title": "Avaya Conversational Intelligence: A Real-Time System for Spoken Language Understanding in Human-Human Call Center Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/an19b_interspeech.html": {
    "title": "Robust Keyword Spotting via Recycle-Pooling for Mobile Game",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chylek19_interspeech.html": {
    "title": "Multimodal Dialog with the MALACH Audiovisual Archive",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jelil19_interspeech.html": {
    "title": "SpeechMarker: A Voice Based Multi-Level Attendance Application",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wu19h_interspeech.html": {
    "title": "Robust Sound Recognition: A Neuromorphic Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hu19c_interspeech.html": {
    "title": "The CUHK Dysarthric Speech Recognition Systems for English and Cantonese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schiel19_interspeech.html": {
    "title": "BAS Web Services for Automatic Subtitle Creation and Anonymization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/voe19_interspeech.html": {
    "title": "A User-Friendly and Adaptable Re-Implementation of an Acoustic Prominence Detection and Annotation Tool",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dominguez19_interspeech.html": {
    "title": "PyToBI: A Toolkit for ToBI Labeling Under Python",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/levy19_interspeech.html": {
    "title": "GECKO — A Tool for Effective Annotation of Human Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lo19b_interspeech.html": {
    "title": "SLP-AA: Tools for Sign Language Phonetic and Phonological Research",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19t_interspeech.html": {
    "title": "SANTLR: Speech Annotation Toolkit for Low Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gruber19_interspeech.html": {
    "title": "Web-Based Speech Synthesis Editor",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/perrotin19_interspeech.html": {
    "title": "GFM-Voc: A Real-Time Voice Quality Modification System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/szekely19_interspeech.html": {
    "title": "Off the Cuff: Exploring Extemporaneous Speech Delivery with TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kessler19_interspeech.html": {
    "title": "Synthesized Spoken Names: Biases Impacting Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bernardo19_interspeech.html": {
    "title": "Unbabel Talk — Human Verified Translations for Voice Instant Messaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rabiee19_interspeech.html": {
    "title": "Adjusting Pleasure-Arousal-Dominance for Continuous Emotional Text-to-Speech Synthesizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lapata19_interspeech.html": {
    "title": "Learning Natural Language Interfaces with Neural Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nautsch19c_interspeech.html": {
    "title": "The GDPR & Speech Data: Reflections of Legal and Technology Communities, First Steps Towards a Common Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/srivastava19_interspeech.html": {
    "title": "Privacy-Preserving Adversarial Representation Learning in ASR: Reality or Illusion?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nelus19_interspeech.html": {
    "title": "Privacy-Preserving Siamese Feature Extraction for Gender Recognition versus Speaker Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nelus19b_interspeech.html": {
    "title": "Privacy-Preserving Variational Information Feature Extraction for Domestic Activity Monitoring versus Speaker Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/thaine19_interspeech.html": {
    "title": "Extracting Mel-Frequency and Bark-Frequency Cepstral Coefficients from Encrypted Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zarazaga19_interspeech.html": {
    "title": "Sound Privacy: A Conversational Speech Corpus for Quantifying the Experience of Privacy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/soto19_interspeech.html": {
    "title": "Improving Code-Switched Language Modeling Performance Using Cognate Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lee19d_interspeech.html": {
    "title": "Linguistically Motivated Parallel Data Augmentation for Code-Switch Language Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rallabandi19_interspeech.html": {
    "title": "Variational Attention Using Articulatory Priors for Generating Code Mixed Speech Using Monolingual Corpora",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19l_interspeech.html": {
    "title": "Code-Switching Detection Using ASR-Generated Language Posteriors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/biswas19b_interspeech.html": {
    "title": "Semi-Supervised Acoustic Model Training for Five-Lingual Code-Switched ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ylmaz19b_interspeech.html": {
    "title": "Multi-Graph Decoding for Code-Switching ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/seki19_interspeech.html": {
    "title": "End-to-End Multilingual Multi-Speaker Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/guasch19_interspeech.html": {
    "title": "Survey Talk: Realistic Physics-Based Computational Voice Production",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mohapatra19_interspeech.html": {
    "title": "An Extended Two-Dimensional Vocal Tract Model for Fast Acoustic Simulation of Single-Axis Symmetric Three-Dimensional Tubes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/birkholz19_interspeech.html": {
    "title": "Perceptual Optimization of an Enhanced Geometric Vocal Fold Model for Articulatory Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gao19e_interspeech.html": {
    "title": "Articulatory Copy Synthesis Based on a Genetic Algorithm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shahrebabaki19_interspeech.html": {
    "title": "A Phonetic-Level Analysis of Different Input Features for Articulatory Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tuske19_interspeech.html": {
    "title": "Advancing Sequence-to-Sequence Based Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hannun19_interspeech.html": {
    "title": "Sequence-to-Sequence Speech Recognition with Time-Depth Separable Convolutions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/baskar19_interspeech.html": {
    "title": "Semi-Supervised Sequence-to-Sequence ASR Using Unpaired Speech and Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bai19c_interspeech.html": {
    "title": "Learn Spelling from Teachers: Transferring Knowledge from Language Models to Sequence-to-Sequence Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/irie19_interspeech.html": {
    "title": "On the Choice of Modeling Unit for Sequence-to-Sequence Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/weninger19b_interspeech.html": {
    "title": "Listen, Attend, Spell and Adapt: Speaker Adapted Sequence-to-Sequence ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/runarsdottir19_interspeech.html": {
    "title": "Lattice Re-Scoring During Manual Editing for Automatic Error Correction of ASR Transcripts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fukunaga19_interspeech.html": {
    "title": "GPU-Based WFST Decoding with Extra Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jorge19_interspeech.html": {
    "title": "Real-Time One-Pass Decoder for Speech Recognition Using LSTM Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/seki19b_interspeech.html": {
    "title": "Vectorized Beam Search for CTC-Attention-Based Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/serrino19_interspeech.html": {
    "title": "Contextual Recovery of Out-of-Lattice Named Entities in Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/novitasari19_interspeech.html": {
    "title": "Sequence-to-Sequence Learning via Attention Transfer for Incremental Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lian19b_interspeech.html": {
    "title": "Unsupervised Representation Learning with Future Observation Prediction for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/phan19_interspeech.html": {
    "title": "Spatio-Temporal Attention Pooling for Audio Scene Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shi19d_interspeech.html": {
    "title": "Subspace Pooling Based Temporal Features Extraction for Audio Event Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19l_interspeech.html": {
    "title": "Multi-Scale Time-Frequency Attention for Acoustic Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/song19b_interspeech.html": {
    "title": "Acoustic Scene Classification by Implicitly Identifying Distinct Sound Events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/qi19_interspeech.html": {
    "title": "Parameter-Transfer Learning for Low-Resource Individualization of Head-Related Transfer Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19i_interspeech.html": {
    "title": "Prosodic Characteristics of Mandarin Declarative and Interrogative Utterances in Parkinson's Disease",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/morovelazquez19_interspeech.html": {
    "title": "Study of the Performance of Automatic Speech Recognition Systems in Speakers with Parkinson's Disease",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19m_interspeech.html": {
    "title": "Towards the Speech Features of Mild Cognitive Impairment: Universal Evidence from Structured and Unstructured Connected Speech of Chinese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19n_interspeech.html": {
    "title": "Child Speech Disorder Detection with Siamese Recurrent Network Using Speech Attribute Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/korzekwa19_interspeech.html": {
    "title": "Interpretable Deep Learning Model for the Detection and Reconstruction of Dysarthric Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/noufi19_interspeech.html": {
    "title": "Vocal Biomarker Assessment Following Pediatric Traumatic Brain Injury: A Retrospective Cohort Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/scharenborg19b_interspeech.html": {
    "title": "Survey Talk: Reaching Over the Gap: Cross- and Interdisciplinary Research on Human and Automatic Speech Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ogawa19_interspeech.html": {
    "title": "Improved Deep Duel Model for Rescoring N-Best Speech Recognition List Using Backward LSTMLM and Ensemble Encoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/irie19b_interspeech.html": {
    "title": "Language Modeling with Deep Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/raju19_interspeech.html": {
    "title": "Scalable Multi Corpora Neural Language Models for ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/likhomanenko19_interspeech.html": {
    "title": "Who Needs Words? Lexicon-Free Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/latif19_interspeech.html": {
    "title": "Direct Modelling of Speech Emotion from Raw Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sarma19_interspeech.html": {
    "title": "Improving Emotion Identification Using Phone Posteriors in Raw Speech Waveform Based DNN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cao19_interspeech.html": {
    "title": "Pyramid Memory Block and Timestep Attention for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/oates19_interspeech.html": {
    "title": "Robust Speech Emotion Recognition Under Different Encoding Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gosztolya19c_interspeech.html": {
    "title": "Using the Bag-of-Audio-Word Feature Representation of ASR DNN Posteriors for Paralinguistic Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/williams19c_interspeech.html": {
    "title": "Disentangling Style Factors from Speaker Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hsu19c_interspeech.html": {
    "title": "Sentence Prosody and Wh-Indeterminates in Taiwan Mandarin",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hu19d_interspeech.html": {
    "title": "Frication as a Vowel Feature? — Evidence from the Rui'an Wu Chinese Dialect",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19m_interspeech.html": {
    "title": "Vowels and Diphthongs in the Xupu Xiang Chinese Dialect",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/albuquerque19_interspeech.html": {
    "title": "Age-Related Changes in European Portuguese Vowel Acoustics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lalhminghlui19_interspeech.html": {
    "title": "Vowel-Tone Interaction in Two Tibeto-Burman Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rodriguez19_interspeech.html": {
    "title": "The Vowel System of Korebaju",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ibrahim19_interspeech.html": {
    "title": "Fundamental Frequency Accommodation in Multi-Party Human-Robot Game Interactions: The Effect of Winning or Losing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wagner19_interspeech.html": {
    "title": "Pitch Accent Trajectories Across Different Conditions of Visibility and Information Structure — Evidence from Spontaneous Dyadic Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/betz19_interspeech.html": {
    "title": "The Greennn Tree — Lengthening Position Influences Uncertainty Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/si19_interspeech.html": {
    "title": "CNN-BLSTM Based Question Detection from Dialogs Considering Phase and Context Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/metcalf19_interspeech.html": {
    "title": "Mirroring to Build Trust in Digital Assistants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/raveh19_interspeech.html": {
    "title": "Three's a Crowd? Effects of a Second Human on Vocal Accommodation with a Voice Assistant",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19o_interspeech.html": {
    "title": "Adversarial Regularization for End-to-End Robust Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/monteiro19_interspeech.html": {
    "title": "Combining Speaker Recognition and Metric Learning for Speaker-Dependent Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19n_interspeech.html": {
    "title": "VAE-Based Regularization for Deep Speaker Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mingote19b_interspeech.html": {
    "title": "Language Recognition Using Triplet Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jung19c_interspeech.html": {
    "title": "Spatial Pyramid Encoding with Convex Length Normalization for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/heo19b_interspeech.html": {
    "title": "End-to-End Losses Based on Speaker Basis Vectors and All-Speaker Hard Negative Mining for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jiang19_interspeech.html": {
    "title": "An Effective Deep Embedding Learning Architecture for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/qin19b_interspeech.html": {
    "title": "Far-Field End-to-End Text-Dependent Speaker Verification Based on Mixed Training Data with Transfer Learning and Enrollment Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ren19_interspeech.html": {
    "title": "Two-Stage Training for Chinese Dialect Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kaminishi19_interspeech.html": {
    "title": "Investigation on Blind Bandwidth Extension with a Non-Linear Function and its Evaluation of x-Vector-Based Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/khan19_interspeech.html": {
    "title": "Auto-Encoding Nearest Neighbor i-Vectors for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zheng19b_interspeech.html": {
    "title": "Towards a Fault-Tolerant Speaker Verification System: A Regularization Approach to Reduce the Condition Number",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/taherian19_interspeech.html": {
    "title": "Deep Learning Based Multi-Channel Speaker Recognition in Noisy and Reverberant Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yang19g_interspeech.html": {
    "title": "Joint Optimization of Neural Acoustic Beamforming and Dereverberation with x-Vectors for Robust Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/miao19b_interspeech.html": {
    "title": "A New Time-Frequency Attention Mechanism for TDNN and CNN-LSTM-TDNN, with Application to Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19n_interspeech.html": {
    "title": "An Attention-Based Hybrid Network for Automatic Detection of Alzheimer's Disease from Narrative Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ma19b_interspeech.html": {
    "title": "Investigating the Lombard Effect Influence on End-to-End Audio-Visual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ooster19_interspeech.html": {
    "title": "Computer, Test My Hearing\": Accurate Speech Audiometry with Smart Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/eshky19_interspeech.html": {
    "title": "Synchronising Audio and Ultrasound by Learning Cross-Modal Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pan19_interspeech.html": {
    "title": "Automatic Hierarchical Attention Neural Network for Detecting AD",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nallanthighal19_interspeech.html": {
    "title": "Deep Sensing of Breathing Signal During Conversational Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/biadsy19_interspeech.html": {
    "title": "Parrotron: An End-to-End Speech-to-Speech Conversion Model and its Applications to Hearing-Impaired Speech and Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19j_interspeech.html": {
    "title": "Exploiting Visual Features Using Bayesian Gated Neural Networks for Disordered Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vougioukas19_interspeech.html": {
    "title": "Video-Driven Speech Reconstruction Using Generative Adversarial Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19k_interspeech.html": {
    "title": "On the Use of Pitch Features for Disordered Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shillingford19_interspeech.html": {
    "title": "Large-Scale Visual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/razavi19_interspeech.html": {
    "title": "Investigating Linguistic and Semantic Features for Turn-Taking Prediction in Open-Domain Human-Computer Conversation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bechet19_interspeech.html": {
    "title": "Benchmarking Benchmarks: Introducing New Automatic Indicators for Benchmarking Spoken Language Understanding Corpora",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19l_interspeech.html": {
    "title": "A Neural Turn-Taking Model without RNN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/coman19_interspeech.html": {
    "title": "An Incremental Turn-Taking Model for Task-Oriented Dialog Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/su19b_interspeech.html": {
    "title": "Personalized Dialogue Response Generation Learned from Monologues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/heldner19_interspeech.html": {
    "title": "Voice Quality as a Turn-Taking Cue",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hara19_interspeech.html": {
    "title": "Turn-Taking Prediction Based on Detection of Transition Relevance Place",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lala19_interspeech.html": {
    "title": "Analysis of Effect and Timing of Fillers in Natural Turn-Taking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/horiguchi19_interspeech.html": {
    "title": "Multimodal Response Obligation Detection with Unsupervised Online Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/su19c_interspeech.html": {
    "title": "Follow-Up Question Generation Using Neural Tensor Network-Based Domain Ontology Population in an Interview Coaching System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tran19_interspeech.html": {
    "title": "On the Role of Style in Parsing Speech with Neural Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pasad19_interspeech.html": {
    "title": "On the Contributions of Visual and Textual Supervision in Low-Resource Semantic Speech Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19p_interspeech.html": {
    "title": "Automatic Detection of Off-Topic Spoken Responses Using Very Deep Convolutional Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/piunova19_interspeech.html": {
    "title": "Rescoring Keyword Search Confidence Estimates with Graph-Based Re-Ranking Using Acoustic Word Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/segal19_interspeech.html": {
    "title": "SpeechYOLO: Detection and Localization of Speech Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/oktem19_interspeech.html": {
    "title": "Prosodic Phrase Alignment for Machine Dubbing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tannander19_interspeech.html": {
    "title": "Spot the Pleasant People! Navigating the Cocktail Party Buzz",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19o_interspeech.html": {
    "title": "Neural Text Clustering with Document-Level Attention Based on Dynamic Soft Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bach19_interspeech.html": {
    "title": "Noisy BiLSTM-Based Models for Disfluency Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/singh19b_interspeech.html": {
    "title": "Subword RNNLM Approximations for Out-Of-Vocabulary Keyword Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/maekaku19_interspeech.html": {
    "title": "Simultaneous Detection and Localization of a Wake-Up Word Using Multi-Task Learning of the Duration and Endpoint",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lee19e_interspeech.html": {
    "title": "On Mitigating Acoustic Feedback in Hearing Aids with Frequency Warping by All-Pass Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fazel19_interspeech.html": {
    "title": "Deep Multitask Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19o_interspeech.html": {
    "title": "Deep Learning for Joint Acoustic Echo and Noise Cancellation with Nonlinear Distortions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/srensen19_interspeech.html": {
    "title": "Harmonic Beamformers for Non-Intrusive Speech Intelligibility Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mamun19b_interspeech.html": {
    "title": "Convolutional Neural Network-Based Speech Enhancement for Cochlear Implant Recipients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/srensen19b_interspeech.html": {
    "title": "Validation of the Non-Intrusive Codebook-Based Short Time Objective Intelligibility Metric for Processed Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/arai19_interspeech.html": {
    "title": "Predicting Speech Intelligibility of Enhanced Speech Using Phone Accuracy of DNN-Based ASR System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bu19_interspeech.html": {
    "title": "A Novel Method to Correct Steering Vectors in MVDR Beamformer for Noise Robust ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lee19f_interspeech.html": {
    "title": "End-to-End Multi-Channel Speech Enhancement Using Inter-Channel Time-Restricted Attention on Raw Waveform",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gu19b_interspeech.html": {
    "title": "Neural Spatial Filter: Target Speaker Speech Separation Assisted with Directional Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/afouras19_interspeech.html": {
    "title": "My Lips Are Concealed: Audio-Visual Speech Enhancement Through Obstructions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fujita19_interspeech.html": {
    "title": "End-to-End Neural Speaker Diarization with Permutation-Free Objectives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/india19_interspeech.html": {
    "title": "Self Multi-Head Attention for Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vinals19b_interspeech.html": {
    "title": "Phonetically-Aware Embeddings, Wide Residual Networks with Time-Delay Neural Networks and Self Attention Models for the 2018 NIST Speaker Recognition Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tu19_interspeech.html": {
    "title": "Variational Domain Adversarial Learning for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19m_interspeech.html": {
    "title": "A Unified Framework for Speaker and Utterance Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nandwana19c_interspeech.html": {
    "title": "Analysis of Critical Metadata Factors for the Calibration of Speaker Recognition Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/novotny19_interspeech.html": {
    "title": "Factorization of Discriminatively Trained i-Vector Extractor for Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/salvati19_interspeech.html": {
    "title": "End-to-End Speaker Identification in Noisy and Reverberant Environments Using Raw Waveform Convolutional Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/naini19_interspeech.html": {
    "title": "Whisper to Neutral Mapping Using Cosine Similarity Maximization in i-Vector Space for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhu19b_interspeech.html": {
    "title": "Mixup Learning Strategies for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ferrer19_interspeech.html": {
    "title": "Optimizing a Speaker Embedding Extractor Through Backend-Driven Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lee19g_interspeech.html": {
    "title": "The NEC-TT 2018 Speaker Verification System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zheng19c_interspeech.html": {
    "title": "Autoencoder-Based Semi-Supervised Curriculum Learning for Out-of-Domain Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cai19d_interspeech.html": {
    "title": "Multi-Channel Training for End-to-End Speaker Recognition Under Reverberant and Noisy Environment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cai19e_interspeech.html": {
    "title": "The DKU-SMIIP System for NIST 2018 Speaker Recognition Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wiesner19_interspeech.html": {
    "title": "Pretraining by Backtranslation for End-to-End ASR in Low-Resource Settings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kim19b_interspeech.html": {
    "title": "Cross-Attention End-to-End ASR for Two-Party Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chorowski19_interspeech.html": {
    "title": "Towards Using Context-Dependent Symbols in CTC Without State-Tying Decision Trees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fan19b_interspeech.html": {
    "title": "An Online Attention-Based Model for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tian19b_interspeech.html": {
    "title": "Self-Attention Transducers for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19u_interspeech.html": {
    "title": "Improving Transformer-Based Speech Recognition Systems with Compressed Structure and Speech Attributes Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bang19_interspeech.html": {
    "title": "Extending an Acoustic Data-Driven Phone Set for Spontaneous Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/moriya19_interspeech.html": {
    "title": "Joint Maximization Decoder with Neural Converters for Fully Neural Network-Based Japanese Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/parcollet19b_interspeech.html": {
    "title": "Real to H-Space Encoder for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yi19b_interspeech.html": {
    "title": "Ectc-Docd: An End-to-End Structure with CTC Encoder and OCD Decoder for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/denisov19_interspeech.html": {
    "title": "End-to-End Multi-Speaker Speech Recognition Using Speaker Embeddings and Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hayashi19_interspeech.html": {
    "title": "Pre-Trained Text Embeddings for Enhanced Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/szekely19b_interspeech.html": {
    "title": "Spontaneous Conversational Speech Synthesis from Found Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/klimkov19_interspeech.html": {
    "title": "Fine-Grained Robust Prosody Transfer for Single-Speaker Neural Text-To-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hussain19_interspeech.html": {
    "title": "Speech Driven Backchannel Generation Using Deep Q-Network for Enhancing Engagement in Human-Robot Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/koriyama19_interspeech.html": {
    "title": "Semi-Supervised Prosody Modeling Using Deep Gaussian Process Latent Variable Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nikulasdottir19_interspeech.html": {
    "title": "Bootstrapping a Text Normalization System for an Inflected Language. Numbers as a Test Case",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/guo19e_interspeech.html": {
    "title": "Exploiting Syntactic Features in a Parsed Tree to Improve End-to-End TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ni19_interspeech.html": {
    "title": "Duration Modeling with Global Phoneme-Duration Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/aubin19_interspeech.html": {
    "title": "Improving Speech Synthesis with Discourse Relations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tits19_interspeech.html": {
    "title": "Visualization and Interpretation of Latent Spaces for Controlling Expressive Speech Synthesis Through Audio Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yang19h_interspeech.html": {
    "title": "Pre-Trained Text Representations for Improving Front-End Text Processing in Mandarin Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pan19b_interspeech.html": {
    "title": "A Mandarin Prosodic Boundary Prediction Model Based on Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gokcen19_interspeech.html": {
    "title": "Dual Encoder Classifier Models as Constraints in Neural Text Normalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19v_interspeech.html": {
    "title": "Knowledge-Based Linguistic Encoding for End-to-End Mandarin Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shankar19c_interspeech.html": {
    "title": "Automated Emotion Morphing in Speech Based on Diffeomorphic Curve Registration and Highway Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/connaghan19_interspeech.html": {
    "title": "Use of Beiwe Smartphone App to Identify and Track Speech Decline in Amyotrophic Lateral Sclerosis (ALS)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rowe19_interspeech.html": {
    "title": "Profiling Speech Motor Impairments in Persons with Amyotrophic Lateral Sclerosis: An Acoustic-Based Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mayle19_interspeech.html": {
    "title": "Diagnosing Dysarthria with Long Short-Term Memory Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sudro19_interspeech.html": {
    "title": "Modification of Devoicing Error in Cleft Lip and Palate Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/eshghi19_interspeech.html": {
    "title": "Reduced Task Adaptation in Alternating Motion Rate Tasks as an Early Marker of Bulbar Involvement in Amyotrophic Lateral Sclerosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19q_interspeech.html": {
    "title": "Towards the Speech Features of Early-Stage Dementia: Design and Application of the Mandarin Elderly Cognitive Speech Database",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19p_interspeech.html": {
    "title": "Acoustic Characteristics of Lexical Tone Disruption in Mandarin Speakers After Brain Damage",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hermes19_interspeech.html": {
    "title": "Intragestural Variation in Natural Sentence Production: Essential Tremor Patients Treated with DBS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kalita19_interspeech.html": {
    "title": "Nasal Air Emission in Sibilant Fricatives of Cleft Lip and Palate Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/serrano19_interspeech.html": {
    "title": "Parallel vs. Non-Parallel Voice Conversion for Esophageal Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dubey19b_interspeech.html": {
    "title": "Hypernasality Severity Detection Using Constant Q Cepstral Coefficients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/niu19_interspeech.html": {
    "title": "Automatic Depression Level Detection via ℓp-Norm Pooling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bn19_interspeech.html": {
    "title": "Comparison of Speech Tasks and Recording Devices for Voice Based Automatic Classification of Healthy Subjects and Patients with Amyotrophic Lateral Sclerosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19r_interspeech.html": {
    "title": "A Modified Algorithm for Multiple Input Spectrogram Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bahmaninezhad19_interspeech.html": {
    "title": "A Comprehensive Study of Speech Separation: Spectrogram vs Waveform Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/inan19_interspeech.html": {
    "title": "Evaluating Audiovisual Source Separation in the Context of Video Conferencing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ditter19_interspeech.html": {
    "title": "Influence of Speaker-Specific Parameters on Speech Separation Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zegers19_interspeech.html": {
    "title": "CNN-LSTM Models for Multi-Speaker Source Separation Using Bayesian Hyper Parameter Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bear19_interspeech.html": {
    "title": "Towards Joint Sound Scene and Polyphonic Sound Event Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fan19c_interspeech.html": {
    "title": "Discriminative Learning for Monaural Speech Separation Using Deep Embedding Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yousefi19_interspeech.html": {
    "title": "Probabilistic Permutation Invariant Training for Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shi19e_interspeech.html": {
    "title": "Which Ones Are Speaking? Speaker-Inferred Model for Multi-Talker Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shi19f_interspeech.html": {
    "title": "End-to-End Monaural Speech Separation with Multi-Scale Dynamic Weighted Gated Dilated Convolutional Pyramid Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lluis19_interspeech.html": {
    "title": "End-to-End Music Source Separation: Is it Possible in the Waveform Domain?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/foley19_interspeech.html": {
    "title": "Elpis, an Accessible Speech-to-Text Tool",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gruber19b_interspeech.html": {
    "title": "Framework for Conducting Tasks Requiring Human Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19k_interspeech.html": {
    "title": "Multimedia Simultaneous Translation System for Minority Language Communication with Mandarin",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dikici19_interspeech.html": {
    "title": "The SAIL LABS Media Mining Indexer and the CAVA Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/goel19b_interspeech.html": {
    "title": "CaptionAI: A Real-Time Multilingual Captioning Application",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  }
}