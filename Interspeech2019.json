{
  "https://www.isca-speech.org/archive/interspeech_2019/tokuda19_interspeech.html": {
    "title": "Statistical Approach to Speech Synthesis: Past, Present and Future",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "fa87441f84e6fe211195648b103707bbe9bd2367",
    "semantic_title": "statistical approach to speech synthesis: past, present and future",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wu19_interspeech.html": {
    "title": "Advances in Automatic Speech Recognition for Child Speech Using Factored Time Delay Neural Network",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) has shown huge advances in adult speech; however, when the models are tested on child speech, the performance does not achieve satisfactory word error rates (WER). This is mainly due to the high variance in acoustic features of child speech and the lack of clean, labeled corpora. We apply the factored time delay neural network (TDNN-F) to the child speech domain, finding that it yields better performance. To enable our models to handle the different noise conditions and extremely small corpora, we augment the original training data by adding noise and reverberation. Compared with conventional GMM-HMM and TDNN systems, TDNN-F does better on two widely accessible corpora: CMU Kids and CSLU Kids, and on the combination of these two. Our system achieves a 26% relative improvement in WER",
    "checked": true,
    "id": "beeaa7417e7818f737c2958550757735982fc49b",
    "semantic_title": "advances in automatic speech recognition for child speech using factored time delay neural network",
    "citation_count": 32
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yeung19_interspeech.html": {
    "title": "A Frequency Normalization Technique for Kindergarten Speech Recognition Inspired by the Role of fo in Vowel Perception",
    "volume": "main",
    "abstract": "Accurate automatic speech recognition (ASR) of kindergarten speech is particularly important as this age group may benefit the most from voice-based educational tools. Due to the lack of young child speech data, kindergarten ASR systems often are trained using older child or adult speech. This study proposes a fundamental frequency (f )-based normalization technique to reduce the spectral mismatch between kindergarten and older child speech. The technique is based on the tonotopic distances between formants and f developed to model vowel perception. This proposed procedure only relies on the computation of median f across an utterance. Tonotopic distances for vowel perception were reformulated as a linear relationship between formants and f to provide an effective approach for frequency normalization. This reformulation was verified by examining the formants and f of child vowel productions. A 208-word ASR experiment using older child speech for training and kindergarten speech for testing was performed to examine the effectiveness of the proposed technique against piecewise vocal tract length, F3-based, and subglottal resonance normalization techniques. Results suggest that the proposed technique either has performance advantages or requires the computation of fewer parameters",
    "checked": true,
    "id": "b521d1c626053e5252bc0fcd3e16d63efde5ba3c",
    "semantic_title": "a frequency normalization technique for kindergarten speech recognition inspired by the role of fo in vowel perception",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gale19_interspeech.html": {
    "title": "Improving ASR Systems for Children with Autism and Language Impairment Using Domain-Focused DNN Transfer Techniques",
    "volume": "main",
    "abstract": "This study explores building and improving an automatic speech recognition (ASR) system for children aged 6–9 years and diagnosed with autism spectrum disorder (ASD), language impairment (LI), or both. Working with only 1.5 hours of target data in which children perform the Clinical Evaluation of Language Fundamentals Recalling Sentences task, we apply deep neural network (DNN) weight transfer techniques to adapt a large DNN model trained on the LibriSpeech corpus of adult speech. To begin, we aim to find the best proportional training rates of the DNN layers. Our best configuration yields a 29.38% word error rate (WER). Using this configuration, we explore the effects of quantity and similarity of data augmentation in transfer learning. We augment our training with portions of the OGI Kids' Corpus, adding 4.6 hours of typically developing speakers aged kindergarten through 3 grade. We find that 2 grade data alone — approximately the mean age of the target data — outperforms other grades and all the sets combined. Doubling the data for 1 , 2 , and 3 grade, we again compare each grade as well as pairs of grades. We find the combination of 1 and 2 grade performs best at a 26.21% WER",
    "checked": true,
    "id": "9826fb6c64b76d5d0809a7c6ae5914342c782930",
    "semantic_title": "improving asr systems for children with autism and language impairment using domain-focused dnn transfer techniques",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ribeiro19_interspeech.html": {
    "title": "Ultrasound Tongue Imaging for Diarization and Alignment of Child Speech Therapy Sessions",
    "volume": "main",
    "abstract": "We investigate the automatic processing of child speech therapy sessions using ultrasound visual biofeedback, with a specific focus on complementing acoustic features with ultrasound images of the tongue for the tasks of speaker diarization and time-alignment of target words. For speaker diarization, we propose an ultrasound-based time-domain signal which we call estimated tongue activity. For word-alignment, we augment an acoustic model with low-dimensional representations of ultrasound images of the tongue, learned by a convolutional neural network. We conduct our experiments using the Ultrasuite repository of ultrasound and speech recordings for child speech therapy sessions. For both tasks, we observe that systems augmented with ultrasound data outperform corresponding systems using only the audio signal",
    "checked": true,
    "id": "e940bdd66421ab3d0d26434798344f846553c35f",
    "semantic_title": "ultrasound tongue imaging for diarization and alignment of child speech therapy sessions",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/loukina19_interspeech.html": {
    "title": "Automated Estimation of Oral Reading Fluency During Summer Camp e-Book Reading with MyTurnToRead",
    "volume": "main",
    "abstract": "Use of speech technologies in the classroom is often limited by the inferior acoustic conditions as well as other factors that might affect the quality of the recordings. We describe MyTurnToRead, an e-book-based app designed to support an interleaved listening and reading experience, where the child takes turns reading aloud with a virtual partner. The child's reading turns are recorded, and processed by an automated speech analysis system in order to provide feedback or track improvement in reading skill. We describe the architecture of the speech processing back-end and evaluate system performance on the data collected in several summer camps where children used the app on consumer-grade devices as part of the camp programming. We show that while the quality of the audio recordings varies greatly, our estimates of student oral reading fluency are very good: for example, the correlation between ASR-based and transcription-based estimates of reading fluency at the speaker level is r=0.93. These are also highly correlated with an external measure of reading comprehension",
    "checked": true,
    "id": "4b6866be7c111007cf4cb949b36c2123cc7f285e",
    "semantic_title": "automated estimation of oral reading fluency during summer camp e-book reading with myturntoread",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lopes19_interspeech.html": {
    "title": "Sustained Vowel Game: A Computer Therapy Game for Children with Dysphonia",
    "volume": "main",
    "abstract": "Problems in vocal quality are common in 4 to 12-year-old children, which may affect their health as well as their social interactions and development process. The sustained vowel exercise is widely used by speech and language pathologists for the child's voice recovery and vocal re-education. Nonetheless, despite being an important voice exercise, it can be a monotonous and tedious activity for children. Here, we propose a computer therapy game that uses the sustained vowel exercise to motivate children on doing this exercise often. In addition, the game gives visual feedback on the child's performance, which helps the child understand how to improve the voice production. The game uses a vowel classification model learned with a support vector machine and Mel frequency cepstral coefficients. A user test with 14 children showed that when using the game, children achieve longer phonation times than without the game. Also, it shows that the visual feedback helps and motivates children on improving their sustained vowel productions",
    "checked": true,
    "id": "594c6449ddd016ae5ac10079546d39b518cee779",
    "semantic_title": "sustained vowel game: a computer therapy game for children with dysphonia",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/esposito19_interspeech.html": {
    "title": "The Dependability of Voice on Elders' Acceptance of Humanoid Agents",
    "volume": "main",
    "abstract": "The research on ambient assistive technology is concerned with features humanoid agents should show in order to gain user acceptance. However, differently aged groups may have different requirements. This paper is particularly focused on agent's voice preferences among elders, young adults, and adolescents To this aim 316 users organized in groups of 45/46 subjects of which 3 groups of elders (65+ years old), 2 of young adults (aged between 22–35 years), and 2 of adolescents (aged between 14–16 years) were recruited and administered the Virtual Agent Acceptance Questionnaire (VAAQ), after watching video-clips of mute and speaking agents, in order to test their preferences in terms of willingness to interact, pragmatic and hedonic qualities, and attractiveness, of proposed speaking and mute agents. In addition, the elders were also tested on listening only the agent's. The results suggest that voice is primary for getting elder's acceptance of virtual humanoid agents in contrast to young adults and adolescents which accept equally well either mute or speaking agents",
    "checked": true,
    "id": "7e956e27098fe50c55e709ed2ba4247ac24f8f5e",
    "semantic_title": "the dependability of voice on elders' acceptance of humanoid agents",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/niebuhr19_interspeech.html": {
    "title": "God as Interlocutor — Real or Imaginary? Prosodic Markers of Dialogue Speech and Expected Efficacy in Spoken Prayer",
    "volume": "main",
    "abstract": "We analyze the phonetic correlates of petitionary prayer in 22 Christian practitioners. Our aim is to examine if praying is characterized by prosodic markers of dialogue speech and expected efficacy. Three similar conditions are compared; 1) requests to God, 2) requests to a human recipient, 3) requests to an imaginary person. We find that making requests to God is clearly distinguishable from making requests to both human and imaginary interlocutors. Requests to God are, unlike requests to an imaginary person, characterized by markers of dialogue speech (as opposed to monologue speech), including, a higher f0 level, a larger f0 range, and a slower speaking rate. In addition, requests to God differ from those made to both human and imaginary persons in markers of expected efficacy on the part of the speaker. These markers are related to a more careful speech production, including almost complete lack of hesitations, more pauses, and a much longer speaking time",
    "checked": false,
    "id": "8608531f9219f0f0b8ed895c26a9dfa3bd95cb17",
    "semantic_title": "god as interlocutor - real or imaginary? prosodic markers of dialogue speech and expected efficacy in spoken prayer",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cohn19_interspeech.html": {
    "title": "Expressiveness Influences Human Vocal Alignment Toward voice-AI",
    "volume": "main",
    "abstract": "This study explores whether people align to expressive speech spoken by a voice-activated artificially intelligent device (voice-AI), specifically Amazon's Alexa. Participants shadowed words produced by the Alexa voice in two acoustically distinct conditions: \"regular\" and \"expressive\", containing more exaggerated pitch contours and longer word durations. Another group of participants rated the shadowed items, in an AXB perceptual similarity task, as an assessment of overall degree of vocal alignment. Results show greater vocal alignment toward expressive speech produced by the Alexa voice and, furthermore, systematic variation based on speaker gender. Overall, these findings have applications to the field of affective computing in understanding human responses to synthesized emotional expressiveness",
    "checked": true,
    "id": "7499570fd0ddc4fe1da060aeb8a5fcec15f33755",
    "semantic_title": "expressiveness influences human vocal alignment toward voice-ai",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lai19_interspeech.html": {
    "title": "Detecting Topic-Oriented Speaker Stance in Conversational Speech",
    "volume": "main",
    "abstract": "Being able to detect topics and speaker stances in conversations is a key requirement for developing spoken language understanding systems that are personalized and adaptive. In this work, we explore how topic-oriented speaker stance is expressed in conversational speech. To do this, we present a new set of topic and stance annotations of the CallHome corpus of spontaneous dialogues. Specifically, we focus on six stances — positivity, certainty, surprise, amusement, interest, and comfort — which are useful for characterizing important aspects of a conversation, such as whether a conversation is going well or not. Based on this, we investigate the use of neural network models for automatically detecting speaker stance from speech in multi-turn, multi-speaker contexts. In particular, we examine how performance changes depending on how input feature representations are constructed and how this is related to dialogue structure. Our experiments show that incorporating both lexical and acoustic features is beneficial for stance detection. However, we observe variation in whether using hierarchical models for encoding lexical and acoustic information improves performance, suggesting that some aspects of speaker stance are expressed more locally than others. Overall, our findings highlight the importance of modelling interaction dynamics and non-lexical content for stance detection",
    "checked": true,
    "id": "ab76b65badae8872ce191ea6173d0fdf1f1c3536",
    "semantic_title": "detecting topic-oriented speaker stance in conversational speech",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sebastian19_interspeech.html": {
    "title": "Fusion Techniques for Utterance-Level Emotion Recognition Combining Speech and Transcripts",
    "volume": "main",
    "abstract": "In human perception and understanding, a number of different and complementary cues are adopted according to different modalities. Various emotional states in communication between humans reflect this variety of cues across modalities. Recent developments in multi-modal emotion recognition utilize deep-learning techniques to achieve remarkable performances, with models based on different features suitable for text, audio and vision. This work focuses on cross-modal fusion techniques over deep learning models for emotion detection from spoken audio and corresponding transcripts We investigate the use of long short-term memory (LSTM) recurrent neural network (RNN) with pre-trained word embedding for text-based emotion recognition and convolutional neural network (CNN) with utterance-level descriptors for emotion recognition from speech. Various fusion strategies are adopted on these models to yield an overall score for each of the emotional categories. Intra-modality dynamics for each emotion is captured in the neural network designed for the specific modality. Fusion techniques are employed to obtain the inter-modality dynamics. Speaker and session-independent experiments on IEMOCAP multi-modal emotion detection dataset show the effectiveness of the proposed approaches. This method yields state-of-the-art results for utterance-level emotion recognition based on speech and text",
    "checked": true,
    "id": "7b43738839277ba7123e8df056df983b79c14530",
    "semantic_title": "fusion techniques for utterance-level emotion recognition combining speech and transcripts",
    "citation_count": 37
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rajwadi19_interspeech.html": {
    "title": "Explaining Sentiment Classification",
    "volume": "main",
    "abstract": "This paper presents a novel 1-D sentiment classifier trained on the benchmark IMDB dataset. The classifier is a 1-D convolutional neural network with repeated convolution and max pooling layers. The main contribution of this work is the demonstration of a deconvolution technique for 1-D convolutional neural networks that is agnostic to specific architecture types. This deconvolution technique enables text classification to be explained, a feature that is important for NLP-based decision support systems, as well as being an invaluable diagnostic tool",
    "checked": true,
    "id": "03849bdecfc720717d3953c8fe9f4ded437f1d1b",
    "semantic_title": "explaining sentiment classification",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kleinlein19_interspeech.html": {
    "title": "Predicting Group-Level Skin Attention to Short Movies from Audio-Based LSTM-Mixture of Experts Models",
    "volume": "main",
    "abstract": "Electrodermal activity (EDA) is a psychophysiological indicator that can be considered a somatic marker of the emotional and attentional reaction of subjects towards stimuli like audiovisual content. EDA measurements are not biased by the cognitive process of giving an opinion or a score to characterize the subjective perception, and group-level EDA recordings integrate the reaction of an audience, thus reducing the signal noise. This paper contributes to the field of audience's attention prediction to video content, extending previous novel work on the use of EDA as ground truth for prediction algorithms. Videos are segmented into shorter clips attending to the audience's increasing or decreasing attention, and we process videos' audio waveform to extract meaningful aural embeddings from a VGGish model pretrained on the Audioset database. While previous similar work on attention level prediction using only audio accomplished 69.83% accuracy, we propose a Mixture of Experts approach to train a binary classifier that outperforms the main existing state-of-the-art approaches predicting increasing and decreasing attention levels with 81.76% accuracy. These results confirm the usefulness of providing acoustic features with a semantic significance, and the convenience of considering experts over partitions of the dataset in order to predict group-level attention from audio",
    "checked": true,
    "id": "13c30a3e0888f4d1af79c05fddaae06895380a4f",
    "semantic_title": "predicting group-level skin attention to short movies from audio-based lstm-mixture of experts models",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schluter19_interspeech.html": {
    "title": "Survey Talk: Modeling in Automatic Speech Recognition: Beyond Hidden Markov Models",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "17e5123e8954a077ab2e2e933b812eec5878a0ce",
    "semantic_title": "survey talk: modeling in automatic speech recognition: beyond hidden markov models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pham19_interspeech.html": {
    "title": "Very Deep Self-Attention Networks for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "Recently, end-to-end sequence-to-sequence models for speech recognition have gained significant interest in the research community. While previous architecture choices revolve around time-delay neural networks (TDNN) and long short-term memory (LSTM) recurrent neural networks, we propose to use self-attention via the Transformer architecture as an alternative. Our analysis shows that deep Transformer networks with high learning capacity are able to exceed performance from previous end-to-end approaches and even match the conventional hybrid systems. Moreover, we trained very deep models with up to 48 Transformer layers for both encoder and decoders combined with stochastic residual connections, which greatly improve generalizability and training efficiency. The resulting models outperform all previous end-to-end ASR approaches on the Switchboard benchmark. An ensemble of these models achieve 9.9% and 17.7% WER on Switchboard and CallHome test sets respectively. This finding brings our end-to-end models to competitive levels with previous hybrid systems. Further, with model ensembling the Transformers can outperform certain hybrid systems, which are more complicated in terms of both structure and training procedure",
    "checked": true,
    "id": "f2bb7e2f5a1afad5370159c15760c44df93c0438",
    "semantic_title": "very deep self-attention networks for end-to-end speech recognition",
    "citation_count": 143
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19_interspeech.html": {
    "title": "Jasper: An End-to-End Convolutional Neural Acoustic Model",
    "volume": "main",
    "abstract": "In this paper we report state-of-the-art results on LibriSpeech among end-to-end speech recognition models without any external training data. Our model, Jasper, uses only 1D convolutions, batch normalization, ReLU, dropout, and residual connections. To improve training, we further introduce a new layer-wise optimizer called NovoGrad. Through experiments, we demonstrate that the proposed deep architecture performs as well or better than more complex choices. Our deepest Jasper variant uses 54 convolutional layers. With this architecture, we achieve 2.95% WER using a beam-search decoder with an external neural language model and 3.86% WER with a greedy decoder on LibriSpeech test-clean. We also report competitive results on Wall Street Journal and the Hub5'00 conversational evaluation datasets",
    "checked": true,
    "id": "d85b2af4f163383bbfa62b73d5f0b179868cc9a8",
    "semantic_title": "jasper: an end-to-end convolutional neural acoustic model",
    "citation_count": 203
  },
  "https://www.isca-speech.org/archive/interspeech_2019/moritz19_interspeech.html": {
    "title": "Unidirectional Neural Network Architectures for End-to-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "In hybrid automatic speech recognition (ASR) systems, neural networks are used as acoustic models (AMs) to recognize phonemes that are composed to words and sentences using pronunciation dictionaries, hidden Markov models, and language models, which can be jointly represented by a weighted finite state transducer (WFST). The importance of capturing temporal context by an AM has been studied and discussed in prior work. In an end-to-end ASR system, however, all components are merged into a single neural network, i.e., the breakdown into an AM and the different parts of the WFST model is no longer possible. This implies that end-to-end neural network architectures have even stronger requirements for processing long contextual information. Bidirectional long short-term memory (BLSTM) neural networks have demonstrated state-of-the-art results in end-to-end ASR but are unsuitable for streaming applications. Latency-controlled BLSTMs account for this by limiting the future context seen by the backward directed recurrence using chunk-wise processing. In this paper, we propose two new unidirectional neural network architectures, the time-delay LSTM (TDLSTM) and the parallel time-delayed LSTM (PTDLSTM) streams, which both limit the processing latency to a fixed size and demonstrate significant improvements compared to prior art on a variety of ASR tasks",
    "checked": true,
    "id": "6aed9eedd5cb712c3d902152cb94f2f18ba4c729",
    "semantic_title": "unidirectional neural network architectures for end-to-end automatic speech recognition",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/belinkov19_interspeech.html": {
    "title": "Analyzing Phonetic and Graphemic Representations in End-to-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end neural network systems for automatic speech recognition (ASR) are trained from acoustic features to text transcriptions. In contrast to modular ASR systems, which contain separately-trained components for acoustic modeling, pronunciation lexicon, and language modeling, the end-to-end paradigm is both conceptually simpler and has the potential benefit of training the entire system on the end task. However, such neural network models are more opaque: it is not clear how to interpret the role of different parts of the network and what information it learns during training. In this paper, we analyze the learned internal representations in an end-to-end ASR model. We evaluate the representation quality in terms of several classification tasks, comparing phonemes and graphemes, as well as different articulatory features. We study two languages (English and Arabic) and three datasets, finding remarkable consistency in how different properties are represented in different layers of the deep neural network",
    "checked": true,
    "id": "facccce4f8d059d6156cf3ce536786eb43016939",
    "semantic_title": "analyzing phonetic and graphemic representations in end-to-end automatic speech recognition",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tawara19_interspeech.html": {
    "title": "Multi-Channel Speech Enhancement Using Time-Domain Convolutional Denoising Autoencoder",
    "volume": "main",
    "abstract": "This paper investigates the use of time-domain convolutional denoising autoencoders (TCDAEs) with multiple channels as a method of speech enhancement. In general, denoising autoencoders (DAEs), deep learning systems that map noise-corrupted into clean waveforms, have been shown to generate high-quality signals while working in the time domain without the intermediate stage of phase modeling. Convolutional DAEs are one of the popular structures which learns a mapping between noise-corrupted and clean waveforms with convolutional denoising autoencoder. Multi-channel signals for TCDAEs are promising because the different times of arrival of a signal can be directly processed with their convolutional structure, Up to this time, TCDAEs have only been applied to single-channel signals. This paper explorers the effectiveness of TCDAEs in a multi-channel configuration. A multi-channel TCDAEs are evaluated on multi-channel speech enhancement experiments, yielding significant improvement over single-channel DAEs in terms of signal-to-distortion ratio, perceptual evaluation of speech quality (PESQ), and word error rate",
    "checked": true,
    "id": "69161fc1b301054f7b4423085530e2dc6083255d",
    "semantic_title": "multi-channel speech enhancement using time-domain convolutional denoising autoencoder",
    "citation_count": 29
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tesch19_interspeech.html": {
    "title": "On Nonlinear Spatial Filtering in Multichannel Speech Enhancement",
    "volume": "main",
    "abstract": "Using multiple microphones for speech enhancement allows for exploiting spatial information for improved performance. In most cases, the spatial filter is selected to be a linear function of the input as, for example, the minimum variance distortionless response (MVDR) beamformer. For non-Gaussian distributed noise, however, the minimum mean square error (MMSE) optimal spatial filter may be nonlinear Potentially, such nonlinear functional relationships could be learned by deep neural networks. However, the performance would depend on many parameters and the architecture of the neural network. Therefore, in this paper, we more generally analyze the potential benefit of nonlinear spatial filters as a function of the multivariate kurtosis of the noise distribution The results imply that using a nonlinear spatial filter is only worth the effort if the noise data follows a distribution with a multivariate kurtosis that is considerably higher than for a Gaussian. In this case, we report a performance difference of up to 2.6 dB segmental signal-to-noise ratio (SNR) improvement for artificial stationary noise. We observe an advantage of 1.2dB for the nonlinear spatial filter over the linear one even for real-world noise data from the CHiME-3 dataset given oracle data for parameter estimation",
    "checked": false,
    "id": "8945e5c963f77d663cf9832c31dcd2da0469ada9",
    "semantic_title": "nonlinear spatial filtering in multichannel speech enhancement",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/martindonas19_interspeech.html": {
    "title": "Multi-Channel Block-Online Source Extraction Based on Utterance Adaptation",
    "volume": "main",
    "abstract": "This paper deals with multi-channel speech recognition in scenarios with multiple speakers. Recently, the spectral characteristics of a target speaker, extracted from an adaptation utterance, have been used to guide a neural network mask estimator to focus on that speaker. In this work we present two variants of speaker-aware neural networks, which exploit both spectral and spatial information to allow better discrimination between target and interfering speakers. Thus, we introduce either a spatial pre-processing prior to the mask estimation or a spatial plus spectral speaker characterization block whose output is directly fed into the neural mask estimator. The target speaker's spectral and spatial signature is extracted from an adaptation utterance recorded at the beginning of a session. We further adapt the architecture for low-latency processing by means of block-online beamforming that recursively updates the signal statistics. Experimental results show that the additional spatial information clearly improves source extraction, in particular in the same-gender case, and that our proposal achieves state-of-the-art performance in terms of distortion reduction and recognition accuracy",
    "checked": true,
    "id": "396c65bbf4098d767586ea0210a8bfa0b829405b",
    "semantic_title": "multi-channel block-online source extraction based on utterance adaptation",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bagheri19_interspeech.html": {
    "title": "Exploiting Multi-Channel Speech Presence Probability in Parametric Multi-Channel Wiener Filter",
    "volume": "main",
    "abstract": "In this paper, we present a practical implementation of the parametric multi-channel Wiener filter (PMWF) noise reduction algorithm. In particular, we extend on methods that incorporate the multi-channel speech presence probability (MC-SPP) in the PMWF derivation and its output. The use of the MC-SPP brings several advantages. Firstly, the MC-SPP allows for better estimates of noise and speech statistics, for which we derive a direct update of the inverse of the noise power spectral density (PSD). Secondly, the MC-SPP is used to control the trade-off parameter in PMWF which, with proper tuning, outperforms the traditional approach with a fixed trade-off parameter. Thirdly, the MC-SPP for each frequency-band is used to obtain the MMSE estimate of the desired speech signal at the output, where we control the maximum amount of noise reduction based on our application. Experimental results on a large number of simulated scenarios show significant benefits of employing MC-SPP in terms of SNR improvements and speech distortion",
    "checked": true,
    "id": "7eb423a18481d37e7d766e9b083a190effecc6a9",
    "semantic_title": "exploiting multi-channel speech presence probability in parametric multi-channel wiener filter",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/togami19_interspeech.html": {
    "title": "Variational Bayesian Multi-Channel Speech Dereverberation Under Noisy Environments with Probabilistic Convolutive Transfer Function",
    "volume": "main",
    "abstract": "In this paper, we propose a multi-channel speech dereverberation method which can reduce reverberation even when acoustic transfer functions (ATFs) are time varying under noisy environments. The microphone input signal is modeled as a convolutive mixture in a time-frequency domain so as to incorporate late reverberation whose tap length is longer than frame size of short term Fourier transform. To reduce reverberation effectively under the time-varying ATF conditions, the proposed method extends the deterministic convolutive transfer function (D-CTF) into a probabilistic convolutive transfer function (P-CTF). A variational Bayesian framework was applied to approximation of a joint posterior probability density functions of a speech source signal and the ATFs. Variational posterior probability density functions and the other parameters are iteratively updated so as to maximize an evidence lower bound (ELBO). Experimental results when the ATFs are time-varying and there is background noise showed that the proposed method can reduce reverberation more accurately than the Weighted Prediction error (WPE) and the Kalman-EM for dereverberation (KEMD)",
    "checked": true,
    "id": "96273401679e1d2da953e3367712c11fd17ee7f1",
    "semantic_title": "variational bayesian multi-channel speech dereverberation under noisy environments with probabilistic convolutive transfer function",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nakatani19_interspeech.html": {
    "title": "Simultaneous Denoising and Dereverberation for Low-Latency Applications Using Frame-by-Frame Online Unified Convolutional Beamformer",
    "volume": "main",
    "abstract": "This article presents frame-by-frame online processing algorithms for a Weighted Power minimization Distortionless response convolutional beamformer (WPD). The WPD unifies widely-used multichannel dereverberation and denoising methods, namely a weighted prediction error based dereverberation method (WPE) and a minimum power distortionless response beamformer (MPDR) into a single convolutional beamformer, and achieves simultaneous dereverberation and denoising based on maximum likelihood estimation. We derive two different online algorithms, one based on frame-by-frame recursive updating of the spatio-temporal covariance matrix of the captured signal, and the other on recursive least square estimation of the convolutional beamformer. In addition, for both algorithms, the desired signal's relative transfer function (RTF) is estimated by online processing using a neural network based online mask estimation. Experiments using the REVERB challenge dataset show the effectiveness of both algorithms in terms of objective speech enhancement measures and automatic speech recognition (ASR) performance",
    "checked": true,
    "id": "460ecb228570a9fbbb12d0cdfa4670a0110e3870",
    "semantic_title": "simultaneous denoising and dereverberation for low-latency applications using frame-by-frame online unified convolutional beamformer",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2019/snyder19_interspeech.html": {
    "title": "Individual Variation in Cognitive Processing Style Predicts Differences in Phonetic Imitation of Device and Human Voices",
    "volume": "main",
    "abstract": "Phonetic imitation, or implicitly matching the acoustic-phonetic patterns of another speaker, has been empirically associated with natural tendencies to promote successful social communication, as well as individual differences in personality and cognitive processing style. The present study explores whether individual differences in cognitive processing style, as indexed by self-reported scored from the Autism-Spectrum Quotient (AQ) questionnaire, are linked to the way people imitate the vocal productions by two digital device voices (i.e., Apple's Siri) and two human voices. Subjects first performed a word shadowing task of human and device voices and then completed the self-administered AQ. We assessed imitation of two acoustic properties: f0 and vowel duration. We find that the attention to detail and the imagination subscale scores on the AQ mediated degree of imitation of f0 and vowel duration, respectively. The findings yield new insight to speech production and perception mechanisms and how it interacts with individual cognitive processing style differences",
    "checked": true,
    "id": "b3516010239a9e4fd3061581793a335fe4615682",
    "semantic_title": "individual variation in cognitive processing style predicts differences in phonetic imitation of device and human voices",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2019/illa19_interspeech.html": {
    "title": "An Investigation on Speaker Specific Articulatory Synthesis with Speaker Independent Articulatory Inversion",
    "volume": "main",
    "abstract": "Estimating speech representations from articulatory movements is known as articulatory-to-acoustic forward (AAF) mapping. Typically this mapping is learned using directly measured articulatory movement in a subject-specific manner. Such AAF mapping has been shown to benefit the speech synthesis applications. In this work, we investigate the speaker similarity and naturalness of utterances generated by AAF which is driven by the articulatory movements from a subject (referred to as cross speaker) different from the speaker (target speaker) used for training AAF mapping. Experiments are performed with directly measured articulatory data from 9 speakers (8 target speakers and 1 cross speaker), which are recorded using Electromagnetic articulograph AG501. Experiments are also performed with articulatory features estimated using speaker independent acoustic-to-articulatory inversion (SI-AAI) model trained on 26 reference speakers. Objective evaluation on target speakers reveal that the articulatory features estimated from SI-AAI result in a lower Mel-cepstrum distortion compared to that using directly measured articulatory features. Further, listening tests reveal that the directly measured articulatory movements preserve the speaker similarity better than estimated ones. Although, for naturalness, articulatory movements predicted by SI-AAI perform better than the direct measurements",
    "checked": true,
    "id": "9e3b686c1443d8e9d263de95cd46fcd9c825edd8",
    "semantic_title": "an investigation on speaker specific articulatory synthesis with speaker independent articulatory inversion",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19_interspeech.html": {
    "title": "Individual Difference of Relative Tongue Size and its Acoustic Effects",
    "volume": "main",
    "abstract": "This study examines how the speaker's tongue size contributes to generating dynamic characteristics of speaker individuality. The relative tongue size (RTS) has been proposed as an index for the tongue area within the oropharyngeal cavity on the midsagittal magnetic resonance imaging (MRI). Our earlier studies have shown that the smaller the RTS, the faster the tongue movement. In this study, acoustic consequences of individual RTS values were analyzed by comparing tongue movement velocity and formant transition rate. The materials used were cine-MRI data and acoustic signals during production of a sentence and two words produced by two female speakers with contrasting RTS values. The results indicate that the speaker with the small RTS value exhibited the faster changes of tongue positions and formant transitions than the speakers with the large RTS values. Since the tongue size is uncontrollable by a speaker's intention, the RTS can be regarded as one of the causal factors of dynamic individual characteristics in the lower frequency region of speech signals",
    "checked": true,
    "id": "68d0c775c0c976117da4b914321e89e5cfa9a717",
    "semantic_title": "individual difference of relative tongue size and its acoustic effects",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yoshinaga19_interspeech.html": {
    "title": "Individual Differences of Airflow and Sound Generation in the Vocal Tract of Sibilant /s/",
    "volume": "main",
    "abstract": "To clarify the individual differences of flow and sound characteristics of sibilant /s/, the large eddy simulation of compressible flow was applied to vocal tract geometries of five subjects pronouncing /s/. The vocal tract geometry was extracted by separately collecting images of digital dental casts and the vocal tract of /s/. The computational grids were constructed for each geometry, and flow and acoustic fields were predicted by the simulation. Results of the simulation showed that jet flow in the vocal tract was disturbed and fluctuated, and the sound source of /s/ was generated in different place for each subject. With an increment of the jet velocity, not only the overall sound amplitude but also the spectral mean was increased, indicating that the increment of the jet velocity contributes to the increase of amplitudes in a higher frequency range among different vocal tract geometries",
    "checked": true,
    "id": "7b273a1c00e585e0677ed99968cf1c18ecbe706f",
    "semantic_title": "individual differences of airflow and sound generation in the vocal tract of sibilant /s/",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/uttam19_interspeech.html": {
    "title": "Hush-Hush Speak: Speech Reconstruction Using Silent Videos",
    "volume": "main",
    "abstract": "Speech Reconstruction is the task of recreation of speech using silent videos as input. In the literature, it is also referred to as lipreading. In this paper, we design an encoder-decoder architecture which takes silent videos as input and outputs an audio spectrogram of the reconstructed speech. The model, despite being a speaker-independent model, achieves comparable results on speech reconstruction to the current state-of-the-art speaker-dependent model. We also perform user studies to infer speech intelligibility. Additionally, we test the usability of the trained model using bilingual speech",
    "checked": true,
    "id": "ec9fda5db1ced6058824ce0c8fe9cb6e1d20c24e",
    "semantic_title": "hush-hush speak: speech reconstruction using silent videos",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/saha19_interspeech.html": {
    "title": "SPEAK YOUR MIND! Towards Imagined Speech Recognition with Hierarchical Deep Learning",
    "volume": "main",
    "abstract": "Speech-related Brain Computer Interface (BCI) technologies provide effective vocal communication strategies for controlling devices through speech commands interpreted from brain signals. In order to infer imagined speech from active thoughts, we propose a novel hierarchical deep learning BCI system for subject-independent classification of 11 speech tokens including phonemes and words. Our novel approach exploits predicted articulatory information of six phonological categories (e.g., nasal, bilabial) as an intermediate step for classifying the phonemes and words, thereby finding discriminative signal responsible for natural speech synthesis. The proposed network is composed of hierarchical combination of spatial and temporal CNN cascaded with a deep autoencoder. Our best models on the KARA database achieve an average accuracy of 83.42% across the six different binary phonological classification tasks, and 53.36% for the individual token identification task, significantly outperforming our baselines. Ultimately, our work suggests the possible existence of a brain imagery footprint for the underlying articulatory movement related to different sounds that can be used to aid imagined speech decoding",
    "checked": true,
    "id": "fb2029d6587f3099b9b62b3abc601c64bd48fefc",
    "semantic_title": "speak your mind! towards imagined speech recognition with hierarchical deep learning",
    "citation_count": 31
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chung19_interspeech.html": {
    "title": "An Unsupervised Autoregressive Model for Speech Representation Learning",
    "volume": "main",
    "abstract": "This paper proposes a novel unsupervised autoregressive neural model for learning generic speech representations. In contrast to other speech representation learning methods that aim to remove noise or speaker variabilities, ours is designed to preserve information for a wide range of downstream tasks. In addition, the proposed model does not require any phonetic or word boundary labels, allowing the model to benefit from large quantities of unlabeled data. Speech representations learned by our model significantly improve performance on both phone classification and speaker verification over the surface features and other supervised and unsupervised approaches. Further analysis shows that different levels of speech information are captured by our model at different layers. In particular, the lower layers tend to be more discriminative for speakers, while the upper layers provide more phonetic content",
    "checked": true,
    "id": "2f803165d054ee89bec2401368ceb9e75bad8b60",
    "semantic_title": "an unsupervised autoregressive model for speech representation learning",
    "citation_count": 322
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19_interspeech.html": {
    "title": "Harmonic-Aligned Frame Mask Based on Non-Stationary Gabor Transform with Application to Content-Dependent Speaker Comparison",
    "volume": "main",
    "abstract": "We propose harmonic-aligned frame mask for speech signals using non-stationary Gabor transform (NSGT). A frame mask operates on the transfer coefficients of a signal and consequently converts the signal into a counterpart signal. It depicts the difference between the two signals. In preceding studies, frame masks based on regular Gabor transform were applied to single-note instrumental sound analysis. This study extends the frame mask approach to speech signals. For voiced speech, the fundamental frequency is usually changing consecutively over time. We employ NSGT with pitch-dependent and therefore time-varying frequency resolution to attain harmonic alignment in the transform domain and hence yield harmonic-aligned frame masks for speech signals. We propose to apply the harmonic-aligned frame mask to content-dependent speaker comparison. Frame masks, computed from voiced signals of a same vowel but from different speakers, were utilized as similarity measures to compare and distinguish the speaker identities (SID). Results obtained with deep neural networks demonstrate that the proposed frame mask is valid in representing speaker characteristics and shows a potential for SID applications in limited data scenarios",
    "checked": true,
    "id": "7cdcd85e2e9e44c5b1040079313d48886ce27553",
    "semantic_title": "harmonic-aligned frame mask based on non-stationary gabor transform with application to content-dependent speaker comparison",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/m19_interspeech.html": {
    "title": "Glottal Closure Instants Detection from Speech Signal by Deep Features Extracted from Raw Speech and Linear Prediction Residual",
    "volume": "main",
    "abstract": "Glottal closure instants (GCI) also called as instants of significant excitation occur during abrupt closure of vocal folds is a well-studied problem for its many potential applications in speech processing. Speech signal or its transformed linear prediction residual (LPR) is the most popular signal representations for GCI detection. In this paper, we propose a supervised classification based GCI detection method, in which, we train multiple convolution neural networks to determine the suitable feature representation for efficient GCI detection. Also, we show that the combined model trained with joint acoustic-residual deep features and the model trained with low pass filtered speech significantly increases the detection accuracy. We have manually annotated the speech signal for ground truth GCI using electroglottograph (EGG) as a reference signal. The evaluation results showed that the proposed model trained with very small and less diverse data performs significantly better than the traditional signal processing and most recent data-driven approaches",
    "checked": true,
    "id": "bf70044aefbf7c8433e93116f1ed9a4e85ca669c",
    "semantic_title": "glottal closure instants detection from speech signal by deep features extracted from raw speech and linear prediction residual",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pascual19_interspeech.html": {
    "title": "Learning Problem-Agnostic Speech Representations from Multiple Self-Supervised Tasks",
    "volume": "main",
    "abstract": "Learning good representations without supervision is still an open issue in machine learning, and is particularly challenging for speech signals, which are often characterized by long sequences with a complex hierarchical structure. Some recent works, however, have shown that it is possible to derive useful speech representations by employing a self-supervised encoder-discriminator approach. This paper proposes an improved self-supervised method, where a single neural encoder is followed by multiple workers that jointly solve different self-supervised tasks. The needed consensus across different tasks naturally imposes meaningful constraints to the encoder, contributing to discover general representations and to minimize the risk of learning superficial ones. Experiments show that the proposed approach can learn transferable, robust, and problem-agnostic features that carry on relevant information from the speech signal, such as speaker identity, phonemes, and even higher-level features such as emotional cues. In addition, a number of design choices make the encoder easily exportable, facilitating its direct usage or adaptation to different problems",
    "checked": true,
    "id": "b3beb9bc7395a8a489b9c64c46329a84d45968bd",
    "semantic_title": "learning problem-agnostic speech representations from multiple self-supervised tasks",
    "citation_count": 197
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nellore19_interspeech.html": {
    "title": "Excitation Source and Vocal Tract System Based Acoustic Features for Detection of Nasals in Continuous Speech",
    "volume": "main",
    "abstract": "The aim of the current study is to propose acoustic features for detection of nasals in continuous speech. Acoustic features that represent certain characteristics of speech production are extracted. Features representing excitation source characteristics are extracted using zero frequency filtering method. Features representing vocal tract system characteristics are extracted using zero time windowing method Feature sets are formed by combining certain subsets of the features mentioned above. These feature sets are evaluated for their representativeness of nasals in continuous speech in three different languages, namely, English, Hindi and Telugu. Results show that nasal detection is reliable and consistent across all the languages mentioned above",
    "checked": true,
    "id": "997642aa05d29b542130ef90653e306fae7c8260",
    "semantic_title": "excitation source and vocal tract system based acoustic features for detection of nasals in continuous speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chatziagapi19_interspeech.html": {
    "title": "Data Augmentation Using GANs for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "In this work, we address the problem of data imbalance for the task of Speech Emotion Recognition (SER). We investigate conditioned data augmentation using Generative Adversarial Networks (GANs), in order to generate samples for underrepresented emotions. We adapt and improve a conditional GAN architecture to generate synthetic spectrograms for the minority class. For comparison purposes, we implement a series of signal-based data augmentation methods. The proposed GAN-based approach is evaluated on two datasets, namely IEMOCAP and FEEL-25k, a large multi-domain dataset. Results demonstrate a 10% relative performance improvement in IEMOCAP and 5% in FEEL-25k, when augmenting the minority classes",
    "checked": true,
    "id": "f3c25b8aa1f7e2a00b947c38d52dcaa6b3da31bd",
    "semantic_title": "data augmentation using gans for speech emotion recognition",
    "citation_count": 82
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kons19_interspeech.html": {
    "title": "High Quality, Lightweight and Adaptable TTS Using LPCNet",
    "volume": "main",
    "abstract": "We present a lightweight adaptable neural TTS system with high quality output. The system is composed of three separate neural network blocks: prosody prediction, acoustic feature prediction and Linear Prediction Coding Net as a neural vocoder. This system can synthesize speech with close to natural quality while running 3 times faster than real-time on a standard CPU The modular setup of the system allows for simple adaptation to new voices with a small amount of data We first demonstrate the ability of the system to produce high quality speech when trained on large, high quality datasets. Following that, we demonstrate its adaptability by mimicking unseen voices using 5 to 20 minutes long datasets with lower recording quality. Large scale Mean Opinion Score quality and similarity tests are presented, showing that the system can adapt to unseen voices with quality gap of 0.12 and similarity gap of 3% compared to natural speech for male voices and quality gap of 0.35 and similarity of gap of 9% for female voices",
    "checked": true,
    "id": "52daae0ff7d09d4b11ab447fa0cb57e2ba1d12b6",
    "semantic_title": "high quality, lightweight and adaptable tts using lpcnet",
    "citation_count": 46
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lorenzotrueba19_interspeech.html": {
    "title": "Towards Achieving Robust Universal Neural Vocoding",
    "volume": "main",
    "abstract": "This paper explores the potential universality of neural vocoders. We train a WaveRNN-based vocoder on 74 speakers coming from 17 languages. This vocoder is shown to be capable of generating speech of consistently good quality (98% relative mean MUSHRA when compared to natural speech) regardless of whether the input spectrogram comes from a speaker or style seen during training or from an out-of-domain scenario when the recording conditions are studio-quality. When the recordings show significant changes in quality, or when moving towards non-speech vocalizations or singing, the vocoder still significantly outperforms speaker-dependent vocoders, but operates at a lower average relative MUSHRA of 75%. These results are shown to be consistent across languages, regardless of them being seen during training (e.g. English or Japanese) or unseen (e.g. Wolof, Swahili, Ahmaric)",
    "checked": true,
    "id": "93d857da76fdeeece7ed641f4d48e1e9770e8315",
    "semantic_title": "towards achieving robust universal neural vocoding",
    "citation_count": 78
  },
  "https://www.isca-speech.org/archive/interspeech_2019/neekhara19_interspeech.html": {
    "title": "Expediting TTS Synthesis with Adversarial Vocoding",
    "volume": "main",
    "abstract": "Recent approaches in text-to-speech (TTS) synthesis employ neural network strategies to vocode perceptually-informed spectrogram representations directly into listenable waveforms. Such vocoding procedures create a computational bottleneck in modern TTS pipelines. We propose an alternative approach which utilizes generative adversarial networks (GANs) to learn mappings from perceptually-informed spectrograms to simple magnitude spectrograms which can be heuristically vocoded. Through a user study, we show that our approach significantly outperforms naïve vocoding strategies while being hundreds of times faster than neural network vocoders used in state-of-the-art TTS systems. We also show that our method can be used to achieve state-of-the-art results in unsupervised synthesis of individual words of speech",
    "checked": true,
    "id": "3b978703968c2e3f8a41b0d34f870bfc2228677f",
    "semantic_title": "expediting tts synthesis with adversarial vocoding",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mustafa19_interspeech.html": {
    "title": "Analysis by Adversarial Synthesis — A Novel Approach for Speech Vocoding",
    "volume": "main",
    "abstract": "Classical parametric speech coding techniques provide a compact representation for speech signals. This affords a very low transmission rate but with a reduced perceptual quality of the reconstructed signals. Recently, autoregressive deep generative models such as WaveNet and SampleRNN have been used as speech vocoders to scale up the perceptual quality of the reconstructed signals without increasing the coding rate. However, such models suffer from a very slow signal generation mechanism due to their sample-by-sample modelling approach. In this work, we introduce a new methodology for neural speech vocoding based on generative adversarial networks (GANs). A fake speech signal is generated from a very compressed representation of the glottal excitation using conditional GANs as a deep generative model. This fake speech is then refined using the LPC parameters of the original speech signal to obtain a natural reconstruction. The reconstructed speech waveforms based on this approach show a higher perceptual quality than the classical vocoder counterparts according to subjective and objective evaluation scores for a dataset of 30 male and female speakers. Moreover, the usage of GANs enables to generate signals in one-shot compared to autoregressive generative models. This makes GANs promising for exploration to implement high-quality neural vocoders",
    "checked": false,
    "id": "07a9dc45559f8c4cc2a8740e36e18f29db8e7a3d",
    "semantic_title": "analysis by adversarial synthesis - a novel approach for speech vocoding",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wu19b_interspeech.html": {
    "title": "Quasi-Periodic WaveNet Vocoder: A Pitch Dependent Dilated Convolution Model for Parametric Speech Generation",
    "volume": "main",
    "abstract": "In this paper, we propose a quasi-periodic neural network (QPNet) vocoder with a novel network architecture named pitch-dependent dilated convolution (PDCNN) to improve the pitch controllability of WaveNet (WN) vocoder. The effectiveness of the WN vocoder to generate high-fidelity speech samples from given acoustic features has been proved recently. However, because of the fixed dilated convolution and generic network architecture, the WN vocoder hardly generates speech with given F values which are outside the range observed in training data. Consequently, the WN vocoder lacks the pitch controllability which is one of the essential capabilities of conventional vocoders. To address this limitation, we propose the PDCNN component which has the time-variant adaptive dilation size related to the given F values and a cascade network structure of the QPNet vocoder to generate quasi-periodic signals such as speech. Both objective and subjective tests are conducted, and the experimental results demonstrate the better pitch controllability of the QPNet vocoder compared to the same and double sized WN vocoders while attaining comparable speech qualities",
    "checked": true,
    "id": "ff01789535aa1535f610739176083f17c6239d2f",
    "semantic_title": "quasi-periodic wavenet vocoder: a pitch dependent dilated convolution model for parametric speech generation",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tian19_interspeech.html": {
    "title": "A Speaker-Dependent WaveNet for Voice Conversion with Non-Parallel Data",
    "volume": "main",
    "abstract": "In a typical voice conversion system, vocoder is commonly used for speech-to-features analysis and features-to-speech synthesis. However, vocoder can be a source of speech quality degradation. This paper presents a novel approach to voice conversion using WaveNet for non-parallel training data. Instead of reconstructing speech with intermediate features, the proposed approach utilizes the WaveNet to map the Phonetic PosteriorGrams (PPGs) to the waveform samples directly. In this way, we avoid the estimation errors arising from vocoding and feature conversion. Additionally, as PPG is assumed to be speaker independent, the proposed approach also reduces the feature mismatch problem in WaveNet vocoder based solutions. Experimental results conducted on the CMU-ARCTIC database show that the proposed approach significantly outperforms the traditional vocoder and WaveNet Vocoder baselines in terms of speech quality",
    "checked": true,
    "id": "b8360a4ec8d54f5bbc4b8081730a47b9c7fe026e",
    "semantic_title": "a speaker-dependent wavenet for voice conversion with non-parallel data",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2019/han19_interspeech.html": {
    "title": "Survey Talk: When Attention Meets Speech Applications: Speech & Speaker Recognition Perspective",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "984a38f397b3211963b36e2049b945afaadaebbf",
    "semantic_title": "survey talk: when attention meets speech applications: speech & speaker recognition perspective",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhao19_interspeech.html": {
    "title": "Attention-Enhanced Connectionist Temporal Classification for Discrete Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Discrete speech emotion recognition (SER), the assignment of a single emotion label to an entire speech utterance, is typically performed as a sequence-to-label task. This approach, however, is limited, in that it can result in models that do not capture temporal changes in the speech signal, including those indicative of a particular emotion. One potential solution to overcome this limitation is to model SER as a sequence-to-sequence task instead. In this regard, we have developed an attention-based bidirectional long short-term memory (BLSTM) neural network in combination with a connectionist temporal classification (CTC) objective function (Attention-BLSTM-CTC) for SER. We also assessed the benefits of incorporating two contemporary attention mechanisms, namely component attention and quantum attention, into the CTC framework. To the best of the authors' knowledge, this is the first time that such a hybrid architecture has been employed for SER.We demonstrated the effectiveness of our approach on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) and FAU-Aibo Emotion corpora. The experimental results demonstrate that our proposed model outperforms current state-of-the-art approaches",
    "checked": true,
    "id": "920f779bef257922d7685244f1a7afc1e4d6ad86",
    "semantic_title": "attention-enhanced connectionist temporal classification for discrete speech emotion recognition",
    "citation_count": 45
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19b_interspeech.html": {
    "title": "Attentive to Individual: A Multimodal Emotion Recognition Network with Personalized Attention Profile",
    "volume": "main",
    "abstract": "A growing number of human-centered applications benefit from continuous advancements in the emotion recognition technology. Many emotion recognition algorithms have been designed to model multimodal behavior cues to achieve high performances. However, most of them do not consider the modulating factors of an individual's personal attributes in his/her expressive behaviors. In this work, we propose a Personalized Attributes-Aware Attention Network (PAaAN) with a novel personalized attention mechanism to perform emotion recognition using speech and language cues. The attention profile is learned from embeddings of an individual's profile, acoustic, and lexical behavior data. The profile embedding is derived using linguistics inquiry word count computed between the target speaker and a large set of movie scripts. Our method achieves the state-of-the-art 70.3% unweighted accuracy in a four class emotion recognition task on the IEMOCAP. Further analysis reveals that affect-related semantic categories are emphasized differently for each speaker in the corpus showing the effectiveness of our attention mechanism for personalization",
    "checked": true,
    "id": "1515d8597ef03ff52cda5bc3551fbdee6350c2bd",
    "semantic_title": "attentive to individual: a multimodal emotion recognition network with personalized attention profile",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gallardoantolin19_interspeech.html": {
    "title": "A Saliency-Based Attention LSTM Model for Cognitive Load Classification from Speech",
    "volume": "main",
    "abstract": "Cognitive Load (CL) refers to the amount of mental demand that a given task imposes on an individual's cognitive system and it can affect his/her productivity in very high load situations. In this paper, we propose an automatic system capable of classifying the CL level of a speaker by analyzing his/her voice. Our research on this topic goes into two main directions. In the first one, we focus on the use of Long Short-Term Memory (LSTM) networks with different weighted pooling strategies for CL level classification. In the second contribution, for overcoming the need of a large amount of training data, we propose a novel attention mechanism that uses the Kalinli's auditory saliency model. Experiments show that our proposal outperforms significantly both, a baseline system based on Support Vector Machines (SVM) and a LSTM-based system with logistic regression attention model",
    "checked": true,
    "id": "af4f0d9acf3841703855891c12ce24f6b41da608",
    "semantic_title": "a saliency-based attention lstm model for cognitive load classification from speech",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mallolragolta19_interspeech.html": {
    "title": "A Hierarchical Attention Network-Based Approach for Depression Detection from Transcribed Clinical Interviews",
    "volume": "main",
    "abstract": "The high prevalence of depression in society has given rise to a need for new digital tools that can aid its early detection. Among other effects, depression impacts the use of language. Seeking to exploit this, this work focuses on the detection of depressed and non-depressed individuals through the analysis of linguistic information extracted from transcripts of clinical interviews with a virtual agent. Specifically, we investigated the advantages of employing hierarchical attention-based networks for this task. Using Global Vectors (GloVe) pretrained word embedding models to extract low-level representations of the words, we compared hierarchical local-global attention networks and hierarchical contextual attention networks. We performed our experiments on the Distress Analysis Interview Corpus - Wizard of Oz (DAIC-WoZ) dataset, which contains audio, visual, and linguistic information acquired from participants during a clinical session. Our results using the DAIC-WoZ test set indicate that hierarchical contextual attention networks are the most suitable configuration to detect depression from transcripts. The configuration achieves an Unweighted Average Recall (UAR) of .66 using the test set, surpassing our baseline, a Recurrent Neural Network that does not use attention",
    "checked": true,
    "id": "8db761dc173e30b0882390892fe92af7acd11208",
    "semantic_title": "a hierarchical attention network-based approach for depression detection from transcribed clinical interviews",
    "citation_count": 30
  },
  "https://www.isca-speech.org/archive/interspeech_2019/carmantini19_interspeech.html": {
    "title": "Untranscribed Web Audio for Low Resource Speech Recognition",
    "volume": "main",
    "abstract": "Speech recognition models are highly susceptible to mismatch in the acoustic and language domains between the training and the evaluation data. For low resource languages, it is difficult to obtain transcribed speech for target domains, while untranscribed data can be collected with minimal effort. Recently, a method applying lattice-free maximum mutual information (LF-MMI) to untranscribed data has been found to be effective for semi-supervised training. However, weaker initial models and domain mismatch can result in high deletion rates for the semi-supervised model. Therefore, we propose a method to force the base model to overgenerate possible transcriptions, relying on the ability of LF-MMI to deal with uncertainty. On data from the IARPA MATERIAL programme, our new semi-supervised method outperforms the standard semi-supervised method, yielding significant gains when adapting for mismatched bandwidth and domain",
    "checked": true,
    "id": "07a873b8a6c00dc72978ef7e98160b3e245c4bca",
    "semantic_title": "untranscribed web audio for low resource speech recognition",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/luscher19_interspeech.html": {
    "title": "RWTH ASR Systems for LibriSpeech: Hybrid vs Attention",
    "volume": "main",
    "abstract": "We present state-of-the-art automatic speech recognition (ASR) systems employing a standard hybrid DNN/HMM architecture compared to an attention-based encoder-decoder design for the LibriSpeech task. Detailed descriptions of the system development, including model design, pretraining schemes, training schedules, and optimization approaches are provided for both system architectures. Both hybrid DNN/HMM and attention-based systems employ bi-directional LSTMs for acoustic modeling/encoding. For language modeling, we employ both LSTM and Transformer based architectures. All our systems are built using RWTH's open-source toolkits RASR and RETURNN. To the best knowledge of the authors, the results obtained when training on the full LibriSpeech training set, are the best published currently, both for the hybrid DNN/HMM and the attention-based systems. Our single hybrid system even outperforms previous results obtained from combining eight single systems. Our comparison shows that on the LibriSpeech 960h task, the hybrid DNN/HMM system outperforms the attention-based system by 15% relative on the clean and 40% relative on the other test sets in terms of word error rate. Moreover, experiments on a reduced 100h-subset of the LibriSpeech training corpus even show a more pronounced margin between the hybrid DNN/HMM and attention-based architectures",
    "checked": false,
    "id": "744196b6cb5091c0760d05ef068a92a6cd531587",
    "semantic_title": "rwth asr systems for librispeech: hybrid vs attention - w/o data augmentation",
    "citation_count": 211
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kanda19_interspeech.html": {
    "title": "Auxiliary Interference Speaker Loss for Target-Speaker Speech Recognition",
    "volume": "main",
    "abstract": "In this paper, we propose a novel auxiliary loss function for target-speaker automatic speech recognition (ASR). Our method automatically extracts and transcribes target speaker's utterances from a monaural mixture of multiple speakers speech given a short sample of the target speaker. The proposed auxiliary loss function attempts to additionally maximize interference speaker ASR accuracy during training. This will regularize the network to achieve a better representation for speaker separation, thus achieving better accuracy on the target-speaker ASR. We evaluated our proposed method using two-speaker-mixed speech in various signal-to-interference-ratio conditions. We first built a strong target-speaker ASR baseline based on the state-of-the-art lattice-free maximum mutual information. This baseline achieved a word error rate (WER) of 18.06% on the test set while a normal ASR trained with clean data produced a completely corrupted result (WER of 84.71%). Then, our proposed loss further reduced the WER by 6.6% relative to this strong baseline, achieving a WER of 16.87%. In addition to the accuracy improvement, we also showed that the auxiliary output branch for the proposed loss can even be used for a secondary ASR for interference speakers' speech",
    "checked": true,
    "id": "388d41b99c9c0867301f345c65877a2796225ead",
    "semantic_title": "auxiliary interference speaker loss for target-speaker speech recognition",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2019/meng19_interspeech.html": {
    "title": "Speaker Adaptation for Attention-Based End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "We propose three regularization-based speaker adaptation approaches to adapt the attention-based encoder-decoder (AED) model with very limited adaptation data from target speakers for end-to-end automatic speech recognition. The first method is Kullback-Leibler divergence (KLD) regularization, in which the output distribution of a speaker-dependent (SD) AED is forced to be close to that of the speaker-independent (SI) model by adding a KLD regularization to the adaptation criterion. To compensate for the asymmetric deficiency in KLD regularization, an adversarial speaker adaptation (ASA) method is proposed to regularize the deep-feature distribution of the SD AED through the adversarial learning of an auxiliary discriminator and the SD AED. The third approach is the multi-task learning, in which an SD AED is trained to jointly perform the primary task of predicting a large number of output units and an auxiliary task of predicting a small number of output units to alleviate the target sparsity issue. Evaluated on a Microsoft short message dictation task, all three methods are highly effective in adapting the AED model, achieving up to 12.2% and 3.0% word error rate improvement over an SI AED trained from 3400 hours data for supervised and unsupervised adaptation, respectively",
    "checked": true,
    "id": "ce77b9212751e1afd10c7fccd5271a806dfbb445",
    "semantic_title": "speaker adaptation for attention-based end-to-end speech recognition",
    "citation_count": 29
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19_interspeech.html": {
    "title": "Large Margin Training for Attention Based End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end speech recognition systems are typically evaluated using the maximum a posterior criterion. Since only one hypothesis is involved during evaluation, the ideal number of hypotheses for training should also be one. In this study, we propose a large margin training scheme for attention based end-to-end speech recognition. Using only one training hypothesis, the large margin training strategy achieves the same performance as the minimum word error rate criterion using four hypotheses. The theoretical derivation in this study is widely applicable to other sequence discriminative criteria such as maximum mutual information. In addition, this paper provides a more succinct formulation of the large margin concept, paving the road towards a better combination of support vector machine and deep neural network",
    "checked": true,
    "id": "72abad6cd58731dafa8e7e35b2ce7192f8f6fc72",
    "semantic_title": "large margin training for attention based end-to-end speech recognition",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mac19_interspeech.html": {
    "title": "Large-Scale Mixed-Bandwidth Deep Neural Network Acoustic Modeling for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "In automatic speech recognition (ASR), wideband (WB) and narrowband (NB) speech signals with different sampling rates typically use separate acoustic models. Therefore mixed-bandwidth (MB) acoustic modeling has important practical values for ASR system deployment. In this paper, we extensively investigate large-scale MB deep neural network acoustic modeling for ASR using 1,150 hours of WB data and 2,300 hours of NB data. We study various MB strategies including downsampling, upsampling and bandwidth extension for MB acoustic modeling and evaluate their performance on 8 diverse WB and NB test sets from various application domains. To deal with the large amounts of training data, distributed training is carried out on multiple GPUs using synchronous data parallelism",
    "checked": true,
    "id": "2804c86dc045206d9759f4f9813e5b66cdbb2771",
    "semantic_title": "large-scale mixed-bandwidth deep neural network acoustic modeling for automatic speech recognition",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/milde19_interspeech.html": {
    "title": "SparseSpeech: Unsupervised Acoustic Unit Discovery with Memory-Augmented Sequence Autoencoders",
    "volume": "main",
    "abstract": "We propose a sparse sequence autoencoder model for unsupervised acoustic unit discovery, based on bidirectional LSTM encoders/decoders with a sparsity-inducing bottleneck. The sparsity layer is based on memory-augmented neural networks, with a differentiable embedding memory bank addressed from the encoder. The decoder reconstructs the encoded input feature sequence from an utterance-level context embedding and the bottleneck representation. At some time steps, the input to the decoder is randomly omitted by applying sequence dropout, forcing the decoder to learn about the temporal structure of the sequence. We propose a bootstrapping training procedure, after which the network can be trained end-to-end with standard back-propagation. Sparsity of the generated representation can be controlled with a parameter in the proposed loss function. We evaluate the units with the ABX discriminability on minimal triphone pairs and also on entire words. Forcing the network to favor highly sparse memory addressings in the memory component yields symbolic-like representations of speech that are very compact and still offer better ABX discriminability than MFCC",
    "checked": true,
    "id": "a98badb3e7503d7be09c01a43f85429505d4c907",
    "semantic_title": "sparsespeech: unsupervised acoustic unit discovery with memory-augmented sequence autoencoders",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ondel19_interspeech.html": {
    "title": "Bayesian Subspace Hidden Markov Model for Acoustic Unit Discovery",
    "volume": "main",
    "abstract": "This work tackles the problem of learning a set of language specific acoustic units from unlabeled speech recordings given a set of labeled recordings from other languages. Our approach may be described by the following two steps procedure: first the model learns the notion of acoustic units from the labelled data and then the model uses its knowledge to find new acoustic units on the target language. We implement this process with the Bayesian Subspace Hidden Markov Model (SHMM), a model akin to the Subspace Gaussian Mixture Model (SGMM) where each low dimensional embedding represents an acoustic unit rather than just a HMM's state. The subspace is trained on 3 languages from the GlobalPhone corpus (German, Polish and Spanish) and the AUs are discovered on the TIMIT corpus. Results, measured in equivalent Phone Error Rate, show that this approach significantly outperforms previous HMM based acoustic units discovery systems and compares favorably with the Variational Auto Encoder-HMM",
    "checked": true,
    "id": "57d1734db27c6ce1aae420d56182a4dab9f4fa5c",
    "semantic_title": "bayesian subspace hidden markov model for acoustic unit discovery",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2019/higuchi19_interspeech.html": {
    "title": "Speaker Adversarial Training of DPGMM-Based Feature Extractor for Zero-Resource Languages",
    "volume": "main",
    "abstract": "We propose a novel framework for extracting speaker-invariant features for zero-resource languages. A deep neural network (DNN)-based acoustic model is normalized against speakers via adversarial training: a multi-task learning process trains a shared bottleneck feature to be discriminative to phonemes and independent of speakers. However, owing to the absence of phoneme labels, zero-resource languages cannot employ adversarial multi-task (AMT) learning for speaker normalization. In this work, we obtain a posteriorgram from a Dirichlet process Gaussian mixture model (DPGMM) and utilize the posterior vector for supervision of the phoneme estimation in the AMT training. The AMT network is designed so that the DPGMM posteriorgram itself is embedded in a speaker-invariant feature space. The proposed network is expected to resolve the potential problem that the posteriorgram may lack reliability as a phoneme representation if the DPGMM components are intermingled with phoneme and speaker information. Based on the Zero Resource Speech Challenges, we conduct phoneme discriminant experiments on the extracted features. The results of the experiments show that the proposed framework extracts discriminative features, suppressing the variety in speakers",
    "checked": true,
    "id": "a62a5322c25a9074fd2499c88a46690afd176755",
    "semantic_title": "speaker adversarial training of dpgmm-based feature extractor for zero-resource languages",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/prasad19_interspeech.html": {
    "title": "Building Large-Vocabulary ASR Systems for Languages Without Any Audio Training Data",
    "volume": "main",
    "abstract": "When building automatic speech recognition (ASR) systems, typically some amount of audio and text data in the target language is needed. While text data can be obtained relatively easily across many languages, transcribed audio data is challenging to obtain. This presents a barrier to making voice technologies available in more languages of the world. In this paper, we present a way to build an ASR system system for a language even in the absence of any audio training data in that language at all. We do this by simply re-using an existing acoustic model from a phonologically similar language, without any kind of modification or adaptation towards the target language. The basic insight is that, if two languages are sufficiently similar in terms of their phonological system, an acoustic model should hold up relatively well when used for another language. We describe how we tailor our pronunciation models to enable such re-use, and show experimental results across a number of languages from various language families. We also provide a theoretical analysis of situations in which this approach is likely to work. Our results show that it is possible to achieve less than 20% word error rate (WER) using this method",
    "checked": true,
    "id": "6a3ea42e8cd381e3ed95a0d4d965409172728aa1",
    "semantic_title": "building large-vocabulary asr systems for languages without any audio training data",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2019/azuh19_interspeech.html": {
    "title": "Towards Bilingual Lexicon Discovery From Visually Grounded Speech Audio",
    "volume": "main",
    "abstract": "In this paper, we present a method for the discovery of word-like units and their approximate translations from visually grounded speech across multiple languages. We first train a neural network model to map images and their spoken audio captions in both English and Hindi to a shared, multimodal embedding space. Next, we use this model to segment and cluster regions of the spoken captions which approximately correspond to words. Finally, we exploit between-cluster similarities in the embedding space to associate English pseudo-word clusters with Hindi pseudo-word clusters, and show that many of these cluster pairings capture semantic translations between English and Hindi words. We present quantitative cross-lingual clustering results, as well as qualitative results in the form of a bilingual picture dictionary",
    "checked": true,
    "id": "7ab9392167bdbaa272c95178d50fb08bcfba7148",
    "semantic_title": "towards bilingual lexicon discovery from visually grounded speech audio",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/feng19_interspeech.html": {
    "title": "Improving Unsupervised Subword Modeling via Disentangled Speech Representation Learning and Transformation",
    "volume": "main",
    "abstract": "This study tackles unsupervised subword modeling in the zero-resource scenario, learning frame-level speech representation that is phonetically discriminative and speaker-invariant, using only untranscribed speech for target languages. Frame label acquisition is an essential step in solving this problem. High quality frame labels should be in good consistency with golden transcriptions and robust to speaker variation. We propose to improve frame label acquisition in our previously adopted deep neural network-bottleneck feature (DNN-BNF) architecture by applying the factorized hierarchical variational autoencoder (FHVAE). FHVAEs learn to disentangle linguistic content and speaker identity information encoded in speech. By discarding or unifying speaker information, speaker-invariant features are learned and fed as inputs to DPGMM frame clustering and DNN-BNF training. Experiments conducted on ZeroSpeech 2017 show that our proposed approaches achieve 2.4% and 0.6% absolute ABX error rate reductions in across- and within-speaker conditions, comparing to the baseline DNN-BNF system without applying FHVAEs. Our proposed approaches significantly outperform vocal tract length normalization in improving frame labeling and subword modeling",
    "checked": true,
    "id": "3bad6b7a7d17eaa0027bee97d6b92cbcb40a33d1",
    "semantic_title": "improving unsupervised subword modeling via disentangled speech representation learning and transformation",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nissen19_interspeech.html": {
    "title": "Listeners' Ability to Identify the Gender of Preadolescent Children in Different Linguistic Contexts",
    "volume": "main",
    "abstract": "This study evaluated listeners' ability to identify the gender of preadolescent children from speech samples of varying length and linguistic context. The listeners were presented with a total of 190 speech samples in four different categories of linguistic context: segments, words, sentences, and discourse. The listeners were instructed to evaluate each speech sample and decide whether the speaker was a male or female and rate their level of confidence in their decision. Results showed listeners identified the gender of the speakers with a high degree of accuracy, ranging from 86% to 95%. Significant differences in listener judgments were found across the four levels of linguistic context, with segments having the lowest accuracy (83%) and discourse the highest accuracy (99%). At the segmental level, the listeners' identification of each speaker's gender was greater for vowels than for fricatives, with both types of phoneme being identified at a rate well above chance. Significant differences in identification were found between the /s/ and /ʃ/ fricatives, but not between the four corner vowels. The perception of gender is likely multifactorial, with listeners possibly using phonetic, prosodic, or stylistic speech cues to determine a speaker's gender",
    "checked": true,
    "id": "f32d178eaffa86fefa8ea014fb67e5ca1b521184",
    "semantic_title": "listeners' ability to identify the gender of preadolescent children in different linguistic contexts",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ahlers19_interspeech.html": {
    "title": "Sibilant Variation in New Englishes: A Comparative Sociophonetic Study of Trinidadian and American English /s(tr)/-Retraction",
    "volume": "main",
    "abstract": "The retraction of /s/, particularly in /str/ clusters, toward [ʃ] has been investigated in British, Australian, and American English and shown to be conditioned phonetically and sociolinguistically. To date, however, no research exists on the retraction of /s/ in New Englishes, the nativized Englishes spoken in postcolonial territories like the Caribbean. We take up this research gap and present the results of a large-scale comparative acoustic analysis of /s/-retraction in Trinidadian English (TrinE) and American English (AmE), using Center of Gravity measurements of more than 23,500 sibilants produced by 181 speakers from two speech corpora The results show that, in TrinE, /str/ is considerably retracted toward [ʃtɹ], while all other /sC(r)/ clusters are non-retracted and acoustically close to singleton /s/; less retracted realizations of /str/ occur across word boundaries. Although a statistically significant contrast is overall maintained between /ʃ/ and the sibilant in /str/, there is considerable overlap across many speakers. The comparison between TrinE and AmE indicates that, while sibilants in TrinE overall show acoustically lower values, both varieties have in common that retraction is limited to /str/ contexts and significantly larger in younger speakers. The degree of /str/-retraction, however, is overall larger in TrinE than AmE",
    "checked": true,
    "id": "2ff5f37edd3cec2c2ec4405cb345ee89bf34027a",
    "semantic_title": "sibilant variation in new englishes: a comparative sociophonetic study of trinidadian and american english /s(tr)/-retraction",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gubian19_interspeech.html": {
    "title": "Tracking the New Zealand English NEAR/SQUARE Merger Using Functional Principal Components Analysis",
    "volume": "main",
    "abstract": "The focus of the study is the application of functional principal components analysis (FPCA) to a sound change in progress in which the square and near falling diphthongs are merging in New Zealand English. FPCA approximated the trajectory shapes of the first two formant frequencies (F1/F2) in a large acoustic database of read New Zealand English speech spanning three different age groups and two regions. The derived FPCA parameters showed a greater degree of centralisation and monophthongisation in square than in near. Compatibly with the evidence of an ongoing sound change in which square is shifting towards near, these shape differences were more marked for older than for younger/mid-age speakers. There was no effect of region nor of place of articulation of the preceding consonant; there was a trend for the merger to be more advanced in low frequency words. The study underlines the benefits of FPCA for quantifying the many types of sound changes involving subtle shifts in speech dynamics. In particular, multi-dimensional trajectory shape differences can be quantified without the need for vowel targets nor for determining the influence of the parameters — in this case of the first two formant frequencies — independently of each other",
    "checked": true,
    "id": "731f20b2a2efc4f4ea684a387f972a33390237d4",
    "semantic_title": "tracking the new zealand english near/square merger using functional principal components analysis",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gessinger19_interspeech.html": {
    "title": "Phonetic Accommodation in a Wizard-of-Oz Experiment: Intonation and Segments",
    "volume": "main",
    "abstract": "This paper discusses phonetic accommodation of 20 native German speakers interacting with the simulated spoken dialogue system Mirabella in a Wizard-of-Oz experiment. The study examines intonation of wh-questions and pronunciation of allophonic contrasts in German. In a question-and-answer exchange with the system, the users produce predominantly falling intonation patterns for wh-questions when the system does so as well. The number of rising patterns on the part of the users increases significantly when Mirabella produces questions with rising intonation. In a map task, Mirabella provides information about hidden items while producing variants of two allophonic contrasts which are dispreferred by the users. For the [ɪç] vs. [ɪk] contrast in the suffix ⟨-ig⟩, the number of dispreferred variants on the part of the users increases significantly during the map task. For the [εː] vs. [eː] contrast as a realization of stressed ⟨-ä-⟩, such a convergence effect is not found on the group level, yet still occurs for some individual users. Almost every user converges to the system to a substantial degree for a subset of the examined features, but we also find maintenance of preferred variants and even occasional divergence. This individual variation is in line with previous findings in accommodation research",
    "checked": true,
    "id": "3cc2acd1c36c2a4a4482cfb215f63a3cbfd374cc",
    "semantic_title": "phonetic accommodation in a wizard-of-oz experiment: intonation and segments",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/niebuhr19b_interspeech.html": {
    "title": "PASCAL and DPA: A Pilot Study on Using Prosodic Competence Scores to Predict Communicative Skills for Team Working and Public Speaking",
    "volume": "main",
    "abstract": "Strong communication skills in public-speaking and team-working exercises are associated with specific acoustic-prosodic profiles and strategies. We hypothesize that analyzing and assessing these profiles and strategies allows us to predict communicative skills. To that end, we used two analysis methods, one for charismatic and persuasive public speaking (PASCAL), and one for cooperative communication (DPA). PASCAL and DPA competency scores are determined on an acoustic basis for speech recordings of 21 students whose task was to co-create, in 7 teams of 3 students, a fully functioning weather station over 14 weeks in an Electrical Engineering project course — and to jointly write a development report about it afterwards. Results show that the students' PASCAL scores are significantly correlated with both the grade in their final oral project presentation and the grade of their written report as assessed by an independent lecturer group. The DPA scores correlate with better time-management and team working as well as with the quality and functionality of the designed product. Explanations for the links between student performance and acoustic competence scores are discussed",
    "checked": true,
    "id": "b6db8d80d41454d6a26e0ab861ea960fe73c8f51",
    "semantic_title": "pascal and dpa: a pilot study on using prosodic competence scores to predict communicative skills for team working and public speaking",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/michalsky19_interspeech.html": {
    "title": "Towards the Prosody of Persuasion in Competitive Negotiation. The Relationship Between f0 and Negotiation Success in Same Sex Sales Tasks",
    "volume": "main",
    "abstract": "Prosodic features play a key role in a speaker's persuasive power. However, previous studies on persuasion have been focused on public speaking and the signaling of leadership, while acoustic studies on negotiation have been primarily concerned with cooperative interactions. In this study we are taking a first step into investigating the role of acoustic-prosodic cues in competitive negotiation, focusing on f0 in same-sex negotiations. Specifically, we ask whether the prosodic correlates of persuasive speech are comparable for public speaking and negotiation. Sixty-two speakers (44f/18m) in 31 same-sex pairs participated in a competitive task to bargain over the selling price of a fictional company. We find a significant correlation between a speaker's f0 features and his/her interlocutor's concession range. In line with findings from public speaking, greater f0 excursions and higher f0 minima correlate with negotiation success. However, while the female speakers also show an expected elevated f0 mean, the opposite is the case for male speakers. We propose that in competitive negotiation, displaying dominance may overrule showing passion in contrast to public speaking, but only for male speakers",
    "checked": true,
    "id": "2a75a198f1b396effd78e8bcccd78c75dc62c5c4",
    "semantic_title": "towards the prosody of persuasion in competitive negotiation. the relationship between f0 and negotiation success in same sex sales tasks",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sager19_interspeech.html": {
    "title": "VESUS: A Crowd-Annotated Database to Study Emotion Production and Perception in Spoken English",
    "volume": "main",
    "abstract": "We introduce the Varied Emotion in Syntactically Uniform Speech (VESUS) repository as a new resource for the speech community. VESUS is a lexically controlled database, in which a semantically neutral script is portrayed with different emotional inflections. In total, VESUS contains over 250 distinct phrases, each read by ten actors in five emotional states. We use crowd sourcing to obtain ten human ratings for the perceived emotional content of each utterance. Our unique database construction enables a multitude of scientific and technical explorations. To jumpstart this effort, we provide benchmark performance on three distinct emotion recognition tasks using VESUS: longitudinal speaker analysis, extrapolating across syntactical complexity, and generalization to a new speaker",
    "checked": true,
    "id": "88fbd781267892f6c50d9bcdb5cfdfe7558fac9f",
    "semantic_title": "vesus: a crowd-annotated database to study emotion production and perception in spoken english",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2019/koh19_interspeech.html": {
    "title": "Building the Singapore English National Speech Corpus",
    "volume": "main",
    "abstract": "The National Speech Corpus (NSC) is the first large-scale Singapore English corpus spearheaded by the Info-communications and Media Development Authority of Singapore. It aims to become an important source of open speech data for automatic speech recognition (ASR) research and speech-related applications. The first release of the corpus features more than 2000 hours of orthographically transcribed read speech data designed with the inclusion of locally relevant words. It is available for public and commercial use upon request at \"www.imda.gov.sg/nationalspeechcorpus\", under the Singapore Open Data License. An accompanying lexicon is currently in the works and will be published soon. In addition, another 1000 hours of conversational speech data will be made available in the near future under the second release of NSC. This paper reports on the development and collection process of the read speech and conversational speech corpora",
    "checked": true,
    "id": "5d86886822404c7398edfad18b01b922eabccec2",
    "semantic_title": "building the singapore english national speech corpus",
    "citation_count": 28
  },
  "https://www.isca-speech.org/archive/interspeech_2019/picheny19_interspeech.html": {
    "title": "Challenging the Boundaries of Speech Recognition: The MALACH Corpus",
    "volume": "main",
    "abstract": "There has been huge progress in speech recognition over the last several years. Tasks once thought extremely difficult, such as SWITCHBOARD, now approach levels of human performance. The MALACH corpus (LDC catalog LDC2012S05), a 375-Hour subset of a large archive of Holocaust testimonies collected by the Survivors of the Shoah Visual History Foundation, presents significant challenges to the speech community. The collection consists of unconstrained, natural speech filled with disfluencies, heavy accents, age-related coarticulations, un-cued speaker and language switching, and emotional speech - all still open problems for speech recognition systems. Transcription is challenging even for skilled human annotators. This paper proposes that the community place focus on the MALACH corpus to develop speech recognition systems that are more robust with respect to accents, disfluencies and emotional speech. To reduce the barrier for entry, a lexicon and training and testing setups have been created and baseline results using current deep learning technologies are presented. The metadata has just been released by LDC (LDC2019S11). It is hoped that this resource will enable the community to build on top of these baselines so that the extremely important information in these and related oral histories becomes accessible to a wider audience",
    "checked": true,
    "id": "8bcbcaa3f507d1d3c26f9a378a7c1d488caacf11",
    "semantic_title": "challenging the boundaries of speech recognition: the malach corpus",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ramteke19_interspeech.html": {
    "title": "NITK Kids' Speech Corpus",
    "volume": "main",
    "abstract": "This paper introduces speech database for analyzing children's speech. The proposed database of children is recorded in Kannada language (one of the South Indian languages) from children between age 2.5 to 6.5 years. The database is named as National Institute of Technology Karnataka Kids' Speech Corpus (NITK Kids' Speech Corpus). The relevant design considerations for the database collection are discussed in detail. It is divided into four age groups with an interval of 1 year between each age group. The speech corpus includes nearly 10 hours of speech recordings from 160 children. For each age range, the data is recorded from 40 children (20 male and 20 female). Further, the effect of developmental changes on the speech from 2.5 to 6.5 years are analyzed using pitch and formant analysis. Some of the potential applications, of the NITK Kids' Speech Corpus, such as, systematic study on the language learning ability of children, phonological process analysis and children speech recognition are discussed",
    "checked": true,
    "id": "057e9df4772bc756051edaad4bcfa1068908e493",
    "semantic_title": "nitk kids' speech corpus",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ali19_interspeech.html": {
    "title": "Towards Variability Resistant Dialectal Speech Evaluation",
    "volume": "main",
    "abstract": "We study the problem of evaluating automatic speech recognition (ASR) systems that target dialectal speech input. A major challenge in this case is that the orthography of dialects is typically not standardized. From an ASR evaluation perspective, this means that there is no clear gold standard for the expected output, and several possible outputs could be considered correct according to different human annotators, which makes standard word error rate (WER) inadequate as an evaluation metric. Specifically targeting the case of Arabic dialects, which are also morphologically rich and complex, we propose a number of alternative WER-based metrics that vary in terms of text representation, including different degrees of morphological abstraction and spelling normalization.We evaluate the efficacy of these metrics by comparing their correlation with human judgments on a validation set of 1,000 utterances. Our results show that the use of morphological abstractions and spelling normalization produces systems with higher correlation with human judgment. We released the code and the datasets to the research community",
    "checked": true,
    "id": "438a0502b893be669eb80e4043bbed52835a0843",
    "semantic_title": "towards variability resistant dialectal speech evaluation",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fallgren19_interspeech.html": {
    "title": "How to Annotate 100 Hours in 45 Minutes",
    "volume": "main",
    "abstract": "Speech data found in the wild hold many advantages over artificially constructed speech corpora in terms of ecological validity and cultural worth. Perhaps most importantly, there is a lot of it. However, the combination of great quantity, noisiness and variation poses a challenge for its access and processing. Generally speaking, automatic approaches to tackle the problem require good labels for training, while manual approaches require time. In this study, we provide further evidence for a semi-supervised, human-in-the-loop framework that previously has shown promising results for browsing and annotating large quantities of found audio data quickly. The findings of this study show that a 100-hour long subset of the Fearless Steps corpus can be annotated for speech activity in less than 45 minutes, a fraction of the time it would take traditional annotation methods, without a loss in performance",
    "checked": true,
    "id": "7eae889eed59ac7813412a96b00e6a9990c8c47d",
    "semantic_title": "how to annotate 100 hours in 45 minutes",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/diez19_interspeech.html": {
    "title": "Bayesian HMM Based x-Vector Clustering for Speaker Diarization",
    "volume": "main",
    "abstract": "This paper presents a simplified version of the previously proposed diarization algorithm based on Bayesian Hidden Markov Models, which uses Variational Bayesian inference for very fast and robust clustering of x-vector (neural network based speaker embeddings). The presented results show that this clustering algorithm provides significant improvements in diarization performance as compared to the previously used Agglomerative Hierarchical Clustering. The output of this system can be further employed as an initialization for a second stage VB diarization system, using frame-wise MFCC features as input, to obtain optimal results",
    "checked": true,
    "id": "55277df8e04cc75d46470318d9ffbffe365527ee",
    "semantic_title": "bayesian hmm based x-vector clustering for speaker diarization",
    "citation_count": 60
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vestman19_interspeech.html": {
    "title": "Unleashing the Unused Potential of i-Vectors Enabled by GPU Acceleration",
    "volume": "main",
    "abstract": "Speaker embeddings are continuous-value vector representations that allow easy comparison between voices of speakers with simple geometric operations. Among others, i-vector and x-vector have emerged as the mainstream methods for speaker embedding. In this paper, we illustrate the use of modern computation platform to harness the benefit of GPU acceleration for i-vector extraction. In particular, we achieve an acceleration of 3000 times in frame posterior computation compared to real time and 25 times in training the i-vector extractor compared to the CPU baseline from Kaldi toolkit. This significant speed-up allows the exploration of ideas that were hitherto impossible. In particular, we show that it is beneficial to update the universal background model (UBM) and re-compute frame alignments while training the i-vector extractor. Additionally, we are able to study different variations of i-vector extractors more rigorously than before. In this process, we reveal some undocumented details of Kaldi's i-vector extractor and show that it outperforms the standard formulation by a margin of 1 to 2% when tested with VoxCeleb speaker verification protocol. All of our findings are asserted by ensemble averaging the results from multiple runs with random start",
    "checked": true,
    "id": "cccc0dc0167dc5a919922d5fb44c431c545e9e1f",
    "semantic_title": "unleashing the unused potential of i-vectors enabled by gpu acceleration",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shon19_interspeech.html": {
    "title": "MCE 2018: The 1st Multi-Target Speaker Detection and Identification Challenge Evaluation",
    "volume": "main",
    "abstract": "The Multi-target Challenge aims to assess how well current speech technology is able to determine whether or not a recorded utterance was spoken by one of a large number of blacklisted speakers. It is a form of multi-target speaker detection based on real-world telephone conversations. Data recordings are generated from call center customer-agent conversations. The task is to measure how accurately one can detect 1) whether a test recording is spoken by a blacklisted speaker, and 2) which specific blacklisted speaker was talking. This paper outlines the challenge and provides its baselines, results, and discussions",
    "checked": true,
    "id": "b3918fab36f106e83e016a3e33d260ad656191c4",
    "semantic_title": "mce 2018: the 1st multi-target speaker detection and identification challenge evaluation",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gao19_interspeech.html": {
    "title": "Improving Aggregation and Loss Function for Better Embedding Learning in End-to-End Speaker Verification System",
    "volume": "main",
    "abstract": "Deep embedding learning based speaker verification (SV) methods have recently achieved significant performance improvement over traditional i-vector systems, especially for short duration utterances. Embedding learning commonly consists of three components: frame-level feature processing, utterance-level embedding learning, and loss function to discriminate between speakers. For the learned embeddings, a back-end model (i.e., Linear Discriminant Analysis followed by Probabilistic Linear Discriminant Analysis (LDA-PLDA)) is generally applied as a similarity measure. In this paper, we propose to further improve the effectiveness of deep embedding learning methods in the following components: (1) A multi-stage aggregation strategy, exploited to hierarchically fuse time-frequency context information for effective frame-level feature processing. (2) A discriminant analysis loss is designed for end-to-end training, which aims to explicitly learn the discriminative embeddings, i.e. with small intra-speaker and large inter-speaker variances. To evaluate the effectiveness of the proposed improvements, we conduct extensive experiments on the VoxCeleb1 dataset. The results outperform state-of-the-art systems by a significant margin. It is also worth noting that the results are obtained using a simple cosine metric instead of the more complex LDA-PLDA backend scoring",
    "checked": true,
    "id": "547e7c3a786e4880f50f4650f1c9b08c69b08253",
    "semantic_title": "improving aggregation and loss function for better embedding learning in end-to-end speaker verification system",
    "citation_count": 54
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lin19_interspeech.html": {
    "title": "LSTM Based Similarity Measurement with Spectral Clustering for Speaker Diarization",
    "volume": "main",
    "abstract": "More and more neural network approaches have achieved considerable improvement upon submodules of speaker diarization system, including speaker change detection and segment-wise speaker embedding extraction. Still, in the clustering stage, traditional algorithms like probabilistic linear discriminant analysis (PLDA) are widely used for scoring the similarity between two speech segments. In this paper, we propose a supervised method to measure the similarity matrix between all segments of an audio recording with sequential bidirectional long short-term memory networks (Bi-LSTM). Spectral clustering is applied on top of the similarity matrix to further improve the performance. Experimental results show that our system significantly outperforms the state-of-the-art methods and achieves a diarization error rate of 6.63% on the NIST SRE 2000 CALLHOME database",
    "checked": true,
    "id": "d900a0d828cef4ea7da7082e970a7fd4119f86aa",
    "semantic_title": "lstm based similarity measurement with spectral clustering for speaker diarization",
    "citation_count": 68
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chung19b_interspeech.html": {
    "title": "Who Said That?: Audio-Visual Speaker Diarisation of Real-World Meetings",
    "volume": "main",
    "abstract": "The goal of this work is to determine ‘who spoke when' in real-world meetings. The method takes surround-view video and single or multi-channel audio as inputs, and generates robust diarisation outputs To achieve this, we propose a novel iterative approach that first enrolls speaker models using audio-visual correspondence, then uses the enrolled models together with the visual information to determine the active speaker We show strong quantitative and qualitative performance on a dataset of real-world meetings. The method is also evaluated on the public AMI meeting corpus, on which we demonstrate results that exceed all comparable methods. We also show that beamforming can be used together with the video to further improve the performance when multi-channel audio is available",
    "checked": true,
    "id": "0b04f4b0a6caeca85282c5e3baa5f24706c0cbe3",
    "semantic_title": "who said that?: audio-visual speaker diarisation of real-world meetings",
    "citation_count": 27
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xie19_interspeech.html": {
    "title": "Multi-PLDA Diarization on Children's Speech",
    "volume": "main",
    "abstract": "Children's speech and other vocalizations pose challenges for speaker diarization. The spontaneity of kids causes rapid or delayed phonetic variations in an utterance, which makes speaker's information difficult to extract. Fast speaker turns and long overlap in conversations between children and their guardians makes correct segmentation even harder compared to, say a business meeting. In this work, we explore diarization of child-guardian interactions. We investigate the effectiveness of adding children's speech to adult data in Probabilistic Linear Discriminant Analysis (PLDA) training. We also train each of two PLDAs with separate objective to a coarse or fine classification of speakers. A fusion of the two PLDAs is examined. By performing this fusion, we expect to improve on children's speech while preserving adult segmentations. Our experimental results show that including children's speech helps reduce DER by 2.7%, achieving a best overall DER of 33.1% with the x-vector system. A fusion system yields a reasonable 33.3% DER that validates our concept",
    "checked": true,
    "id": "c04f91ed76aa8f6410312229520383db361b2a4f",
    "semantic_title": "multi-plda diarization on children's speech",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mccree19_interspeech.html": {
    "title": "Speaker Diarization Using Leave-One-Out Gaussian PLDA Clustering of DNN Embeddings",
    "volume": "main",
    "abstract": "Many modern systems for speaker diarization, such as the top-performing JHU system in the DIHARD 2018 challenge, rely on clustering of DNN speaker embeddings followed by HMM resegmentation. Two problems with this approach are that parameters need significant retuning for different applications, and that the DNN contributes only to the clustering task and not the resegmentation. This paper presents two contributions: an improved HMM segment assignment algorithm using leave-one-out Gaussian PLDA scoring, and an approach to training the DNN such that embeddings directly optimize performance of this scoring method with generatively updated PLDA parameters. Initial experiments with this new system are very promising, achieving state-of-the-art performance for two separate tasks (Callhome and DIHARD18) without any task-dependent parameter tuning",
    "checked": true,
    "id": "224d6aef7d6522f9c97bd8cea764704653df0192",
    "semantic_title": "speaker diarization using leave-one-out gaussian plda clustering of dnn embeddings",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ghahabi19_interspeech.html": {
    "title": "Speaker-Corrupted Embeddings for Online Speaker Diarization",
    "volume": "main",
    "abstract": "Speaker diarization is more challenging in presence of background noise or music, frequent speaker changes, and cross talks. In an online scenario, the decision should be made at time, given only the current short segment and the speakers detected in the past, which makes the task even harder. In this work, an online robust speaker diarization algorithm is proposed in which speech segments are represented by low dimensional vectors referred to as speaker-corrupted embeddings. The proposed speaker embedding network is a deep neural network which takes speaker-corrupted supervectors as input, uses variable ReLU (VReLU) as an activation function, and tries to discriminate the background speakers. Speaker corruption is performed by adding supervectors built by 20 speech frames from other speakers to the supervectors of a given speaker. It is shown that speaker corruption, VReLU, and input dropout increase the generalization power of the proposed network. To increase the robustness, the proposed embeddings are concatenated with LDA transformed supervectors. Experimental results on the Albayzin 2018 evaluation set show a competitive accuracy, more robustness, and much lower computational cost compared to typical offline algorithms",
    "checked": true,
    "id": "f110ea60287aaa90a86c7a256361bad209b44416",
    "semantic_title": "speaker-corrupted embeddings for online speaker diarization",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/park19_interspeech.html": {
    "title": "Speaker Diarization with Lexical Information",
    "volume": "main",
    "abstract": "This work presents a novel approach for speaker diarization to leverage lexical information provided by automatic speech recognition. We propose a speaker diarization system that can incorporate word-level speaker turn probabilities with speaker embeddings into a speaker clustering process to improve the overall diarization accuracy. To integrate lexical and acoustic information in a comprehensive way during clustering, we introduce an adjacency matrix integration for spectral clustering. Since words and word boundary information for word-level speaker turn probability estimation are provided by a speech recognition system, our proposed method works without any human intervention for manual transcriptions. We show that the proposed method improves diarization performance on various evaluation datasets compared to the baseline diarization system using acoustic information only in speaker embeddings",
    "checked": true,
    "id": "828444dfe221ac7ce5c0d1a5d7a7db7d1d78b7ce",
    "semantic_title": "speaker diarization with lexical information",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shafey19_interspeech.html": {
    "title": "Joint Speech Recognition and Speaker Diarization via Sequence Transduction",
    "volume": "main",
    "abstract": "Speech applications dealing with conversations require not only recognizing the spoken words, but also determining who spoke when. The task of assigning words to speakers is typically addressed by merging the outputs of two separate systems, namely, an automatic speech recognition (ASR) system and a speaker diarization (SD) system. The two systems are trained independently with different objective functions. Often the SD systems operate directly on the acoustics and are not constrained to respect word boundaries and this deficiency is overcome in an ad hoc manner. Motivated by recent advances in sequence to sequence learning, we propose a novel approach to tackle the two tasks by a joint ASR and SD system using a recurrent neural network transducer. Our approach utilizes both linguistic and acoustic cues to infer speaker roles, as opposed to typical SD systems, which only use acoustic cues. We evaluated the performance of our approach on a large corpus of medical conversations between physicians and patients. Compared to a competitive conventional baseline, our approach improves word-level diarization error rate from 15.8% to 2.2%",
    "checked": true,
    "id": "6632853535bd7f7f9c438d19467341f6e46a63e5",
    "semantic_title": "joint speech recognition and speaker diarization via sequence transduction",
    "citation_count": 71
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cumani19_interspeech.html": {
    "title": "Normal Variance-Mean Mixtures for Unsupervised Score Calibration",
    "volume": "main",
    "abstract": "Generative calibration models have shown to be an effective alternative to traditional discriminative score calibration techniques, such as Logistic Regression (LogReg). Provided that the score distribution assumptions are sufficiently accurate, generative approaches not only have similar or better performance with respect to LogReg, but also allow for unsupervised or semi-supervised training Recently, we have proposed non-Gaussian linear calibration models able to overcome the limitations of Gaussian approaches. Although these models allow for better characterization of score distributions, they still require the target and non-target distributions to be reciprocally symmetric In this work we further extend these models to cover asymmetric score distributions, as to improve calibration for both supervised and unsupervised scenarios. The improvements have been assessed on NIST SRE 2010 telephone data",
    "checked": true,
    "id": "364ce471c1cddcc930c6e1a88696851ae05db1c0",
    "semantic_title": "normal variance-mean mixtures for unsupervised score calibration",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yamamoto19_interspeech.html": {
    "title": "Speaker Augmentation and Bandwidth Extension for Deep Speaker Embedding",
    "volume": "main",
    "abstract": "This paper investigates a novel data augmentation approach to train deep neural networks (DNNs) used for speaker embedding, i.e. to extract representation that allows easy comparison between speaker voices with a simple geometric operation. Data augmentation is used to create new examples from an existing training set, thereby increasing the quantity of training data improves the robustness of the model. We attempt to increase the number of speakers in the training set by generating new speakers via voice conversion. This speaker augmentation expands the coverage of speakers in the embedding space in contrast to conventional audio augmentation methods which focus on within-speaker variability. With an increased number of speakers in the training set, the DNN is trained to produce a better speaker-discriminative embedding. We also advocate using bandwidth extension to augment narrowband speech for a wideband application. Text-independent speaker recognition experiments in Speakers in the Wild (SITW) demonstrate a 17.9% reduction in minimum detection cost with speaker augmentation. The combined use of the two techniques provides further improvement",
    "checked": true,
    "id": "2bda55920cdef57fa4cd3829a98d178c611a8871",
    "semantic_title": "speaker augmentation and bandwidth extension for deep speaker embedding",
    "citation_count": 49
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ylmaz19_interspeech.html": {
    "title": "Large-Scale Speaker Diarization of Radio Broadcast Archives",
    "volume": "main",
    "abstract": "This paper describes our initial efforts to build a large-scale speaker diarization (SD) and identification system on a recently digitized radio broadcast archive from the Netherlands which has more than 6500 audio tapes with 3000 hours of Frisian-Dutch speech recorded between 1950–2016. The employed large-scale diarization scheme involves two stages: (1) tape-level speaker diarization providing pseudo-speaker identities and (2) speaker linking to relate pseudo-speakers appearing in multiple tapes. Having access to the speaker models of several frequently appearing speakers from the previously collected FAME! speech corpus, we further perform speaker identification by linking these known speakers to the pseudo-speakers identified at the first stage. In this work, we present a recently created longitudinal and multilingual SD corpus designed for large-scale SD research and evaluate the performance of a new speaker linking system using x-vectors with PLDA to quantify cross-tape speaker similarity on this corpus. The performance of this speaker linking system is evaluated on a small subset of the archive which is manually annotated with speaker information. The speaker linking performance reported on this subset (53 hours) and the whole archive (3000 hours) is compared to quantify the impact of scaling up in the amount of speech data",
    "checked": true,
    "id": "8518d62a10a49cb180a1586d13c3973a36bdf81b",
    "semantic_title": "large-scale speaker diarization of radio broadcast archives",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dubey19_interspeech.html": {
    "title": "Toeplitz Inverse Covariance Based Robust Speaker Clustering for Naturalistic Audio Streams",
    "volume": "main",
    "abstract": "Speaker diarization determines who spoke and when? in an audio stream. In this study, we propose a model-based approach for robust speaker clustering using i-vectors. The i-vectors extracted from different segments of same speaker are correlated. We model this correlation with a Markov Random Field (MRF) network. Leveraging the advancements in MRF modeling, we used Toeplitz Inverse Covariance (TIC) matrix to represent the MRF correlation network for each speaker. This approaches captures the sequential structure of i-vectors (or equivalent speaker turns) belonging to same speaker in an audio stream. A variant of standard Expectation Maximization (EM) algorithm is adopted for deriving closed-form solution using dynamic programming (DP) and the alternating direction method of multiplier (ADMM). Our diarization system has four steps: (1) ground-truth segmentation; (2) i-vector extraction; (3) post-processing (mean subtraction, principal component analysis, and length-normalization) ; and (4) proposed speaker clustering. We employ cosine K-means and movMF speaker clustering as baseline approaches. Our evaluation data is derived from: (i) CRSS-PLTL corpus, and (ii) two meetings subset of the AMI corpus. Relative reduction in diarization error rate (DER) for CRSS-PLTL corpus is 43.22% using the proposed advancements as compared to baseline. For AMI meetings IS1000a and IS1003b, relative DER reduction is 29.37% and 9.21%, respectively",
    "checked": true,
    "id": "7342cbe19f130ca0de319dca36d75648cec2ad70",
    "semantic_title": "toeplitz inverse covariance based robust speaker clustering for naturalistic audio streams",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kovacs19_interspeech.html": {
    "title": "Examining the Combination of Multi-Band Processing and Channel Dropout for Robust Speech Recognition",
    "volume": "main",
    "abstract": "A pivotal question in Automatic Speech Recognition (ASR) is the robustness of the trained models. In this study, we investigate the combination of two methods commonly applied to increase the robustness of ASR systems. On the one hand, inspired by auditory experiments and signal processing considerations, multi-band band processing has been used for decades to improve the noise robustness of speech recognition. On the other hand, dropout is a commonly used regularization technique to prevent overfitting by keeping the model from becoming over-reliant on a small set of neurons. We hypothesize that the careful combination of the two approaches would lead to increased robustness, by preventing the resulting model from over-rely on any given band To verify our hypothesis, we investigate various approaches for the combination of the two methods using the Aurora-4 corpus. The results obtained corroborate our initial assumption, and show that the proper combination of the two techniques leads to increased robustness, and to significantly lower word error rates (WERs). Furthermore, we find that the accuracy scores attained here compare favourably to those reported recently on the clean training scenario of the Aurora-4 corpus",
    "checked": true,
    "id": "9f5b8831cd40e6987a70016d107c928ba508dc07",
    "semantic_title": "examining the combination of multi-band processing and channel dropout for robust speech recognition",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/soni19_interspeech.html": {
    "title": "Label Driven Time-Frequency Masking for Robust Continuous Speech Recognition",
    "volume": "main",
    "abstract": "The application of Time-Frequency (T-F) masking based approaches for Automatic Speech Recognition has been shown to provide significant gains in system performance in the presence of additive noise. Such approaches give performance improvement when the T-F masking front-end is trained jointly with the acoustic model. However, such systems still rely on a pre-trained T-F masking enhancement block, trained using pairs of clean and noisy speech signals. Pre-training is necessary due to large number of parameters associated with the enhancement network. In this paper, we propose a flat-start joint training of a network that has both a T-F masking based enhancement block and a phoneme classification block. In particular, we use fully convolutional network as an enhancement front-end to reduce the number of parameters. We train the network by jointly updating the parameters of both these blocks using tied Context-Dependent phoneme states as targets. We observe that pretraining of the proposed enhancement block is not necessary for the convergence. In fact, the proposed flat-start joint training converges faster than the baseline multi-condition trained model. The experiments performed on Aurora-4 database show 7.06% relative improvement over multi-conditioned baseline. We get similar improvements for unseen test conditions as well",
    "checked": true,
    "id": "8bfe5bb970fc99148f1c8c6b666312fd05a12f21",
    "semantic_title": "label driven time-frequency masking for robust continuous speech recognition",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wu19c_interspeech.html": {
    "title": "Speaker-Invariant Feature-Mapping for Distant Speech Recognition via Adversarial Teacher-Student Learning",
    "volume": "main",
    "abstract": "Feature mapping (FM) jointly trained with acoustic model (AFM) is commonly used for single-channel speech enhancement. However, the performance is affected by the inter-speaker variability. In this paper, we propose speaker-invariant AFM (SIAFM) aiming at curtailing the inter-talker variability while achieving speech enhancement. In SIAFM, a feature-mapping network, an acoustic model and a speaker classifier network are jointly optimized to minimize the feature-mapping loss and the senone classification loss, and simultaneously min-maximize the speaker classification loss. Evaluated on AMI dataset, the proposed SIAFM achieves 4.8% and 7.0% relative word error rate (WER) reduction on the overlapped and non-overlapped condition over the baseline acoustic model trained with single distant microphone (SDM) data. Additionally, the SIAFM obtains 3.0% relative overlapped WER and 4.2% relative non-overlapped WER decrease over the multi-conditional (MCT) acoustic model. To further promote the performance of SIAFM, we employ teacher-student learning (TS), in which the posterior probabilities generated by the individual headset microphone (IHM) data can be used in lieu of labels to train the SIAFM model. The experiments show that compared with MCT model, SIAFM with TS (SIAFM-TS) can reach 4.2% relative overlapped WER and 6.3% relative non-overlapped WER decrease respectively",
    "checked": true,
    "id": "a84b392aeb4bd31da086baf4a88484d550e1023c",
    "semantic_title": "speaker-invariant feature-mapping for distant speech recognition via adversarial teacher-student learning",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ming19_interspeech.html": {
    "title": "Full-Sentence Correlation: A Method to Handle Unpredictable Noise for Robust Speech Recognition",
    "volume": "main",
    "abstract": "We describe the theory and implementation of full-sentence speech correlation for speech recognition, and demonstrate its superior robustness to unseen/untrained noise. For the Aurora 2 data, trained with only clean speech, the new method performs competitively against the state-of-the-art with multicondition training and adaptation, and achieves the lowest word error rate in very low SNR (-5 dB). Further experiments with highly nonstationary noise (pop song, broadcast news, etc.) show the surprising ability of the new method to handle unpredictable noise. The new method adds several novel developments to our previous research, including the modeling of the speaker characteristics along with other acoustic and semantic features of speech for separating speech from noise, and a novel Viterbi algorithm to implement full-sentence correlation for speech recognition",
    "checked": true,
    "id": "a34eae29fb8a199ba961d39fdb6b64c154df27bb",
    "semantic_title": "full-sentence correlation: a method to handle unpredictable noise for robust speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/soni19b_interspeech.html": {
    "title": "Generative Noise Modeling and Channel Simulation for Robust Speech Recognition in Unseen Conditions",
    "volume": "main",
    "abstract": "Multi-conditioned training is a state-of-the-art approach to achieve robustness in Automatic Speech Recognition (ASR) systems. This approach works well in practice for seen degradation conditions. However, the performance of such system is still an issue for unseen degradation conditions. In this work we consider distortions due to additive noise and channel mismatch. To achieve the robustness to additive noise, we propose a parametric generative model for noise signals. By changing the parameters of the proposed generative model, various noise signals can be generated and used to develop a multi-conditioned dataset for ASR system training. The generative model is designed to span the feature space of Mel Filterbank Energies by using band-limited white noise signals as basis. To simulate channel distortions, we propose to shift the mean of log spectral magnitude using utterances with estimated channel distortions. Experiments performed on the Aurora 4 noisy speech database show that using noise types generated from the proposed generative model for multi-conditioned training provides significant performance gain for additive noise in unseen conditions. We compare our results with those from multi-conditioning by various real noise databases including environmental and other real life noises",
    "checked": true,
    "id": "4ca5ede1da48cf0649a8cef7dc2269812177cb3e",
    "semantic_title": "generative noise modeling and channel simulation for robust speech recognition in unseen conditions",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kumar19_interspeech.html": {
    "title": "Far-Field Speech Enhancement Using Heteroscedastic Autoencoder for Improved Speech Recognition",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) systems trained on clean speech do not perform well in far-field scenario. Degradation in word error rate (WER) can be as large as 40% in this mismatched scenario. Typically, speech enhancement is applied to map speech from far-field condition to clean condition using a neural network, commonly known as denoising autoencoder (DA). Such speech enhancement technique has shown significant improvement in ASR accuracy. It is a common practice to use mean-square error (MSE) loss to train DA which is based on regression model with residual noise modeled by zero-mean and constant co-variance Gaussian distribution. However, both these assumptions are not optimal, especially in highly non-stationary noisy and far-field scenario. Here, we propose a more generalized loss based on non-zero mean and heteroscedastic co-variance distribution for the residual variables. On the top, we present several novel DA architectures that are more suitable for the heteroscedastic loss. It is shown that the proposed methods outperform the conventional DA and MSE loss by a large margin. We observe relative improvement of 7.31% in WER compared to conventional DA and overall, a relative improvement of 14.4% compared to mismatched train and test scenario",
    "checked": true,
    "id": "8827899043786dee7563d4f93005c2b519fee75e",
    "semantic_title": "far-field speech enhancement using heteroscedastic autoencoder for improved speech recognition",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/delcroix19_interspeech.html": {
    "title": "End-to-End SpeakerBeam for Single Channel Target Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end (E2E) automatic speech recognition (ASR) that directly maps a sequence of speech features into a sequence of characters using a single neural network has received a lot of attention as it greatly simplifies the training and decoding pipelines and enables optimizing the whole system E2E. Recently, such systems have been extended to recognize speech mixtures by inserting a speech separation mechanism into the neural network, allowing to output recognition results for each speaker in the mixture. However, speech separation suffers from a global permutation ambiguity issue, i.e. arbitrary mapping between source speakers and outputs. We argue that this ambiguity would seriously limit the practical use of E2E separation systems. SpeakerBeam has been proposed as an alternative to speech separation to mitigate the global permutation ambiguity. SpeakerBeam aims at extracting only a target speaker in a mixture based on his/her speech characteristics, thus avoiding the global permutation problem. In this paper, we combine SpeakerBeam and an E2E ASR system to allow E2E training of a target speech recognition system. We show promising target speech recognition results in mixtures of two speakers, and discuss interesting properties of the proposed system in terms of speech enhancement and diarization ability",
    "checked": true,
    "id": "931cbd9d689e9fd6bd91f4e8e1dbdd7fbb6df9de",
    "semantic_title": "end-to-end speakerbeam for single channel target speech recognition",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hsu19_interspeech.html": {
    "title": "NIESR: Nuisance Invariant End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "Deep neural network models for speech recognition have achieved great success recently, but they can learn incorrect associations between the target and nuisance factors of speech (e.g., speaker identities, background noise, etc.), which can lead to overfitting. While several methods have been proposed to tackle this problem, existing methods incorporate additional information about nuisance factors during training to develop invariant models. However, enumeration of all possible nuisance factors in speech data and the collection of their annotations is difficult and expensive. We present a robust training scheme for end-to-end speech recognition that adopts an unsupervised adversarial invariance induction framework to separate out essential factors for speech-recognition from nuisances without using any supplementary labels besides the transcriptions. Experiments show that the speech recognition model trained with the proposed training scheme achieves relative improvements of 5.48% on WSJ0, 6.16% on CHiME3, and 6.61% on TIMIT dataset over the base model. Additionally, the proposed method achieves a relative improvement of 14.44% on the combined WSJ0+CHiME3 dataset",
    "checked": true,
    "id": "b510695b9c324eec07576346ceeefcc4cf35387f",
    "semantic_title": "niesr: nuisance invariant end-to-end speech recognition",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/suzuki19_interspeech.html": {
    "title": "Knowledge Distillation for Throat Microphone Speech Recognition",
    "volume": "main",
    "abstract": "Throat microphones are robust against external noise because they receive vibrations directly from the skin, however, their available speech data is limited. This work aims to improve the speech recognition accuracy of throat microphones, and we propose a knowledge distillation method of hybrid DNN-HMM acoustic model. This method distills the knowledge from acoustic model trained with a large amount of close-talk microphone speech data (teacher model) to acoustic model for throat microphones (student model) using a small amount of parallel data of throat and close-talk microphones. The frontend network of the student model contains a feature mapping network from throat microphone acoustic features to close-talk microphone bottleneck features, and the back-end network is a phonetic discrimination network from close-talk microphone bottleneck features. We attempted to improve recognition accuracy further by initializing student model parameters using pretrained front-end and back-end networks. Experimental results using Japanese read speech data showed that the proposed approach achieved 9.8% relative improvement of character error rate (14.3% → 12.9%) compared to the hybrid acoustic model trained only with throat microphone speech data. Furthermore, under noise environments of approximately 70 dBA or higher, the throat microphone system with our approach outperformed the close-talk microphone system",
    "checked": true,
    "id": "fe6fc80fa0aa98ae83fbbc1acc085c774abdc6f5",
    "semantic_title": "knowledge distillation for throat microphone speech recognition",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wu19d_interspeech.html": {
    "title": "Improved Speaker-Dependent Separation for CHiME-5 Challenge",
    "volume": "main",
    "abstract": "This paper summarizes several contributions for improving the speaker-dependent separation system for CHiME-5 challenge, which aims to solve the problem of multi-channel, highly-overlapped conversational speech recognition in a dinner party scenario with reverberations and non-stationary noises. Specifically, we adopt a speaker-aware training method by using i-vector as the target speaker information for multi-talker speech separation. With only one unified separation model for all speakers, we achieve a 10% absolute improvement in terms of word error rate (WER) over the previous baseline of 80.28% on the development set by leveraging our newly proposed data processing techniques and beamforming approach. With our improved back-end acoustic model, we further reduce WER to 60.15% which surpasses the result of our submitted CHiME-5 challenge system without applying any fusion techniques",
    "checked": true,
    "id": "a89322e687af8bf64bcd1c0ee22482b4070c12b2",
    "semantic_title": "improved speaker-dependent separation for chime-5 challenge",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19b_interspeech.html": {
    "title": "Bridging the Gap Between Monaural Speech Enhancement and Recognition with Distortion-Independent Acoustic Modeling",
    "volume": "main",
    "abstract": "Monaural speech enhancement has made dramatic advances in recent years. Although enhanced speech has been demonstrated to have better intelligibility and quality for human listeners, feeding it directly to automatic speech recognition (ASR) systems trained with noisy speech has not produced expected improvements in ASR performance. The lack of an enhancement benefit on recognition, or the gap between monaural speech enhancement and recognition, is often attributed to speech distortions introduced in the enhancement process. In this study, we analyze the distortion problem and propose a distortion-independent acoustic modeling scheme. Experimental results show that the distortion-independent acoustic model is able to overcome the distortion problem. Moreover, it can be used with various speech enhancement models. Both the distortion-independent and a noise-dependent acoustic model perform better than the previous best system on the CHiME-2 corpus. The noise-dependent acoustic model achieves a word error rate of 8.7%, outperforming the previous best result by 6.5% relatively",
    "checked": true,
    "id": "1e7b0f5d6745cd3e36638b5adeabcd42becbebcd",
    "semantic_title": "bridging the gap between monaural speech enhancement and recognition with distortion-independent acoustic modeling",
    "citation_count": 33
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19c_interspeech.html": {
    "title": "Enhanced Spectral Features for Distortion-Independent Acoustic Modeling",
    "volume": "main",
    "abstract": "It has recently been shown that a distortion-independent acoustic modeling method is able to overcome the distortion problem caused by speech enhancement. In this study, we improve the distortion-independent acoustic model by feeding it with enhanced spectral features. Using enhanced magnitude spectra, the automatic speech recognition (ASR) system achieves a word error rate of 7.8% on the CHiME-2 corpus, outperforming our previous best system by more than 10% relatively. Compared with the corresponding enhanced waveform signal based system, systems using enhanced spectral features obtain up to 24% relative improvement. These comparisons show that speech enhancement is helpful for robust ASR and that enhanced spectral features are more suitable for ASR tasks than enhanced waveform signals",
    "checked": true,
    "id": "1a18b768e870d8dc9ab81dc17f6732b664172736",
    "semantic_title": "enhanced spectral features for distortion-independent acoustic modeling",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/neekhara19b_interspeech.html": {
    "title": "Universal Adversarial Perturbations for Speech Recognition Systems",
    "volume": "main",
    "abstract": "In this work, we demonstrate the existence of universal adversarial audio perturbations that cause mis-transcription of audio signals by automatic speech recognition (ASR) systems. We propose an algorithm to find a single quasi-imperceptible perturbation, which when added to any arbitrary speech signal, will most likely fool the victim speech recognition model. Our experiments demonstrate the application of our proposed technique by crafting audio-agnostic universal perturbations for the state-of-the-art ASR system — Mozilla DeepSpeech. Additionally, we show that such perturbations generalize to a significant extent across models that are not available during training, by performing a transferability test on a WaveNet based ASR system",
    "checked": true,
    "id": "1a6446a451472b1c5815ce3e00cf84eb7e641a2a",
    "semantic_title": "universal adversarial perturbations for speech recognition systems",
    "citation_count": 82
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fujimoto19_interspeech.html": {
    "title": "One-Pass Single-Channel Noisy Speech Recognition Using a Combination of Noisy and Enhanced Features",
    "volume": "main",
    "abstract": "This paper introduces a method of noise-robust automatic speech recognition (ASR) that remains effective under one-pass single-channel processing. Under these constraints, the use of single-channel speech enhancement seems to be a reasonable noise-robust approach to ASR, because complicated techniques requiring multi-pass processing cannot be used. However, in many cases, single-channel speech enhancement seriously deteriorates the accuracy of ASR because of speech distortion. In addition, the advanced acoustic modeling framework (joint training) is relatively ineffective in the case of single-channel processing. To overcome these problems, we propose a noise-robust acoustic modeling framework based on a feature-level combination of noisy speech and enhanced speech. To obtain further improvements, we also adopt a sub-network-level combination of noisy and enhanced speech, and a gating mechanism that can dynamically select appropriate speech features. Through comparative evaluations, we confirm that the proposed method successfully improves the accuracy of ASR in noisy environments under strong constraints",
    "checked": true,
    "id": "b1ae7cda95d5ee2da1f33521954b742d4a68e4db",
    "semantic_title": "one-pass single-channel noisy speech recognition using a combination of noisy and enhanced features",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19_interspeech.html": {
    "title": "Jointly Adversarial Enhancement Training for Robust End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "Recently, the end-to-end system has made significant breakthroughs in the field of speech recognition. However, this single end-to-end architecture is not especially robust to the input variations interfered of noises and reverberations, resulting in performance degradation dramatically in reality. To alleviate this issue, the mainstream approach is to use a well-designed speech enhancement module as the front-end of ASR. However, enhancement modules would result in speech distortions and mismatches to training, which sometimes degrades the ASR performance. In this paper, we propose a jointly adversarial enhancement training to boost robustness of end-to-end systems. Specifically, we use a jointly compositional scheme of mask-based enhancement network, attention-based encoder-decoder network and discriminant network during training. The discriminator is used to distinguish between the enhanced features from enhancement network and clean features, which could guide enhancement network to output towards the realistic distribution. With the joint optimization of the recognition, enhancement and adversarial loss, the compositional scheme is expected to learn more robust representations for the recognition task automatically. Systematic experiments on AISHELL-1 show that the proposed method improves the noise robustness of end-to-end systems and achieves the relative error rate reduction of 4.6% over the multi-condition training",
    "checked": true,
    "id": "e564f49e872dab8a030f705434d9c5c195900d78",
    "semantic_title": "jointly adversarial enhancement training for robust end-to-end speech recognition",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yang19_interspeech.html": {
    "title": "Predicting Humor by Learning from Time-Aligned Comments",
    "volume": "main",
    "abstract": "In this paper, we describe a novel approach for generating unsupervised humor labels using time-aligned user comments, and predicting humor using audio information alone. We collected 241 videos of comedy movies and gameplay videos from one of the largest Chinese video-sharing websites. We generate unsupervised humor labels from laughing comments, and find high agreement between these labels and human annotations. From these unsupervised labels, we build deep learning models using speech and text features, which obtain an AUC of 0.751 in predicting humor on a manually annotated test set. To our knowledge, this is the first study predicting perceived humor in large-scale audio data",
    "checked": true,
    "id": "5a414cd30caa50f26136f2fc351cb0abf7c2897f",
    "semantic_title": "predicting humor by learning from time-aligned comments",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dinkov19_interspeech.html": {
    "title": "Predicting the Leading Political Ideology of YouTube Channels Using Acoustic, Textual, and Metadata Information",
    "volume": "main",
    "abstract": "We address the problem of predicting the leading political ideology, i.e., left-center-right bias, for YouTube channels of news media. Previous work on the problem has focused exclusively on text and on analysis of the language used, topics discussed, sentiment, and the like. In contrast, here we study videos, which yields an interesting multimodal setup. Starting with gold annotations about the leading political ideology of major world news media from Media Bias/Fact Check, we searched on YouTube to find their corresponding channels, and we downloaded a recent sample of videos from each channel. We crawled more than 1,000 YouTube hours along with the corresponding subtitles and metadata, thus producing a new multimodal dataset. We further developed a multimodal deep-learning architecture for the task. Our analysis shows that the use of acoustic signal helped to improve bias detection by more than 6% absolute over using text and metadata only. We release the dataset to the research community, hoping to help advance the field of multi-modal political bias detection",
    "checked": true,
    "id": "afa0f48dab3884c0a6e309699bf67efa2f8d5b61",
    "semantic_title": "predicting the leading political ideology of youtube channels using acoustic, textual, and metadata information",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2019/an19_interspeech.html": {
    "title": "Mitigating Gender and L1 Differences to Improve State and Trait Recognition",
    "volume": "main",
    "abstract": "Automatic detection of speaker states and traits is made more difficult by intergroup differences in how they are distributed and expressed in speech and language. In this study, we explore various deep learning architectures for incorporating demographic information into the classification task. We find that early and late fusion of demographic information both improve performance on the task of personality recognition, and a multitask learning model, which performs best, also significantly improves deception detection accuracy. Our findings establish a new state-of-the-art for personality recognition and deception detection on the CXD corpus, and suggest new best practices for mitigating intergroup differences to improve speaker state and trait recognition",
    "checked": true,
    "id": "ea64d4e06812956961f34ce8c47a5e084cf57712",
    "semantic_title": "mitigating gender and l1 differences to improve state and trait recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/weninger19_interspeech.html": {
    "title": "Deep Learning Based Mandarin Accent Identification for Accent Robust ASR",
    "volume": "main",
    "abstract": "In this paper, we present an in-depth study on the classification of regional accents in Mandarin speech. Experiments are carried out on Mandarin speech data systematically collected from 15 different geographical regions in China for broad coverage. We explore bidirectional Long Short-Term Memory (bLSTM) networks and i-vectors to model longer-term acoustic context. Starting from the classification of the collected data into the 15 regional accents, we derive a three-class grouping via non-metric dimensional scaling (NMDS), for which 68.4% average recall can be obtained. Furthermore, we evaluate a state-of-the-art ASR system on the accented data and demonstrate that the character error rate (CER) strongly varies among these accent groups, even if i-vector speaker adaptation is used. Finally, we show that model selection based on the prediction of our bLSTM accent classifier can yield up to 7.6% CER reduction for accented speech",
    "checked": true,
    "id": "862ecbbcd9f57145fc49b01ca4240d7559c063df",
    "semantic_title": "deep learning based mandarin accent identification for accent robust asr",
    "citation_count": 25
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gosztolya19_interspeech.html": {
    "title": "Calibrating DNN Posterior Probability Estimates of HMM/DNN Models to Improve Social Signal Detection from Audio Data",
    "volume": "main",
    "abstract": "To detect social signals such as laughter or filler events from audio data, a straightforward choice is to apply a Hidden Markov Model (HMM) in combination with a Deep Neural Network (DNN) that supplies the local class posterior estimates ( HMM/DNN hybrid model). However, the posterior estimates of the DNN may be suboptimal due to a mismatch between the cost function used during training (e.g. frame-level cross-entropy) and the actual evaluation metric (e.g. segment-level F score). In this study, we show experimentally that by employing a simple posterior probability calibration technique on the DNN outputs, the performance of the HMM/DNN workflow can be significantly improved. Specifically, we apply a linear transformation on the activations of the output layer right before using the softmax function, and fine-tune the parameters of this transformation. Out of the calibration approaches tested, we got the best F scores when the posterior calibration process was adjusted so as to maximize the actual HMM-based evaluation metric",
    "checked": true,
    "id": "ef3071f4e011c444fd49968d6ff7bc8129dbc9e7",
    "semantic_title": "calibrating dnn posterior probability estimates of hmm/dnn models to improve social signal detection from audio data",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mori19_interspeech.html": {
    "title": "Conversational and Social Laughter Synthesis with WaveNet",
    "volume": "main",
    "abstract": "The studies of laughter synthesis are relatively few, and they are still in a preliminary stage. We explored the possibility of applying WaveNet to laughter synthesis. WaveNet is potentially more suitable to model laughter waveforms that do not have a well-established theory of production like speech signals. Conversational laughter was modelled with a spontaneous dialogue speech corpus based on WaveNet. To obtain more stable laughter generation, conditioning WaveNet by power contour was proposed. Experimental results showed that the synthesized laughter by WaveNet was perceived as closer to natural laughter than HMM-based synthesized laughter",
    "checked": true,
    "id": "d764e4687e398b33a32f9da6fff391703f34cd22",
    "semantic_title": "conversational and social laughter synthesis with wavenet",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ludusan19_interspeech.html": {
    "title": "Laughter Dynamics in Dyadic Conversations",
    "volume": "main",
    "abstract": "Human verbal communication is a complex phenomenon involving dynamics that normally result in the alignment of participants on several modalities, and across various linguistic domains. We examined here whether such dynamics occur also for paralinguistic events, in particular, in the case of laughter. Using a conversational corpus containing dyadic interactions in three languages (French, German and Mandarin Chinese), we investigated three measures of alignment: convergence, synchrony and agreement. Support for convergence and synchrony was found in all three languages, although the level of support varied with the language, while the agreement in laughter type was found to be significant for the German data. The implications of these findings towards a better understanding of the role of laughter in human communication are discussed",
    "checked": true,
    "id": "3393da047f281383f283258bbc5ba6240bbd607c",
    "semantic_title": "laughter dynamics in dyadic conversations",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/truong19_interspeech.html": {
    "title": "Towards an Annotation Scheme for Complex Laughter in Speech Corpora",
    "volume": "main",
    "abstract": "Although laughter research has gained quite some interest over the past few years, a shared description of how to annotate laughter and its sub-units is still missing. We present a first attempt towards an annotation scheme that contributes to improving the homogeneity and transparency with which laughter is annotated. This includes the integration of respiratory noises as well as stretches of speech-laughs, and to a limited extend to smiled speech and short silent intervals. Inter-annotator agreement is assessed while applying the scheme to different corpora where laughter is evoked through different methods and varying settings. Annotating laughter becomes more complex when the situation in which laughter occurs becomes more spontaneous and social. There is a substantial disagreement among the annotators with respect to temporal alignment (when does a unit start and when does it end) and unit classification, particularly the determination of starts/ends of laughter episodes. In summary, this detailed laughter annotation study reflects the need for better investigations of the various components of laughter",
    "checked": true,
    "id": "3a3d936f196c010dcbdfa0858e63f2cf08a5e15e",
    "semantic_title": "towards an annotation scheme for complex laughter in speech corpora",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/baird19_interspeech.html": {
    "title": "Using Speech to Predict Sequentially Measured Cortisol Levels During a Trier Social Stress Test",
    "volume": "main",
    "abstract": "The effect of stress on the human body is substantial, potentially resulting in serious health implications. Furthermore, with modern stressors seemingly on the increase, there is an abundance of contributing factors which lead to a diagnosis of acute stress. However, observing biological stress reactions usually includes costly and time consuming sequential fluid-based samples to determine the degree of biological stress. On the contrary, a speech monitoring approach would allow for a non-invasive indication of stress. To evaluate the efficacy of the speech signal as a marker of stress, we explored, for the first time, the relationship between sequential cortisol samples and speech-based features. Utilising a novel corpus of 43 individuals undergoing a standardised Trier Social Stress Test (TSST), we extract a variety of feature sets and observe a correlation between speech and sequential cortisol measurements. For prediction of mean cortisol levels from speech, results show that for the entire TSST oral presentation, handcrafted COMPARE features achieve best results of 0.244 root mean square error [0 ;1] for the sample 20 minutes after the TSST. Correlation also increases at minute 20, with a Spearman's correlation coefficient of 0.421, and Cohen's d of 0.883 between the baseline and minute 20 cortisol predictions",
    "checked": true,
    "id": "7b5204d925ea5b5b88c257f75088145a47dc749d",
    "semantic_title": "using speech to predict sequentially measured cortisol levels during a trier social stress test",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2019/baird19b_interspeech.html": {
    "title": "Sincerity in Acted Speech: Presenting the Sincere Apology Corpus and Results",
    "volume": "main",
    "abstract": "The ability to discern an individual's level of sincerity varies from person to person and across cultures. Sincerity is typically a key indication of personality traits such as trustworthiness, and portraying sincerity can be integral to an abundance of scenarios, e. g. , when apologising. Speech signals are one important factor when discerning sincerity and, with more modern interactions occurring remotely, automatic approaches for the recognition of sincerity from speech are beneficial during both interpersonal and professional scenarios. In this study we present details of the Sincere Apology Corpus ( Sina-C). Annotated by 22 individuals for their perception of sincerity, Sina-C is an English acted-speech corpus of 32 speakers, apologising in multiple ways. To provide an updated baseline for the corpus, various machine learning experiments are conducted. Finding that extracting deep data-representations (utilising the Deep Spectrum toolkit) from the speech signals is best suited. Classification results on the binary (sincere / not sincere) task are at best 79.2% Unweighted Average Recall and for regression, in regards to the degree of sincerity, a Root Mean Square Error of 0.395 from the standardised range [-1.51; 1.72] is obtained",
    "checked": true,
    "id": "821ebbd0dceada9f7182d7ba416b7a0fcca365b4",
    "semantic_title": "sincerity in acted speech: presenting the sincere apology corpus and results",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/niebuhr19c_interspeech.html": {
    "title": "Do not Hesitate! — Unless You Do it Shortly or Nasally: How the Phonetics of Filled Pauses Determine Their Subjective Frequency and Perceived Speaker Performance",
    "volume": "main",
    "abstract": "In this paper, we test whether the perception of filled-pause (FP) frequency and public-speaking performance are mediated by the phonetic characteristics of FPs. In particular, total duration, vowel-formant pattern (if present), and nasal-segment proportion of FPs were correlated with perceptual data of 29 German listeners who rated excerpts of business presentations given by 68 German-speaking managers. Results show strong inter-speaker differences in how and how often FPs are realized. Moreover, differences in FP duration and nasal proportion are significantly correlated with estimated (i.e. subjective) FP frequency and perceived speaker performance. The shorter and more nasal a speaker's FPs are, the more do listeners underestimate the speaker's actual FP frequency and the higher they rate the speaker's public-speaking performance. The results are discussed in terms of their implications for FP saliency and rhetorical training",
    "checked": false,
    "id": "5efda0c06a3679af74366a1f43738c56c1ee0649",
    "semantic_title": "do not hesitate! - unless you do it shortly or nasally: how the phonetics of filled pauses determine their subjective frequency and perceived speaker performance",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vasquezcorrea19_interspeech.html": {
    "title": "Phonet: A Tool Based on Gated Recurrent Neural Networks to Extract Phonological Posteriors from Speech",
    "volume": "main",
    "abstract": "There are a lot of features that can be extracted from speech signals for different applications such as automatic speech recognition or speaker verification. However, for pathological speech processing there is a need to extract features about the presence of the disease or the state of the patients that are comprehensible for clinical experts. Phonological posteriors are a group of features that can be interpretable by the clinicians and at the same time carry suitable information about the patient's speech. This paper presents a tool to extract phonological posteriors directly from speech signals. The proposed method consists of a bank of parallel bidirectional recurrent neural networks to estimate the posterior probabilities of the occurrence of different phonological classes. The proposed models are able to detect the phonological classes with accuracies over 90%. In addition, the trained models are available to be used by the research community interested in the topic",
    "checked": true,
    "id": "5ea63993284d8efbaf063294b72955b41d1a73a1",
    "semantic_title": "phonet: a tool based on gated recurrent neural networks to extract phonological posteriors from speech",
    "citation_count": 26
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chang19_interspeech.html": {
    "title": "Code-Switching Sentence Generation by Generative Adversarial Networks and its Application to Data Augmentation",
    "volume": "main",
    "abstract": "Code-switching is about dealing with alternative languages in speech or text. It is partially speaker-dependent and domain-related, so completely explaining the phenomenon by linguistic rules is challenging. Compared to most monolingual tasks, insufficient data is an issue for code-switching. To mitigate the issue without expensive human annotation, we proposed an unsupervised method for code-switching data augmentation. By utilizing a generative adversarial network, we can generate intra-sentential code-switching sentences from monolingual sentences. We applied the proposed method on two corpora, and the result shows that the generated code-switching sentences improve the performance of code-switching language models",
    "checked": true,
    "id": "cbfac953843d16a6447c7547ab7048eccfa142c9",
    "semantic_title": "code-switching sentence generation by generative adversarial networks and its application to data augmentation",
    "citation_count": 54
  },
  "https://www.isca-speech.org/archive/interspeech_2019/meier19_interspeech.html": {
    "title": "Comparative Analysis of Think-Aloud Methods for Everyday Activities in the Context of Cognitive Robotics",
    "volume": "main",
    "abstract": "We describe our efforts to compare data collection methods using two think-aloud protocols in preparation to be used as a basis for automatic structuring and labeling of a large database of high-dimensional human activities data into a valuable resource for research in cognitive robotics. The envisioned dataset, currently in development, will contain synchronously recorded multimodal data, including audio, video, and biosignals (eye-tracking, motion-tracking, muscle and brain activity) from about 100 participants performing everyday activities while describing their task through use of think-aloud protocols. This paper provides details of our pilot recordings in the well-established and scalable \"table setting scenario,\" describes the concurrent and retrospective think-aloud protocols used, the methods used to analyze them, and compares their potential impact on the data collected as well as the automatic data segmentation and structuring process",
    "checked": true,
    "id": "5122e8acc32d7089f508fb9c11e33712c033e85d",
    "semantic_title": "comparative analysis of think-aloud methods for everyday activities in the context of cognitive robotics",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/beeferman19_interspeech.html": {
    "title": "RadioTalk: A Large-Scale Corpus of Talk Radio Transcripts",
    "volume": "main",
    "abstract": "We introduce RadioTalk, a corpus of speech recognition transcripts sampled from talk radio broadcasts in the United States between October of 2018 and March of 2019. The corpus is intended for use by researchers in the fields of natural language processing, conversational analysis, and the social sciences. The corpus encompasses approximately 2.8 billion words of automatically transcribed speech from 284,000 hours of radio, together with metadata about the speech, such as geographical location, speaker turn boundaries, gender, and radio program information. In this paper we summarize why and how we prepared the corpus, give some descriptive statistics on stations, shows and speakers, and carry out a few high-level analyses",
    "checked": true,
    "id": "455c31d401981a49b08d7f6fefd62101734b925d",
    "semantic_title": "radiotalk: a large-scale corpus of talk radio transcripts",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mdhaffar19_interspeech.html": {
    "title": "Qualitative Evaluation of ASR Adaptation in a Lecture Context: Application to the PASTEL Corpus",
    "volume": "main",
    "abstract": "Lectures are usually known to be highly specialised in that they deal with multiple and domain specific topics. This context is challenging for Automatic Speech Recognition (ASR) systems since they are sensitive to topic variability. Language Model (LM) adaptation is a commonly used technique to address the mismatch problem between training and test data. In this paper, we are interested in a qualitative analysis in order to relevantly compare the accuracy of the LM adaptation. While word error rate is the most common metric used to evaluate ASR systems, we consider that this metric cannot provide accurate information. Consequently, we explore the use of other metrics based on individual word error rate, indexability, and capability of building relevant requests for information retrieval from the ASR outputs. Experiments are carried out on the PASTEL corpus, a new dataset in French language, composed of lecture recordings, manual chaptering, manual transcriptions, and slides. While an adapted LM allows us to reduce the global classical word error rate by 15.62% in relative, we show that this reduction reaches 44.2% when computed on relevant words only. These observations are confirmed with the high LM adaptation gains obtained with indexability and information retrieval metrics",
    "checked": true,
    "id": "e2d6614b1fc90fce21d47f2ceced68690222d042",
    "semantic_title": "qualitative evaluation of asr adaptation in a lecture context: application to the pastel corpus",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/marinelli19_interspeech.html": {
    "title": "Active Annotation: Bootstrapping Annotation Lexicon and Guidelines for Supervised NLU Learning",
    "volume": "main",
    "abstract": "Natural Language Understanding (NLU) models are typically trained in a supervised learning framework. In the case of intent classification, the predicted labels are predefined and based on the designed annotation schema while the labeling process is based on a laborious task where annotators manually inspect each utterance and assign the corresponding label. We propose an Active Annotation (AA) approach where we combine an unsupervised learning method in the embedding space, a human-in-the-loop verification process, and linguistic insights to create lexicons that can be open categories and adapted over time. In particular, annotators define the y-label space on-the-fly during the annotation using an iterative process and without the need for prior knowledge about the input data. We evaluate the proposed annotation paradigm in a real use-case NLU scenario. Results show that our Active Annotation paradigm achieves accurate and higher quality training data, with an annotation speed of an order of magnitude higher with respect to the traditional human-only driven baseline annotation methodology",
    "checked": true,
    "id": "6eca4a8bada8c59373366241e0f474e18ab28194",
    "semantic_title": "active annotation: bootstrapping annotation lexicon and guidelines for supervised nlu learning",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dabike19_interspeech.html": {
    "title": "Automatic Lyric Transcription from Karaoke Vocal Tracks: Resources and a Baseline System",
    "volume": "main",
    "abstract": "Automatic sung speech recognition is a relatively understudied topic that has been held back by a lack of large and freely available datasets. This has recently changed thanks to the release of the DAMP Sing! dataset, a 1100 hour karaoke dataset originating from the social music-making company, Smule. This paper presents work undertaken to define an easily replicable, automatic speech recognition benchmark for this data. In particular, we describe how transcripts and alignments have been recovered from Karaoke prompts and timings; how suitable training, development and test sets have been defined with varying degrees of accent variability; and how language models have been developed using lyric data from the LyricWikia website. Initial recognition experiments have been performed using factored-layer TDNN acoustic models with lattice-free MMI training using Kaldi. The best WER is 19.60% — a new state-of-the-art for this type of data. The paper concludes with a discussion of the many challenging problems that remain to be solved. Dataset definitions and Kaldi scripts have been made available so that the benchmark is easily replicable",
    "checked": true,
    "id": "97c3344232933d29bc4f9e5f548a3cbebc3ac72d",
    "semantic_title": "automatic lyric transcription from karaoke vocal tracks: resources and a baseline system",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19b_interspeech.html": {
    "title": "Detecting Mismatch Between Speech and Transcription Using Cross-Modal Attention",
    "volume": "main",
    "abstract": "In this paper, we propose to detect mismatches between speech and transcriptions using deep neural networks. Although it is generally assumed there are no mismatches in some speech related applications, it is hard to avoid the errors due to one reason or another. Moreover, the use of mismatched data probably leads to performance reduction when training a model. In our work, instead of detecting the errors by computing the distance between manual transcriptions and text strings obtained using a speech recogniser, we view mismatch detection as a classification task and merge speech and transcription features using deep neural networks. To enhance detection ability, we use cross-modal attention mechanism in our approach by learning the relevance between the features obtained from the two modalities. To evaluate the effectiveness of our approach, we test it on Factored WSJCAM0 by randomly setting three kinds of mismatch, word deletion, insertion or substitution. To test its robustness, we train our models using a small number of samples and detect mismatch with different number of words being removed, inserted, and substituted. In our experiments, the results show the use of our approach for mismatch detection is close to 80% on insertion and deletion and outperforms the baseline",
    "checked": true,
    "id": "663e927200e8fe8f3e2c443bc19e108e5c7388dd",
    "semantic_title": "detecting mismatch between speech and transcription using cross-modal attention",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vidal19_interspeech.html": {
    "title": "EpaDB: A Database for Development of Pronunciation Assessment Systems",
    "volume": "main",
    "abstract": "In this paper, we describe the methodology for collecting and annotating a new database designed for conducting research and development on pronunciation assessment. While a significant amount of research has been done in the area of pronunciation assessment, to our knowledge, no database is available for public use for research in the field. Considering this need, we created EpaDB (English Pronunciation by Argentinians Database), which is composed of English phrases read by native Spanish speakers with different levels of English proficiency. The recordings are annotated with ratings of pronunciation quality at phrase-level and detailed phonetic alignments and transcriptions indicating which phones were actually pronounced by the speakers. We present inter-rater agreement, the effect of each phone on overall perceived non-nativeness, and the frequency of specific pronunciation errors",
    "checked": true,
    "id": "13b584e57b52a93af5352a6ac0480dabd15a5813",
    "semantic_title": "epadb: a database for development of pronunciation assessment systems",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/angerbauer19_interspeech.html": {
    "title": "Automatic Compression of Subtitles with Neural Networks and its Effect on User Experience",
    "volume": "main",
    "abstract": "Understanding spoken language can be impeded through factors like noisy environments, hearing impairments or lack of proficiency. Subtitles can help in those cases. However, for fast speech or limited screen size, it might be advantageous to compress the subtitles to their most relevant content. Therefore, we address automatic sentence compression in this paper. We propose a neural network model based on an encoder-decoder approach with the possibility of integrating the desired compression ratio. Using this model, we conduct a user study to investigate the effects of compressed subtitles on user experience. Our results show that compressed subtitles can suffice for comprehension but may pose additional cognitive load",
    "checked": true,
    "id": "297bac44f284a0b62add3223158cef33e8452d98",
    "semantic_title": "automatic compression of subtitles with neural networks and its effect on user experience",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/luo19_interspeech.html": {
    "title": "Integrating Video Retrieval and Moment Detection in a Unified Corpus for Video Question Answering",
    "volume": "main",
    "abstract": "Traditional video question answering models have been designed to retrieve videos to answer input questions. A drawback of this scenario is that users have to watch the entire video to find their desired answer. Recent work presented unsupervised neural models with attention mechanisms to find moments or segments from retrieved videos to provide accurate answers to input questions. Although these two tasks look similar, the latter is more challenging because the former task only needs to judge whether the question is answered in a video and returns the entire video, while the latter is expected to judge which moment within a video matches the question and accurately returns a segment of the video. Moreover, there is a lack of labeled data for training moment detection models. In this paper, we focus on integrating video retrieval and moment detection in a unified corpus. We further develop two models — a self-attention convolutional network and a memory network — for the tasks. Experimental results on our corpus show that the neural models can accurately detect and retrieve moments in supervised settings",
    "checked": true,
    "id": "a58cd5975fe958e11642edfc63e479c9c066b57d",
    "semantic_title": "integrating video retrieval and moment detection in a unified corpus for video question answering",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gutz19_interspeech.html": {
    "title": "Early Identification of Speech Changes Due to Amyotrophic Lateral Sclerosis Using Machine Classification",
    "volume": "main",
    "abstract": "We used a machine learning (ML) approach to detect bulbar amyotrophic lateral sclerosis (ALS) prior to the onset of overt speech symptoms. The dataset included speech samples from 123 participants who were stratified by sex and into three groups: healthy controls, ALS symptomatic, and ALS presymptomatic. We compared models trained on three group pairs (symptomatic-control, presymptomatic-control, and all ALS-control participants). Using acoustic features obtained with the OpenSMILE ComParE13 configuration, we tested several feature filtering techniques. ML classification was achieved using an SVM model and leave-one-out cross-validation. The most successful model, which was trained on symptomatic-control data, yielded an AUC=0.99 for females and AUC=0.91 for males. Models trained on all ALS-control participants had high diagnostic accuracy for classifying symptomatic and presymptomatic ALS participants (females: AUC=0.85; males: AUC=0.91). Additionally, probabilities from these models correlated with speaking rate (females: Spearman coefficient=-0.60, p<0.001; males: Spearman coefficient=-0.43, p<0.001) and intelligible speaking rate (females: Spearman coefficient=-0.65, p<0.001; males: Spearman coefficient=-0.40, p<0.01), indicating their possible use as a severity index of bulbar motor involvement in ALS. These results highlight the importance of stratifying patients by speech severity when testing diagnostic models and demonstrate the potential of ML classification in early detection and progress monitoring of ALS",
    "checked": true,
    "id": "b90d5faeaf7a610b722b66abe5a3ca9ded010d2b",
    "semantic_title": "early identification of speech changes due to amyotrophic lateral sclerosis using machine classification",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/k19_interspeech.html": {
    "title": "Automatic Detection of Breath Using Voice Activity Detection and SVM Classifier with Application on News Reports",
    "volume": "main",
    "abstract": "Breath detection during speech has broad applications ranging from emotion recognition to detection of diseases. Most of the breath detection equipment are contact based. In the proposed method, we use a voice activity detector (VAD) to find the non-speech region and searches the breath only in this region since breath is a non-speech activity. This reduces the execution time. A support vector machine (SVM) classifier is used with radial basis function (RBF) kernel trained on the cepstrogram feature to detect the breaths in the non-speech regions. The classifier output is post-processed to join breathing segments which are closely spaced and remove small duration breaths. Speech breathing rate is calculated as the ratio of the number of breaths to the time between the first and last breath. The algorithm is tested on a student evaluation database. The algorithm yields an F1 Score of 94% and root mean square error (RMSE) of 7.08 breaths/min for the speech-breathing rate. The output has been validated using thermal videos. The breaths have been classified as full and partial detection based on the Intersection over Union (IOU). The algorithm is also tested on some news channel reports which gave a minimum F1 Score of 73%",
    "checked": true,
    "id": "470da741e8afd6e2ab4eabcb7791f92de754da78",
    "semantic_title": "automatic detection of breath using voice activity detection and svm classifier with application on news reports",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/heo19_interspeech.html": {
    "title": "Acoustic Scene Classification Using Teacher-Student Learning with Soft-Labels",
    "volume": "main",
    "abstract": "Acoustic scene classification identifies an input segment into one of the pre-defined classes using spectral information. The spectral information of acoustic scenes may not be mutually exclusive due to common acoustic properties across different classes, such as babble noises included in both airports and shopping malls. However, conventional training procedure based on one-hot labels does not consider the similarities between different acoustic scenes. We exploit teacher-student learning with the purpose to derive soft-labels that consider common acoustic properties among different acoustic scenes. In teacher-student learning, the teacher network produces soft-labels, based on which the student network is trained. We investigate various methods to extract soft-labels that better represent similarities across different scenes. Such attempts include extracting soft-labels from multiple audio segments that are defined as an identical acoustic scene. Experimental results demonstrate the potential of our approach, showing a classification accuracy of 77.36% on the DCASE 2018 task 1 validation set",
    "checked": true,
    "id": "0f7016fca9cf4f3c89683d94530f0eb6a66af8de",
    "semantic_title": "acoustic scene classification using teacher-student learning with soft-labels",
    "citation_count": 20
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19_interspeech.html": {
    "title": "Rare Sound Event Detection Using Deep Learning and Data Augmentation",
    "volume": "main",
    "abstract": "There is an increasing interest in smart environment and a growing adoption of smart devices. Smart assistants such as Google Home and Amazon Alexa, although focus on speech, could be extended to identify domestic events in real-time to provide more and better smart functions. Sound event detection aims to detect multiple target sound events that may happen simultaneously. The task is challenging due to the overlapping of sound events, the highly imbalanced nature of target and non-target data, and the complicated real-world background noise. In this paper, we proposed a unified approach that takes advantages of both the deep learning and data augmentation. A convolutional neural network (CNN) was combined with a feed-forward neural network (FNN) to improve the detection performance, and a dynamic time warping based data augmentation (DA) method was proposed to address the data imbalance problem. Experiments on several datasets showed a more than 7% increase in accuracy compared to the state-of-the-art approaches",
    "checked": true,
    "id": "4c9e18ce3877a4d90bc0848b57a42b4e8809bba7",
    "semantic_title": "rare sound event detection using deep learning and data augmentation",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sharma19_interspeech.html": {
    "title": "A Combination of Model-Based and Feature-Based Strategy for Speech-to-Singing Alignment",
    "volume": "main",
    "abstract": "Speech and singing are different in many ways. In this work, we propose a novel method to align phonetically identical spoken lyric with a singing vocal in a speech-singing parallel corpus, that is needed in speech-to-singing conversion. We attempt to align speech to singing vocal using a combination of model-based forced alignment and feature-based dynamic time warping (DTW). We first obtain the word boundaries of speech and singing vocals with forced alignment using speech and singing adapted acoustic models, respectively. We consider that speech acoustic models are more accurate than singing acoustic models, therefore, boundaries of spoken words are more accurate than sung words. By searching in the neighborhood of the sung word boundaries in the singing vocal, we hope to improve the alignment between spoken words and sung words. Considering the word boundaries as landmark, we perform speech-to-singing alignment at frame-level using DTW. The proposed method is able to achieve a 47.5% reduction in terms of word boundary error over the baseline, and subsequent improvement of singing quality in a speech-to-singing conversion system",
    "checked": true,
    "id": "07b79f26a8a84f7ddbd81d8222ffd1e55e9ea4bd",
    "semantic_title": "a combination of model-based and feature-based strategy for speech-to-singing alignment",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shrem19_interspeech.html": {
    "title": "Dr.VOT: Measuring Positive and Negative Voice Onset Time in the Wild",
    "volume": "main",
    "abstract": "Voice Onset Time (VOT), a key measurement of speech for basic research and applied medical studies, is the time between the onset of a stop burst and the onset of voicing. When the voicing onset precedes burst onset the VOT is negative; if voicing onset follows the burst, it is positive. In this work, we present a deep-learning model for accurate and reliable measurement of VOT in naturalistic speech. The proposed system addresses two critical issues: it can measure positive and negative VOT equally well, and it is trained to be robust to variation across annotations. Our approach is based on the structured prediction framework, where the feature functions are defined to be RNNs. These learn to capture segmental variation in the signal. Results suggest that our method substantially improves over the current state-of-the-art. In contrast to previous work, our Deep and Robust VOT annotator, Dr.VOT, can successfully estimate negative VOTs while maintaining state-of-the-art performance on positive VOTs. This high level of performance generalizes to new corpora without further retraining",
    "checked": false,
    "id": "90d7175eb18eab5bb969e6e35bd8e2cc6881e43c",
    "semantic_title": "dr.vot : measuring positive and negative voice onset time in the wild",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hui19_interspeech.html": {
    "title": "Effects of Base-Frequency and Spectral Envelope on Deep-Learning Speech Separation and Recognition Models",
    "volume": "main",
    "abstract": "Base-frequencies (F0) and spectral envelopes play an important role in speech separation and recognition by humans. Two experiments were conducted to study how trained networks for multi-speaker speech separation/recognition are affected by difference of F0 and spectral envelopes between source signals. The first experiment examined the effects of natural F0/envelope on the performance of speech separation. Results showed that when the two target signals differed in F0 by ±3 semitones or more or differed in the envelope by a scaling factor larger than 1.08 or less than 0.92, separation performance improved significantly. This is consistent with human listeners and is the first finding for deep learning-network (DNN) models. The second experiment tested the effect of F0/envelope difference on multi-speaker automatic speech recognition(ASR) system's performance. Results showed that multi-speaker recognition result also significantly rely on F0/envelope differences. The overall results indicated that the dependency of the existing automatic systems on monaural cues is similar to that of human, while automatic systems still perform inferior than human on same tasks",
    "checked": true,
    "id": "342580d32d493b25fbc0c52f87a58091f54e4e5f",
    "semantic_title": "effects of base-frequency and spectral envelope on deep-learning speech separation and recognition models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shah19_interspeech.html": {
    "title": "Phone Aware Nearest Neighbor Technique Using Spectral Transition Measure for Non-Parallel Voice Conversion",
    "volume": "main",
    "abstract": "Nearest Neighbor (NN)-based alignment techniques are popular in non-parallel Voice Conversion (VC). The performance of NN-based alignment improves with the information about phone boundary. However, estimating the exact phone boundary is a challenging task. If text corresponding to the utterance is available, the Hidden Markov Model (HMM) can be used to identify the phone boundaries. However, it requires a large amount of training data that is difficult to collect in realistic VC scenarios. Hence, we propose to exploit a Spectral Transition Measure (STM)-based alignment technique that does not require apriori training data. The idea behind STM is that neurons in the auditory or visual cortex respond strongly to the transitional stimuli compared to the steady-state stimuli. The phone boundaries estimated using the STM algorithm are then applied to the NN technique to obtain the aligned spectral features of the source and target speakers. Proposed STM+NN alignment technique is giving on an average 13.67% relative improvement in phonetic accuracy (PA) compared to the NN-based alignment technique. The improvement in %PA after alignment has positively reflected in the better performance in terms of speech quality and speaker similarity (in particular, a relative improvement of 13.63% and 13.26% , respectively) of the converted voice",
    "checked": true,
    "id": "13f8cd2bc61dfdf514172e27441f075285a8ee91",
    "semantic_title": "phone aware nearest neighbor technique using spectral transition measure for non-parallel voice conversion",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shankar19_interspeech.html": {
    "title": "Weakly Supervised Syllable Segmentation by Vowel-Consonant Peak Classification",
    "volume": "main",
    "abstract": "We present a novel approach for blind syllable segmentation that combines model-based feature selection with data-driven classification. In particular, we learn a function that maps short-term energy peaks of a speech utterance onto either the vowel or consonant class. The features used for classification capture spectral and energy signatures which are characteristic of the phonetic properties of the English language. The identified vowel peaks subsequently act as the nucleus of our syllable segments. We demonstrate the effectiveness of our proposed method using nested cross validation on 400 unique test utterances taken randomly from the TIMIT dataset containing over 5000 syllables in total. Our hybrid approach achieves lower insertion rate than the state-of-the-art segmentation methods and a lower deletion rate than all the baseline comparisons",
    "checked": true,
    "id": "3faa3dcb4fc2ac82e54cd021251743b35419bf15",
    "semantic_title": "weakly supervised syllable segmentation by vowel-consonant peak classification",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mateju19_interspeech.html": {
    "title": "An Approach to Online Speaker Change Point Detection Using DNNs and WFSTs",
    "volume": "main",
    "abstract": "In this paper, a new approach to speaker change point (SCP) detection is presented. This method is suitable for online applications (e.g., real-time broadcast monitoring). It is designed in a series of consecutive experiments, aiming at quality of detection as well as low latency. The resulting scheme utilizes a convolution neural network (CNN), whose output is smoothed by a decoder. The CNN is trained using data complemented by artificial examples to reduce different types of errors, and the decoder is based on a weighted finite state transducer (WFST) with the forced length of the transition model. Results obtained on data taken from the COST278 database show that our online approach yields results comparable with an offline multi-pass LIUM toolkit while operating online with a low latency",
    "checked": true,
    "id": "d6f8ef95caef1ebcf2e12fea23a5e76e39f5f5a0",
    "semantic_title": "an approach to online speaker change point detection using dnns and wfsts",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tang19_interspeech.html": {
    "title": "Regression and Classification for Direction-of-Arrival Estimation with Convolutional Recurrent Neural Networks",
    "volume": "main",
    "abstract": "We present a novel learning-based approach to estimate the direction-of-arrival (DOA) of a sound source using a convolutional recurrent neural network (CRNN) trained via regression on synthetic data and Cartesian labels. We also describe an improved method to generate synthetic data to train the neural network using state-of-the-art sound propagation algorithms that model specular as well as diffuse reflections of sound. We compare our model against three other CRNNs trained using different formulations of the same problem: classification on categorical labels, and regression on spherical coordinate labels. In practice, our model achieves up to 43% decrease in angular error over prior methods. The use of diffuse reflection results in 34% and 41% reduction in angular prediction errors on LOCATA and SOFA datasets, respectively, over prior methods based on image-source methods. Our method results in an additional 3% error reduction over prior schemes that use classification networks, and we use 36% fewer network parameters",
    "checked": true,
    "id": "f282a6e0ca8f3f6bdc32fd471094145bd847f319",
    "semantic_title": "regression and classification for direction-of-arrival estimation with convolutional recurrent neural networks",
    "citation_count": 28
  },
  "https://www.isca-speech.org/archive/interspeech_2019/paul19_interspeech.html": {
    "title": "Non-Parallel Voice Conversion Using Weighted Generative Adversarial Networks",
    "volume": "main",
    "abstract": "In this paper, we suggest a novel way to train Generative Adversarial Network (GAN) for the purpose of non-parallel, many-to-many voice conversion. The goal of voice conversion (VC) is to transform speech from a source speaker to that of a target speaker without changing the phonetic contents. Based on ideas from Game Theory, we suggest to multiply the gradient of the Generator with suitable weights. Weights are calculated so that they increase the power of fake samples that fool the Discriminator resulting in a stronger Generator. Motivated by a recently presented GAN based approach for VC, StarGAN-VC, we suggest a variation to StarGAN, referred to as Weighted StarGAN (WeStarGAN). The experiments are conducted on standard CMU ARCTIC database. WeStarGAN-VC approach achieves significantly better relative performance and is clearly preferred over recently proposed StarGAN-VC method in terms of speech subjective quality and speaker similarity with 75% and 65% preference scores, respectively",
    "checked": true,
    "id": "f87e009ed2c0010b40894bee954d2fae8ad81c5a",
    "semantic_title": "non-parallel voice conversion using weighted generative adversarial networks",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chou19_interspeech.html": {
    "title": "One-Shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization",
    "volume": "main",
    "abstract": "Recently, voice conversion (VC) without parallel data has been successfully adapted to multi-target scenario in which a single model is trained to convert the input voice to many different speakers. However, such model suffers from the limitation that it can only convert the voice to the speakers in the training data, which narrows down the applicable scenario of VC. In this paper, we proposed a novel one-shot VC approach which is able to perform VC by only an example utterance from source and target speaker respectively, and the source and target speaker do not even need to be seen during training. This is achieved by disentangling speaker and content representations with instance normalization (IN). Objective and subjective evaluation shows that our model is able to generate the voice similar to target speaker. In addition to the performance measurement, we also demonstrate that this model is able to learn meaningful speaker representations without any supervision",
    "checked": true,
    "id": "c77fa76a857051a6c7deb135a45af8d4a5f32f0f",
    "semantic_title": "one-shot voice conversion by separating speaker and content representations with instance normalization",
    "citation_count": 162
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lu19_interspeech.html": {
    "title": "One-Shot Voice Conversion with Global Speaker Embeddings",
    "volume": "main",
    "abstract": "Building a voice conversion (VC) system for a new target speaker typically requires a large amount of speech data from the target speaker. This paper investigates a method to build a VC system for arbitrary target speaker using one given utterance without any adaptation training process. Inspired by global style tokens (GSTs), which recently has been shown to be effective in controlling the style of synthetic speech, we propose the use of global speaker embeddings (GSEs) to control the conversion target of the VC system. Speaker-independent phonetic posteriorgrams (PPGs) are employed as the local condition input to a conditional WaveNet synthesizer for waveform generation of the target speaker. Meanwhile, spectrograms are extracted from the given utterance and fed into a reference encoder, the generated reference embedding is then employed as attention query to the GSEs to produce the speaker embedding, which is employed as the global condition input to the WaveNet synthesizer to control the generated waveform's speaker identity. In experiments, when compared with an adaptation training based any-to-any VC system, the proposed GSEs based VC approach performs equally well or better in both speech naturalness and speaker similarity, with apparently higher flexibility to the comparison",
    "checked": true,
    "id": "f411384d0fc902e6492ea0863aa6f7d910b4abeb",
    "semantic_title": "one-shot voice conversion with global speaker embeddings",
    "citation_count": 32
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tobing19_interspeech.html": {
    "title": "Non-Parallel Voice Conversion with Cyclic Variational Autoencoder",
    "volume": "main",
    "abstract": "In this paper, we present a novel technique for a non-parallel voice conversion (VC) with the use of cyclic variational auto-encoder (CycleVAE)-based spectral modeling. In a variational autoencoder (VAE) framework, a latent space, usually with a Gaussian prior, is used to encode a set of input features. In a VAE-based VC, the encoded latent features are fed into a decoder, along with speaker-coding features, to generate estimated spectra with either the original speaker identity (reconstructed) or another speaker identity (converted). Due to the non-parallel modeling condition, the converted spectra can not be directly optimized, which heavily degrades the performance of a VAE-based VC. In this work, to overcome this problem, we propose to use CycleVAE-based spectral model that indirectly optimizes the conversion flow by recycling the converted features back into the system to obtain corresponding cyclic reconstructed spectra that can be directly optimized. The cyclic flow can be continued by using the cyclic reconstructed features as input for the next cycle. The experimental results demonstrate the effectiveness of the proposed CycleVAE-based VC, which yields higher accuracy of converted spectra, generates latent features with higher correlation degree, and significantly improves the quality and conversion accuracy of the converted speech",
    "checked": true,
    "id": "a63ad584efd71d9ab0700f083b6fa7b516c8e1d8",
    "semantic_title": "non-parallel voice conversion with cyclic variational autoencoder",
    "citation_count": 54
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kaneko19_interspeech.html": {
    "title": "StarGAN-VC2: Rethinking Conditional Methods for StarGAN-Based Voice Conversion",
    "volume": "main",
    "abstract": "Non-parallel multi-domain voice conversion (VC) is a technique for learning mappings among multiple domains without relying on parallel data. This is important but challenging owing to the requirement of learning multiple mappings and the non-availability of explicit supervision. Recently, StarGAN-VC has garnered attention owing to its ability to solve this problem only using a single generator. However, there is still a gap between real and converted speech. To bridge this gap, we rethink conditional methods of StarGAN-VC, which are key components for achieving non-parallel multi-domain VC in a single model, and propose an improved variant called StarGAN-VC2. Particularly, we rethink conditional methods in two aspects: training objectives and network architectures. For the former, we propose a source-and-target conditional adversarial loss that allows all source domain data to be convertible to the target domain data. For the latter, we introduce a modulation-based conditional method that can transform the modulation of the acoustic feature in a domain-specific manner. We evaluated our methods on non-parallel multi-speaker VC. An objective evaluation demonstrates that our proposed methods improve speech quality in terms of both global and local structure measures. Furthermore, a subjective evaluation shows that StarGAN-VC2 outperforms StarGAN-VC in terms of naturalness and speaker similarity",
    "checked": true,
    "id": "3830bb029e2abd6240ecacf74ec95f584aa2c167",
    "semantic_title": "stargan-vc2: rethinking conditional methods for stargan-based voice conversion",
    "citation_count": 103
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kurita19_interspeech.html": {
    "title": "Robustness of Statistical Voice Conversion Based on Direct Waveform Modification Against Background Sounds",
    "volume": "main",
    "abstract": "This paper presents an investigation of the robustness of statistical voice conversion (VC) under noisy environments. To develop various VC applications, such as augmented vocal production and augmented speech production, it is necessary to handle noisy input speech because some background sounds, such as external noise and an accompanying sound, usually exist in a real environment. In this paper, we investigate an impact of the background sounds on the conversion performance in singing voice conversion focusing on two main VC frameworks, 1) vocoder-based VC and 2) vocoder-free VC based on direct waveform modification. We conduct a subjective evaluation on the converted singing voice quality under noisy conditions and reveal that the vocoder-free VC is more robust against background sounds compared with the vocoder-based VC. We also analyze the robustness of statistical VC and show that a kurtosis ratio of power spectral components before and after conversion is useful as an objective metric to evaluate it without using any target reference signals",
    "checked": true,
    "id": "97e142eee1bc2e8bad7b3a1b11b595750bacbdff",
    "semantic_title": "robustness of statistical voice conversion based on direct waveform modification against background sounds",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhao19b_interspeech.html": {
    "title": "Fast Learning for Non-Parallel Many-to-Many Voice Conversion with Residual Star Generative Adversarial Networks",
    "volume": "main",
    "abstract": "This paper proposes a fast learning framework for non-parallel many-to-many voice conversion with residual Star Generative Adversarial Networks (StarGAN). In addition to the state-of-the-art StarGAN-VC approach that learns an unreferenced mapping between a group of speakers' acoustic features for nonparallel many-to-many voice conversion, our method, which we call Res-StarGAN-VC, presents an enhancement by incorporating a residual mapping. The idea is to leverage on the shared linguistic content between source and target features during conversion. The residual mapping is realized by using identity shortcut connections from the input to the output of the generator in Res-StarGAN-VC. Such shortcut connections accelerate the learning process of the network with no increase of parameters and computational complexity. They also help generate high-quality fake samples at the very beginning of the adversarial training. Experiments and subjective evaluations show that the proposed method offers (1) significantly faster convergence in adversarial training and (2) clearer pronunciations and better speaker similarity of converted speech, compared to the StarGAN-VC baseline on both mono-lingual and cross-lingual many-to-many voice conversion tasks",
    "checked": true,
    "id": "03a401ff5d20cf8b3cbc29ebcd517662e403106e",
    "semantic_title": "fast learning for non-parallel many-to-many voice conversion with residual star generative adversarial networks",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2019/juvela19_interspeech.html": {
    "title": "GELP: GAN-Excited Linear Prediction for Speech Synthesis from Mel-Spectrogram",
    "volume": "main",
    "abstract": "Recent advances in neural network -based text-to-speech have reached human level naturalness in synthetic speech. The present sequence-to-sequence models can directly map text to mel-spectrogram acoustic features, which are convenient for modeling, but present additional challenges for vocoding (i.e., waveform generation from the acoustic features). High-quality synthesis can be achieved with neural vocoders, such as WaveNet, but such autoregressive models suffer from slow sequential inference. Meanwhile, their existing parallel inference counterparts are difficult to train and require increasingly large model sizes. In this paper, we propose an alternative training strategy for a parallel neural vocoder utilizing generative adversarial networks, and integrate a linear predictive synthesis filter into the model. Results show that the proposed model achieves significant improvement in inference speed, while outperforming a WaveNet in copy-synthesis quality",
    "checked": false,
    "id": "77634fb70549ebff71e4a39936ae351f630812b5",
    "semantic_title": "gelp: gan-excited liner prediction for speech synthesis from mel-spectrogram",
    "citation_count": 26
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yamamoto19b_interspeech.html": {
    "title": "Probability Density Distillation with Generative Adversarial Networks for High-Quality Parallel Waveform Generation",
    "volume": "main",
    "abstract": "This paper proposes an effective probability density distillation (PDD) algorithm for WaveNet-based parallel waveform generation (PWG) systems. Recently proposed teacher-student frameworks in the PWG system have successfully achieved a real-time generation of speech signals. However, the difficulties optimizing the PDD criteria without auxiliary losses result in quality degradation of synthesized speech. To generate more natural speech signals within the teacher-student framework, we propose a novel optimization criterion based on generative adversarial networks (GANs). In the proposed method, the inverse autoregressive flow-based student model is incorporated as a generator in the GAN framework, and jointly optimized by the PDD mechanism with the proposed adversarial learning method. As this process encourages the student to model the distribution of realistic speech waveform, the perceptual quality of the synthesized speech becomes much more natural. Our experimental results verify that the PWG systems with the proposed method outperform both those using conventional approaches, and also autoregressive generation systems with a well-trained teacher WaveNet",
    "checked": true,
    "id": "2f6e03d60bcd38c76811463eb653e0d5012b9480",
    "semantic_title": "probability density distillation with generative adversarial networks for high-quality parallel waveform generation",
    "citation_count": 39
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mohammadi19_interspeech.html": {
    "title": "One-Shot Voice Conversion with Disentangled Representations by Leveraging Phonetic Posteriorgrams",
    "volume": "main",
    "abstract": "We propose voice conversion model from arbitrary source speaker to arbitrary target speaker with disentangled representations. Voice conversion is a task to convert the voice of spoken utterance of source speaker to that of target speaker. Most prior work require to know either source speaker or target speaker or both in training, with either parallel or non-parallel corpus. Instead, we study the problem of voice conversion in nonparallel speech corpora and one-shot learning setting. We convert an arbitrary sentences of an arbitrary source speaker to target speakers given only one or few target speaker training utterances. To achieve this, we propose to use disentangled representations of speaker identity and linguistic context. We use a recurrent neural network (RNN) encoder for speaker embedding and phonetic posteriorgram as linguistic context encoding, along with a RNN decoder to generate converted utterances. Ours is a simpler model without adversarial training or hierarchical model design and thus more efficient. In the subjective tests, our approach achieved significantly better results compared to baseline regarding similarity",
    "checked": true,
    "id": "420b0c4773068b4507684cc361b4ccb9ea811bc1",
    "semantic_title": "one-shot voice conversion with disentangled representations by leveraging phonetic posteriorgrams",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19c_interspeech.html": {
    "title": "Investigation of F0 Conditioning and Fully Convolutional Networks in Variational Autoencoder Based Voice Conversion",
    "volume": "main",
    "abstract": "In this work, we investigate the effectiveness of two techniques for improving variational autoencoder (VAE) based voice conversion (VC). First, we reconsider the relationship between vocoder features extracted using the high quality vocoders adopted in conventional VC systems, and hypothesize that the spectral features are in fact F0 dependent. Such hypothesis implies that during the conversion phase, the latent codes and the converted features in VAE based VC are in fact source F0 dependent. To this end, we propose to utilize the F0 as an additional input of the decoder. The model can learn to disentangle the latent code from the F0 and thus generates converted F0 dependent converted features. Second, to better capture temporal dependencies of the spectral features and the F0 pattern, we replace the frame wise conversion structure in the original VAE based VC framework with a fully convolutional network structure. Our experiments demonstrate that the degree of disentanglement as well as the naturalness of the converted speech are indeed improved",
    "checked": true,
    "id": "b4dd9991a937cf7a7a542f527fc38397c2752dd9",
    "semantic_title": "investigation of f0 conditioning and fully convolutional networks in variational autoencoder based voice conversion",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19b_interspeech.html": {
    "title": "Jointly Trained Conversion Model and WaveNet Vocoder for Non-Parallel Voice Conversion Using Mel-Spectrograms and Phonetic Posteriorgrams",
    "volume": "main",
    "abstract": "The N10 system in the Voice Conversion Challenge 2018 (VCC 2018) has achieved high voice conversion (VC) performance in terms of speech naturalness and speaker similarity. We believe that further improvements can be gained from joint optimization (instead of separate optimization) of the conversion model and WaveNet vocoder, as well as leveraging information from the acoustic representation of the speech waveform, e.g. from Mel-spectrograms. In this paper, we propose a VC architecture to jointly train a conversion model that maps phonetic posteriorgrams (PPGs) to Mel-spectrograms and a WaveNet vocoder. The conversion model has a bottle-neck layer, whose outputs are concatenated with PPGs before being fed into the WaveNet vocoder as local conditioning. A weighted sum of a Mel-spectrogram prediction loss and a WaveNet loss is used as the objective function to jointly optimize parameters of the conversion model and the WaveNet vocoder. Objective and subjective evaluation results show that the proposed approach is capable of achieving significantly improved quality in voice conversion in terms of speech naturalness and speaker similarity of the converted speech for both cross-gender and intra-gender conversions",
    "checked": true,
    "id": "4012133ae119d3f3b556c0b1c66da9eda961b7a7",
    "semantic_title": "jointly trained conversion model and wavenet vocoder for non-parallel voice conversion using mel-spectrograms and phonetic posteriorgrams",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19b_interspeech.html": {
    "title": "Generative Adversarial Networks for Unpaired Voice Transformation on Impaired Speech",
    "volume": "main",
    "abstract": "This paper focuses on using voice conversion (VC) to improve the speech intelligibility of surgical patients who have had parts of their articulators removed. Due to the difficulty of data collection, VC without parallel data is highly desired. Although techniques for unparallel VC — for example, CycleGAN — have been developed, they usually focus on transforming the speaker identity, and directly transforming the speech of one speaker to that of another speaker and as such do not address the task here. In this paper, we propose a new approach for unparallel VC. The proposed approach transforms impaired speech to normal speech while preserving the linguistic content and speaker characteristics. To our knowledge, this is the first end-to-end GAN-based unsupervised VC model applied to impaired speech. The experimental results show that the proposed approach outperforms CycleGAN",
    "checked": true,
    "id": "7a9dd5562ddd3943925a7b06aecb1268d266dfb1",
    "semantic_title": "generative adversarial networks for unpaired voice transformation on impaired speech",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ding19_interspeech.html": {
    "title": "Group Latent Embedding for Vector Quantized Variational Autoencoder in Non-Parallel Voice Conversion",
    "volume": "main",
    "abstract": "This paper proposes a Group Latent Embedding for Vector Quantized Variational Autoencoders (VQ-VAE) used in nonparallel Voice Conversion (VC). Previous studies have shown that VQ-VAE can generate high-quality VC syntheses when it is paired with a powerful decoder. However, in a conventional VQ-VAE, adjacent atoms in the embedding dictionary can represent entirely different phonetic content. Therefore, the VC syntheses can have mispronunciations and distortions whenever the output of the encoder is quantized to an atom representing entirely different phonetic content. To address this issue, we propose an approach that divides the embedding dictionary into groups and uses the weighted average of atoms in the nearest group as the latent embedding. We conducted both objective and subjective experiments on the non-parallel CSTR VCTK corpus. Results show that the proposed approach significantly improves the acoustic quality of the VC syntheses compared to the traditional VQ-VAE (13.7% relative improvement) while retaining the voice identity of the target speaker",
    "checked": true,
    "id": "d2f7e74336baddc6e5e1ac607ad3e33cb24068bd",
    "semantic_title": "group latent embedding for vector quantized variational autoencoder in non-parallel voice conversion",
    "citation_count": 41
  },
  "https://www.isca-speech.org/archive/interspeech_2019/stephenson19_interspeech.html": {
    "title": "Semi-Supervised Voice Conversion with Amortized Variational Inference",
    "volume": "main",
    "abstract": "In this work we introduce a semi-supervised approach to the voice conversion problem, in which speech from a source speaker is converted into speech of a target speaker. The proposed method makes use of both parallel and non-parallel utterances from the source and target simultaneously during training. This approach can be used to extend existing parallel data voice conversion systems such that they can be trained with semi-supervision. We show that incorporating semi-supervision improves the voice conversion performance compared to fully supervised training when the number of parallel utterances is limited as in many practical applications. Additionally, we find that increasing the number non-parallel utterances used in training continues to improve performance when the amount of parallel training data is held constant",
    "checked": true,
    "id": "1f356df1333122c4e90b13681e57fb4d1dd71715",
    "semantic_title": "semi-supervised voice conversion with amortized variational inference",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dey19_interspeech.html": {
    "title": "Exploiting Semi-Supervised Training Through a Dropout Regularization in End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "In this paper, we explore various approaches for semi-supervised learning in an end-to-end automatic speech recognition (ASR) framework. The first step in our approach involves training a seed model on the limited amount of labelled data. Additional unlabelled speech data is employed through a data-selection mechanism to obtain the best hypothesized output, further used to retrain the seed model. However, uncertainties of the model may not be well captured with a single hypothesis. As opposed to this technique, we apply a dropout mechanism to capture the uncertainty by obtaining multiple hypothesized text transcripts of an speech recording. We assume that the diversity of automatically generated transcripts for an utterance will implicitly increase the reliability of the model. Finally, the data-selection process is also applied on these hypothesized transcripts to reduce the uncertainty. Experiments on freely-available TEDLIUM corpus and proprietary Adobe's internal dataset show that the proposed approach significantly reduces ASR errors, compared to the baseline model",
    "checked": true,
    "id": "8cfbaed6f1edecd51226f5a9e3c4645588e24746",
    "semantic_title": "exploiting semi-supervised training through a dropout regularization in end-to-end speech recognition",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kim19_interspeech.html": {
    "title": "Improved Vocal Tract Length Perturbation for a State-of-the-Art End-to-End Speech Recognition System",
    "volume": "main",
    "abstract": "In this paper, we present an improved vocal tract length perturbation (VTLP) algorithm as a data augmentation technique. VTLP is usually accomplished by adjusting the center frequencies of mel filterbank in [1]. Compared to the conventional approach, we re-synthesize waveforms from the frequency-warped spectra using overlap and addition (OLA). This approach had two advantages: First, we can apply an \"acoustic simulator\" [2, 3] after performing the VTLP-based frequency warping. Second, we may use a different window length for frequency warping from that used in feature processing. We observe that the best performance was obtained when the warping coefficient distribution is between 0.8 and 1.2, and the window length is 50 ms. We obtained 3.66% WER and 12.39% WER on the Librispeech test-clean and test-other using an attention-based end-to-end speech recognition system without using any Language Models (LMs). Using the shallow-fusion technique with a Transformer LM, we achieved 2.44% WER and 8.29% WER on the Librispeech test-clean and test-other sets. To the best of our knowledge, the 2.44% WER on the test-clean is the best result ever reported on this test set",
    "checked": true,
    "id": "82a51742f7693a3a0df7096560f0534037ebbccf",
    "semantic_title": "improved vocal tract length perturbation for a state-of-the-art end-to-end speech recognition system",
    "citation_count": 28
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhu19_interspeech.html": {
    "title": "Multi-Accent Adaptation Based on Gate Mechanism",
    "volume": "main",
    "abstract": "When only a limited amount of accented speech data is available, to promote multi-accent speech recognition performance, the conventional approach is accent-specific adaptation, which adapts the baseline model to multiple target accents independently. To simplify the adaptation procedure, we explore adapting the baseline model to multiple target accents simultaneously with multi-accent mixed data. Thus, we propose using accent-specific top layer with gate mechanism (AST-G) to realize multi-accent adaptation. Compared with the baseline model and accent-specific adaptation, AST-G achieves 9.8% and 1.9% average relative WER reduction respectively. However, in real-world applications, we can't obtain the accent category label for inference in advance. Therefore, we apply using an accent classifier to predict the accent label. To jointly train the acoustic model and the accent classifier, we propose the multi-task learning with gate mechanism (MTL-G). As the accent label prediction could be inaccurate, it performs worse than the accent-specific adaptation. Yet, in comparison with the baseline model, MTL-G achieves 5.1% average relative WER reduction",
    "checked": true,
    "id": "b194db16b1fdddd5b1b087429a754a625f7215c6",
    "semantic_title": "multi-accent adaptation based on gate mechanism",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2019/guo19_interspeech.html": {
    "title": "Unsupervised Adaptation with Adversarial Dropout Regularization for Robust Speech Recognition",
    "volume": "main",
    "abstract": "Recent adversarial methods proposed for unsupervised domain adaptation of acoustic models try to fool a specific domain discriminator and learn both senone-discriminative and domain-invariant hidden feature representations. However, a drawback of these approaches is that the feature generator simply aligns different features into the same distribution without considering the class boundaries of the target domain data. Thus, ambiguous target domain features can be generated near the decision boundaries, decreasing speech recognition performance. In this study, we propose to use Adversarial Dropout Regularization (ADR) in acoustic modeling to overcome the foregoing issue. Specifically, we optimize the senone classifier to make its decision boundaries lie in the class boundaries of unlabeled target data. Then, the feature generator learns to create features far away from the decision boundaries, which are more discriminative. We apply the ADR approach on the CHiME-3 corpus and the proposed method yields up to 12.9% relative WER reductions compared with the baseline trained on source domain data only and further improvement over the widely used gradient reversal layer method",
    "checked": true,
    "id": "bbe07b9c03cb3b11521876b2216faf151f6002db",
    "semantic_title": "unsupervised adaptation with adversarial dropout regularization for robust speech recognition",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kitza19_interspeech.html": {
    "title": "Cumulative Adaptation for BLSTM Acoustic Models",
    "volume": "main",
    "abstract": "This paper addresses the robust speech recognition problem as an adaptation task. Specifically, we investigate the cumulative application of adaptation methods. A bidirectional Long Short-Term Memory (BLSTM) based neural network, capable of learning temporal relationships and translation invariant representations, is used for robust acoustic modeling. Further, i-vectors were used as an input to the neural network to perform instantaneous speaker and environment adaptation, providing 8% relative improvement in word error rate on the NIST Hub5 2000 evaluation testset. By enhancing the first-pass i-vector based adaptation with a second-pass adaptation using speaker and environment dependent transformations within the network, a further relative improvement of 5% in word error rate was achieved. We have reevaluated the features used to estimate i-vectors and their normalization to achieve the best performance in a modern large scale automatic speech recognition system",
    "checked": true,
    "id": "8a568abeba0c2668cbbd58ccc094b8257c9ad4d1",
    "semantic_title": "cumulative adaptation for blstm acoustic models",
    "citation_count": 31
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xie19b_interspeech.html": {
    "title": "Fast DNN Acoustic Model Speaker Adaptation by Learning Hidden Unit Contribution Features",
    "volume": "main",
    "abstract": "Speaker adaptation techniques play a key role in reducing the mismatch between automatic speech recognition (ASR) systems and target users. Deep neural network (DNN) acoustic model adaptation by learning speaker-dependent hidden unit contributions (LHUC) scaling vectors has been widely used. The standard LHUC method not only requires multiple decoding passes in test time but also a substantial amount of adaptation data for robust parameter estimation. In order to address the issues, an efficient method of predicting and compressing the LHUC scaling vectors directly from acoustic features using a time-delay DNN (TDNN) and an online averaging layer is proposed in this paper. The resulting LHUC vectors are then used as auxiliary features to adapt DNN acoustic models. Experiments conducted on a 300-hour Switchboard corpus showed that the DNN and TDNN systems using the proposed predicted LHUC features consistently outperformed the corresponding baseline systems by up to about 9% relative reductions of word error rate. Being combined with i-Vector based adaptation, the LHUC feature adapted TDNN systems demonstrated consistent improvement over comparable i-Vector adapted TDNN system",
    "checked": true,
    "id": "1d79786bd857768a2dad047836d54b067807a719",
    "semantic_title": "fast dnn acoustic model speaker adaptation by learning hidden unit contribution features",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tsunoo19_interspeech.html": {
    "title": "End-to-End Adaptation with Backpropagation Through WFST for On-Device Speech Recognition System",
    "volume": "main",
    "abstract": "An on-device DNN-HMM speech recognition system efficiently works with a limited vocabulary in the presence of a variety of predictable noise. In such a case, vocabulary and environment adaptation is highly effective. In this paper, we propose a novel method of end-to-end (E2E) adaptation, which adjusts not only an acoustic model (AM) but also a weighted finite-state transducer (WFST). We convert a pretrained WFST to a trainable neural network and adapt the system to target environments/vocabulary by E2E joint training with an AM. We replicate Viterbi decoding with forward-backward neural network computation, which is similar to recurrent neural networks (RNNs). By pooling output score sequences, a vocabulary posterior for each utterance is obtained and used for discriminative loss computation. Experiments using 2–10 hours of English/Japanese adaptation datasets indicate that the fine-tuning of only WFSTs and that of only AMs are both comparable to a state-of-the-art adaptation method, and E2E joint training of the two components achieves the best recognition performance. We also adapt each language system to the other language using the adaptation data, and the results show that the proposed method also works well for language adaptations",
    "checked": true,
    "id": "e4b741df848e29236139abb6d9f033b496eb619c",
    "semantic_title": "end-to-end adaptation with backpropagation through wfst for on-device speech recognition system",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sar19_interspeech.html": {
    "title": "Learning Speaker Aware Offsets for Speaker Adaptation of Neural Networks",
    "volume": "main",
    "abstract": "In this work, we present an unsupervised long short-term memory (LSTM) layer normalization technique that we call adaptation by speaker aware offsets (ASAO). These offsets are learned using an auxiliary network attached to the main senone classifier. The auxiliary network takes main network LSTM activations as input and tries to reconstruct speaker, (speaker,phone) and (speaker,senone)-level averages of the activations by minimizing the mean-squared error. Once the auxiliary network is jointly trained with the main network, during test time we do not need additional information for the test data as the network will generate the offset itself. Unlike many speaker adaptation studies which only adapt fully connected layers, our method is applicable to LSTM layers in addition to fully-connected layers. In our experiments, we investigate the effect of ASAO of LSTM layers at different depths. We also show its performance when the inputs are already speaker adapted by feature space maximum likelihood linear regression (fMLLR). In addition, we compare ASAO with a speaker adversarial training framework. ASAO achieves higher senone classification accuracy and lower word error rate (WER) than both the unadapted models and the adversarial model on the HUB4 dataset, with an absolute WER reduction of up to 2%",
    "checked": true,
    "id": "7640a918f798fbbf8034a72a13bf30029954d1f8",
    "semantic_title": "learning speaker aware offsets for speaker adaptation of neural networks",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sim19_interspeech.html": {
    "title": "An Investigation into On-Device Personalization of End-to-End Automatic Speech Recognition Models",
    "volume": "main",
    "abstract": "Speaker-independent speech recognition systems trained with data from many users are generally robust against speaker variability and work well for a large population of speakers. However, these systems do not always generalize well for users with very different speech characteristics. This issue can be addressed by building personalized systems that are designed to work well for each specific user. In this paper, we investigate the idea of securely training personalized end-to-end speech recognition models on mobile devices so that user data and models never leave the device and are never stored on a server. We study how the mobile training environment impacts performance by simulating on-device data consumption. We conduct experiments using data collected from speech impaired users for personalization. Our results show that personalization achieved 63.7% relative word error rate reduction when trained in a server environment and 58.1% in a mobile environment. Moving to on-device personalization resulted in 18.7% performance degradation, in exchange for improved scalability and data privacy. To train the model on device, we split the gradient computation into two and achieved 45% memory reduction at the expense of 42% increase in training time",
    "checked": true,
    "id": "aec9df8489c58cb822f911f184ce3b0c61363dda",
    "semantic_title": "an investigation into on-device personalization of end-to-end automatic speech recognition models",
    "citation_count": 50
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jain19_interspeech.html": {
    "title": "A Multi-Accent Acoustic Model Using Mixture of Experts for Speech Recognition",
    "volume": "main",
    "abstract": "A major challenge in Automatic Speech Recognition(ASR) systems is to handle speech from a diverse set of accents. A model trained using a single accent performs rather poorly when confronted with different accents. One of the solutions is a multi-condition model trained on all the accents. However the performance improvement in this approach might be rather limited. Otherwise, accent-specific models might be trained but they become impractical as number of accents increases. In this paper, we propose a novel acoustic model architecture based on Mixture of Experts (MoE) which works well on multiple accents without having the overhead of training separate models for separate accents. The work is based on our earlier work, termed as MixNet, where we showed performance improvement by separation of phonetic class distributions in the feature space. In this paper, we propose an architecture that helps to compensate phonetic and accent variabilities which helps in even better discrimination among the classes. These variabilities are learned in a joint frame-work, and produce consistent improvements over all the individual accents, amounting to an overall 18% relative improvement in accuracy compared to baseline trained in multi-condition style",
    "checked": true,
    "id": "834c5744ccaabbf1930c96f781ebcd4ee0ef76bc",
    "semantic_title": "a multi-accent acoustic model using mixture of experts for speech recognition",
    "citation_count": 23
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shor19_interspeech.html": {
    "title": "Personalizing ASR for Dysarthric and Accented Speech with Limited Data",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) systems have dramatically improved over the last few years. ASR systems are most often trained from ‘typical' speech, which means that underrepresented groups don't experience the same level of improvement. In this paper, we present and evaluate finetuning techniques to improve ASR for users with non-standard speech. We focus on two types of non-standard speech: speech from people with amyotrophic lateral sclerosis (ALS) and accented speech. We train personalized models that achieve 62% and 35% relative WER improvement on these two groups, bringing the absolute WER for ALS speakers, on a test set of message bank phrases, down to 10% for mild dysarthria and 20% for more serious dysarthria. We show that 71% of the improvement comes from only 5 minutes of training data. Finetuning a particular subset of layers (with many fewer parameters) often gives better results than finetuning the entire model. This is the first step towards building state of the art ASR models for dysarthric speech",
    "checked": true,
    "id": "df3f3c908be233e8aee9872c28258741f4b5ba0a",
    "semantic_title": "personalizing asr for dysarthric and accented speech with limited data",
    "citation_count": 76
  },
  "https://www.isca-speech.org/archive/interspeech_2019/peskov19_interspeech.html": {
    "title": "Mitigating Noisy Inputs for Question Answering",
    "volume": "main",
    "abstract": "Natural language processing systems are often downstream of unreliable inputs: machine translation, optical character recognition, or speech recognition. For instance, virtual assistants can only answer your questions after understanding your speech. We investigate and mitigate the effects of noise from Automatic Speech Recognition systems on two factoid Question Answering ( qa) tasks. Integrating confidences into the model and forced decoding of unknown words are empirically shown to improve the accuracy of downstream neural qa systems. We create and train models on a synthetic corpus of over 500,000 noisy sentences and evaluate on two human corpora from Quizbowl and Jeopardy! competitions",
    "checked": true,
    "id": "970383c0a41d7ae1ec4b8abaa3033778203377b9",
    "semantic_title": "mitigating noisy inputs for question answering",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gupta19_interspeech.html": {
    "title": "One-vs-All Models for Asynchronous Training: An Empirical Analysis",
    "volume": "main",
    "abstract": "Any given classification problem can be modeled using multiclass or One-vs-All (OVA) architecture. An OVA system consists of as many OVA models as the number of classes, providing the advantage of asynchrony, where each OVA model can be re-trained independent of other models. This is particularly advantageous in settings where scalable model training is a consideration (for instance in an industrial environment where multiple and frequent updates need to be made to the classification system). In this paper, we conduct empirical analysis on realizing independent updates to OVA models and its impact on the accuracy of the overall OVA system. Given that asynchronous updates lead to differences in training datasets for OVA models, we first define a metric to quantify the differences in datasets. Thereafter, using Natural Language Understanding as a task of interest, we estimate the impact of three factors: (i) number of classes, (ii) number of data points and, (iii) divergences in training datasets across OVA models; on the OVA system accuracy. Finally, we observe the accuracy impact of increased asynchrony in a Spoken Language Understanding system. We analyze the results and establish that the proposed metric correlates strongly with the model performances in both the experimental settings",
    "checked": true,
    "id": "63e190652d91b23b60a9b6bcc5c706f56eebbf88",
    "semantic_title": "one-vs-all models for asynchronous training: an empirical analysis",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/marzinotto19_interspeech.html": {
    "title": "Adapting a FrameNet Semantic Parser for Spoken Language Understanding Using Adversarial Learning",
    "volume": "main",
    "abstract": "This paper presents a new semantic frame parsing model, based on Berkeley FrameNet, adapted to process spoken documents in order to perform information extraction from broadcast contents. Building upon previous work that had shown the effectiveness of adversarial learning for domain generalization in the context of semantic parsing of encyclopedic written documents, we propose to extend this approach to elocutionary style generalization. The underlying question throughout this study is whether adversarial learning can be used to combine data from different sources and train models on a higher level of abstraction in order to increase their robustness to lexical and stylistic variations as well as automatic speech recognition errors. The proposed strategy is evaluated on a French corpus of encyclopedic written documents and a smaller corpus of radio podcast transcriptions, both annotated with a FrameNet paradigm. We show that adversarial learning increases all models generalization capabilities both on manual and automatic speech transcription as well as on encyclopedic data",
    "checked": true,
    "id": "f6a520994035941a2a2e301003643fefbe0e0914",
    "semantic_title": "adapting a framenet semantic parser for spoken language understanding using adversarial learning",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/parcollet19_interspeech.html": {
    "title": "M2H-GAN: A GAN-Based Mapping from Machine to Human Transcripts for Speech Understanding",
    "volume": "main",
    "abstract": "Deep learning is at the core of recent spoken language understanding (SLU) related tasks. More precisely, deep neural networks (DNNs) drastically increased the performances of SLU systems, and numerous architectures have been proposed. In the real-life context of theme identification of telephone conversations, it is common to hold both a human, manual (TRS) and an automatically transcribed (ASR) versions of the conversations. Nonetheless, and due to production constraints, only the ASR transcripts are considered to build automatic classifiers. TRS transcripts are only used to measure the performances of ASR systems. Moreover, the recent performances in term of classification accuracy, obtained by DNN related systems are close to the performances reached by humans, and it becomes difficult to further increase the performances by only considering the ASR transcripts. This paper proposes to distillates the TRS knowledge available during the training phase within the ASR representation, by using a new generative adversarial network called M2H-GAN to generate a TRS-like version of an ASR document, to improve the theme identification performances",
    "checked": true,
    "id": "4f26b0e04985a7014e8e374d4832c9e556ec8cf8",
    "semantic_title": "m2h-gan: a gan-based mapping from machine to human transcripts for speech understanding",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/georges19_interspeech.html": {
    "title": "Ultra-Compact NLU: Neuronal Network Binarization as Regularization",
    "volume": "main",
    "abstract": "This paper describes an approach for intent classification and tagging on embedded devices, such as smart watches. We describe a technique to train neuronal networks where the final neuronal network weights are binary. This enables memory bandwidth optimized inference and efficient computation even on constrained/embedded platforms The flow of the approach is as follows: tf-idf word selection method reduces the number of overall weights. Bag-of-Words features are used with a feedforward and recurrent neuronal network for intent classification and tagging, respectively. A novel double Gaussian based regularization term is used to train the network. Finally, the weights are almost clipped lossless to -1 or 1 which results in a tiny binary neuronal network for intent classification and tagging Our technique is evaluated using a text corpus of transcribed and annotated voice queries. The test domain is \"lights control\". We compare the intent and tagging accuracy of the ultra-compact binary neuronal network with our baseline system. The novel approach yields comparable accuracy but reduces the model size by a factor of 16: from 160kB to 10kB",
    "checked": true,
    "id": "8f967d0d3f43ecfdfa847775e24e8a5dab6b239c",
    "semantic_title": "ultra-compact nlu: neuronal network binarization as regularization",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lugosch19_interspeech.html": {
    "title": "Speech Model Pre-Training for End-to-End Spoken Language Understanding",
    "volume": "main",
    "abstract": "Whereas conventional spoken language understanding (SLU) systems map speech to text, and then text to intent, end-to-end SLU systems map speech directly to intent through a single trainable model. Achieving high accuracy with these end-to-end models without a large amount of training data is difficult. We propose a method to reduce the data requirements of end-to-end SLU in which the model is first pre-trained to predict words and phonemes, thus learning good features for SLU. We introduce a new SLU dataset, Fluent Speech Commands, and show that our method improves performance both when the full dataset is used for training and when only a small subset is used. We also describe preliminary experiments to gauge the model's ability to generalize to new phrases not heard during training",
    "checked": true,
    "id": "24d7b1487202e3aaf329df3d8135ae6eabefaa45",
    "semantic_title": "speech model pre-training for end-to-end spoken language understanding",
    "citation_count": 244
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shivakumar19_interspeech.html": {
    "title": "Spoken Language Intent Detection Using Confusion2Vec",
    "volume": "main",
    "abstract": "Decoding speaker's intent is a crucial part of spoken language understanding (SLU). The presence of noise or errors in the text transcriptions, in real life scenarios make the task more challenging. In this paper, we address the spoken language intent detection under noisy conditions imposed by automatic speech recognition (ASR) systems. We propose to employ confusion2vec word feature representation to compensate for the errors made by ASR and to increase the robustness of the SLU system. The confusion2vec, motivated from human speech production and perception, models acoustic relationships between words in addition to the semantic and syntactic relations of words in human language. We hypothesize that ASR often makes errors relating to acoustically similar words, and the confusion2vec with inherent model of acoustic relationships between words is able to compensate for the errors. We demonstrate through experiments on the ATIS benchmark dataset, the robustness of the proposed model to achieve state-of-the-art results under noisy ASR conditions. Our system reduces classification error rate (CER) by 20.84% and improves robustness by 37.48% (lower CER degradation) relative to the previous state-of-the-art going from clean to noisy transcripts. Improvements are also demonstrated when training the intent detection models on noisy transcripts",
    "checked": true,
    "id": "944c90a271afad85d4443f8a63284b708e38913a",
    "semantic_title": "spoken language intent detection using confusion2vec",
    "citation_count": 24
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tomashenko19_interspeech.html": {
    "title": "Investigating Adaptation and Transfer Learning for End-to-End Spoken Language Understanding from Speech",
    "volume": "main",
    "abstract": "This work investigates speaker adaptation and transfer learning for spoken language understanding (SLU). We focus on the direct extraction of semantic tags from the audio signal using an end-to-end neural network approach. We demonstrate that the learning performance of the target predictive function for the semantic slot filling task can be substantially improved by speaker adaptation and by various knowledge transfer approaches. First, we explore speaker adaptive training (SAT) for end-to-end SLU models and propose to use zero pseudo i-vectors for more efficient model initialization and pretraining in SAT. Second, in order to improve the learning convergence for the target semantic slot filling (SF) task, models trained for different tasks, such as automatic speech recognition and named entity extraction are used to initialize neural end-to-end models trained for the target task. In addition, we explore the impact of the knowledge transfer for SLU from a speech recognition task trained in a different language. These approaches allow to develop end-to-end SLU systems in low-resource data scenarios when there is no enough in-domain semantically labeled data, but other resources, such as word transcriptions for the same or another language or named entity annotation, are available",
    "checked": true,
    "id": "60db06f7a29fa9281cabc15f3d1ef8036abe96f5",
    "semantic_title": "investigating adaptation and transfer learning for end-to-end spoken language understanding from speech",
    "citation_count": 25
  },
  "https://www.isca-speech.org/archive/interspeech_2019/song19_interspeech.html": {
    "title": "Topic-Aware Dialogue Speech Recognition with Transfer Learning",
    "volume": "main",
    "abstract": "Dialogue speech widely exists in scenarios such as chitchat, meeting and customer service. General-purpose speech recognition systems usually neglect the topic information in the context of dialogue speech, which has great potential for improving the performance of speech recognition. In this paper, we propose a transfer learning mechanism to conduct topic-aware recognition for dialogue speech. We first propose a new probabilistic topic model named Dialogue Speech Topic Model (DSTM) that is specialized for modeling the context of dialogue speech. We further propose a novel transfer learning mechanism for DSTM to significantly reduce its training cost while preserving its effectiveness for accurate topic inference. The experiment results demonstrate that proposed techniques in language model adaptation effectively improve the performance of the state-of-the-art Automatic Speech Recognition (ASR) system",
    "checked": true,
    "id": "1e564639cd2a37c97afc9959c14aec529adc7de7",
    "semantic_title": "topic-aware dialogue speech recognition with transfer learning",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/masumura19_interspeech.html": {
    "title": "Improving Conversation-Context Language Models with Multiple Spoken Language Understanding Models",
    "volume": "main",
    "abstract": "In this paper, we integrate fully neural network based conversation-context language models (CCLMs) that are suitable for handling multi-turn conversational automatic speech recognition (ASR) tasks, with multiple neural spoken language understanding (SLU) models. A main strength of CCLMs is their capacity to take long-range interactive contexts beyond utterance boundaries into consideration. However, it is hard to optimize the CCLMs so as to fully exploit the long-range interactive contexts because conversation-level training datasets are often limited. In order to mitigate this problem, our key idea is to introduce various SLU models that are developed for spoken dialogue systems into the CCLMs. In our proposed method (which we call \"SLU-assisted CCLM\"), hierarchical recurrent encoder-decoder based language modeling is extended so as to handle various utterance-level SLU results of preceding utterances in a continuous space. We expect that the SLU models will help the CCLMs to properly understand semantic meanings of long-range interactive contexts and to fully leverage them for estimating a next utterance. Our experiments on contact center dialogue ASR tasks demonstrate that SLU-assisted CCLMs combined with three types of SLU models can yield ASR performance improvements",
    "checked": true,
    "id": "3e08eb1fbc4617976173f97db03b49ea88014241",
    "semantic_title": "improving conversation-context language models with multiple spoken language understanding models",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chien19_interspeech.html": {
    "title": "Meta Learning for Hyperparameter Optimization in Dialogue System",
    "volume": "main",
    "abstract": "The performance of dialogue system based on deep reinforcement learning (DRL) highly depends on the selected hyperparameters in DRL algorithms. Traditionally, Gaussian process (GP) provides a probabilistic approach to Bayesian optimization for sequential search which is beneficial to select optimal hyperparameter. However, GP suffers from the expanding computation when the dimension of hyperparameters and the number of search points are increased. This paper presents a meta learning approach to carry out multifidelity Bayesian optimization where a two-level recurrent neural network (RNN) is developed for sequential learning and optimization. The search space is explored via the first-level RNN with cheap and low fidelity over a global region of hyperparameters. The optimization is then exploited and leveraged by the second-level RNN with a high fidelity on the successively small regions. The experiments on the hyperparameter optimization for dialogue system based on the deep Q network show the effectiveness and efficiency by using the proposed multifidelity Bayesian optimization",
    "checked": true,
    "id": "93e4b6acda7e725ffa4104e270273d96ba97e816",
    "semantic_title": "meta learning for hyperparameter optimization in dialogue system",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2019/williams19_interspeech.html": {
    "title": "Zero Shot Intent Classification Using Long-Short Term Memory Networks",
    "volume": "main",
    "abstract": "We describe a zero shot approach to intent classification that allows for the identification of intents that were not present during training. Our approach makes use of a Long-short Term Memory neural network to encode user queries and intents and uses these encodings to score previously unseen intents based on their semantic similarity to the queries. We test our model on intent classification in a personal digital assistant and show an improvement of 15% over a strong baseline. We also investigate the effect of adding a few training samples for the previously unseen intents in a few shot learning setting and show improvements of up to 16% over the baseline method",
    "checked": true,
    "id": "a40b648c13158b6d87744dcf94d40834653f56c8",
    "semantic_title": "zero shot intent classification using long-short term memory networks",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/korpusik19_interspeech.html": {
    "title": "A Comparison of Deep Learning Methods for Language Understanding",
    "volume": "main",
    "abstract": "In this paper, we compare a suite of neural networks (recurrent, convolutional, and the recently proposed BERT model) to a CRF with hand-crafted features on three semantic tagging corpora: the Air Travel Information System (ATIS) benchmark, restaurant queries, and written and spoken meal descriptions. Our motivation is to investigate pre-trained BERT's transferability to the domains we are interested in. We demonstrate that neural networks without feature engineering outperform state-of-the-art statistical and deep learning approaches on all three tasks (except written meal descriptions, where the CRF is slightly better) and that deep, attention-based BERT, in particular, surpasses state-of-the-art results on these tasks. Error analysis shows the models are less confident when making errors, enabling the system to follow up with the user when uncertain",
    "checked": true,
    "id": "684597268c6ac614439d4f0d5de1756a668fdb8a",
    "semantic_title": "a comparison of deep learning methods for language understanding",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kobayashi19_interspeech.html": {
    "title": "Slot Filling with Weighted Multi-Encoders for Out-of-Domain Values",
    "volume": "main",
    "abstract": "This paper proposes a new method for slot filling of out-of-domain (OOD) slot values, which are not included in the training data, in spoken dialogue systems. Word embeddings have been proposed to estimate the OOD slot values included in the word embedding model from keyword information. At the same time, context information is an important clue for estimation because the values in a given slot tend to appear in similar contexts. The proper use of either or both keyword and context information depends on the sentence. Conventional methods input a whole sentence into an encoder and extract important clues by the attention mechanism. However, it is difficult to properly distinguish context and keyword information from the encoder outputs because these two features are already mixed. Our proposed method uses two encoders, which distinctly encode contexts and keywords, respectively. The model calculates weights for the two encoders based on a user utterance and estimates a slot with weighted outputs from the two encoders. Experimental results show that the proposed method achieves a 50% relative improvement in F1 score compared with a baseline model, which detects slot values from user utterances and estimates slots at once with a single encoder",
    "checked": true,
    "id": "179d8f0960980e48baa50e9094878eccbf2908e4",
    "semantic_title": "slot filling with weighted multi-encoders for out-of-domain values",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/seneviratne19_interspeech.html": {
    "title": "Multi-Corpus Acoustic-to-Articulatory Speech Inversion",
    "volume": "main",
    "abstract": "There are several technologies like Electromagnetic articulometry (EMA), ultrasound, real-time Magnetic Resonance Imaging (MRI), and X-ray microbeam that are used to measure speech articulatory movements. Each of these techniques provides a different view of the vocal tract. The measurements performed using the similar techniques also differ greatly due to differences in the placement of sensors, and the anatomy of speakers. This limits most articulatory studies to single datasets. However to yield better results in its applications, the speech inversion systems should be more generalized, which requires the combination of data from multiple sources. This paper proposes a multi-task learning based deep neural network architecture for acoustic-to-articulatory speech inversion trained using three different articulatory datasets — two of them were measured using EMA, and one using X-ray microbeam. Experiments show improved accuracy of the proposed acoustic-to-articulatory mapping compared to the systems trained using single datasets",
    "checked": true,
    "id": "cc79683297c657277b5b82084e0e36134d03bac8",
    "semantic_title": "multi-corpus acoustic-to-articulatory speech inversion",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dash19_interspeech.html": {
    "title": "Towards a Speaker Independent Speech-BCI Using Speaker Adaptation",
    "volume": "main",
    "abstract": "Neurodegenerative diseases such as amyotrophic lateral sclerosis (ALS) can cause locked-in-syndrome (fully paralyzed but aware). Brain-computer interface (BCI) may be the only option to restore their communication. Current BCIs typically use visual or attention correlates in neural activities to select letters randomly displayed on a screen, which are extremely slow (a few words per minute). Speech-BCIs, which aim to convert the brain activity patterns to speech (neural speech decoding), hold the potential to enable faster communication. Although a few recent studies have shown the potential of neural speech decoding, those are focused on speaker-dependent models. In this study, we investigated speaker-independent neural speech decoding of five continuous phrases from Magnetoencephalography (MEG) signals while 8 subjects produced speech covertly (imagination) or overtly (articulation). We have used both supervised and unsupervised speaker adaptation strategies for implementing a speaker independent model. Experimental results demonstrated that the proposed adaptation-based speaker-independent model has significantly improved decoding performance. To our knowledge, this is the first demonstration of the possibility of speaker-independent neural speech decoding",
    "checked": true,
    "id": "f6e52681fb672dfc7f7ce422395d5b67d2687651",
    "semantic_title": "towards a speaker independent speech-bci using speaker adaptation",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sheth19_interspeech.html": {
    "title": "Identifying Input Features for Development of Real-Time Translation of Neural Signals to Text",
    "volume": "main",
    "abstract": "One of the main goals in Brain-Computer Interface (BCI) research is to help patients with faltering communication abilities due to neurodegenerative diseases produce text or speech output using their neural recordings. However, practical implementation of such a system has proven difficult due to limitations in the speed, accuracy, and training time of existing interfaces. In this paper, we contribute to this endeavour by isolating appropriate input features from speech-producing neural signals that will feed into a machine learning classifier to identify target phonemes. Analysing data from six subjects, we discern frequency bands that encapsulate differential information regarding production of vowels and consonants broadly, and more specifically nasals and semivowels. Subsequent spatial localization analysis reveals the underlying cortical regions responsible for different phoneme categories. Anatomical locations along with their respective frequency bands act as prospective feature sets for machine learning classifiers. We demonstrate this classification ability in a preliminary language reconstruction task and show an average word classification accuracy of 30.6% (p<0.001)",
    "checked": true,
    "id": "2e9dab07ec25a3b5bf032c170c99eccfa432dd74",
    "semantic_title": "identifying input features for development of real-time translation of neural signals to text",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/silva19_interspeech.html": {
    "title": "Exploring Critical Articulator Identification from 50Hz RT-MRI Data of the Vocal Tract",
    "volume": "main",
    "abstract": "The study of the static and dynamic aspects of speech production can profit from technologies such as electromagnetic midsagittal articulography (EMA) and real-time magnetic resonance (RTMRI). These can improve our knowledge on which articulators and gestures are involved in producing specific sounds and foster improved speech production models, paramount to advance, e.g., articulatory speech synthesis. Previous work, by the authors, has shown that critical articulator identification could be performed from RTMRI data of the vocal tract, with encouraging results, by extending the applicability of an unsupervised statistical identification method previously proposed for EMA data. Nevertheless, the slower time resolution of the considered RT-MRI corpus (14 Hz), when compared to EMA, potentially influencing the ability to select the most suitable representative configuration for each phone — paramount for strongly dynamic phones, e.g., nasal vowels —, and the lack of a richer set of contexts — relevant for observing coarticulation effects —, were identified as limitations. This article addresses these limitations by exploring critical articulator identification from a faster RTMRI corpus (50 Hz), for European Portuguese, providing a richer set of contexts, and testing how fusing the articulatory data of two speakers might influence critical articulator determination",
    "checked": true,
    "id": "a0b5fbc2de84d0050a8bbea9b120e37c131a4cb0",
    "semantic_title": "exploring critical articulator identification from 50hz rt-mri data of the vocal tract",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/douros19_interspeech.html": {
    "title": "Towards a Method of Dynamic Vocal Tract Shapes Generation by Combining Static 3D and Dynamic 2D MRI Speech Data",
    "volume": "main",
    "abstract": "We present an algorithm for augmenting the shape of the vocal tract using 3D static and 2D dynamic speech MRI data. While static 3D images have better resolution and provide spatial information, 2D dynamic images capture the transitions. The aim of this work is to combine strong points of these two types of data to obtain better image quality of 2D dynamic images and extend the 2D dynamic images to the 3D domain To produce a 3D dynamic consonant-vowel (CV) sequence, our algorithm takes as input the 2D CV transition and the static 3D targets for C and V. To obtain the enhanced sequence of images, the first step is to find a transformation between the 2D images and the mid-sagittal slice of the acoustically corresponding 3D image stack, and then find a transformation between neighbouring sagittal slices in the 3D static image stack. Combination of these transformations allows producing the final set of images. In the present study we first examined the transformation from the 3D mid-sagittal frame to the 2D video in order to improve image quality and then we examined the extension of the 2D video to the 3rd dimension with the aim to enrich spatial information",
    "checked": true,
    "id": "d67862fc963561190e929e778132e0e62c311903",
    "semantic_title": "towards a method of dynamic vocal tract shapes generation by combining static 3d and dynamic 2d mri speech data",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rasskazova19_interspeech.html": {
    "title": "Temporal Coordination of Articulatory and Respiratory Events Prior to Speech Initiation",
    "volume": "main",
    "abstract": "The investigation of the speech planning processes, in particular the timing between acoustic and articulatory onset, has recently received a lot of attention. Respiration has not been considered in this process so far, although it is involved and may be well coordinated with the oral articulators prior and at the onset of the utterance. In light of these considerations, we investigated the temporal coordination between acoustic, respiratory and articulatory events prior to utterance onset. For this purpose 12 native speakers of German have been recorded with Electromagnetic Articulography and Inductance Plethysmography reading sentences that were controlled for length and stress of the first word. The initial segment of the utterance was either /t/ or /n/. The results for six speakers so far indicate that early speech preparation consists of mouth opening during the inhalation phase. The onset of expiration seems to be tightly coupled with the acoustic and the articulatory onset, particularly with the constriction interval of the tongue tip gesture in the first segment. Manner of articulation of the initial segment seems to affect the temporal fine-tuning of preparatory events",
    "checked": true,
    "id": "bb5dbb9b1f2bd931dead024c171b014771e4f542",
    "semantic_title": "temporal coordination of articulatory and respiratory events prior to speech initiation",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gubian19b_interspeech.html": {
    "title": "Zooming in on Spatiotemporal V-to-C Coarticulation with Functional PCA",
    "volume": "main",
    "abstract": "It has long been proposed in speech production research that in CV sequences, the movement for consonant and vowel are initiated synchronously. However, mostly due to limitations on the statistical analysis of articulator motion over time, this could only be shown in a limited fashion, based on positional differences at a single time point during consonantal constriction formation. It is unknown to which extent this observation generalizes to earlier timepoints. In this paper, we illustrate the use of functional principal component analysis (FPCA) for the statistical analysis of articulator motion over time. Using articulography data, we quantify CV coarticulation during constriction formation of [k] in two vowel contexts. We show how FPCA enables us to analyse both horizontal and vertical movement components over time in a single model while preserving information on temporal variability. We combine FPCA with linear mixed modelling to obtain estimated mean trajectories and confidence bands for [k] in the two vowel contexts. Results show that well before the timepoint of peak velocity the vowel causes a substantial spatial separation of the consonantal trajectories, estimated to be at least 3 mm at peak velocity. This lends support to the hypothesis that vowel and consonant are initiated synchronously",
    "checked": true,
    "id": "80d9da86e9215c288cc900043bb1e61aee4e2bff",
    "semantic_title": "zooming in on spatiotemporal v-to-c coarticulation with functional pca",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/csapo19_interspeech.html": {
    "title": "Ultrasound-Based Silent Speech Interface Built on a Continuous Vocoder",
    "volume": "main",
    "abstract": "Recently it was shown that within the Silent Speech Interface (SSI) field, the prediction of F0 is possible from Ultrasound Tongue Images (UTI) as the articulatory input, using Deep Neural Networks for articulatory-to-acoustic mapping. Moreover, text-to-speech synthesizers were shown to produce higher quality speech when using a continuous pitch estimate, which takes non-zero pitch values even when voicing is not present. Therefore, in this paper on UTI-based SSI, we use a simple continuous F0 tracker which does not apply a strict voiced /unvoiced decision. Continuous vocoder parameters (ContF0, Maximum Voiced Frequency and Mel-Generalized Cepstrum) are predicted using a convolutional neural network, with UTI as input. The results demonstrate that during the articulatory-to-acoustic mapping experiments, the continuous F0 is predicted with lower error, and the continuous vocoder produces slightly more natural synthesized speech than the baseline vocoder using standard discontinuous F0",
    "checked": true,
    "id": "586d74f48dc077b9b4a5c538a6efb269fc153d99",
    "semantic_title": "ultrasound-based silent speech interface built on a continuous vocoder",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2019/klein19_interspeech.html": {
    "title": "Assessing Acoustic and Articulatory Dimensions of Speech Motor Adaptation with Random Forests",
    "volume": "main",
    "abstract": "Although most modern theories of speech production assume that representations of speech sounds are multidimensional encompassing acoustic and articulatory information, speech motor learning studies which assess the degree of adaptation in both dimensions are few and far between. In the current paper, we present an auditory perturbation study of German sibilant [s] in which speakers' audio and articulatory movements were recorded by means of electromagnetic articulography. Random Forest, a supervised learning algorithm, was employed to classify speakers' responses produced under unaltered or perturbed feedback based either on acoustic or articulatory parameters. Preliminary results demonstrate that while classification accuracy increases in the acoustic dimension as the perturbation session goes on, the classification accuracy in the articulatory dimension, although overall higher, remains approximately at the same level. This suggests that the adaptation process is characterized by active exploration of the articulatory space which is guided by speakers' auditory feedback",
    "checked": true,
    "id": "5a453c06c86ceb6cc71da04846ac5472f0a8f22f",
    "semantic_title": "assessing acoustic and articulatory dimensions of speech motor adaptation with random forests",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/takemoto19_interspeech.html": {
    "title": "Speech Organ Contour Extraction Using Real-Time MRI and Machine Learning Method",
    "volume": "main",
    "abstract": "Real-time MRI can be used to obtain videos that describe articulatory movements during running speech. For detailed analysis based on a large number of video frames, it is necessary to extract the contours of speech organs, such as the tongue, semi-automatically. The present study attempted to extract the contours of speech organs from videos using a machine learning method. First, an expert operator manually extracted the contours from the frames of a video to build training data sets. The learning operators, or learners, then extracted the contours from each frame of the video. Finally, the errors representing the geometrical distance between the extracted contours and the ground truth, which were the contours excluded from the training data sets, were examined. The results showed that the contours extracted using machine learning were closer to the ground truth than the contours traced by other expert and non-expert operators. In addition, using the same learners, the contours were extracted from other naive videos obtained during different speech tasks of the same subject. As a result, the errors in those videos were similar to those in the video in which the learners were trained",
    "checked": true,
    "id": "d473df3cdaab94de1083256f4d764e77439b05e8",
    "semantic_title": "speech organ contour extraction using real-time mri and machine learning method",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/leeuwen19_interspeech.html": {
    "title": "CNN-Based Phoneme Classifier from Vocal Tract MRI Learns Embedding Consistent with Articulatory Topology",
    "volume": "main",
    "abstract": "Recent advances in real-time magnetic resonance imaging (rtMRI) of the vocal tract provides opportunities for studying human speech. This modality together with acquired speech may enable the mapping of articulatory configurations to acoustic features. In this study, we take the first step by training a deep learning model to classify 27 different phonemes from midsagittal MR images of the vocal tract An American English database was used to train a convolutional neural network for classifying vowels (13 classes), consonants (14 classes) and all phonemes (27 classes) of 17 subjects. Classification top-1 accuracy of the test set for all phonemes was 57%. Error analysis showed voiced and unvoiced sounds often being confused. Moreover, we performed principal component analysis on the network's embedding and observed topological similarities between the network learned representation and the vowel diagram. Saliency maps gave insight into the anatomical regions most important for classification and show congruence with known regions of articulatory importance We demonstrate the feasibility for deep learning to distinguish between phonemes from MRI. Network analysis can be used to improve understanding of normal articulation and speech and, in the future, impaired speech. This study brings us a step closer to the articulatory-to-acoustic mapping from rtMRI",
    "checked": true,
    "id": "fe8c88ce388e17d9dd2bc3485437464dc7f631ea",
    "semantic_title": "cnn-based phoneme classifier from vocal tract mri learns embedding consistent with articulatory topology",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mucke19_interspeech.html": {
    "title": "Strength and Structure: Coupling Tones with Oral Constriction Gestures",
    "volume": "main",
    "abstract": "According to the segmental anchor hypothesis within the Autosegmental-Metrical approach, tones are aligned with segmental boundaries of consonant and vowels in the acoustic domain. In prenuclear rising pitch accents (LH*), the rise is assumed to occur in the vicinity of the accented syllable it is phonologically associated with. However, there are differences in the alignment patterns within and across languages that cannot be captured within the AM approach. In the present study, we investigate the coordination of tonal and oral constriction gestures within Articulatory Phonology. Therefore, we model the coordination of prenuclear LH* pitch accents in Catalan, Northern and Southern German with respect to syllable production on the basis of recordings with a 2D electromagnetic articulography. We provide an extended coupled oscillators model that allows for balanced and imbalanced coupling strengths. Based on examples, we show that the observed differences in alignment patterns for prenuclear rising pitch accents can be modelled with the same underlying coordinative structures/coupling modes for vocalic and tonal gestures and that surface differences arise from gradient variation in coupling strengths",
    "checked": true,
    "id": "3ebf743f94ad9537ec8d7d8db1474bed5a797b33",
    "semantic_title": "strength and structure: coupling tones with oral constriction gestures",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kleijn19_interspeech.html": {
    "title": "Salient Speech Representations Based on Cloned Networks",
    "volume": "main",
    "abstract": "We define salient features as features that are shared by signals that are defined as being equivalent by a system designer. The definition allows the designer to contribute qualitative information. We aim to find salient features that are useful as conditioning for generative networks. We extract salient features by jointly training a set of clones of an encoder network. Each network clone receives as input a different signal from a set of equivalent signals. The objective function encourages the network clones to map their input into a set of features that is identical across the clones. It additionally encourages feature independence and, optionally, reconstruction of a desired target signal by a decoder. As an application, we train a system that extracts a time-sequence of feature vectors of speech and uses it as a conditioning of a WaveNet generative system, facilitating both coding and enhancement",
    "checked": true,
    "id": "fa5c2be217b6592aebbc7726014692434d0d58dc",
    "semantic_title": "salient speech representations based on cloned networks",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ramanathi19_interspeech.html": {
    "title": "ASR Inspired Syllable Stress Detection for Pronunciation Evaluation Without Using a Supervised Classifier and Syllable Level Features",
    "volume": "main",
    "abstract": "Automatic syllable stress detection is typically performed with a supervised classifier considering manually annotated stress markings and features computed within the syllable segments derived from phoneme transcriptions and their time-aligned boundaries. However, the manual annotation is tedious and the errors in estimating segmental information could degrade stress detection accuracy. In order to circumvent these, we propose to estimate stress markings in automatic speech recognition (ASR) framework involving finite-state-transducer (FST) without using annotated stress markings and segmental information. For this, we train the ASR system with native English data along with pronunciation lexicon containing canonical stress markings and decode non-native utterances as pronunciations embedded with stress markings. In the decoding, we use an FST encoded with the pronunciations derived using phoneme transcriptions and the instructions involved in a typical manual annotation. Experiments are conducted on polysyllabic words taken from ISLE corpus containing utterances spoken by Italian and German speakers and using the ASR models trained with three corpora. Among all the three models, the highest stress detection accuracies with the proposed approach respectively on Italian & German speakers are found to be 2.07% & 1.19% higher than and comparable to those with the two supervised classification approaches used as baselines",
    "checked": true,
    "id": "ead6031f9b40ec5d259b5ec7cfac8ee6f9ea0a29",
    "semantic_title": "asr inspired syllable stress detection for pronunciation evaluation without using a supervised classifier and syllable level features",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mannem19_interspeech.html": {
    "title": "Acoustic and Articulatory Feature Based Speech Rate Estimation Using a Convolutional Dense Neural Network",
    "volume": "main",
    "abstract": "In this paper, we propose a speech rate estimation approach using a convolutional dense neural network (CDNN). The CDNN based approach uses the acoustic and articulatory features for speech rate estimation. The Mel Frequency Cepstral Coefficients (MFCCs) are used as acoustic features and the articulograms representing time-varying vocal tract profile are used as articulatory features. The articulogram is computed from a real-time magnetic resonance imaging (rtMRI) video in the midsagittal plane of a subject while speaking. However, in practice, the articulogram features are not directly available, unlike acoustic features from speech recording. Thus, we use an Acoustic-to-Articulatory Inversion method using a bidirectional long-short-term memory network which estimates the articulogram features from the acoustics. The proposed CDNN based approach using estimated articulatory features requires both acoustic and articulatory features during training but it requires only acoustic data during testing. Experiments are conducted using rtMRI videos from four subjects each speaking 460 sentences. The Pearson correlation coefficient is used to evaluate the speech rate estimation. It is found that the CDNN based approach gives a better correlation coefficient than the temporal and selected sub-band correlation (TCSSBC) based baseline scheme by 81.58% and 73.68% (relative) in seen and unseen subject conditions respectively",
    "checked": true,
    "id": "d78b4ebd047a0d38dfbcfadb53d9666e08dd6a47",
    "semantic_title": "acoustic and articulatory feature based speech rate estimation using a convolutional dense neural network",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/springenberg19_interspeech.html": {
    "title": "Predictive Auxiliary Variational Autoencoder for Representation Learning of Global Speech Characteristics",
    "volume": "main",
    "abstract": "Unsupervised learning represents an important opportunity for obtaining useful speech representations. Recently, variational autoencoders (VAEs) have been shown to extract useful representations in an unsupervised manner. These models are usually not designed to explicitly disentangle specific sources of information. When processing data of sequential nature which involves multi-timescale information, disentanglement can however be beneficial. In this paper we address this issue by developing a predictive auxiliary variational autoencoder to obtain speech representations at different timescales. We will present an auxiliary lower bound which is used to develop a model that we call the Predictive Aux-VAE. The model is designed to disentangle global from local information into a dedicated auxiliary variable. Learned representations are analysed with respect to their ability to capture global speech characteristics. We observe that representations of individual speakers are separated well in the latent space and can successfully be used in a subsequent speaker identification task where they achieve high classification accuracy, comparable to a fully supervised model. Moreover, manipulating the global variable allows to change global characteristics while retaining the local content during generation which demonstrates the success of our model to disentangle global from local information",
    "checked": true,
    "id": "bc70d4c7d375986e79875cae2d714ea0521fbd5a",
    "semantic_title": "predictive auxiliary variational autoencoder for representation learning of global speech characteristics",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2019/paraskevopoulos19_interspeech.html": {
    "title": "Unsupervised Low-Rank Representations for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "We examine the use of linear and non-linear dimensionality reduction algorithms for extracting low-rank feature representations for speech emotion recognition. Two feature sets are used, one based on low-level descriptors and their aggregations (IS10) and one modeling recurrence dynamics of speech (RQA), as well as their fusion. We report speech emotion recognition (SER) results for learned representations on two databases using different classification methods. Classification with low-dimensional representations yields performance improvement in a variety of settings. This indicates that dimensionality reduction is an effective way to combat the curse of dimensionality for SER. Visualization of features in two dimensions provides insight into discriminatory abilities of reduced feature sets",
    "checked": true,
    "id": "eebd7e974f84c49edbee62b1c1c8b2557d254b9e",
    "semantic_title": "unsupervised low-rank representations for speech emotion recognition",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dhiman19_interspeech.html": {
    "title": "On the Suitability of the Riesz Spectro-Temporal Envelope for WaveNet Based Speech Synthesis",
    "volume": "main",
    "abstract": "We address the problem of estimating the time-varying spectral envelope of a speech signal using a spectro-temporal demodulation technique. Unlike the conventional spectrogram, we consider a pitch-adaptive spectrogram and model a spectro-temporal patch using an amplitude- and frequency-modulated two-dimensional (2-D) cosine signal. We employ a demodulation technique based on the Riesz transform that we proposed recently to estimate the amplitude and frequency modulations. The amplitude modulation (AM) corresponds to the vocal-tract filter magnitude response (or envelope) and the frequency modulation (FM) corresponds to the excitation. We consider the AM and demonstrate its effectiveness by incorporating it as an acoustic feature for local conditioning in the statistical WaveNet vocoder for the task of speech synthesis. The quality of the synthesized speech obtained with the Riesz envelope is compared with that obtained using the envelope estimated by the WORLD vocoder. Objective measures and subjective listening tests on the CMU-Arctic database show that the quality of synthesis is superior to that obtained using the WORLD envelope. This study thus establishes the Riesz envelope as an efficient alternative to the WORLD envelope",
    "checked": true,
    "id": "543da25d1d63ac5d607181021d2b2d5ba35b660e",
    "semantic_title": "on the suitability of the riesz spectro-temporal envelope for wavenet based speech synthesis",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xu19_interspeech.html": {
    "title": "Autonomous Emotion Learning in Speech: A View of Zero-Shot Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Conventionally, speech emotion recognition is achieved using passive learning approaches. Differing from such approaches, we herein propose and develop a dynamic method of autonomous emotion learning based on zero-shot learning. The proposed methodology employs emotional dimensions as the attributes in the zero-shot learning paradigm, resulting in two phases of learning, namely attribute learning and label learning. Attribute learning connects the paralinguistic features and attributes utilising speech with known emotional labels, while label learning aims at defining unseen emotions through the attributes. The experimental results achieved on the CINEMO corpus indicate that zero-shot learning is a useful technique for autonomous speech-based emotion learning, achieving accuracies considerably better than chance level and an attribute-based gold-standard setup. Furthermore, different emotion recognition tasks, emotional attributes, and employed approaches strongly influence system performance",
    "checked": true,
    "id": "98960684fc96c0f9c2594bec3b69e59bda567713",
    "semantic_title": "autonomous emotion learning in speech: a view of zero-shot speech emotion recognition",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sudhakara19_interspeech.html": {
    "title": "An Improved Goodness of Pronunciation (GoP) Measure for Pronunciation Evaluation with DNN-HMM System Considering HMM Transition Probabilities",
    "volume": "main",
    "abstract": "Goodness of pronunciation (GoP) is typically formulated with Gaussian mixture model-hidden Markov model (GMM-HMM) based acoustic models considering HMM state transition probabilities (STPs) and GMM likelihoods of context dependent phonemes. On the other hand, deep neural network (DNN)-HMM based acoustic models employed sub-phonemic (senone) posteriors instead of GMM likelihoods along with STPs. However, each senone is shared across many states; thus, there is no one-to-one correspondence between them. In order to circumvent this, most of the existing works have proposed modifications to the GoP formulation considering only posteriors neglecting the STPs. In this work, we derive a formulation for the GoP and it results in the formulation involving both senone posteriors and STPs. Further, we illustrate the steps to implement the proposed GoP formulation in Kaldi, a state-of-the-art automatic speech recognition toolkit. Experiments are conducted on English data collected from Indian speakers using acoustic models trained with native English data from LibriSpeech and Fisher-English corpora. The highest improvement in the correlation coefficient between the scores from the formulations and the expert ratings is found to be 14.89% (relative) better with the proposed approach compared to the best of the existing formulations that don't include STPs",
    "checked": true,
    "id": "aa07cb8a956ba9f7bf7a425997bfcb37a0e4b6cc",
    "semantic_title": "an improved goodness of pronunciation (gop) measure for pronunciation evaluation with dnn-hmm system considering hmm transition probabilities",
    "citation_count": 41
  },
  "https://www.isca-speech.org/archive/interspeech_2019/saha19b_interspeech.html": {
    "title": "Low Resource Automatic Intonation Classification Using Gated Recurrent Unit (GRU) Networks Pre-Trained with Synthesized Pitch Patterns",
    "volume": "main",
    "abstract": "Second language learners of British English (BE) are typically trained to learn four intonation classes — Glide-up, Glide-down, Dive and Take-off. We predict the intonation class in a learner's utterance by modeling the temporal dependencies in the pitch patterns with gated recurrent unit (GRU) networks. For these, we pre-train the GRU network using a set of synthesized pitch patterns representing each intonation class. For the synthesis, we propose to obtain pitch patterns from the tone sequences representing each intonation class obtained from domain knowledge. Experiments are conducted on speech data collected from experts in a spoken English training material for teaching BE intonation. The absolute improvements in the unweighted average recall (UAR) using the proposed scheme with pre-training are found to be 4.14% and 6.01% respectively over the proposed approach without pre-training and the baseline scheme that uses hidden Markov models (HMMs)",
    "checked": true,
    "id": "acee10c113dacb578d603a69f091dcb688886482",
    "semantic_title": "low resource automatic intonation classification using gated recurrent unit (gru) networks pre-trained with synthesized pitch patterns",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vasquezcorrea19b_interspeech.html": {
    "title": "Apkinson: A Mobile Solution for Multimodal Assessment of Patients with Parkinson's Disease",
    "volume": "main",
    "abstract": "Parkinson's disease is a neurological disorder that produces different motor impairments in the patients. The longitudinal assessment of the neurological state of patients is important to improve their quality of life. We introduced Apkinson, a smartphone application to evaluate continuously the speech and movement deficits of Parkinson's patients, who receive feedback about their current state after performing different exercises. The speech assessment considers phonation, articulation, and prosody capabilities of the patients. Movement exercises captured with the inertial sensors of the smartphone evaluated symptoms in the upper and lower limbs",
    "checked": true,
    "id": "dea4695e88ce239222a76605e6db34cd790b5459",
    "semantic_title": "apkinson: a mobile solution for multimodal assessment of patients with parkinson's disease",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kiss19_interspeech.html": {
    "title": "Depression State Assessment: Application for Detection of Depression by Speech",
    "volume": "main",
    "abstract": "We present an application that detects depression by speech based on a speech feature extraction engine. The input of the application is a read speech sample and the output is predicted depression severity level (Beck Depression Inventory). The application analyses the speech sample and evaluates it using support vector regression (SVR). The developed system could assist general medical staff if no specialist is present to aid the diagnosis. If there is a suspicion that the speaker is suffering from depression, it is inevitable to seek special medical assistance. The application supports five native languages: English, French, German, Hungarian and Italian",
    "checked": true,
    "id": "899c31418f8ba698e97bd97a2a113300327942e7",
    "semantic_title": "depression state assessment: application for detection of depression by speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yarra19_interspeech.html": {
    "title": "SPIRE-fluent: A Self-Learning App for Tutoring Oral Fluency to Second Language English Learners",
    "volume": "main",
    "abstract": "Second language (L2) learners often achieve oral fluency by correct pronunciation of words with appropriate pauses. It has been shown that the L2 learners improve their language skills using mobile apps in a self-learning manner. Effective learning is possible with apps that provide detailed feedback. However, apps that train oral fluency in an automatic way are not available. In this work, we present SPIRE-fluent app, which provides an automatic feedback with scores representing learner's pronunciation quality, for each word in a sentence and for the entire sentence. The word specific scores are computed based on the correctness of pronunciation with respect to the expert's audio. Further, the app displays the syllables uttered and a set of two types of pauses produced by the learners and the expert while speaking the sentence. Considering this as a feedback, the learner can correct their mistakes based on the mismatches between those utterances. In addition, it also estimates any pause made by the learners within a word and highlights the syllable containing the phoneme preceding the pause",
    "checked": true,
    "id": "f8798fa1886254d9a549968c9eb0f3b6d0a0ca79",
    "semantic_title": "spire-fluent: a self-learning app for tutoring oral fluency to second language english learners",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nissen19b_interspeech.html": {
    "title": "Using Real-Time Visual Biofeedback for Second Language Instruction",
    "volume": "main",
    "abstract": "This demonstration will illustrate how using real-time visual biofeedback, through a relatively new type of electropalatographic (EPG) sensor, might facilitate improved pronunciation for learners of a second language (L2). The manner in which the EPG sensor is created and its use to track lingua-palatal articulation patterns will be described to individuals. This presentation will also include an explanation of how a student can visualize the contact patterns of their speech using the associated instructional software. A brief tutorial on the features of the instructional software will also be explained during the \"show and tell\" presentation",
    "checked": true,
    "id": "3016e98fff247bcba150ffa3aa326a39ea1dd0f8",
    "semantic_title": "using real-time visual biofeedback for second language instruction",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/miwardelli19_interspeech.html": {
    "title": "Splash: Speech and Language Assessment in Schools and Homes",
    "volume": "main",
    "abstract": "This paper presents a tablet-based app for Speech and Language Assessment in Schools and Homes ( Splash) to provide a first screening for young children aged 4–6 years to assess their speech and language skills. The app aims to be easy-to-administer with an adult, such as a teacher or parent, directing the child through the tasks. Three fun games have been developed to assess receptive language, expressive language and connected speech, respectively. Currently in proof-of-concept mode, when complete Splash will use automatic spoken language processing to give an instant estimate of a child's communication ability and provide guidance on whether to speak specialist support. While not a diagnostic tool, the aim is for Splash to be used to provide immediate reassurance or direction to concerned parents, guardians or teachers as it can be administered by anyone, anywhere",
    "checked": true,
    "id": "f45b3de380ebedd00262246e3c53b136e6891fae",
    "semantic_title": "splash: speech and language assessment in schools and homes",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/annand19_interspeech.html": {
    "title": "Using Ultrasound Imaging to Create Augmented Visual Biofeedback for Articulatory Practice",
    "volume": "main",
    "abstract": "Ultrasound images of the tongue surface can be used to provide real-time visual feedback for clinical practitioners and speakers adjusting pronunciation patterns. However, rapid and complex movements of the tongue can be difficult to interpret and directly relate to desired changes. We are developing a method for simplified visual feedback controlled by efficient, real-time tracking of tongue contours in ultrasound images. Our feedback and control paradigm are briefly discussed, and video of a potential game-like biofeedback stimulus is demonstrated",
    "checked": true,
    "id": "829cfd92b8745b3a69467a6a1327a77fd774e46e",
    "semantic_title": "using ultrasound imaging to create augmented visual biofeedback for articulatory practice",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2019/radostev19_interspeech.html": {
    "title": "Speech-Based Web Navigation for Limited Mobility Users",
    "volume": "main",
    "abstract": "We present a novel approach that introduces the strengths of voice assistants into a web browser that makes the task of web navigation a lot more accessible to all users, especially under limited mobility circumstances. Voice assistants have now been widely adopted and is providing great user experience for getting simple actions done quickly or getting a quick answer to a question. On the other hand, the benefits of voice assistants have not yet penetrated to the scenarios such as web navigation, which has so far been driven by mouse, keyboard and touch-based input only. In this paper, we demonstrate our speech-based web navigation system, and show that our system improves the completion of the web navigation task on both PC and mobile phone significantly as compared with an out-of-the-box voice assistants on this task",
    "checked": true,
    "id": "6d00006d18a1a9348b08434440e90a9b26c577c8",
    "semantic_title": "speech-based web navigation for limited mobility users",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schultz19_interspeech.html": {
    "title": "Biosignal Processing for Human-Machine Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ryant19_interspeech.html": {
    "title": "The Second DIHARD Diarization Challenge: Dataset, Task, and Baselines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/singh19_interspeech.html": {
    "title": "LEAP Diarization System for the Second DIHARD Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vinals19_interspeech.html": {
    "title": "ViVoLAB Speaker Diarization System for the DIHARD 2019 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zajic19_interspeech.html": {
    "title": "UWB-NTIS Speaker Diarization System for the DIHARD II 2019 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/park19b_interspeech.html": {
    "title": "The Second DIHARD Challenge: System Description for USC-SAIL Team",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/novoselov19_interspeech.html": {
    "title": "Speaker Diarization with Deep Speaker Embeddings for DIHARD Challenge II",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/todisco19_interspeech.html": {
    "title": "ASVspoof 2019: Future Horizons in Spoofed and Fake Audio Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lai19b_interspeech.html": {
    "title": "ASSERT: Anti-Spoofing with Squeeze-Excitation and Residual Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chettri19_interspeech.html": {
    "title": "Ensemble Models for Spoofing Detection in Automatic Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cai19_interspeech.html": {
    "title": "The DKU Replay Detection System for the ASVspoof 2019 Challenge: On Data Augmentation, Feature Representation, Classification, and Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/biaobrzeski19_interspeech.html": {
    "title": "Robust Bayesian and Light Neural Networks for Voice Spoofing Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lavrentyeva19_interspeech.html": {
    "title": "STC Antispoofing Systems for the ASVspoof2019 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yang19b_interspeech.html": {
    "title": "The SJTU Robust Anti-Spoofing System for the ASVspoof 2019 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/alluri19_interspeech.html": {
    "title": "IIIT-H Spoofing Countermeasures for Automatic Speaker Verification Spoofing and Countermeasures Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19c_interspeech.html": {
    "title": "Anti-Spoofing Speaker Verification System with Multi-Feature Integration and Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/williams19b_interspeech.html": {
    "title": "Speech Replay Detection with x-Vector Attack Embeddings and Spectral Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/das19_interspeech.html": {
    "title": "Long Range Acoustic Features for Spoofed Speech Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chang19b_interspeech.html": {
    "title": "Transfer-Representation Learning for Detecting Spoofing Attacks with Converted and Synthesized Speech in Automatic Speaker Verification System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gomezalanis19_interspeech.html": {
    "title": "A Light Convolutional GRU-RNN Deep Feature Extractor for ASV Spoofing Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zeinali19_interspeech.html": {
    "title": "Detecting Spoofing Attacks Using VGG and SincNet: BUT-Omilia Submission to ASVspoof 2019 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/alzantot19_interspeech.html": {
    "title": "Deep Residual Neural Networks for Audio Spoofing Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jung19_interspeech.html": {
    "title": "Replay Attack Detection with Complementary High-Resolution Information Using End-to-End DNN for the ASVspoof 2019 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dunbar19_interspeech.html": {
    "title": "The Zero Resource Speech Challenge 2019: TTS Without T",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/feng19b_interspeech.html": {
    "title": "Combining Adversarial Training and Disentangled Speech Representation for Robust Zero-Resource Subword Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yusuf19_interspeech.html": {
    "title": "Temporally-Aware Acoustic Unit Discovery for Zerospeech 2019 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/eloff19_interspeech.html": {
    "title": "Unsupervised Acoustic Unit Discovery for Speech Synthesis Using Discrete Latent-Variable Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19c_interspeech.html": {
    "title": "Unsupervised End-to-End Learning of Discrete Linguistic Units for Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/s19_interspeech.html": {
    "title": "Zero Resource Speech Synthesis Using Transcripts Derived from Perceptual Acoustic Units",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tjandra19_interspeech.html": {
    "title": "VQVAE Unsupervised Unit Discovery and Multi-Scale Code2Spec Inverter for Zerospeech Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/niehues19_interspeech.html": {
    "title": "Survey Talk: A Survey on Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jia19_interspeech.html": {
    "title": "Direct Speech-to-Speech Translation with a Sequence-to-Sequence Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19d_interspeech.html": {
    "title": "End-to-End Speech Translation with Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gangi19_interspeech.html": {
    "title": "Adapting Transformer to End-to-End Spoken Language Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hillis19_interspeech.html": {
    "title": "Unsupervised Phonetic and Word Level Discovery for Speech to Speech Translation for Unwritten Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bhattacharya19_interspeech.html": {
    "title": "Deep Speaker Recognition: Modular or Monolithic?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19d_interspeech.html": {
    "title": "On the Usage of Phonetic Information for Text-Independent Speaker Embedding Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ravanelli19_interspeech.html": {
    "title": "Learning Speaker Representations with Mutual Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/you19_interspeech.html": {
    "title": "Multi-Task Learning with High-Order Statistics for x-Vector Based Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wu19e_interspeech.html": {
    "title": "Data Augmentation Using Variational Autoencoder for Embedding Based Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/you19b_interspeech.html": {
    "title": "Deep Neural Network Embeddings with Gating Mechanisms for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bhat19_interspeech.html": {
    "title": "Neural Transition Systems for Modeling Hierarchical Semantic Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vukotic19_interspeech.html": {
    "title": "Mining Polysemous Triplets with Recurrent Neural Networks for Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ray19_interspeech.html": {
    "title": "Iterative Delexicalization for Improved Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bhosale19_interspeech.html": {
    "title": "End-to-End Spoken Language Understanding: Bootstrapping in Low Resource Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/takatsu19_interspeech.html": {
    "title": "Recognition of Intentions of Users' Short Responses for Conversational News Delivery System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/caubriere19_interspeech.html": {
    "title": "Curriculum-Based Transfer Learning for an Effective End-to-End Spoken Language Understanding and Domain Portability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dash19b_interspeech.html": {
    "title": "Spatial and Spectral Fingerprint in the Brain: Speaker Identification from Single Trial MEG Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nijveld19_interspeech.html": {
    "title": "ERP Signal Analysis with Temporal Resolution Using a Time Window Bank",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bosch19_interspeech.html": {
    "title": "Phase Synchronization Between EEG Signals as a Function of Differences Between Stimuli Characteristics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kharaman19_interspeech.html": {
    "title": "The Processing of Prosodic Cues to Rhetorical Question Interpretation: Psycholinguistic and Neurolinguistics Evidence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/scharenborg19_interspeech.html": {
    "title": "The Neural Correlates Underlying Lexically-Guided Perceptual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/parmonangan19_interspeech.html": {
    "title": "Speech Quality Evaluation of Synthesized Japanese Speech Using EEG",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19d_interspeech.html": {
    "title": "Multi-Microphone Adaptive Noise Cancellation for Robust Hotword Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhao19c_interspeech.html": {
    "title": "Multi-Task Multi-Network Joint-Learning of Deep Residual Networks and Cycle-Consistency Generative Adversarial Networks for Robust Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/khokhlov19_interspeech.html": {
    "title": "R-Vectors: New Technique for Adaptation to Room Acoustics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kanda19b_interspeech.html": {
    "title": "Guided Source Separation Meets a Strong ASR Backend: Hitachi/Paderborn University Joint Investigation for Dinner Party ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/drude19_interspeech.html": {
    "title": "Unsupervised Training of Neural Mask-Based Beamforming",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ma19_interspeech.html": {
    "title": "Acoustic Model Ensembling Using Effective Data Augmentation for CHiME-5 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19d_interspeech.html": {
    "title": "Survey Talk: End-to-End Deep Neural Network Based Speaker and Language Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/padi19_interspeech.html": {
    "title": "Attention Based Hybrid i-Vector BLSTM Model for Language Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jung19b_interspeech.html": {
    "title": "RawNet: Advanced End-to-End Deep Neural Network Using Raw Waveforms for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rao19_interspeech.html": {
    "title": "Target Speaker Extraction for Multi-Talker Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mazzawi19_interspeech.html": {
    "title": "Improving Keyword Spotting and Language Identification via Neural Architecture Search at Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zheng19_interspeech.html": {
    "title": "Forward-Backward Decoding for Regularizing End-to-End TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/guo19b_interspeech.html": {
    "title": "A New GAN-Based End-to-End TTS Training Algorithm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/he19_interspeech.html": {
    "title": "Robust Sequence-to-Sequence Acoustic Modeling with Stepwise Monotonic Attention for Neural TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19b_interspeech.html": {
    "title": "Joint Training Framework for Text-to-Speech and Voice Conversion Using Multi-Source Tacotron and WaveNet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/luong19_interspeech.html": {
    "title": "Training Multi-Speaker Neural Text-to-Speech Systems Using Speaker-Imbalanced Speech Corpora",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/okamoto19_interspeech.html": {
    "title": "Real-Time Neural Text-to-Speech with Sequence-to-Sequence Acoustic Model and WaveGlow or Single Gaussian WaveRNN Vocoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kafle19_interspeech.html": {
    "title": "Fusion Strategy for Prosodic and Lexical Representations of Word Importance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chien19b_interspeech.html": {
    "title": "Self Attention in Variational Sequential Learning for Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sun19_interspeech.html": {
    "title": "Multi-Modal Sentiment Analysis Using Deep Canonical Correlation Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shen19_interspeech.html": {
    "title": "Interpreting and Improving Deep Neural SLU Models via Vocabulary Importance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tundik19_interspeech.html": {
    "title": "Assessing the Semantic Space Bias Caused by ASR Error Propagation and its Effect on Spoken Document Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19e_interspeech.html": {
    "title": "Latent Topic Attention for Domain Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/narisetty19_interspeech.html": {
    "title": "A Unified Bayesian Source Modelling for Determined Blind Source Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/takahashi19_interspeech.html": {
    "title": "Recursive Speech Separation for Unknown Number of Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/appeltans19_interspeech.html": {
    "title": "Practical Applicability of Deep Neural Networks for Overlapping Speaker Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gu19_interspeech.html": {
    "title": "Speech Separation Using Independent Vector Analysis with an Amplitude Variable Gaussian Mixture Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yang19c_interspeech.html": {
    "title": "Improved Speech Separation with Time-and-Frequency Cross-Domain Joint Embedding and Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wichern19_interspeech.html": {
    "title": "WHAM!: Extending Speech Separation to Noisy Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nautsch19_interspeech.html": {
    "title": "Survey Talk: Preserving Privacy in Speaker and Speech Characterisation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chermaz19_interspeech.html": {
    "title": "Evaluating Near End Listening Enhancement Algorithms in Realistic Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/edraki19_interspeech.html": {
    "title": "Improvement and Assessment of Spectro-Temporal Modulation Analysis for Speech Intelligibility Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19c_interspeech.html": {
    "title": "Listener Preference on the Local Criterion for Ideal Binary-Masked Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dinh19_interspeech.html": {
    "title": "Using a Manifold Vocoder for Spectral Voice and Style Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/platen19_interspeech.html": {
    "title": "Multi-Span Acoustic Modelling Using Raw Waveform Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/merboldt19_interspeech.html": {
    "title": "An Analysis of Local Monotonic Attention Variants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sun19b_interspeech.html": {
    "title": "Layer Trajectory BLSTM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/karita19_interspeech.html": {
    "title": "Improving Transformer-Based End-to-End Speech Recognition with Connectionist Temporal Classification and Language Model Integration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19d_interspeech.html": {
    "title": "Trainable Dynamic Subsampling for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhao19d_interspeech.html": {
    "title": "Shallow-Fusion End-to-End Contextual Biasing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nasir19_interspeech.html": {
    "title": "Modeling Interpersonal Linguistic Coordination in Conversations Using Word Mover's Distance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/du19_interspeech.html": {
    "title": "Bag-of-Acoustic-Words for Mental Health Assessment: A Deep Autoencoding Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/voleti19_interspeech.html": {
    "title": "Objective Assessment of Social Skills Using Automated Language Analysis for Identification of Schizophrenia and Bipolar Disorder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/matton19_interspeech.html": {
    "title": "Into the Wild: Transitioning from Recognizing Mood in Clinical Interactions to Personal Conversations for Individuals with Bipolar Disorder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rohanian19_interspeech.html": {
    "title": "Detecting Depression with Word-Level Multimodal Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/espywilson19_interspeech.html": {
    "title": "Assessing Neuromotor Coordination in Depression Using Inverted Vocal Tract Variables",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/paul19b_interspeech.html": {
    "title": "Towards Universal Dialogue Act Tagging for Task-Oriented Dialogues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/goel19_interspeech.html": {
    "title": "HyST: A Hybrid Approach for Flexible and Accurate Dialogue State Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/martinek19_interspeech.html": {
    "title": "Multi-Lingual Dialogue Act Recognition with Deep Learning Methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chao19_interspeech.html": {
    "title": "BERT-DST: Scalable End-to-End Dialogue State Tracking with Bidirectional Encoder Representations from Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/griol19_interspeech.html": {
    "title": "Discovering Dialog Rules by Means of an Evolutionary Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19c_interspeech.html": {
    "title": "Active Learning for Domain Classification in a Commercial Spoken Personal Assistant",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sadjadi19_interspeech.html": {
    "title": "The 2018 NIST Speaker Recognition Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/villalba19_interspeech.html": {
    "title": "State-of-the-Art Speaker Recognition for Telephone and Video Speech: The JHU-MIT Submission for NIST SRE18",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/garciaromero19_interspeech.html": {
    "title": "x-Vector DNN Refinement with Full-Length Recordings for Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lee19_interspeech.html": {
    "title": "I4U Submission to NIST SRE 2018: Leveraging from a Decade of Shared Experiences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/khoury19_interspeech.html": {
    "title": "Pindrop Labs' Submission to the First Multi-Target Speaker Detection and Identification Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/garciaromero19b_interspeech.html": {
    "title": "Speaker Recognition Benchmark Using the CHiME-5 Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ayllon19_interspeech.html": {
    "title": "Investigating the Effects of Noisy and Reverberant Speech in Text-to-Speech Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kuo19_interspeech.html": {
    "title": "Selection and Training Schemes for Improving TTS Voice Built on Found Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/braude19_interspeech.html": {
    "title": "All Together Now: The Living Audio Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zen19_interspeech.html": {
    "title": "LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shamsi19_interspeech.html": {
    "title": "Corpus Design Using Convolutional Auto-Encoder Embeddings for Audio-Book Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hojo19_interspeech.html": {
    "title": "Evaluating Intention Communication by TTS Using Explicit Definitions of Illocutionary Act Performance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lo19_interspeech.html": {
    "title": "MOSNet: Deep Learning-Based Objective Assessment for Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fong19_interspeech.html": {
    "title": "Investigating the Robustness of Sequence-to-Sequence Text-to-Speech Models to Imperfectly-Transcribed Training Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/govender19_interspeech.html": {
    "title": "Using Pupil Dilation to Measure Cognitive Load When Listening to Text-to-Speech in Quiet and in Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/douros19b_interspeech.html": {
    "title": "A Multimodal Real-Time MRI Articulatory Corpus of French for Speech Research",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19d_interspeech.html": {
    "title": "A Chinese Dataset for Identifying Speakers in Novels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/park19c_interspeech.html": {
    "title": "CSS10: A Collection of Single Speaker Speech Datasets for 10 Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/karaulov19_interspeech.html": {
    "title": "Attention Model for Articulatory Features Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tong19_interspeech.html": {
    "title": "Unbiased Semi-Supervised LF-MMI Training Using Dropout",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cui19_interspeech.html": {
    "title": "Acoustic Model Optimization Based on Evolutionary Stochastic Gradient Descent with Anchors for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shah19b_interspeech.html": {
    "title": "Whether to Pretrain DNN or not?: An Empirical Analysis for Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/goyal19_interspeech.html": {
    "title": "Detection of Glottal Closure Instants from Raw Speech Using Convolutional Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fainberg19_interspeech.html": {
    "title": "Lattice-Based Lightly-Supervised Acoustic Model Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/michel19_interspeech.html": {
    "title": "Comparison of Lattice-Free and Lattice-Based Sequence Discriminative Training Criteria for LVCSR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/masumura19b_interspeech.html": {
    "title": "End-to-End Automatic Speech Recognition with a Reconstruction Criterion Using Speech-to-Text and Text-to-Speech Encoder-Decoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/heba19_interspeech.html": {
    "title": "Char+CV-CTC: Combining Graphemes and Consonant/Vowel Units for CTC-Based ASR Using Multitask Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kurata19_interspeech.html": {
    "title": "Guiding CTC Posterior Spike Timings for Improved Posterior Fusion and Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fukuda19_interspeech.html": {
    "title": "Direct Neuron-Wise Fusion of Cognate Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ladkat19_interspeech.html": {
    "title": "Two Tiered Distributed Training Algorithm for Acoustic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19f_interspeech.html": {
    "title": "Exploring the Encoder Layers of Discriminative Autoencoders for LVCSR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kurata19b_interspeech.html": {
    "title": "Multi-Task CTC Training with Auxiliary Feature Reconstruction for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19e_interspeech.html": {
    "title": "Framewise Supervised Training Towards End-to-End Speech Recognition Models: First Results",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/georgiou19_interspeech.html": {
    "title": "Deep Hierarchical Fusion with Application in Sentiment Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mitra19_interspeech.html": {
    "title": "Leveraging Acoustic Cues and Paralinguistic Embeddings to Detect Expression from Voice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/parry19_interspeech.html": {
    "title": "Analysis of Deep Learning Architectures for Cross-Corpus Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19e_interspeech.html": {
    "title": "A Path Signature Approach for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/egorow19_interspeech.html": {
    "title": "Employing Bottleneck and Convolutional Features for Speech-Based Physical Load Detection on Limited Data Amounts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhao19e_interspeech.html": {
    "title": "Speech Emotion Recognition in Dyadic Dialogues with Attentive Interaction Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhong19_interspeech.html": {
    "title": "Predicting Group Performances Using a Personality Composite-Network Architecture During Collaborative Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chao19b_interspeech.html": {
    "title": "Enforcing Semantic Consistency for Cross Corpus Valence Regression from Speech Using Adversarial Discrepancy Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mao19_interspeech.html": {
    "title": "Deep Learning of Segment-Level Feature Representation with Multiple Instance Learning for Utterance-Level Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/triantafyllopoulos19_interspeech.html": {
    "title": "Towards Robust Speech Emotion Recognition Using Deep Residual Networks for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19f_interspeech.html": {
    "title": "Towards Discriminative Representations and Unbiased Predictions: Class-Specific Angular Softmax for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jalal19_interspeech.html": {
    "title": "Learning Temporal Clusters Using Capsule Routing for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dapolito19_interspeech.html": {
    "title": "L2 Pronunciation Accuracy and Context: A Pilot Study on the Realization of Geminates in Italian as L2 by French Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jamakovic19_interspeech.html": {
    "title": "The Monophthongs of Formal Nigerian English: An Acoustic Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/arantes19_interspeech.html": {
    "title": "Quantifying Fundamental Frequency Modulation as a Function of Language, Speaking Style and Speaker",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kelly19_interspeech.html": {
    "title": "The Voicing Contrast in Stops and Affricates in the Western Armenian of Lebanon",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jatteau19_interspeech.html": {
    "title": "Gra[f] e!\" Word-Final Devoicing of Obstruents in Standard French: An Acoustic Study Based on Large Corpora",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19g_interspeech.html": {
    "title": "Acoustic Indicators of Deception in Mandarin Daily Conversations Recorded from an Interactive Game",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schuppler19_interspeech.html": {
    "title": "Prosodic Effects on Plosive Duration in German and Austrian German",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/johny19_interspeech.html": {
    "title": "Cross-Lingual Consistency of Phonological Features: An Empirical Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/guitardivent19_interspeech.html": {
    "title": "Are IP Initial Vowels Acoustically More Distinct? Results from LDA and CNN Classifications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wei19_interspeech.html": {
    "title": "Neural Network-Based Modeling of Phonetic Durations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/moczanow19_interspeech.html": {
    "title": "An Acoustic Study of Vowel Undershoot in a System with Several Degrees of Prominence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/berger19_interspeech.html": {
    "title": "A Preliminary Study of Charismatic Speech on YouTube: Correlating Prosodic Variation with Counts of Subscribers, Views and Likes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/luo19b_interspeech.html": {
    "title": "Phonetic Detail Encoding in Explaining the Size of Speech Planning Window",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zarka19_interspeech.html": {
    "title": "Acoustic Cues to Topic and Narrow Focus in Egyptian Arabic",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/alowonou19_interspeech.html": {
    "title": "Acoustic and Articulatory Study of Ewe Vowels: A Comparative Study of Male and Female",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/guo19c_interspeech.html": {
    "title": "Speech Augmentation via Speaker-Specific Noise in Unseen Environment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hao19_interspeech.html": {
    "title": "UNetGAN: A Robust Speech Enhancement Approach in Time Domain for Extremely Low Signal-to-Noise Ratio Condition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pascual19b_interspeech.html": {
    "title": "Towards Generalized Speech Enhancement with Generative Adversarial Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19g_interspeech.html": {
    "title": "A Convolutional Neural Network with Non-Local Module for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lin19b_interspeech.html": {
    "title": "IA-NET: Acceleration and Compression of Speech Enhancement Using Integer-Adder Deep Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chai19_interspeech.html": {
    "title": "KL-Divergence Regularized Deep Neural Network Adaptation for Low-Resource Speaker-Dependent Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/llombart19_interspeech.html": {
    "title": "Speech Enhancement with Wide Residual Networks in Reverberant Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/reddy19_interspeech.html": {
    "title": "A Scalable Noisy Speech Dataset and Online Subjective Test Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/adiga19_interspeech.html": {
    "title": "Speech Enhancement for Noise-Robust Speech Synthesis Using Wasserstein GAN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pv19_interspeech.html": {
    "title": "A Non-Causal FFTNet Architecture for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/braithwaite19_interspeech.html": {
    "title": "Speech Enhancement with Variance Constrained Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kyriakopoulos19_interspeech.html": {
    "title": "A Deep Learning Approach to Automatic Characterisation of Rhythm in Non-Native English Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/merkx19_interspeech.html": {
    "title": "Language Learning Using Speech to Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/skidmore19_interspeech.html": {
    "title": "Using Alexa for Flashcard-Based Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hansen19_interspeech.html": {
    "title": "The 2019 Inaugural Fearless Steps Challenge: A Giant Leap for Naturalistic Audio",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19e_interspeech.html": {
    "title": "Completely Unsupervised Phoneme Recognition by a Generative Adversarial Network Harmonized with Iteratively Refined Hidden Markov Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/trisitichoke19_interspeech.html": {
    "title": "Analysis of Native Listeners' Facial Microexpressions While Shadowing Non-Native Speech — Potential of Shadowers' Facial Expressions for Comprehensibility Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/karhila19_interspeech.html": {
    "title": "Transparent Pronunciation Scoring Using Articulatorily Weighted Phoneme Edit Distance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yoon19_interspeech.html": {
    "title": "Development of Robust Automated Scoring Models Using Adversarial Input for Oral Proficiency Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lu19b_interspeech.html": {
    "title": "Impact of ASR Performance on Spoken Grammatical Error Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yang19d_interspeech.html": {
    "title": "Self-Imitating Feedback Generation Using GAN for Computer-Assisted Pronunciation Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hori19_interspeech.html": {
    "title": "Joint Student-Teacher Learning for Audio-Visual Scene-Aware Dialog",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gopalakrishnan19_interspeech.html": {
    "title": "Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kubasova19_interspeech.html": {
    "title": "Analyzing Verbal and Nonverbal Features for Predicting Group Performance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/martinez19_interspeech.html": {
    "title": "Identifying Therapist and Client Personae for Therapeutic Alliance Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/haake19_interspeech.html": {
    "title": "Do Hesitations Facilitate Processing of Partially Defective System Utterances? An Exploratory Eye Tracking Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19h_interspeech.html": {
    "title": "Influence of Contextuality on Prosodic Realization of Information Structure in Chinese Dialogues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gjoreski19_interspeech.html": {
    "title": "Cross-Lingual Transfer Learning for Affective Spoken Dialogue Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yu19_interspeech.html": {
    "title": "Identifying Personality Traits Using Overlap Dynamics in Multiparty Dialogue",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/aldeneh19_interspeech.html": {
    "title": "Identifying Mood Episodes Using Dialogue Features from Clinical Interviews",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lubold19_interspeech.html": {
    "title": "Do Conversational Partners Entrain on Articulatory Precision?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lian19_interspeech.html": {
    "title": "Conversational Emotion Analysis via Attention Mechanisms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/oneill19_interspeech.html": {
    "title": "The Effect of Phoneme Distribution on Perceptual Similarity in English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kakouros19_interspeech.html": {
    "title": "Prosodic Representations of Prominence Classification Neural Networks and Autoencoders Using Bottleneck Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/peperkamp19_interspeech.html": {
    "title": "Compensation for French Liquid Deletion During Auditory Sentence Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kocharov19_interspeech.html": {
    "title": "Prosodic Factors Influencing Vowel Reduction in Russian",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gobl19_interspeech.html": {
    "title": "Time to Frequency Domain Mapping of the Voice Source: The Influence of Open Quotient and Glottal Skew on the Low End of the Source Spectrum",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chodroff19_interspeech.html": {
    "title": "Testing the Distinctiveness of Intonational Tunes: Evidence from Imitative Productions in American English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/park19d_interspeech.html": {
    "title": "A Study of a Cross-Language Perception Based on Cortical Analysis Using Biomimetic STRFs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sturm19_interspeech.html": {
    "title": "Perceptual Evaluation of Early versus Late F0 Peaks in the Intonation Structure of Czech Question-Word Questions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kelterer19_interspeech.html": {
    "title": "Acoustic Correlates of Phonation Type in Chichimec",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chien19c_interspeech.html": {
    "title": "F0 Variability Measures Based on Glottal Closure Instants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tavi19_interspeech.html": {
    "title": "Recognition of Creaky Voice from Emergency Calls",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xu19b_interspeech.html": {
    "title": "Direct F0 Estimation with Neural-Network-Based Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sharma19b_interspeech.html": {
    "title": "Real Time Online Visual End Point Detection Using Unidirectional LSTM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ardaillon19_interspeech.html": {
    "title": "Fully-Convolutional Network for Pitch Estimation of Speech Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dong19_interspeech.html": {
    "title": "Vocal Pitch Extraction in Polyphonic Music Using Convolutional Residual Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sharma19c_interspeech.html": {
    "title": "Multi-Level Adaptive Speech Activity Detector for Speech in Naturalistic Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sharma19d_interspeech.html": {
    "title": "On the Importance of Audio-Source Separation for Singer Identification in Polyphonic Music",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/terasawa19_interspeech.html": {
    "title": "Investigating the Physiological and Acoustic Contrasts Between Choral and Operatic Singing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lin19c_interspeech.html": {
    "title": "Optimizing Voice Activity Detection for Noisy Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yamamoto19c_interspeech.html": {
    "title": "Small-Footprint Magic Word Detection Method Using Convolutional LSTM Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gupta19b_interspeech.html": {
    "title": "Acoustic Modeling for Automatic Lyrics-to-Audio Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vafeiadis19_interspeech.html": {
    "title": "Two-Dimensional Convolutional Recurrent Neural Networks for Speech Activity Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kaburagi19_interspeech.html": {
    "title": "A Study of Soprano Singing in Light of the Source-Filter Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zou19_interspeech.html": {
    "title": "Boosting Character-Based Chinese Speech Synthesis via Multi-Task Learning and Dictionary Tutoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xue19_interspeech.html": {
    "title": "Building a Mixed-Lingual Neural TTS System with Only Monolingual Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sokolov19_interspeech.html": {
    "title": "Neural Machine Translation for Multilingual Grapheme-to-Phoneme Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/taylor19_interspeech.html": {
    "title": "Analysis of Pronunciation Learning in End-to-End Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19f_interspeech.html": {
    "title": "End-to-End Text-to-Speech for Low-Resource Languages by Cross-Lingual Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19e_interspeech.html": {
    "title": "Learning to Speak Fluently in a Foreign Language: Multilingual Speech Synthesis and Cross-Language Voice Cloning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/juzova19_interspeech.html": {
    "title": "Unified Language-Independent DNN-Based G2P Converter",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dai19_interspeech.html": {
    "title": "Disambiguation of Chinese Polyphones in an End-to-End Framework with Semantic Features Extracted by Pre-Trained BERT",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yolchuyeva19_interspeech.html": {
    "title": "Transformer Based Grapheme-to-Phoneme Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bleyan19_interspeech.html": {
    "title": "Developing Pronunciation Models in New Languages Faster by Exploiting Common Grapheme-to-Phoneme Correspondences Across Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19g_interspeech.html": {
    "title": "Cross-Lingual, Multi-Speaker Text-To-Speech Synthesis Using Neural Speaker Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cai19b_interspeech.html": {
    "title": "Polyphone Disambiguation for Mandarin Chinese Using Conditional Neural Network with Multi-Level Embedding Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sun19c_interspeech.html": {
    "title": "Token-Level Ensemble Distillation for Grapheme-to-Phoneme Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19i_interspeech.html": {
    "title": "Multilingual Speech Recognition with Corpus Relatedness Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/arsikere19_interspeech.html": {
    "title": "Multi-Dialect Acoustic Modeling Using Phone Mapping and Online i-Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kannan19_interspeech.html": {
    "title": "Large-Scale Multilingual Speech Recognition with a Streaming End-to-End Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mendes19_interspeech.html": {
    "title": "Recognition of Latin American Spanish Using Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/viglino19_interspeech.html": {
    "title": "End-to-End Accented Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19j_interspeech.html": {
    "title": "End-to-End Articulatory Attribute Modeling for Low-Resource Multilingual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/taneja19_interspeech.html": {
    "title": "Exploiting Monolingual Speech Corpora for Code-Mixed Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hu19_interspeech.html": {
    "title": "Phoneme-Based Contextualization for Cross-Lingual Speech Recognition in End-to-End Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/khassanov19_interspeech.html": {
    "title": "Constrained Output Embeddings for End-to-End Code-Switching Speech Recognition with Only Monolingual Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zeng19_interspeech.html": {
    "title": "On the End-to-End Solution to Mandarin-English Code-Switching Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19f_interspeech.html": {
    "title": "Towards Language-Universal Mandarin-English Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/swarup19_interspeech.html": {
    "title": "Improving ASR Confidence Scores for Alexa Using Acoustic and Hypothesis Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19g_interspeech.html": {
    "title": "Investigation of Transformer Based Spelling Correction Model for CTC-Based End-to-End Mandarin Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/peyser19_interspeech.html": {
    "title": "Improving Performance of End-to-End ASR on Numeric Sequences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bai19_interspeech.html": {
    "title": "A Time Delay Neural Network with Shared Weight Self-Attention for Small-Footprint Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kao19_interspeech.html": {
    "title": "Sub-Band Convolutional Neural Networks for Small-Footprint Spoken Term Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19k_interspeech.html": {
    "title": "Investigating Radical-Based End-to-End Speech Recognition Systems for Chinese Dialects and Japanese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/guo19d_interspeech.html": {
    "title": "Joint Decoding of CTC Based Systems for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tanaka19_interspeech.html": {
    "title": "A Joint End-to-End and DNN-HMM Hybrid Automatic Speech Recognition System with Transferring Sharable Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/malhotra19_interspeech.html": {
    "title": "Active Learning Methods for Low Resource End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/karafiat19_interspeech.html": {
    "title": "Analysis of Multilingual Sequence-to-Sequence Speech Recognition Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zapotoczny19_interspeech.html": {
    "title": "Lattice Generation in Attention-Based Speech Recognition Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jansche19_interspeech.html": {
    "title": "Sampling from Stochastic Finite Automata with Applications to CTC Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dudziak19_interspeech.html": {
    "title": "ShrinkML: End-to-End ASR Model Compression Using Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gaur19_interspeech.html": {
    "title": "Acoustic-to-Phrase Models for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19l_interspeech.html": {
    "title": "Performance Monitoring for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cohn19b_interspeech.html": {
    "title": "The Role of Musical Experience in the Perceptual Weighting of Acoustic Cues for the Obstruent Coda Voicing Contrast in American English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lewandowski19_interspeech.html": {
    "title": "Individual Differences in Implicit Attention to Phonetic Detail in Speech Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lalonde19_interspeech.html": {
    "title": "Effects of Natural Variability in Cross-Modal Temporal Correlations on Audiovisual Speech Recognition Benefit",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bentum19_interspeech.html": {
    "title": "Listening with Great Expectations: An Investigation of Word Form Anticipations in Naturalistic Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bentum19b_interspeech.html": {
    "title": "Quantifying Expectation Modulation in Human Speech Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/turner19_interspeech.html": {
    "title": "Perception of Pitch Contours in Speech and Nonspeech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bosch19b_interspeech.html": {
    "title": "Analyzing Reaction Time and Error Sequences in Lexical Decision Experiments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19e_interspeech.html": {
    "title": "Automatic Detection of the Temporal Segmentation of Hand Movements in British English Cued Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yokoe19_interspeech.html": {
    "title": "Place Shift as an Autonomous Process: Evidence from Japanese Listeners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/meyer19_interspeech.html": {
    "title": "A Perceptual Study of CV Syllables in Both Spoken and Whistled Speech: A Tashlhiyt Berber Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hsieh19_interspeech.html": {
    "title": "Consonant Classification in Mandarin Based on the Depth Image Feature: A Pilot Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/levari19_interspeech.html": {
    "title": "The Different Roles of Expectations in Phonetic and Lexical Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/segedin19_interspeech.html": {
    "title": "Perceptual Adaptation to Device and Human Voices: Learning and Generalization of a Phonetic Shift Across Real and Voice-AI Talkers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/papadimitriou19_interspeech.html": {
    "title": "End-to-End Convolutional Sequence Learning for ASL Fingerspelling Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/somandepalli19_interspeech.html": {
    "title": "Multiview Shared Subspace Learning Across Speakers and Speech Commands",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/belitz19_interspeech.html": {
    "title": "A Machine Learning Based Clustering Protocol for Determining Hearing Aid Initial Configurations from Pure-Tone Audiograms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nguyen19_interspeech.html": {
    "title": "Acoustic Scene Classification with Mismatched Devices Using CliqueNets and Mixup Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ahmed19_interspeech.html": {
    "title": "DeepLung: Smartphone Convolutional Neural Network-Based Inference of Lung Anomalies for Pulmonary Patients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/moore19_interspeech.html": {
    "title": "On the Use/Misuse of the Term ‘Phoneme",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/muckenhirn19_interspeech.html": {
    "title": "Understanding and Visualizing Raw Waveform-Based CNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kilgour19_interspeech.html": {
    "title": "Fréchet Audio Distance: A Reference-Free Metric for Evaluating Music Enhancement Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gong19_interspeech.html": {
    "title": "ReMASC: Realistic Replay Attack Corpus for Voice Controlled Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bt19_interspeech.html": {
    "title": "Analyzing Intra-Speaker and Inter-Speaker Vocal Tract Impedance Characteristics in a Low-Dimensional Feature Space Using t-SNE",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lee19b_interspeech.html": {
    "title": "Directional Audio Rendering Using a Neural Network Based Personalized HRTF",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pienaar19_interspeech.html": {
    "title": "Online Speech Processing and Analysis Suite",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/maurer19_interspeech.html": {
    "title": "Formant Pattern and Spectral Shape Ambiguity of Vowel Sounds, and Related Phenomena of Vowel Acoustics — Exemplary Evidence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/noll19_interspeech.html": {
    "title": "Sound Tools eXtended (STx) 5.0 — A Powerful Sound Analysis Tool Optimized for Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/eldesouki19_interspeech.html": {
    "title": "FarSpeech: Arabic Natural Language Processing for Live Arabic Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/haider19_interspeech.html": {
    "title": "A System for Real-Time Privacy Preserving Data Collection for Ambient Assisted Living",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gupta19c_interspeech.html": {
    "title": "NUS Speak-to-Sing: A Web Platform for Personalized Speech-to-Singing Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kaltenbacher19_interspeech.html": {
    "title": "Physiology and Physics of Voice Production",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schuller19_interspeech.html": {
    "title": "The INTERSPEECH 2019 Computational Paralinguistics Challenge: Styrian Dialects, Continuous Sleepiness, Baby Sounds & Orca Activity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dubagunta19_interspeech.html": {
    "title": "Using Speech Production Knowledge for Raw Waveform Modelling Based Styrian Dialect Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/elsner19_interspeech.html": {
    "title": "Deep Neural Baselines for Computational Paralinguistics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kisler19_interspeech.html": {
    "title": "Styrian Dialect Classification: Comparing and Fusing Classifiers Based on a Feature Selection Using a Genetic Algorithm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yeh19_interspeech.html": {
    "title": "Using Attention Networks and Adversarial Augmentation for Styrian Dialect Continuous Sleepiness and Baby Sound Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wu19f_interspeech.html": {
    "title": "Ordinal Triplet Loss: Investigating Sleepiness Detection from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ravi19_interspeech.html": {
    "title": "Voice Quality and Between-Frame Entropy for Sleepiness Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gosztolya19b_interspeech.html": {
    "title": "Using Fisher Vector and Bag-of-Audio-Words Representations to Identify Styrian Dialects, Sleepiness, Baby & Orca Sounds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/das19b_interspeech.html": {
    "title": "Instantaneous Phase and Long-Term Acoustic Cues for Orca Activity Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schiller19_interspeech.html": {
    "title": "Relevance-Based Feature Masking: Improving Neural Network Based Whale Classification Through Explainable Artificial Intelligence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/caraty19_interspeech.html": {
    "title": "Spatial, Temporal and Spectral Multiresolution Analysis for the INTERSPEECH 2019 ComParE Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wu19g_interspeech.html": {
    "title": "The DKU-LENOVO Systems for the INTERSPEECH 2019 Computational Paralinguistic Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nandwana19_interspeech.html": {
    "title": "The VOiCES from a Distance Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/novoselov19b_interspeech.html": {
    "title": "STC Speaker Recognition Systems for the VOiCES from a Distance Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/matejka19_interspeech.html": {
    "title": "Analysis of BUT Submission in Far-Field Scenarios of VOiCES 2019 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/medennikov19_interspeech.html": {
    "title": "The STC ASR System for the VOiCES from a Distance Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chong19_interspeech.html": {
    "title": "The I2R's ASR System for the VOiCES from a Distance Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nandwana19b_interspeech.html": {
    "title": "The VOiCES from a Distance Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/novoselov19c_interspeech.html": {
    "title": "STC Speaker Recognition Systems for the VOiCES from a Distance Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/matejka19b_interspeech.html": {
    "title": "Analysis of BUT Submission in Far-Field Scenarios of VOiCES 2019 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/medennikov19b_interspeech.html": {
    "title": "The STC ASR System for the VOiCES from a Distance Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chong19b_interspeech.html": {
    "title": "The I2R's ASR System for the VOiCES from a Distance Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jati19_interspeech.html": {
    "title": "Multi-Task Discriminative Training of Hybrid DNN-TVM Model for Speaker Verification with Noisy and Far-Field Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/snyder19b_interspeech.html": {
    "title": "The JHU Speaker Recognition System for the VOiCES 2019 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19h_interspeech.html": {
    "title": "Intel Far-Field Speaker Recognition System for VOiCES Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sun19d_interspeech.html": {
    "title": "The I2R's Submission to VOiCES Distance Speaker Recognition Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liang19_interspeech.html": {
    "title": "The LeVoice Far-Field Speech Recognition System for VOiCES from a Distance Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19f_interspeech.html": {
    "title": "The JHU ASR System for VOiCES from a Distance Challenge 2019",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cai19c_interspeech.html": {
    "title": "The DKU System for the Speaker Recognition Task of the 2019 VOiCES from a Distance Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hauptman19_interspeech.html": {
    "title": "Identifying Distinctive Acoustic and Spectral Features in Parkinson's Disease",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/drioli19_interspeech.html": {
    "title": "Aerodynamics and Lumped-Masses Combined with Delay Lines for Modeling Vertical and Anterior-Posterior Phase Differences in Pathological Vocal Fold Vibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kadiri19_interspeech.html": {
    "title": "Mel-Frequency Cepstral Coefficients of Voice Source Waveforms for Classification of Phonation Types in Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cho19_interspeech.html": {
    "title": "Automatic Detection of Autism Spectrum Disorder in Children Using Acoustic and Text Features from Brief Natural Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schoentgen19_interspeech.html": {
    "title": "Analysis and Synthesis of Vocal Flutter and Vocal Jitter",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schaeffler19_interspeech.html": {
    "title": "Reliability of Clinical Voice Parameters Captured with Smartphones — Measurements of Added Noise and Spectral Tilt",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/moore19b_interspeech.html": {
    "title": "Say What? A Dataset for Exploring the Error Patterns That Two ASR Engines Make",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ward19_interspeech.html": {
    "title": "Survey Talk: Prosody Research and Applications: The State of the Art",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/roessig19_interspeech.html": {
    "title": "Dimensions of Prosodic Prominence in an Attractor Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/suni19_interspeech.html": {
    "title": "Comparative Analysis of Prosodic Characteristics Using WaveNet Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/murphy19_interspeech.html": {
    "title": "The Role of Voice Quality in the Perception of Prominence in Synthetic Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/albar19_interspeech.html": {
    "title": "Phonological Awareness of French Rising Contours in Japanese Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/okawa19_interspeech.html": {
    "title": "Audio Classification of Bit-Representation Waveform",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mulimani19_interspeech.html": {
    "title": "Locality-Constrained Linear Coding Based Fused Visual Features for Robust Acoustic Event Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shen19b_interspeech.html": {
    "title": "Learning How to Listen: A Temporal-Frequential Attention Model for Sound Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ford19_interspeech.html": {
    "title": "A Deep Residual Network for Large-Scale Acoustic Scene Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/reddy19b_interspeech.html": {
    "title": "Supervised Classifiers for Audio Impairments with Noisy Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tarantino19_interspeech.html": {
    "title": "Self-Attention for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nachmani19_interspeech.html": {
    "title": "Unsupervised Singing Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lee19c_interspeech.html": {
    "title": "Adversarially Trained End-to-End Korean Singing Voice Synthesis System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yi19_interspeech.html": {
    "title": "Singing Voice Synthesis Using Deep Autoregressive Neural Networks for Acoustic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dahmani19_interspeech.html": {
    "title": "Conditional Variational Auto-Encoder for Text-Driven Expressive AudioVisual Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ayllon19b_interspeech.html": {
    "title": "A Strategy for Improved Phone-Level Lyrics-to-Audio Alignment for Speech-to-Singing Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/biasuttolervat19_interspeech.html": {
    "title": "Modeling Labial Coarticulation with Bidirectional Gated Recurrent Networks and Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/park19e_interspeech.html": {
    "title": "SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/audhkhasi19_interspeech.html": {
    "title": "Forget a Bit to Learn Better: Soft Forgetting for CTC-Based Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/miao19_interspeech.html": {
    "title": "Online Hybrid CTC/Attention Architecture for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19h_interspeech.html": {
    "title": "A Highly Efficient Distributed Deep Learning System for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19i_interspeech.html": {
    "title": "Knowledge Distillation for End-to-End Monaural Multi-Talker ASR System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/menne19_interspeech.html": {
    "title": "Analysis of Deep Clustering as Preprocessing for Automatic Speech Recognition of Sparsely Overlapping Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bradlow19_interspeech.html": {
    "title": "Survey Talk: Recognition of Foreign-Accented Speech: Challenges and Opportunities for Human and Computer Speech Communication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/novak19_interspeech.html": {
    "title": "The Effects of Time Expansion on English as a Second Language Individuals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shi19_interspeech.html": {
    "title": "Capturing L1 Influence on L2 Pronunciation by Simulating Perceptual Space Using Acoustic Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19h_interspeech.html": {
    "title": "Cognitive Factors in Thai-Naïve Mandarin Speakers' Imitation of Thai Lexical Tones",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tremblay19_interspeech.html": {
    "title": "Foreign-Language Knowledge Enhances Artificial-Language Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/abujabal19_interspeech.html": {
    "title": "Neural Named Entity Recognition from Subword Units",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bhati19_interspeech.html": {
    "title": "Unsupervised Acoustic Segmentation and Clustering Using Siamese Network Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yusuf19b_interspeech.html": {
    "title": "An Empirical Evaluation of DTW Subsampling Methods for Keyword Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yang19e_interspeech.html": {
    "title": "Linguistically-Informed Training of Acoustic Word Embeddings for Low-Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19g_interspeech.html": {
    "title": "Multimodal Word Discovery and Retrieval with Phone Sequence and Image Concepts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/boito19_interspeech.html": {
    "title": "Empirical Evaluation of Sequence-to-Sequence Models for Word Discovery in Low-Resource Settings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xue19b_interspeech.html": {
    "title": "Direct-Path Signal Cross-Correlation Estimation for Sound Source Localization in Reverberation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/grondin19_interspeech.html": {
    "title": "Multiple Sound Source Localization with SVD-PHAT",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19j_interspeech.html": {
    "title": "Robust DOA Estimation Based on Convolutional Neural Network and Time-Frequency Masking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/masuyama19_interspeech.html": {
    "title": "Multichannel Loss Function for Supervised Speech Source Separation by Mask-Based Beamforming",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19m_interspeech.html": {
    "title": "Direction-Aware Speaker Beam for Multi-Channel Speaker Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ochiai19_interspeech.html": {
    "title": "Multimodal SpeakerBeam: Single Channel Target Speech Extraction with Audio-Visual Speaker Clues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/germain19_interspeech.html": {
    "title": "Speech Denoising with Deep Feature Losses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19h_interspeech.html": {
    "title": "VoiceFilter: Targeted Voice Separation by Speaker-Conditioned Spectrogram Masking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liao19_interspeech.html": {
    "title": "Incorporating Symbolic Sequential Modeling for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mowlaee19_interspeech.html": {
    "title": "Maximum a posteriori Speech Enhancement Based on Double Spectrum",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yao19_interspeech.html": {
    "title": "Coarse-to-Fine Optimization for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hui19b_interspeech.html": {
    "title": "Kernel Machines Beat Deep Neural Networks on Mask-Based Single-Channel Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/metze19_interspeech.html": {
    "title": "Survey Talk: Multimodal Processing of Speech and Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shrivastava19_interspeech.html": {
    "title": "MobiVSR : Efficient and Light-Weight Neural Network for Visual Speech Recognition on Mobile Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kandala19_interspeech.html": {
    "title": "Speaker Adaptation for Lip-Reading Using Visual Identity Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/koumparoulis19_interspeech.html": {
    "title": "MobiLipNet: Resource-Efficient Deep Learning Based Lipreading",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/qu19_interspeech.html": {
    "title": "LipSound: Neural Mel-Spectrogram Reconstruction for Lip Reading",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sainath19_interspeech.html": {
    "title": "Two-Pass End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lam19_interspeech.html": {
    "title": "Extract, Adapt and Recognize: An End-to-End Neural Network for Corrupted Monaural Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gowda19_interspeech.html": {
    "title": "Multi-Task Multi-Resolution Char-to-BPE Cross-Attention Decoder for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/han19b_interspeech.html": {
    "title": "Multi-Stride Self-Attention for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hu19b_interspeech.html": {
    "title": "LF-MMI Training of Bayesian and Gaussian Process Time Delay Neural Networks for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lu19c_interspeech.html": {
    "title": "Self-Teaching Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19n_interspeech.html": {
    "title": "Improved End-to-End Speech Emotion Recognition Using Self Attention Mechanism and Multitask Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schmitt19_interspeech.html": {
    "title": "Continuous Emotion Recognition in Speech — Do We Need Recurrence?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ouyang19_interspeech.html": {
    "title": "Speech Based Emotion Prediction: Can a Linear Model Work?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ando19_interspeech.html": {
    "title": "Speech Emotion Recognition Based on Multi-Label Emotion Existence Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gorrostieta19_interspeech.html": {
    "title": "Gender De-Biasing in Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bao19_interspeech.html": {
    "title": "CycleGAN-Based Emotion Style Transfer as Data Augmentation for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bollepalli19_interspeech.html": {
    "title": "Lombard Speech Synthesis Using Transfer Learning in a Tacotron Text-to-Speech System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/seshadri19_interspeech.html": {
    "title": "Augmented CycleGANs for Continuous Scale Normal-to-Lombard Speaking Style Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhao19f_interspeech.html": {
    "title": "Foreign Accent Conversion by Synthesizing Speech from Phonetic Posteriorgrams",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shankar19b_interspeech.html": {
    "title": "A Multi-Speaker Emotion Morphing Model Using Highway Networks and Maximum Likelihood Objective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lapidot19_interspeech.html": {
    "title": "Effects of Waveform PMF on Anti-Spoofing Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gao19b_interspeech.html": {
    "title": "Nonparallel Emotional Speech Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/stafylakis19_interspeech.html": {
    "title": "Self-Supervised Speaker Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nautsch19b_interspeech.html": {
    "title": "Privacy-Preserving Speaker Recognition with Cohort Score Normalisation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19f_interspeech.html": {
    "title": "Large Margin Softmax Loss for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hajavi19_interspeech.html": {
    "title": "A Deep Neural Network for Short-Segment Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhou19_interspeech.html": {
    "title": "Deep Speaker Embedding Extraction with Channel-Wise Feature Responses and Additive Supervision Softmax Loss Function",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shon19b_interspeech.html": {
    "title": "VoiceID Loss: Speech Enhancement for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/avila19_interspeech.html": {
    "title": "Blind Channel Response Estimation for Replay Attack Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/patil19_interspeech.html": {
    "title": "Energy Separation-Based Instantaneous Frequency Estimation for Cochlear Cepstral Feature for Replay Spoof Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mingote19_interspeech.html": {
    "title": "Optimization of False Acceptance/Rejection Rates and Decision Threshold for End-to-End Text-Dependent Speaker Verification Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fan19_interspeech.html": {
    "title": "Deep Hashing for Speaker Identification and Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/marras19_interspeech.html": {
    "title": "Adversarial Optimization for Dictionary Attacks on Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gunendradasan19_interspeech.html": {
    "title": "An Adaptive-Q Cochlear Model for Replay Spoofing Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yun19_interspeech.html": {
    "title": "An End-to-End Text-Independent Speaker Verification Framework with a Keyword Adversarial Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/seo19_interspeech.html": {
    "title": "Shortcut Connections Based Deep Speaker Embeddings for End-to-End Speaker Verification System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/you19c_interspeech.html": {
    "title": "Device Feature Extractor for Replay Spoofing Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19i_interspeech.html": {
    "title": "Cross-Domain Replay Spoofing Attack Detection Using Domain Adversarial Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kanagasundaram19_interspeech.html": {
    "title": "A Study of x-Vector Based Speaker Recognition on Short Utterances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19i_interspeech.html": {
    "title": "Tied Mixture of Factor Analyzers Layer to Combine Frame Level Representations in Neural Speaker Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wickramasinghe19_interspeech.html": {
    "title": "Biologically Inspired Adaptive-Q Filterbanks for Replay Spoofing Attack Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bousquet19_interspeech.html": {
    "title": "On Robustness of Unsupervised Domain Adaptation for Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shon19c_interspeech.html": {
    "title": "Large-Scale Speaker Retrieval on Random Speaker Variability Subspace",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yoshioka19_interspeech.html": {
    "title": "Meeting Transcription Using Asynchronous Distant Microphones",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/thomas19_interspeech.html": {
    "title": "Detection and Recovery of OOVs for Improved English Broadcast News Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/farooq19_interspeech.html": {
    "title": "Improving Large Vocabulary Urdu Speech Recognition System Using Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tang19b_interspeech.html": {
    "title": "Hybrid Arbitration Using Raw ASR String and NLU Information — Taking the Best of Both Embedded World and Cloud World",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/szaszak19_interspeech.html": {
    "title": "Leveraging a Character, Word and Prosody Triplet for an ASR Error Robust and Agglutination Friendly Punctuation Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pellegrini19_interspeech.html": {
    "title": "The Airbus Air Traffic Control Speech Recognition 2018 Challenge: Towards ATC Automatic Transcription and Call Sign Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/oneata19_interspeech.html": {
    "title": "Kite: Automatic Speech Recognition for Unmanned Aerial Vehicles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19j_interspeech.html": {
    "title": "Exploring Methods for the Automatic Detection of Errors in Manual Transcription",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/biswas19_interspeech.html": {
    "title": "Improved Low-Resource Somali Speech Recognition by Semi-Supervised Acoustic and Language Model Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/helgadottir19_interspeech.html": {
    "title": "The Althingi ASR System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gupta19d_interspeech.html": {
    "title": "CRIM's Speech Transcription and Call Sign Detection System for the ATC Airbus Challenge Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rutowski19_interspeech.html": {
    "title": "Optimizing Speech-Input Length for Speaker-Independent Depression Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pietrowicz19_interspeech.html": {
    "title": "A New Approach for Automating Analysis of Responses on Verbal Fluency Tests from Subjects At-Risk for Schizophrenia",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jeancolas19_interspeech.html": {
    "title": "Comparison of Telephone Recordings and Professional Microphone Recordings for Early Detection of Parkinson's Disease, Using Mel-Frequency Cepstral Coefficients with Gaussian Mixture Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/janbakhshi19_interspeech.html": {
    "title": "Spectral Subspace Analysis for Automatic Assessment of Pathological Speech Intelligibility",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pasquale19_interspeech.html": {
    "title": "An Investigation of Therapeutic Rapport Through Prosody in Brief Psychodynamic Psychotherapy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rueda19_interspeech.html": {
    "title": "Feature Representation of Pathophysiology of Parkinsonian Dysarthria",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/onu19_interspeech.html": {
    "title": "Neural Transfer Learning for Cry-Based Diagnosis of Perinatal Asphyxia",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hong19_interspeech.html": {
    "title": "Investigating the Variability of Voice Quality and Pain Levels as a Function of Multiple Clinical Parameters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lopez19_interspeech.html": {
    "title": "Assessing Parkinson's Disease from Speech Using Fisher Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/klumpp19_interspeech.html": {
    "title": "Feature Space Visualization with Spatial Similarity Maps for Pathological Speech Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chakravarthula19_interspeech.html": {
    "title": "Predicting Behavior in Cancer-Afflicted Patient and Spouse Interactions Using Speech and Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/qin19_interspeech.html": {
    "title": "Automatic Assessment of Language Impairment Based on Raw ASR Output",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fu19_interspeech.html": {
    "title": "Effects of Spectral and Temporal Cues to Mandarin Concurrent-Vowels Identification for Normal-Hearing and Hearing-Impaired Listeners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zayats19_interspeech.html": {
    "title": "Disfluencies and Human Speech Transcription Errors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/parhammer19_interspeech.html": {
    "title": "The Influence of Distraction on Speech Processing: How Selective is Selective Attention?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hazan19_interspeech.html": {
    "title": "Subjective Evaluation of Communicative Effort for Younger and Older Adults in Interactive Tasks with Energetic and Informational Masking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/davis19_interspeech.html": {
    "title": "Perceiving Older Adults Producing Clear and Lombard Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ariasvergara19_interspeech.html": {
    "title": "Phone-Attribute Posteriors to Evaluate the Speech of Cochlear Implant Users",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hodoshima19_interspeech.html": {
    "title": "Effects of Urgent Speech and Congruent/Incongruent Text on Speech Intelligibility in Noise and Reverberation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mamun19_interspeech.html": {
    "title": "Quantifying Cochlear Implant Users' Ability for Speaker Identification Using CI Auditory Stimuli",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/felker19_interspeech.html": {
    "title": "Lexically Guided Perceptual Learning of a Vowel Shift in an Interactive L2 Listening Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/paulus19_interspeech.html": {
    "title": "Talker Intelligibility and Listening Effort with Temporally Modified Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ward19b_interspeech.html": {
    "title": "R2SPIN: Re-Recording the Revised Speech Perception in Noise Test",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19j_interspeech.html": {
    "title": "Contributions of Consonant-Vowel Transitions to Mandarin Tone Identification in Simulated Electric-Acoustic Hearing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pirhosseinloo19_interspeech.html": {
    "title": "Monaural Speech Enhancement with Dilated Convolutions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liao19b_interspeech.html": {
    "title": "Noise Adaptive Speech Enhancement Using Domain Adversarial Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ge19_interspeech.html": {
    "title": "Environment-Dependent Attention-Driven Recurrent Convolutional Neural Network for Robust Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pariente19_interspeech.html": {
    "title": "A Statistically Principled and Computationally Efficient Approach to Speech Enhancement Using Variational Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lin19d_interspeech.html": {
    "title": "Speech Enhancement Using Forked Generative Adversarial Networks with Spectral Subtraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zezario19_interspeech.html": {
    "title": "Specialized Speech Enhancement Model Selection Based on Learned Non-Intrusive Quality Assessment Metric",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chuang19_interspeech.html": {
    "title": "Speaker-Aware Deep Denoising Autoencoder with Embedded Speaker Identity for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19g_interspeech.html": {
    "title": "Investigation of Cost Function for Supervised Monaural Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shi19b_interspeech.html": {
    "title": "Deep Attention Gated Dilated Temporal Convolutional Networks with Intra-Parallel Convolutional Modules for End-to-End Monaural Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19k_interspeech.html": {
    "title": "Masking Estimation with Phase Restoration of Clean Speech for Monaural Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/llombart19b_interspeech.html": {
    "title": "Progressive Speech Enhancement with Residual Connections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19k_interspeech.html": {
    "title": "Acoustic Model Bootstrapping Using Semi-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mantena19_interspeech.html": {
    "title": "Bandwidth Embeddings for Mixed-Bandwidth Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/khare19_interspeech.html": {
    "title": "Adversarial Black-Box Attacks on Automatic Speech Recognition Systems Using Multi-Objective Evolutionary Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/soomro19_interspeech.html": {
    "title": "Towards Debugging Deep Neural Networks by Generating Speech Utterances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ding19b_interspeech.html": {
    "title": "Compression of CTC-Trained Acoustic Models by Dynamic Frame-Wise Distillation or Segment-Wise N-Best Hypotheses Imitation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lopezespejo19_interspeech.html": {
    "title": "Keyword Spotting for Hearing Assistive Devices Robust to External Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/doulaty19_interspeech.html": {
    "title": "Latent Dirichlet Allocation Based Acoustic Data Selection for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19o_interspeech.html": {
    "title": "Target Speaker Recovery and Recognition Network with Average x-Vector and Global Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/suzuki19b_interspeech.html": {
    "title": "Lyrics Recognition from Singing Voice Focused on Correspondence Between Voice and Notes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hsu19b_interspeech.html": {
    "title": "Transfer Learning from Audio-Visual Grounding to Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/luo19c_interspeech.html": {
    "title": "Cross-Corpus Speech Emotion Recognition Using Semi-Supervised Transfer Non-Negative Matrix Factorization with Adaptation Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tammewar19_interspeech.html": {
    "title": "Modeling User Context for Valence Prediction from Narratives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chakraborty19_interspeech.html": {
    "title": "Front-End Feature Compensation and Denoising for Noise Robust Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19p_interspeech.html": {
    "title": "The Contribution of Acoustic Features Analysis to Model Emotion Perceptual Process for Language Diversity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rajan19_interspeech.html": {
    "title": "Design and Development of a Multi-Lingual Speech Corpora (TaMaR-EmoDB) for Emotion Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sridhar19_interspeech.html": {
    "title": "Speech Emotion Recognition with a Reject Option",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jin19_interspeech.html": {
    "title": "Development of Emotion Rankers Based on Intended and Perceived Emotion Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gideon19_interspeech.html": {
    "title": "Emotion Recognition from Natural Phone Conversations in Individuals with and without Recent Suicidal Ideation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nazareth19_interspeech.html": {
    "title": "An Acoustic and Lexical Analysis of Emotional Valence in Spontaneous Speech: Autobiographical Memory Recall in Older Adults",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhao19g_interspeech.html": {
    "title": "Does the Lombard Effect Improve Emotional Communication in Noise? — Analysis of Emotional Speech Acted in Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gharsellaoui19_interspeech.html": {
    "title": "Linear Discriminant Differential Evolution for Feature Selection in Emotional Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sahu19_interspeech.html": {
    "title": "Multi-Modal Learning for Speech Emotion Recognition: An Analysis and Comparison of ASR Outputs with Ground Truth Transcription",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/spinu19_interspeech.html": {
    "title": "Articulatory Characteristics of Secondary Palatalization in Romanian Fricatives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ratko19_interspeech.html": {
    "title": "Articulation of Vowel Length Contrasts in Australian English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/deme19_interspeech.html": {
    "title": "V-to-V Coarticulation Induced Acoustic and Articulatory Variability of Vowels: The Effect of Pitch-Accent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/king19_interspeech.html": {
    "title": "The Contribution of Lip Protrusion to Anglo-English /r/: Evidence from Hyper- and Non-Hyperarticulated Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/marko19_interspeech.html": {
    "title": "Articulatory Analysis of Transparent Vowel /iː/ in Harmonic and Antiharmonic Hungarian Stems: Is There a Difference?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cunha19_interspeech.html": {
    "title": "On the Role of Oral Configurations in European Portuguese Nasal Vowels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xiong19_interspeech.html": {
    "title": "Residual + Capsule Networks (ResCap) for Simultaneous Single-Channel Overlapped Keyword Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19i_interspeech.html": {
    "title": "A Study for Improving Device-Directed Speech Detection Toward Frictionless Human-Machine Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/su19_interspeech.html": {
    "title": "Unsupervised Methods for Audio Classification from Lecture Discussion Recordings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ashihara19_interspeech.html": {
    "title": "Neural Whispered Speech Detection with Imbalanced Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bergler19_interspeech.html": {
    "title": "Deep Learning for Orca Call Type Identification — A Fully Unsupervised Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sacchi19_interspeech.html": {
    "title": "Open-Vocabulary Keyword Spotting with Audio and Text Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gao19c_interspeech.html": {
    "title": "ToneNet: A CNN Model of Tone Classification of Mandarin Chinese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/choi19_interspeech.html": {
    "title": "Temporal Convolution for Real-Time Keyword Spotting on Mobile Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19j_interspeech.html": {
    "title": "Audio Tagging with Compact Feedforward Sequential Memory Network and Audio-to-Audio Ratio Based Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yang19f_interspeech.html": {
    "title": "Music Genre Classification Using Duplicated Convolutional Layers in Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/carmi19_interspeech.html": {
    "title": "A Storyteller's Tale: Literature Audiobooks Genre Classification Using CNN and RNN Architectures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hwang19_interspeech.html": {
    "title": "Parameter Enhancement for MELP Speech Codec in Noisy Communication Environment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhen19_interspeech.html": {
    "title": "Cascaded Cross-Module Residual Learning Towards Lightweight End-to-End Speech Coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/backstrom19_interspeech.html": {
    "title": "End-to-End Optimization of Source Models for Speech and Audio Coding Using a Machine Learning Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/valin19_interspeech.html": {
    "title": "A Real-Time Wideband Neural Vocoder at 1.6kb/s Using LPCNet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fuchs19_interspeech.html": {
    "title": "Super-Wideband Spectral Envelope Modeling for Speech Coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19q_interspeech.html": {
    "title": "Speech Audio Super-Resolution for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gupta19e_interspeech.html": {
    "title": "Artificial Bandwidth Extension Using H∞ Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mittag19_interspeech.html": {
    "title": "Quality Degradation Diagnosis for Voice Networks — Estimating the Perceived Noisiness, Coloration, and Discontinuity of Transmitted Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chai19b_interspeech.html": {
    "title": "A Cross-Entropy-Guided (CEG) Measure for Speech Enhancement Front-End Assessing Performances of Back-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/moller19_interspeech.html": {
    "title": "Extending the E-Model Towards Super-Wideband and Fullband Speech Communication Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sadhu19_interspeech.html": {
    "title": "Modulation Vectors as Robust Feature Representation for ASR in Domain Mismatched Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19r_interspeech.html": {
    "title": "Prosody Usage Optimization for Children Speech Recognition with Zero Resource Children Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/agrawal19_interspeech.html": {
    "title": "Unsupervised Raw Waveform Representation Learning for ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ramsay19_interspeech.html": {
    "title": "Low-Dimensional Bottleneck Features for On-Device Continuous Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/riviello19_interspeech.html": {
    "title": "Binary Speech Features for Keyword Spotting Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schneider19_interspeech.html": {
    "title": "wav2vec: Unsupervised Pre-Training for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cho19b_interspeech.html": {
    "title": "Automatic Detection of Prosodic Focus in American English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/menon19_interspeech.html": {
    "title": "Feature Exploration for Almost Zero-Resource ASR-Free Keyword Spotting Using a Multilingual Bottleneck Extractor and Correspondence Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/loweimi19_interspeech.html": {
    "title": "On Learning Interpretable CNNs with Parametric Modulated Kernel-Based Filters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/verwimp19_interspeech.html": {
    "title": "Reverse Transfer Learning: Can Word Embeddings Trained for Different NLP Tasks Improve Neural Language Models?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19l_interspeech.html": {
    "title": "Joint Grapheme and Phoneme Embeddings for Contextual End-to-End ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19h_interspeech.html": {
    "title": "Character-Aware Sub-Word Level Language Modeling for Uyghur and Turkish ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pusateri19_interspeech.html": {
    "title": "Connecting and Comparing Language Model Interpolation Techniques",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/khassanov19b_interspeech.html": {
    "title": "Enriching Rare Word Representations in Neural Language Models by Embedding Matrix Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yu19b_interspeech.html": {
    "title": "Comparative Study of Parametric and Representation Uncertainty Modeling for Recurrent Neural Network Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/agenbag19_interspeech.html": {
    "title": "Improving Automatically Induced Lexicons for Highly Agglutinating Languages Using Data-Driven Morphological Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/coucheirolimeres19_interspeech.html": {
    "title": "Attention-Based Word Vector Prediction with LSTMs and its Application to the OOV Problem in ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gao19d_interspeech.html": {
    "title": "Code-Switching Sentence Generation by Bert and Generative Adversarial Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ritchie19_interspeech.html": {
    "title": "Unified Verbalization for Speech Recognition & Synthesis Across Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sharma19e_interspeech.html": {
    "title": "Better Morphology Prediction for Better Speech Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sennema19_interspeech.html": {
    "title": "Vietnamese Learners Tackling the German /ʃt/ in Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lewis19_interspeech.html": {
    "title": "An Articulatory-Acoustic Investigation into GOOSE-Fronting in German-English Bilinguals Residing in London, UK",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jenne19_interspeech.html": {
    "title": "Multimodal Articulation-Based Pronunciation Error Detection with Spectrogram and Acoustic Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/foltz19_interspeech.html": {
    "title": "Using Prosody to Discover Word Order Alternations in a Novel Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bradlow19b_interspeech.html": {
    "title": "Speaking Rate, Information Density, and Information Rate in First-Language and Second-Language Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/graham19_interspeech.html": {
    "title": "Articulation Rate as a Metric in Spoken Language Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xu19c_interspeech.html": {
    "title": "Learning Alignment for Multimodal Emotion Recognition from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/peperkamp19b_interspeech.html": {
    "title": "Liquid Deletion in French Child-Directed Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/seidl19_interspeech.html": {
    "title": "Towards Detection of Canonical Babbling by Citizen Scientists: Performance as a Function of Clip Length",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ludusan19b_interspeech.html": {
    "title": "Nasal Consonant Discrimination in Infant- and Adult-Directed Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/marklund19_interspeech.html": {
    "title": "No Distributional Learning in Adults from Attended Listening to Non-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rasanen19_interspeech.html": {
    "title": "A Computational Model of Early Language Acquisition from Audiovisual Experiences of Young Infants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/du19b_interspeech.html": {
    "title": "The Production of Chinese Affricates /ts/ and /tsh/ by Native Urdu Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19s_interspeech.html": {
    "title": "Multi-Stream Network with Temporal Attention for Environmental Sound Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cerutti19_interspeech.html": {
    "title": "Neural Network Distillation on IoT Platforms for Sound Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lu19d_interspeech.html": {
    "title": "Class-Wise Centroid Distance Metric Learning for Acoustic Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bai19b_interspeech.html": {
    "title": "A Hybrid Approach to Acoustic Scene Classification Based on Universal Acoustic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/he19b_interspeech.html": {
    "title": "Hierarchical Pooling Structure for Weakly Labeled Sound Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/xia19_interspeech.html": {
    "title": "Sound Event Detection in Multichannel Audio Using Convolutional Time-Frequency-Channel Squeeze and Excitation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pham19b_interspeech.html": {
    "title": "A Robust Framework for Acoustic Scene Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shi19c_interspeech.html": {
    "title": "Compression of Acoustic Event Detection Models with Quantized Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19m_interspeech.html": {
    "title": "An End-to-End Audio Classification System Based on Raw Waveforms and Mix-Training Strategy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19k_interspeech.html": {
    "title": "Few-Shot Audio Classification with Attentional Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lu19e_interspeech.html": {
    "title": "Semi-Supervised Audio Classification with Consistency-Based Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mizgajski19_interspeech.html": {
    "title": "Avaya Conversational Intelligence: A Real-Time System for Spoken Language Understanding in Human-Human Call Center Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/an19b_interspeech.html": {
    "title": "Robust Keyword Spotting via Recycle-Pooling for Mobile Game",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chylek19_interspeech.html": {
    "title": "Multimodal Dialog with the MALACH Audiovisual Archive",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jelil19_interspeech.html": {
    "title": "SpeechMarker: A Voice Based Multi-Level Attendance Application",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wu19h_interspeech.html": {
    "title": "Robust Sound Recognition: A Neuromorphic Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hu19c_interspeech.html": {
    "title": "The CUHK Dysarthric Speech Recognition Systems for English and Cantonese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/schiel19_interspeech.html": {
    "title": "BAS Web Services for Automatic Subtitle Creation and Anonymization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/voe19_interspeech.html": {
    "title": "A User-Friendly and Adaptable Re-Implementation of an Acoustic Prominence Detection and Annotation Tool",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dominguez19_interspeech.html": {
    "title": "PyToBI: A Toolkit for ToBI Labeling Under Python",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/levy19_interspeech.html": {
    "title": "GECKO — A Tool for Effective Annotation of Human Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lo19b_interspeech.html": {
    "title": "SLP-AA: Tools for Sign Language Phonetic and Phonological Research",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19t_interspeech.html": {
    "title": "SANTLR: Speech Annotation Toolkit for Low Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gruber19_interspeech.html": {
    "title": "Web-Based Speech Synthesis Editor",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/perrotin19_interspeech.html": {
    "title": "GFM-Voc: A Real-Time Voice Quality Modification System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/szekely19_interspeech.html": {
    "title": "Off the Cuff: Exploring Extemporaneous Speech Delivery with TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kessler19_interspeech.html": {
    "title": "Synthesized Spoken Names: Biases Impacting Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bernardo19_interspeech.html": {
    "title": "Unbabel Talk — Human Verified Translations for Voice Instant Messaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rabiee19_interspeech.html": {
    "title": "Adjusting Pleasure-Arousal-Dominance for Continuous Emotional Text-to-Speech Synthesizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lapata19_interspeech.html": {
    "title": "Learning Natural Language Interfaces with Neural Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nautsch19c_interspeech.html": {
    "title": "The GDPR & Speech Data: Reflections of Legal and Technology Communities, First Steps Towards a Common Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/srivastava19_interspeech.html": {
    "title": "Privacy-Preserving Adversarial Representation Learning in ASR: Reality or Illusion?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nelus19_interspeech.html": {
    "title": "Privacy-Preserving Siamese Feature Extraction for Gender Recognition versus Speaker Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nelus19b_interspeech.html": {
    "title": "Privacy-Preserving Variational Information Feature Extraction for Domestic Activity Monitoring versus Speaker Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/thaine19_interspeech.html": {
    "title": "Extracting Mel-Frequency and Bark-Frequency Cepstral Coefficients from Encrypted Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zarazaga19_interspeech.html": {
    "title": "Sound Privacy: A Conversational Speech Corpus for Quantifying the Experience of Privacy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/soto19_interspeech.html": {
    "title": "Improving Code-Switched Language Modeling Performance Using Cognate Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lee19d_interspeech.html": {
    "title": "Linguistically Motivated Parallel Data Augmentation for Code-Switch Language Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rallabandi19_interspeech.html": {
    "title": "Variational Attention Using Articulatory Priors for Generating Code Mixed Speech Using Monolingual Corpora",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19l_interspeech.html": {
    "title": "Code-Switching Detection Using ASR-Generated Language Posteriors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/biswas19b_interspeech.html": {
    "title": "Semi-Supervised Acoustic Model Training for Five-Lingual Code-Switched ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ylmaz19b_interspeech.html": {
    "title": "Multi-Graph Decoding for Code-Switching ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/seki19_interspeech.html": {
    "title": "End-to-End Multilingual Multi-Speaker Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/guasch19_interspeech.html": {
    "title": "Survey Talk: Realistic Physics-Based Computational Voice Production",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mohapatra19_interspeech.html": {
    "title": "An Extended Two-Dimensional Vocal Tract Model for Fast Acoustic Simulation of Single-Axis Symmetric Three-Dimensional Tubes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/birkholz19_interspeech.html": {
    "title": "Perceptual Optimization of an Enhanced Geometric Vocal Fold Model for Articulatory Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gao19e_interspeech.html": {
    "title": "Articulatory Copy Synthesis Based on a Genetic Algorithm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shahrebabaki19_interspeech.html": {
    "title": "A Phonetic-Level Analysis of Different Input Features for Articulatory Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tuske19_interspeech.html": {
    "title": "Advancing Sequence-to-Sequence Based Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hannun19_interspeech.html": {
    "title": "Sequence-to-Sequence Speech Recognition with Time-Depth Separable Convolutions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/baskar19_interspeech.html": {
    "title": "Semi-Supervised Sequence-to-Sequence ASR Using Unpaired Speech and Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bai19c_interspeech.html": {
    "title": "Learn Spelling from Teachers: Transferring Knowledge from Language Models to Sequence-to-Sequence Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/irie19_interspeech.html": {
    "title": "On the Choice of Modeling Unit for Sequence-to-Sequence Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/weninger19b_interspeech.html": {
    "title": "Listen, Attend, Spell and Adapt: Speaker Adapted Sequence-to-Sequence ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/runarsdottir19_interspeech.html": {
    "title": "Lattice Re-Scoring During Manual Editing for Automatic Error Correction of ASR Transcripts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fukunaga19_interspeech.html": {
    "title": "GPU-Based WFST Decoding with Extra Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jorge19_interspeech.html": {
    "title": "Real-Time One-Pass Decoder for Speech Recognition Using LSTM Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/seki19b_interspeech.html": {
    "title": "Vectorized Beam Search for CTC-Attention-Based Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/serrino19_interspeech.html": {
    "title": "Contextual Recovery of Out-of-Lattice Named Entities in Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/novitasari19_interspeech.html": {
    "title": "Sequence-to-Sequence Learning via Attention Transfer for Incremental Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lian19b_interspeech.html": {
    "title": "Unsupervised Representation Learning with Future Observation Prediction for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/phan19_interspeech.html": {
    "title": "Spatio-Temporal Attention Pooling for Audio Scene Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shi19d_interspeech.html": {
    "title": "Subspace Pooling Based Temporal Features Extraction for Audio Event Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19l_interspeech.html": {
    "title": "Multi-Scale Time-Frequency Attention for Acoustic Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/song19b_interspeech.html": {
    "title": "Acoustic Scene Classification by Implicitly Identifying Distinct Sound Events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/qi19_interspeech.html": {
    "title": "Parameter-Transfer Learning for Low-Resource Individualization of Head-Related Transfer Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19i_interspeech.html": {
    "title": "Prosodic Characteristics of Mandarin Declarative and Interrogative Utterances in Parkinson's Disease",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/morovelazquez19_interspeech.html": {
    "title": "Study of the Performance of Automatic Speech Recognition Systems in Speakers with Parkinson's Disease",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19m_interspeech.html": {
    "title": "Towards the Speech Features of Mild Cognitive Impairment: Universal Evidence from Structured and Unstructured Connected Speech of Chinese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19n_interspeech.html": {
    "title": "Child Speech Disorder Detection with Siamese Recurrent Network Using Speech Attribute Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/korzekwa19_interspeech.html": {
    "title": "Interpretable Deep Learning Model for the Detection and Reconstruction of Dysarthric Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/noufi19_interspeech.html": {
    "title": "Vocal Biomarker Assessment Following Pediatric Traumatic Brain Injury: A Retrospective Cohort Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/scharenborg19b_interspeech.html": {
    "title": "Survey Talk: Reaching Over the Gap: Cross- and Interdisciplinary Research on Human and Automatic Speech Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ogawa19_interspeech.html": {
    "title": "Improved Deep Duel Model for Rescoring N-Best Speech Recognition List Using Backward LSTMLM and Ensemble Encoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/irie19b_interspeech.html": {
    "title": "Language Modeling with Deep Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/raju19_interspeech.html": {
    "title": "Scalable Multi Corpora Neural Language Models for ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/likhomanenko19_interspeech.html": {
    "title": "Who Needs Words? Lexicon-Free Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/latif19_interspeech.html": {
    "title": "Direct Modelling of Speech Emotion from Raw Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sarma19_interspeech.html": {
    "title": "Improving Emotion Identification Using Phone Posteriors in Raw Speech Waveform Based DNN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cao19_interspeech.html": {
    "title": "Pyramid Memory Block and Timestep Attention for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/oates19_interspeech.html": {
    "title": "Robust Speech Emotion Recognition Under Different Encoding Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gosztolya19c_interspeech.html": {
    "title": "Using the Bag-of-Audio-Word Feature Representation of ASR DNN Posteriors for Paralinguistic Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/williams19c_interspeech.html": {
    "title": "Disentangling Style Factors from Speaker Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hsu19c_interspeech.html": {
    "title": "Sentence Prosody and Wh-Indeterminates in Taiwan Mandarin",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hu19d_interspeech.html": {
    "title": "Frication as a Vowel Feature? — Evidence from the Rui'an Wu Chinese Dialect",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19m_interspeech.html": {
    "title": "Vowels and Diphthongs in the Xupu Xiang Chinese Dialect",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/albuquerque19_interspeech.html": {
    "title": "Age-Related Changes in European Portuguese Vowel Acoustics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lalhminghlui19_interspeech.html": {
    "title": "Vowel-Tone Interaction in Two Tibeto-Burman Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rodriguez19_interspeech.html": {
    "title": "The Vowel System of Korebaju",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ibrahim19_interspeech.html": {
    "title": "Fundamental Frequency Accommodation in Multi-Party Human-Robot Game Interactions: The Effect of Winning or Losing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wagner19_interspeech.html": {
    "title": "Pitch Accent Trajectories Across Different Conditions of Visibility and Information Structure — Evidence from Spontaneous Dyadic Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/betz19_interspeech.html": {
    "title": "The Greennn Tree — Lengthening Position Influences Uncertainty Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/si19_interspeech.html": {
    "title": "CNN-BLSTM Based Question Detection from Dialogs Considering Phase and Context Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/metcalf19_interspeech.html": {
    "title": "Mirroring to Build Trust in Digital Assistants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/raveh19_interspeech.html": {
    "title": "Three's a Crowd? Effects of a Second Human on Vocal Accommodation with a Voice Assistant",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19o_interspeech.html": {
    "title": "Adversarial Regularization for End-to-End Robust Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/monteiro19_interspeech.html": {
    "title": "Combining Speaker Recognition and Metric Learning for Speaker-Dependent Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19n_interspeech.html": {
    "title": "VAE-Based Regularization for Deep Speaker Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mingote19b_interspeech.html": {
    "title": "Language Recognition Using Triplet Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jung19c_interspeech.html": {
    "title": "Spatial Pyramid Encoding with Convex Length Normalization for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/heo19b_interspeech.html": {
    "title": "End-to-End Losses Based on Speaker Basis Vectors and All-Speaker Hard Negative Mining for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/jiang19_interspeech.html": {
    "title": "An Effective Deep Embedding Learning Architecture for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/qin19b_interspeech.html": {
    "title": "Far-Field End-to-End Text-Dependent Speaker Verification Based on Mixed Training Data with Transfer Learning and Enrollment Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ren19_interspeech.html": {
    "title": "Two-Stage Training for Chinese Dialect Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kaminishi19_interspeech.html": {
    "title": "Investigation on Blind Bandwidth Extension with a Non-Linear Function and its Evaluation of x-Vector-Based Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/khan19_interspeech.html": {
    "title": "Auto-Encoding Nearest Neighbor i-Vectors for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zheng19b_interspeech.html": {
    "title": "Towards a Fault-Tolerant Speaker Verification System: A Regularization Approach to Reduce the Condition Number",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/taherian19_interspeech.html": {
    "title": "Deep Learning Based Multi-Channel Speaker Recognition in Noisy and Reverberant Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yang19g_interspeech.html": {
    "title": "Joint Optimization of Neural Acoustic Beamforming and Dereverberation with x-Vectors for Robust Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/miao19b_interspeech.html": {
    "title": "A New Time-Frequency Attention Mechanism for TDNN and CNN-LSTM-TDNN, with Application to Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19n_interspeech.html": {
    "title": "An Attention-Based Hybrid Network for Automatic Detection of Alzheimer's Disease from Narrative Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ma19b_interspeech.html": {
    "title": "Investigating the Lombard Effect Influence on End-to-End Audio-Visual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ooster19_interspeech.html": {
    "title": "Computer, Test My Hearing\": Accurate Speech Audiometry with Smart Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/eshky19_interspeech.html": {
    "title": "Synchronising Audio and Ultrasound by Learning Cross-Modal Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pan19_interspeech.html": {
    "title": "Automatic Hierarchical Attention Neural Network for Detecting AD",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nallanthighal19_interspeech.html": {
    "title": "Deep Sensing of Breathing Signal During Conversational Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/biadsy19_interspeech.html": {
    "title": "Parrotron: An End-to-End Speech-to-Speech Conversion Model and its Applications to Hearing-Impaired Speech and Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19j_interspeech.html": {
    "title": "Exploiting Visual Features Using Bayesian Gated Neural Networks for Disordered Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vougioukas19_interspeech.html": {
    "title": "Video-Driven Speech Reconstruction Using Generative Adversarial Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19k_interspeech.html": {
    "title": "On the Use of Pitch Features for Disordered Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shillingford19_interspeech.html": {
    "title": "Large-Scale Visual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/razavi19_interspeech.html": {
    "title": "Investigating Linguistic and Semantic Features for Turn-Taking Prediction in Open-Domain Human-Computer Conversation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bechet19_interspeech.html": {
    "title": "Benchmarking Benchmarks: Introducing New Automatic Indicators for Benchmarking Spoken Language Understanding Corpora",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19l_interspeech.html": {
    "title": "A Neural Turn-Taking Model without RNN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/coman19_interspeech.html": {
    "title": "An Incremental Turn-Taking Model for Task-Oriented Dialog Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/su19b_interspeech.html": {
    "title": "Personalized Dialogue Response Generation Learned from Monologues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/heldner19_interspeech.html": {
    "title": "Voice Quality as a Turn-Taking Cue",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hara19_interspeech.html": {
    "title": "Turn-Taking Prediction Based on Detection of Transition Relevance Place",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lala19_interspeech.html": {
    "title": "Analysis of Effect and Timing of Fillers in Natural Turn-Taking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/horiguchi19_interspeech.html": {
    "title": "Multimodal Response Obligation Detection with Unsupervised Online Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/su19c_interspeech.html": {
    "title": "Follow-Up Question Generation Using Neural Tensor Network-Based Domain Ontology Population in an Interview Coaching System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tran19_interspeech.html": {
    "title": "On the Role of Style in Parsing Speech with Neural Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pasad19_interspeech.html": {
    "title": "On the Contributions of Visual and Textual Supervision in Low-Resource Semantic Speech Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19p_interspeech.html": {
    "title": "Automatic Detection of Off-Topic Spoken Responses Using Very Deep Convolutional Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/piunova19_interspeech.html": {
    "title": "Rescoring Keyword Search Confidence Estimates with Graph-Based Re-Ranking Using Acoustic Word Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/segal19_interspeech.html": {
    "title": "SpeechYOLO: Detection and Localization of Speech Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/oktem19_interspeech.html": {
    "title": "Prosodic Phrase Alignment for Machine Dubbing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tannander19_interspeech.html": {
    "title": "Spot the Pleasant People! Navigating the Cocktail Party Buzz",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19o_interspeech.html": {
    "title": "Neural Text Clustering with Document-Level Attention Based on Dynamic Soft Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bach19_interspeech.html": {
    "title": "Noisy BiLSTM-Based Models for Disfluency Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/singh19b_interspeech.html": {
    "title": "Subword RNNLM Approximations for Out-Of-Vocabulary Keyword Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/maekaku19_interspeech.html": {
    "title": "Simultaneous Detection and Localization of a Wake-Up Word Using Multi-Task Learning of the Duration and Endpoint",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lee19e_interspeech.html": {
    "title": "On Mitigating Acoustic Feedback in Hearing Aids with Frequency Warping by All-Pass Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fazel19_interspeech.html": {
    "title": "Deep Multitask Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhang19o_interspeech.html": {
    "title": "Deep Learning for Joint Acoustic Echo and Noise Cancellation with Nonlinear Distortions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/srensen19_interspeech.html": {
    "title": "Harmonic Beamformers for Non-Intrusive Speech Intelligibility Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mamun19b_interspeech.html": {
    "title": "Convolutional Neural Network-Based Speech Enhancement for Cochlear Implant Recipients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/srensen19b_interspeech.html": {
    "title": "Validation of the Non-Intrusive Codebook-Based Short Time Objective Intelligibility Metric for Processed Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/arai19_interspeech.html": {
    "title": "Predicting Speech Intelligibility of Enhanced Speech Using Phone Accuracy of DNN-Based ASR System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bu19_interspeech.html": {
    "title": "A Novel Method to Correct Steering Vectors in MVDR Beamformer for Noise Robust ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lee19f_interspeech.html": {
    "title": "End-to-End Multi-Channel Speech Enhancement Using Inter-Channel Time-Restricted Attention on Raw Waveform",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gu19b_interspeech.html": {
    "title": "Neural Spatial Filter: Target Speaker Speech Separation Assisted with Directional Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/afouras19_interspeech.html": {
    "title": "My Lips Are Concealed: Audio-Visual Speech Enhancement Through Obstructions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fujita19_interspeech.html": {
    "title": "End-to-End Neural Speaker Diarization with Permutation-Free Objectives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/india19_interspeech.html": {
    "title": "Self Multi-Head Attention for Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/vinals19b_interspeech.html": {
    "title": "Phonetically-Aware Embeddings, Wide Residual Networks with Time-Delay Neural Networks and Self Attention Models for the 2018 NIST Speaker Recognition Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tu19_interspeech.html": {
    "title": "Variational Domain Adversarial Learning for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/liu19m_interspeech.html": {
    "title": "A Unified Framework for Speaker and Utterance Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nandwana19c_interspeech.html": {
    "title": "Analysis of Critical Metadata Factors for the Calibration of Speaker Recognition Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/novotny19_interspeech.html": {
    "title": "Factorization of Discriminatively Trained i-Vector Extractor for Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/salvati19_interspeech.html": {
    "title": "End-to-End Speaker Identification in Noisy and Reverberant Environments Using Raw Waveform Convolutional Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/naini19_interspeech.html": {
    "title": "Whisper to Neutral Mapping Using Cosine Similarity Maximization in i-Vector Space for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zhu19b_interspeech.html": {
    "title": "Mixup Learning Strategies for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ferrer19_interspeech.html": {
    "title": "Optimizing a Speaker Embedding Extractor Through Backend-Driven Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lee19g_interspeech.html": {
    "title": "The NEC-TT 2018 Speaker Verification System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zheng19c_interspeech.html": {
    "title": "Autoencoder-Based Semi-Supervised Curriculum Learning for Out-of-Domain Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cai19d_interspeech.html": {
    "title": "Multi-Channel Training for End-to-End Speaker Recognition Under Reverberant and Noisy Environment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/cai19e_interspeech.html": {
    "title": "The DKU-SMIIP System for NIST 2018 Speaker Recognition Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wiesner19_interspeech.html": {
    "title": "Pretraining by Backtranslation for End-to-End ASR in Low-Resource Settings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kim19b_interspeech.html": {
    "title": "Cross-Attention End-to-End ASR for Two-Party Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chorowski19_interspeech.html": {
    "title": "Towards Using Context-Dependent Symbols in CTC Without State-Tying Decision Trees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fan19b_interspeech.html": {
    "title": "An Online Attention-Based Model for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tian19b_interspeech.html": {
    "title": "Self-Attention Transducers for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19u_interspeech.html": {
    "title": "Improving Transformer-Based Speech Recognition Systems with Compressed Structure and Speech Attributes Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bang19_interspeech.html": {
    "title": "Extending an Acoustic Data-Driven Phone Set for Spontaneous Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/moriya19_interspeech.html": {
    "title": "Joint Maximization Decoder with Neural Converters for Fully Neural Network-Based Japanese Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/parcollet19b_interspeech.html": {
    "title": "Real to H-Space Encoder for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yi19b_interspeech.html": {
    "title": "Ectc-Docd: An End-to-End Structure with CTC Encoder and OCD Decoder for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/denisov19_interspeech.html": {
    "title": "End-to-End Multi-Speaker Speech Recognition Using Speaker Embeddings and Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hayashi19_interspeech.html": {
    "title": "Pre-Trained Text Embeddings for Enhanced Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/szekely19b_interspeech.html": {
    "title": "Spontaneous Conversational Speech Synthesis from Found Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/klimkov19_interspeech.html": {
    "title": "Fine-Grained Robust Prosody Transfer for Single-Speaker Neural Text-To-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hussain19_interspeech.html": {
    "title": "Speech Driven Backchannel Generation Using Deep Q-Network for Enhancing Engagement in Human-Robot Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/koriyama19_interspeech.html": {
    "title": "Semi-Supervised Prosody Modeling Using Deep Gaussian Process Latent Variable Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/nikulasdottir19_interspeech.html": {
    "title": "Bootstrapping a Text Normalization System for an Inflected Language. Numbers as a Test Case",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/guo19e_interspeech.html": {
    "title": "Exploiting Syntactic Features in a Parsed Tree to Improve End-to-End TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ni19_interspeech.html": {
    "title": "Duration Modeling with Global Phoneme-Duration Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/aubin19_interspeech.html": {
    "title": "Improving Speech Synthesis with Discourse Relations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/tits19_interspeech.html": {
    "title": "Visualization and Interpretation of Latent Spaces for Controlling Expressive Speech Synthesis Through Audio Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yang19h_interspeech.html": {
    "title": "Pre-Trained Text Representations for Improving Front-End Text Processing in Mandarin Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/pan19b_interspeech.html": {
    "title": "A Mandarin Prosodic Boundary Prediction Model Based on Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gokcen19_interspeech.html": {
    "title": "Dual Encoder Classifier Models as Constraints in Neural Text Normalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/li19v_interspeech.html": {
    "title": "Knowledge-Based Linguistic Encoding for End-to-End Mandarin Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shankar19c_interspeech.html": {
    "title": "Automated Emotion Morphing in Speech Based on Diffeomorphic Curve Registration and Highway Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/connaghan19_interspeech.html": {
    "title": "Use of Beiwe Smartphone App to Identify and Track Speech Decline in Amyotrophic Lateral Sclerosis (ALS)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/rowe19_interspeech.html": {
    "title": "Profiling Speech Motor Impairments in Persons with Amyotrophic Lateral Sclerosis: An Acoustic-Based Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/mayle19_interspeech.html": {
    "title": "Diagnosing Dysarthria with Long Short-Term Memory Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/sudro19_interspeech.html": {
    "title": "Modification of Devoicing Error in Cleft Lip and Palate Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/eshghi19_interspeech.html": {
    "title": "Reduced Task Adaptation in Alternating Motion Rate Tasks as an Early Marker of Bulbar Involvement in Amyotrophic Lateral Sclerosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19q_interspeech.html": {
    "title": "Towards the Speech Features of Early-Stage Dementia: Design and Application of the Mandarin Elderly Cognitive Speech Database",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/chen19p_interspeech.html": {
    "title": "Acoustic Characteristics of Lexical Tone Disruption in Mandarin Speakers After Brain Damage",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/hermes19_interspeech.html": {
    "title": "Intragestural Variation in Natural Sentence Production: Essential Tremor Patients Treated with DBS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/kalita19_interspeech.html": {
    "title": "Nasal Air Emission in Sibilant Fricatives of Cleft Lip and Palate Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/serrano19_interspeech.html": {
    "title": "Parallel vs. Non-Parallel Voice Conversion for Esophageal Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dubey19b_interspeech.html": {
    "title": "Hypernasality Severity Detection Using Constant Q Cepstral Coefficients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/niu19_interspeech.html": {
    "title": "Automatic Depression Level Detection via ℓp-Norm Pooling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bn19_interspeech.html": {
    "title": "Comparison of Speech Tasks and Recording Devices for Voice Based Automatic Classification of Healthy Subjects and Patients with Amyotrophic Lateral Sclerosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/wang19r_interspeech.html": {
    "title": "A Modified Algorithm for Multiple Input Spectrogram Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bahmaninezhad19_interspeech.html": {
    "title": "A Comprehensive Study of Speech Separation: Spectrogram vs Waveform Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/inan19_interspeech.html": {
    "title": "Evaluating Audiovisual Source Separation in the Context of Video Conferencing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/ditter19_interspeech.html": {
    "title": "Influence of Speaker-Specific Parameters on Speech Separation Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/zegers19_interspeech.html": {
    "title": "CNN-LSTM Models for Multi-Speaker Source Separation Using Bayesian Hyper Parameter Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/bear19_interspeech.html": {
    "title": "Towards Joint Sound Scene and Polyphonic Sound Event Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/fan19c_interspeech.html": {
    "title": "Discriminative Learning for Monaural Speech Separation Using Deep Embedding Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/yousefi19_interspeech.html": {
    "title": "Probabilistic Permutation Invariant Training for Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shi19e_interspeech.html": {
    "title": "Which Ones Are Speaking? Speaker-Inferred Model for Multi-Talker Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/shi19f_interspeech.html": {
    "title": "End-to-End Monaural Speech Separation with Multi-Scale Dynamic Weighted Gated Dilated Convolutional Pyramid Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/lluis19_interspeech.html": {
    "title": "End-to-End Music Source Separation: Is it Possible in the Waveform Domain?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/foley19_interspeech.html": {
    "title": "Elpis, an Accessible Speech-to-Text Tool",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/gruber19b_interspeech.html": {
    "title": "Framework for Conducting Tasks Requiring Human Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/huang19k_interspeech.html": {
    "title": "Multimedia Simultaneous Translation System for Minority Language Communication with Mandarin",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/dikici19_interspeech.html": {
    "title": "The SAIL LABS Media Mining Indexer and the CAVA Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2019/goel19b_interspeech.html": {
    "title": "CaptionAI: A Real-Time Multilingual Captioning Application",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  }
}