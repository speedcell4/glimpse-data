{
  "https://jmlr.org/papers/v25/19-301.html": {
    "title": "On Truthing Issues in Supervised Classification",
    "volume": "main",
    "abstract": "Ideal supervised classification assumes known correct labels, but various truthing issues can arise in practice: noisy labels; multiple, conflicting labels for a sample; missing labels; and different labeler combinations for different samples. Previous work introduced a noisy-label model, which views the observed noisy labels as random variables conditioned on the unobserved correct labels. It has mainly focused on estimating the conditional distribution of the noisy labels and the class prior, as well as estimating the correct labels or training with noisy labels. In a complementary manner, given the conditional distribution and class prior, we apply estimation theory to classifier testing, training, and comparison of different combinations of labelers. First, for binary classification, we construct a testing model and derive approximate marginal posteriors for accuracy, precision, recall, probability of false alarm, and F-score, and joint posteriors for ROC and precision-recall analysis. We propose minimum mean-square error (MMSE) testing, which employs empirical Bayes algorithms to estimate the testing-model parameters and then computes optimal point estimates and credible regions for the metrics. We extend the approach to multi-class classification to obtain optimal estimates of accuracy and individual confusion-matrix elements. Second, we present a unified view of training that covers probabilistic (i.e., discriminative or generative) and non-probabilistic models. For the former, we adjust maximum-likelihood or maximum a posteriori training for truthing issues; for the latter, we propose MMSE training, which minimizes the MMSE estimate of the empirical risk. We also describe suboptimal training that is compatible with existing infrastructure. Third, we observe that mutual information lets one express any labeler combination as an equivalent single labeler, implying that multiple mediocre labelers can be as informative as, or more informative than, a single expert labeler. Experiments demonstrate the effectiveness of the methods and confirm the implication",
    "checked": false,
    "id": "de7f953fdbeec9e6639d0d150ebfd0ff0874683b",
    "semantic_title": "expansion and shrinkage of localization for weakly-supervised semantic segmentation",
    "citation_count": 26,
    "authors": [
      "Jonathan K. Su"
    ]
  },
  "https://jmlr.org/papers/v25/21-0264.html": {
    "title": "Lower Complexity Bounds of Finite-Sum Optimization Problems: The Results and Construction",
    "volume": "main",
    "abstract": "In this paper we study the lower complexity bounds for finite-sum optimization problems, where the objective is the average of $n$ individual component functions. We consider a so-called proximal incremental first-order oracle (PIFO) algorithm, which employs the individual component function's gradient and proximal information provided by PIFO to update the variable. To incorporate loopless methods, we also allow the PIFO algorithm to obtain the full gradient infrequently. We develop a novel approach to constructing the hard instances, which partitions the tridiagonal matrix of classical examples into $n$ groups. This construction is friendly to the analysis of PIFO algorithms. Based on this construction, we establish the lower complexity bounds for finite-sum minimax optimization problems when the objective is convex-concave or nonconvex-strongly-concave and the class of component functions is $L$-average smooth. Most of these bounds are nearly matched by existing upper bounds up to log factors. We also derive similar lower bounds for finite-sum minimization problems as previous work under both smoothness and average smoothness assumptions. Our lower bounds imply that proximal oracles for smooth functions are not much more powerful than gradient oracles",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuze Han",
      "Guangzeng Xie",
      "Zhihua Zhang"
    ]
  },
  "https://jmlr.org/papers/v25/21-1137.html": {
    "title": "Power of knockoff: The impact of ranking algorithm, augmented design, and symmetric statistic",
    "volume": "main",
    "abstract": "The knockoff filter is a recent false discovery rate (FDR) control method for high-dimensional linear models. We point out that knockoff has three key components: ranking algorithm, augmented design, and symmetric statistic, and each component admits multiple choices. By considering various combinations of the three components, we obtain a collection of variants of knockoff. All these variants guarantee finite-sample FDR control, and our goal is to compare their power. We assume a Rare and Weak signal model on regression coeffi- cients and compare the power of different variants of knockoff by deriving explicit formulas of false positive rate and false negative rate. Our results provide new insights on how to improve power when controlling FDR at a targeted level. We also compare the power of knockoff with its propotype - a method that uses the same ranking algorithm but has access to an ideal threshold. The comparison reveals the additional price one pays by finding a data-driven threshold to control FDR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Tracy Ke",
      "Jun S. Liu",
      "Yucong Ma"
    ]
  },
  "https://jmlr.org/papers/v25/21-1205.html": {
    "title": "Fast Policy Extragradient Methods for Competitive Games with Entropy Regularization",
    "volume": "main",
    "abstract": "This paper investigates the problem of computing the equilibrium of competitive games in the form of two-player zero-sum games, which is often modeled as a constrained saddle-point optimization problem with probability simplex constraints. Despite recent efforts in understanding the last-iterate convergence of extragradient methods in the unconstrained setting, the theoretical underpinnings of these methods in the constrained settings, especially those using multiplicative updates, remain highly inadequate, even when the objective function is bilinear. Motivated by the algorithmic role of entropy regularization in single-agent reinforcement learning and game theory, we develop provably efficient extragradient methods to find the quantal response equilibrium (QRE)---which are solutions to zero-sum two-player matrix games with entropy regularization---at a linear rate. The proposed algorithms can be implemented in a decentralized manner, where each player executes symmetric and multiplicative updates iteratively using its own payoff without observing the opponent's actions directly. In addition, by controlling the knob of entropy regularization, the proposed algorithms can locate an approximate Nash equilibrium of the unregularized matrix game at a sublinear rate without assuming the Nash equilibrium to be unique. Our methods also lead to efficient policy extragradient algorithms for solving (entropy-regularized) zero-sum Markov games at similar rates. All of our convergence rates are nearly dimension-free, which are independent of the size of the state and action spaces up to logarithm factors, highlighting the positive role of entropy regularization for accelerating convergence",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shicong Cen",
      "Yuting Wei",
      "Yuejie Chi"
    ]
  },
  "https://jmlr.org/papers/v25/22-0402.html": {
    "title": "Seeded Graph Matching for the Correlated Gaussian Wigner Model via the Projected Power Method",
    "volume": "main",
    "abstract": "In the graph matching problem we observe two graphs $G,H$ and the goal is to find an assignment (or matching) between their vertices such that some measure of edge agreement is maximized. We assume in this work that the observed pair $G,H$ has been drawn from the Correlated Gaussian Wigner (CGW) model -- a popular model for correlated weighted graphs -- where the entries of the adjacency matrices of $G$ and $H$ are independent Gaussians and each edge of $G$ is correlated with one edge of $H$ (determined by the unknown matching) with the edge correlation described by a parameter $\\sigma \\in [0,1)$. In this paper, we analyse the performance of the projected power method (PPM) as a seeded graph matching algorithm where we are given an initial partially correct matching (called the seed) as side information. We prove that if the seed is close enough to the ground-truth matching, then with high probability, PPM iteratively improves the seed and recovers the ground-truth matching (either partially or exactly) in $O(\\log n)$ iterations. Our results prove that PPM works even in regimes of constant $\\sigma$, thus extending the analysis in (Mao et al., 2023) for the sparse Correlated Erdos-Renyi (CER) model to the (dense) CGW model. As a byproduct of our analysis, we see that the PPM framework generalizes some of the state-of-art algorithms for seeded graph matching. We support and complement our theoretical findings with numerical experiments on synthetic data",
    "checked": false,
    "id": "44c09824b294898d512f324461fc8d84a5cc0bc6",
    "semantic_title": "seeded graph matching for the correlated wigner model via the projected power method",
    "citation_count": 2,
    "authors": [
      "Ernesto Araya",
      "Guillaume Braun",
      "Hemant Tyagi"
    ]
  },
  "https://jmlr.org/papers/v25/22-0687.html": {
    "title": "Model-Free Representation Learning and Exploration in Low-Rank MDPs",
    "volume": "main",
    "abstract": "The low-rank MDP has emerged as an important model for studying representation learning and exploration in reinforcement learning. With a known representation, several model-free exploration strategies exist. In contrast, all algorithms for the unknown representation setting are model-based, thereby requiring the ability to model the full dynamics. In this work, we present the first model-free representation learning algorithms for low-rank MDPs. The key algorithmic contribution is a new minimax representation learning objective, for which we provide variants with differing tradeoffs in their statistical and computational properties. We interleave this representation learning step with an exploration strategy to cover the state space in a reward-free manner. The resulting algorithms are provably sample efficient and can accommodate general function approximation to scale to complex environments",
    "checked": false,
    "id": "202055c2ded63cd1d36bf81479e17a4595ba3009",
    "semantic_title": "efficient model-free exploration in low-rank mdps",
    "citation_count": 2,
    "authors": [
      "Aditya Modi",
      "Jinglin Chen",
      "Akshay Krishnamurthy",
      "Nan Jiang",
      "Alekh Agarwal"
    ]
  },
  "https://jmlr.org/papers/v25/22-0801.html": {
    "title": "Decorrelated Variable Importance",
    "volume": "main",
    "abstract": "Because of the widespread use of black box prediction methods such as random forests and neural nets, there is renewed interest in developing methods for quantifying variable importance as part of the broader goal of interpretable prediction. A popular approach is to define a variable importance parameter --- known as LOCO (Leave Out COvariates) --- based on dropping covariates from a regression model. This is essentially a nonparametric version of $R^2$. This parameter is very general and can be estimated nonparametrically, but it can be hard to interpret because it is affected by correlation between covariates. We propose a method for mitigating the effect of correlation by defining a modified version of LOCO. This new parameter is difficult to estimate nonparametrically, but we show how to estimate it using semiparametric models",
    "checked": false,
    "id": "04ac6aaef409574fef63484a48f751ee496cbe23",
    "semantic_title": "puboi: a tunable benchmark with variable importance",
    "citation_count": 2,
    "authors": [
      "Isabella Verdinelli",
      "Larry Wasserman"
    ]
  },
  "https://jmlr.org/papers/v25/22-1120.html": {
    "title": "On Efficient and Scalable Computation of the Nonparametric Maximum Likelihood Estimator in Mixture Models",
    "volume": "main",
    "abstract": "In this paper, we focus on the computation of the nonparametric maximum likelihood estimator (NPMLE) in multivariate mixture models. Our approach discretizes this infinite dimensional convex optimization problem by setting fixed support points for the NPMLE and optimizing over the mixing proportions. We propose an efficient and scalable semismooth Newton based augmented Lagrangian method (ALM). Our algorithm outperforms the state-of-the-art methods (Kim et al., 2020; Koenker and Gu, 2017), capable of handling $n \\approx 10^6$ data points with $m \\approx 10^4$ support points. A key advantage of our approach is its strategic utilization of the solution's sparsity, leading to structured sparsity in Hessian computations. As a result, our algorithm demonstrates better scaling in terms of $m$ when compared to the mixsqp method (Kim et al., 2020). The computed NPMLE can be directly applied to denoising the observations in the framework of empirical Bayes. We propose new denoising estimands in this context along with their consistent estimates. Extensive numerical experiments are conducted to illustrate the efficiency of our ALM. In particular, we employ our method to analyze two astronomy data sets: (i) Gaia-TGAS Catalog (Anderson et al., 2018) containing approximately $1.4 \\times 10^6$ data points in two dimensions, and (ii) a data set from the APOGEE survey (Majewski et al., 2017) with approximately $2.7 \\times 10^4$ data points",
    "checked": true,
    "id": "3e825dd7ef352803255b99841c0ad5d60f982021",
    "semantic_title": "on efficient and scalable computation of the nonparametric maximum likelihood estimator in mixture models",
    "citation_count": 4,
    "authors": [
      "Yangjing Zhang",
      "Ying Cui",
      "Bodhisattva Sen",
      "Kim-Chuan Toh"
    ]
  },
  "https://jmlr.org/papers/v25/22-1251.html": {
    "title": "Exploration, Exploitation, and Engagement in Multi-Armed Bandits with Abandonment",
    "volume": "main",
    "abstract": "The traditional multi-armed bandit (MAB) model for recommendation systems assumes the user stays in the system for the entire learning horizon. In new online education platforms such as ALEKS or new video recommendation systems such as TikTok, the amount of time a user spends on the app depends on how engaging the recommended contents are. Users may temporarily leave the system if the recommended items cannot engage the users. To understand the exploration, exploitation, and engagement in these systems, we propose a new model, called MAB-A where \"A\" stands for abandonment and the abandonment probability depends on the current recommended item and the user's past experience (called state). We propose two algorithms, ULCB and KL-ULCB, both of which do more exploration (being optimistic) when the user likes the previous recommended item and less exploration (being pessimistic) when the user does not. We prove that both ULCB and KL-ULCB achieve logarithmic regret, $O(\\log K)$, where $K$ is the number of visits (or episodes). Furthermore, the regret bound under KL-ULCB is asymptotically sharp. We also extend the proposed algorithms to the general-state setting. Simulation results show that the proposed algorithms have significantly lower regret than the traditional UCB and KL-UCB, and Q-learning-based algorithms",
    "checked": false,
    "id": "ff9885f3ac91fac6594110b334370a344bbc761f",
    "semantic_title": "exploration. exploitation, and engagement in multi-armed bandits with abandonment",
    "citation_count": 2,
    "authors": [
      "Zixian Yang",
      "Xin Liu",
      "Lei Ying"
    ]
  },
  "https://jmlr.org/papers/v25/22-1317.html": {
    "title": "Modeling Random Networks with Heterogeneous Reciprocity",
    "volume": "main",
    "abstract": "Reciprocity, or the tendency of individuals to mirror behavior, is a key measure that describes information exchange in a social network. Users in social networks tend to engage in different levels of reciprocal behavior. Differences in such behavior may indicate the existence of communities that reciprocate links at varying rates. In this paper, we develop methodology to model the diverse reciprocal behavior in growing social networks. In particular, we present a preferential attachment model with heterogeneous reciprocity that imitates the attraction users have for popular users, plus the heterogeneous nature by which they reciprocate links. We compare Bayesian and frequentist model fitting techniques for large networks, as well as computationally efficient variational alternatives. Cases where the number of communities is known and unknown are both considered. We apply the presented methods to the analysis of Facebook and Reddit networks where users have non-uniform reciprocal behavior patterns. The fitted model captures the heavy-tailed nature of the empirical degree distributions in the datasets and identifies multiple groups of users that differ in their tendency to reply to and receive responses to wallposts and comments",
    "checked": true,
    "id": "3353aad04307f4f793bd37c1ae9ba788b90502fb",
    "semantic_title": "modeling random networks with heterogeneous reciprocity",
    "citation_count": 1,
    "authors": [
      "Daniel Cirkovic",
      "Tiandong Wang"
    ]
  },
  "https://jmlr.org/papers/v25/22-1396.html": {
    "title": "Estimating the Minimizer and the Minimum Value of a Regression Function under Passive Design",
    "volume": "main",
    "abstract": "We propose a new method for estimating the minimizer $\\boldsymbol{x}^*$ and the minimum value $f^*$ of a smooth and strongly convex regression function $f$ from the observations contaminated by random noise. Our estimator $\\boldsymbol{z}_n$ of the minimizer $\\boldsymbol{x}^*$ is based on a version of the projected gradient descent with the gradient estimated by a regularized local polynomial algorithm. Next, we propose a two-stage procedure for estimation of the minimum value $f^*$ of regression function $f$. At the first stage, we construct an accurate enough estimator of $\\boldsymbol{x}^*$, which can be, for example, $\\boldsymbol{z}_n$. At the second stage, we estimate the function value at the point obtained in the first stage using a rate optimal nonparametric procedure. We derive non-asymptotic upper bounds for the quadratic risk and optimization risk of $\\boldsymbol{z}_n$, and for the risk of estimating $f^*$. We establish minimax lower bounds showing that, under certain choice of parameters, the proposed algorithms achieve the minimax optimal rates of convergence on the class of smooth and strongly convex functions",
    "checked": true,
    "id": "c2c80e8e501118e1f46536b1e0d618ec062ad2b7",
    "semantic_title": "estimating the minimizer and the minimum value of a regression function under passive design",
    "citation_count": 0,
    "authors": [
      "Arya Akhavan",
      "Davit Gogolashvili",
      "Alexandre B. Tsybakov"
    ]
  },
  "https://jmlr.org/papers/v25/23-0119.html": {
    "title": "Critically Assessing the State of the Art in Neural Network Verification",
    "volume": "main",
    "abstract": "Recent research has proposed various methods to formally verify neural networks against minimal input perturbations; this verification task is also known as local robustness verification. The research area of local robustness verification is highly diverse, as verifiers rely on a multitude of techniques, including mixed integer programming and satisfiability modulo theories. At the same time, the problem instances encountered when performing local robustness verification differ based on the network to be verified, the property to be verified and the specific network input. This raises the question of which verification algorithm is most suitable for solving specific types of instances of the local robustness verification problem. To answer this question, we performed a systematic performance analysis of several CPU- and GPU-based local robustness verification systems on a newly and carefully assembled set of 79 neural networks, of which we verified a broad range of robustness properties, while taking a practitioner's point of view -- a perspective that complements the insights from initiatives such as the VNN competition, where the participating tools are carefully adapted to the given benchmarks by their developers. Notably, we show that no single best algorithm dominates performance across all verification problem instances. Instead, our results reveal complementarities in verifier performance and illustrate the potential of leveraging algorithm portfolios for more efficient local robustness verification. We quantify this complementarity using various performance measures, such as the Shapley value. Furthermore, we confirm the notion that most algorithms only support ReLU-based networks, while other activation functions remain under-supported",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthias König",
      "Annelot W. Bosman",
      "Holger H. Hoos",
      "Jan N. van Rijn"
    ]
  },
  "https://jmlr.org/papers/v25/23-0237.html": {
    "title": "A Comparison of Continuous-Time Approximations to Stochastic Gradient Descent",
    "volume": "main",
    "abstract": "Applying a stochastic gradient descent (SGD) method for minimizing an objective gives rise to a discrete-time process of estimated parameter values. In order to better understand the dynamics of the estimated values, many authors have considered continuous-time approximations of SGD. We refine existing results on the weak error of first-order ODE and SDE approximations to SGD for non-infinitesimal learning rates. In particular, we explicitly compute the linear term in the error expansion of gradient flow and two of its stochastic counterparts, with respect to a discretization parameter $h$. In the example of linear regression, we demonstrate the general inferiority of the deterministic gradient flow approximation in comparison to the stochastic ones, for batch sizes which are not too large. Further, we demonstrate that for Gaussian features an SDE approximation with state-independent noise (CC) is preferred over using a state-dependent coefficient (NCC). The same comparison holds true for features of low kurtosis or large batch sizes. However, the relationship reverses for highly leptokurtic features or small batch sizes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefan Ankirchner",
      "Stefan Perko"
    ]
  },
  "https://jmlr.org/papers/v25/23-0356.html": {
    "title": "Improving physics-informed neural networks with meta-learned optimization",
    "volume": "main",
    "abstract": "We show that the error achievable using physics-informed neural networks for solving differential equations can be substantially reduced when these networks are trained using meta-learned optimization methods rather than using fixed, hand-crafted optimizers as traditionally done. We choose a learnable optimization method based on a shallow multi-layer perceptron that is meta-trained for specific classes of differential equations. We illustrate meta-trained optimizers for several equations of practical relevance in mathematical physics, including the linear advection equation, Poisson's equation, the Korteweg-de Vries equation and Burgers' equation. We also illustrate that meta-learned optimizers exhibit transfer learning abilities, in that a meta-trained optimizer on one differential equation can also be successfully deployed on another differential equation",
    "checked": true,
    "id": "4f9f96679e943f7447ac431dffea506767d3cf3f",
    "semantic_title": "improving physics-informed neural networks with meta-learned optimization",
    "citation_count": 6,
    "authors": [
      "Alex Bihlo"
    ]
  },
  "https://jmlr.org/papers/v25/23-0549.html": {
    "title": "On the Effect of Initialization: The Scaling Path of 2-Layer Neural Networks",
    "volume": "main",
    "abstract": "In supervised learning, the regularization path is sometimes used as a convenient theoretical proxy for the optimization path of gradient descent initialized from zero. In this paper, we study a modification of the regularization path for infinite-width 2-layer ReLU neural networks with nonzero initial distribution of the weights at different scales. By exploiting a link with unbalanced optimal-transport theory, we show that, despite the non-convexity of the 2-layer network training, this problem admits an infinite-dimensional convex counterpart. We formulate the corresponding functional-optimization problem and investigate its main properties. In particular, we show that, as the scale of the initialization ranges between $0$ and $+\\infty$, the associated path interpolates continuously between the so-called kernel and rich regimes. Numerical experiments confirm that, in our setting, the scaling path and the final states of the optimization path behave similarly, even beyond these extreme points",
    "checked": true,
    "id": "7e44a6681ad51b652b27aa5f2faf309ec8c1b057",
    "semantic_title": "on the effect of initialization: the scaling path of 2-layer neural networks",
    "citation_count": 1,
    "authors": [
      "Sebastian Neumayer",
      "Lénaïc Chizat",
      "Michael Unser"
    ]
  },
  "https://jmlr.org/papers/v25/23-0661.html": {
    "title": "Localized Debiased Machine Learning: Efficient Inference on Quantile Treatment Effects and Beyond",
    "volume": "main",
    "abstract": "We consider estimating a low-dimensional parameter in an estimating equation involving high-dimensional nuisance functions that depend on the target parameter as an input. A central example is the efficient estimating equation for the (local) quantile treatment effect ((L)QTE) in causal inference, which involves the covariate-conditional cumulative distribution function evaluated at the quantile to be estimated. Existing approaches based on flexibly estimating the nuisances and plugging in the estimates, such as debiased machine learning (DML), require we learn the nuisance at all possible inputs. For (L)QTE, DML requires we learn the whole covariate-conditional cumulative distribution function. We instead propose localized debiased machine learning (LDML), which avoids this burdensome step and needs only estimate nuisances at a single initial rough guess for the target parameter. For (L)QTE, LDML involves learning just two regression functions, a standard task for machine learning methods. We prove that under lax rate conditions our estimator has the same favorable asymptotic behavior as the infeasible estimator that uses the unknown true nuisances. Thus, LDML notably enables practically-feasible and theoretically-grounded efficient estimation of important quantities in causal inference such as (L)QTEs when we must control for many covariates and/or flexible relationships, as we demonstrate in empirical studies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nathan Kallus",
      "Xiaojie Mao",
      "Masatoshi Uehara"
    ]
  },
  "https://jmlr.org/papers/v25/23-0893.html": {
    "title": "On Sufficient Graphical Models",
    "volume": "main",
    "abstract": "We introduce a sufficient graphical model by applying the recently developed nonlinear sufficient dimension reduction techniques to the evaluation of conditional independence. The graphical model is nonparametric in nature, as it does not make distributional assumptions such as the Gaussian or copula Gaussian assumptions. However, unlike a fully nonparametric graphical model, which relies on the high-dimensional kernel to characterize conditional independence, our graphical model is based on conditional independence given a set of sufficient predictors with a substantially reduced dimension. In this way we avoid the curse of dimensionality that comes with a high-dimensional kernel. We develop the population-level properties, convergence rate, and variable selection consistency of our estimate. By simulation comparisons and an analysis of the DREAM 4 Challenge data set, we demonstrate that our method outperforms the existing methods when the Gaussian or copula Gaussian assumptions are violated, and its performance remains excellent in the high-dimensional setting",
    "checked": true,
    "id": "d1874b50b937cf6135c0e7320c09303a005cbcd6",
    "semantic_title": "on sufficient graphical models",
    "citation_count": 0,
    "authors": [
      "Bing Li",
      "Kyongwon Kim"
    ]
  },
  "https://jmlr.org/papers/v25/23-1015.html": {
    "title": "Black Box Variational Inference with a Deterministic Objective: Faster, More Accurate, and Even More Black Box",
    "volume": "main",
    "abstract": "Automatic differentiation variational inference (ADVI) offers fast and easy-to-use posterior approximation in multiple modern probabilistic programming languages. However, its stochastic optimizer lacks clear convergence criteria and requires tuning parameters. Moreover, ADVI inherits the poor posterior uncertainty estimates of mean-field variational Bayes (MFVB). We introduce \"deterministic ADVI\" (DADVI) to address these issues. DADVI replaces the intractable MFVB objective with a fixed Monte Carlo approximation, a technique known in the stochastic optimization literature as the \"sample average approximation\" (SAA). By optimizing an approximate but deterministic objective, DADVI can use off-the-shelf second-order optimization, and, unlike standard mean-field ADVI, is amenable to more accurate posterior covariances via linear response (LR). In contrast to existing worst-case theory, we show that, on certain classes of common statistical problems, DADVI and the SAA can perform well with relatively few samples even in very high dimensions, though we also show that such favorable results cannot extend to variational approximations that are too expressive relative to mean-field ADVI. We show on a variety of real-world problems that DADVI reliably finds good solutions with default settings (unlike ADVI) and, together with LR covariances, is typically faster and more accurate than standard ADVI",
    "checked": true,
    "id": "d249c77652aaf90d881c488c582823fd626c28a1",
    "semantic_title": "black box variational inference with a deterministic objective: faster, more accurate, and even more black box",
    "citation_count": 7,
    "authors": [
      "Ryan Giordano",
      "Martin Ingram",
      "Tamara Broderick"
    ]
  },
  "https://jmlr.org/papers/v25/20-075.html": {
    "title": "Nonparametric Inference under B-bits Quantization",
    "volume": "main",
    "abstract": "Statistical inference based on lossy or incomplete samples is often needed in research areas such as signal/image processing, medical image storage, remote sensing, signal transmission. In this paper, we propose a nonparametric testing procedure based on samples quantized to $B$ bits through a computationally efficient algorithm. Under mild technical conditions, we establish the asymptotic properties of the proposed test statistic and investigate how the testing power changes as $B$ increases. In particular, we show that if $B$ exceeds a certain threshold, the proposed nonparametric testing procedure achieves the classical minimax rate of testing (Shang and Cheng, 2015) for spline models. We further extend our theoretical investigations to a nonparametric linearity test and an adaptive nonparametric test, expanding the applicability of the proposed methods. Extensive simulation studies {together with a real-data analysis} are used to demonstrate the validity and effectiveness of the proposed tests",
    "checked": false,
    "id": "b50b3a362c6a954c40aba6439710a03128137611",
    "semantic_title": "ohq: on-chip hardware-aware quantization",
    "citation_count": 0,
    "authors": [
      "Kexuan Li",
      "Ruiqi Liu",
      "Ganggang Xu",
      "Zuofeng Shang"
    ]
  },
  "https://jmlr.org/papers/v25/21-1125.html": {
    "title": "Iterate Averaging in the Quest for Best Test Error",
    "volume": "main",
    "abstract": "We analyse and explain the increased generalisation performance of iterate averaging using a Gaussian process perturbation model between the true and batch risk surface on the high dimensional quadratic. We derive three phenomena from our theoretical results: (1) The importance of combining iterate averaging (IA) with large learning rates and regularisation for improved generalisation. (2) Justification for less frequent averaging. (3) That we expect adaptive gradient methods to work equally well, or better, with iterate averaging than their non-adaptive counterparts. Inspired by these results, together with empirical investigations of the importance of appropriate regularisation for the solution diversity of the iterates, we propose two adaptive algorithms with iterate averaging. These give significantly better results compared to stochastic gradient descent (SGD), require less tuning and do not require early stopping or validation set monitoring. We showcase the efficacy of our approach on the CIFAR-10/100, ImageNet and Penn Treebank datasets on a variety of modern and classical network architectures",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diego Granziol",
      "Nicholas P. Baskerville",
      "Xingchen Wan",
      "Samuel Albanie",
      "Stephen Roberts"
    ]
  },
  "https://jmlr.org/papers/v25/21-1190.html": {
    "title": "Pursuit of the Cluster Structure of Network Lasso: Recovery Condition and Non-convex Extension",
    "volume": "main",
    "abstract": "Network lasso (NL for short) is a technique for estimating models by simultaneously clustering data samples and fitting the models to them. It often succeeds in forming clusters thanks to the geometry of the sum of $\\ell_2$ norm employed therein, but there may be limitations due to the convexity of the regularizer. This paper focuses on clustering generated by NL and strengthens it by creating a non-convex extension, called network trimmed lasso (NTL for short). Specifically, we initially investigate a sufficient condition that guarantees the recovery of the latent cluster structure of NL on the basis of the result of Sun et al. (2021) for convex clustering, which is a special case of NL for ordinary clustering. Second, we extend NL to NTL to incorporate a cardinality (or, $\\ell_0$-)constraint and rewrite the constrained optimization problem defined with the $\\ell_0$ norm, a discontinuous function, into an equivalent unconstrained continuous optimization problem. We develop ADMM algorithms to solve NTL and show their convergence results. Numerical illustrations indicate that the non-convex extension provides a more clear-cut cluster structure when NL fails to form clusters without incorporating prior knowledge of the associated parameters",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shotaro Yagishita",
      "Jun-ya Gotoh"
    ]
  },
  "https://jmlr.org/papers/v25/22-0068.html": {
    "title": "On the Generalization of Stochastic Gradient Descent with Momentum",
    "volume": "main",
    "abstract": "While momentum-based accelerated variants of stochastic gradient descent (SGD) are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. In this work, we first show that there exists a convex loss function for which the stability gap for multiple epochs of SGD with standard heavy-ball momentum (SGDM) becomes unbounded. Then, for smooth Lipschitz loss functions, we analyze a modified momentum-based update rule, i.e., SGD with early momentum (SGDEM) under a broad range of step-sizes, and show that it can train machine learning models for multiple epochs with a guarantee for generalization. Finally, for the special case of strongly convex loss functions, we find a range of momentum such that multiple epochs of standard SGDM, as a special form of SGDEM, also generalizes. Extending our results on generalization, we also develop an upper bound on the expected true risk, in terms of the number of training steps, sample size, and momentum. Our experimental evaluations verify the consistency between the numerical results and our theoretical bounds. SGDEM improves the generalization error of SGDM when training ResNet-18 on ImageNet in practical distributed settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Ramezani-Kebrya",
      "Kimon Antonakopoulos",
      "Volkan Cevher",
      "Ashish Khisti",
      "Ben Liang"
    ]
  },
  "https://jmlr.org/papers/v25/22-0487.html": {
    "title": "Post-Regularization Confidence Bands for Ordinary Differential Equations",
    "volume": "main",
    "abstract": "Ordinary differential equation (ODE) is an important tool to study a system of biological and physical processes. A central question in ODE modeling is to infer the significance of individual regulatory effect of one signal variable on another. However, building confidence band for ODE with unknown regulatory relations is challenging, and it remains largely an open question. In this article, we construct the post-regularization confidence band for the individual regulatory function in ODE with unknown functionals and noisy data observations. Our proposal is the first of its kind, and is built on two novel ingredients. The first is a new localized kernel learning approach that combines reproducing kernel learning with local Taylor approximation, and the second is a new de-biasing method that tackles infinite-dimensional functionals and additional measurement errors. We show that the constructed confidence band has the desired asymptotic coverage probability, and the recovered regulatory network approaches the truth with probability tending to one. We establish the theoretical properties when the number of variables in the system can be either smaller or larger than the number of sampling time points, and we study the regime-switching phenomenon. We demonstrate the efficacy of the proposed method through both simulations and illustrations with two data applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaowu Dai",
      "Lexin Li"
    ]
  },
  "https://jmlr.org/papers/v25/22-0719.html": {
    "title": "Deep Nonparametric Estimation of Operators between Infinite Dimensional Spaces",
    "volume": "main",
    "abstract": "Learning operators between infinitely dimensional spaces is an important learning task arising in machine learning, imaging science, mathematical modeling and simulations, etc. This paper studies the nonparametric estimation of Lipschitz operators using deep neural networks. Non-asymptotic upper bounds are derived for the generalization error of the empirical risk minimizer over a properly chosen network class. Under the assumption that the target operator exhibits a low dimensional structure, our error bounds decay as the training sample size increases, with an attractive fast rate depending on the intrinsic dimension in our estimation. Our assumptions cover most scenarios in real applications and our results give rise to fast rates by exploiting low dimensional structures of data in operator estimation. We also investigate the influence of network structures (e.g., network width, depth, and sparsity) on the generalization error of the neural network estimator and propose a general suggestion on the choice of network structures to maximize the learning efficiency quantitatively",
    "checked": true,
    "id": "922c0e4ddfe314d143cf7bbe8c055e55be9bd221",
    "semantic_title": "deep nonparametric estimation of operators between infinite dimensional spaces",
    "citation_count": 24,
    "authors": [
      "Hao Liu",
      "Haizhao Yang",
      "Minshuo Chen",
      "Tuo Zhao",
      "Wenjing Liao"
    ]
  },
  "https://jmlr.org/papers/v25/22-0846.html": {
    "title": "On Tail Decay Rate Estimation of Loss Function Distributions",
    "volume": "main",
    "abstract": "The study of loss-function distributions is critical to characterize a model's behaviour on a given machine-learning problem. While model quality is commonly measured by the average loss assessed on a testing set, this quantity does not ascertain the existence of the mean of the loss distribution. Conversely, the existence of a distribution's statistical moments can be verified by examining the thickness of its tails. Cross-validation schemes determine a family of testing loss distributions conditioned on the training sets. By marginalizing across training sets, we can recover the overall (marginal) loss distribution, whose tail-shape we aim to estimate. Small sample-sizes diminish the reliability and efficiency of classical tail-estimation methods like Peaks-Over-Threshold, and we demonstrate that this effect is notably significant when estimating tails of marginal distributions composed of conditional distributions with substantial tail-location variability. We mitigate this problem by utilizing a result we prove: under certain conditions, the marginal-distribution's tail-shape parameter is the maximum tail-shape parameter across the conditional distributions underlying the marginal. We label the resulting approach as `cross-tail estimation (CTE)'. We test CTE in a series of experiments on simulated and real data showing the improved robustness and quality of tail estimation as compared to classical approaches",
    "checked": true,
    "id": "32cf30bebbef012ba33eb410d5c6f3137ba0fa20",
    "semantic_title": "on tail decay rate estimation of loss function distributions",
    "citation_count": 0,
    "authors": [
      "Etrit Haxholli",
      "Marco Lorenzi"
    ]
  },
  "https://jmlr.org/papers/v25/22-1170.html": {
    "title": "Numerically Stable Sparse Gaussian Processes via Minimum Separation using Cover Trees",
    "volume": "main",
    "abstract": "Gaussian processes are frequently deployed as part of larger machine learning and decision-making systems, for instance in geospatial modeling, Bayesian optimization, or in latent Gaussian models. Within a system, the Gaussian process model needs to perform in a stable and reliable manner to ensure it interacts correctly with other parts of the system. In this work, we study the numerical stability of scalable sparse approximations based on inducing points. To do so, we first review numerical stability, and illustrate typical situations in which Gaussian process models can be unstable. Building on stability theory originally developed in the interpolation literature, we derive sufficient and in certain cases necessary conditions on the inducing points for the computations performed to be numerically stable. For low-dimensional tasks such as geospatial modeling, we propose an automated method for computing inducing points satisfying these conditions. This is done via a modification of the cover tree data structure, which is of independent interest. We additionally propose an alternative sparse approximation for regression with a Gaussian likelihood which trades off a small amount of performance to further improve stability. We provide illustrative examples showing the relationship between stability of calculations and predictive performance of inducing point methods on spatial tasks",
    "checked": true,
    "id": "c1bf6d502c0eb8b3854fc119943d2a2b46d9d950",
    "semantic_title": "numerically stable sparse gaussian processes via minimum separation using cover trees",
    "citation_count": 4,
    "authors": [
      "Alexander Terenin",
      "David R. Burt",
      "Artem Artemev",
      "Seth Flaxman",
      "Mark van der Wilk",
      "Carl Edward Rasmussen",
      "Hong Ge"
    ]
  },
  "https://jmlr.org/papers/v25/22-1296.html": {
    "title": "Optimal Bump Functions for Shallow ReLU networks: Weight Decay, Depth Separation, Curse of Dimensionality",
    "volume": "main",
    "abstract": "In this note, we study how neural networks with a single hidden layer and ReLU activation interpolate data drawn from a radially symmetric distribution with target labels 1 at the origin and 0 outside the unit ball, if no labels are known inside the unit ball. With weight decay regularization and in the infinite neuron, infinite data limit, we prove that a unique radially symmetric minimizer exists, whose average parameters and Lipschitz constant grow as $d$ and $\\sqrt{d}$ respectively. We furthermore show that the average weight variable grows exponentially in $d$ if the label $1$ is imposed on a ball of radius $\\varepsilon$ rather than just at the origin. By comparison, a neural networks with two hidden layers can approximate the target function without encountering the curse of dimensionality",
    "checked": false,
    "id": "da81f66de3786a4570c5da263bb7a7b74114d19a",
    "semantic_title": "optimal bump functions for shallow relu networks: weight decay, depth separation and the curse of dimensionality",
    "citation_count": 1,
    "authors": [
      "Stephan Wojtowytsch"
    ]
  },
  "https://jmlr.org/papers/v25/22-1392.html": {
    "title": "Additive smoothing error in backward variational inference for general state-space models",
    "volume": "main",
    "abstract": "We consider the problem of state estimation in general state-space models using variational inference. For a generic variational family defined using the same backward decomposition as the actual joint smoothing distribution, we establish under mixing assumptions that the variational approximation of expectations of additive state functionals induces an error which grows at most linearly in the number of observations. This guarantee is consistent with the known upper bounds for the approximation of smoothing distributions using standard Monte Carlo methods. We illustrate our theoretical result with state-of-the art variational solutions based both on the backward parameterization and on alternatives using forward decompositions. This numerical study proposes guidelines for variational inference based on neural networks in state-space models",
    "checked": true,
    "id": "981ddb0d690fd03f9adada2ca5eb3a5c93a3f909",
    "semantic_title": "additive smoothing error in backward variational inference for general state-space models",
    "citation_count": 2,
    "authors": [
      "Mathis Chagneux",
      "Elisabeth Gassiat",
      "Pierre Gloaguen",
      "Sylvain Le Corff"
    ]
  },
  "https://jmlr.org/papers/v25/23-0062.html": {
    "title": "Rates of convergence for density estimation with generative adversarial networks",
    "volume": "main",
    "abstract": "In this work we undertake a thorough study of the non-asymptotic properties of the vanilla generative adversarial networks (GANs). We prove an oracle inequality for the Jensen-Shannon (JS) divergence between the underlying density $\\mathsf{p}^*$ and the GAN estimate with a significantly better statistical error term compared to the previously known results. The advantage of our bound becomes clear in application to nonparametric density estimation. We show that the JS-divergence between the GAN estimate and $\\mathsf{p}^*$ decays as fast as $(\\log{n}/n)^{2\\beta/(2\\beta + d)}$, where $n$ is the sample size and $\\beta$ determines the smoothness of $\\mathsf{p}^*$. This rate of convergence coincides (up to logarithmic factors) with minimax optimal for the considered class of densities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikita Puchkin",
      "Sergey Samsonov",
      "Denis Belomestny",
      "Eric Moulines",
      "Alexey Naumov"
    ]
  },
  "https://jmlr.org/papers/v25/23-0220.html": {
    "title": "Stochastic Modified Flows, Mean-Field Limits and Dynamics of Stochastic Gradient Descent",
    "volume": "main",
    "abstract": "We propose new limiting dynamics for stochastic gradient descent in the small learning rate regime called stochastic modified flows. These SDEs are driven by a cylindrical Brownian motion and improve the so-called stochastic modified equations by having regular diffusion coefficients and by matching the multi-point statistics. As a second contribution, we introduce distribution dependent stochastic modified flows which we prove to describe the fluctuating limiting dynamics of stochastic gradient descent in the small learning rate - infinite width scaling regime",
    "checked": true,
    "id": "93e588c21dcdb53a43105d961d098b7a8d5cc17a",
    "semantic_title": "stochastic modified flows, mean-field limits and dynamics of stochastic gradient descent",
    "citation_count": 1,
    "authors": [
      "Benjamin Gess",
      "Sebastian Kassing",
      "Vitalii Konarovskyi"
    ]
  },
  "https://jmlr.org/papers/v25/23-0314.html": {
    "title": "Sample-efficient Adversarial Imitation Learning",
    "volume": "main",
    "abstract": "Imitation learning, in which learning is performed by demonstration, has been studied and advanced for sequential decision-making tasks in which a reward function is not predefined. However, imitation learning methods still require numerous expert demonstration samples to successfully imitate an expert's behavior. To improve sample efficiency, we utilize self-supervised representation learning, which can generate vast training signals from the given data. In this study, we propose a self-supervised representation-based adversarial imitation learning method to learn state and action representations that are robust to diverse distortions and temporally predictive, on non-image control tasks. In particular, in comparison with existing self-supervised learning methods for tabular data, we propose a different corruption method for state and action representations that is robust to diverse distortions. We theoretically and empirically observe that making an informative feature manifold with less sample complexity significantly improves the performance of imitation learning. The proposed method shows a 39% relative improvement over existing adversarial imitation learning methods on MuJoCo in a setting limited to 100 expert state-action pairs. Moreover, we conduct comprehensive ablations and additional experiments using demonstrations with varying optimality to provide insights into a range of factors",
    "checked": true,
    "id": "cccc5007aac6df819d8e8890add4d6c0e69b5fff",
    "semantic_title": "sample-efficient adversarial imitation learning",
    "citation_count": 0,
    "authors": [
      "Dahuin Jung",
      "Hyungyu Lee",
      "Sungroh Yoon"
    ]
  },
  "https://jmlr.org/papers/v25/23-0488.html": {
    "title": "Heterogeneous-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "The necessity for cooperation among intelligent machines has popularised cooperative multi-agent reinforcement learning (MARL) in AI research. However, many research endeavours heavily rely on parameter sharing among agents, which confines them to only homogeneous-agent setting and leads to training instability and lack of convergence guarantees. To achieve effective cooperation in the general heterogeneous-agent setting, we propose Heterogeneous-Agent Reinforcement Learning (HARL) algorithms that resolve the aforementioned issues. Central to our findings are the multi-agent advantage decomposition lemma and the sequential update scheme. Based on these, we develop the provably correct Heterogeneous-Agent Trust Region Learning (HATRL), and derive HATRPO and HAPPO by tractable approximations. Furthermore, we discover a novel framework named Heterogeneous-Agent Mirror Learning (HAML), which strengthens theoretical guarantees for HATRPO and HAPPO and provides a general template for cooperative MARL algorithmic designs. We prove that all algorithms derived from HAML inherently enjoy monotonic improvement of joint return and convergence to Nash Equilibrium. As its natural outcome, HAML validates more novel algorithms in addition to HATRPO and HAPPO, including HAA2C, HADDPG, and HATD3, which generally outperform their existing MA-counterparts. We comprehensively test HARL algorithms on six challenging benchmarks and demonstrate their superior effectiveness and stability for coordinating heterogeneous agents compared to strong baselines such as MAPPO and QMIX",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Zhong",
      "Jakub Grudzien Kuba",
      "Xidong Feng",
      "Siyi Hu",
      "Jiaming Ji",
      "Yaodong Yang"
    ]
  },
  "https://jmlr.org/papers/v25/23-0572.html": {
    "title": "Pygmtools: A Python Graph Matching Toolkit",
    "volume": "MLOSS",
    "abstract": "Graph matching aims to find node-to-node matching among multiple graphs, which is a fundamental yet challenging problem. To facilitate graph matching in scientific research and industrial applications, pygmtools is released, which is a Python graph matching toolkit that implements a comprehensive collection of two-graph matching and multi-graph matching solvers, covering both learning-free solvers as well as learning-based neural graph matching solvers. Our implementation supports numerical backends including Numpy, PyTorch, Jittor, Paddle, runs on Windows, MacOS and Linux, and is friendly to install and configure. Comprehensive documentations covering beginner's guide, API reference and examples are available online. pygmtools is open-sourced under Mulan PSL v2 license",
    "checked": true,
    "id": "d35968607b69cccb3cce34f21803b87de341170b",
    "semantic_title": "pygmtools: a python graph matching toolkit",
    "citation_count": 0,
    "authors": [
      "Runzhong Wang",
      "Ziao Guo",
      "Wenzheng Pan",
      "Jiale Ma",
      "Yikai Zhang",
      "Nan Yang",
      "Qi Liu",
      "Longxuan Wei",
      "Hanxue Zhang",
      "Chang Liu",
      "Zetian Jiang",
      "Xiaokang Yang",
      "Junchi Yan"
    ]
  },
  "https://jmlr.org/papers/v25/23-0802.html": {
    "title": "Effect-Invariant Mechanisms for Policy Generalization",
    "volume": "main",
    "abstract": "Policy learning is an important component of many real-world learning systems. A major challenge in policy learning is how to adapt efficiently to unseen environments or tasks. Recently, it has been suggested to exploit invariant conditional distributions to learn models that generalize better to unseen environments. However, assuming invariance of entire conditional distributions (which we call full invariance) may be too strong of an assumption in practice. In this paper, we introduce a relaxation of full invariance called effect-invariance (e-invariance for short) and prove that it is sufficient, under suitable assumptions, for zero-shot policy generalization. We also discuss an extension that exploits e-invariance when we have a small sample from the test environment, enabling few-shot policy generalization. Our work does not assume an underlying causal graph or that the data are generated by a structural causal model; instead, we develop testing procedures to test e-invariance directly from data. We present empirical results using simulated data and a mobile health intervention dataset to demonstrate the effectiveness of our approach",
    "checked": true,
    "id": "997926163e4373e56d1315aabcdf6a1009035c9e",
    "semantic_title": "effect-invariant mechanisms for policy generalization",
    "citation_count": 0,
    "authors": [
      "Sorawit Saengkyongam",
      "Niklas Pfister",
      "Predrag Klasnja",
      "Susan Murphy",
      "Jonas Peters"
    ]
  },
  "https://jmlr.org/papers/v25/23-0912.html": {
    "title": "Deep Network Approximation: Beyond ReLU to Diverse Activation Functions",
    "volume": "main",
    "abstract": "This paper explores the expressive power of deep neural networks for a diverse range of activation functions. An activation function set $\\mathscr{A}$ is defined to encompass the majority of commonly used activation functions, such as $\\mathtt{ReLU}$, $\\mathtt{LeakyReLU}$, $\\mathtt{ReLU}^2$, $\\mathtt{ELU}$, $\\mathtt{CELU}$, $\\mathtt{SELU}$, $\\mathtt{Softplus}$, $\\mathtt{GELU}$, $\\mathtt{SiLU}$, $\\mathtt{Swish}$, $\\mathtt{Mish}$, $\\mathtt{Sigmoid}$, $\\mathtt{Tanh}$, $\\mathtt{Arctan}$, $\\mathtt{Softsign}$, $\\mathtt{dSiLU}$, and $\\mathtt{SRS}$. We demonstrate that for any activation function $\\varrho\\in \\mathscr{A}$, a $\\mathtt{ReLU}$ network of width $N$ and depth $L$ can be approximated to arbitrary precision by a $\\varrho$-activated network of width $3N$ and depth $2L$ on any bounded set. This finding enables the extension of most approximation results achieved with $\\mathtt{ReLU}$ networks to a wide variety of other activation functions, albeit with slightly increased constants. Significantly, we establish that the (width,$\\,$depth) scaling factors can be further reduced from $(3,2)$ to $(1,1)$ if $\\varrho$ falls within a specific subset of $\\mathscr{A}$. This subset includes activation functions such as $\\mathtt{ELU}$, $\\mathtt{CELU}$, $\\mathtt{SELU}$, $\\mathtt{Softplus}$, $\\mathtt{GELU}$, $\\mathtt{SiLU}$, $\\mathtt{Swish}$, and $\\mathtt{Mish}$",
    "checked": true,
    "id": "68aa26eff595d796d416f4ea47b9c5a9fa663557",
    "semantic_title": "deep network approximation: beyond relu to diverse activation functions",
    "citation_count": 2,
    "authors": [
      "Shijun Zhang",
      "Jianfeng Lu",
      "Hongkai Zhao"
    ]
  },
  "https://jmlr.org/papers/v25/21-0233.html": {
    "title": "Sparse NMF with Archetypal Regularization: Computational and Robustness Properties",
    "volume": "main",
    "abstract": "We consider the problem of sparse nonnegative matrix factorization (NMF) using archetypal regularization. The goal is to represent a collection of data points as nonnegative linear combinations of a few nonnegative sparse factors with appealing geometric properties, arising from the use of archetypal regularization. We generalize the notion of robustness studied in Javadi and Montanari (2019) (without sparsity) to the notions of (a) strong robustness that implies each estimated archetype is close to the underlying archetypes and (b) weak robustness that implies there exists at least one recovered archetype that is close to the underlying archetypes. Our theoretical results on robustness guarantees hold under minimal assumptions on the underlying data, and applies to settings where the underlying archetypes need not be sparse. We present theoretical results and illustrative examples to strengthen the insights underlying the notions of robustness. We propose new algorithms for our optimization problem; and present numerical experiments on synthetic and real data sets that shed further insights into our proposed framework and theoretical developments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kayhan Behdin",
      "Rahul Mazumder"
    ]
  },
  "https://jmlr.org/papers/v25/21-0316.html": {
    "title": "Distributed Gaussian Mean Estimation under Communication Constraints: Optimal Rates and Communication-Efficient Algorithms",
    "volume": "main",
    "abstract": "Distributed estimation of a Gaussian mean under communication constraints is studied in a decision theoretical framework. Minimax rates of convergence, which characterize the tradeoff between communication costs and statistical accuracy, are established under the independent protocols. Communication-efficient and statistically optimal procedures are developed. In the univariate case, the optimal rate depends only on the total communication budget, so long as each local machine has at least one bit. However, in the multivariate case, the minimax rate depends on the specific allocations of the communication budgets among the local machines. Although optimal estimation of a Gaussian mean is relatively simple in the conventional setting, it is quite involved under communication constraints, both in terms of the optimal procedure design and the lower bound argument. An essential step is the decomposition of the minimax estimation problem into two stages, localization and refinement. This critical decomposition provides a framework for both the lower bound analysis and optimal procedure design. The optimality results and techniques developed in the present paper can be useful for solving other problems such as distributed nonparametric function estimation and sparse signal recovery",
    "checked": false,
    "id": "4923e0bbe79f26cdea99190d6c40113cea5f4a8d",
    "semantic_title": "correlated quantization for distributed mean estimation and optimization",
    "citation_count": 9,
    "authors": [
      "T. Tony Cai",
      "Hongji Wei"
    ]
  },
  "https://jmlr.org/papers/v25/21-0831.html": {
    "title": "Convergence for nonconvex ADMM, with applications to CT imaging",
    "volume": "main",
    "abstract": "The alternating direction method of multipliers (ADMM) algorithm is a powerful and flexible tool for complex optimization problems of the form $\\min\\{f(x)+g(y) : Ax+By=c\\}$. ADMM exhibits robust empirical performance across a range of challenging settings including nonsmoothness and nonconvexity of the objective functions $f$ and $g$, and provides a simple and natural approach to the inverse problem of image reconstruction for computed tomography (CT) imaging. From the theoretical point of view, existing results for convergence in the nonconvex setting generally assume smoothness in at least one of the component functions in the objective. In this work, our new theoretical results provide convergence guarantees under a restricted strong convexity assumption without requiring smoothness or differentiability, while still allowing differentiable terms to be treated approximately if needed. We validate these theoretical results empirically, with a simulated example where both $f$ and $g$ are nondifferentiable---and thus outside the scope of existing theory---as well as a simulated CT image reconstruction problem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rina Foygel Barber",
      "Emil Y. Sidky"
    ]
  },
  "https://jmlr.org/papers/v25/21-1343.html": {
    "title": "On the Sample Complexity and Metastability of Heavy-tailed Policy Search in Continuous Control",
    "volume": "main",
    "abstract": "Reinforcement learning is a framework for interactive decision-making with incentives sequentially revealed across time without a system dynamics model. Due to its scaling to continuous spaces, we focus on policy search where one iteratively improves a parameterized policy with stochastic policy gradient (PG) updates. In tabular Markov Decision Problems (MDPs), under persistent exploration and suitable parameterization, global optimality may be obtained. By contrast, in continuous space, the non-convexity poses a pathological challenge as evidenced by existing convergence results being mostly limited to stationarity or arbitrary local extrema. To close this gap, we step towards persistent exploration in continuous space through policy parameterizations defined by distributions of heavier tails defined by tail-index parameter $\\alpha$, which increases the likelihood of jumping in state space. Doing so invalidates smoothness conditions of the score function common to PG. Thus, we establish how the convergence rate to stationarity depends on the policy's tail index $\\alpha$, a Hölder continuity parameter, integrability conditions, and an exploration tolerance parameter introduced here for the first time. Further, we characterize the dependence of the set of local maxima on the tail index through an exit and transition time analysis of a suitably defined Markov chain, identifying that policies associated with Lévy Processes of a heavier tail converge to wider peaks. This phenomenon yields improved stability to perturbations in supervised learning, which we corroborate also manifests in improved performance of policy search, especially when myopic and farsighted incentives are misaligned",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amrit Singh Bedi",
      "Anjaly Parayil",
      "Junyu Zhang",
      "Mengdi Wang",
      "Alec Koppel"
    ]
  },
  "https://jmlr.org/papers/v25/22-0667.html": {
    "title": "Survival Kernets: Scalable and Interpretable Deep Kernel Survival Analysis with an Accuracy Guarantee",
    "volume": "main",
    "abstract": "Kernel survival analysis models estimate individual survival distributions with the help of a kernel function, which measures the similarity between any two data points. Such a kernel function can be learned using deep kernel survival models. In this paper, we present a new deep kernel survival model called a survival kernet, which scales to large datasets in a manner that is amenable to model interpretation and also theoretical analysis. Specifically, the training data are partitioned into clusters based on a recently developed training set compression scheme for classification and regression called kernel netting that we extend to the survival analysis setting. At test time, each data point is represented as a weighted combination of these clusters, and each such cluster can be visualized. For a special case of survival kernets, we establish a finite-sample error bound on predicted survival distributions that is, up to a log factor, optimal. Whereas scalability at test time is achieved using the aforementioned kernel netting compression strategy, scalability during training is achieved by a warm-start procedure based on tree ensembles such as XGBoost and a heuristic approach to accelerating neural architecture search. On four standard survival analysis datasets of varying sizes (up to roughly 3 million data points), we show that survival kernets are highly competitive compared to various baselines tested in terms of time-dependent concordance index. Our code is available at: https://github.com/georgehc/survival-kernets",
    "checked": true,
    "id": "ea69f42a6322d78cd0c340b89be8017769a02bd2",
    "semantic_title": "survival kernets: scalable and interpretable deep kernel survival analysis with an accuracy guarantee",
    "citation_count": 2,
    "authors": [
      "George H. Chen"
    ]
  },
  "https://jmlr.org/papers/v25/22-0810.html": {
    "title": "Personalized PCA: Decoupling Shared and Unique Features",
    "volume": "main",
    "abstract": "In this paper, we tackle a significant challenge in PCA: heterogeneity. When data are collected from different sources with heterogeneous trends while still sharing some congruency, it is critical to extract shared knowledge while retaining the unique features of each source. To this end, we propose personalized PCA (PerPCA), which uses mutually orthogonal global and local principal components to encode both unique and shared features. We show that, under mild conditions, both unique and shared features can be identified and recovered by a constrained optimization problem, even if the covariance matrices are immensely different. Also, we design a fully federated algorithm inspired by distributed Stiefel gradient descent to solve the problem. The algorithm introduces a new group of operations called generalized retractions to handle orthogonality constraints, and only requires global PCs to be shared across sources. We prove the linear convergence of the algorithm under suitable assumptions. Comprehensive numerical experiments highlight PerPCA's superior performance in feature extraction and prediction from heterogeneous datasets. As a systematic approach to decouple shared and unique features from heterogeneous datasets, PerPCA finds applications in several tasks, including video segmentation, topic extraction, and feature clustering",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Naichen Shi",
      "Raed Al Kontar"
    ]
  },
  "https://jmlr.org/papers/v25/22-0891.html": {
    "title": "Invariant and Equivariant Reynolds Networks",
    "volume": "MLOSS",
    "abstract": "Various data exhibit symmetry, including permutations in graphs and point clouds. Machine learning methods that utilize this symmetry have achieved considerable success. In this study, we explore learning models for data exhibiting group symmetry. Our focus is on transforming deep neural networks using Reynolds operators, which average over the group to convert a function into an invariant or equivariant form. While learning methods based on Reynolds operators are well-established, they often face computational complexity challenges. To address this, we introduce two new methods that reduce the computational burden associated with the Reynolds operator: (i) Although the Reynolds operator traditionally averages over the entire group, we demonstrate that it can be effectively approximated by averaging over specific subsets of the group, termed the Reynolds design. (ii) We reveal that the pre-model does not require all input variables. Instead, using a select number of partial inputs (Reynolds dimension) is sufficient to achieve a universally applicable model. Employing these methods, which hinge on the Reynolds design and Reynolds dimension concepts, allows us to construct universally applicable models with manageable computational complexity. Our experiments on benchmark data indicate that our approach is more efficient than existing methods",
    "checked": true,
    "id": "cb9d38437f342b4bdf60e6e7f0ac142a1f22bba0",
    "semantic_title": "invariant and equivariant reynolds networks",
    "citation_count": 0,
    "authors": [
      "Akiyoshi Sannai",
      "Makoto Kawano",
      "Wataru Kumagai"
    ]
  },
  "https://jmlr.org/papers/v25/22-1198.html": {
    "title": "Mean-Square Analysis of Discretized Itô Diffusions for Heavy-tailed Sampling",
    "volume": "main",
    "abstract": "We analyze the complexity of sampling from a class of heavy-tailed distributions by discretizing a natural class of Itô diffusions associated with weighted Poincaré inequalities. Based on a mean-square analysis, we establish the iteration complexity for obtaining a sample whose distribution is $\\epsilon$ close to the target distribution in the Wasserstein-2 metric. In this paper, our results take the mean-square analysis to its limits, i.e., we invariably only require that the target density has finite variance, the minimal requirement for a mean-square analysis. To obtain explicit estimates, we compute upper bounds on certain moments associated with heavy-tailed targets under various assumptions. We also provide similar iteration complexity results for the case where only function evaluations of the unnormalized target density are available by estimating the gradients using a Gaussian smoothing technique. We provide illustrative examples based on the multivariate $t$-distribution",
    "checked": true,
    "id": "2572ab244919db4bc75fe597a10137a503b64ccc",
    "semantic_title": "mean-square analysis of discretized itô diffusions for heavy-tailed sampling",
    "citation_count": 1,
    "authors": [
      "Ye He",
      "Tyler Farghly",
      "Krishnakumar Balasubramanian",
      "Murat A. Erdogdu"
    ]
  },
  "https://jmlr.org/papers/v25/22-1389.html": {
    "title": "Multiple Descent in the Multiple Random Feature Model",
    "volume": "main",
    "abstract": "Recent works have demonstrated a double descent phenomenon in over-parameterized learning. Although this phenomenon has been investigated by recent works, it has not been fully understood in theory. In this paper, we investigate the multiple descent phenomenon in a class of multi-component prediction models. We first consider a \"double random feature model\" (DRFM) concatenating two types of random features, and study the excess risk achieved by the DRFM in ridge regression. We calculate the precise limit of the excess risk under the high dimensional framework where the training sample size, the dimension of data, and the dimension of random features tend to infinity proportionally. Based on the calculation, we further theoretically demonstrate that the risk curves of DRFMs can exhibit triple descent. We then provide a thorough experimental study to verify our theory. At last, we extend our study to the \"multiple random feature model\" (MRFM), and show that MRFMs ensembling $K$ types of random features may exhibit $(K+1)$-fold descent. Our analysis points out that risk curves with a specific number of descent generally exist in learning multi-component prediction models",
    "checked": true,
    "id": "439c2c3016d7827312302112181a1b795dae6f72",
    "semantic_title": "multiple descent in the multiple random feature model",
    "citation_count": 3,
    "authors": [
      "Xuran Meng",
      "Jianfeng Yao",
      "Yuan Cao"
    ]
  },
  "https://jmlr.org/papers/v25/23-0038.html": {
    "title": "Probabilistic Forecasting with Generative Networks via Scoring Rule Minimization",
    "volume": "main",
    "abstract": "Probabilistic forecasting relies on past observations to provide a probability distribution for a future outcome, which is often evaluated against the realization using a scoring rule. Here, we perform probabilistic forecasting with generative neural networks, which parametrize distributions on high-dimensional spaces by transforming draws from a latent variable. Generative networks are typically trained in an adversarial framework. In contrast, we propose to train generative networks to minimize a predictive-sequential (or prequential) scoring rule on a recorded temporal sequence of the phenomenon of interest, which is appealing as it corresponds to the way forecasting systems are routinely evaluated. Adversarial-free minimization is possible for some scoring rules; hence, our framework avoids the cumbersome hyperparameter tuning and uncertainty underestimation due to unstable adversarial training, thus unlocking reliable use of generative networks in probabilistic forecasting. Further, we prove consistency of the minimizer of our objective with dependent data, while adversarial training assumes independence. We perform simulation studies on two chaotic dynamical models and a benchmark data set of global weather observations; for this last example, we define scoring rules for spatial data by drawing from the relevant literature. Our method outperforms state-of-the-art adversarial approaches, especially in probabilistic calibration, while requiring less hyperparameter tuning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Pacchiardi",
      "Rilwan A. Adewoyin",
      "Peter Dueben",
      "Ritabrata Dutta"
    ]
  },
  "https://jmlr.org/papers/v25/23-0286.html": {
    "title": "A Multilabel Classification Framework for Approximate Nearest Neighbor Search",
    "volume": "main",
    "abstract": "To learn partition-based index structures for approximate nearest neighbor (ANN) search, both supervised and unsupervised machine learning algorithms have been used. Existing supervised algorithms select all the points that belong to the same partition element as the query point as nearest neighbor candidates. Consequently, they formulate the learning task as finding a partition in which the nearest neighbors of a query point belong to the same partition element with it as often as possible. In contrast, we formulate the candidate set selection in ANN search directly as a multilabel classification problem where the labels correspond to the nearest neighbors of the query point. In the proposed framework, partition-based index structures are interpreted as partitioning classifiers for solving this classification problem. Empirical results suggest that, when combined with any partitioning strategy, the natural classifier based on the proposed framework leads to a strictly improved performance compared to the earlier candidate set selection methods. We also prove a sufficient condition for the consistency of a partitioning classifier for ANN search, and illustrate the result by verifying this condition for chronological $k$-d trees and (both dense and sparse) random projection trees",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ville Hyvönen",
      "Elias Jääsaari",
      "Teemu Roos"
    ]
  },
  "https://jmlr.org/papers/v25/23-0439.html": {
    "title": "Efficient Modality Selection in Multimodal Learning",
    "volume": "main",
    "abstract": "Multimodal learning aims to learn from data of different modalities by fusing information from heterogeneous sources. Although it is beneficial to learn from more modalities, it is often infeasible to use all available modalities under limited computational resources. Modeling with all available modalities can also be inefficient and unnecessary when information across input modalities overlaps. In this paper, we study the modality selection problem, which aims to select the most useful subset of modalities for learning under a cardinality constraint. To that end, we propose a unified theoretical framework to quantify the learning utility of modalities, and we identify dependence assumptions to flexibly model the heterogeneous nature of multimodal data, which also allows efficient algorithm design. Accordingly, we derive a greedy modality selection algorithm via submodular maximization, which selects the most useful modalities with an optimality guarantee on learning performance. We also connect marginal-contribution-based feature importance scores, such as Shapley value, from the feature selection domain to the context of modality selection, to efficiently compute the importance of individual modality. We demonstrate the efficacy of our theoretical results and modality selection algorithms on 2 synthetic and 4 real-world data sets on a diverse range of multimodal data",
    "checked": false,
    "id": "0fa1bb8bd7c49fa9984da8684440252cdc92f586",
    "semantic_title": "eﬃcient modality selection in multimodal learning",
    "citation_count": 0,
    "authors": [
      "Yifei He",
      "Runxiang Cheng",
      "Gargi Balasubramaniam",
      "Yao-Hung Hubert Tsai",
      "Han Zhao"
    ]
  },
  "https://jmlr.org/papers/v25/23-0576.html": {
    "title": "Adam-family Methods for Nonsmooth Optimization with Convergence Guarantees",
    "volume": "main",
    "abstract": "In this paper, we present a comprehensive study on the convergence properties of Adam-family methods for nonsmooth optimization, especially in the training of nonsmooth neural networks. We introduce a novel two-timescale framework that adopts a two-timescale updating scheme, and prove its convergence properties under mild assumptions. Our proposed framework encompasses various popular Adam-family methods, providing convergence guarantees for these methods in training nonsmooth neural networks. Furthermore, we develop stochastic subgradient methods that incorporate gradient clipping techniques for training nonsmooth neural networks with heavy-tailed noise. Through our framework, we show that our proposed methods converge even when the evaluation noises are only assumed to be integrable. Extensive numerical experiments demonstrate the high efficiency and robustness of our proposed methods",
    "checked": true,
    "id": "d7b114bff7cd490531ab19059711d02b0943b931",
    "semantic_title": "adam-family methods for nonsmooth optimization with convergence guarantees",
    "citation_count": 6,
    "authors": [
      "Nachuan Xiao",
      "Xiaoyin Hu",
      "Xin Liu",
      "Kim-Chuan Toh"
    ]
  },
  "https://jmlr.org/papers/v25/23-1042.html": {
    "title": "Trained Transformers Learn Linear Models In-Context",
    "volume": "main",
    "abstract": "Attention-based neural networks such as transformers have demonstrated a remarkable ability to exhibit in-context learning (ICL): Given a short prompt sequence of tokens from an unseen task, they can formulate relevant per-token and next-token predictions without any parameter updates. By embedding a sequence of labeled training data and unlabeled test data as a prompt, this allows for transformers to behave like supervised learning algorithms. Indeed, recent work has shown that when training transformer architectures over random instances of linear regression problems, these models' predictions mimic those of ordinary least squares. Towards understanding the mechanisms underlying this phenomenon, we investigate the dynamics of ICL in transformers with a single linear self-attention layer trained by gradient flow on linear regression tasks. We show that despite non-convexity, gradient flow with a suitable random initialization finds a global minimum of the objective function. At this global minimum, when given a test prompt of labeled examples from a new prediction task, the transformer achieves prediction error competitive with the best linear predictor over the test prompt distribution. We additionally characterize the robustness of the trained transformer to a variety of distribution shifts and show that although a number of shifts are tolerated, shifts in the covariate distribution of the prompts are not. Motivated by this, we consider a generalized ICL setting where the covariate distributions can vary across prompts. We show that although gradient flow succeeds at finding a global minimum in this setting, the trained transformer is still brittle under mild covariate shifts. We complement this finding with experiments on large, nonlinear transformer architectures which we show are more robust under covariate shifts",
    "checked": true,
    "id": "4a7530bbaee7563ee244f3ffed6b706bd96f08a8",
    "semantic_title": "trained transformers learn linear models in-context",
    "citation_count": 58,
    "authors": [
      "Ruiqi Zhang",
      "Spencer Frei",
      "Peter L. Bartlett"
    ]
  },
  "https://jmlr.org/papers/v25/18-566.html": {
    "title": "Resource-Efficient Neural Networks for Embedded Systems",
    "volume": "main",
    "abstract": "While machine learning is traditionally a resource intensive task, embedded systems, autonomous navigation, and the vision of the Internet of Things fuel the interest in resource-efficient approaches. These approaches aim for a carefully chosen trade-off between performance and resource consumption in terms of computation and energy. The development of such approaches is among the major challenges in current machine learning research and key to ensure a smooth transition of machine learning technology from a scientific environment with virtually unlimited computing resources into everyday's applications. In this article, we provide an overview of the current state of the art of machine learning techniques facilitating these real-world requirements. In particular, we focus on resource-efficient inference based on deep neural networks (DNNs), the predominant machine learning models of the past decade. We give a comprehensive overview of the vast literature that can be mainly split into three non-mutually exclusive categories: (i) quantized neural networks, (ii) network pruning, and (iii) structural efficiency. These techniques can be applied during training or as post-processing, and they are widely used to reduce the computational demands in terms of memory footprint, inference speed, and energy efficiency. We also briefly discuss different concepts of embedded hardware for DNNs and their compatibility with machine learning techniques as well as potential for energy and latency reduction. We substantiate our discussion with experiments on well-known benchmark data sets using compression techniques (quantization, pruning) for a set of resource-constrained embedded systems, such as CPUs, GPUs and FPGAs. The obtained results highlight the difficulty of finding good trade-offs between resource efficiency and prediction quality",
    "checked": false,
    "id": "ebc8e41e14b01203650ccd6d9418b6e53c35aff8",
    "semantic_title": "toward energy efficient stt-mram-based near memory computing architecture for embedded systems",
    "citation_count": 0,
    "authors": [
      "Wolfgang Roth",
      "Günther Schindler",
      "Bernhard Klein",
      "Robert Peharz",
      "Sebastian Tschiatschek",
      "Holger Fröning",
      "Franz Pernkopf",
      "Zoubin Ghahramani"
    ]
  },
  "https://jmlr.org/papers/v25/21-1256.html": {
    "title": "Optimal First-Order Algorithms as a Function of Inequalities",
    "volume": "main",
    "abstract": "In this work, we present a novel algorithm design methodology that finds the optimal algorithm as a function of inequalities. Specifically, we restrict convergence analyses of algorithms to use a prespecified subset of inequalities, rather than utilizing all true inequalities, and find the optimal algorithm subject to this restriction. This methodology allows us to design algorithms with certain desired characteristics. As concrete demonstrations of this methodology, we find new state-of-the-art accelerated first-order gradient methods using randomized coordinate updates and backtracking line searches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chanwoo Park",
      "Ernest K. Ryu"
    ]
  },
  "https://jmlr.org/papers/v25/22-0285.html": {
    "title": "Axiomatic effect propagation in structural causal models",
    "volume": "main",
    "abstract": "We study effect propagation in a causal directed acyclic graph (DAG), with the goal of providing a flow-based decomposition of the effect (i.e., change in the outcome variable) as a result of changes in the source variables. We first compare various ideas on causality to quantify effect propagation, such as direct and indirect effects, path-specific effects, and degree of responsibility. We discuss the shortcomings of such approaches and propose a flow-based methodology, which we call recursive Shapley value (RSV). By considering a broader set of counterfactuals than existing methods, RSV obeys a unique adherence to four desirable flow-based axioms. Further, we provide a general path-based characterization of RSV for an arbitrary non-parametric structural equations model (SEM) defined on the underlying DAG. Interestingly, for the special class of linear SEMs, RSV exhibits a simple and tractable characterization (and hence, computation), which recovers the classical method of path coefficients and is equivalent to path-specific effects. For non-parametric SEMs, we use our general characterization to develop an unbiased Monte-Carlo estimation procedure with an exponentially decaying sample complexity. We showcase the application of RSV on two challenging problems on causality (causal overdetermination and causal unfairness)",
    "checked": false,
    "id": "fa613630b4d39616eed288721ffd38f409885781",
    "semantic_title": "risk analysis of airplane upsets in flight: an integrated system framework and analysis methodology",
    "citation_count": 1,
    "authors": [
      "Raghav Singal",
      "George Michailidis"
    ]
  },
  "https://jmlr.org/papers/v25/22-0796.html": {
    "title": "Polygonal Unadjusted Langevin Algorithms: Creating stable and efficient adaptive algorithms for neural networks",
    "volume": "main",
    "abstract": "We present a new class of Langevin-based algorithms, which overcomes many of the known shortcomings of popular adaptive optimizers that are currently used for the fine tuning of deep learning models. Its underpinning theory relies on recent advances of Euler-Krylov polygonal approximations for stochastic differential equations (SDEs) with monotone coefficients. As a result, it inherits the stability properties of tamed algorithms, while it addresses other known issues, e.g. vanishing gradients in deep learning. In particular, we provide a nonasymptotic analysis and full theoretical guarantees for the convergence properties of an algorithm of this novel class, which we named TH$\\varepsilon$O POULA (or, simply, TheoPouLa). Finally, several experiments are presented with different types of deep learning models, which show the superior performance of TheoPouLa over many popular adaptive optimization algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong-Young Lim",
      "Sotirios Sabanis"
    ]
  },
  "https://jmlr.org/papers/v25/22-1197.html": {
    "title": "Monotonic Risk Relationships under Distribution Shifts for Regularized Risk Minimization",
    "volume": "main",
    "abstract": "Machine learning systems are often applied to data that is drawn from a different distribution than the training distribution. Recent work has shown that for a variety of classification and signal reconstruction problems, the out-of-distribution performance is strongly linearly correlated with the in-distribution performance. If this relationship or more generally a monotonic one holds, it has important consequences. For example, it allows to optimize performance on one distribution as a proxy for performance on the other. In this paper, we study conditions under which a monotonic relationship between the performances of a model on two distributions is expected. We prove an exact asymptotic linear relation for squared error and a monotonic relation for misclassification error for ridge-regularized general linear models under covariate shift, as well as an approximate linear relation for linear inverse problems",
    "checked": true,
    "id": "6a8d93484220fca8a548db04574a806cd0b9ff7d",
    "semantic_title": "monotonic risk relationships under distribution shifts for regularized risk minimization",
    "citation_count": 0,
    "authors": [
      "Daniel LeJeune",
      "Jiayu Liu",
      "Reinhard Heckel"
    ]
  },
  "https://jmlr.org/papers/v25/23-0044.html": {
    "title": "Revisiting RIP Guarantees for Sketching Operators on Mixture Models",
    "volume": "main",
    "abstract": "In the context of sketching for compressive mixture modeling, we revisit existing proofs of the Restricted Isometry Property of sketching operators with respect to certain mixtures models. After examining the shortcomings of existing guarantees, we propose an alternative analysis that circumvents the need to assume importance sampling when drawing random Fourier features to build random sketching operators. Our analysis is based on new deterministic bounds on the restricted isometry constant that depend solely on the set of frequencies used to define the sketching operator; then we leverage these bounds to establish concentration inequalities for random sketching operators that lead to the desired RIP guarantees. Our analysis also opens the door to theoretical guarantees for structured sketching with frequencies associated to fast random linear operators",
    "checked": true,
    "id": "9abe42e37248ca78f58f17b2a8635bf5b5104328",
    "semantic_title": "revisiting rip guarantees for sketching operators on mixture models",
    "citation_count": 2,
    "authors": [
      "Ayoub Belhadji",
      "Rémi Gribonval"
    ]
  },
  "https://jmlr.org/papers/v25/23-0371.html": {
    "title": "A projected semismooth Newton method for a class of nonconvex composite programs with strong prox-regularity",
    "volume": "main",
    "abstract": "This paper aims to develop a Newton-type method to solve a class of nonconvex composite programs. In particular, the nonsmooth part is possibly nonconvex. To tackle the nonconvexity, we develop a notion of strong prox-regularity which is related to the singleton property and Lipschitz continuity of the associated proximal operator, and we verify it in various classes of functions, including weakly convex functions, indicator functions of proximally smooth sets, and two specific sphere-related nonconvex nonsmooth functions. In this case, the problem class we are concerned with covers smooth optimization problems on manifold and certain composite optimization problems on manifold. For the latter, the proposed algorithm is the first second-order type method. Combining with the semismoothness of the proximal operator, we design a projected semismooth Newton method to find a root of the natural residual induced by the proximal gradient method. Due to the possible nonconvexity of the feasible domain, an extra projection is added to the usual semismooth Newton step and new criteria are proposed for the switching between the projected semismooth Newton step and the proximal step. The global convergence is then established under the strong prox-regularity. Based on the BD regularity condition, we establish local superlinear convergence. Numerical experiments demonstrate the effectiveness of our proposed method compared with state-of-the-art ones",
    "checked": true,
    "id": "7221dad74fade190c7bb35db5a71e5390058fdf4",
    "semantic_title": "a projected semismooth newton method for a class of nonconvex composite programs with strong prox-regularity",
    "citation_count": 2,
    "authors": [
      "Jiang Hu",
      "Kangkang Deng",
      "Jiayuan Wu",
      "Quanzheng Li"
    ]
  },
  "https://jmlr.org/papers/v25/23-0446.html": {
    "title": "Data Thinning for Convolution-Closed Distributions",
    "volume": "main",
    "abstract": "We propose data thinning, an approach for splitting an observation into two or more independent parts that sum to the original observation, and that follow the same distribution as the original observation, up to a (known) scaling of a parameter. This very general proposal is applicable to any convolution-closed distribution, a class that includes the Gaussian, Poisson, negative binomial, gamma, and binomial distributions, among others. Data thinning has a number of applications to model selection, evaluation, and inference. For instance, cross-validation via data thinning provides an attractive alternative to the usual approach of cross-validation via sample splitting, especially in settings in which the latter is not applicable. In simulations and in an application to single-cell RNA-sequencing data, we show that data thinning can be used to validate the results of unsupervised learning approaches, such as k-means clustering and principal components analysis, for which traditional sample splitting is unattractive or unavailable",
    "checked": true,
    "id": "65427cbffb61c75f4b101f9c0cfdb06554295631",
    "semantic_title": "data thinning for convolution-closed distributions",
    "citation_count": 8,
    "authors": [
      "Anna Neufeld",
      "Ameer Dharamshi",
      "Lucy L. Gao",
      "Daniela Witten"
    ]
  },
  "https://jmlr.org/papers/v25/23-0456.html": {
    "title": "Existence and Minimax Theorems for Adversarial Surrogate Risks in Binary Classification",
    "volume": "main",
    "abstract": "We prove existence, minimax, and complementary slackness theorems for adversarial surrogate risks in binary classification. These results extend recent work that established analogous minimax and existence theorems for the adversarial classification risk. We show that such statements continue to hold for a very general class of surrogate losses; moreover, we remove some of the technical restrictions present in prior work. Our results provide an explanation for the phenomenon of transfer attacks and inform new directions in algorithm development",
    "checked": true,
    "id": "46fc0d9eb634b2e0baa7fa61248731c640ae4243",
    "semantic_title": "existence and minimax theorems for adversarial surrogate risks in binary classification",
    "citation_count": 8,
    "authors": [
      "Natalie S. Frank",
      "Jonathan Niles-Weed"
    ]
  },
  "https://jmlr.org/papers/v25/23-0777.html": {
    "title": "Decomposed Linear Dynamical Systems (dLDS) for learning the latent components of neural dynamics",
    "volume": "main",
    "abstract": "Learning interpretable representations of neural dynamics at a population level is a crucial first step to understanding how observed neural activity relates to perception and behavior. Models of neural dynamics often focus on either low-dimensional projections of neural activity or on learning dynamical systems that explicitly relate to the neural state over time. We discuss how these two approaches are interrelated by considering dynamical systems as representative of flows on a low-dimensional manifold. Building on this concept, we propose a new decomposed dynamical system model that represents complex non-stationary and nonlinear dynamics of time series data as a sparse combination of simpler, more interpretable components. Our model is trained through a dictionary learning procedure, where we leverage recent results in tracking sparse vectors over time. The decomposed nature of the dynamics is more expressive than previous switched approaches for a given number of parameters and enables modeling of overlapping and non-stationary dynamics. In both continuous-time and discrete-time instructional examples, we demonstrate that our model effectively approximates the original system, learns efficient representations, and captures smooth transitions between dynamical modes. Furthermore, we highlight our model's ability to efficiently capture and demix population dynamics generated from multiple independent subnetworks, a task that is computationally impractical for switched models. Finally, we apply our model to neural \"full brain\" recordings of C. elegans data, illustrating a diversity of dynamics that is obscured when classified into discrete states",
    "checked": true,
    "id": "ddfcaefb2efcddee117420624c972a21c6f88425",
    "semantic_title": "decomposed linear dynamical systems (dlds) for learning the latent components of neural dynamics",
    "citation_count": 5,
    "authors": [
      "Noga Mudrik",
      "Yenho Chen",
      "Eva Yezerets",
      "Christopher J. Rozell",
      "Adam S. Charles"
    ]
  },
  "https://jmlr.org/papers/v25/23-0970.html": {
    "title": "Causal-learn: Causal Discovery in Python",
    "volume": "MLOSS",
    "abstract": "Causal discovery aims at revealing causal relations from observational data, which is a fundamental task in science and engineering. We describe causal-learn, an open-source Python library for causal discovery. This library focuses on bringing a comprehensive collection of causal discovery methods to both practitioners and researchers. It provides easy-to-use APIs for non-specialists, modular building blocks for developers, detailed documentation for learners, and comprehensive methods for all. Different from previous packages in R or Java, causal-learn is fully developed in Python, which could be more in tune with the recent preference shift in programming languages within related communities. The library is available at https://github.com/py-why/causal-learn",
    "checked": true,
    "id": "e35c6e468378555c5204c968c1cdec1a28e03a77",
    "semantic_title": "causal-learn: causal discovery in python",
    "citation_count": 17,
    "authors": [
      "Yujia Zheng",
      "Biwei Huang",
      "Wei Chen",
      "Joseph Ramsey",
      "Mingming Gong",
      "Ruichu Cai",
      "Shohei Shimizu",
      "Peter Spirtes",
      "Kun Zhang"
    ]
  },
  "https://jmlr.org/papers/v25/21-0076.html": {
    "title": "Scaling the Convex Barrier with Sparse Dual Algorithms",
    "volume": "main",
    "abstract": "Tight and efficient neural network bounding is crucial to the scaling of neural network verification systems. Many efficient bounding algorithms have been presented recently, but they are often too loose to verify more challenging properties. This is due to the weakness of the employed relaxation, which is usually a linear program of size linear in the number of neurons. While a tighter linear relaxation for piecewise-linear activations exists, it comes at the cost of exponentially many constraints and currently lacks an efficient customized solver. We alleviate this deficiency by presenting two novel dual algorithms: one operates a subgradient method on a small active set of dual variables, the other exploits the sparsity of Frank-Wolfe type optimizers to incur only a linear memory cost. Both methods recover the strengths of the new relaxation: tightness and a linear separation oracle. At the same time, they share the benefits of previous dual approaches for weaker relaxations: massive parallelism, GPU implementation, low cost per iteration and valid bounds at any time. As a consequence, we can obtain better bounds than off-the-shelf solvers in only a fraction of their running time, attaining significant formal verification speed-ups",
    "checked": false,
    "id": "72aee7315cb7ca395465452aa8f06255c6c3ef27",
    "semantic_title": "linear optimization over homogeneous matrix cones",
    "citation_count": 2,
    "authors": [
      "Alessandro De Palma",
      "Harkirat Singh Behl",
      "Rudy Bunel",
      "Philip H.S. Torr",
      "M. Pawan Kumar"
    ]
  },
  "https://jmlr.org/papers/v25/21-1405.html": {
    "title": "Low-rank Variational Bayes correction to the Laplace method",
    "volume": "main",
    "abstract": "Approximate inference methods like the Laplace method, Laplace approximations and variational methods, amongst others, are popular methods when exact inference is not feasible due to the complexity of the model or the abundance of data. In this paper we propose a hybrid approximate method called Low-Rank Variational Bayes correction (VBC), that uses the Laplace method and subsequently a Variational Bayes correction in a lower dimension, to the joint posterior mean. The cost is essentially that of the Laplace method which ensures scalability of the method, in both model complexity and data size. Models with fixed and unknown hyperparameters are considered, for simulated and real examples, for small and large data sets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Janet van Niekerk",
      "Haavard Rue"
    ]
  },
  "https://jmlr.org/papers/v25/22-0743.html": {
    "title": "An Embedding Framework for the Design and Analysis of Consistent Polyhedral Surrogates",
    "volume": "main",
    "abstract": "We formalize and study the natural approach of designing convex surrogate loss functions via embeddings, for discrete problems such as classification, ranking, or structured prediction. In this approach, one embeds each of the finitely many predictions (e.g. rankings) as a point in $\\mathbb{R}^d$, assigns the original loss values to these points, and \"convexifies\" the loss in some way to obtain a surrogate. We establish a strong connection between this approach and polyhedral (piecewise-linear convex) surrogate losses: every discrete loss is embedded by some polyhedral loss, and every polyhedral loss embeds some discrete loss. Moreover, an embedding gives rise to a consistent link function as well as linear surrogate regret bounds. Our results are constructive, as we illustrate with several examples. In particular, our framework gives succinct proofs of consistency or inconsistency for existing polyhedral surrogates, and for inconsistent surrogates, it further reveals the discrete losses for which these surrogates are consistent. We go on to show additional structure of embeddings, such as the equivalence of embedding and matching Bayes risks, and the equivalence of various notions of non-redudancy. Using these results, we establish that indirect elicitation, a necessary condition for consistency, is also sufficient when working with polyhedral surrogates",
    "checked": true,
    "id": "93502133541f5e095b3facd33231d296d20bbb81",
    "semantic_title": "an embedding framework for the design and analysis of consistent polyhedral surrogates",
    "citation_count": 5,
    "authors": [
      "Jessie Finocchiaro",
      "Rafael M. Frongillo",
      "Bo Waggoner"
    ]
  },
  "https://jmlr.org/papers/v25/22-1112.html": {
    "title": "Mathematical Framework for Online Social Media Auditing",
    "volume": "main",
    "abstract": "Social media platforms (SMPs) leverage algorithmic filtering (AF) as a means of selecting the content that constitutes a user's feed with the aim of maximizing their rewards. Selectively choosing the contents to be shown on the user's feed may yield a certain extent of influence, either minor or major, on the user's decision-making, compared to what it would have been under a natural/fair content selection. As we have witnessed over the past decade, algorithmic filtering can cause detrimental side effects, ranging from biasing individual decisions to shaping those of society as a whole, for example, diverting users' attention from whether to get the COVID-19 vaccine or inducing the public to choose a presidential candidate. The government's constant attempts to regulate the adverse effects of AF are often complicated, due to bureaucracy, legal affairs, and financial considerations. On the other hand SMPs seek to monitor their own algorithmic activities to avoid being fined for exceeding the allowable threshold. In this paper, we mathematically formalize this framework and utilize it to construct a data-driven statistical auditing procedure to regulate AF from deflecting users' beliefs over time, along with sample complexity guarantees. This state-of-the-art algorithm can be used either by authorities acting as external regulators or by SMPs for self-auditing",
    "checked": true,
    "id": "135a773889a2bb17566ac65227aaef6270412130",
    "semantic_title": "mathematical framework for online social media auditing",
    "citation_count": 0,
    "authors": [
      "Wasim Huleihel",
      "Yehonathan Refael"
    ]
  },
  "https://jmlr.org/papers/v25/22-1347.html": {
    "title": "Improving Lipschitz-Constrained Neural Networks by Learning Activation Functions",
    "volume": "main",
    "abstract": "Lipschitz-constrained neural networks have several advantages over unconstrained ones and can be applied to a variety of problems, making them a topic of attention in the deep learning community. Unfortunately, it has been shown both theoretically and empirically that they perform poorly when equipped with ReLU activation functions. By contrast, neural networks with learnable 1-Lipschitz linear splines are known to be more expressive. In this paper, we show that such networks correspond to global optima of a constrained functional optimization problem that consists of the training of a neural network composed of 1-Lipschitz linear layers and 1-Lipschitz freeform activation functions with second-order total-variation regularization. Further, we propose an efficient method to train these neural networks. Our numerical experiments show that our trained networks compare favorably with existing 1-Lipschitz neural architectures",
    "checked": true,
    "id": "69504fb1c11dd0429b32915ba64e69759942ef35",
    "semantic_title": "improving lipschitz-constrained neural networks by learning activation functions",
    "citation_count": 7,
    "authors": [
      "Stanislas Ducotterd",
      "Alexis Goujon",
      "Pakshal Bohra",
      "Dimitris Perdios",
      "Sebastian Neumayer",
      "Michael Unser"
    ]
  },
  "https://jmlr.org/papers/v25/23-0347.html": {
    "title": "On Unbiased Estimation for Partially Observed Diffusions",
    "volume": "MLOSS",
    "abstract": "We consider a class of diffusion processes with finite-dimensional parameters and partially observed at discrete time instances. We propose a methodology to unbiasedly estimate the expectation of a given functional of the diffusion process conditional on parameters and data. When these unbiased estimators with appropriately chosen functionals are employed within an expectation-maximization algorithm or a stochastic gradient method, this enables statistical inference using the maximum likelihood or Bayesian framework. Compared to existing approaches, the use of our unbiased estimators allows one to remove any time-discretization bias and Markov chain Monte Carlo burn-in bias. Central to our methodology is a novel and natural combination of multilevel randomization schemes and unbiased Markov chain Monte Carlo methods, and the development of new couplings of multiple conditional particle filters. We establish under assumptions that our estimators are unbiased and have finite variance. We illustrate various aspects of our method on an Ornstein--Uhlenbeck model, a logistic diffusion model for population dynamics, and a neural network model for grid cells",
    "checked": false,
    "id": "1c0c4baaeb5eee3f23756c60f23d8fc28560a4d1",
    "semantic_title": "unbiased parameter estimation for partially observed diffusions",
    "citation_count": 0,
    "authors": [
      "Jeremy Heng",
      "Jeremie Houssineau",
      "Ajay Jasra"
    ]
  },
  "https://jmlr.org/papers/v25/23-0413.html": {
    "title": "Off-Policy Action Anticipation in Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "Learning anticipation in Multi-Agent Reinforcement Learning (MARL) is a reasoning paradigm where agents anticipate the learning steps of other agents to improve cooperation among themselves. As MARL uses gradient-based optimization, learning anticipation requires using Higher-Order Gradients (HOG), with so-called HOG methods. Existing HOG methods are based on policy parameter anticipation, i.e., agents anticipate the changes in policy parameters of other agents. Currently, however, these existing HOG methods have only been developed for differentiable games or games with small state spaces. In this work, we demonstrate that in the case of non-differentiable games with large state spaces, existing HOG methods do not perform well and are inefficient due to their inherent limitations related to policy parameter anticipation and multiple sampling stages. To overcome these problems, we propose Off-Policy Action Anticipation (OffPA2), a novel framework that approaches learning anticipation through action anticipation, i.e., agents anticipate the changes in actions of other agents, via off-policy sampling. We theoretically analyze our proposed OffPA2 and employ it to develop multiple HOG methods that are applicable to non-differentiable games with large state spaces. We conduct a large set of experiments and illustrate that our proposed HOG methods outperform the existing ones regarding efficiency and performance",
    "checked": true,
    "id": "415f7875e7c1e39200a55cd11c5345b280b5edd3",
    "semantic_title": "off-policy action anticipation in multi-agent reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Ariyan Bighashdel",
      "Daan de Geus",
      "Pavol Jancura",
      "Gijs Dubbelman"
    ]
  },
  "https://jmlr.org/papers/v25/23-0450.html": {
    "title": "Learnability of Linear Port-Hamiltonian Systems",
    "volume": "main",
    "abstract": "A complete structure-preserving learning scheme for single-input/single-output (SISO) linear port-Hamiltonian systems is proposed. The construction is based on the solution, when possible, of the unique identification problem for these systems, in ways that reveal fundamental relationships between classical notions in control theory and crucial properties in the machine learning context, like structure-preservation and expressive power. In the canonical case, it is shown that, {up to initializations,} the set of uniquely identified systems can be explicitly characterized as a smooth manifold endowed with global Euclidean coordinates, which allows concluding that the parameter complexity necessary for the replication of the dynamics is only $\\mathcal{O}(n)$ and not $\\mathcal{O}(n^2)$, as suggested by the standard parametrization of these systems. Furthermore, it is shown that linear port-Hamiltonian systems can be learned while remaining agnostic about the dimension of the underlying data-generating system. Numerical experiments show that this methodology can be used to efficiently estimate linear port-Hamiltonian systems out of input-output realizations, making the contributions in this paper the first example of a structure-preserving machine learning paradigm for linear port-Hamiltonian systems based on explicit representations of this model category",
    "checked": true,
    "id": "26bff5c4fdc34b066663e309855fe1adce15eee3",
    "semantic_title": "learnability of linear port-hamiltonian systems",
    "citation_count": 0,
    "authors": [
      "Juan-Pablo Ortega",
      "Daiying Yin"
    ]
  },
  "https://jmlr.org/papers/v25/23-0708.html": {
    "title": "Tangential Wasserstein Projections",
    "volume": "main",
    "abstract": "We develop a notion of projections between sets of probability measures using the geometric properties of the $2$-Wasserstein space. In contrast to existing methods, it is designed for multivariate probability measures that need not be regular, and is computationally efficient to implement via regression. The idea is to work on tangent cones of the Wasserstein space using generalized geodesics. Its structure and computational properties make the method applicable in a variety of settings where probability measures need not be regular, from causal inference to the analysis of object data. An application to estimating causal effects yields a generalization of the synthetic controls method for systems with general heterogeneity described via multivariate probability measures",
    "checked": true,
    "id": "ffd90325ff7eef385e7ce6447607d4f51163a7cf",
    "semantic_title": "tangential wasserstein projections",
    "citation_count": 1,
    "authors": [
      "Florian Gunsilius",
      "Meng Hsuan Hsieh",
      "Myung Jin Lee"
    ]
  },
  "https://jmlr.org/papers/v25/23-0870.html": {
    "title": "Scaling Instruction-Finetuned Language Models",
    "volume": "main",
    "abstract": "Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation, RealToxicityPrompts). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PaLM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks (at time of release), such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyung Won Chung",
      "Le Hou",
      "Shayne Longpre",
      "Barret Zoph",
      "Yi Tay",
      "William Fedus",
      "Yunxuan Li",
      "Xuezhi Wang",
      "Mostafa Dehghani",
      "Siddhartha Brahma",
      "Albert Webson",
      "Shixiang Shane Gu",
      "Zhuyun Dai",
      "Mirac Suzgun",
      "Xinyun Chen",
      "Aakanksha Chowdhery",
      "Alex Castro-Ros",
      "Marie Pellat",
      "Kevin Robinson",
      "Dasha Valter",
      "Sharan Narang",
      "Gaurav Mishra",
      "Adams Yu",
      "Vincent Zhao",
      "Yanping Huang",
      "Andrew Dai",
      "Hongkun Yu",
      "Slav Petrov",
      "Ed H. Chi",
      "Jeff Dean",
      "Jacob Devlin",
      "Adam Roberts",
      "Denny Zhou",
      "Quoc V. Le",
      "Jason Wei"
    ]
  },
  "https://jmlr.org/papers/v25/23-1415.html": {
    "title": "Policy Gradient Methods in the Presence of Symmetries and State Abstractions",
    "volume": "main",
    "abstract": "Reinforcement learning (RL) on high-dimensional and complex problems relies on abstraction for improved efficiency and generalization. In this paper, we study abstraction in the continuous-control setting, and extend the definition of Markov decision process (MDP) homomorphisms to the setting of continuous state and action spaces. We derive a policy gradient theorem on the abstract MDP for both stochastic and deterministic policies. Our policy gradient results allow for leveraging approximate symmetries of the environment for policy optimization. Based on these theorems, we propose a family of actor-critic algorithms that are able to learn the policy and the MDP homomorphism map simultaneously, using the lax bisimulation metric. Finally, we introduce a series of environments with continuous symmetries to further demonstrate the ability of our algorithm for action abstraction in the presence of such symmetries. We demonstrate the effectiveness of our method on our environments, as well as on challenging visual control tasks from the DeepMind Control Suite. Our method's ability to utilize MDP homomorphisms for representation learning leads to improved performance, and the visualizations of the latent space clearly demonstrate the structure of the learned abstraction",
    "checked": true,
    "id": "3960ea842b8b89819f80cf2cd77adc321ff744a1",
    "semantic_title": "policy gradient methods in the presence of symmetries and state abstractions",
    "citation_count": 1,
    "authors": [
      "Prakash Panangaden",
      "Sahand Rezaei-Shoshtari",
      "Rosie Zhao",
      "David Meger",
      "Doina Precup"
    ]
  },
  "https://jmlr.org/papers/v25/19-556.html": {
    "title": "Pareto Smoothed Importance Sampling",
    "volume": "main",
    "abstract": "Importance weighting is a general way to adjust Monte Carlo integration to account for draws from the wrong distribution, but the resulting estimate can be highly variable when the importance ratios have a heavy right tail. This routinely occurs when there are aspects of the target distribution that are not well captured by the approximating distribution, in which case more stable estimates can be obtained by modifying extreme importance ratios. We present a new method for stabilizing importance weights using a generalized Pareto distribution fit to the upper tail of the distribution of the simulated importance ratios. The method, which empirically performs better than existing methods for stabilizing importance sampling estimates, includes stabilized effective sample size estimates, Monte Carlo error estimates, and convergence diagnostics. The presented Pareto $\\hat{k}$ finite sample convergence rate diagnostic is useful for any Monte Carlo estimator",
    "checked": false,
    "id": "e9fea1eb2ba01587253d2d74d616e229318cdd26",
    "semantic_title": "sampling matters: sgd smoothing through importance sampling",
    "citation_count": 0,
    "authors": [
      "Aki Vehtari",
      "Daniel Simpson",
      "Andrew Gelman",
      "Yuling Yao",
      "Jonah Gabry"
    ]
  },
  "https://jmlr.org/papers/v25/21-1132.html": {
    "title": "Data Summarization via Bilevel Optimization",
    "volume": "main",
    "abstract": "The increasing availability of massive data sets poses various challenges for machine learning. Prominent among these is learning models under hardware or human resource constraints. In such resource-constrained settings, a simple yet powerful approach is operating on small subsets of the data. Coresets are weighted subsets of the data that provide approximation guarantees for the optimization objective. However, existing coreset constructions are highly model-specific and are limited to simple models such as linear regression, logistic regression, and k-means. In this work, we propose a generic coreset construction framework that formulates the coreset selection as a cardinality-constrained bilevel optimization problem. In contrast to existing approaches, our framework does not require model-specific adaptations and applies to any twice differentiable model, including neural networks. We show the effectiveness of our framework for a wide range of models in various settings, including training non-convex models online and batch active learning",
    "checked": false,
    "id": "6aaccfd925a42a045bdce1a178071a2403f568e9",
    "semantic_title": "meta-dag: meta causal discovery via bilevel optimization",
    "citation_count": 1,
    "authors": [
      "Zalán Borsos",
      "Mojmír Mutný",
      "Marco Tagliasacchi",
      "Andreas Krause"
    ]
  },
  "https://jmlr.org/papers/v25/21-1536.html": {
    "title": "Differentially private methods for managing model uncertainty in linear regression",
    "volume": "main",
    "abstract": "In this article, we propose differentially private methods for hypothesis testing, model averaging, and model selection for normal linear models. We propose Bayesian methods based on mixtures of $g$-priors and non-Bayesian methods based on likelihood-ratio statistics and information criteria. The procedures are asymptotically consistent and straightforward to implement with existing software. We focus on practical issues such as adjusting critical values so that hypothesis tests have adequate type I error rates and quantifying the uncertainty introduced by the privacy-ensuring mechanisms",
    "checked": false,
    "id": "93ced2b3adbfc875f35a6c598d986c07e61159c5",
    "semantic_title": "diﬀerentially private methods for managing model uncertainty in linear regression models",
    "citation_count": 0,
    "authors": [
      "Víctor Peña",
      "Andrés F. Barrientos"
    ]
  },
  "https://jmlr.org/papers/v25/22-0416.html": {
    "title": "Towards Explainable Evaluation Metrics for Machine Translation",
    "volume": "main",
    "abstract": "Unlike classical lexical overlap metrics such as BLEU, most current evaluation metrics for machine translation (for example, COMET or BERTScore) are based on black-box large language models. They often achieve strong correlations with human judgments, but recent research indicates that the lower-quality classical metrics remain dominant, one of the potential reasons being that their decision processes are more transparent. To foster more widespread acceptance of novel high-quality metrics, explainability thus becomes crucial. In this concept paper, we identify key properties as well as key goals of explainable machine translation metrics and provide a comprehensive synthesis of recent techniques, relating them to our established goals and properties. In this context, we also discuss the latest state-of-the-art approaches to explainable metrics based on generative models such as ChatGPT and GPT4. Finally, we contribute a vision of next-generation approaches, including natural language explanations. We hope that our work can help catalyze and guide future research on explainable evaluation metrics and, mediately, also contribute to better and more transparent machine translation systems",
    "checked": true,
    "id": "8c835daaf7720a168e5d3d669f419765c510bbaf",
    "semantic_title": "towards explainable evaluation metrics for machine translation",
    "citation_count": 5,
    "authors": [
      "Christoph Leiter",
      "Piyawat Lertvittayakumjorn",
      "Marina Fomicheva",
      "Wei Zhao",
      "Yang Gao",
      "Steffen Eger"
    ]
  },
  "https://jmlr.org/papers/v25/22-0670.html": {
    "title": "Distributed Estimation on Semi-Supervised Generalized Linear Model",
    "volume": "main",
    "abstract": "Semi-supervised learning is devoted to using unlabeled data to improve the performance of machine learning algorithms. In this paper, we study the semi-supervised generalized linear model (GLM) in the distributed setup. In the cases of single or multiple machines containing unlabeled data, we propose two distributed semi-supervised algorithms based on the distributed approximate Newton method. When the labeled local sample size is small, our algorithms still give a consistent estimation, while fully supervised methods fail to converge. Moreover, we theoretically prove that the convergence rate is greatly improved when sufficient unlabeled data exists. Therefore, the proposed method requires much fewer rounds of communications to achieve the optimal rate than its fully-supervised counterpart. In the case of the linear model, we prove the rate lower bound after one round of communication, which shows that rate improvement is essential. Finally, several simulation analyses and real data studies are provided to demonstrate the effectiveness of our method",
    "checked": true,
    "id": "b908c0f215cab93f224c8ef62c57e206a0f646f3",
    "semantic_title": "distributed estimation on semi-supervised generalized linear model",
    "citation_count": 0,
    "authors": [
      "Jiyuan Tu",
      "Weidong Liu",
      "Xiaojun Mao"
    ]
  },
  "https://jmlr.org/papers/v25/22-0816.html": {
    "title": "Unlabeled Principal Component Analysis and Matrix Completion",
    "volume": "main",
    "abstract": "We introduce robust principal component analysis from a data matrix in which the entries of its columns have been corrupted by permutations, termed Unlabeled Principal Component Analysis (UPCA). Using algebraic geometry, we establish that UPCA is a well-defined algebraic problem since we prove that the only matrices of minimal rank that agree with the given data are row-permutations of the ground-truth matrix, arising as the unique solutions of a polynomial system of equations. Further, we propose an efficient two-stage algorithmic pipeline for UPCA suitable for the practically relevant case where only a fraction of the data have been permuted. Stage-I employs outlier-robust PCA methods to estimate the ground-truth column-space. Equipped with the column-space, Stage-II applies recent methods for unlabeled sensing to restore the permuted data. Allowing for missing entries on top of permutations in UPCA leads to the problem of unlabeled matrix completion, for which we derive theory and algorithms of similar flavor. Experiments on synthetic data, face images, educational and medical records reveal the potential of our algorithms for applications such as data privatization and record linkage",
    "checked": false,
    "id": "fec999f8f292415f80c04754c6236b65b4bfe4ca",
    "semantic_title": "grid construction of 5g beam-space via matrix completion and principal component analysis",
    "citation_count": 0,
    "authors": [
      "Yunzhen Yao",
      "Liangzu Peng",
      "Manolis C. Tsakiris"
    ]
  },
  "https://jmlr.org/papers/v25/22-1038.html": {
    "title": "Functional Directed Acyclic Graphs",
    "volume": "main",
    "abstract": "In this article, we introduce a new method to estimate a directed acyclic graph (DAG) from multivariate functional data. We build on the notion of faithfulness that relates a DAG with a set of conditional independences among the random functions. We develop two linear operators, the conditional covariance operator and the partial correlation operator, to characterize and evaluate the conditional independence. Based on these operators, we adapt and extend the PC-algorithm to estimate the functional directed graph, so that the computation time depends on the sparsity rather than the full size of the graph. We study the asymptotic properties of the two operators, derive their uniform convergence rates, and establish the uniform consistency of the estimated graph, all of which are obtained while allowing the graph size to diverge to infinity with the sample size. We demonstrate the efficacy of our method through both simulations and an application to a time-course proteomic dataset",
    "checked": false,
    "id": "e0e048f365fe1a541e9edfb779fc1fee38f8b078",
    "semantic_title": "causal discovery via causal star graphs",
    "citation_count": 2,
    "authors": [
      "Kuang-Yao Lee",
      "Lexin Li",
      "Bing Li"
    ]
  },
  "https://jmlr.org/papers/v25/23-0188.html": {
    "title": "Choosing the Number of Topics in LDA Models – A Monte Carlo Comparison of Selection Criteria",
    "volume": "main",
    "abstract": "Selecting the number of topics in Latent Dirichlet Allocation (LDA) models is considered to be a difficult task, for which various approaches have been proposed. In this paper the performance of the recently developed singular Bayesian information criterion (sBIC) is evaluated and compared to the performance of alternative model selection criteria. The sBIC is a generalization of the standard BIC that can be applied to singular statistical models. The comparison is based on Monte Carlo simulations and carried out for several alternative settings, varying with respect to the number of topics, the number of documents and the size of documents in the corpora. Performance is measured using different criteria which take into account the correct number of topics, but also whether the relevant topics from the considered data generation processes (DGPs) are revealed. Practical recommendations for LDA model selection in applications are derived",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Victor Bystrov",
      "Viktoriia Naboka-Krell",
      "Anna Staszewska-Bystrova",
      "Peter Winker"
    ]
  },
  "https://jmlr.org/papers/v25/23-0636.html": {
    "title": "ptwt - The PyTorch Wavelet Toolbox",
    "volume": "MLOSS",
    "abstract": "The fast wavelet transform is an essential workhorse in signal processing. Wavelets are local in the spatial- or temporal- and the frequency-domain. This property enables frequency domain analysis while preserving some spatiotemporal information. Until recently, wavelets rarely appeared in the machine learning literature. We provide the PyTorch Wavelet Toolbox to make wavelet methods more accessible to the deep learning community. Our PyTorch Wavelet Toolbox is well documented. A pip package is installable with `pip install ptwt`",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moritz Wolter",
      "Felix Blanke",
      "Jochen Garcke",
      "Charles Tapley Hoyt"
    ]
  },
  "https://jmlr.org/papers/v25/23-0698.html": {
    "title": "Tight Convergence Rate Bounds for Optimization Under Power Law Spectral Conditions",
    "volume": "main",
    "abstract": "Performance of optimization on quadratic problems sensitively depends on the low-lying part of the spectrum. For large (effectively infinite-dimensional) problems, this part of the spectrum can often be naturally represented or approximated by power law distributions, resulting in power law convergence rates for iterative solutions of these problems by gradient-based algorithms. In this paper, we propose a new spectral condition providing tighter upper bounds for problems with power law optimization trajectories. We use this condition to build a complete picture of upper and lower bounds for a wide range of optimization algorithms - Gradient Descent, Steepest Descent, Heavy Ball, and Conjugate Gradients - with an emphasis on the underlying schedules of learning rate and momentum. In particular, we demonstrate how an optimally accelerated method, its schedule, and convergence upper bound can be obtained in a unified manner for a given shape of the spectrum. Also, we provide first proofs of tight lower bounds for convergence rates of Steepest Descent and Conjugate Gradients under spectral power laws with general exponents. Our experiments show that the obtained convergence bounds and acceleration strategies are not only relevant for exactly quadratic optimization problems, but also fairly accurate when applied to the training of neural networks",
    "checked": true,
    "id": "006092fa578c4ec9fa1403c115825515c6720c3e",
    "semantic_title": "tight convergence rate bounds for optimization under power law spectral conditions",
    "citation_count": 4,
    "authors": [
      "Maksim Velikanov",
      "Dmitry Yarotsky"
    ]
  },
  "https://jmlr.org/papers/v25/23-0866.html": {
    "title": "On the Eigenvalue Decay Rates of a Class of Neural-Network Related Kernel Functions Defined on General Domains",
    "volume": "main",
    "abstract": "In this paper, we provide a strategy to determine the eigenvalue decay rate (EDR) of a large class of kernel functions defined on a general domain rather than $\\mathbb{S}^{d}$. This class of kernel functions include but are not limited to the neural tangent kernel associated with neural networks with different depths and various activation functions. After proving that the dynamics of training the wide neural networks uniformly approximated that of the neural tangent kernel regression on general domains, we can further illustrate the minimax optimality of the wide neural network provided that the underground truth function $f\\in [\\mathcal H_{\\mathrm{NTK}}]^{s}$, an interpolation space associated with the RKHS $\\mathcal{H}_{\\mathrm{NTK}}$ of NTK. We also showed that the overfitted neural network can not generalize well. We believe our approach for determining the EDR of kernels might be also of independent interests",
    "checked": true,
    "id": "3687a90a1a85163d94a08dd5c2fe5a7f42936ff0",
    "semantic_title": "on the eigenvalue decay rates of a class of neural-network related kernel functions defined on general domains",
    "citation_count": 3,
    "authors": [
      "Yicheng Li",
      "Zixiong Yu",
      "Guhan Chen",
      "Qian Lin"
    ]
  },
  "https://jmlr.org/papers/v25/23-1073.html": {
    "title": "Win: Weight-Decay-Integrated Nesterov Acceleration for Faster Network Training",
    "volume": "main",
    "abstract": "Training deep networks on large-scale datasets is computationally challenging. This work explores the problem of \"how to accelerate adaptive gradient algorithms in a general manner\", and proposes an effective Weight-decay-Integrated Nesterov acceleration (Win) to accelerate adaptive algorithms. Taking AdamW and Adam as examples, per iteration, we construct a dynamical loss that combines the vanilla training loss and a dynamic regularizer inspired by proximal point method, and respectively minimize the first- and second-order Taylor approximations of dynamical loss to update variable. This yields our Win acceleration that uses a conservative step and an aggressive step to update, and linearly combines these two updates for acceleration. Next, we extend Win into Win2 which uses multiple aggressive update steps for faster convergence. Then we apply Win and Win2 to the popular LAMB and SGD optimizers. Our transparent derivation could provide insights for other accelerated methods and their integration into adaptive algorithms. Besides, we theoretically justify the faster convergence of Win- and Win2-accelerated AdamW, Adam and LAMB to their non-accelerated counterparts. Experimental results demonstrates the faster convergence speed and superior performance of our Win- and Win2-accelerated AdamW, Adam, LAMB and SGD over their vanilla counterparts on vision classification and language modeling tasks",
    "checked": false,
    "id": "c99be6b5cd24ae05f60256989efbefc7252c7717",
    "semantic_title": "win: weight-decay-integrated nesterov acceleration for adaptive gradient algorithms",
    "citation_count": 6,
    "authors": [
      "Pan Zhou",
      "Xingyu Xie",
      "Zhouchen Lin",
      "Kim-Chuan Toh",
      "Shuicheng Yan"
    ]
  },
  "https://jmlr.org/papers/v25/23-1257.html": {
    "title": "On the Learnability of Out-of-distribution Detection",
    "volume": "main",
    "abstract": "Supervised learning aims to train a classifier under the assumption that training and test data are from the same distribution. To ease the above assumption, researchers have studied a more realistic setting: out-of-distribution (OOD) detection, where test data may come from classes that are unknown during training (i.e., OOD data). Due to the unavailability and diversity of OOD data, good generalization ability is crucial for effective OOD detection algorithms, and corresponding learning theory is still an open problem. To study the generalization of OOD detection, this paper investigates the probably approximately correct (PAC) learning theory of OOD detection that fits the commonly used evaluation metrics in the literature. First, we find a necessary condition for the learnability of OOD detection. Then, using this condition, we prove several impossibility theorems for the learnability of OOD detection under some scenarios. Although the impossibility theorems are frustrating, we find that some conditions of these impossibility theorems may not hold in some practical scenarios. Based on this observation, we next give several necessary and sufficient conditions to characterize the learnability of OOD detection in some practical scenarios. Lastly, we offer theoretical support for representative OOD detection works based on our OOD theory",
    "checked": true,
    "id": "4ff175285ef575f4dc24a518869139382665c12e",
    "semantic_title": "on the learnability of out-of-distribution detection",
    "citation_count": 0,
    "authors": [
      "Zhen Fang",
      "Yixuan Li",
      "Feng Liu",
      "Bo Han",
      "Jie Lu"
    ]
  },
  "https://jmlr.org/papers/v25/21-0022.html": {
    "title": "Learning Non-Gaussian Graphical Models via Hessian Scores and Triangular Transport",
    "volume": "main",
    "abstract": "Undirected probabilistic graphical models represent the conditional dependencies, or Markov properties, of a collection of random variables. Knowing the sparsity of such a graphical model is valuable for modeling multivariate distributions and for efficiently performing inference. While the problem of learning graph structure from data has been studied extensively for certain parametric families of distributions, most existing methods fail to consistently recover the graph structure for non-Gaussian data. Here we propose an algorithm for learning the Markov structure of continuous and non-Gaussian distributions. To characterize conditional independence, we introduce a score based on integrated Hessian information from the joint log-density, and we prove that this score upper bounds the conditional mutual information for a general class of distributions. To compute the score, our algorithm SING estimates the density using a deterministic coupling, induced by a triangular transport map, and iteratively exploits sparse structure in the map to reveal sparsity in the graph. For certain non-Gaussian datasets, we show that our algorithm recovers the graph structure even with a biased approximation to the density. Among other examples, we apply SING to learn the dependencies between the states of a chaotic dynamical system with local interactions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ricardo Baptista",
      "Rebecca Morrison",
      "Olivier Zahm",
      "Youssef Marzouk"
    ]
  },
  "https://jmlr.org/papers/v25/21-1181.html": {
    "title": "A Semi-parametric Estimation of Personalized Dose-response Function Using Instrumental Variables",
    "volume": "main",
    "abstract": "In the application of instrumental variable analysis that conducts causal inference in the presence of unmeasured confounding, invalid instrumental variables and weak instrumental variables often exist which complicate the analysis. In this paper, we propose a model-free dimension reduction procedure to select the invalid instrumental variables and refine them into lower-dimensional linear combinations. The procedure also combines the weak instrumental variables into a few stronger instrumental variables that best condense their information. We then introduce the personalized dose-response function that incorporates the subject's personal characteristics into the conventional dose-response function, and use the reduced data from dimension reduction to propose a novel and easily implementable nonparametric estimator of this function. The proposed approach is suitable for both discrete and continuous treatment variables, and is robust to the dimensionality of data. Its effectiveness is illustrated by the simulation studies and the data analysis of ADNI-DoD study, where the causal relationship between depression and dementia is investigated",
    "checked": true,
    "id": "21a78e0a6add430fc5b1d70a3d7e7c114dc249e8",
    "semantic_title": "a semi-parametric estimation of personalized dose-response function using instrumental variables",
    "citation_count": 0,
    "authors": [
      "Wei Luo",
      "Yeying Zhu",
      "Xuekui Zhang",
      "Lin Lin"
    ]
  },
  "https://jmlr.org/papers/v25/22-0083.html": {
    "title": "Spatial meshing for general Bayesian multivariate models",
    "volume": "main",
    "abstract": "Quantifying spatial and/or temporal associations in multivariate geolocated data of different types is achievable via spatial random effects in a Bayesian hierarchical model, but severe computational bottlenecks arise when spatial dependence is encoded as a latent Gaussian process (GP) in the increasingly common large scale data settings on which we focus. The scenario worsens in non-Gaussian models because the reduced analytical tractability leads to additional hurdles to computational efficiency. In this article, we introduce Bayesian models of spatially referenced data in which the likelihood or the latent process (or both) are not Gaussian. First, we exploit the advantages of spatial processes built via directed acyclic graphs, in which case the spatial nodes enter the Bayesian hierarchy and lead to posterior sampling via routine Markov chain Monte Carlo (MCMC) methods. Second, motivated by the possible inefficiencies of popular gradient-based sampling approaches in the multivariate contexts on which we focus, we introduce the simplified manifold preconditioner adaptation (SiMPA) algorithm which uses second order information about the target but avoids expensive matrix operations. We demostrate the performance and efficiency improvements of our methods relative to alternatives in extensive synthetic and real world remote sensing and community ecology applications with large scale data at up to hundreds of thousands of spatial locations and up to tens of outcomes. Software for the proposed methods is part of R package meshed, available on CRAN",
    "checked": true,
    "id": "d0684f0c160df89afc00f3f038041a3a34e6454b",
    "semantic_title": "spatial meshing for general bayesian multivariate models",
    "citation_count": 5,
    "authors": [
      "Michele Peruzzi",
      "David B. Dunson"
    ]
  },
  "https://jmlr.org/papers/v25/22-0488.html": {
    "title": "Nonparametric Estimation of Non-Crossing Quantile Regression Process with Deep ReQU Neural Networks",
    "volume": "main",
    "abstract": "We propose a penalized nonparametric approach to estimating the quantile regression process (QRP) in a nonseparable model using rectifier quadratic unit (ReQU) activated deep neural networks and introduce a novel penalty function to enforce non-crossing of quantile regression curves. We establish the non-asymptotic excess risk bounds for the estimated QRP and derive the mean integrated squared error for the estimated QRP under mild smoothness and regularity conditions. To establish these non-asymptotic risk and estimation error bounds, we also develop a new error bound for approximating $C^s$ smooth functions with $s >1$ and their derivatives using ReQU activated neural networks. This is a new approximation result for ReQU networks and is of independent interest and may be useful in other problems. Our numerical experiments demonstrate that the proposed method is competitive with or outperforms two existing methods, including methods using reproducing kernels and random forests for nonparametric quantile regression",
    "checked": false,
    "id": "df63595a48ad8ac938ddebb8c9f0b83c75eb56a9",
    "semantic_title": "estimation of non-crossing quantile regression process with deep requ neural networks",
    "citation_count": 2,
    "authors": [
      "Guohao Shen",
      "Yuling Jiao",
      "Yuanyuan Lin",
      "Joel L. Horowitz",
      "Jian Huang"
    ]
  },
  "https://jmlr.org/papers/v25/22-0673.html": {
    "title": "Minimax Rates for High-Dimensional Random Tessellation Forests",
    "volume": "main",
    "abstract": "Random forests are a popular class of algorithms used for regression and classification. The algorithm introduced by Breiman in 2001 and many of its variants are ensembles of randomized decision trees built from axis-aligned partitions of the feature space. One such variant, called Mondrian forests, was proposed to handle the online setting and is the first class of random forests for which minimax optimal rates were obtained in arbitrary dimension. However, the restriction to axis-aligned splits fails to capture dependencies between features, and random forests that use oblique splits have shown improved empirical performance for many tasks. This work shows that a large class of random forests with general split directions also achieve minimax optimal rates in arbitrary dimension. This class includes STIT forests, a generalization of Mondrian forests to arbitrary split directions, and random forests derived from Poisson hyperplane tessellations. These are the first results showing that random forest variants with oblique splits can obtain minimax optimality in arbitrary dimension. Our proof technique relies on the novel application of the theory of stationary random tessellations in stochastic geometry to statistical learning theory",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eliza O'Reilly",
      "Ngoc Mai Tran"
    ]
  },
  "https://jmlr.org/papers/v25/22-0832.html": {
    "title": "Stochastic Approximation with Decision-Dependent Distributions: Asymptotic Normality and Optimality",
    "volume": "main",
    "abstract": "We analyze a stochastic approximation algorithm for decision-dependent problems, wherein the data distribution used by the algorithm evolves along the iterate sequence. The primary examples of such problems appear in performative prediction and its multiplayer extensions. We show that under mild assumptions, the deviation between the average iterate of the algorithm and the solution is asymptotically normal, with a covariance that clearly decouples the effects of the gradient noise and the distributional shift. Moreover, building on the work of Hájek and Le Cam, we show that the asymptotic performance of the algorithm with averaging is locally minimax optimal",
    "checked": true,
    "id": "1493e1ddec3ca6ca3305219401b14dfd93139362",
    "semantic_title": "stochastic approximation with decision-dependent distributions: asymptotic normality and optimality",
    "citation_count": 2,
    "authors": [
      "Joshua Cutler",
      "Mateo Díaz",
      "Dmitriy Drusvyatskiy"
    ]
  },
  "https://jmlr.org/papers/v25/22-1312.html": {
    "title": "The good, the bad and the ugly sides of data augmentation: An implicit spectral regularization perspective",
    "volume": "main",
    "abstract": "Data augmentation (DA) is a powerful workhorse for bolstering performance in modern machine learning. Specific augmentations like translations and scaling in computer vision are traditionally believed to improve generalization by generating new (artificial) data from the same distribution. However, this traditional viewpoint does not explain the success of prevalent augmentations in modern machine learning (e.g. randomized masking, cutout, mixup), that greatly alter the training data distribution. In this work, we develop a new theoretical framework to characterize the impact of a general class of DA on underparameterized and overparameterized linear model generalization. Our framework reveals that DA induces implicit spectral regularization through a combination of two distinct effects: a) manipulating the relative proportion of eigenvalues of the data covariance matrix in a training-data-dependent manner, and b) uniformly boosting the entire spectrum of the data covariance matrix through ridge regression. These effects, when applied to popular augmentations, give rise to a wide variety of phenomena, including discrepancies in generalization between overparameterized and underparameterized regimes and differences between regression and classification tasks. Our framework highlights the nuanced and sometimes surprising impacts of DA on generalization, and serves as a testbed for novel augmentation design",
    "checked": true,
    "id": "75a444bce143cca2e762c63ab59fd1edfab8c7ae",
    "semantic_title": "the good, the bad and the ugly sides of data augmentation: an implicit spectral regularization perspective",
    "citation_count": 9,
    "authors": [
      "Chi-Heng Lin",
      "Chiraag Kaushik",
      "Eva L. Dyer",
      "Vidya Muthukumar"
    ]
  },
  "https://jmlr.org/papers/v25/23-0295.html": {
    "title": "Exploration of the Search Space of Gaussian Graphical Models for Paired Data",
    "volume": "main",
    "abstract": "We consider the problem of learning a Gaussian graphical model in the case where the observations come from two dependent groups sharing the same variables. We focus on a family of coloured Gaussian graphical models specifically suited for the paired data problem. Commonly, graphical models are ordered by the submodel relationship so that the search space is a lattice, called the model inclusion lattice. We introduce a novel order between models, named the twin order. We show that, embedded with this order, the model space is a lattice that, unlike the model inclusion lattice, is distributive. Furthermore, we provide the relevant rules for the computation of the neighbours of a model. The latter are more efficient than the same operations in the model inclusion lattice, and are then exploited to achieve a more efficient exploration of the search space. These results can be applied to improve the efficiency of both greedy and Bayesian model search procedures. Here, we implement a stepwise backward elimination procedure and evaluate its performance both on synthetic and real-world data",
    "checked": true,
    "id": "9b8009845b1f4e07964cdb50fd81a730cb7ef734",
    "semantic_title": "exploration of the search space of gaussian graphical models for paired data",
    "citation_count": 1,
    "authors": [
      "Alberto Roverato",
      "Dung Ngoc Nguyen"
    ]
  },
  "https://jmlr.org/papers/v25/23-0645.html": {
    "title": "Sparse Representer Theorems for Learning in Reproducing Kernel Banach Spaces",
    "volume": "main",
    "abstract": "Sparsity of a learning solution is a desirable feature in machine learning. Certain reproducing kernel Banach spaces (RKBSs) are appropriate hypothesis spaces for sparse learning methods. The goal of this paper is to understand what kind of RKBSs can promote sparsity for learning solutions. We consider two typical learning models in an RKBS: the minimum norm interpolation (MNI) problem and the regularization problem. We first establish an explicit representer theorem for solutions of these problems, which represents the extreme points of the solution set by a linear combination of the extreme points of the subdifferential set, of the norm function, which is data-dependent. We then propose sufficient conditions on the RKBS that can transform the explicit representation of the solutions to a sparse kernel representation having fewer terms than the number of the observed data. Under the proposed sufficient conditions, we investigate the role of the regularization parameter on sparsity of the regularized solutions. We further show that two specific RKBSs, the sequence space $\\ell_1(\\mathbb{N})$ and the measure space, can have sparse representer theorems for both MNI and regularization models",
    "checked": true,
    "id": "881e658762940f9d18971df09eb501c2e92de08c",
    "semantic_title": "sparse representer theorems for learning in reproducing kernel banach spaces",
    "citation_count": 1,
    "authors": [
      "Rui Wang",
      "Yuesheng Xu",
      "Mingsong Yan"
    ]
  },
  "https://jmlr.org/papers/v25/23-0740.html": {
    "title": "Overparametrized Multi-layer Neural Networks: Uniform Concentration of Neural Tangent Kernel and Convergence of Stochastic Gradient Descent",
    "volume": "main",
    "abstract": "There have been exciting progresses in understanding the convergence of gradient descent (GD) and stochastic gradient descent (SGD) in overparameterized neural networks through the lens of neural tangent kernel (NTK). However, there remain two significant gaps between theory and practice. First, the existing convergence theory only takes into account the contribution of the NTK from the last hidden layer, while in practice the intermediate layers also play an instrumental role. Second, most existing works assume that the training data are provided a priori in a batch, while less attention has been paid to the important setting where the training data arrive in a stream. In this paper, we close these two gaps. We first show that with random initialization, the NTK function converges to some deterministic function uniformly for all layers as the number of neurons tends to infinity. Then we apply the uniform convergence result to further prove that the prediction error of multi-layer neural networks under SGD converges in expectation in the streaming data setting. A key ingredient in our proof is to show the number of activation patterns of an $L$-layer neural network with width $m$ is only polynomial in $m$ although there are $mL$ neurons in total",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaming Xu",
      "Hanjing Zhu"
    ]
  },
  "https://jmlr.org/papers/v25/23-0985.html": {
    "title": "A General Framework for the Analysis of Kernel-based Tests",
    "volume": "main",
    "abstract": "Kernel-based tests provide a simple yet effective framework that uses the theory of reproducing kernel Hilbert spaces to design non-parametric testing procedures. In this paper, we propose new theoretical tools that can be used to study the asymptotic behaviour of kernel-based tests in various data scenarios and in different testing problems. Unlike current approaches, our methods avoid working with U and V-statistics expansions that usually lead to lengthy and tedious computations and asymptotic approximations. Instead, we work directly with random functionals on the Hilbert space to analyse kernel-based tests. By harnessing the use of random functionals, our framework leads to much cleaner analyses, involving less tedious computations. Additionally, it offers the advantage of accommodating pre-existing knowledge regarding test-statistics as many of the random functionals considered in applications are known statistics that have been studied comprehensively. To demonstrate the efficacy of our approach, we thoroughly examine two categories of kernel tests, along with three specific examples of kernel tests, including a novel kernel test for conditional independence testing",
    "checked": true,
    "id": "ff19bfd0d6d40b82dc3c6c33bf7ad2be121358c5",
    "semantic_title": "a general framework for the analysis of kernel-based tests",
    "citation_count": 4,
    "authors": [
      "Tamara Fernández",
      "Nicolás Rivera"
    ]
  },
  "https://jmlr.org/papers/v25/23-1086.html": {
    "title": "MAP- and MLE-Based Teaching",
    "volume": "main",
    "abstract": "Imagine a learner $L$ who tries to infer a hidden concept from a collection of observations. Building on the work of Ferri et al we assume the learner to be parameterized by priors $P(c)$ and by $c$-conditional likelihoods $P(z|c)$ where $c$ ranges over all concepts in a given class $C$ and $z$ ranges over all observations in an observation set $Z$. $L$ is called a MAP-learner (resp.~an MLE-learner) if it thinks of a collection $S$ of observations as a random sample and returns the concept with the maximum a-posteriori probability (resp.~the concept which maximizes the $c$-conditional likelihood of $S$). Depending on whether $L$ assumes that $S$ is obtained from ordered or unordered sampling resp.~from sampling with or without replacement, we can distinguish four different sampling modes. Given a target concept $c^* \\in C$, a teacher for a MAP-learner $L$ aims at finding a smallest collection of observations that causes $L$ to return $c^*$. This approach leads in a natural manner to various notions of a MAP- or MLE-teaching dimension of a concept class $C$. Our main results are as follows. First, we show that this teaching model has some desirable monotonicity properties. Second we clarify how the four sampling modes are related to each other. As for the (important!) special case, where concepts are subsets of a domain and observations are 0,1-labeled examples, we obtain some additional results. First of all, we characterize the MAP- and MLE-teaching dimension associated with an optimally parameterized MAP-learner graph-theoretically. From this central result, some other ones are easy to derive. It is shown, for instance, that the MLE-teaching dimension is either equal to the MAP-teaching dimension or exceeds the latter by $1$. It is shown furthermore that these dimensions can be bounded from above by the so-called antichain number, the VC-dimension and related combinatorial parameters. Moreover they can be computed in polynomial time",
    "checked": true,
    "id": "69109a5fd8f5432a467efe7f065adaddba3640af",
    "semantic_title": "map- and mle-based teaching",
    "citation_count": 0,
    "authors": [
      "Hans Ulrich Simon",
      "Jan Arne Telle"
    ]
  },
  "https://jmlr.org/papers/v25/23-1318.html": {
    "title": "Scaling Speech Technology to 1,000+ Languages",
    "volume": "main",
    "abstract": "Expanding the language coverage of speech technology has the potential to improve access to information for many more people. However, current speech technology is restricted to about one hundred languages which is a small fraction of the over 7,000 languages spoken around the world. The Massively Multilingual Speech (MMS) project increases the number of supported languages by 10-40x, depending on the task while providing improved accuracy compared to prior work. The main ingredients are a new dataset based on readings of publicly available religious texts and effectively leveraging self-supervised learning. We built pre-trained wav2vec 2.0 models covering 1,406 languages, a single multilingual automatic speech recognition model for 1,107 languages, speech synthesis models for the same number of languages, as well as a language identification model for 4,017 languages. Experiments show that our multilingual speech recognition model more than halves the word error rate of Whisper on 54 languages of the FLEURS benchmark while being trained on a small fraction of the labeled data",
    "checked": false,
    "id": "0f416c637a5a78435e6b12ebf1ce891224de0edc",
    "semantic_title": "scaling speech technology to 1, 000+ languages",
    "citation_count": 101,
    "authors": [
      "Vineel Pratap",
      "Andros Tjandra",
      "Bowen Shi",
      "Paden Tomasello",
      "Arun Babu",
      "Sayani Kundu",
      "Ali Elkahky",
      "Zhaoheng Ni",
      "Apoorv Vyas",
      "Maryam Fazel-Zarandi",
      "Alexei Baevski",
      "Yossi Adi",
      "Xiaohui Zhang",
      "Wei-Ning Hsu",
      "Alexis Conneau",
      "Michael Auli"
    ]
  },
  "https://jmlr.org/papers/v25/21-0748.html": {
    "title": "Adaptivity and Non-stationarity: Problem-dependent Dynamic Regret for Online Convex Optimization",
    "volume": "main",
    "abstract": "We investigate online convex optimization in non-stationary environments and choose dynamic regret as the performance measure, defined as the difference between cumulative loss incurred by the online algorithm and that of any feasible comparator sequence. Let $T$ be the time horizon and $P_T$ be the path length that essentially reflects the non-stationarity of environments, the state-of-the-art dynamic regret is $\\mathcal{O}(\\sqrt{T(1+P_T)})$. Although this bound is proved to be minimax optimal for convex functions, in this paper, we demonstrate that it is possible to further enhance the guarantee for some easy problem instances, particularly when online functions are smooth. Specifically, we introduce novel online algorithms that can exploit smoothness and replace the dependence on $T$ in dynamic regret with problem-dependent quantities: the variation in gradients of loss functions, the cumulative loss of the comparator sequence, and the minimum of these two terms. These quantities are at most $\\mathcal{O}(T)$ while could be much smaller in benign environments. Therefore, our results are adaptive to the intrinsic difficulty of the problem, since the bounds are tighter than existing results for easy problems and meanwhile safeguard the same rate in the worst case. Notably, our proposed algorithms can achieve favorable dynamic regret with only one gradient per iteration, sharing the same gradient query complexity as the static regret minimization methods. To accomplish this, we introduce the collaborative online ensemble framework. The proposed framework employs a two-layer online ensemble to handle non-stationarity, and uses optimistic online learning and further introduces crucial correction terms to enable effective collaboration within the meta-base two layers, thereby attaining adaptivity. We believe the framework can be useful for broader problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Zhao",
      "Yu-Jie Zhang",
      "Lijun Zhang",
      "Zhi-Hua Zhou"
    ]
  },
  "https://jmlr.org/papers/v25/21-1504.html": {
    "title": "Semi-supervised Inference for Block-wise Missing Data without Imputation",
    "volume": "main",
    "abstract": "We consider statistical inference for single or low-dimensional parameters in a high-dimensional linear model under a semi-supervised setting, wherein the data are a combination of a labelled block-wise missing data set of a relatively small size and a large unlabelled data set. The proposed method utilises both labelled and unlabelled data without any imputation or removal of the missing observations. The asymptotic properties of the estimator are established under regularity conditions. Hypothesis testing for low-dimensional coefficients are also studied. Extensive simulations are conducted to examine the theoretical results. The method is evaluated on the Alzheimer's Disease Neuroimaging Initiative data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shanshan Song",
      "Yuanyuan Lin",
      "Yong Zhou"
    ]
  },
  "https://jmlr.org/papers/v25/22-0186.html": {
    "title": "Materials Discovery using Max K-Armed Bandit",
    "volume": "main",
    "abstract": "Search algorithms for bandit problems are applicable in materials discovery. However, objectives of the conventional bandit problem are different from those of materials discovery. The conventional bandit problem aims to maximize the total rewards, whereas materials discovery aims to achieve breakthroughs in material properties. The max $K$-armed bandit (MKB) problem, which aims to acquire the single best reward, matches with the discovery tasks better than the conventional bandit. However, typical MKB algorithms are not directly applicable to materials discovery due to some difficulties. The typical algorithms have many hyperparameters and some difficulty in the directly implementation for the materials discovery. Thus, we propose a new MKB algorithm using an upper confidence bound of expected improvement of the best reward. This approach is guaranteed to be asymptotic to greedy oracles, which does not depend on the time horizon. In addition, compared with other MKB algorithms, the proposed algorithm has only one hyperparameter, which is advantageous in materials discovery. We applied the proposed algorithm to synthetic problems and molecular-design demonstrations using a Monte Carlo tree search. According to the results, the proposed algorithm stably outperformed other bandit algorithms in the late stage of the search process, unless the optimal arm coincides in the MKB and conventional bandit settings",
    "checked": true,
    "id": "2059672c85b3296bfd75dc5657315524206bf6b6",
    "semantic_title": "materials discovery using max k-armed bandit",
    "citation_count": 0,
    "authors": [
      "Nobuaki Kikkawa",
      "Hiroshi Ohno"
    ]
  },
  "https://jmlr.org/papers/v25/22-0493.html": {
    "title": "AMLB: an AutoML Benchmark",
    "volume": "main",
    "abstract": "Comparing different AutoML frameworks is notoriously challenging and often done incorrectly. We introduce an open and extensible benchmark that follows best practices and avoids common mistakes when comparing AutoML frameworks. We conduct a thorough comparison of 9 well-known AutoML frameworks across 71 classification and 33 regression tasks. The differences between the AutoML frameworks are explored with a multi-faceted analysis, evaluating model accuracy, its trade-offs with inference time, and framework failures. We also use Bradley-Terry trees to discover subsets of tasks where the relative AutoML framework rankings differ. The benchmark comes with an open-source tool that integrates with many AutoML frameworks and automates the empirical evaluation process end-to-end: from framework installation and resource allocation to in-depth evaluation. The benchmark uses public data sets, can be easily extended with other AutoML frameworks and tasks, and has a website with up-to-date results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pieter Gijsbers",
      "Marcos L. P. Bueno",
      "Stefan Coors",
      "Erin LeDell",
      "Sébastien Poirier",
      "Janek Thomas",
      "Bernd Bischl",
      "Joaquin Vanschoren"
    ]
  },
  "https://jmlr.org/papers/v25/22-0735.html": {
    "title": "Nonparametric Regression for 3D Point Cloud Learning",
    "volume": "main",
    "abstract": "In recent years, there has been an exponentially increased amount of point clouds collected with irregular shapes in various areas. Motivated by the importance of solid modeling for point clouds, we develop a novel and efficient smoothing tool based on multivariate splines over the triangulation to extract the underlying signal and build up a 3D solid model from the point cloud. The proposed method can denoise or deblur the point cloud effectively, provide a multi-resolution reconstruction of the actual signal, and handle sparse and irregularly distributed point clouds to recover the underlying trajectory. In addition, our method provides a natural way of numerosity data reduction. We establish the theoretical guarantees of the proposed method, including the convergence rate and asymptotic normality of the estimator, and show that the convergence rate achieves optimal nonparametric convergence. We also introduce a bootstrap method to quantify the uncertainty of the estimators. Through extensive simulation studies and a real data example, we demonstrate the superiority of the proposed method over traditional smoothing methods in terms of estimation accuracy and efficiency of data reduction",
    "checked": false,
    "id": "e0379bbecb520c64c89a3247c9775f4bbe86e633",
    "semantic_title": "alreg: registration of 3d point clouds using active learning",
    "citation_count": 1,
    "authors": [
      "Xinyi Li",
      "Shan Yu",
      "Yueying Wang",
      "Guannan Wang",
      "Li Wang",
      "Ming-Jun Lai"
    ]
  },
  "https://jmlr.org/papers/v25/22-0988.html": {
    "title": "Information Processing Equalities and the Information–Risk Bridge",
    "volume": "main",
    "abstract": "We introduce two new classes of measures of information for statistical experiments which generalise and subsume φ-divergences, integral probability metrics, N-distances (MMD), and (f,Γ) divergences between two or more distributions. This enables us to derive a simple geometrical relationship between measures of information and the Bayes risk of a statistical decision problem, thus extending the variational φ-divergence representation to multiple distributions in an entirely symmetric manner. The new families of divergence are closed under the action of Markov operators which yields an information processing equality which is a refinement and generalisation of the classical information processing inequality. This equality gives insight into the significance of the choice of the hypothesis class in classical risk minimization",
    "checked": false,
    "id": "ee0a6996b6ff2dd79e81769a77d59510ca085352",
    "semantic_title": "information processing equalities and the information-risk bridge",
    "citation_count": 4,
    "authors": [
      "Robert C. Williamson",
      "Zac Cranko"
    ]
  },
  "https://jmlr.org/papers/v25/23-0121.html": {
    "title": "Multi-class Probabilistic Bounds for Majority Vote Classifiers with Partially Labeled Data",
    "volume": "main",
    "abstract": "In this paper, we propose a probabilistic framework for analyzing a multi-class majority vote classifier in the case where training data is partially labeled. First, we derive a multi-class transductive bound over the risk of the majority vote classifier, which is based on the classifier's vote distribution over each class. Then, we introduce a mislabeling error model to analyze the error of the majority vote classifier in the case of the pseudo-labeled training data. We derive a generalization bound over the majority vote error when imperfect labels are given, taking into account the mean and the variance of the prediction margin. Finally, we demonstrate an application of the derived transductive bound for self-training to find automatically the confidence threshold used to determine unlabeled examples for pseudo-labeling. Empirical results on different data sets show the effectiveness of our framework compared to several state-of-the-art semi-supervised approaches",
    "checked": false,
    "id": "4f9eefd9a34ea8ceb4b76ffe28983268c765d419",
    "semantic_title": "multi-class probabilistic bounds for majority vote classiﬁers with partially labeled data",
    "citation_count": 0,
    "authors": [
      "Vasilii Feofanov",
      "Emilie Devijver",
      "Massih-Reza Amini"
    ]
  },
  "https://jmlr.org/papers/v25/23-0570.html": {
    "title": "Unsupervised Anomaly Detection Algorithms on Real-world Data: How Many Do We Need?",
    "volume": "main",
    "abstract": "In this study we evaluate 33 unsupervised anomaly detection algorithms on 52 real-world multivariate tabular data sets, performing the largest comparison of unsupervised anomaly detection algorithms to date. On this collection of data sets, the EIF (Extended Isolation Forest) algorithm significantly outperforms the most other algorithms. Visualizing and then clustering the relative performance of the considered algorithms on all data sets, we identify two clear clusters: one with \"local\" data sets, and another with \"global\" data sets. \"Local\" anomalies occupy a region with low density when compared to nearby samples, while \"global\" occupy an overall low density region in the feature space. On the local data sets the $k$NN ($k$-nearest neighbor) algorithm comes out on top. On the global data sets, the EIF (extended isolation forest) algorithm performs the best. Also taking into consideration the algorithms' computational complexity, a toolbox with these two unsupervised anomaly detection algorithms suffices for finding anomalies in this representative collection of multivariate data sets. By providing access to code and data sets, our study can be easily reproduced and extended with more algorithms and/or data sets",
    "checked": true,
    "id": "f9d54ea2ae4a256ec9721ecebb9c6284abe1edc5",
    "semantic_title": "unsupervised anomaly detection algorithms on real-world data: how many do we need?",
    "citation_count": 1,
    "authors": [
      "Roel Bouman",
      "Zaharah Bukhsh",
      "Tom Heskes"
    ]
  },
  "https://jmlr.org/papers/v25/23-0680.html": {
    "title": "PhAST: Physics-Aware, Scalable, and Task-Specific GNNs for Accelerated Catalyst Design",
    "volume": "main",
    "abstract": "Mitigating the climate crisis requires a rapid transition towards lower-carbon energy. Catalyst materials play a crucial role in the electrochemical reactions involved in numerous industrial processes key to this transition, such as renewable energy storage and electrofuel synthesis. To reduce the energy spent on such activities, we must quickly discover more efficient catalysts to drive electrochemical reactions. Machine learning (ML) holds the potential to efficiently model materials properties from large amounts of data, accelerating electrocatalyst design. The Open Catalyst Project OC20 dataset was constructed to that end. However, ML models trained on OC20 are still neither scalable nor accurate enough for practical applications. In this paper, we propose task-specific innovations applicable to most architectures, enhancing both computational efficiency and accuracy. This includes improvements in (1) the graph creation step, (2) atom representations, (3) the energy prediction head, and (4) the force prediction head. We describe these contributions, referred to as PhAST, and evaluate them thoroughly on multiple architectures. Overall, PhAST improves energy MAE by 4 to 42% while dividing compute time by 3 to 8× depending on the targeted task/model. PhAST also enables CPU training, leading to 40× speedups in highly parallelized settings. Python package: https://phast.readthedocs.io",
    "checked": true,
    "id": "7114a1d6d22ce98408ed58955ac3040068af70b8",
    "semantic_title": "phast: physics-aware, scalable, and task-specific gnns for accelerated catalyst design",
    "citation_count": 5,
    "authors": [
      "Alexandre Duval",
      "Victor Schmidt",
      "Santiago Miret",
      "Yoshua Bengio",
      "Alex Hernández-García",
      "David Rolnick"
    ]
  },
  "https://jmlr.org/papers/v25/23-0811.html": {
    "title": "Random Forest Weighted Local Fréchet Regression with Random Objects",
    "volume": "main",
    "abstract": "Statistical analysis is increasingly confronted with complex data from metric spaces. Petersen and Müller (2019) established a general paradigm of Fréchet regression with complex metric space valued responses and Euclidean predictors. However, the local approach therein involves nonparametric kernel smoothing and suffers from the curse of dimensionality. To address this issue, we in this paper propose a novel random forest weighted local Fréchet regression paradigm. The main mechanism of our approach relies on a locally adaptive kernel generated by random forests. Our first method uses these weights as the local average to solve the conditional Fréchet mean, while the second method performs local linear Fréchet regression, both significantly improving existing Fréchet regression methods. Based on the theory of infinite order U-processes and infinite order $M_{m_n}$-estimator, we establish the consistency, rate of convergence, and asymptotic normality for our local constant estimator, which covers the current large sample theory of random forests with Euclidean responses as a special case. Numerical studies show the superiority of our methods with several commonly encountered types of responses such as distribution functions, symmetric positive-definite matrices, and sphere data. The practical merits of our proposals are also demonstrated through the application to New York taxi data and human mortality data",
    "checked": false,
    "id": "f29c57a2244939072df8922a649b14da1db4ec70",
    "semantic_title": "random forest weighted local fr\\'echet regression with random objects",
    "citation_count": 0,
    "authors": [
      "Rui Qiu",
      "Zhou Yu",
      "Ruoqing Zhu"
    ]
  },
  "https://jmlr.org/papers/v25/23-1027.html": {
    "title": "QDax: A Library for Quality-Diversity and Population-based Algorithms with Hardware Acceleration",
    "volume": "MLOSS",
    "abstract": "QDax is an open-source library with a streamlined and modular API for Quality-Diversity (QD) optimisation algorithms in Jax. The library serves as a versatile tool for optimisation purposes, ranging from black-box optimisation to continuous control. QDax offers implementations of popular QD, Neuroevolution, and Reinforcement Learning (RL) algorithms, supported by various examples. All the implementations can be just-in-time compiled with Jax, facilitating efficient execution across multiple accelerators, including GPUs and TPUs. These implementations effectively demonstrate the framework's flexibility and user-friendliness, easing experimentation for research purposes. Furthermore, the library is thoroughly documented and has 93% test coverage",
    "checked": true,
    "id": "73fe99d18a11e12ea44df0d5c1a6c8fb4117498c",
    "semantic_title": "qdax: a library for quality-diversity and population-based algorithms with hardware acceleration",
    "citation_count": 8,
    "authors": [
      "Felix Chalumeau",
      "Bryan Lim",
      "Raphaël Boige",
      "Maxime Allard",
      "Luca Grillotti",
      "Manon Flageat",
      "Valentin Macé",
      "Guillaume Richard",
      "Arthur Flajolet",
      "Thomas Pierrot",
      "Antoine Cully"
    ]
  },
  "https://jmlr.org/papers/v25/23-1225.html": {
    "title": "Neural Hilbert Ladders: Multi-Layer Neural Networks in Function Space",
    "volume": "main",
    "abstract": "To characterize the function space explored by neural networks (NNs) is an important aspect of learning theory. In this work, noticing that a multi-layer NN generates implicitly a hierarchy of reproducing kernel Hilbert spaces (RKHSs) -named a neural Hilbert ladder (NHL) - we define the function space as an infinite union of RKHSs, which generalizes the existing Barron space theory of two-layer NNs. We then establish several theoretical properties of the new space. First, we prove a correspondence between functions expressed by L-layer NNs and those belonging to L-level NHLs. Second, we prove generalization guarantees for learning an NHL with a controlled complexity measure. Third, we derive a non-Markovian dynamics of random fields that governs the evolution of the NHL which is induced by the training of multi-layer NNs in an infinite-width mean-field limit. Fourth, we show examples of depth separation in NHLs under the ReLU activation function. Finally, we perform numerical experiments to illustrate the feature learning aspect of NN training through the lens of NHLs",
    "checked": true,
    "id": "f082d693e6a42b5af5bf8be02b5bbfa9efadc991",
    "semantic_title": "neural hilbert ladders: multi-layer neural networks in function space",
    "citation_count": 0,
    "authors": [
      "Zhengdao Chen"
    ]
  },
  "https://jmlr.org/papers/v25/23-1360.html": {
    "title": "More PAC-Bayes bounds: From bounded losses, to losses with general tail behaviors, to anytime validity",
    "volume": "main",
    "abstract": "In this paper, we present new high-probability PAC-Bayes bounds for different types of losses. Firstly, for losses with a bounded range, we recover a strengthened version of Catoni's bound that holds uniformly for all parameter values. This leads to new fast-rate and mixed-rate bounds that are interpretable and tighter than previous bounds in the literature. In particular, the fast-rate bound is equivalent to the Seeger--Langford bound. Secondly, for losses with more general tail behaviors, we introduce two new parameter-free bounds: a PAC-Bayes Chernoff analogue when the loss' cumulative generating function is bounded, and a bound when the loss' second moment is bounded. These two bounds are obtained using a new technique based on a discretization of the space of possible events for the \"in probability\" parameter optimization problem. This technique is both simpler and more general than previous approaches optimizing over a grid on the parameters' space. Finally, using a simple technique that is applicable to any existing bound, we extend all previous results to anytime-valid bounds",
    "checked": false,
    "id": "05fba907547020ef41b3bc8c89c46e34b1dba3fb",
    "semantic_title": "more pac-bayes bounds: from bounded losses, to losses with general tail behaviors, to anytime-validity",
    "citation_count": 4,
    "authors": [
      "Borja Rodríguez-Gálvez",
      "Ragnar Thobaben",
      "Mikael Skoglund"
    ]
  },
  "https://jmlr.org/papers/v25/20-1423.html": {
    "title": "Stable Implementation of Probabilistic ODE Solvers",
    "volume": "main",
    "abstract": "Probabilistic solvers for ordinary differential equations (ODEs) provide efficient quantification of numerical uncertainty associated with the simulation of dynamical systems. Their convergence rates have been established by a growing body of theoretical analysis. However, these algorithms suffer from numerical instability when run at high order or with small step sizes---that is, exactly in the regime in which they achieve the highest accuracy. The present work proposes and examines a solution to this problem. It involves three components: accurate initialisation, a coordinate change preconditioner that makes numerical stability concerns step-size-independent, and square-root implementation. Using all three techniques enables numerical computation of probabilistic solutions of ODEs with algorithms of order up to 11, as demonstrated on a set of challenging test problems. The resulting rapid convergence is shown to be competitive with high-order, state-of-the-art, classical methods. As a consequence, a barrier between analysing probabilistic ODE solvers and applying them to interesting machine learning problems is effectively removed",
    "checked": false,
    "id": "adcb4814244dfedc8dad61ce52eafaada75e8fd5",
    "semantic_title": "a unified sampling framework for solver searching of diffusion probabilistic models",
    "citation_count": 1,
    "authors": [
      "Nicholas Krämer",
      "Philipp Hennig"
    ]
  },
  "https://jmlr.org/papers/v25/21-0916.html": {
    "title": "Finite-time Analysis of Globally Nonstationary Multi-Armed Bandits",
    "volume": "main",
    "abstract": "We consider nonstationary multi-armed bandit problems where the model parameters of the arms change over time. We introduce the adaptive resetting bandit (ADR-bandit), a bandit algorithm class that leverages adaptive windowing techniques from literature on data streams. We first provide new guarantees on the quality of estimators resulting from adaptive windowing techniques, which are of independent interest. Furthermore, we conduct a finite-time analysis of ADR-bandit in two typical environments: an abrupt environment where changes occur instantaneously and a gradual environment where changes occur progressively. We demonstrate that ADR-bandit has nearly optimal performance when abrupt or gradual changes occur in a coordinated manner that we call global changes. We demonstrate that forced exploration is unnecessary when we assume such global changes. Unlike the existing nonstationary bandit algorithms, ADR-bandit has optimal performance in stationary environments as well as nonstationary environments with global changes. Our experiments show that the proposed algorithms outperform the existing approaches in synthetic and real-world environments",
    "checked": false,
    "id": "1a82f864bfe7e64a6598b65bb18b36a9cd6093f7",
    "semantic_title": "data-driven adaptive testing resource allocation strategies for real-time monitoring of infectious diseases",
    "citation_count": 0,
    "authors": [
      "Junpei Komiyama",
      "Edouard Fouché",
      "Junya Honda"
    ]
  },
  "https://jmlr.org/papers/v25/21-1423.html": {
    "title": "Nonasymptotic analysis of Stochastic Gradient Hamiltonian Monte Carlo under local conditions for nonconvex optimization",
    "volume": "main",
    "abstract": "We provide a nonasymptotic analysis of the convergence of the stochastic gradient Hamiltonian Monte Carlo (SGHMC) to a target measure in Wasserstein-2 distance without assuming log-concavity. Our analysis quantifies key theoretical properties of the SGHMC as a sampler under local conditions which significantly improves the findings of previous results. In particular, we prove that the Wasserstein-2 distance between the target and the law of the SGHMC is uniformly controlled by the step-size of the algorithm, therefore demonstrate that the SGHMC can provide high-precision results uniformly in the number of iterations. The analysis also allows us to obtain nonasymptotic bounds for nonconvex optimization problems under local conditions and implies that the SGHMC, when viewed as a nonconvex optimizer, converges to a global minimum with the best known rates. We apply our results to obtain nonasymptotic bounds for scalable Bayesian inference and nonasymptotic generalization bounds",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "O. Deniz Akyildiz",
      "Sotirios Sabanis"
    ]
  },
  "https://jmlr.org/papers/v25/22-0079.html": {
    "title": "Faster Rates of Differentially Private Stochastic Convex Optimization",
    "volume": "main",
    "abstract": "In this paper, we revisit the problem of Differentially Private Stochastic Convex Optimization (DP-SCO) and provide excess population risks for some special classes of functions that are faster than the previous results of general convex and strongly convex functions. In the first part of the paper, we study the case where the population risk function satisfies the Tysbakov Noise Condition (TNC) with some parameter $\\theta>1$. Specifically, we first show that under some mild assumptions on the loss functions, there is an algorithm whose output could achieve an upper bound of $\\tilde{O}((\\frac{1}{\\sqrt{n}}+\\frac{d}{n\\epsilon})^\\frac{\\theta}{\\theta-1}) $ and $\\tilde{O}((\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d\\log(1/\\delta)}}{n\\epsilon})^\\frac{\\theta}{\\theta-1})$ for $\\epsilon$-DP and $(\\epsilon, \\delta)$-DP, respectively when $\\theta\\geq 2$, where $n$ is the sample size and $d$ is the dimension of the space. Then we address the inefficiency issue, improve the upper bounds by $\\text{Poly}(\\log n)$ factors and extend to the case where $\\theta\\geq \\bar{\\theta}>1$ for some known $\\bar{\\theta}$. Next, we show that the excess population risk of population functions satisfying TNC with parameter $\\theta\\geq 2$ is always lower bounded by $\\Omega((\\frac{d}{n\\epsilon})^\\frac{\\theta}{\\theta-1}) $ and $\\Omega((\\frac{\\sqrt{d\\log(1/\\delta)}}{n\\epsilon})^\\frac{\\theta}{\\theta-1})$ for $\\epsilon$-DP and $(\\epsilon, \\delta)$-DP, respectively, which matches our upper bounds. In the second part, we focus on a special case where the population risk function is strongly convex. Unlike the previous studies, here we assume the loss function is non-negative and the optimal value of population risk is sufficiently small. With these additional assumptions, we propose a new method whose output could achieve an upper bound of $O(\\frac{d\\log(1/\\delta)}{n^2\\epsilon^2}+\\frac{1}{n^{\\tau}})$ and $O(\\frac{d^2}{n^2\\epsilon^2}+\\frac{1}{n^{\\tau}})$ for any $\\tau> 1$ in $(\\epsilon,\\delta)$-DP and $\\epsilon$-DP model respectively if the sample size $n$ is sufficiently large. These results circumvent their corresponding lower bounds in (Feldman et al., 2020) for general strongly convex functions. Finally, we conduct experiments of our new methods on real-world data. Experimental results also provide new insights into established theories",
    "checked": false,
    "id": "ae0ce013b8c17e6c93ea4c679b3023951e01ac51",
    "semantic_title": "faster differentially private convex optimization via second-order methods",
    "citation_count": 2,
    "authors": [
      "Jinyan Su",
      "Lijie Hu",
      "Di Wang"
    ]
  },
  "https://jmlr.org/papers/v25/22-1105.html": {
    "title": "The Non-Overlapping Statistical Approximation to Overlapping Group Lasso",
    "volume": "main",
    "abstract": "The group lasso penalty is widely used to introduce structured sparsity in statistical learning, characterized by its ability to eliminate predefined groups of parameters automatically. However, when the groups overlap, solving the group lasso problem can be time-consuming in high-dimensional settings due to groups' non-separability. This computational challenge has limited the applicability of the overlapping group lasso penalty in cutting-edge areas, such as gene pathway selection and graphical model estimation. This paper introduces a non-overlapping and separable penalty designed to efficiently approximate the overlapping group lasso penalty. The approximation substantially enhances the computational efficiency in optimization, especially for large-scale and high-dimensional problems. We show that the proposed penalty is the tightest separable relaxation of the overlapping group lasso norm within the family of $\\ell_{q_1}/\\ell_{q_2}$ norms. Moreover, the estimators derived from our proposed norm are statistically equivalent to those derived from the overlapping group lasso penalty in terms of estimation error, support recovery, and minimax rate under the squared loss. The effectiveness of our method is demonstrated through extensive simulation examples and a predictive task of cancer tumors",
    "checked": true,
    "id": "269c9468ef2f1bd8d6cc5084f6f9a2f73bccf66b",
    "semantic_title": "the non-overlapping statistical approximation to overlapping group lasso",
    "citation_count": 0,
    "authors": [
      "Mingyu Qi",
      "Tianxi  Li"
    ]
  },
  "https://jmlr.org/papers/v25/22-1324.html": {
    "title": "Differentially Private Data Release for Mixed-type Data via Latent Factor Models",
    "volume": "main",
    "abstract": "Differential privacy is a particular data privacy-preserving technology which enables synthetic data or statistical analysis results to be released with a minimum disclosure of private information from individual records. The tradeoff between privacy-preserving and utility guarantee is always a challenge for differential privacy technology, especially for synthetic data generation. In this paper, we propose a differentially private data synthesis algorithm for mixed-type data with correlation based on latent factor models. The proposed method can add a relatively small amount of noise to synthetic data under a given level of privacy protection while capturing correlation information. Moreover, the proposed algorithm can generate synthetic data preserving the same data type as mixed-type original data, which greatly improves the utility of synthetic data. The key idea of our method is to perturb the factor matrix and factor loading matrix to construct a synthetic data generation model, and to utilize link functions with privacy protection to ensure consistency of synthetic data type with original data. The proposed method can generate privacy-preserving synthetic data at low computation cost even when the original data is high-dimensional. In theory, we establish differentially private properties of the proposed method. Our numerical studies also demonstrate superb performance of the proposed method on the utility guarantee of the statistical analysis based on privacy-preserved synthetic data",
    "checked": false,
    "id": "4503ec244887731fe067770f85a81b678e2bf087",
    "semantic_title": "differentially private counterfactuals via functional mechanism",
    "citation_count": 2,
    "authors": [
      "Yanqing Zhang",
      "Qi Xu",
      "Niansheng Tang",
      "Annie Qu"
    ]
  },
  "https://jmlr.org/papers/v25/23-0182.html": {
    "title": "Functions with average smoothness: structure, algorithms, and learning",
    "volume": "main",
    "abstract": "We initiate a program of average smoothness analysis for efficiently learning real-valued functions on metric spaces. Rather than using the Lipschitz constant as the regularizer, we define a local slope at each point and gauge the function complexity as the average of these values. Since the mean can be dramatically smaller than the maximum, this complexity measure can yield considerably sharper generalization bounds --- assuming that these admit a refinement where the Lipschitz constant is replaced by our average of local slopes. Our first major contribution is to obtain just such distribution-sensitive bounds. This required overcoming a number of technical challenges, perhaps the most formidable of which was bounding the empirical covering numbers, which can be much worse-behaved than the ambient ones. Our combinatorial results are accompanied by efficient algorithms for smoothing the labels of the random sample, as well as guarantees that the extension from the sample to the whole space will continue to be, with high probability, smooth on average. Along the way we discover a surprisingly rich combinatorial and analytic structure in the function class we define",
    "checked": false,
    "id": "b07e2bc6a69e359e452ba5a714df8a6d6df3d53e",
    "semantic_title": "digital image processing and machine learning",
    "citation_count": 0,
    "authors": [
      "Yair Ashlagi",
      "Lee-Ad Gottlieb",
      "Aryeh Kontorovich"
    ]
  },
  "https://jmlr.org/papers/v25/23-0253.html": {
    "title": "Predictive Inference with Weak Supervision",
    "volume": "main",
    "abstract": "The expense of acquiring labels in large-scale statistical machine learning makes partially and weakly-labeled data attractive, though it is not always apparent how to leverage such data for model fitting or validation. We present a methodology to bridge the gap between partial supervision and validation, developing a conformal prediction framework to provide valid predictive confidence sets---sets that cover a true label with a prescribed probability, independent of the underlying distribution---using weakly labeled data. To do so, we introduce a (necessary) new notion of coverage and predictive validity, then develop several application scenarios, providing efficient algorithms for classification and several large-scale structured prediction problems. We corroborate the hypothesis that the new coverage definition allows for tighter and more informative (but valid) confidence sets through several experiments",
    "checked": true,
    "id": "bbae123411ce764aedd7cc2a5a9f76dc674f4528",
    "semantic_title": "predictive inference with weak supervision",
    "citation_count": 10,
    "authors": [
      "Maxime Cauchois",
      "Suyash Gupta",
      "Alnur Ali",
      "John C. Duchi"
    ]
  },
  "https://jmlr.org/papers/v25/23-0461.html": {
    "title": "Generative Adversarial Ranking Nets",
    "volume": "main",
    "abstract": "We propose a new adversarial training framework -- generative adversarial ranking networks (GARNet) to learn from user preferences among a list of samples so as to generate data meeting user-specific criteria. Verbosely, GARNet consists of two modules: a ranker and a generator. The generator fools the ranker to raise generated samples to the top; while the ranker learns to rank generated samples at the bottom. Meanwhile, the ranker learns to rank samples regarding the interested property by training with preferences collected on real samples. The adversarial ranking game between the ranker and the generator enables an alignment between the generated data distribution and the user-preferred data distribution with theoretical guarantees and empirical verification. Specifically, we first prove that when training with full preferences on a discrete property, the learned distribution of GARNet rigorously coincides with the distribution specified by the given score vector based on user preferences. The theoretical results are then extended to partial preferences on a discrete property and further generalized to preferences on a continuous property. Meanwhile, numerous experiments show that GARNet can retrieve the distribution of user-desired data based on full/partial preferences in terms of various interested properties (i.e., discrete/continuous property, single/multiple properties). Code is available at https://github.com/EvaFlower/GARNet",
    "checked": false,
    "id": "d350c1e036207b57de41ee9db7ce4900f6def96a",
    "semantic_title": "generative adversarial nets for social scientists",
    "citation_count": 0,
    "authors": [
      "Yinghua Yao",
      "Yuangang Pan",
      "Jing Li",
      "Ivor W. Tsang",
      "Xin Yao"
    ]
  },
  "https://jmlr.org/papers/v25/23-0537.html": {
    "title": "OpenBox: A Python Toolkit for Generalized Black-box Optimization",
    "volume": "MLOSS",
    "abstract": "Black-box optimization (BBO) has a broad range of applications, including automatic machine learning, experimental design, and database knob tuning. However, users still face challenges when applying BBO methods to their problems at hand with existing software packages in terms of applicability, performance, and efficiency. This paper presents OpenBox, an open-source BBO toolkit with improved usability. It implements user-friendly interfaces and visualization for users to define and manage their tasks. The modular design behind OpenBox facilitates its flexible deployment in existing systems. Experimental results demonstrate the effectiveness and efficiency of OpenBox over existing systems. The source code of OpenBox is available at https://github.com/PKU-DAIR/open-box",
    "checked": true,
    "id": "2bce7a26af564fc3c95a1dfe48fd796848f93035",
    "semantic_title": "openbox: a python toolkit for generalized black-box optimization",
    "citation_count": 2,
    "authors": [
      "Huaijun Jiang",
      "Yu Shen",
      "Yang Li",
      "Beicheng Xu",
      "Sixian Du",
      "Wentao Zhang",
      "Ce Zhang",
      "Bin Cui"
    ]
  },
  "https://jmlr.org/papers/v25/23-0791.html": {
    "title": "Linear Distance Metric Learning with Noisy Labels",
    "volume": "main",
    "abstract": "In linear distance metric learning, we are given data in one Euclidean metric space and the goal is to find an appropriate linear map to another Euclidean metric space which respects certain distance conditions as much as possible. In this paper, we formalize a simple and elegant method which reduces to a general continuous convex loss optimization problem, and for different noise models we derive the corresponding loss functions. We show that even if the data is noisy, the ground truth linear metric can be learned with any precision provided access to enough samples, and we provide a corresponding sample complexity bound. Moreover, we present an effective way to truncate the learned model to a low-rank model that can provably maintain the accuracy in the loss function and in parameters -- the first such results of this type. Several experimental observations on synthetic and real data sets support and inform our theoretical results",
    "checked": true,
    "id": "21721381fef9bb66a16e29748b660aca6cd455fd",
    "semantic_title": "linear distance metric learning with noisy labels",
    "citation_count": 0,
    "authors": [
      "Meysam Alishahi",
      "Anna Little",
      "Jeff M. Phillips"
    ]
  },
  "https://jmlr.org/papers/v25/23-1159.html": {
    "title": "An Algorithm with Optimal Dimension-Dependence for Zero-Order Nonsmooth Nonconvex Stochastic Optimization",
    "volume": "main",
    "abstract": "We study the complexity of producing $(\\delta,\\epsilon)$-stationary points of Lipschitz objectives which are possibly neither smooth nor convex, using only noisy function evaluations. Recent works proposed several stochastic zero-order algorithms that solve this task, all of which suffer from a dimension-dependence of $\\Omega(d^{3/2})$ where $d$ is the dimension of the problem, which was conjectured to be optimal. We refute this conjecture by providing a faster algorithm that has complexity $O(d\\delta^{-1}\\epsilon^{-3})$, which is optimal (up to numerical constants) with respect to $d$ and also optimal with respect to the accuracy parameters $\\delta,\\epsilon$, thus solving an open question due to Lin et al. (2022). Moreover, the convergence rate achieved by our algorithm is also optimal for smooth objectives, proving that in the nonconvex stochastic zero-order setting, nonsmooth optimization is as easy as smooth optimization. We provide algorithms that achieve the aforementioned convergence rate in expectation as well as with high probability. Our analysis is based on a simple yet powerful lemma regarding the Goldstein-subdifferential set, which allows utilizing recent advancements in first-order nonsmooth nonconvex optimization",
    "checked": true,
    "id": "ad95645af92ed24f4970dfff13927fb8282ce35a",
    "semantic_title": "an algorithm with optimal dimension-dependence for zero-order nonsmooth nonconvex stochastic optimization",
    "citation_count": 2,
    "authors": [
      "Guy Kornowski",
      "Ohad Shamir"
    ]
  },
  "https://jmlr.org/papers/v25/21-0495.html": {
    "title": "Sum-of-norms clustering does not separate nearby balls",
    "volume": "main",
    "abstract": "Sum-of-norms clustering is a popular convexification of $K$-means clustering. We show that, if the dataset is made of a large number of independent random variables distributed according to the uniform measure on the union of two disjoint balls of unit radius, and if the balls are sufficiently close to one another, then sum-of-norms clustering will typically fail to recover the decomposition of the dataset into two clusters. As the dimension tends to infinity, this happens even when the distance between the centers of the two balls is taken to be as large as $2\\sqrt{2}$. In order to show this, we introduce and analyze a continuous version of sum-of-norms clustering, where the dataset is replaced by a general measure. In particular, we state and prove a local-global characterization of the clustering that seems to be new even in the case of discrete datapoints",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Dunlap",
      "Jean-Christophe Mourrat"
    ]
  },
  "https://jmlr.org/papers/v25/21-1367.html": {
    "title": "Spectral learning of multivariate extremes",
    "volume": "main",
    "abstract": "We propose a spectral clustering algorithm for analyzing the dependence structure of multivariate extremes. More specifically, we focus on the asymptotic dependence of multivariate extremes characterized by the angular or spectral measure in extreme value theory. Our work studies the theoretical performance of spectral clustering based on a random $k$-nearest neighbor graph constructed from an extremal sample, i.e., the angular part of random vectors for which the radius exceeds a large threshold. In particular, we derive the asymptotic distribution of extremes arising from a linear factor model and prove that, under certain conditions, spectral clustering can consistently identify the clusters of extremes arising in this model. Leveraging this result we propose a simple consistent estimation strategy for learning the angular measure. Our theoretical findings are complemented with numerical experiments illustrating the finite sample performance of our methods",
    "checked": false,
    "id": "d22fdab6f7158329d347c04a8811d96f4ffe6e58",
    "semantic_title": "visible-nir hyperspectral classification of grass based on multivariate smooth mapping and extreme active learning approach",
    "citation_count": 0,
    "authors": [
      "Marco Avella Medina",
      "Richard A Davis",
      "Gennady Samorodnitsky"
    ]
  },
  "https://jmlr.org/papers/v25/22-0049.html": {
    "title": "Classification with Deep Neural Networks and Logistic Loss",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) trained with the logistic loss (also known as the cross entropy loss) have made impressive advancements in various binary classification tasks. Despite the considerable success in practice, generalization analysis for binary classification with deep neural networks and the logistic loss remains scarce. The unboundedness of the target function for the logistic loss in binary classification is the main obstacle to deriving satisfactory generalization bounds. In this paper, we aim to fill this gap by developing a novel theoretical analysis and using it to establish tight generalization bounds for training fully connected ReLU DNNs with logistic loss in binary classification. Our generalization analysis is based on an elegant oracle-type inequality which enables us to deal with the boundedness restriction of the target function. Using this oracle-type inequality, we establish generalization bounds for fully connected ReLU DNN classifiers $\\hat{f}^{\\text{FNN}}_n$ trained by empirical logistic risk minimization with respect to i.i.d. samples of size $n$, which lead to sharp rates of convergence as $n\\to\\infty$. In particular, we obtain optimal convergence rates for $\\hat{f}^{\\text{FNN}}_n$ (up to some logarithmic factor) only requiring the Hölder smoothness of the conditional class probability $\\eta$ of data. Moreover, we consider a compositional assumption that requires $\\eta$ to be the composition of several vector-valued multivariate functions of which each component function is either a maximum value function or a Hölder smooth function only depending on a small number of its input variables. Under this assumption, we can even derive optimal convergence rates for $\\hat{f}^{\\text{FNN}}_n$ (up to some logarithmic factor) which are independent of the input dimension of data. This result explains why in practice DNN classifiers can overcome the curse of dimensionality and perform well in high-dimensional classification problems. Furthermore, we establish dimension-free rates of convergence under other circumstances such as when the decision boundary is piecewise smooth and the input data are bounded away from it. Besides the novel oracle-type inequality, the sharp convergence rates presented in our paper also owe to a tight error bound for approximating the natural logarithm function near zero (where it is unbounded) by ReLU DNNs. In addition, we justify our claims for the optimality of rates by proving corresponding minimax lower bounds. All these results are new in the literature and will deepen our theoretical understanding of classification with deep neural networks",
    "checked": true,
    "id": "d3d4b42fbed8f734cc31cbc3a6ac2f8a8ef006a1",
    "semantic_title": "classification with deep neural networks and logistic loss",
    "citation_count": 5,
    "authors": [
      "Zihan Zhang",
      "Lei Shi",
      "Ding-Xuan Zhou"
    ]
  },
  "https://jmlr.org/papers/v25/22-0395.html": {
    "title": "Random Subgraph Detection Using Queries",
    "volume": "main",
    "abstract": "The planted densest subgraph detection problem refers to the task of testing whether in a given (random) graph there is a subgraph that is unusually dense. Specifically, we observe an undirected and unweighted graph on $n$ vertices. Under the null hypothesis, the graph is a realization of an Erdös-R{\\'e}nyi graph with edge probability (or, density) $q$. Under the alternative, there is a subgraph on $k$ vertices with edge probability $p>q$. The statistical as well as the computational barriers of this problem are well-understood for a wide range of the edge parameters $p$ and $q$. In this paper, we consider a natural variant of the above problem, where one can only observe a relatively small part of the graph using adaptive edge queries. For this model, we determine the number of queries necessary and sufficient (accompanied with a quasi-polynomial optimal algorithm) for detecting the presence of the planted subgraph. We also propose a polynomial-time algorithm which is able to detect the planted subgraph, albeit with more queries compared to the above lower bound. We conjecture that in the leftover regime, no polynomial-time algorithms exist. Our results resolve two open questions posed in the past literature",
    "checked": false,
    "id": "32f48d00d793d0336f3c4272346d4e599cbdc60b",
    "semantic_title": "finding cliques and dense subgraphs using edge queries",
    "citation_count": 1,
    "authors": [
      "Wasim Huleihel",
      "Arya Mazumdar",
      "Soumyabrata Pal"
    ]
  },
  "https://jmlr.org/papers/v25/22-1127.html": {
    "title": "Margin-Based Active Learning of Classifiers",
    "volume": "main",
    "abstract": "We study active learning of multiclass classifiers, focusing on the realizable transductive setting. The input is a finite subset $X$ of some metric space, and the concept to be learned is a partition $\\mathcal{C}$ of $X$ into $k$ classes. The goal is to learn $\\mathcal{C}$ by querying the labels of as few elements of $X$ as possible. This is a useful subroutine in pool-based active learning, and is motivated by applications where labels are expensive to obtain. Our main result is that, in very different settings, there exist interesting notions of margin that yield efficient active learning algorithms. First, we consider the case $X \\subset \\mathbb{R}^m$, assuming that each class has an unknown \"personalized\" margin separating it from the rest. Second, we consider the case where $X$ is a finite metric space, and the classes are convex with margin according to the geodesic distances in the thresholded connectivity graph. In both cases, we give algorithms that learn $\\mathcal{C}$ exactly, in polynomial time, using $\\mathcal{O}(\\log n)$ label queries, where $\\mathcal{O}(\\cdot)$ hides a near-optimal dependence on the dimension of the metric spaces. Our results actually hold for or can be adapted to more general settings, such as pseudometric and semimetric spaces",
    "checked": false,
    "id": "1a942158c872d155831f6d27e451f1ccf6f21f97",
    "semantic_title": "small-text: active learning for text classification in python",
    "citation_count": 13,
    "authors": [
      "Marco Bressan",
      "Nicolò Cesa-Bianchi",
      "Silvio Lattanzi",
      "Andrea Paudice"
    ]
  },
  "https://jmlr.org/papers/v25/23-0072.html": {
    "title": "Learning Optimal Dynamic Treatment Regimens Subject to Stagewise Risk Controls",
    "volume": "main",
    "abstract": "Dynamic treatment regimens (DTRs) aim at tailoring individualized sequential treatment rules that maximize cumulative beneficial outcomes by accommodating patients' heterogeneity in decision-making. For many chronic diseases including type 2 diabetes mellitus (T2D), treatments are usually multifaceted in the sense that aggressive treatments with a higher expected reward are also likely to elevate the risk of acute adverse events. In this paper, we propose a new weighted learning framework, namely benefit-risk dynamic treatment regimens (BR-DTRs), to address the benefit-risk trade-off. The new framework relies on a backward learning procedure by restricting the induced risk of the treatment rule to be no larger than a pre-specified risk constraint at each treatment stage. Computationally, the estimated treatment rule solves a weighted support vector machine problem with a modified smooth constraint. Theoretically, we show that the proposed DTRs are Fisher consistent, and we further obtain the convergence rates for both the value and risk functions. Finally, the performance of the proposed method is demonstrated via extensive simulation studies and application to a real study for T2D patients",
    "checked": true,
    "id": "9ea71228e292a82468347168ae23e02d69a40754",
    "semantic_title": "learning optimal dynamic treatment regimens subject to stagewise risk controls",
    "citation_count": 0,
    "authors": [
      "Mochuan Liu",
      "Yuanjia Wang",
      "Haoda Fu",
      "Donglin Zeng"
    ]
  },
  "https://jmlr.org/papers/v25/23-0234.html": {
    "title": "Regimes of No Gain in Multi-class Active Learning",
    "volume": "main",
    "abstract": "We consider nonparametric classification with smooth regression functions, where it is well known that notions of margin in $\\mathbb{P}(Y=y|X=x)$ determine fast or slow rates in both active and passive learning. Here we elucidate a striking distinction---most relevant in multi-class settings---between active and passive learning. Namely, we show that some seemingly benign nuances in notions of margin---involving the uniqueness of the Bayes classes, which have no apparent effect on rates in passive learning---determine whether or not any active learner can outperform passive learning rates. While a shorter conference version of this work already alluded to these nuances, it focused on the binary case and thus failed to be conclusive as to the source of difficulty in the multi-class setting: we show here that it suffices that the Bayes classifier fails to be unique, as opposed to needing all classes to be Bayes optimal, for active learning to yield no gain over passive learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gan Yuan",
      "Yunfan Zhao",
      "Samory Kpotufe"
    ]
  },
  "https://jmlr.org/papers/v25/23-0322.html": {
    "title": "Fairness guarantees in multi-class classification with demographic parity",
    "volume": "main",
    "abstract": "Algorithmic Fairness is an established area of machine learning, willing to reduce the influence of hidden bias in the data. Yet, despite its wide range of applications, very few works consider the multi-class classification setting from the fairness perspective. We focus on this question and extend the definition of approximate fairness in the case of Demographic Parity to multi-class classification. We specify the corresponding expressions of the optimal fair classifiers in the attribute-aware case and both for binary and multi-categorical sensitive attributes. This suggests a plug-in data-driven procedure, for which we establish theoretical guarantees. The enhanced estimator is proved to mimic the behavior of the optimal rule both in terms of fairness and risk. Notably, fairness guarantees are distribution-free. The approach is evaluated on both synthetic and real datasets and reveals very effective in decision making with a preset level of unfairness. In addition, our method is competitive (if not better) with the state-of-the-art in binary and multi-class tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christophe Denis",
      "Romuald Elie",
      "Mohamed Hebiri",
      "François Hu"
    ]
  },
  "https://jmlr.org/papers/v25/23-0536.html": {
    "title": "Bagging Provides Assumption-free Stability",
    "volume": "main",
    "abstract": "Bagging is an important technique for stabilizing machine learning models. In this paper, we derive a finite-sample guarantee on the stability of bagging for any model. Our result places no assumptions on the distribution of the data, on the properties of the base algorithm, or on the dimensionality of the covariates. Our guarantee applies to many variants of bagging and is optimal up to a constant. Empirical results validate our findings, showing that bagging successfully stabilizes even highly unstable base algorithms",
    "checked": true,
    "id": "ec8f5322eee56653565fbbb2b778fa69ee4153c6",
    "semantic_title": "bagging provides assumption-free stability",
    "citation_count": 4,
    "authors": [
      "Jake A. Soloff",
      "Rina Foygel Barber",
      "Rebecca Willett"
    ]
  },
  "https://jmlr.org/papers/v25/23-0615.html": {
    "title": "Representation Learning via Manifold Flattening and Reconstruction",
    "volume": "main",
    "abstract": "A common assumption for real-world, learnable data is its possession of some low-dimensional structure, and one way to formalize this structure is through the manifold hypothesis: that learnable data lies near some low-dimensional manifold. Deep learning architectures often have a compressive autoencoder component, where data is mapped to a lower-dimensional latent space, but often many architecture design choices are done by hand, since such models do not inherently exploit mathematical structure of the data. To utilize this geometric data structure, we propose an iterative process in the style of a geometric flow for explicitly constructing a pair of neural networks layer-wise that linearize and reconstruct an embedded submanifold, from finite samples of this manifold. Our such-generated neural networks, called Flattening Networks (FlatNet), are theoretically interpretable, computationally feasible at scale, and generalize well to test data, a balance not typically found in manifold-based learning methods. We present empirical results and comparisons to other models on synthetic high-dimensional manifold data and 2D image data. Our code is publicly available",
    "checked": true,
    "id": "6088b7ee7d2dc642760ea6b6fb86fa4383712f6c",
    "semantic_title": "representation learning via manifold flattening and reconstruction",
    "citation_count": 4,
    "authors": [
      "Michael Psenka",
      "Druv Pai",
      "Vishal Raman",
      "Shankar Sastry",
      "Yi Ma"
    ]
  },
  "https://jmlr.org/papers/v25/23-1066.html": {
    "title": "Granger Causal Inference in Multivariate Hawkes Processes by Minimum Message Length",
    "volume": "main",
    "abstract": "Multivariate Hawkes processes (MHPs) are versatile probabilistic tools used to model various real-life phenomena: earthquakes, operations on stock markets, neuronal activity, virus propagation and many others. In this paper, we focus on MHPs with exponential decay kernels and estimate connectivity graphs, which represent the Granger causal relations between their components. We approach this inference problem by proposing an optimization criterion and model selection algorithm based on the minimum message length (MML) principle. MML compares Granger causal models using the Occam's razor principle in the following way: even when models have a comparable goodness-of-fit to the observed data, the one generating the most concise explanation of the data is preferred. While most of the state-of-art methods using lasso-type penalization tend to overfitting in scenarios with short time horizons, the proposed MML-based method achieves high F1 scores in these settings. We conduct a numerical study comparing the proposed algorithm to other related classical and state-of-art methods, where we achieve the highest F1 scores in specific sparse graph settings. We illustrate the proposed method also on G7 sovereign bond data and obtain causal connections, which are in agreement with the expert knowledge available in the literature",
    "checked": true,
    "id": "7b00083658a51b2d52e641111c86027bb79310f1",
    "semantic_title": "granger causal inference in multivariate hawkes processes by minimum message length",
    "citation_count": 0,
    "authors": [
      "Katerina Hlaváčková-Schindler",
      "Anna Melnykova",
      "Irene Tubikanec"
    ]
  },
  "https://jmlr.org/papers/v25/23-1185.html": {
    "title": "Topological Node2vec: Enhanced Graph Embedding via Persistent Homology",
    "volume": "main",
    "abstract": "Node2vec is a graph embedding method that learns a vector representation for each node of a weighted graph while seeking to preserve relative proximity and global structure. Numerical experiments suggest Node2vec struggles to recreate the topology of the input graph. To resolve this we introduce a topological loss term to be added to the training loss of Node2vec which tries to align the persistence diagram (PD) of the resulting embedding as closely as possible to that of the input graph. Following results in computational optimal transport, we carefully adapt entropic regularization to PD metrics, allowing us to measure the discrepancy between PDs in a differentiable way. Our modified loss function can then be minimized through gradient descent to reconstruct both the geometry and the topology of the input graph. We showcase the benefits of this approach using demonstrative synthetic examples",
    "checked": true,
    "id": "bd5e7eddd1d447b08603fb7fe6bf063bc2bc00fb",
    "semantic_title": "topological node2vec: enhanced graph embedding via persistent homology",
    "citation_count": 1,
    "authors": [
      "Yasuaki Hiraoka",
      "Yusuke Imoto",
      "Théo Lacombe",
      "Killian Meehan",
      "Toshiaki Yachimura"
    ]
  }
}