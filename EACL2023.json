{
  "https://aclanthology.org/2023.eacl-main.1": {
    "title": "PiC: A Phrase-in-Context Dataset for Phrase Understanding and Semantic Search",
    "abstract": "While contextualized word embeddings have been a de-facto standard, learning contextualized phrase embeddings is less explored and being hindered by the lack of a human-annotated benchmark that tests machine understanding of phrase semantics given a context sentence or paragraph (instead of phrases alone). To fill this gap, we propose PiC—a dataset of ∼28K of noun phrases accompanied by their contextual Wikipedia pages and a suite of three tasks for training and evaluating phrase embeddings. Training on PiC improves ranking-models’ accuracy and remarkably pushes span selection (SS) models (i.e., predicting the start and end index of the target phrase) near human accuracy, which is 95% Exact Match (EM) on semantic search given a query phrase and a passage. Interestingly, we find evidence that such impressive performance is because the SS models learn to better capture the common meaning of a phrase regardless of its actual context. SotA models perform poorly in distinguishing two senses of the same phrase in two contexts (∼60% EM) and in estimating the similarity between two different phrases in the same context (∼70% EM)",
    "volume": "main",
    "checked": true,
    "id": "307c256abf29a0d96802b9cc8f48458ea58ca1bb",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.2": {
    "title": "Enhancing Dialogue Summarization with Topic-Aware Global- and Local- Level Centrality",
    "abstract": "Dialogue summarization aims to condense a given dialogue into a simple and focused summary text. Typically, both the roles’ viewpoints and conversational topics change in the dialogue stream. Thus how to effectively handle the shifting topics and select the most salient utterance becomes one of the major challenges of this task. In this paper, we propose a novel topic-aware Global-Local Centrality (GLC) model to help select the salient context from all sub-topics. The centralities are constructed in both global level and local level. The global one aims to identify vital sub-topics in the dialogue and the local one aims to select the most important context in each sub-topic. Specifically, the GLC collects sub-topic based on the utterance representations. And each utterance is aligned with one sub-topic. Based on the sub-topics, the GLC calculates global- and local-level centralities. Finally, we combine the two to guide the model to capture both salient context and sub-topics when generating summaries. Experimental results show that our model outperforms strong baselines on three public dialogue summarization datasets: CSDS, MC, and SAMSUM. Further analysis demonstrates that our GLC can exactly identify vital contents from sub-topics.~{footnote{Once our paper is accepted, we will release our code here.}",
    "volume": "main",
    "checked": true,
    "id": "2da8b8fa46ab233ac69d1e936766fedfe0c3e3f2",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.3": {
    "title": "Exploiting Summarization Data to Help Text Simplification",
    "abstract": "One of the major problems with text simplification is the lack of high-quality data. The sources of simplification datasets are limited to Wikipedia and Newsela, restricting further development of this field. In this paper, we analyzed the similarity between text summarization and text simplification and exploited summarization data to help simplify. First, we proposed an alignment algorithm to extract sentence pairs from summarization datasets. Then, we designed four attributes to characterize the degree of simplification and proposed a method to filter suitable pairs. We named these pairs Sum4Simp (S4S). Next, we conducted human evaluations to show that S4S is high-quality and compared it with a real simplification dataset. Finally, we conducted experiments to illustrate that the S4S can improve the performance of several mainstream simplification models, especially in low-resource scenarios",
    "volume": "main",
    "checked": true,
    "id": "2e235b642f0217d3dc9886494934bc3947d354b2",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.4": {
    "title": "Shironaam: Bengali News Headline Generation using Auxiliary Information",
    "abstract": "Automatic headline generation systems have the potential to assist editors in finding interesting headlines to attract visitors or readers. However, the performance of headline generation systems remains challenging due to the unavailability of sufficient parallel data for low-resource languages like Bengali and the lack of ideal approaches to develop a system for headline generation using pre-trained language models, especially for long news articles. To address these challenges, we present Shironaam, a large-scale dataset in Bengali containing over 240K news article-headline pairings with auxiliary data such as image captions, topic words, and category information. Unlike other headline generation models, this paper uses this auxiliary information to better model this task. Furthermore, we utilize the contextualized language models to design encoder-decoder model for Bengali news headline generation and follow a simple yet cost-effective coarse-to-fine approach using topic-words to retrieve important sentences considering the fixed length requirement of the pre-trained language models. Finally, we conduct extensive experiments on our dataset containing news articles of 13 different categories to demonstrate the effectiveness of incorporating auxiliary information and evaluate our system on a wide range of metrics. The experimental results demonstrate that our methods bring significant improvements (i.e., 3 to 10 percentage points across all evaluation metrics) over the baselines. Also to illustrate the utility and robustness, we report experimental results in few-shot and non-few-shot settings",
    "volume": "main",
    "checked": true,
    "id": "44246a150f3402f43efcafbed90d7646bf6dd4f7",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.5": {
    "title": "PCC: Paraphrasing with Bottom-k Sampling and Cyclic Learning for Curriculum Data Augmentation",
    "abstract": "Curriculum Data Augmentation (CDA) improves neural models by presenting synthetic data with increasing difficulties from easy to hard. However, traditional CDA simply treats the ratio of word perturbation as the difficulty measure and goes through the curriculums only once. This paper presents {textbf{PCC}: {textbf{P}araphrasing with Bottom-k Sampling and {textbf{C}yclic Learning for {textbf{C}urriculum Data Augmentation, a novel CDA framework via paraphrasing, which exploits the textual paraphrase similarity as the curriculum difficulty measure. We propose a curriculum-aware paraphrase generation module composed of three units: a paraphrase candidate generator with bottom-k sampling, a filtering mechanism and a difficulty measure. We also propose a cyclic learning strategy that passes through the curriculums multiple times. The bottom-k sampling is proposed to generate super-hard instances for the later curriculums. Experimental results on few-shot text classification as well as dialogue generation indicate that PCC surpasses competitive baselines. Human evaluation and extensive case studies indicate that bottom-k sampling effectively generates super-hard instances, and PCC significantly improves the baseline dialogue agent.{footnote{Code will be released upon publication.}",
    "volume": "main",
    "checked": true,
    "id": "ba9146a5c2c68e3d4bc9a803223821ddde967f2a",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.6": {
    "title": "A Two-Sided Discussion of Preregistration of NLP Research",
    "abstract": "Van Miltenburg et al. (2021) suggest NLP research should adopt preregistration to prevent fishing expeditions and to promote publication of negative results. At face value, this is a very reasonable suggestion, seemingly solving many methodological problems with NLP research. We discuss pros and cons - some old, some new: a) Preregistration is challenged by the practice of retrieving hypotheses after the results are known; b) preregistration may bias NLP toward confirmatory research; c) preregistration must allow for reclassification of research as exploratory; d) preregistration may increase publication bias; e) preregistration may increase flag-planting; f) preregistration may increase p-hacking; and finally, g) preregistration may make us less risk tolerant. We cast our discussion as a dialogue, presenting both sides of the debate",
    "volume": "main",
    "checked": true,
    "id": "58019a6c71c48b1d591dbfe1061538b485258c05",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.7": {
    "title": "WinoDict: Probing language models for in-context word acquisition",
    "abstract": "We introduce a new in-context learning paradigm to measure Large Language Models’ (LLMs) ability to learn novel words during inference. In particular, we rewrite Winograd-style co-reference resolution problems by replacing the key concept word with a synthetic but plausible word that the model must understand to complete the task. Solving this task requires the model to make use of the dictionary definition of the new word given in the prompt. This benchmark addresses word acquisition, one important aspect of the diachronic degradation known to afflict LLMs. As LLMs are frozen in time at the moment they are trained, they are normally unable to reflect the way language changes over time. We show that the accuracy of LLMs compared to the original Winograd tasks decreases radically in our benchmark, thus identifying a limitation of current models and providing a benchmark to measure future improvements in LLMs ability to do in-context learning",
    "volume": "main",
    "checked": true,
    "id": "775c439186b037c09cd9f95b9daf81d23ca21b54",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.8": {
    "title": "Sentiment as an Ordinal Latent Variable",
    "abstract": "Sentiment analysis has become a central tool in various disciplines outside of natural language processing. In particular in applied and domain-specific settings with strong requirements for interpretable methods, dictionary-based approaches are still a popular choice. However, existing dictionaries are often limited in coverage, static once annotation is completed and sentiment scales differ widely; some are discrete others continuous. We propose a Bayesian generative model that learns a composite sentiment dictionary as an interpolation between six existing dictionaries with different scales. We argue that sentiment is a latent concept with intrinsically ranking-based characteristics — the word “excellent” may be ranked more positive than “great” and “okay”, but it is hard to express how much more exactly. This prompts us to enforce an ordinal scale of ordered discrete sentiment values in our dictionary. We achieve this through an ordering transformation in the priors of our model. We evaluate the model intrinsically by imputing missing values in existing dictionaries. Moreover, we conduct extrinsic evaluations through sentiment classification tasks. Finally, we present two extension: first, we present a method to augment dictionary-based approaches with word embeddings to construct sentiment scales along new semantic axes. Second, we demonstrate a Latent Dirichlet Allocation-inspired variant of our model that learns document topics that are ordered by sentiment",
    "volume": "main",
    "checked": true,
    "id": "d484eb847123921a8111bfbb270273d945fc7fcc",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.9": {
    "title": "Nationality Bias in Text Generation",
    "abstract": "Little attention is placed on analyzing nationality bias in language models, especially when nationality is highly used as a factor in increasing the performance of social NLP models. This paper examines how a text generation model, GPT-2, accentuates pre-existing societal biases about country-based demonyms. We generate stories using GPT-2 for various nationalities and use sensitivity analysis to explore how the number of internet users and the country’s economic status impacts the sentiment of the stories. To reduce the propagation of biases through large language models (LLM), we explore the debiasing method of adversarial triggering. Our results show that GPT-2 demonstrates significant bias against countries with lower internet users, and adversarial triggering effectively reduces the same",
    "volume": "main",
    "checked": true,
    "id": "75ddc4fb91332f95222d74449d96b9f7c8f976c7",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.10": {
    "title": "Investigating data partitioning strategies for crosslinguistic low-resource ASR evaluation",
    "abstract": "Many automatic speech recognition (ASR) data sets include a single pre-defined test set consisting of one or more speakers whose speech never appears in the training set. This “hold-speaker(s)-out” data partitioning strategy, however, may not be ideal for data sets in which the number of speakers is very small.This study investigates ten different data split methods for five languages with minimal ASR training resources. We find that (1) model performance varies greatly depending on which speaker is selected for testing; (2) the average word error rate (WER) across all held-out speakers is comparable not only to the average WER over multiple random splits but also to any given individual random split; (3) WER is also generally comparable when the data is split heuristically or adversarially; (4) utterance duration and intensity are comparatively more predictive factors of variability regardless of the data split.These results suggest that the widely used hold-speakers-out approach to ASR data partitioning can yield results that do not reflect model performance on unseen data or speakers. Random splits can yield more reliable and generalizable estimates when facing data sparsity",
    "volume": "main",
    "checked": true,
    "id": "4026f15ad08ece55b16c66b22a80660526ab6fe1",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.11": {
    "title": "Shortcomings of Question Answering Based Factuality Frameworks for Error Localization",
    "abstract": "Despite recent progress in abstractive summarization, models often generate summaries with factual errors. Numerous approaches to detect these errors have been proposed, the most popular of which are question answering (QA)-based factuality metrics. These have been shown to work well at predicting summary-level factuality and have potential to localize errors within summaries, but this latter capability has not been systematically evaluated in past research. In this paper, we conduct the first such analysis and find that, contrary to our expectations, QA-based frameworks fail to correctly identify error spans in generated summaries and are outperformed by trivial exact match baselines. Our analysis reveals a major reason for such poor localization: questions generated by the QG module often inherit errors from non-factual summaries which are then propagated further into downstream modules. Moreover, even human-in-the-loop question generation cannot easily offset these problems. Our experiments conclusively show that there exist fundamental issues with localization using the QA framework which cannot be fixed solely by stronger QA and QG models",
    "volume": "main",
    "checked": true,
    "id": "f91f38829b84125db1df2cf490e87fb2b94f9c44",
    "citation_count": 2
  },
  "https://aclanthology.org/2023.eacl-main.12": {
    "title": "Socratic Question Generation: A Novel Dataset, Models, and Evaluation",
    "abstract": "Socratic questioning is a form of reflective inquiry often employed in education to encourage critical thinking in students, and to elicit awareness of beliefs and perspectives in a subject during therapeutic counseling. Specific types of Socratic questions are employed for enabling reasoning and alternate views against the context of individual personal opinions on a topic. Socratic contexts are different from traditional question generation contexts where “answer-seeking” questions are generated against a given formal passage on a topic, narrative stories or conversations.We present SocratiQ, the first large dataset of 110K (question, context) pairs for enabling studies on Socratic Question Generation (SoQG). We provide an in-depth study on the various types of Socratic questions and present models for generating Socratic questions against a given context through prompt tuning. Our automated and human evaluation results demonstrate that our SoQG models can produce realistic, type-sensitive, human-like Socratic questions enabling potential applications in counseling and coaching",
    "volume": "main",
    "checked": true,
    "id": "6c46d7e44626c9634903ddc624df8a344f0277c4",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.13": {
    "title": "Do we need Label Regularization to Fine-tune Pre-trained Language Models?",
    "abstract": "Knowledge Distillation (KD) is a prominent neural model compression technique that heavily relies on teacher network predictions to guide the training of a student model. Considering the ever-growing size of pre-trained language models (PLMs), KD is often adopted in many NLP tasks involving PLMs. However, it is evident that in KD, deploying the teacher network during training adds to the memory and computational requirements of training. In the computer vision literature, the necessity of the teacher network is put under scrutiny by showing that KD is a label regularization technique that can be replaced with lighter teacher-free variants such as the label-smoothing technique. However, to the best of our knowledge, this issue is not investigated in NLP. Therefore, this work concerns studying different label regularization techniques and whether we actually need them to improve the fine-tuning of smaller PLM networks on downstream tasks. In this regard, we did a comprehensive set of experiments on different PLMs such as BERT, RoBERTa, and GPT with more than 600 distinct trials and ran each configuration five times. This investigation led to a surprising observation that KD and other label regularization techniques do not play any meaningful role over regular fine-tuning when the student model is pre-trained. We further explore this phenomenon in different settings of NLP and computer vision tasks and demonstrate that pre-training itself acts as a kind of regularization, and additional label regularization is unnecessary",
    "volume": "main",
    "checked": true,
    "id": "6b66ea79bc382ec5dd695ccb0b437bf38e3de556",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.14": {
    "title": "COVID-VTS: Fact Extraction and Verification on Short Video Platforms",
    "abstract": "We introduce a new benchmark, COVID-VTS, for fact-checking multi-modal information involving short-duration videos with COVID19- focused information from both the real world and machine generation. We propose, TwtrDetective, an effective model incorporating cross-media consistency checking to detect token-level malicious tampering in different modalities, and generate explanations. Due to the scarcity of training data, we also develop an efficient and scalable approach to automatically generate misleading video posts by event manipulation or adversarial matching. We investigate several state-of-the-art models and demonstrate the superiority of TwtrDetective",
    "volume": "main",
    "checked": true,
    "id": "b31e167ecb58038d3fb3f6818d64a4505eb59cfb",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.15": {
    "title": "Multimodal Graph Transformer for Multimodal Question Answering",
    "abstract": "Despite the success of Transformer models in vision and language tasks, they often learn knowledge from enormous data implicitly and cannot utilize structured input data directly. On the other hand, structured learning approaches such as graph neural networks (GNNs) that integrate prior information can barely compete with Transformer models.In this work, we aim to benefit from both worlds and propose a novel Multimodal Graph Transformer for question answering tasks that requires performing reasoning across multiple modalities. We introduce a graph-involved plug-and-play quasi-attention mechanism to incorporate multimodal graph information, acquired from text and visual data, to the vanilla self-attention as effective prior. In particular, we construct the text graph, dense region graph, and semantic graph to generate adjacency matrices, and then compose them with input vision and language features to perform downstream reasoning. Such a way of regularizing self-attention with graph information significantly improves the inferring ability and helps align features from different modalities.We validate the effectiveness of Multimodal Graph Transformer over its Transformer baselines on GQA, VQAv2, and MultiModalQA datasets",
    "volume": "main",
    "checked": true,
    "id": "5f09c0dc0cdbb662be761bd34a87e68e3ca02a57",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.16": {
    "title": "Retrieval Enhanced Data Augmentation for Question Answering on Privacy Policies",
    "abstract": "Prior studies in privacy policies frame the question answering (QA) task as identifying the most relevant text segment or a list of sentences from a policy document given a user query. Existing labeled datasets are heavily imbalanced (only a few relevant segments), limiting the QA performance in this domain. In this paper, we develop a data augmentation framework based on ensembling retriever models that captures the relevant text segments from unlabeled policy documents and expand the positive examples in the training set. In addition, to improve the diversity and quality of the augmented data, we leverage multiple pre-trained language models (LMs) and cascaded them with noise reduction oracles. Using our augmented data on the PrivacyQA benchmark, we elevate the existing baseline by a large margin (10% F1) and achieve a new state-of-the-art F1 score of 50%. Our ablation studies provide further insights into the effectiveness of our approach",
    "volume": "main",
    "checked": true,
    "id": "a7972d8f9f1ed21293a355e925006536fe6fe4df",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.17": {
    "title": "FastKASSIM: A Fast Tree Kernel-Based Syntactic Similarity Metric",
    "abstract": "Syntax is a fundamental component of language, yet few metrics have been employed to capture syntactic similarity or coherence at the utterance- and document-level. The existing standard document-level syntactic similarity metric is computationally expensive and performs inconsistently when faced with syntactically dissimilar documents. To address these challenges, we present FastKASSIM, a metric for utterance- and document-level syntactic similarity which pairs and averages the most similar constituency parse trees between a pair of documents based on tree kernels. FastKASSIM is more robust to syntactic dissimilarities and runs up to to 5.32 times faster than its predecessor over documents in the r/ChangeMyView corpus. FastKASSIM’s improvements allow us to examine hypotheses in two settings with large documents. We find that syntactically similar arguments on r/ChangeMyView tend to be more persuasive, and that syntax is predictive of authorship attribution in the Australian High Court Judgment corpus",
    "volume": "main",
    "checked": true,
    "id": "b274acf5b045f762ff6269b4eef25cff90fc9e4e",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.18": {
    "title": "Friend-training: Learning from Models of Different but Related Tasks",
    "abstract": "Current self-training methods such as standard self-training, co-training, tri-training, and others often focus on improving model performance on a single task, utilizing differences in input features, model architectures, and training processes. However, many tasks in natural language processing are about different but related aspects of language, and models trained for one task can be great teachers for other related tasks. In this work, we propose friend-training, a cross-task self-training framework, where models trained to do different tasks are used in an iterative training, pseudo-labeling, and retraining process to help each other for better selection of pseudo-labels. With two dialogue understanding tasks, conversational semantic role labeling and dialogue rewriting, chosen for a case study, we show that the models trained with the friend-training framework achieve the best performance compared to strong baselines",
    "volume": "main",
    "checked": true,
    "id": "ecdc3343a47394790f999fbee1b1f76c9e55e4f9",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.19": {
    "title": "Understanding Transformer Memorization Recall Through Idioms",
    "abstract": "To produce accurate predictions, language models (LMs) must balance between generalization and memorization. Yet, little is known about the mechanism by which transformer LMs employ their memorization capacity. When does a model decide to output a memorized phrase, and how is this phrase then retrieved from memory? In this work, we offer the first methodological framework for probing and characterizing recall of memorized sequences in transformer LMs. First, we lay out criteria for detecting model inputs that trigger memory recall, and propose idioms as inputs that typically fulfill these criteria. Next, we construct a dataset of English idioms and use it to compare model behavior on memorized vs. non-memorized inputs. Specifically, we analyze the internal prediction construction process by interpreting the model’s hidden representations as a gradual refinement of the output probability distribution. We find that across different model sizes and architectures, memorized predictions are a two-step process: early layers promote the predicted token to the top of the output distribution, and upper layers increase model confidence. This suggests that memorized information is stored and retrieved in the early layers of the network. Last, we demonstrate the utility of our methodology beyond idioms in memorized factual statements. Overall, our work makes a first step towards understanding memory recall, and provides a methodological basis for future studies of transformer memorization",
    "volume": "main",
    "checked": true,
    "id": "870693e9de7711d46ec621eab77b5b282f2caf47",
    "citation_count": 3
  },
  "https://aclanthology.org/2023.eacl-main.20": {
    "title": "A Discerning Several Thousand Judgments: GPT-3 Rates the Article + Adjective + Numeral + Noun Construction",
    "abstract": "Knowledge of syntax includes knowledge of rare, idiosyncratic constructions. LLMs must overcome frequency biases in order to master such constructions. In this study, I prompt GPT-3 to give acceptability judgments on the English-language Article + Adjective + Numeral + Noun construction (e.g., “a lovely five days”). I validate the prompt using the CoLA corpus of acceptability judgments and then zero in on the AANN construction. I compare GPT- 3’s judgments to crowdsourced human judgments on a subset of sentences. GPT-3’s judgments are broadly similar to human judgments and generally align with proposed constraints in the literature but, in some cases, GPT-3’s judgments and human judgments diverge from the literature and from each other",
    "volume": "main",
    "checked": true,
    "id": "ce32f6c26e089c6c8d6e0b4f4c48dfe4aae615ba",
    "citation_count": 3
  },
  "https://aclanthology.org/2023.eacl-main.21": {
    "title": "Triple-Hybrid Energy-based Model Makes Better Calibrated Natural Language Understanding Models",
    "abstract": "Though pre-trained language models achieve notable success in many applications, it’s usually controversial for over-confident predictions. Specifically, the in-distribution (ID) miscalibration and out-of-distribution (OOD) detection are main concerns. Recently, some works based on energy-based models~(EBM) have shown great improvements on both ID calibration and OOD detection for images. However, it’s rarely explored in natural language understanding tasks due to the non-differentiability of text data which makes it more difficult for EBM training. In this paper, we first propose a triple-hybrid EBM which combines the benefits of classifier, conditional generative model and marginal generative model altogether. Furthermore, we leverage contrastive learning to approximately train the proposed model, which circumvents the non-differentiability issue of text data. Extensive experiments have been done on GLUE and six other multiclass datasets in various domains. Our model outperforms previous methods in terms of ID calibration and OOD detection by a large margin while maintaining competitive accuracy",
    "volume": "main",
    "checked": true,
    "id": "969f54b6e4ad943e4976e9b661fa7272a59633ae",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.22": {
    "title": "A weakly supervised textual entailment approach to zero-shot text classification",
    "abstract": "Zero-shot text classification is a widely studied task that deals with a lack of annotated data. The most common approach is to reformulate it as a textual entailment problem, enabling classification into unseen classes. This work explores an effective approach that trains on a weakly supervised dataset generated from traditional classification data. We empirically study the relation between the performance of the entailment task, which is used as a proxy, and the target zero-shot text classification task. Our findings reveal that there is no linear correlation between both tasks, to the extent that it can be detrimental to lengthen the fine-tuning process even when the model is still learning, and propose a straightforward method to stop training on time. As a proof of concept, we introduce a domain-specific zero-shot text classifier that was trained on Microsoft Academic Graph data. The model, called SCIroShot, achieves state-of-the-art performance in the scientific domain and competitive results in other areas. Both the model and evaluation benchmark are publicly available on HuggingFace and GitHub",
    "volume": "main",
    "checked": true,
    "id": "adaa4577d92b23ea2fe2895b003eda689a98fcae",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.23": {
    "title": "Fair Enough: Standardizing Evaluation and Model Selection for Fairness Research in NLP",
    "abstract": "Modern NLP systems exhibit a range of biases, which a growing literature on model debiasing attempts to correct. However, current progress is hampered by a plurality of definitions of bias, means of quantification, and oftentimes vague relation between debiasing algorithms and theoretical measures of bias. This paper seeks to clarify the current situation and plot a course for meaningful progress in fair learning, with two key contributions: (1) making clear inter-relations among the current gamut of methods, and their relation to fairness theory; and (2) addressing the practical problem of model selection, which involves a trade-off between fairness and accuracy and has led to systemic issues in fairness research. Putting them together, we make several recommendations to help shape future work",
    "volume": "main",
    "checked": true,
    "id": "7711d788b73f04635a5cc0ab4bd7bcbe9665bce7",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.24": {
    "title": "CHARD: Clinical Health-Aware Reasoning Across Dimensions for Text Generation Models",
    "abstract": "We motivate and introduce CHARD: Clinical Health-Aware Reasoning across Dimensions, to investigate the capability of text generation models to act as implicit clinical knowledge bases and generate free-flow textual explanations about various health-related conditions across several dimensions. We collect and present an associated dataset, CHARDat, consisting of explanations about 52 health conditions across three clinical dimensions. We conduct extensive experiments using BART and T5 along with data augmentation, and perform automatic, human, and qualitative analyses. We show that while our models can perform decently, CHARD is very challenging with strong potential for further exploration",
    "volume": "main",
    "checked": true,
    "id": "ad62d710f1854daf372680263f50a4e135e309f2",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.25": {
    "title": "Prompt Tuning with Contradictory Intentions for Sarcasm Recognition",
    "abstract": "Recently, prompt tuning has achieved promising results in a variety of natural language processing (NLP) tasks. The typical approach is to insert text pieces (i.e. templates) into the input and transform downstream tasks into the same form as pre-training. In essence, a high-quality template is the foundation of prompt tuning to support the performance of the converted cloze-style task. However, for sarcasm recognition, it is time-consuming and requires increasingly sophisticated domain knowledge to determine the appropriate templates and label words due to its highly figurative nature. In this work, we propose SarcPrompt, to incorporate the prior knowledge about contradictory intentions into prompt tuning for sarcasm recognition. SarcPrompt is inspired by that the speaker usually says the opposite of what they actually mean in the sarcastic text. Based on this idea, we explicitly mimic the actual intention by prompt construction and indicate whether the actual intention is contradictory to the literal content by verbalizer engineering. Experiments on three public datasets with standard and low-resource settings demonstrate the effectiveness of our SarcPrompt for sarcasm recognition",
    "volume": "main",
    "checked": true,
    "id": "6e799514114128f0b98b62d94088a21aa97b8cd6",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.26": {
    "title": "COMBO: A Complete Benchmark for Open KG Canonicalization",
    "abstract": "Open knowledge graph (KG) consists of (subject, relation, object) triples extracted from millions of raw text. The subject and object noun phrases and the relation in open KG have severe redundancy and ambiguity and need to be canonicalized. Existing datasets for open KG canonicalization only provide gold entity-level canonicalization for noun phrases. In this paper, we present COMBO, a Complete Benchmark for Open KG canonicalization. Compared with existing datasets, we additionally provide gold canonicalization for relation phrases, gold ontology-level canonicalization for noun phrases, as well as source sentences from which triples are extracted. We also propose metrics for evaluating each type of canonicalization. On the COMBO dataset, we empirically compare previously proposed canonicalization methods as well as a few simple baseline methods based on pretrained language models. We find that properly encoding the phrases in a triple using pretrained language models results in better relation canonicalization and ontology-level canonicalization of the noun phrase. We release our dataset, baselines, and evaluation scripts at path/to/url",
    "volume": "main",
    "checked": true,
    "id": "032244fb8ff881f4f12345e9afc7ea5627952f4a",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.27": {
    "title": "UScore: An Effective Approach to Fully Unsupervised Evaluation Metrics for Machine Translation",
    "abstract": "The vast majority of evaluation metrics for machine translation are supervised, i.e., (i) are trained on human scores, (ii) assume the existence of reference translations, or (iii) leverage parallel data. This hinders their applicability to cases where such supervision signals are not available. In this work, we develop fully unsupervised evaluation metrics. To do so, we leverage similarities and synergies between evaluation metric induction, parallel corpus mining, and MT systems. In particular, we use an unsupervised evaluation metric to mine pseudo-parallel data, which we use to remap deficient underlying vector spaces (in an iterative manner) and to induce an unsupervised MT system, which then provides pseudo-references as an additional component in the metric. Finally, we also induce unsupervised multilingual sentence embeddings from pseudo-parallel data. We show that our fully unsupervised metrics are effective, i.e., they beat supervised competitors on 4 out of our 5 evaluation datasets. We make our code publicly available",
    "volume": "main",
    "checked": true,
    "id": "7e7cf447c6ed27980d56ee15cf80a39ab39137a6",
    "citation_count": 6
  },
  "https://aclanthology.org/2023.eacl-main.28": {
    "title": "Assistive Recipe Editing through Critiquing",
    "abstract": "There has recently been growing interest in the automatic generation of cooking recipes that satisfy some form of dietary restrictions, thanks in part to the availability of online recipe data. Prior studies have used pre-trained language models, or relied on small paired recipe data (e.g., a recipe paired with a similar one that satisfies a dietary constraint). However, pre-trained language models generate inconsistent or incoherent recipes, and paired datasets are not available at scale. We address these deficiencies with RecipeCrit, a hierarchical denoising auto-encoder that edits recipes given ingredient-level critiques. The model is trained for recipe completion to learn semantic relationships within recipes. Our work’s main innovation is our unsupervised critiquing module that allows users to edit recipes by interacting with the predicted ingredients; the system iteratively rewrites recipes to satisfy users’ feedback. Experiments onthe Recipe1M recipe dataset show that our model can more effectively edit recipes compared to strong language-modeling baselines, creating recipes that satisfy user constraints and are more correct, serendipitous, coherent, and relevant as measured by human judges",
    "volume": "main",
    "checked": true,
    "id": "9a008a1e1bfb663a4c2df30e2e582199aa9d6d0a",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.29": {
    "title": "DiTTO: A Feature Representation Imitation Approach for Improving Cross-Lingual Transfer",
    "abstract": "Zero-shot cross-lingual transfer is promising, however has been shown to be sub-optimal, with inferior transfer performance across low-resource languages. In this work, we envision languages as domains for improving zero-shot transfer by jointly reducing the feature incongruity between the source and the target language and increasing the generalization capabilities of pre-trained multilingual transformers. We show that our approach, DiTTO, significantly outperforms the standard zero-shot fine-tuning method on multiple datasets across all languages using solely unlabeled instances in the target language. Empirical results show that jointly reducing feature incongruity for multiple target languages is vital for successful cross-lingual transfer. Moreover, our model enables better cross-lingual transfer than standard fine-tuning methods, even in the few-shot setting",
    "volume": "main",
    "checked": true,
    "id": "58d8d2967c424117625ee63835aa6389f59cd183",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.30": {
    "title": "John is 50 years old, can his son be 65?\" Evaluating NLP Models' Understanding of Feasibility",
    "abstract": "In current NLP research, large-scale language models and their abilities are widely being discussed. Some recent works have also found notable failures of these models. Often these failure examples involve complex reasoning abilities. This work focuses on a simple commonsense ability, reasoning about when an action (or its effect) is feasible. To this end, we introduce FeasibilityQA, a question-answering dataset involving binary classification (BCQ) and multi-choice multi-correct questions (MCQ) that test understanding of feasibility. We show that even state-of-the-art models such as GPT-3, GPT-2, and T5 struggle to answer the feasibility questions correctly. Specifically, on (MCQ, BCQ) questions, GPT-3 achieves accuracy of just (19%, 62%) and (25%, 64%) in zero-shot and few-shot settings, respectively. We also evaluate models by providing relevant knowledge statements required to answer the question and find that the additional knowledge leads to a 7% gain in performance, but the overall performance still remains low. These results make one wonder how much commonsense knowledge about action feasibility is encoded in state-of-the-art models and how well they can reason about it",
    "volume": "main",
    "checked": true,
    "id": "9a3f1a51ab2b3816655e4c4f58a022421ea7b34b",
    "citation_count": 2
  },
  "https://aclanthology.org/2023.eacl-main.31": {
    "title": "Efficient Encoders for Streaming Sequence Tagging",
    "abstract": "A naive application of state-of-the-art bidirectional encoders for streaming sequence tagging would require encoding each token from scratch for each new token in an incremental streaming input (like transcribed speech). The lack of re-usability of previous computation leads to a higher number of Floating Point Operations (or FLOPs) and higher number of unnecessary label flips. Increased FLOPs consequently lead to higher wall-clock time and increased label flipping leads to poorer streaming performance. In this work, we present a Hybrid Encoder with Adaptive Restart (HEAR) that addresses these issues while maintaining the performance of bidirectional encoders over the offline (or complete) and improving streaming (or incomplete) inputs. HEAR has a Hybrid unidirectional-bidirectional encoder architecture to perform sequence tagging, along with an Adaptive Restart Module (ARM) to selectively guide the restart of bidirectional portion of the encoder. Across four sequence tagging tasks, HEAR offers FLOP savings in streaming settings upto 71.1% and also outperforms bidirectional encoders for streaming predictions by upto +10% streaming exact match",
    "volume": "main",
    "checked": true,
    "id": "14b4c3ef9329f2df1d8896be253b00b77cda026f",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.32": {
    "title": "Retrieve-and-Fill for Scenario-based Task-Oriented Semantic Parsing",
    "abstract": "Task-oriented semantic parsing models have achieved strong results in recent years, but unfortunately do not strike an appealing balance between model size, runtime latency, and cross-domain generalizability. We tackle this problem by introducing scenario-based semantic parsing: a variant of the original task which first requires disambiguating an utterance’s “scenario” (an intent-slot template with variable leaf spans) before generating its frame, complete with ontology and utterance tokens. This formulation enables us to isolate coarse-grained and fine-grained aspects of the task, each of which we solve with off-the-shelf neural modules, also optimizing for the axes outlined above. Concretely, we create a Retrieve-and-Fill (RAF) architecture comprised of (1) a retrieval module which ranks the best scenario given an utterance and (2) a filling module which imputes spans into the scenario to create the frame. Our model is modular, differentiable, interpretable, and allows us to garner extra supervision from scenarios. RAF achieves strong results in high-resource, low-resource, and multilingual settings, outperforming recent approaches by wide margins despite, using base pre-trained encoders, small sequence lengths, and parallel decoding",
    "volume": "main",
    "checked": true,
    "id": "7b4b1bab91319c63b7d247737f86a74428747b81",
    "citation_count": 2
  },
  "https://aclanthology.org/2023.eacl-main.33": {
    "title": "Document Flattening: Beyond Concatenating Context for Document-Level Neural Machine Translation",
    "abstract": "Existing work in document-level neural machine translation commonly concatenates several consecutive sentences as a pseudo-document, and then learns inter-sentential dependencies. This strategy limits the model’s ability to leverage information from distant context. We overcome this limitation with a novel Document Flattening (DocFlat) technique that integrates Flat-Batch Attention (FBA) and Neural Context Gate (NCG) into Transformer model to utilizes information beyond the pseudo-document boundaries. FBA allows the model to attend to all the positions in the batch and model the relationships between positions explicitly and NCG identifies the useful information from the distant context. We conduct comprehensive experiments and analyses on three benchmark datasets for English-German translation, and validate the effectiveness of two variants of DocFlat. Empirical results show that our approach outperforms strong baselines with statistical significance on BLEU, COMET and accuracy on the contrastive test set. The analyses highlight that DocFlat is highly effective in capturing the long-range information",
    "volume": "main",
    "checked": true,
    "id": "67e0c379a21643b42b05206ea8c85610e9338ad4",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.34": {
    "title": "Scaling Back-Translation with Domain Text Generation for Sign Language Gloss Translation",
    "abstract": "Sign language gloss translation aims to translate the sign glosses into spoken language texts, which is challenging due to the scarcity of labeled gloss-text parallel data. Back translation (BT), which generates pseudo-parallel data by translating in-domain spoken language texts into sign glosses, has been applied to alleviate the data scarcity problem. However, the lack of large-scale high-quality in-domain spoken language text data limits the effect of BT. In this paper, to overcome the limitation, we propose a Prompt based domain text Generation (PGen) approach to produce the large-scale in-domain spoken language text data. Specifically, PGen randomly concatenates sentences from the original in-domain spoken language text data as prompts to induce a pre-trained language model (i.e., GPT-2) to generate spoken language texts in a similar style. Experimental results on three benchmarks of sign language gloss translation in varied languages demonstrate that BT with spoken language texts generated by PGen significantly outperforms the compared methods. In addition, as the scale of spoken language texts generated by PGen increases, the BT technique can achieve further improvements, demonstrating the effectiveness of our approach. We release the code and data for facilitating future research in this field",
    "volume": "main",
    "checked": true,
    "id": "7d7fa25e5cc7a805d5c6dab85eeeb19c96f28af1",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.35": {
    "title": "Realistic Conversational Question Answering with Answer Selection based on Calibrated Confidence and Uncertainty Measurement",
    "abstract": "Conversational Question Answering (ConvQA) models aim at answering a question with its relevant paragraph and previous question-answer pairs that occurred during conversation multiple times. To apply such models to a real-world scenario, some existing work uses predicted answers, instead of unavailable ground-truth answers, as the conversation history for inference. However, since these models usually predict wrong answers, using all the predictions without filtering significantly hampers the model performance. To address this problem, we propose to filter out inaccurate answers in the conversation history based on their estimated confidences and uncertainties from the ConvQA model, without making any architectural changes. Moreover, to make the confidence and uncertainty values more reliable, we propose to further calibrate them, thereby smoothing the model predictions. We validate our models, Answer Selection-based realistic Conversation Question Answering, on two standard ConvQA datasets, and the results show that our models significantly outperform relevant baselines. Code is available at: https://github.com/starsuzi/AS-ConvQA",
    "volume": "main",
    "checked": true,
    "id": "346cbb2e7e54af4e46cc3697be0f2f7e75638052",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.36": {
    "title": "PANCETTA: Phoneme Aware Neural Completion to Elicit Tongue Twisters Automatically",
    "abstract": "Tongue twisters are meaningful sentences that are difficult to pronounce. The process of automatically generating tongue twisters is challenging since the generated utterance must satisfy two conditions at once: phonetic difficulty and semantic meaning. Furthermore, phonetic difficulty is itself hard to characterize and is expressed in natural tongue twisters through a heterogeneous mix of phenomena such as alliteration and homophony. In this paper, we propose PANCETTA: Phoneme Aware Neural Completion to Elicit Tongue Twisters Automatically. We leverage phoneme representations to capture the notion of phonetic difficulty, and we train language models to generate original tongue twisters on two proposed task settings. To do this, we curate a dataset called TT-Corp, consisting of existing English tongue twisters. Through automatic and human evaluation, as well as qualitative analysis, we show that PANCETTA generates novel, phonetically difficult, fluent, and semantically meaningful tongue twisters",
    "volume": "main",
    "checked": true,
    "id": "f2f83a1e928c5542f5ad13109dfbaebfab29ab5b",
    "citation_count": 2
  },
  "https://aclanthology.org/2023.eacl-main.37": {
    "title": "A User-Centered, Interactive, Human-in-the-Loop Topic Modelling System",
    "abstract": "Human-in-the-loop topic modelling incorporates users’ knowledge into the modelling process, enabling them to refine the model iteratively. Recent research has demonstrated the value of user feedback, but there are still issues to consider, such as the difficulty in tracking changes, comparing different models and the lack of evaluation based on real-world examples of use. We developed a novel, interactive human-in-the-loop topic modeling system with a user-friendly interface that enables users compare and record every step they take, and a novel topic words suggestion feature to help users provide feedback that is faithful to the ground truth. Our system also supports not only what traditional topic models can do, i.e., learning the topics from the whole corpus, but also targeted topic modelling, i.e., learning topics for specific aspects of the corpus. In this article, we provide an overview of the system and present the results of a series of user studies designed to assess the value of the system in progressively more realistic applications of topic modelling",
    "volume": "main",
    "checked": true,
    "id": "ae973674ff64dbf39eef613803947723f2acca06",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.38": {
    "title": "A Survey of Methods for Addressing Class Imbalance in Deep-Learning Based Natural Language Processing",
    "abstract": "Many natural language processing (NLP) tasks are naturally imbalanced, as some target categories occur much more frequently than others in the real world. In such scenarios, current NLP models tend to perform poorly on less frequent classes. Addressing class imbalance in NLP is an active research topic, yet, finding a good approach for a particular task and imbalance scenario is difficult.In this survey, the first overview on class imbalance in deep-learning based NLP, we first discuss various types of controlled and real-world class imbalance.Our survey then covers approaches that have been explicitly proposed for class-imbalanced NLP tasks or, originating in the computer vision community, have been evaluated on them.We organize the methods by whether they are based on sampling, data augmentation, choice of loss function, staged learning, or model design.Finally, we discuss open problems and how to move forward",
    "volume": "main",
    "checked": true,
    "id": "458f6f96930f207381bba51987cae9f84ecebb84",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.39": {
    "title": "Extracting or Guessing? Improving Faithfulness of Event Temporal Relation Extraction",
    "abstract": "In this paper, we seek to improve the faithfulness of TempRel extraction models from two perspectives. The first perspective is to extract genuinely based on contextual description. To achieve this, we propose to conduct counterfactual analysis to attenuate the effects of two significant types of training biases: the event trigger bias and the frequent label bias. We also add tense information into event representations to explicitly place an emphasis on the contextual description. The second perspective is to provide proper uncertainty estimation and abstain from extraction when no relation is described in the text. By parameterization of Dirichlet Prior over the model-predicted categorical distribution, we improve the model estimates of the correctness likelihood and make TempRel predictions more selective. We also employ temperature scaling to recalibrate the model confidence measure after bias mitigation. Through experimental analysis on MATRES, MATRES-DS, and TDDiscourse, we demonstrate that our model extracts TempRel and timelines more faithfully compared to SOTA methods, especially under distribution shifts",
    "volume": "main",
    "checked": true,
    "id": "e4ce946fa04258aa35bac4da7b33597dfbd358cd",
    "citation_count": 5
  },
  "https://aclanthology.org/2023.eacl-main.40": {
    "title": "LoFT: Enhancing Faithfulness and Diversity for Table-to-Text Generation via Logic Form Control",
    "abstract": "Logical Table-to-Text (LT2T) generation is tasked with generating logically faithful sentences from tables. There currently exists two challenges in the field: 1) Faithfulness: how to generate sentences that are factually correct given the table content; 2) Diversity: how to generate multiple sentences that offer different perspectives on the table. This work proposes LoFT, which utilizes logic forms as fact verifiers and content planners to control LT2T generation. Experimental results on the LogicNLG dataset demonstrate that LoFT is the first model that addresses unfaithfulness and lack of diversity issues simultaneously. Our code is publicly available at https://github.com/Yale-LILY/LoFT",
    "volume": "main",
    "checked": true,
    "id": "05c3a61c5106793dbc0ca4c6d81be888234a13d8",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.41": {
    "title": "PromptDA: Label-guided Data Augmentation for Prompt-based Few Shot Learners",
    "abstract": "Recent advances in large pre-trained language models (PLMs) lead to impressive gains on natural language understanding (NLU) tasks with task-specific fine-tuning. However, directly fine-tuning PLMs heavily relies on sufficient labeled training instances, which are usually hard to obtain. Prompt-based tuning on PLMs has shown to be powerful for various downstream few-shot tasks. Existing works studying prompt-based tuning for few-shot NLU tasks mainly focus on deriving proper label words with a verbalizer or generating prompt templates to elicit semantics from PLMs. In addition, conventional data augmentation strategies such as synonym substitution are also widely adopted in low-resource scenarios. However, the improvements they bring to prompt-based few-shot learning have been demonstrated to be marginal. Thus, an important research question arises as follows: how to design effective data augmentation methods for prompt-based few-shot tuning? To this end, considering the label semantics are essential in prompt-based tuning, we propose a novel label-guided data augmentation framework PromptDA, which exploits the enriched label semantic information for data augmentation. Extensive experiment results on few-shot text classification tasks show that our proposed framework achieves superior performances by effectively leveraging label semantics and data augmentation for natural language understanding",
    "volume": "main",
    "checked": true,
    "id": "176ec99005b5085d5d9a34fb770d75d34166c9f5",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.42": {
    "title": "Incorporating Question Answering-Based Signals into Abstractive Summarization via Salient Span Selection",
    "abstract": "In this work, we propose a method for incorporating question-answering (QA) signals into a summarization model. Our method identifies salient noun phrases (NPs) in the input document by automatically generating wh-questions that are answered by the NPs and automatically determining whether those questions are answered in the gold summaries. This QA-based signal is incorporated into a two-stage summarization model which first marks salient NPs in the input document using a classification model, then conditionally generates a summary. Our experiments demonstrate that the models trained using QA-based supervision generate higher-quality summaries than baseline methods of identifying salient spans on benchmark summarization datasets. Further, we show that the content of the generated summaries can be controlled based on which NPs are marked in the input document. Finally, we propose a method of augmenting the training data so the gold summaries are more consistent with the marked input spans used during training and show how this results in models which learn to better exclude unmarked document content",
    "volume": "main",
    "checked": true,
    "id": "8d24eabd178144a3f651381adc0206b312e13e0c",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.43": {
    "title": "Patient Outcome and Zero-shot Diagnosis Prediction with Hypernetwork-guided Multitask Learning",
    "abstract": "Multitask deep learning has been applied to patient outcome prediction from text, taking clinical notes as input and training deep neural networks with a joint loss function of multiple tasks.However, the joint training scheme of multitask learning suffers from inter-task interference, and diagnosis prediction among the multiple tasks has the generalizability issue due to rare diseases or unseen diagnoses.To solve these challenges, we propose a hypernetwork-based approach that generates task-conditioned parameters and coefficients of multitask prediction heads to learn task-specific prediction and balance the multitask learning.We also incorporate semantic task information to improve the generalizability of our task-conditioned multitask model. Experiments on early and discharge notes extracted from the real-world MIMIC database show our method can achieve better performance on multitask patient outcome prediction than strong baselines in most cases.Besides, our method can effectively handle the scenario with limited information and improve zero-shot prediction on unseen diagnosis categories",
    "volume": "main",
    "checked": true,
    "id": "f155eae9ef4d899fef2faefcd2a68e43af9de15b",
    "citation_count": 3
  },
  "https://aclanthology.org/2023.eacl-main.44": {
    "title": "A Kind Introduction to Lexical and Grammatical Aspect, with a Survey of Computational Approaches",
    "abstract": "Aspectual meaning refers to how the internal temporal structure of situations is presented.This includes whether a situation is described as a state or as an event, whether the situation is finished or ongoing, and whether it is viewed as a whole or with a focus on a particular phase.This survey gives an overview of computational approaches to modeling lexical and grammatical aspect along with intuitive explanations of the necessary linguistic concepts and terminology.In particular, we describe the concepts of stativity, telicity, habituality, perfective and imperfective, as well as influential inventories of eventuality and situation types.Aspect is a crucial component of semantics, especially for precise reporting of the temporal structure of situations, and future NLP approaches need to be able to handle and evaluate it systematically",
    "volume": "main",
    "checked": true,
    "id": "6923b2dd4f2a691496a6931c59150189cd496d76",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.45": {
    "title": "Incorporating Context into Subword Vocabularies",
    "abstract": "Most current popular subword tokenizers are trained based on word frequency statistics over a corpus, without considering information about co-occurrence or context. Nevertheless, the resulting vocabularies are used in language models’ highly contextualized settings. We present SaGe, a tokenizer that tailors subwords for their downstream use by baking in the contextualized signal at the vocabulary creation phase. We show that SaGe does a better job than current widespread tokenizers in keeping token contexts cohesive, while not incurring a large price in terms of encoding efficiency or domain robustness. SaGe improves performance on English GLUE classification tasks as well as on NER, and on Inference and NER in Turkish, demonstrating its robustness to language properties such as morphological exponence and agglutination",
    "volume": "main",
    "checked": true,
    "id": "a0568d0bf4cbfcb5a17a20c576dfc421c4ccfb7c",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.46": {
    "title": "LoRaLay: A Multilingual and Multimodal Dataset for Long Range and Layout-Aware Summarization",
    "abstract": "Text Summarization is a popular task and an active area of research for the Natural Language Processing community. By definition, it requires to account for long input texts, a characteristic which poses computational challenges for neural models. Moreover, real-world documents come in a variety of complex, visually-rich, layouts. This information is of great relevance, whether to highlight salient content or to encode long-range interactions between textual passages. Yet, all publicly available summarization datasets only provide plain text content. To facilitate research on how to exploit visual/layout information to better capture long-range dependencies in summarization models, we present LoRaLay, a collection of datasets for long-range summarization with accompanying visual/layout information. We extend existing and popular English datasets (arXiv and PubMed) with layout information and propose four novel datasets – consistently built from scholar resources – covering French, Spanish, Portuguese, and Korean languages. Further, we propose new baselines merging layout-aware and long-range models – two orthogonal approaches – and obtain state-of-the-art results, showing the importance of combining both lines of research",
    "volume": "main",
    "checked": true,
    "id": "68571a8c3598888035c5c4f0a99793434ad61ef4",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.47": {
    "title": "ViHOS: Hate Speech Spans Detection for Vietnamese",
    "abstract": "The rise in hateful and offensive language directed at other users is one of the adverse side effects of the increased use of social networking platforms. This could make it difficult for human moderators to review tagged comments filtered by classification systems. To help address this issue, we present the ViHOS (Vietnamese Hate and Offensive Spans) dataset, the first human-annotated corpus containing 26k spans on 11k comments. We also provide definitions of hateful and offensive spans in Vietnamese comments as well as detailed annotation guidelines. Besides, we conduct experiments with various state-of-the-art models. Specifically, XLM-R_Large achieved the best F1-scores in Single span detection and All spans detection, while PhoBERT_Large obtained the highest in Multiple spans detection. Finally, our error analysis demonstrates the difficulties in detecting specific types of spans in our data for future research. Our dataset is released on GitHub",
    "volume": "main",
    "checked": true,
    "id": "85ebf1543df95361823499bf66f645a4283d1f3f",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.48": {
    "title": "Vote'n'Rank: Revision of Benchmarking with Social Choice Theory",
    "abstract": "The development of state-of-the-art systems in different applied areas of machine learning (ML) is driven by benchmarks, which have shaped the paradigm of evaluating generalisation capabilities from multiple perspectives. Although the paradigm is shifting towards more fine-grained evaluation across diverse tasks, the delicate question of how to aggregate the performances has received particular interest in the community. In general, benchmarks follow the unspoken utilitarian principles, where the systems are ranked based on their mean average score over task-specific metrics. Such aggregation procedure has been viewed as a sub-optimal evaluation protocol, which may have created the illusion of progress. This paper proposes Vote’n’Rank, a framework for ranking systems in multi-task benchmarks under the principles of the social choice theory. We demonstrate that our approach can be efficiently utilised to draw new insights on benchmarking in several ML sub-fields and identify the best-performing systems in research and development case studies. The Vote’n’Rank’s procedures are more robust than the mean average while being able to handle missing performance scores and determine conditions under which the system becomes the winner",
    "volume": "main",
    "checked": true,
    "id": "923f3137714c096107edb6bf656b1e1220194d0e",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.49": {
    "title": "Combining Parameter-efficient Modules for Task-level Generalisation",
    "abstract": "A modular design encourages neural models to disentangle and recombine different facets of knowledge to generalise more systematically to new tasks. In this work, we assume that each task is associated with a subset of latent skills from an (arbitrary size) inventory. In turn, each skill corresponds to a parameter-efficient (sparse / low-rank) model adapter. By jointly learning adapters and a routing function that allocates skills to each task, the full network is instantiated as the average of the parameters of active skills. We propose several inductive biases that encourage re-usage and composition of the skills, including variable-size skill allocation and a dual-speed learning rate. We evaluate our latent-skill model in two main settings: 1) multitask reinforcement learning for instruction following on 8 levels of the BabyAI platform; and 2) few-shot fine-tuning of language models on 160 NLP tasks of the CrossFit benchmark. We find that the modular design of our network enhances sample efficiency in reinforcement learning and few-shot generalisation in supervised learning, compared to a series of baselines. These include models where parameters are fully shared, task-specific, conditionally generated (HyperFormer), or sparse mixture-of-experts (TaskMoE)",
    "volume": "main",
    "checked": true,
    "id": "2cc134293669b20dce3d55a67d08fea665745e7b",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.50": {
    "title": "Self-imitation Learning for Action Generation in Text-based Games",
    "abstract": "In this work, we study reinforcement learning (RL) in solving text-based games. We address the challenge of combinatorial action space, by proposing a confidence-based self-imitation model to generate action candidates for the RL agent. Firstly, we leverage the self-imitation learning to rank and exploit past valuable trajectories to adapt a pre-trained language model (LM) towards a target game. Then, we devise a confidence-based strategy to measure the LM’s confidence with respect to a state, thus adaptively pruning the generated actions to yield a more compact set of action candidates. In multiple challenging games, our model demonstrates promising performance in comparison to the baselines",
    "volume": "main",
    "checked": true,
    "id": "9bfc28cab6f0e35ccfdbf94e568838e3b0530726",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.51": {
    "title": "Investigating the Effect of Relative Positional Embeddings on AMR-to-Text Generation with Structural Adapters",
    "abstract": "Text generation from Abstract Meaning Representation (AMR) has substantially benefited from the popularized Pretrained Language Models (PLMs). Myriad approaches have linearized the input graph as a sequence of tokens to fit the PLM tokenization requirements. Nevertheless, this transformation jeopardizes the structural integrity of the graph and is therefore detrimental to its resulting representation. To overcome this issue, Ribeiro et al. (2021b) have recently proposed StructAdapt, a structure-aware adapter which injects the input graph connectivity within PLMs using Graph Neural Networks (GNNs). In this paper, we investigate the influence of Relative Position Embeddings (RPE) on AMR-to-Text, and, in parallel, we examine the robustness of StructAdapt. Through ablation studies, graph attack and link prediction, we reveal that RPE might be partially encoding input graphs. We suggest further research regarding the role of RPE will provide valuable insights for Graph-to-Text generation",
    "volume": "main",
    "checked": true,
    "id": "f2909fcd0a1c265097490ce43f5065ef6486310d",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.52": {
    "title": "On the Intersection of Context-Free and Regular Languages",
    "abstract": "The Bar-Hillel construction is a classic result in formal language theory. It shows, by a simple construction, that the intersection of a context-free language and a regular language is itself context-free. In the construction, the regular language is specified by a finite-state automaton. However, neither the original construction (Bar-Hillel et al., 1961) nor its weighted extension (Nederhof and Satta, 2003) can handle finite-state automata with ε-arcs. While it is possible to remove ε-arcs from a finite-state automaton efficiently without modifying the language, such an operation modifies the automaton’s set of paths. We give a construction that generalizes the Bar- Hillel in the case the desired automaton has ε-arcs, and further prove that our generalized construction leads to a grammar that encodes the structure of both the input automaton and grammar while retaining the asymptotic size of the original construction",
    "volume": "main",
    "checked": true,
    "id": "8daf4c457efb1b7eb821a4de25e645f133dc9657",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.53": {
    "title": "Social Influence Dialogue Systems: A Survey of Datasets and Models For Social Influence Tasks",
    "abstract": "Dialogue systems capable of social influence such as persuasion, negotiation, and therapy, are essential for extending the use of technology to numerous realistic scenarios. However, existing research primarily focuses on either task-oriented or open-domain scenarios, a categorization that has been inadequate for capturing influence skills systematically. There exists no formal definition or category for dialogue systems with these skills and data-driven efforts in this direction are highly limited. In this work, we formally define and introduce the category of social influence dialogue systems that influence users’ cognitive and emotional responses, leading to changes in thoughts, opinions, and behaviors through natural conversations. We present a survey of various tasks, datasets, and methods, compiling the progress across seven diverse domains. We discuss the commonalities and differences between the examined systems, identify limitations, and recommend future directions. This study serves as a comprehensive reference for social influence dialogue systems to inspire more dedicated research and discussion in this emerging area",
    "volume": "main",
    "checked": true,
    "id": "a1275e92f4830e5bbd53bc1b1fa44a9a2f024f88",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.54": {
    "title": "Aggregating Crowdsourced and Automatic Judgments to Scale Up a Corpus of Anaphoric Reference for Fiction and Wikipedia Texts",
    "abstract": "Although several datasets annotated for anaphoric reference / coreference exist, even the largest such datasets have limitations in term of size, range of domains, coverage of anaphoric phenomena, and size of documents included. Yet, the approaches proposed to scale up anaphoric annotation haven’t so far resulted in datasets overcoming these limitations. In this paper, we introduce a new release of a corpus for anaphoric reference labelled via a game-with-a-purpose. This new release is comparable in size to the largest existing corpora for anaphoric reference due in part to substantial activity by the players, in part thanks to the use of a new resolve-and-aggregate paradigm to ‘complete’ markable annotations through the combination of an anaphoric resolver and an aggregation method for anaphoric reference. The proposed method could be adopted to greatly speed up annotation time in other projects involving games-with-a-purpose. In addition, the corpus covers genres for which no comparable size datasets exist (Fiction and Wikipedia); it covers singletons and non-referring expressions; and it includes a substantial number of long documents ( 2K in length)",
    "volume": "main",
    "checked": true,
    "id": "477a49db30be71417c5ce5d1f255d3c9da9fbe63",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.55": {
    "title": "What Makes Sentences Semantically Related? A Textual Relatedness Dataset and Empirical Study",
    "abstract": "The degree of semantic relatedness of two units of language has long been considered fundamental to understanding meaning. Additionally, automatically determining relatedness has many applications such as question answering and summarization. However, prior NLP work has largely focused on semantic similarity, a subset of relatedness, because of a lack of relatedness datasets. In this paper, we introduce a dataset for Semantic Textual Relatedness, STR-2022, that has 5,500 English sentence pairs manually annotated using a comparative annotation framework, resulting in fine-grained scores. We show that human intuition regarding relatedness of sentence pairs is highly reliable, with a repeat annotation correlation of 0.84. We use the dataset to explore questions on what makes sentences semantically related. We also show the utility of STR-2022 for evaluating automatic methods of sentence representation and for various downstream NLP tasks.Our dataset, data statement, and annotation questionnaire can be found at: https://doi.org/10.5281/zenodo.7599667",
    "volume": "main",
    "checked": true,
    "id": "32fa98e5131245ffc5492e0fb6d4dc51f02406a4",
    "citation_count": 4
  },
  "https://aclanthology.org/2023.eacl-main.56": {
    "title": "RevUp: Revise and Update Information Bottleneck for Event Representation",
    "abstract": "The existence of external (“side”) semantic knowledge has been shown to result in more expressive computational event models. To enable the use of side information that may be noisy or missing, we propose a semi-supervised information bottleneck-based discrete latent variable model. We reparameterize the model’s discrete variables with auxiliary continuous latent variables and a light-weight hierarchical structure. Our model is learned to minimize the mutual information between the observed data and optional side knowledge that is not already captured by the new, auxiliary variables. We theoretically show that our approach generalizes past approaches, and perform an empirical case study of our approach on event modeling. We corroborate our theoretical results with strong empirical experiments, showing that the proposed method outperforms previous proposed approaches on multiple datasets",
    "volume": "main",
    "checked": true,
    "id": "3b58cb00927e64fc46df2d804d41832329759681",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.57": {
    "title": "NusaX: Multilingual Parallel Sentiment Dataset for 10 Indonesian Local Languages",
    "abstract": "Natural language processing (NLP) has a significant impact on society via technologies such as machine translation and search engines. Despite its success, NLP technology is only widely available for high-resource languages such as English and Chinese, while it remains inaccessible to many languages due to the unavailability of data resources and benchmarks. In this work, we focus on developing resources for languages in Indonesia. Despite being the second most linguistically diverse country, most languages in Indonesia are categorized as endangered and some are even extinct. We develop the first-ever parallel resource for 10 low-resource languages in Indonesia. Our resource includes sentiment and machine translation datasets, and bilingual lexicons. We provide extensive analyses and describe challenges for creating such resources. We hope this work can spark NLP research on Indonesian and other underrepresented languages",
    "volume": "main",
    "checked": true,
    "id": "11f64ec047782cada21d50efea1e0dc5843675f6",
    "citation_count": 7
  },
  "https://aclanthology.org/2023.eacl-main.58": {
    "title": "The Functional Relevance of Probed Information: A Case Study",
    "abstract": "Recent studies have shown that transformer models like BERT rely on number information encoded in their representations of sentences’ subjects and head verbs when performing subject-verb agreement. However, probing experiments suggest that subject number is also encoded in the representations of all words in such sentences. In this paper, we use causal interventions to show that BERT only uses the subject plurality information encoded in its representations of the subject and words that agree with it in number. We also demonstrate that current probing metrics are unable to determine which words’ representations contain functionally relevant information. This both provides a revised view of subject-verb agreement in language models, and suggests potential pitfalls for current probe usage and evaluation",
    "volume": "main",
    "checked": true,
    "id": "c095cb3c28ba6eb4bd90ed8bee3460bfc2f10a19",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.59": {
    "title": "Do Pretrained Contextual Language Models Distinguish between Hebrew Homograph Analyses?",
    "abstract": "Semitic morphologically-rich languages (MRLs) are characterized by extreme word ambiguity. Because most vowels are omitted in standard texts, many of the words are homographs with multiple possible analyses, each with a different pronunciation and different morphosyntactic properties. This ambiguity goes {{em beyond} word-sense disambiguation (WSD), and may include token segmentation into multiple word units. Previous research on MRLs claimed that standardly trained pre-trained language models (PLMs) based on word-pieces may not sufficiently capture the internal structure of such tokens in order to distinguish between these analyses.Taking Hebrew as a case study, we investigate the extent to which Hebrew homographs can be disambiguated and analyzed using PLMs. We evaluate all existing models for contextualized Hebrew embeddings on a novel Hebrew homograph challenge sets that we deliver. Our empirical results demonstrate that contemporary Hebrew contextualized embeddings outperform non-contextualized embeddings; and that they are most effective for disambiguating segmentation and morphosyntactic features, less so regarding pure word-sense disambiguation. We show that these embeddings are more effective when the number of word-piece splits is limited, and they are more effective for 2-way and 3-way ambiguities than for 4-way ambiguity. We show that the embeddings are equally effective for homographs of both balanced and skewed distributions, whether calculated as masked or unmasked tokens. Finally, we show that these embeddings are as effective for homograph disambiguation with extensive supervised training as with a few-shot setup",
    "volume": "main",
    "checked": true,
    "id": "bc5609cb0ff3fcd110c1423b911f54f9284a45d9",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.60": {
    "title": "Parameter-Efficient Tuning with Special Token Adaptation",
    "abstract": "Parameter-efficient tuning aims at updating only a small subset of parameters when adapting a pretrained model to downstream tasks. In this work, we introduce PASTA, in which we only modify the special token representations (e.g., [SEP] and [CLS] in BERT) before the self-attention module at each layer in Transformer-based models. PASTA achieves comparable performance to fine-tuning in natural language understanding tasks including text classification and NER with up to only 0.029% of total parameters trained. Our work not only provides a simple yet effective way of parameter-efficient tuning, which has a wide range of practical applications when deploying finetuned models for multiple tasks, but also demonstrates the pivotal role of special tokens in pretrained language models",
    "volume": "main",
    "checked": true,
    "id": "02ed150fde2125c8d2c3c0278e607d4a3c2ff852",
    "citation_count": 3
  },
  "https://aclanthology.org/2023.eacl-main.61": {
    "title": "Probing Power by Prompting: Harnessing Pre-trained Language Models for Power Connotation Framing",
    "abstract": "When describing actions, subtle changes in word choice can evoke very different associations with the involved entities. For instance, a company ‘{{it employing} workers’ evokes a more positive connotation than the one ‘{{it exploiting}’ them. This concept is called {{it connotation}. This paper investigates whether pre-trained language models (PLMs) encode such subtle connotative information about {{it power differentials} between involved entities. We design a probing framework for power connotation, building on~{citet{sap-etal-2017-connotation}’s operationalization of {{it connotation frames}. We show that zero-shot prompting of PLMs leads to above chance prediction of power connotation, however fine-tuning PLMs using our framework drastically improves their accuracy. Using our fine-tuned models, we present a case study of {{it power dynamics} in US news reporting on immigration, showing the potential of our framework as a tool for understanding subtle bias in the media",
    "volume": "main",
    "checked": true,
    "id": "60c11f02982bcfe1f8be25c87c82606aeef9758b",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.62": {
    "title": "Zero and Few-Shot Localization of Task-Oriented Dialogue Agents with a Distilled Representation",
    "abstract": "Task-oriented Dialogue (ToD) agents are mostly limited to a few widely-spoken languages, mainly due to the high cost of acquiring training data for each language. Existing low-cost approaches that rely on cross-lingual embeddings or naive machine translation sacrifice a lot of accuracy for data efficiency, and largely fail in creating a usable dialogue agent.We propose automatic methods that use ToD training data in a source language to build a high-quality functioning dialogue agent in another target language that has no training data (i.e. zero-shot) or a small training set (i.e. few-shot). Unlike most prior work in cross-lingual ToD that only focuses on Dialogue State Tracking (DST), we build an end-to-end agent.We show that our approach closes the accuracy gap between few-shot and existing full-shot methods for ToD agents.We achieve this by (1) improving the dialogue data representation, (2) improving entity-aware machine translation, and (3) automatic filtering of noisy translations.We evaluate our approach on the recent bilingual dialogue dataset BiToD.In Chinese to English transfer, in the zero-shot setting, our method achieves 46.7% and 22.0% in Task Success Rate (TSR) and Dialogue Success Rate (DSR) respectively. In the few-shot setting where 10% of the data in the target language is used, we improve the state-of-the-art by 15.2% and 14.0%, coming within 5% of full-shot training",
    "volume": "main",
    "checked": true,
    "id": "4e45be3ac65817313bf0231835e42acc71cd370f",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.63": {
    "title": "Contextual Semantic Parsing for Multilingual Task-Oriented Dialogues",
    "abstract": "Robust state tracking for task-oriented dialogue systems currently remains restricted to a few popular languages.This paper shows that given a large-scale dialogue data set in one language, we can automatically produce an effective semantic parser for other languages using machine translation. We propose automatic translation of dialogue datasets with alignment to ensure faithful translation of slot values and eliminate costly human supervision used in previous benchmarks. We also propose a new contextual semantic parsing model, which encodes the formal slots and values, and only the last agent and user utterances. We show that the succinct representation reduces the compounding effect of translation errors, without harming the accuracy in practice.We evaluate our approach on several dialogue state tracking benchmarks. On RiSAWOZ, CrossWOZ, CrossWOZ-EN, and MultiWOZ-ZH datasets we improve the state of the art by 11%, 17%, 20%, and 0.3% in joint goal accuracy. We present a comprehensive error analysis for all three datasets showing erroneous annotations can lead to misguided judgments on the quality of the model. Finally, we present RiSAWOZ English and German datasets, created using our translation methodology. On these datasets, accuracy is within 11% of the original showing that high-accuracy multilingual dialogue datasets are possible without relying on expensive human annotations. We release our datasets and software open source",
    "volume": "main",
    "checked": true,
    "id": "54ff60be70b36286e288f6d62bc1feb054e95b7e",
    "citation_count": 5
  },
  "https://aclanthology.org/2023.eacl-main.64": {
    "title": "Teacher Intervention: Improving Convergence of Quantization Aware Training for Ultra-Low Precision Transformers",
    "abstract": "Pre-trained Transformer models such as BERT have shown great success in a wide range of applications, but at the cost of substantial increases in model complexity. Quantization-aware training (QAT) is a promising method to lower the implementation cost and energy consumption. However, aggressive quantization below 2-bit causes considerable accuracy degradation due to unstable convergence, especially when the downstream dataset is not abundant. This work proposes a proactive knowledge distillation method called Teacher Intervention (TI) for fast converging QAT of ultra-low precision pre-trained Transformers. TI intervenes layer-wise signal propagation with the intact signal from the teacher to remove the interference of propagated quantization errors, smoothing loss surface of QAT and expediting the convergence. Furthermore, we propose a gradual intervention mechanism to stabilize the recovery of subsections of Transformer layers from quantization. The proposed schemes enable fast convergence of QAT and improve the model accuracy regardless of the diverse characteristics of downstream fine-tuning tasks. We demonstrate that TI consistently achieves superior accuracy with significantly lower fine-tuning iterations on well-known Transformers of natural language processing as well as computer vision compared to the state-of-the-art QAT methods",
    "volume": "main",
    "checked": true,
    "id": "a60469b864bcf770ef940a512a000a791677b075",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.65": {
    "title": "Generative Replay Inspired by Hippocampal Memory Indexing for Continual Language Learning",
    "abstract": "Continual learning aims to accumulate knowledge to solve new tasks without catastrophic forgetting for previously learned tasks. Research on continual learning has led to the development of generative replay, which prevents catastrophic forgetting by generating pseudo-samples for previous tasks and learning them together with new tasks. Inspired by the biological brain, we propose the hippocampal memory indexing to enhance the generative replay by controlling sample generation using compressed features of previous training samples. It enables the generation of a specific training sample from previous tasks, thus improving the balance and quality of generated replay samples. Experimental results indicate that our method effectively controls the sample generation and consistently outperforms the performance of current generative replay methods",
    "volume": "main",
    "checked": true,
    "id": "bb7b6877fdbe459150e3af7b364fe6ba4eddc968",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.66": {
    "title": "A Survey of Multi-task Learning in Natural Language Processing: Regarding Task Relatedness and Training Methods",
    "abstract": "Multi-task learning (MTL) has become increasingly popular in natural language processing (NLP) because it improves the performance of related tasks by exploiting their commonalities and differences. Nevertheless, it is still not understood very well how multi-task learning can be implemented based on the relatedness of training tasks. In this survey, we review recent advances of multi-task learning methods in NLP, with the aim of summarizing them into two general multi-task training methods based on their task relatedness: (i) joint training and (ii) multi-step training. We present examples in various NLP downstream applications, summarize the task relationships and discuss future directions of this promising topic",
    "volume": "main",
    "checked": true,
    "id": "42b103b6b4d4f6ba58d13ab9b3c3f201bb65dd1e",
    "citation_count": 7
  },
  "https://aclanthology.org/2023.eacl-main.67": {
    "title": "Conclusion-based Counter-Argument Generation",
    "abstract": "In real-world debates, the most common way to counter an argument is to reason against its main point, that is, its conclusion. Existing work on the automatic generation of natural language counter-arguments does not address the relation to the conclusion, possibly because many arguments leave their conclusion implicit. In this paper, we hypothesize that the key to effective counter-argument generation is to explicitly model the argument’s conclusion and to ensure that the stance of the generated counter is opposite to that conclusion. In particular, we propose a multitask approach that jointly learns to generate both the conclusion and the counter of an input argument. The approach employs a stance-based ranking component that selects the counter from a diverse set of generated candidates whose stance best opposes the generated conclusion. In both automatic and manual evaluation, we provide evidence that our approach generates more relevant and stance-adhering counters than strong baselines",
    "volume": "main",
    "checked": true,
    "id": "f42ee60999907e40d550550e0d65e10e50c7eab3",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.68": {
    "title": "Question-Answer Sentence Graph for Joint Modeling Answer Selection",
    "abstract": "This research studies graph-based approaches for Answer Sentence Selection (AS2), an essential component for retrieval-based Question Answering (QA) systems. During offline learning, our model constructs a small-scale relevant training graph per question in an unsupervised manner, and integrates with Graph Neural Networks. Graph nodes are question sentence to answer sentence pairs. We train and integrate state-of-the-art (SOTA) models for computing scores between question-question, question-answer, and answer-answer pairs, and use thresholding on relevance scores for creating graph edges. Online inference is then performed to solve the AS2 task on unseen queries. Experiments on two well-known academic benchmarks and a real-world dataset show that our approach consistently outperforms SOTA QA baseline models",
    "volume": "main",
    "checked": true,
    "id": "59318b0606570d156e372939daa5f8f2b4a6aed4",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.69": {
    "title": "Evaluating and Improving the Coreference Capabilities of Machine Translation Models",
    "abstract": "Machine translation (MT) requires a wide range of linguistic capabilities, which current end-to-end models are expected to learn implicitly by observing aligned sentences in bilingual corpora.In this work, we ask: {emph{How well MT models learn coreference resolution via implicit signal?} To answer this question, we develop an evaluation methodology that derives coreference clusters from MT output and evaluates them without requiring annotations in the target language.Following, we evaluate several prominent open-source and commercial MT systems, translating from English to six target languages, and compare them to state-of-the-art coreference resolvers on three challenging benchmarks.Our results show that the monolingual resolvers greatly outperform MT models. Motivated by this result, we experiment with different methods for incorporating the output of coreference resolution models in MT, showing improvement over strong baselines",
    "volume": "main",
    "checked": true,
    "id": "9704c5dc63417d19f7667213a3c2f4a91354e309",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.70": {
    "title": "Document-Level Planning for Text Simplification",
    "abstract": "Most existing work on text simplification is limited to sentence-level inputs, with attempts to iteratively apply these approaches to document-level simplification failing to coherently preserve the discourse structure of the document. We hypothesise that by providing a high-level view of the target document, a simplification plan might help to guide generation. Building upon previous work on controlled, sentence-level simplification, we view a plan as a sequence of labels, each describing one of four sentence-level simplification operations (copy, rephrase, split, or delete). We propose a planning model that labels each sentence in the input document while considering both its context (a window of surrounding sentences) and its internal structure (a token-level representation). Experiments on two simplification benchmarks (Newsela-auto and Wiki-auto) show that our model outperforms strong baselines both on the planning task and when used to guide document-level simplification models",
    "volume": "main",
    "checked": true,
    "id": "91a91cd7f6181e3c92b407d3dd6da955577be753",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.71": {
    "title": "Efficient Hybrid Generation Framework for Aspect-Based Sentiment Analysis",
    "abstract": "Aspect-based sentiment analysis (ABSA) has attracted broad attention due to its commercial value. Natural Language Generation-based (NLG) approaches dominate the recent advance in ABSA tasks. However, current NLG practices are inefficient because most of them directly employ an autoregressive generation framework that cannot efficiently generate location information and semantic representations of ABSA targets. In this paper, we propose a novel framework, namely Efficient Hybrid Generation (EHG) to revolutionize traditions. Specifically, we leverage an Efficient Hybrid Transformer to generate the location and semantic information of ABSA targets in parallel. Besides, we design a novel global hybrid loss function in combination with bipartite matching to achieve end-to-end model training. Extensive experiments demonstrate that our proposed EHG framework outperforms current state-of-the-art methods in almost all cases and outperforms existing NLG-based methods in terms of inference efficiency",
    "volume": "main",
    "checked": true,
    "id": "10b397c145ccdcd979de5f5ea0f3d723ed0a2529",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.72": {
    "title": "What's New? Summarizing Contributions in Scientific Literature",
    "abstract": "With thousands of academic articles shared on a daily basis, it has become increasingly difficult to keep up with the latest scientific findings. To overcome this problem, we introduce a new task of disentangled paper summarization, which seeks to generate separate summaries for the paper contributions and the context of the work, making it easier to identify the key findings shared in articles. For this purpose, we extend the S2ORC corpus of academic articles, which spans a diverse set of domains ranging from economics to psychology, by adding disentangled “contribution” and “context” reference labels. Together with the dataset, we introduce and analyze three baseline approaches: 1) a unified model controlled by input code prefixes, 2) a model with separate generation heads specialized in generating the disentangled outputs, and 3) a training strategy that guides the model using additional supervision coming from inbound and outbound citations. We also propose a comprehensive automatic evaluation protocol which reports the relevance, novelty, and disentanglement of generated outputs. Through a human study involving expert annotators, we show that in 79%, of cases our new task is considered more helpful than traditional scientific paper summarization",
    "volume": "main",
    "checked": true,
    "id": "8dc3d2d6cb0c13d82a70d8155bd9b18805f45be8",
    "citation_count": 5
  },
  "https://aclanthology.org/2023.eacl-main.73": {
    "title": "Find Parent then Label Children: A Two-stage Taxonomy Completion Method with Pre-trained Language Model",
    "abstract": "Taxonomies, which organize domain concepts into hierarchical structures, are crucial for building knowledge systems and downstream applications. As domain knowledge evolves, taxonomies need to be continuously updated to include new concepts.Previous approaches have mainly focused on adding concepts to the leaf nodes of the existing hierarchical tree, which does not fully utilize the taxonomy’s knowledge and is unable to update the original taxonomy structure (usually involving non-leaf nodes). In this paper, we propose a two-stage method called ATTEMPT for taxonomy completion. Our method inserts new concepts into the correct position by finding a parent node and labeling child nodes. Specifically, by combining local nodes with prompts to generate natural sentences, we take advantage of pre-trained language models for hypernym/hyponymy recognition. Experimental results on two public datasets (including six domains) show that ATTEMPT performs best on both taxonomy completion and extension tasks, surpassing existing methods",
    "volume": "main",
    "checked": true,
    "id": "573c3dd5a77dd6790e76689708d37914ea0de497",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.74": {
    "title": "Meta Self-Refinement for Robust Learning with Weak Supervision",
    "abstract": "Training deep neural networks (DNNs) under weak supervision has attracted increasing research attention as it can significantly reduce the annotation cost. However, labels from weak supervision can be noisy, and the high capacity of DNNs enables them to easily overfit the label noise, resulting in poor generalization. Recent methods leverage self-training to build noise-resistant models, in which a teacher trained under weak supervision is used to provide highly confident labels for teaching the students. Nevertheless, the teacher derived from such frameworks may have fitted a substantial amount of noise and therefore produce incorrect pseudo-labels with high confidence, leading to severe error propagation. In this work, we propose Meta Self-Refinement (MSR), a noise-resistant learning framework, to effectively combat label noise from weak supervision. Instead of relying on a fixed teacher trained with noisy labels, we encourage the teacher to refine its pseudo-labels. At each training step, MSR performs a meta gradient descent on the current mini-batch to maximize the student performance on a clean validation set. Extensive experimentation on eight NLP benchmarks demonstrates that MSR is robust against label noise in all settings and outperforms state-of-the-art methods by up to 11.4% in accuracy and 9.26% in F1 score",
    "volume": "main",
    "checked": true,
    "id": "f223abed2285d2472e111e616bf7687b0d08619e",
    "citation_count": 4
  },
  "https://aclanthology.org/2023.eacl-main.75": {
    "title": "Looking for a Needle in a Haystack: A Comprehensive Study of Hallucinations in Neural Machine Translation",
    "abstract": "Although the problem of hallucinations in neural machine translation (NMT) has received some attention, research on this highly pathological phenomenon lacks solid ground. Previous work has been limited in several ways: it often resorts to artificial settings where the problem is amplified, it disregards some (common) types of hallucinations, and it does not validate adequacy of detection heuristics. In this paper, we set foundations for the study of NMT hallucinations. First, we work in a natural setting, i.e., in-domain data without artificial noise neither in training nor in inference. Next, we annotate a dataset of over 3.4k sentences indicating different kinds of critical errors and hallucinations. Then, we turn to detection methods and both revisit methods used previously and propose using glass-box uncertainty-based detectors. Overall, we show that for preventive settings, (i) previously used methods are largely inadequate, (ii) sequence log-probability works best and performs on par with reference-based methods. Finally, we propose DeHallucinator, a simple method for alleviating hallucinations at test time that significantly reduces the hallucinatory rate",
    "volume": "main",
    "checked": true,
    "id": "193fc3bc9f200ccb28892c02979e6c2068a85138",
    "citation_count": 5
  },
  "https://aclanthology.org/2023.eacl-main.76": {
    "title": "Investigating UD Treebanks via Dataset Difficulty Measures",
    "abstract": "Treebanks annotated with Universal Dependencies (UD) are currently available for over 100 languages and are widely utilized by the community. However, their inherent characteristics are hard to measure and are only partially reflected in parser evaluations via accuracy metrics like LAS. In this study, we analyze a large subset of the UD treebanks using three recently proposed accuracy-free dataset analysis methods: dataset cartography, ${mathcal{V}$-information, and minimum description length. Each method provides insights about UD treebanks that would remain undetected if only LAS was considered. Specifically, we identify a number of treebanks that, despite yielding high LAS, contain very little information that is usable by a parser to surpass what can be achieved by simple heuristics. Furthermore, we make note of several treebanks that score consistently low across numerous metrics, indicating a high degree of noise or annotation inconsistency present therein",
    "volume": "main",
    "checked": true,
    "id": "5d038f093ee7e4a1f9591121ffe8653a5763bc59",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.77": {
    "title": "On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex",
    "abstract": "Semantic parsing is a technique aimed at constructing a structured representation of the meaning of a natural-language question. Recent advances in language models trained on code have shown superior performance in generating these representations compared to language models trained solely on natural language text. The existing fine-tuned neural semantic parsers are vulnerable to adversarial attacks on natural-language inputs. While it has been established that the robustness of smaller semantic parsers can be enhanced through adversarial training, this approach is not feasible for large language models in real-world scenarios, as it requires both substantial computational resources and expensive human annotation on in-domain semantic parsing data. This paper presents the first empirical study on the adversarial robustness of a prompt-based semantic parser based on CODEX, a stateof-the-art (SOTA) language model trained on code. Our results demonstrate that the large language model of code is vulnerable to carefully crafted adversarial examples. To overcome this challenge, we propose methods for enhancing robustness without requiring substantial amounts of labelled data or intensive computational resources",
    "volume": "main",
    "checked": true,
    "id": "1c91b23d78944f7f237cb512029c2165972ae9d5",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.78": {
    "title": "Leveraging Task Dependency and Contrastive Learning for Case Outcome Classification on European Court of Human Rights Cases",
    "abstract": "We report on an experiment in case outcome classification on European Court of Human Rights cases where our model first learns to identify the convention articles allegedly violated by the state from case facts descriptions, and subsequently uses that information to classify whether the court finds a violation of those articles. We assess the dependency between these two tasks at the feature and outcome level. Furthermore, we leverage a hierarchical contrastive loss to pull together article-specific representations of cases at the higher level, leading to distinctive article clusters. The cases in each article cluster are further pulled closer based on their outcome, leading to sub-clusters of cases with similar outcomes. Our experiment results demonstrate that, given a static pre-trained encoder, our models produce a small but consistent improvement in classification performance over single-task and joint models without contrastive loss",
    "volume": "main",
    "checked": true,
    "id": "ff28f812113a7082f7d285ed3bf6dcbed49d0320",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.79": {
    "title": "Semi-supervised Relation Extraction via Data Augmentation and Consistency-training",
    "abstract": "Due to the semantic complexity of the Relation extraction (RE) task, obtaining high-quality human labelled data is an expensive and noisy process. To improve the sample efficiency of the models, semi-supervised learning (SSL) methods aim to leverage unlabelled data in addition to learning from limited labelled data points. Recently, strong data augmentation combined with consistency-based semi-supervised learning methods have advanced the state of the art in several SSL tasks. However, adapting these methods to the RE task has been challenging due to the difficulty of data augmentation for RE. In this work, we leverage the recent advances in controlled text generation to perform high-quality data augmentation for the RE task. We further introduce small but significant changes to model architecture that allows for generation of more training data by interpolating different data points in their latent space. These data augmentations along with consistency training result in very competitive results for semi-supervised relation extraction on four benchmark datasets",
    "volume": "main",
    "checked": true,
    "id": "56adc86cc6649ecb0ebb9451e454a0e4ba4db73b",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.80": {
    "title": "Event Temporal Relation Extraction with Bayesian Translational Model",
    "abstract": "Existing models to extract temporal relations between events lack a principled method to incorporate external knowledge. In this study, we introduce Bayesian-Trans, a Bayesian learning-based method that models the temporal relation representations as latent variables and infers their values via Bayesian inference and translational functions. Compared to conventional neural approaches, instead of performing point estimation to find the best set parameters, the proposed model infers the parameters’ posterior distribution directly, enhancing the model’s capability to encode and express uncertainty about the predictions. Experimental results on the three widely used datasets show that Bayesian-Trans outperforms existing approaches for event temporal relation extraction. We additionally present detailed analyses on uncertainty quantification, comparison of priors, and ablation studies, illustrating the benefits of the proposed approach",
    "volume": "main",
    "checked": true,
    "id": "63cb0939c285f4fa7e61117528ac66a697be4303",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.81": {
    "title": "Persona Expansion with Commonsense Knowledge for Diverse and Consistent Response Generation",
    "abstract": "Generating diverse and consistent responses is the ultimate goal of a persona-based dialogue. Although many studies have been conducted, the generated responses tend to be generic and bland due to the personas’ limited descriptiveness. Therefore, it is necessary to expand the given personas for more attractive responses. However, indiscriminate expansion of personas threaten the consistency of responses and therefore reduce the interlocutor’s interest in conversation. To alleviate this issue, we propose a consistent persona expansion framework that improves not only the diversity but also the consistency of persona-based responses. To do so, we define consistency criteria to avoid possible contradictions among personas as follows: 1) Intra-Consistency and 2) Inter-Consistency. Then, we construct a silver profile dataset to deliver the ability to conform with the consistency criteria to the expansion model. Finally, we propose a persona expansion model with an encoder-decoder structure, which considers the relatedness and consistency among personas. Our experiments on the Persona-Chat dataset demonstrate the superiority of the proposed framework",
    "volume": "main",
    "checked": true,
    "id": "73a7d46f4369753985d0125663d67f3d89416196",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.82": {
    "title": "UnifEE: Unified Evidence Extraction for Fact Verification",
    "abstract": "FEVEROUS is a fact extraction and verification task that requires systems to extract evidence of both sentences and table cells from a Wikipedia dump, then predict the veracity of the given claim accordingly. Existing works extract evidence in the two formats separately, ignoring potential connections between them. In this paper, we propose a Unified Evidence Extraction model (UnifEE), which uses a mixed evidence graph to extract the evidence in both formats. With the carefully-designed unified evidence graph, UnifEE allows evidence interactions among all candidates in both formats at similar granularity. Experiments show that, with information aggregated from related evidence candidates in the fusion graph, UnifEE can make better decisions about which evidence should be kept, especially for claims requiring multi-hop reasoning or a combination of tables and texts. Thus it outperforms all previous evidence extraction methods and brings significant improvement in the subsequent claim verification step",
    "volume": "main",
    "checked": true,
    "id": "26e308bd48eb544bf13f901b7fb2a4fa2f1194de",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.83": {
    "title": "MiniALBERT: Model Distillation via Parameter-Efficient Recursive Transformers",
    "abstract": "Pre-trained Language Models (LMs) have become an integral part of Natural Language Processing (NLP) in recent years, due to their superior performance in downstream applications. In spite of this resounding success, the usability of LMs is constrained by computational and time complexity, along with their increasing size; an issue that has been referred to as overparameterisation. Different strategies have been proposed in the literature to alleviate these problems, with the aim to create effective compact models that nearly match the performance of their bloated counterparts with negligible performance losses. One of the most popular techniques in this area of research is model distillation. Another potent but underutilised technique is cross-layer parameter sharing. In this work, we combine these two strategies and present MiniALBERT, a technique for converting the knowledge of fully parameterised LMs (such as BERT) into a compact recursive student. In addition, we investigate the application of bottleneck adapters for layer-wise adaptation of our recursive student, and also explore the efficacy of adapter tuning for fine-tuning of compact models. We test our proposed models on a number of general and biomedical NLP tasks to demonstrate their viability and compare them with the state-of-the-art and other existing compact models. All the codes used in the experiments and the pre-trained compact models will be made publicly available",
    "volume": "main",
    "checked": true,
    "id": "640ae44e26587915fa9920705d726d44cc5f98b2",
    "citation_count": 3
  },
  "https://aclanthology.org/2023.eacl-main.84": {
    "title": "Multilingual Normalization of Temporal Expressions with Masked Language Models",
    "abstract": "The detection and normalization of temporal expressions is an important task and preprocessing step for many applications. However, prior work on normalization is rule-based, which severely limits the applicability in real-world multilingual settings, due to the costly creation of new rules. We propose a novel neural method for normalizing temporal expressions based on masked language modeling. Our multilingual method outperforms prior rule-based systems in many languages, and in particular, for low-resource languages with performance improvements of up to 33 F1 on average compared to the state of the art",
    "volume": "main",
    "checked": true,
    "id": "b7d62a0bab6ca7420be99e5952fac0f9e936da81",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.85": {
    "title": "K-hop neighbourhood regularization for few-shot learning on graphs: A case study of text classification",
    "abstract": "We present FewShotTextGCN, a novel method designed to effectively utilize the properties of word-document graphs for improved learning in low-resource settings. We introduce K-hop Neighbourhood Regularization, a regularizer for heterogeneous graphs, and show that it stabilizes and improves learning when only a few training samples are available. We furthermore propose a simplification in the graph-construction method, which results in a graph that is ∼7 times less dense and yields better performance in little-resource settings while performing on par with the state of the art in high-resource settings. Finally, we introduce a new variant of Adaptive Pseudo-Labeling tailored for word-document graphs. When using as little as 20 samples for training, we outperform a strong TextGCN baseline with 17% in absolute accuracy on average over eight languages. We demonstrate that our method can be applied to document classification without any language model pretraining on a wide range of typologically diverse languages while performing on par with large pretrained language models",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.86": {
    "title": "What Clued the AI Doctor In? On the Influence of Data Source and Quality for Transformer-Based Medical Self-Disclosure Detection",
    "abstract": "Recognizing medical self-disclosure is important in many healthcare contexts, but it has been under-explored by the NLP community. We conduct a three-pronged investigation of this task. We (1) manually expand and refine the only existing medical self-disclosure corpus, resulting in a new, publicly available dataset of 3,919 social media posts with clinically validated labels and high compatibility with the existing task-specific protocol. We also (2) study the merits of pretraining task domain and text style by comparing Transformer-based models for this task, pretrained from general, medical, and social media sources. Our BERTweet condition outperforms the existing state of the art for this task by a relative F1 score increase of 16.73%. Finally, we (3) compare data augmentation techniques for this task, to assess the extent to which medical self-disclosure data may be further synthetically expanded. We discover that this task poses many challenges for data augmentation techniques, and we provide an in-depth analysis of identified trends",
    "volume": "main",
    "checked": true,
    "id": "327faf86e547bb53acaadb7cd830acec172a470c",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.87": {
    "title": "Improving Visual-Semantic Embedding with Adaptive Pooling and Optimization Objective",
    "abstract": "Visual-Semantic Embedding (VSE) aims to learn an embedding space where related visual and semantic instances are close to each other. Recent VSE models tend to design complex structures to pool visual and semantic features into fixed-length vectors and use hard triplet loss for optimization. However, we find that: (1) combining simple pooling methods is no worse than these sophisticated methods; and (2) only considering the most difficult-to-distinguish negative sample leads to slow convergence and poor Recall@K improvement. To this end, we propose an adaptive pooling strategy that allows the model to learn how to aggregate features through a combination of simple pooling methods. We also introduce a strategy to dynamically select a group of negative samples to make the optimization converge faster and perform better. Experimental results on Flickr30K and MS-COCO demonstrate that a standard VSE using our pooling and optimization strategies outperforms current state-of-the-art systems (at least 1.0{% on the metrics of recall) in image-to-text and text-to-image retrieval. Source code of our experiments is available at https://github.com/96-Zachary/vse_2ad",
    "volume": "main",
    "checked": true,
    "id": "9cc1cd6d4ebdb6dc23b052f097b8af2e45c9b9f7",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.88": {
    "title": "Policy-based Reinforcement Learning for Generalisation in Interactive Text-based Environments",
    "abstract": "Text-based environments enable RL agents to learn to converse and perform interactive tasks through natural language. However, previous RL approaches applied to text-based environments show poor performance when evaluated on unseen games. This paper investigates the improvement of generalisation performance through the simple switch from a value-based update method to a policy-based one, within text-based environments. We show that by replacing commonly used value-based methods with REINFORCE with baseline, a far more general agent is produced. The policy-based agent is evaluated on Coin Collector and Question Answering with interactive text (QAit), two text-based environments designed to test zero-shot performance. We see substantial improvements on a variety of zero-shot evaluation experiments, including tripling accuracy on various QAit benchmark configurations. The results indicate that policy-based RL has significantly better generalisation capabilities than value-based methods within such text-based environments, suggesting that RL agents could be applied to more complex natural language environments",
    "volume": "main",
    "checked": true,
    "id": "4c8bde56047ec2a9b2f4564eb32ec49144a6c36b",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.89": {
    "title": "Logic Against Bias: Textual Entailment Mitigates Stereotypical Sentence Reasoning",
    "abstract": "Due to their similarity-based learning objectives, pretrained sentence encoders often internalize stereotypical assumptions that reflect the social biases that exist within their training corpora. In this paper, we describe several kinds of stereotypes concerning different communities that are present in popular sentence representation models, including pretrained next sentence prediction and contrastive sentence representation models. We compare such models to textual entailment models that learn language logic for a variety of downstream language understanding tasks. By comparing strong pretrained models based on text similarity with textual entailment learning, we conclude that the explicit logic learning with textual entailment can significantly reduce bias and improve the recognition of social communities, without an explicit de-biasing process",
    "volume": "main",
    "checked": true,
    "id": "b3dcd48b68bdbb304fa53299496539c054638e0c",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.90": {
    "title": "Entity Tracking via Effective Use of Multi-Task Learning Model and Mention-guided Decoding",
    "abstract": "Cross-task knowledge transfer via multi-task learning has recently made remarkable progress in general NLP tasks. However, entity tracking on the procedural text has not benefited from such knowledge transfer because of its distinct formulation, i.e., tracking the event flow while following structural constraints. State-of-the-art entity tracking approaches either design complicated model architectures or rely on task-specific pre-training to achieve good results. To this end, we propose MeeT, a Multi-task learning-enabled entity Tracking approach, which utilizes knowledge gained from general domain tasks to improve entity tracking. Specifically, MeeT first fine-tunes T5, a pre-trained multi-task learning model, with entity tracking-specialized QA formats, and then employs our customized decoding strategy to satisfy the structural constraints. MeeT achieves state-of-the-art performances on two popular entity tracking datasets, even though it does not require any task-specific architecture design or pre-training",
    "volume": "main",
    "checked": true,
    "id": "631d9ca7ddd89c89c51df3a0cb8f91252fe15efc",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.91": {
    "title": "Conversational Tree Search: A New Hybrid Dialog Task",
    "abstract": "Conversational interfaces provide a flexible and easy way for users to seek information that may otherwise be difficult or inconvenient to obtain. However, existing interfaces generally fall into one of two categories: FAQs, where users must have a concrete question in order to retrieve a general answer, or dialogs, where users must follow a pre-defined path but may receive a personalized answer. In this paper, we introduce Conversational Tree Search (CTS) as a new task that bridges the gap between FAQ-style information retrieval and task-oriented dialog, allowing domain-experts to define dialog trees which can then be converted to an efficient dialog policy that learns only to ask the questions necessary to navigate a user to their goal.We collect a dataset for the travel reimbursement domain and demonstrate a baseline as well as a novel deep Reinforcement Learning architecture for this task. Our results show that the new architecture combines the positive aspects of both the FAQ and dialog system used in the baseline and achieves higher goal completion while skipping unnecessary questions",
    "volume": "main",
    "checked": true,
    "id": "c1edcea80a56e0e6e11668fc86b446af75e36ca7",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.92": {
    "title": "A Human Subject Study of Named Entity Recognition in Conversational Music Recommendation Queries",
    "abstract": "We conducted a human subject study of named entity recognition on a noisy corpus of conversational music recommendation queries, with many irregular and novel named entities. We evaluated the human NER linguistic behaviour in these challenging conditions and compared it with the most common NER systems nowadays, fine-tuned transformers. Our goal was to learn about the task to guide the design of better evaluation methods and NER algorithms. The results showed that NER in our context was quite hard for both human and algorithms under a strict evaluation schema; humans had higher precision, while the model higher recall because of entity exposure especially during pre-training; and entity types had different error patterns (e.g. frequent typing errors for artists). The released corpus goes beyond predefined frames of interaction and can support future work in conversational music recommendation",
    "volume": "main",
    "checked": true,
    "id": "f1522d8ba3980f3c51586f15b39b38ee77662b23",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.93": {
    "title": "Entity Disambiguation with Entity Definitions",
    "abstract": "Local models have recently attained astounding performances in Entity Disambiguation (ED), with generative and extractive formulations being the most promising research directions. However, previous works have so far limited their studies to using, as the textual representation of each candidate, only its Wikipedia title. Although certainly effective, this strategy presents a few critical issues, especially when titles are not sufficiently informative or distinguishable from one another. In this paper, we address this limitation and investigate the extent to which more expressive textual representations can mitigate it. We evaluate our approach thoroughly against standard benchmarks in ED and find extractive formulations to be particularly well-suited to such representations. We report a new state of the art on 2 out of the 6 benchmarks we consider and strongly improve the generalization capability over unseen patterns. We release our code, data and model checkpoints at https://github.com/SapienzaNLP/extend",
    "volume": "main",
    "checked": true,
    "id": "637bf7152547fde890118ee57463b499f0956f67",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.94": {
    "title": "Exploring Paracrawl for Document-level Neural Machine Translation",
    "abstract": "Document-level neural machine translation (NMT) has outperformed sentence-level NMT on a number of datasets. However, document-level NMT is still not widely adopted in realworld translation systems mainly due to the lack of large-scale general-domain training data for document-level NMT. We examine the effectiveness of using Paracrawl for learning document-level translation. Paracrawl is a large-scale parallel corpus crawled from the Internet and contains data from various domains. The official Paracrawl corpus was released as parallel sentences (extracted from parallel webpages) and therefore previous works only used Paracrawl for learning sentence-level translation. In this work, we extract parallel paragraphs from Paracrawl parallel webpages using automatic sentence alignments and we use the extracted parallel paragraphs as parallel documents for training document-level translation models. We show that document-level NMT models trained with only parallel paragraphs from Paracrawl can be used to translate real documents from TED, News and Europarl, outperforming sentence-level NMT models. We also perform a targeted pronoun evaluation and show that document-level models trained with Paracrawl data can help context-aware pronoun translation",
    "volume": "main",
    "checked": true,
    "id": "0e2d8e6bfeb483d0df4fa903ff254c474e3cdd12",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.95": {
    "title": "Poor Man's Quality Estimation: Predicting Reference-Based MT Metrics Without the Reference",
    "abstract": "Machine translation quality estimation (QE) predicts human judgements of a translation hypothesis without seeing the reference. State-of-the-art QE systems based on pretrained language models have been achieving remarkable correlations with human judgements yet they are computationally heavy and require human annotations, which are slow and expensive to create. To address these limitations, we define the problem of metric estimation (ME) where one predicts the automated metric scores also without the reference. We show that even without access to the reference, our model can estimate automated metrics (ρ = 60% for BLEU, ρ = 51% for other metrics) at the sentence-level. Because automated metrics correlate with human judgements, we can leverage the ME task for pre-training a QE model. For the QE task, we find that pre-training on TER is better (ρ = 23%) than training for scratch (ρ = 20%)",
    "volume": "main",
    "checked": true,
    "id": "5150857ca3f5526b26d724cdb70605668e3268fe",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.96": {
    "title": "Integrating Translation Memories into Non-Autoregressive Machine Translation",
    "abstract": "Non-autoregressive machine translation (NAT) has recently made great progress. However, most works to date have focused on standard translation tasks, even though some edit-based NAT models, such as the Levenshtein Transformer (LevT), seem well suited to translate with a Translation Memory (TM). This is the scenario considered here. We first analyze the vanilla LevT model and explain why it does not do well in this setting. We then propose a new variant, TM-LevT, and show how to effectively train this model. By modifying the data presentation and introducing an extra deletion operation, we obtain performance that are on par with an autoregressive approach, while reducing the decoding load. We also show that incorporating TMs during training dispenses to use knowledge distillation, a well-known trick used to mitigate the multimodality issue",
    "volume": "main",
    "checked": true,
    "id": "7824eaa89e264ae58c4a191296a891b07a8c3cbe",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.97": {
    "title": "Shorten the Long Tail for Rare Entity and Event Extraction",
    "abstract": "The distribution of knowledge elements such as entity types and event types is long-tailed in natural language. Hence information extraction datasets naturally conform long-tailed distribution. Although imbalanced datasets can teach the model about the useful real-world bias, deep learning models may learn features not generalizable to rare or unseen expressions of entities or events during evaluation, especially for rare types without sufficient training instances. Existing approaches for the long-tailed learning problem seek to manipulate the training data by re-balancing, augmentation or introducing extra prior knowledge. In comparison, we propose to handle the generalization challenge by making the evaluation instances closer to the frequent training cases. We design a new transformation module that transforms infrequent candidate mention representation during evaluation with the average mention representation in the training dataset. Experimental results on classic benchmarks on three entity or event extraction datasets demonstrates the effectiveness of our framework",
    "volume": "main",
    "checked": true,
    "id": "3208266146715ea8a5b2eb7d9bc655b63a030011",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.98": {
    "title": "Do Deep Neural Networks Capture Compositionality in Arithmetic Reasoning?",
    "abstract": "Compositionality is a pivotal property of symbolic reasoning. However, how well recent neural models capture compositionality remains underexplored in the symbolic reasoning tasks. This study empirically addresses this question by systematically examining recently published pre-trained seq2seq models with a carefully controlled dataset of multi-hop arithmetic symbolic reasoning. We introduce a skill tree on compositionality in arithmetic symbolic reasoning that defines the hierarchical levels of complexity along with three compositionality dimensions: systematicity, productivity, and substitutivity. Our experiments revealed that among the three types of composition, the models struggled most with systematicity, performing poorly even with relatively simple compositions. That difficulty was not resolved even after training the models with intermediate reasoning steps",
    "volume": "main",
    "checked": true,
    "id": "da345b189e4faaaa489f7319640868a37a3932a1",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.99": {
    "title": "BLM-AgrF: A New French Benchmark to Investigate Generalization of Agreement in Neural Networks",
    "abstract": "Successful machine learning systems currently rely on massive amounts of data, which are very effective in hiding some of the shallowness of the learned models. To help train models with more complex and compositional skills, we need challenging data, on which a system is successful only if it detects structure and regularities, that will allow it to generalize. In this paper, we describe a French dataset (BLM-AgrF) for learning the underlying rules of subject-verb agreement in sentences, developed in the BLM framework, a new task inspired by visual IQ tests known as Raven’s Progressive Matrices. In this task, an instance consists of sequences of sentences with specific attributes. To predict the correct answer as the next element of the sequence, a model must correctly detect the generative model used to produce the dataset. We provide details and share a dataset built following this methodology. Two exploratory baselines based on commonly used architectures show that despite the simplicity of the phenomenon, it is a complex problem for deep learning systems",
    "volume": "main",
    "checked": true,
    "id": "40d836bee6eeaa9bb706ceb1b5481fa66fa8104c",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.100": {
    "title": "Robustification of Multilingual Language Models to Real-world Noise in Crosslingual Zero-shot Settings with Robust Contrastive Pretraining",
    "abstract": "Advances in neural modeling have achieved state-of-the-art (SOTA) results on public natural language processing (NLP) benchmarks, at times surpassing human performance. However, there is a gap between public benchmarks and real-world applications where noise, such as typographical or grammatical mistakes, is abundant and can result in degraded performance. Unfortunately, works which evaluate the robustness of neural models on noisy data and propose improvements, are limited to the English language. Upon analyzing noise in different languages, we observe that noise types vary greatly across languages. Thus, existing investigations do not generalize trivially to multilingual settings. To benchmark the performance of pretrained multilingual language models, we construct noisy datasets covering five languages and four NLP tasks and observe a clear gap in the performance between clean and noisy data in the zero-shot cross-lingual setting. After investigating several ways to boost the robustness of multilingual models in this setting, we propose Robust Contrastive Pretraining (RCP). RCP combines data augmentation with a contrastive loss term at the pretraining stage and achieves large improvements on noisy (and original test data) across two sentence-level (+3.2%) and two sequence-labeling (+10 F1-score) multilingual classification tasks",
    "volume": "main",
    "checked": true,
    "id": "7bd8859b5920c7b769e6d40dbdbcd857c1770401",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.101": {
    "title": "Unsupervised Anomaly Detection in Multi-Topic Short-Text Corpora",
    "abstract": "Unsupervised anomaly detection seeks to identify deviant data samples in a dataset without using labels and constitutes a challenging task, particularly when the majority class is heterogeneous. This paper addresses this topic for textual data and aims to determine whether a text sample is an outlier within a potentially multi-topic corpus. To this end, it is crucial to grasp the semantic aspects of words, particularly when dealing with short texts, since it is difficult to syntactically discriminate data samples based only on a few words. Thereby we make use of word embeddings to represent each sample by a dense vector, efficiently capturing the underlying semantics. Then, we rely on the Mixture Model approach to detect which samples deviate the most from the underlying distributions of the corpus. Experiments carried out on real datasets show the effectiveness of the proposed approach in comparison to state-of-the-art techniques both in terms of performance and time efficiency, especially when more than one topic is present in the corpus",
    "volume": "main",
    "checked": true,
    "id": "d42f66f55cd58336074b20a0fb8d9c77629b75ef",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.102": {
    "title": "Metaphor Detection with Effective Context Denoising",
    "abstract": "We propose a novel RoBERTa-based model, RoPPT, which introduces a target-oriented parse tree structure in metaphor detection. Compared to existing models, RoPPT focuses on semantically relevant information and achieves the state-of-the-art on several main metaphor datasets. We also compare our approach against several popular denoising and pruning methods, demonstrating the effectiveness of our approach in context denoising. Our code and dataset can be found at https://github.com/MajiBear000/RoPPT",
    "volume": "main",
    "checked": true,
    "id": "e9a8aca060793ec6bf11ca8d57796a885e7b567b",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.103": {
    "title": "Low-Resource Compositional Semantic Parsing with Concept Pretraining",
    "abstract": "Semantic parsing plays a key role in digital voice assistants such as Alexa, Siri, and Google Assistant by mapping natural language to structured meaning representations. When we want to improve the capabilities of a voice assistant by adding a new domain, the underlying semantic parsing model needs to be retrained using thousands of annotated examples from the new domain, which is time-consuming and expensive. In this work, we present an architecture to perform such domain adaptation automatically, with only a small amount of metadata about the new domain and without any new training data (zero-shot) or with very few examples (few-shot). We use a base seq2seq (sequence-to-sequence) architecture and augment it with a concept encoder that encodes intent and slot tags from the new domain. We also introduce a novel decoder-focused approach to pretrain seq2seq models to be concept aware using Wikidata and use it to help our model learn important concepts and perform well in low-resource settings. We report few-shot and zero-shot results for compositional semantic parsing on the TOPv2 dataset and show that our model outperforms prior approaches in few-shot settings for the TOPv2 and SNIPS datasets",
    "volume": "main",
    "checked": true,
    "id": "cd31009d1163ef892b45b21973d1daf9b254eb7a",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.104": {
    "title": "Made of Steel? Learning Plausible Materials for Components in the Vehicle Repair Domain",
    "abstract": "We propose a novel approach to learn domain-specific plausible materials for components in the vehicle repair domain by probing Pretrained Language Models (PLMs) in a cloze task style setting to overcome the lack of annotated datasets. We devise a new method to aggregate salient predictions from a set of cloze query templates and show that domain-adaptation using either a small, high-quality or a customized Wikipedia corpus boosts performance. When exploring resource-lean alternatives, we find a distilled PLM clearly outperforming a classic pattern-based algorithm. Further, given that 98% of our domain-specific components are multiword expressions, we successfully exploit the compositionality assumption as a way to address data sparsity",
    "volume": "main",
    "checked": true,
    "id": "e6905d21852a67c1f825226849f055e643a2046a",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.105": {
    "title": "Self-Adapted Utterance Selection for Suicidal Ideation Detection in Lifeline Conversations",
    "abstract": "This paper investigates a crucial aspect of mental health by exploring the detection of suicidal ideation in spoken phone conversations between callers and counselors at a suicide prevention hotline.These conversations can be lengthy, noisy, and cover a broad range of topics, making it challenging for NLP models to accurately identify the caller’s suicidal ideation. To address these difficulties, we introduce a novel, self-adaptive approach that identifies the most critical utterances that the NLP model can more easily distinguish. The experiments use real-world Lifeline transcriptions, expertly labeled, and show that our approach outperforms the baseline models in overall performance with an F-score of 66.01%. In detecting the most dangerous cases, our approach achieves a significantly higher F-score of 65.94% compared to the baseline models, an improvement of 8.9%. The selected utterances can also provide valuable insights for suicide prevention research. Furthermore, our approach demonstrates its versatility by showing its effectiveness in sentiment analysis, making it a valuable tool for NLP applications beyond the healthcare domain",
    "volume": "main",
    "checked": true,
    "id": "4db51212790387eac2efb254bc2fb7d045ca44a0",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.106": {
    "title": "Can Pretrained Language Models (Yet) Reason Deductively?",
    "abstract": "Acquiring factual knowledge with Pretrained Language Models (PLMs) has attracted increasing attention, showing promising performance in many knowledge-intensive tasks. Their good performance has led the community to believe that the models do possess a modicum of reasoning competence rather than merely memorising the knowledge. In this paper, we conduct a comprehensive evaluation of the learnable deductive (also known as explicit) reasoning capability of PLMs. Through a series of controlled experiments, we posit two main findings. 1) PLMs inadequately generalise learned logic rules and perform inconsistently against simple adversarial surface form edits. 2) While the deductive reasoning fine-tuning of PLMs does improve their performance on reasoning over unseen knowledge facts, it results in catastrophically forgetting the previously learnt knowledge. Our main results suggest that PLMs cannot yet perform reliable deductive reasoning, demonstrating the importance of controlled examinations and probing of PLMs’ deductive reasoning abilities; we reach beyond (misleading) task performance, revealing that PLMs are still far from robust reasoning capabilities, even for simple deductive tasks",
    "volume": "main",
    "checked": true,
    "id": "66f333c51e2bfa25380069f66500b491218da9c3",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.107": {
    "title": "Selective In-Context Data Augmentation for Intent Detection using Pointwise V-Information",
    "abstract": "This work focuses on in-context data augmentation for intent detection. Having found that augmentation via in-context prompting of large pre-trained language models (PLMs) alone does not improve performance, we introduce a novel approach based on PLMs and pointwise V-information (PVI), a metric that can measure the usefulness of a datapoint for training a model. Our method first fine-tunes a PLM on a small seed of training data and then synthesizes new datapoints - utterances that correspond to given intents. It then employs intent-aware filtering, based on PVI, to remove datapoints that are not helpful to the downstream intent classifier. Our method is thus able to leverage the expressive power of large language models to produce diverse training data. Empirical results demonstrate that our method can produce synthetic training data that achieve state-of-the-art performance on three challenging intent detection datasets under few-shot settings (1.28% absolute improvement in 5-shot and 1.18% absolute in 10-shot, on average) and perform on par with the state-of-the-art in full-shot settings (within 0.01% absolute, on average)",
    "volume": "main",
    "checked": true,
    "id": "027ec9a2aaa81b01d190e8607b2250779e5834dd",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.108": {
    "title": "Multilingual Representation Distillation with Contrastive Learning",
    "abstract": "Multilingual sentence representations from large models encode semantic information from two or more languages and can be used for different cross-lingual information retrieval and matching tasks. In this paper, we integrate contrastive learning into multilingual representation distillation and use it for quality estimation of parallel sentences (i.e., find semantically similar sentences that can be used as translations of each other). We validate our approach with multilingual similarity search and corpus filtering tasks. Experiments across different low-resource languages show that our method greatly outperforms previous sentence encoders such as LASER, LASER3, and LaBSE",
    "volume": "main",
    "checked": true,
    "id": "560263c83671b831ed61bf3c7a31436d3a4bb446",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.109": {
    "title": "On the inconsistency of separable losses for structured prediction",
    "abstract": "In this paper, we prove that separable negative log-likelihood losses for structured prediction are not necessarily Bayes consistent, that is minimizing these losses may not result in a model that predicts the most probable structure in the data distribution for a given input.This fact opens the question of whether these losses are well-adapted for structured prediction and, if so, why",
    "volume": "main",
    "checked": true,
    "id": "e6d5da1e496d79f8dd31f6747950250e4ed3596c",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.110": {
    "title": "A Systematic Search for Compound Semantics in Pretrained BERT Architectures",
    "abstract": "To date, transformer-based models such as BERT have been less successful in predicting compositionality of noun compounds than static word embeddings. This is likely related to a suboptimal use of the encoded information, reflecting an incomplete grasp of how the models represent the meanings of complex linguistic structures. This paper investigates variants of semantic knowledge derived from pretrained BERT when predicting the degrees of compositionality for 280 English noun compounds associated with human compositionality ratings. Our performance strongly improves on earlier unsupervised implementations of pretrained BERT and highlights beneficial decisions in data preprocessing, embedding computation, and compositionality estimation. The distinct linguistic roles of heads and modifiers are reflected by differences in BERT-derived representations, with empirical properties such as frequency, productivity, and ambiguity affecting model performance. The most relevant representational information is concentrated in the initial layers of the model architecture",
    "volume": "main",
    "checked": true,
    "id": "da53f28b2d0be86f9892685debae9934fe30b222",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.111": {
    "title": "Efficiently Upgrading Multilingual Machine Translation Models to Support More Languages",
    "abstract": "With multilingual machine translation (MMT) models continuing to grow in size and number of supported languages, it is natural to reuse and upgrade existing models to save computation as data becomes available in more languages. However, adding new languages requires updating the vocabulary, which complicates the reuse of embeddings. The question of how to reuse existing models while also making architectural changes to provide capacity for both old and new languages has also not been closely studied. In this work, we introduce three techniques that help speed up the effective learning of new languages and alleviate catastrophic forgetting despite vocabulary and architecture mismatches. Our results show that by (1) carefully initializing the network, (2) applying learning rate scaling, and (3) performing data up-sampling, it is possible to exceed the performance of a same-sized baseline model with 30{% computation and recover the performance of a larger model trained from scratch with over 50{% reduction in computation. Furthermore, our analysis reveals that the introduced techniques help learn new directions more effectively and alleviate catastrophic forgetting at the same time. We hope our work will guide research into more efficient approaches to growing languages for these MMT models and ultimately maximize the reuse of existing models",
    "volume": "main",
    "checked": true,
    "id": "c15d46f7b8421a5d252e1711cf1684571dd17a68",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.112": {
    "title": "Summarize and Generate to Back-translate: Unsupervised Translation of Programming Languages",
    "abstract": "Back-translation is widely known for its effectiveness in neural machine translation when there is little to no parallel data. In this approach, a source-to-target model is coupled with a target-to-source model trained in parallel. The target-to-source model generates noisy sources, while the source-to-target model is trained to reconstruct the targets and vice versa. Recent developments of multilingual pre-trained sequence-to-sequence models for programming languages have been very effective for a broad spectrum of downstream software engineering tasks. Hence, training them to build programming language translation systems via back-translation is compelling. However, these models cannot be further trained via back-translation since they learn to output sequences in the same language as the inputs during pre-training. As an alternative, we propose performing back-translation via code summarization and generation. In code summarization, a model learns to generate natural language (NL) summaries given code snippets. In code generation, the model learns to do the opposite. Therefore, target-to-source generation in back-translation can be viewed as a target-to-NL-to-source generation. We show that our proposed approach performs competitively with state-of-the-art methods. We have made the code publicly available",
    "volume": "main",
    "checked": true,
    "id": "636f854b1a3a983e6803eae0277179596cc2cb95",
    "citation_count": 4
  },
  "https://aclanthology.org/2023.eacl-main.113": {
    "title": "The Impacts of Unanswerable Questions on the Robustness of Machine Reading Comprehension Models",
    "abstract": "Pretrained language models have achieved super-human performances on many Machine Reading Comprehension (MRC) benchmarks. Nevertheless, their relative inability to defend against adversarial attacks has spurred skepticism about their natural language understanding. In this paper, we ask whether training with unanswerable questions in SQuAD 2.0 can help improve the robustness of MRC models against adversarial attacks. To explore that question, we fine-tune three state-of-the-art language models on either SQuAD 1.1 or SQuAD 2.0 and then evaluate their robustness under adversarial attacks. Our experiments reveal that current models fine-tuned on SQuAD 2.0 do not initially appear to be any more robust than ones fine-tuned on SQuAD 1.1, yet they reveal a measure of hidden robustness that can be leveraged to realize actual performance gains. Furthermore, we find that robustness of models fine-tuned on SQuAD 2.0 extends on additional out-of-domain datasets. Finally, we introduce a new adversarial attack to reveal of SQuAD 2.0 that current MRC models are learning",
    "volume": "main",
    "checked": true,
    "id": "fdb77d28f5a3170086b2df3c56646af89b4bace9",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.114": {
    "title": "FrameBERT: Conceptual Metaphor Detection with Frame Embedding Learning",
    "abstract": "In this paper, we propose FrameBERT, a BERT-based model that can explicitly learn and incorporate FrameNet Embeddings for concept-level metaphor detection. FrameBERT not only achieves better or comparable performance to the state-of-the-art, but also is more explainable and interpretable compared to existing models, attributing to its ability of accounting for external knowledge of FrameNet",
    "volume": "main",
    "checked": true,
    "id": "768f154c74bae2d8f0bf3e49a1b955d5f1c91f39",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.115": {
    "title": "Towards More Efficient Insertion Transformer with Fractional Positional Encoding",
    "abstract": "Auto-regressive neural sequence models have been shown to be effective across text generation tasks. However, their left-to-right decoding order prevents generation from being parallelized. Insertion Transformer (Stern et al., 2019) is an attractive alternative that allows outputting multiple tokens in a single generation step. Nevertheless, due to the incompatibility between absolute positional encoding and insertion-based generation schemes, it needs to refresh the encoding of every token in the generated partial hypothesis at each step, which could be costly. We design a novel reusable positional encoding scheme for Insertion Transformers called Fractional Positional Encoding (FPE), which allows reusing representations calculated in previous steps. Empirical studies on various text generation tasks demonstrate the effectiveness of FPE, which leads to floating-point operation reduction and latency improvements on batched decoding",
    "volume": "main",
    "checked": true,
    "id": "9816ddd36859eb80e4d22e83f080f3e69ddc708d",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.116": {
    "title": "SODAPOP: Open-Ended Discovery of Social Biases in Social Commonsense Reasoning Models",
    "abstract": "A common limitation of diagnostic tests for detecting social biases in NLP models is that they may only detect stereotypic associations that are pre-specified by the designer of the test. Since enumerating all possible problematic associations is infeasible, it is likely these tests fail to detect biases that are present in a model but not pre-specified by the designer. To address this limitation, we propose SODAPOP (SOcial bias Discovery from Answers about PeOPle), an approach for automatic social bias discovery in social commonsense question-answering. The SODAPOP pipeline generates modified instances from the Social IQa dataset (Sap et al., 2019b) by (1) substituting names associated with different demographic groups, and (2) generating many distractor answers from a masked language model. By using a social commonsense model to score the generated distractors, we are able to uncover the model’s stereotypic associations between demographic groups and an open set of words. We also test SODAPOP on debiased models and show the limitations of multiple state-of-the-art debiasing algorithms",
    "volume": "main",
    "checked": true,
    "id": "4dcef4d040cdbc17eb8e7e39d1456c2a1ab691a0",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.117": {
    "title": "Augmenting Pre-trained Language Models with QA-Memory for Open-Domain Question Answering",
    "abstract": "Existing state-of-the-art methods for open-domain question-answering (ODQA) use an open book approach in which information is first retrieved from a large text corpus or knowledge base (KB) and then reasoned over to produce an answer. A recent alternative is to retrieve from a collection of previously-generated question-answer pairs; this has several practical advantages including being more memory and compute-efficient. Question-answer pairs are also appealing in that they can be viewed as an intermediate between text and KB triples: like KB triples, they often concisely express a single relationship, but like text, have much higher coverage than traditional KBs. In this work, we describe a new QA system that augments a text-to-text model with a large memory of question-answer pairs, and a new pre-training task for the latent step of question retrieval. The pre-training task substantially simplifies training and greatly improves performance on smaller QA benchmarks. Unlike prior systems of this sort, our QA system can also answer multi-hop questions that do not explicitly appear in the collection of stored question-answer pairs",
    "volume": "main",
    "checked": true,
    "id": "9fc3a0e96f2fa6bc7a92a96d61c68f7cb09f0a8f",
    "citation_count": 11
  },
  "https://aclanthology.org/2023.eacl-main.118": {
    "title": "Gold Doesn't Always Glitter: Spectral Removal of Linear and Nonlinear Guarded Attribute Information",
    "abstract": "We describe a simple and effective method (Spectral Attribute removaL; SAL) to remove private or guarded information from neural representations. Our method uses matrix decomposition to project the input representations into directions with reduced covariance with the guarded information rather than maximal covariance as factorization methods normally use. We begin with linear information removal and proceed to generalize our algorithm to the case of nonlinear information removal using kernels. Our experiments demonstrate that our algorithm retains better main task performance after removing the guarded information compared to previous work. In addition, our experiments demonstrate that we need a relatively small amount of guarded attribute data to remove information about these attributes, which lowers the exposure to sensitive data and is more suitable for low-resource scenarios",
    "volume": "main",
    "checked": true,
    "id": "7f72da613f413660c2808e0e71cb4e157ac3b78a",
    "citation_count": 5
  },
  "https://aclanthology.org/2023.eacl-main.119": {
    "title": "CTC Alignments Improve Autoregressive Translation",
    "abstract": "Connectionist Temporal Classification (CTC) is a widely used approach for automatic speech recognition (ASR) that performs conditionally independent monotonic alignment. However for translation, CTC exhibits clear limitations due to the contextual and non-monotonic nature of the task and thus lags behind attentional decoder approaches in terms of translation quality. In this work, we argue that CTC does in fact make sense for translation if applied in a joint CTC/attention framework wherein CTC’s core properties can counteract several key weaknesses of pure-attention models during training and decoding. To validate this conjecture, we modify the Hybrid CTC/Attention model originally proposed for ASR to support text-to-text translation (MT) and speech-to-text translation (ST). Our proposed joint CTC/attention models outperform pure-attention baselines across six benchmark translation tasks",
    "volume": "main",
    "checked": true,
    "id": "ee0d1cdd807b54712f25e2060f4eeda8206f3324",
    "citation_count": 7
  },
  "https://aclanthology.org/2023.eacl-main.120": {
    "title": "Modelling Temporal Document Sequences for Clinical ICD Coding",
    "abstract": "Past studies on the ICD coding problem focus on predicting clinical codes primarily based on the discharge summary. This covers only a small fraction of the notes generated during each hospital stay and leaves potential for improving performance by analysing all the available clinical notes. We propose a hierarchical transformer architecture that uses text across the entire sequence of clinical notes in each hospital stay for ICD coding, and incorporates embeddings for text metadata such as their position, time, and type of note. While using all clinical notes increases the quantity of data substantially, superconvergence can be used to reduce training costs. We evaluate the model on the MIMIC-III dataset. Our model exceeds the prior state-of-the-art when using only discharge summaries as input, and achieves further performance improvements when all clinical notes are used as input",
    "volume": "main",
    "checked": true,
    "id": "fefe6c2eb25da9f9f7982b8718f3abc1de2ada03",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.121": {
    "title": "LongEval: Guidelines for Human Evaluation of Faithfulness in Long-form Summarization",
    "abstract": "While human evaluation remains best practice for accurately judging the faithfulness of automatically-generated summaries, few solutions exist to address the increased difficulty and workload when evaluating long-form summaries. Through a survey of 162 papers on long-form summarization, we first shed light on current human evaluation practices surrounding long-form summaries. We find that 73% of these papers do not perform any human evaluation on model-generated summaries, while other works face new difficulties that manifest when dealing with long documents (e.g., low inter-annotator agreement). Motivated by our survey, we present LongEval, a set of guidelines for human evaluation of faithfulness in long-form summaries that addresses the following challenges: (1) How can we achieve high inter-annotator agreement on faithfulness scores? (2) How can we minimize annotator workload while maintaining accurate faithfulness scores? and (3) Do humans benefit from automated alignment between summary and source snippets? We deploy LongEval in annotation studies on two long-form summarization datasets in different domains (SQuALITY and PubMed), and we find that switching to a finer granularity of judgment (e.g., clause-level) reduces inter-annotator variance in faithfulness scores (e.g., std-dev from 18.5 to 6.8). We also show that scores from a partial annotation of fine-grained units highly correlates with scores from a full annotation workload (0.89 Kendall’s tau using 50% judgements). We release our human judgments, annotation templates, and software as a Python library for future research",
    "volume": "main",
    "checked": true,
    "id": "f1f6c61ed0b80a785e4e5d0d97a454dbe6126c63",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.122": {
    "title": "Cluster-Guided Label Generation in Extreme Multi-Label Classification",
    "abstract": "For extreme multi-label classification (XMC), existing classification-based models poorly per- form for tail labels and often ignore the semantic relations among labels, like treating”Wikipedia” and “Wiki” as independent and separate labels. In this paper, we cast XMC as a generation task (XLGen), where we benefit from pre-trained text-to-text models. However, generating labels from the extremely large label space is challenging without any constraints or guidance. We, therefore, propose to guide label generation using label cluster information to hierarchically generate lower-level labels. We also find that frequency-based label ordering and using decoding ensemble methods are critical factors for the improvements in XLGen. XLGen with cluster guidance significantly outperforms the classification and generation baselines on tail labels, and also generally improves the overall performance in four popular XMC benchmarks. In human evaluation, we also find XLGen generates unseen but plausible labels. Our code is now available at https:// github.com/alexa/xlgen-eacl-2023",
    "volume": "main",
    "checked": true,
    "id": "2d227f3a0203c2491bcf7c77ae594cea3a7caf89",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.123": {
    "title": "Empathy Identification Systems are not Accurately Accounting for Context",
    "abstract": "Understanding empathy in text dialogue data is a difficult, yet critical, skill for effective human-machine interaction. In this work, we ask whether systems are making meaningful progress on this challenge. We consider a simple model that checks if an input utterance is similar to a small set of empathetic examples. Crucially, the model does not look at what the utterance is a response to, i.e., the dialogue context. This model performs comparably to other work on standard benchmarks and even outperforms state-of-the-art models for empathetic rationale extraction by 16.7 points on T-F1 and 4.3 on IOU-F1. This indicates that current systems rely on the surface form of the response, rather than whether it is suitable in context. To confirm this, we create examples with dialogue contexts that change the interpretation of the response and show that current systems continue to label utterances as empathetic. We discuss the implications of our findings, including improvements for empathetic benchmarks and how our model can be an informative baseline",
    "volume": "main",
    "checked": true,
    "id": "5da8ef8573a1504804314f612c97bf7ab6a563df",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.124": {
    "title": "Enhancing Multi-Document Summarization with Cross-Document Graph-based Information Extraction",
    "abstract": "Information extraction (IE) and summarization are closely related, both tasked with presenting a subset of the information contained in a natural language text. However, while IE extracts structural representations, summarization aims to abstract the most salient information into a generated text summary – thus potentially encountering the technical limitations of current text generation methods (e.g., hallucination). To mitigate this risk, this work uses structured IE graphs to enhance the abstractive summarization task. Specifically, we focus on improving Multi-Document Summarization (MDS) performance by using cross-document IE output, incorporating two novel components: (1) the use of auxiliary entity and event recognition systems to focus the summary generation model; (2) incorporating an alignment loss between IE nodes and their text spans to reduce inconsistencies between the IE graphs and text representations. Operationally, both the IE nodes and corresponding text spans are projected into the same embedding space and pairwise distance is minimized. Experimental results on multiple MDS benchmarks show that summaries generated by our model are more factually consistent with the source documents than baseline models while maintaining the same level of abstractiveness",
    "volume": "main",
    "checked": true,
    "id": "3a6f161a17bb031659dcd2167630e6909eefab92",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.125": {
    "title": "What happens before and after: Multi-Event Commonsense in Event Coreference Resolution",
    "abstract": "Event coreference models cluster event mentions pertaining to the same real-world event. Recent models rely on contextualized representations to recognize coreference among lexically or contextually similar mentions. However, models typically fail to leverage commonsense inferences, which is particularly limiting for resolving lexically-divergent mentions. We propose a model that extends event mentions with temporal commonsense inferences. Given a complex sentence with multiple events, e.g., “the man killed his wife and got arrested”, with the target event “arrested”, our model generates plausible events that happen before the target event – such as “the police arrived”, and after it, such as “he was sentenced”. We show that incorporating such inferences into an existing event coreference model improves its performance, and we analyze the coreferences in which such temporal knowledge is required",
    "volume": "main",
    "checked": true,
    "id": "d1b704fc4fe451c2a2f63d9118d444da71a9ebbd",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.126": {
    "title": "Multi-Modal Bias: Introducing a Framework for Stereotypical Bias Assessment beyond Gender and Race in Vision–Language Models",
    "abstract": "Recent breakthroughs in self-supervised training have led to a new class of pretrained vision–language models. While there have been investigations of bias in multimodal models, they have mostly focused on gender and racial bias, giving much less attention to other relevant groups, such as minorities with regard to religion, nationality, sexual orientation, or disabilities. This is mainly due to lack of suitable benchmarks for such groups. We seek to address this gap by providing a visual and textual bias benchmark called MMBias, consisting of around 3,800 images and phrases covering 14 population subgroups. We utilize this dataset to assess bias in several prominent self-supervised multimodal models, including CLIP, ALBEF, and ViLT. Our results show that these models demonstrate meaningful bias favoring certain groups. Finally, we introduce a debiasing method designed specifically for such large pretrained models that can be applied as a post-processing step to mitigate bias, while preserving the remaining accuracy of the model",
    "volume": "main",
    "checked": true,
    "id": "a5bc3c0bce8d105a6b95f999fed4ea59c342cb1d",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.127": {
    "title": "CylE: Cylinder Embeddings for Multi-hop Reasoning over Knowledge Graphs",
    "abstract": "Recent geometric-based approaches have been shown to efficiently model complex logical queries (including the intersection operation) over Knowledge Graphs based on the natural representation of Venn diagram. Existing geometric-based models (using points, boxes embeddings), however, cannot handle the logical negation operation. Further, those using cones embeddings are limited to representing queries by two-dimensional shapes, which reduced their effectiveness in capturing entities query relations for correct answers. To overcome this challenge, we propose unbounded cylinder embeddings (namely CylE), which is a novel geometric-based model based on three-dimensional shapes. Our approach can handle a complete set of basic first-order logic operations (conjunctions, disjunctions and negations). CylE considers queries as Cartesian products of unbounded sector-cylinders and consider a set of nearest boxes corresponds to the set of answer entities. Precisely, the conjunctions can be represented via the intersections of unbounded sector-cylinders. Transforming queries to Disjunctive Normal Form can handle queries with disjunctions. The negations can be represented by considering the closure of complement for an arbitrary unbounded sector-cylinder. Empirical results show that the performance of multi-hop reasoning task using CylE significantly increases over state-of-the-art geometric-based query embedding models for queries without negation. For queries with negation operations, though the performance is on a par with the best performing geometric-based model, CylE significantly outperforms a recent distribution-based model",
    "volume": "main",
    "checked": true,
    "id": "e61027622cd92a7a8b1ed99f406d3c7ab0dda57e",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.128": {
    "title": "Fiction-Writing Mode: An Effective Control for Human-Machine Collaborative Writing",
    "abstract": "We explore the idea of incorporating concepts from writing skills curricula into human-machine collaborative writing scenarios, focusing on adding writing modes as a control for text generation models. Using crowd-sourced workers, we annotate a corpus of narrative text paragraphs with writing mode labels. Classifiers trained on this data achieve an average accuracy of ~87% on held-out data. We fine-tune a set of large language models to condition on writing mode labels, and show that the generated text is recognized as belonging to the specified mode with high accuracy.To study the ability of writing modes to provide fine-grained control over generated text, we devise a novel turn-based text reconstruction game to evaluate the difference between the generated text and the author’s intention. We show that authors prefer text suggestions made by writing mode-controlled models on average 61.1% of the time, with satisfaction scores 0.5 higher on a 5-point ordinal scale. When evaluated by humans, stories generated via collaboration with writing mode-controlled models achieve high similarity with the professionally written target story. We conclude by identifying the most common mistakes found in the generated stories",
    "volume": "main",
    "checked": true,
    "id": "d12955816cb4c3145b61ceac1869bf72f4d9dfea",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.129": {
    "title": "Robustness Challenges in Model Distillation and Pruning for Natural Language Understanding",
    "abstract": "Recent work has focused on compressing pre-trained language models (PLMs) like BERT where the major focus has been to improve the in-distribution performance for downstream tasks. However, very few of these studies have analyzed the impact of compression on the generalizability and robustness of compressed models for out-of-distribution (OOD) data. Towards this end, we study two popular model compression techniques including knowledge distillation and pruning and show that the compressed models are significantly less robust than their PLM counterparts on OOD test sets although they obtain similar performance on in-distribution development sets for a task. Further analysis indicates that the compressed models overfit on the shortcut samples and generalize poorly on the hard ones. We further leverage this observation to develop a regularization strategy for robust model compression based on sample uncertainty",
    "volume": "main",
    "checked": true,
    "id": "8ffe4d395d6b02d65a2301e0104c8c91c2187928",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.130": {
    "title": "Don't Blame the Annotator: Bias Already Starts in the Annotation Instructions",
    "abstract": "In recent years, progress in NLU has been driven by benchmarks. These benchmarks are typically collected by crowdsourcing, where annotators write examples based on annotation instructions crafted by dataset creators. In this work, we hypothesize that annotators pick up on patterns in the crowdsourcing instructions, which bias them to write many similar examples that are then over-represented in the collected data. We study this form of bias, termed instruction bias, in 14 recent NLU benchmarks, showing that instruction examples often exhibit concrete patterns, which are propagated by crowdworkers to the collected data. This extends previous work (Geva et al., 2019) and raises a new concern of whether we are modeling the dataset creator’s instructions, rather than the task. Through a series of experiments, we show that, indeed, instruction bias can lead to overestimation of model performance, and that models struggle to generalize beyond biases originating in the crowdsourcing instructions. We further analyze the influence of instruction bias in terms of pattern frequency and model size, and derive concrete recommendations for creating future NLU benchmarks",
    "volume": "main",
    "checked": true,
    "id": "3a6a97a50695d43d95a015bbb554b2bc0d40394e",
    "citation_count": 12
  },
  "https://aclanthology.org/2023.eacl-main.131": {
    "title": "Performance Prediction via Bayesian Matrix Factorisation for Multilingual Natural Language Processing Tasks",
    "abstract": "Performance prediction for Natural Language Processing (NLP) seeks to reduce the experimental burden resulting from the myriad of different evaluation scenarios, e.g., the combination of languages used in multilingual transfer. In this work, we explore the framework ofBayesian matrix factorisation for performance prediction, as many experimental settings in NLP can be naturally represented in matrix format. Our approach outperforms the state-of-the-art in several NLP benchmarks, including machine translation and cross-lingual entity linking. Furthermore, it also avoids hyperparameter tuning and is able to provide uncertainty estimates over predictions",
    "volume": "main",
    "checked": true,
    "id": "7a2d039e92ddbb13e5c47076f764c169bd0bcc89",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.132": {
    "title": "Unified Neural Topic Model via Contrastive Learning and Term Weighting",
    "abstract": "Two types of topic modeling predominate: generative methods that employ probabilistic latent models and clustering methods that identify semantically coherent groups. This paper newly presents UTopic (Unified neural Topic model via contrastive learning and term weighting) that combines the advantages of these two types. UTopic uses contrastive learning and term weighting to learn knowledge from a pretrained language model and discover influential terms from semantically coherent clusters. Experiments show that the generated topics have a high-quality topic-word distribution in terms of topic coherence, outperforming existing baselines across multiple topic coherence measures. We demonstrate how our model can be used as an add-on to existing topic models and improve their performance",
    "volume": "main",
    "checked": true,
    "id": "b68f430419a55796d4fb501e7c0201dc45f575a1",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.133": {
    "title": "Don't Mess with Mister-in-Between: Improved Negative Search for Knowledge Graph Completion",
    "abstract": "The best methods for knowledge graph completion use a ‘dual-encoding’ framework, a form of neural model with a bottleneck that facilitates fast approximate search over a vast collection of candidates. These approaches are trained using contrastive learning to differentiate between known positive examples and sampled negative instances. The mechanism for sampling negatives to date has been very simple, driven by pragmatic engineering considerations (e.g., using mismatched instances from the same batch). We propose several novel means of finding more informative negatives, based on searching for candidates with high lexical overlaps, from the dual-encoder model and according to knowledge graph structures. Experimental results on four benchmarks show that our best single model improves consistently over previous methods and obtains new state-of-the-art performance, including the challenging large-scale Wikidata5M dataset. Combing different kinds of strategies through model ensembling results in a further performance boost",
    "volume": "main",
    "checked": true,
    "id": "5c170117ceb4c0b4e332e42c2256254d520b7ec4",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.134": {
    "title": "Semantic Frame Induction with Deep Metric Learning",
    "abstract": "Recent studies have demonstrated the usefulness of contextualized word embeddings in unsupervised semantic frame induction. However, they have also revealed that generic contextualized embeddings are not always consistent with human intuitions about semantic frames, which causes unsatisfactory performance for frame induction based on contextualized embeddings. In this paper, we address supervised semantic frame induction, which assumes the existence of frame-annotated data for a subset of predicates in a corpus and aims to build a frame induction model that leverages the annotated data. We propose a model that uses deep metric learning to fine-tune a contextualized embedding model, and we apply the fine-tuned contextualized embeddings to perform semantic frame induction. Our experiments on FrameNet show that fine-tuning with deep metric learning considerably improves the clustering evaluation scores, namely, the B-cubed F-score and Purity F-score, by about 8 points or more. We also demonstrate that our approach is effective even when the number of training instances is small",
    "volume": "main",
    "checked": true,
    "id": "6299bf080920e596f6c3c769caef7a6a0f7d859f",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.135": {
    "title": "The Devil is in the Details: On Models and Training Regimes for Few-Shot Intent Classification",
    "abstract": "In task-oriented dialog (ToD) new intents emerge on regular basis, with a handful of available utterances at best. This renders effective Few-Shot Intent Classification (FSIC) a central challenge for modular ToD systems. Recent FSIC methods appear to be similar: they use pretrained language models (PLMs) to encode utterances and predominantly resort to nearest-neighbor-based inference. However, they also differ in major components: they start from different PLMs, use different encoding architectures and utterance similarity functions, and adopt different training regimes.Coupling of these vital components together with the lack of informative ablations prevents the identification of factors that drive the (reported) FSIC performance. We propose a unified framework to evaluate these components along the following key dimensions:(1) Encoding architectures: Cross-Encoder vs Bi-Encoders;(2) Similarity function: Parameterized (i.e., trainable) vs non-parameterized; (3) Training regimes: Episodic meta-learning vs conventional (i.e., non-episodic) training. Our experimental results on seven FSIC benchmarks reveal three new important findings. First, the unexplored combination of cross-encoder architecture and episodic meta-learning consistently yields the best FSIC performance. Second, episodic training substantially outperforms its non-episodic counterpart. Finally, we show that splitting episodes into support and query sets has a limited and inconsistent effect on performance. Our findings show the importance of ablations and fair comparisons in FSIC. We publicly release our code and data",
    "volume": "main",
    "checked": true,
    "id": "c9d03b250a69858c56d4dfa5b187550d8e2453cb",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.136": {
    "title": "Iterative Document-level Information Extraction via Imitation Learning",
    "abstract": "We present a novel iterative extraction model, IterX, for extracting complex relations, or templates, i.e., N-tuples representing a mapping from named slots to spans of text within a document. Documents may feature zero or more instances of a template of any given type, and the task of template extraction entails identifying the templates in a document and extracting each template’s slot values. Our imitation learning approach casts the problem as a Markov decision process (MDP), and relieves the need to use predefined template orders to train an extractor. It leads to state-of-the-art results on two established benchmarks – 4-ary relation extraction on SciREX and template extraction on MUC-4 – as well as a strong baseline on the new BETTER Granular task",
    "volume": "main",
    "checked": true,
    "id": "60e98c3fedfde46cbd8b90ba6fb182f2e5879ed8",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.137": {
    "title": "CLICK: Contrastive Learning for Injecting Contextual Knowledge to Conversational Recommender System",
    "abstract": "Conversational recommender systems (CRSs) capture a user preference through a conversation. However, the existing CRSs lack capturing comprehensive user preferences. This is because the items mentioned in a conversation are mainly regarded as a user preference. Thus, they have limitations in identifying a user preference from a dialogue context expressed without preferred items. Inspired by the characteristic of an online recommendation community where participants identify a context of a recommendation request and then comment with appropriate items, we exploit the Reddit data. Specifically, we propose a Contrastive Learning approach for Injecting Contextual Knowledge (CLICK) from the Reddit data to the CRS task, which facilitates the capture of a context-level user preference from a dialogue context, regardless of the existence of preferred item-entities. Moreover, we devise a relevance-enhanced contrastive learning loss to consider the fine-grained reflection of multiple recommendable items. We further develop a response generation module to generate a persuasive rationale for a recommendation. Extensive experiments on the benchmark CRS dataset show the effectiveness of CLICK, achieving significant improvements over state-of-the-art methods",
    "volume": "main",
    "checked": true,
    "id": "a8ea55218e0d49b710608c647d7926465782824c",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.138": {
    "title": "LEALLA: Learning Lightweight Language-agnostic Sentence Embeddings with Knowledge Distillation",
    "abstract": "Large-scale language-agnostic sentence embedding models such as LaBSE (Feng et al., 2022) obtain state-of-the-art performance for parallel sentence alignment. However, these large-scale models can suffer from inference speed and computation overhead. This study systematically explores learning language-agnostic sentence embeddings with lightweight models. We demonstrate that a thin-deep encoder can construct robust low-dimensional sentence embeddings for 109 languages. With our proposed distillation methods, we achieve further improvements by incorporating knowledge from a teacher model. Empirical results on Tatoeba, United Nations, and BUCC show the effectiveness of our lightweight models. We release our lightweight language-agnostic sentence embedding models LEALLA on TensorFlow Hub",
    "volume": "main",
    "checked": true,
    "id": "6b329940d7fa70c3de66de25cbe2e36f52f644f8",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.139": {
    "title": "Synthesizing Human Gaze Feedback for Improved NLP Performance",
    "abstract": "Integrating human feedback in models can improve the performance of natural language processing (NLP) models. Feedback can be either explicit (e.g. ranking used in training language models) or implicit (e.g. using human cognitive signals in the form of eyetracking). Prior eye tracking and NLP research reveal that cognitive processes, such as human scanpaths, gleaned from human gaze patterns aid in the understanding and performance of NLP models. However, the collection of real eyetracking data for NLP tasks is challenging due to the requirement of expensive and precise equipment coupled with privacy invasion issues. To address this challenge, we propose ScanTextGAN, a novel model for generating human scanpaths over text. We show that ScanTextGAN-generated scanpaths can approximate meaningful cognitive signals in human gaze patterns. We include synthetically generated scanpaths in four popular NLP tasks spanning six different datasets as proof of concept and show that the models augmented with generated scanpaths improve the performance of all downstream NLP tasks",
    "volume": "main",
    "checked": true,
    "id": "c3349d0493a56f76c27d74c5056ab039f665b7b4",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.140": {
    "title": "Memory-efficient Temporal Moment Localization in Long Videos",
    "abstract": "Temporal Moment Localization is a challenging multi-modal task which aims to identify the start and end timestamps of a moment of interest in an input untrimmed video, given a query in natural language. Solving this task correctly requires understanding the temporal relationships in the entire input video, but processing such long inputs and reasoning about them is memory and computationally expensive. In light of this issue, we propose Stochastic Bucket-wise Feature Sampling (SBFS), a stochastic sampling module that allows methods to process long videos at a constant memory footprint. We further combine SBFS with a new consistency loss to propose Locformer, a Transformer-based model that can process videos as long as 18 minutes. We test our proposals on relevant benchmark datasets, showing that not only can Locformer achieve excellent results, but also that our sampling is more effective than competing counterparts. Concretely, SBFS consistently improves the performance of prior work, by up to 3.13{% in the mean temporal IoU, leading to a new state-of-the-art performance on Charades-STA and YouCookII, while also obtaining up to 12.8x speed-up at testing time and reducing memory requirements by up to 5x",
    "volume": "main",
    "checked": true,
    "id": "9db0747365b48b49aeae6cff0b990ff31d3f35d3",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.141": {
    "title": "Extracting Victim Counts from Text",
    "abstract": "Decision-makers in the humanitarian sector rely on timely and exact information during crisis events. Knowing how many civilians were injured during an earthquake is vital to allocate aids properly. Information about such victim counts are however often only available within full-text event descriptions from newspapers and other reports. Extracting numbers from text is challenging: numbers have different formats and may require numeric reasoning. This renders purely tagging approaches insufficient. As a consequence, fine-grained counts of injured, displaced, or abused victims beyond fatalities are often not extracted and remain unseen. We cast victim count extraction as a question answering (QA) task with a regression or classification objective. We compare tagging approaches: regex, dependency parsing, semantic role labeling, and advanced text-to-text models. Beyond model accuracy, we analyze extraction reliability and robustness which are key for this sensitive task. In particular, we discuss model calibration and investigate out-of-distribution and few-shot performance. Ultimately, we make a comprehensive recommendation on which model to select for different desiderata and data domains. Our work is among the first to apply numeracy-focused large language models in a real-world use case with a positive impact",
    "volume": "main",
    "checked": true,
    "id": "032962bfc563e2caf986d3b3746f3217303be1b0",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.142": {
    "title": "ConEntail: An Entailment-based Framework for Universal Zero and Few Shot Classification with Supervised Contrastive Pretraining",
    "abstract": "A universal classification model aims to generalize to diverse classification tasks in both zero and few shot settings. A promising way toward universal classification is to cast heterogeneous data formats into a dataset-agnostic “meta-task” (e.g., textual entailment, question answering) then pretrain a model on the combined meta dataset. The existing work is either pretrained on specific subsets of classification tasks, or pretrained on both classification and generation data but the model could not fulfill its potential in universality and reliability. These also leave a massive amount of annotated data under-exploited. To fill these gaps, we propose ConEntail, a new framework for universal zero and few shot classification with supervised contrastive pretraining. Our unified meta-task for classification is based on nested entailment. It can be interpreted as “Does sentence a entails [sentence b entails label c]”. This formulation enables us to make better use of 57 annotated classification datasets for supervised contrastive pretraining and universal evaluation. In this way, ConEntail helps the model (1) absorb knowledge from different datasets, and (2) gain consistent performance gain with more pretraining data. In experiments, we compare our model with discriminative and generative models pretrained on the same dataset. The results confirm that our framework effectively exploits existing annotated data and consistently outperforms baselines in both zero (9.4% average improvement) and few shot settings (3.5% average improvement). Our code is available in supplementary materials",
    "volume": "main",
    "checked": true,
    "id": "27f4bb532826f028d0602925f3ebba674236574c",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.143": {
    "title": "Guide the Learner: Controlling Product of Experts Debiasing Method Based on Token Attribution Similarities",
    "abstract": "Several proposals have been put forward in recent years for improving out-of-distribution (OOD) performance through mitigating dataset biases. A popular workaround is to train a robust model by re-weighting training examples based on a secondary biased model. Here, the underlying assumption is that the biased model resorts to shortcut features. Hence, those training examples that are correctly predicted by the biased model are flagged as being biased and are down-weighted during the training of the main model. However, assessing the importance of an instance merely based on the predictions of the biased model may be too naive. It is possible that the prediction of the main model can be derived from another decision-making process that is distinct from the behavior of the biased model. To circumvent this, we introduce a fine-tuning strategy that incorporates the similarity between the main and biased model attribution scores in a Product of Experts (PoE) loss function to further improve OOD performance. With experiments conducted on natural language inference and fact verification benchmarks, we show that our method improves OOD results while maintaining in-distribution (ID) performance",
    "volume": "main",
    "checked": true,
    "id": "7d4252f038164f019694a04ef49373d9640e10ef",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.144": {
    "title": "Task and Sentiment Adaptation for Appraisal Tagging",
    "abstract": "The Appraisal framework in linguistics defines the framework for fine-grained evaluations and opinions and has contributed to sentiment analysis and opinion mining. As developing appraisal-annotated resources requires tagging of several dimensions with distinct semantic taxonomies, it has been primarily conducted manually by human experts through expensive and time-consuming processes. In this paper, we study how to automatically identify and annotate text segments for appraisal. We formulate the problem as a sequence tagging problem and propose novel task and sentiment adapters based on language models for appraisal tagging. Our model, named Adaptive Appraisal (A$ˆ2$), achieves superior performance than baseline adapter-based models and other neural classification models, especially for cross-domain and cross-language settings. Source code for A$ˆ2$ is available at: https://github.com/ltian678/AA-code.git",
    "volume": "main",
    "checked": true,
    "id": "98f7607cab97329dd24510c70f2cc9b2e546effd",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.145": {
    "title": "DREEAM: Guiding Attention with Evidence for Improving Document-Level Relation Extraction",
    "abstract": "Document-level relation extraction (DocRE) is the task of identifying all relations between each entity pair in a document. Evidence, defined as sentences containing clues for the relationship between an entity pair, has been shown to help DocRE systems focus on relevant texts, thus improving relation extraction. However, evidence retrieval (ER) in DocRE faces two major issues: high memory consumption and limited availability of annotations. This work aims at addressing these issues to improve the usage of ER in DocRE. First, we propose DREEAM, a memory-efficient approach that adopts evidence information as the supervisory signal, thereby guiding the attention modules of the DocRE system to assign high weights to evidence. Second, we propose a self-training strategy for DREEAM to learn ER from automatically-generated evidence on massive data without evidence annotations. Experimental results reveal that our approach exhibits state-of-the-art performance on the DocRED benchmark for both DocRE and ER. To the best of our knowledge, DREEAM is the first approach to employ ER self-training",
    "volume": "main",
    "checked": true,
    "id": "51237757750c4618bf114ff329dacb9c997b771a",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.146": {
    "title": "Span-based Named Entity Recognition by Generating and Compressing Information",
    "abstract": "The information bottleneck (IB) principle has been proven effective in various NLP applications.The existing work, however, only used either generative or information compression models to improve the performance of the target task. In this paper, we propose to combine the two types of IB models into one system to enhance Named Entity Recognition (NER).For one type of IB model, we incorporate two unsupervised generative components, span reconstruction and synonym generation, into a span-based NER system.The span reconstruction ensures that the contextualised span representation keeps the span information, while the synonym generation makes synonyms have similar representations even in different contexts. For the other type of IB model, we add a supervised IB layer that performs information compression into the system to preserve useful features for NER in the resulting span representations.Experiments on five different corpora indicate that jointly training both generative and information compression models can enhance the performance of the baseline span-based NER system.Our source code is publicly available at https://github.com/nguyennth/joint-ib-models",
    "volume": "main",
    "checked": true,
    "id": "dd4238d23fea6fc8e9232ca0fec4cad728f213ea",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.147": {
    "title": "An In-depth Analysis of Implicit and Subtle Hate Speech Messages",
    "abstract": "The research carried out so far in detecting abusive content in social media has primarily focused on overt forms of hate speech. While explicit hate speech (HS) is more easily identifiable by recognizing hateful words, messages containing linguistically subtle and implicit forms of HS (as circumlocution, metaphors and sarcasm) constitute a real challenge for automatic systems. While the sneaky and tricky nature of subtle messages might be perceived as less hurtful with respect to the same content expressed clearly, such abuse is at least as harmful as overt abuse. In this paper, we first provide an in-depth and systematic analysis of 7 standard benchmarks for HS detection, relying on a fine-grained and linguistically-grounded definition of implicit and subtle messages. Then, we experiment with state-of-the-art neural network architectures on two supervised tasks, namely implicit HS and subtle HS message classification. We show that while such models perform satisfactory on explicit messages, they fail to detect implicit and subtle content, highlighting the fact that HS detection is not a solved problem and deserves further investigation",
    "volume": "main",
    "checked": true,
    "id": "7bddbdb2a2f35841916b565c8b4e59aa6bb0a5da",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.148": {
    "title": "MTEB: Massive Text Embedding Benchmark",
    "abstract": "Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings todate. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-theart results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb",
    "volume": "main",
    "checked": true,
    "id": "88a74e972898de887ad9587d4c87c3a9f03f1dc5",
    "citation_count": 4
  },
  "https://aclanthology.org/2023.eacl-main.149": {
    "title": "Step by Step Loss Goes Very Far: Multi-Step Quantization for Adversarial Text Attacks",
    "abstract": "We propose a novel gradient-based attack against transformer-based language models that searches for an adversarial example in a continuous space of tokens probabilities. Our algorithm mitigates the gap between adversarial loss for continuous and discrete text representations by performing multi-step quantization in a quantization-compensation loop. Experiments show that our method significantly outperforms other approaches on various natural language processing (NLP) tasks",
    "volume": "main",
    "checked": true,
    "id": "a1a9179cf6c8bb09238407e0daba28ffd51cccda",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.150": {
    "title": "TwiRGCN: Temporally Weighted Graph Convolution for Question Answering over Temporal Knowledge Graphs",
    "abstract": "Recent years have witnessed interest in Temporal Question Answering over Knowledge Graphs (TKGQA), resulting in the development of multiple methods. However, these are highly engineered, thereby limiting their generalizability, and they do not automatically discover relevant parts of the KG during multi-hop reasoning. Relational graph convolutional networks (RGCN) provide an opportunity to address both of these challenges – we explore this direction in the paper. Specifically, we propose a novel, intuitive and interpretable scheme to modulate the messages passed through a KG edge during convolution based on the relevance of its associated period to the question. We also introduce a gating device to predict if the answer to a complex temporal question is likely to be a KG entity or time and use this prediction to guide our scoring mechanism. We evaluate the resulting system, which we call TwiRGCN, on a recent challenging dataset for multi-hop complex temporal QA called TimeQuestions. We show that TwiRGCN significantly outperforms state-of-the-art models on this dataset across diverse question types. Interestingly, TwiRGCN improves accuracy by 9–10 percentage points for the most difficult ordinal and implicit question types",
    "volume": "main",
    "checked": true,
    "id": "831b350b6f665379c819a2c0535f17d7205cea0e",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.151": {
    "title": "ZELDA: A Comprehensive Benchmark for Supervised Entity Disambiguation",
    "abstract": "Entity disambiguation (ED) is the task of disambiguating named entity mentions in text to unique entries in a knowledge base. Due to its industrial relevance, as well as current progress in leveraging pre-trained language models, a multitude of ED approaches have been proposed in recent years. However, we observe a severe lack of uniformity across experimental setups in current ED work,rendering a direct comparison of approaches based solely on reported numbers impossible: Current approaches widely differ in the data set used to train, the size of the covered entity vocabulary, and the usage of additional signals such as candidate lists. To address this issue, we present ZELDA , a novel entity disambiguation benchmark that includes a unified training data set, entity vocabulary, candidate lists, as well as challenging evaluation splits covering 8 different domains. We illustrate its design and construction, and present experiments in which we train and compare current state-of-the-art approaches on our benchmark. To encourage greater direct comparability in the entity disambiguation domain, we make our benchmark publicly available to the research community",
    "volume": "main",
    "checked": true,
    "id": "3bde83fc4c70aa00c214209b0b70890b4610169c",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.152": {
    "title": "GLADIS: A General and Large Acronym Disambiguation Benchmark",
    "abstract": "Acronym Disambiguation (AD) is crucial for natural language understanding on various sources, including biomedical reports, scientific papers, and search engine queries. However, existing acronym disambiguationbenchmarks and tools are limited to specific domains, and the size of prior benchmarks is rather small. To accelerate the research on acronym disambiguation, we construct a new benchmark with three components: (1) a much larger acronym dictionary with 1.5M acronyms and 6.4M long forms; (2) a pre-training corpus with 160 million sentences;(3) three datasets that cover thegeneral, scientific, and biomedical domains.We then pre-train a language model, {emph{AcroBERT}, on our constructed corpus for general acronym disambiguation, and show the challenges and values of our new benchmark",
    "volume": "main",
    "checked": true,
    "id": "627b6f7687e122b5578f095221f66583850f0ea5",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.153": {
    "title": "Probing Cross-Lingual Lexical Knowledge from Multilingual Sentence Encoders",
    "abstract": "Pretrained multilingual language models (LMs) can be successfully transformed into multilingual sentence encoders (SEs; e.g., LaBSE, xMPNet) via additional fine-tuning or model distillation with parallel data. However, it remains unclear how to best leverage them to represent sub-sentence lexical items (i.e., words and phrases) in cross-lingual lexical tasks. In this work, we probe SEs for the amount of cross-lingual lexical knowledge stored in their parameters, and compare them against the original multilingual LMs. We also devise a simple yet efficient method for exposing the cross-lingual lexical knowledge by means of additional fine-tuning through inexpensive contrastive learning that requires only a small amount of word translation pairs. Using bilingual lexical induction (BLI), cross-lingual lexical semantic similarity, and cross-lingual entity linking as lexical probing tasks, we report substantial gains on standard benchmarks (e.g., +10 Precision@1 points in BLI). The results indicate that the SEs such as LaBSE can be ‘rewired’ into effective cross-lingual lexical encoders via the contrastive learning procedure, and that it is possible to expose more cross-lingual lexical knowledge compared to using them as off-the-shelf SEs. This way, we also provide an effective tool for harnessing ‘covert’ multilingual lexical knowledge hidden in multilingual sentence encoders",
    "volume": "main",
    "checked": true,
    "id": "3301604a6b138fb8cadb37df2859fdee52ec5a2f",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.154": {
    "title": "Pento-DIARef: A Diagnostic Dataset for Learning the Incremental Algorithm for Referring Expression Generation from Examples",
    "abstract": "NLP tasks are typically defined extensionally through datasets containing example instantiations (e.g., pairs of image _i_ and text _t_), but motivated intensionally through capabilities invoked in verbal descriptions of the task (e.g., “_t_ is a description of _i_, for which the content of _i_ needs to be recognised and understood”).We present Pento-DIARef, a diagnostic dataset in a visual domain of puzzle pieces where referring expressions are generated by a well-known symbolic algorithm (the “Incremental Algorithm”),which itself is motivated by appeal to a hypothesised capability (eliminating distractors through application of Gricean maxims). Our question then is whether the extensional description (the dataset) is sufficient for a neural model to pick up the underlying regularity and exhibit this capability given the simple task definition of producing expressions from visual inputs. We find that a model supported by a vision detection step and a targeted data generation scheme achieves an almost perfect BLEU@1 score and sentence accuracy, whereas simpler baselines do not",
    "volume": "main",
    "checked": true,
    "id": "33f277fcd508ac2fdea64822a0b0693c1f8e2d16",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.155": {
    "title": "Mitigating Exposure Bias in Grammatical Error Correction with Data Augmentation and Reweighting",
    "abstract": "The most popular approach in grammatical error correction (GEC) is based on sequence-to-sequence (seq2seq) models. Similar to other autoregressive generation tasks, seq2seq GEC also faces the exposure bias problem, i.e., the context tokens are drawn from different distributions during training and testing, caused by the teacher forcing mechanism. In this paper, we propose a novel data manipulation approach to overcome this problem, which includes a data augmentation method during training to mimic the decoder input at inference time, and a data reweighting method to automatically balance the importance of each kind of augmented samples. Experimental results on benchmark GEC datasets show that our method achieves significant improvements compared to prior approaches",
    "volume": "main",
    "checked": true,
    "id": "a4909790f8fd6a0a5fcbfc1bb0b46545f39d8cf8",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.156": {
    "title": "Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training",
    "abstract": "Large-scale vision-language pre-trained (VLP) models are prone to hallucinate non-existent visual objects when generating text based on visual information. In this paper, we systematically study the object hallucination problem from three aspects. First, we examine recent state-of-the-art VLP models, showing that they still hallucinate frequently and models achieving better scores on standard metrics (e.g., CIDEr) could be more unfaithful. Second, we investigate how different types of image encoding in VLP influence hallucination, including region-based, grid-based, and patch-based. Surprisingly, we find that patch-based features perform the best and smaller patch resolution yields a non-trivial reduction in object hallucination. Third, we decouple various VLP objectives and demonstrate that token-level image-text alignment and controlled generation are crucial to reducing hallucination. Based on that, we propose a simple yet effective VLP loss named ObjMLM to further mitigate object hallucination. Results show that it reduces object hallucination by up to 17.4% when tested on two benchmarks (COCO Caption for in-domain and NoCaps for out-of-domain evaluation)",
    "volume": "main",
    "checked": true,
    "id": "7cfbd36c0043098589cbaf18dca2b41d8dc24abe",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.157": {
    "title": "Characterizing the Entities in Harmful Memes: Who is the Hero, the Villain, the Victim?",
    "abstract": "Memes can sway people’s opinions over social media as they combine visual and textual information in an easy-to-consume manner. Since memes instantly turn viral, it becomes crucial to infer their intent and potentially associated harmfulness to take timely measures as needed. A common problem associated with meme comprehension lies in detecting the entities referenced and characterizing the role of each of these entities. Here, we aim to understand whether the meme glorifies, vilifies, or victimizes each entity it refers to. To this end, we address the task of role identification of entities in harmful memes, i.e., detecting who is the ‘hero’, the ‘villain’, and the ‘victim’ in the meme, if any. We utilize HVVMemes – a memes dataset on US Politics and Covid-19 memes, released recently as part of the CONSTRAINT@ACL-2022 shared-task. It contains memes, entities referenced, and their associated roles: hero, villain, victim, and other. We further design VECTOR (Visual-semantic role dEteCToR), a robust multi-modal framework for the task, which integrates entity-based contextual information in the multi-modal representation and compare it to several standard unimodal (text-only or image-only) or multi-modal (image+text) models. Our experimental results show that our proposed model achieves an improvement of 4% over the best baseline and 1% over the best competing stand-alone submission from the shared-task. Besides divulging an extensive experimental setup with comparative analyses, we finally highlight the challenges encountered in addressing the complex task of semantic role labeling within memes",
    "volume": "main",
    "checked": true,
    "id": "d03fc0e109be9a321f78f17b4346a00e0e8e080b",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.158": {
    "title": "Systematic Investigation of Strategies Tailored for Low-Resource Settings for Low-Resource Dependency Parsing",
    "abstract": "In this work, we focus on low-resource dependency parsing for multiple languages. Several strategies are tailored to enhance performance in low-resource scenarios. While these are well-known to the community, it is not trivial to select the best-performing combination of these strategies for a low-resource language that we are interested in, and not much attention has been given to measuring the efficacy of these strategies. We experiment with 5 low-resource strategies for our ensembled approach on 7 Universal Dependency (UD) low-resource languages. Our exhaustive experimentation on these languages supports the effective improvements for languages not covered in pretrained models. We show a successful application of the ensembled system on a truly low-resource language Sanskrit. The code and data are available at: https://github.com/Jivnesh/SanDP",
    "volume": "main",
    "checked": true,
    "id": "0ff21d0bbd3d8b2afe5d2e246ed601189dd85698",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.159": {
    "title": "Compositional Generalisation with Structured Reordering and Fertility Layers",
    "abstract": "Seq2seq models have been shown to struggle with compositional generalisation, i.e. generalising to new and potentially more complex structures than seen during training. Taking inspiration from grammar-based models that excel at compositional generalisation, we present a flexible end-to-end differentiable neural model that composes two structural operations: a fertility step, which we introduce in this work, and a reordering step based on previous work (Wang et al., 2021). To ensure differentiability, we use the expected value of each step, which we compute using dynamic programming. Our model outperforms seq2seq models by a wide margin on challenging compositional splits of realistic semantic parsing tasks that require generalisation to longer examples. It also compares favourably to other models targeting compositional generalisation",
    "volume": "main",
    "checked": true,
    "id": "a0f40a1e4b11f286c87952d0cf7db9429c4f1c53",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.160": {
    "title": "Investigating Multi-source Active Learning for Natural Language Inference",
    "abstract": "In recent years, active learning has been successfully applied to an array of NLP tasks. However, prior work often assumes that training and test data are drawn from the same distribution. This is problematic, as in real-life settings data may stem from several sources of varying relevance and quality. We show that four popular active learning schemes fail to outperform random selection when applied to unlabelled pools comprised of multiple data sources on the task of natural language inference. We reveal that uncertainty-based strategies perform poorly due to the acquisition of collective outliers, i.e., hard-to-learn instances that hamper learning and generalisation. When outliers are removed, strategies are found to recover and outperform random baselines. In further analysis, we find that collective outliers vary in form between sources, and show that hard-to-learn data is not always categorically harmful. Lastly, we leverage dataset cartography to introduce difficulty-stratified testing and find that different strategies are affected differently by example learnability and difficulty",
    "volume": "main",
    "checked": true,
    "id": "c42110c29bc127e92aec580ee050e13408d3ff40",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.161": {
    "title": "Towards a Unified Multi-Domain Multilingual Named Entity Recognition Model",
    "abstract": "Named Entity Recognition is a key Natural Language Processing task whose performance is sensitive to choice of genre and language. A unified NER model across multiple genres and languages is more practical and efficient by leveraging commonalities across genres or languages. In this paper, we propose a novel setup for NER which includes multi-domain and multilingual training and evaluation across 13 domains and 4 languages. We explore a range of approaches to building a unified model using domain and language adaptation techniques. Our experiments highlight multiple nuances to consider while building a unified model, including that naive data pooling fails to obtain good performance, that domain-specific adaptations are more important than language-specific ones and that including domain-specific adaptations in a unified model nears the performance of training multiple dedicated monolingual models at a fraction of their parameter count",
    "volume": "main",
    "checked": true,
    "id": "58d6897869621624724b25b7e3a44daa2f5d5cb9",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.162": {
    "title": "Do Neural Topic Models Really Need Dropout? Analysis of the Effect of Dropout in Topic Modeling",
    "abstract": "Dropout is a widely used regularization trick to resolve the overfitting issue in large feedforward neural networks trained on a small dataset, which performs poorly on the held-out test subset. Although the effectiveness of this regularization trick has been extensively studied for convolutional neural networks, there is a lack of analysis of it for unsupervised models and in particular, VAE-based neural topic models. In this paper, we have analyzed the consequences of dropout in the encoder as well as in the decoder of the VAE architecture in three widely used neural topic models, namely, contextualized topic model (CTM), ProdLDA, and embedded topic model (ETM) using four publicly available datasets. We characterize the dropout effect on these models in terms of the quality and predictive performance of the generated topics",
    "volume": "main",
    "checked": true,
    "id": "7a1a17d019020054295990dec5c5f5f24825b613",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.163": {
    "title": "A Psycholinguistic Analysis of BERT's Representations of Compounds",
    "abstract": "This work studies the semantic representations learned by BERT for compounds, that is, expressions such as sunlight or bodyguard. We build on recent studies that explore semantic information in Transformers at the word level and test whether BERT aligns with human semantic intuitions when dealing with expressions (e.g., sunlight) whose overall meaning depends—to a various extent—on the semantics of the constituent words (sun, light). We leverage a dataset that includes human judgments on two psycholinguistic measures of compound semantic analysis: lexeme meaning dominance (LMD; quantifying the weight of each constituent toward the compound meaning) and semantic transparency (ST; evaluating the extent to which the compound meaning is recoverable from the constituents’ semantics). We show that BERT-based measures moderately align with human intuitions, especially when using contextualized representations, and that LMD is overall more predictable than ST. Contrary to the results reported for ‘standard’ words, higher, more contextualized layers are the best at representing compound meaning. These findings shed new light on the abilities of BERT in dealing with fine-grained semantic phenomena. Moreover, they can provide insights into how speakers represent compounds",
    "volume": "main",
    "checked": true,
    "id": "64a060831e7c0ed222b4ac50ec37bc68ba9a552a",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.164": {
    "title": "Measuring Normative and Descriptive Biases in Language Models Using Census Data",
    "abstract": "We investigate in this paper how distributions of occupations with respect to gender is reflected in pre-trained language models. Such distributions are not always aligned to normative ideals, nor do they necessarily reflect a descriptive assessment of reality. In this paper, we introduce an approach for measuring to what degree pre-trained language models are aligned to normative and descriptive occupational distributions. To this end, we use official demographic information about gender–occupation distributions provided by the national statistics agencies of France, Norway, United Kingdom, and the United States. We manually generate template-based sentences combining gendered pronouns and nouns with occupations, and subsequently probe a selection of ten language models covering the English, French, and Norwegian languages. The scoring system we introduce in this work is language independent, and can be used on any combination of template-based sentences, occupations, and languages. The approach could also be extended to other dimensions of national census data and other demographic variables",
    "volume": "main",
    "checked": true,
    "id": "a722010cc6bb9fffe3b9a06bf9586b5d40bc50f7",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.165": {
    "title": "UDAPTER - Efficient Domain Adaptation Using Adapters",
    "abstract": "We propose two methods to make unsupervised domain adaptation (UDA) more parameter efficient using adapters – small bottleneck layers interspersed with every layer of the large-scale pre-trained language model (PLM). The first method deconstructs UDA into a two-step process: first by adding a domain adapter to learn domain-invariant information and then by adding a task adapter that uses domain-invariant information to learn task representations in the source domain. The second method jointly learns a supervised classifier while reducing the divergence measure. Compared to strong baselines, our simple methods perform well in natural language inference (MNLI) and the cross-domain sentiment classification task. We even outperform unsupervised domain adaptation methods such as DANN and DSN in sentiment classification, and we are within 0.85% F1 for natural language inference task, by fine-tuning only a fraction of the full model parameters. We release our code at this URL",
    "volume": "main",
    "checked": true,
    "id": "10c5079d2baa5a287054d9ddd7806a4c16fd7531",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.166": {
    "title": "Efficient CTC Regularization via Coarse Labels for End-to-End Speech Translation",
    "abstract": "For end-to-end speech translation, regularizing the encoder with the Connectionist Temporal Classification (CTC) objective using the source transcript or target translation as labels can greatly improve quality. However, CTC demands an extra prediction layer over the vocabulary space, bringing in non-negligible model parameters and computational overheads, although this layer becomes useless at inference. In this paper, we re-examine the need for genuine vocabulary labels for CTC for regularization and explore strategies to reduce the CTC label space, targeting improved efficiency without quality degradation. We propose coarse labeling for CTC (CoLaCTC), which merges vocabulary labels via simple heuristic rules, such as using truncation, division or modulo (MOD) operations. Despite its simplicity, our experiments on 4 source and 8 target languages show that CoLaCTC with MOD particularly can compress the label space aggressively to 256 and even further, gaining training efficiency (1.18× ∼ 1.77× speedup depending on the original vocabulary size) yet still delivering comparable or better performance than the CTC baseline. We also show that CoLaCTC successfully generalizes to CTC regularization regardless of using transcript or translation for labeling",
    "volume": "main",
    "checked": true,
    "id": "e1094cb086f8b401cbd62a0ed7769feebdd0c4db",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.167": {
    "title": "Exploring Category Structure with Contextual Language Models and Lexical Semantic Networks",
    "abstract": "The psychological plausibility of word embeddings has been studied through different tasks such as word similarity, semantic priming, and lexical entailment. Recent work on predicting category structure with word embeddings report low correlations with human ratings. (Heyman and Heyman, 2019) showed that static word embeddings fail at predicting typicality using cosine similarity between category and exemplar words, while (Misra et al., 2021)obtain equally modest results for various contextual language models (CLMs) using a Cloze task formulation over hand-crafted taxonomic sentences. In this work, we test a wider array of methods for probing CLMs for predicting typicality scores. Our experiments, using BERT (Devlin et al., 2018), show the importance of using the right type of CLM probes, as our best BERT-based typicality prediction methods improve on previous works. Second, our results highlight the importance of polysemy in this task, as our best results are obtained when contextualization is paired with a disambiguation mechanism as in (Chronis and Erk, 2020). Finally, additional experiments and analyses reveal that Information Content-based WordNet (Miller, 1995) similarities with disambiguation match the performance of the best BERT-based method, and in fact capture complementary information, and when combined with BERT allow for enhanced typicality predictions",
    "volume": "main",
    "checked": true,
    "id": "c926c6c82408761876e5302048d086268f53fc7b",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.168": {
    "title": "An Empirical Study of Clinical Note Generation from Doctor-Patient Encounters",
    "abstract": "Medical doctors spend on average 52 to 102 minutes per day writing clinical notes from their patient encounters (Hripcsak et al., 2011). Reducing this workload calls for relevant and efficient summarization methods. In this paper, we introduce new resources and empirical investigations for the automatic summarization of doctor-patient conversations in a clinical setting. In particular, we introduce the MTS-Dialog dataset; a new collection of 1,700 doctor-patient dialogues and corresponding clinical notes. We use this new dataset to investigate the feasibility of this task and the relevance of existing language models, data augmentation, and guided summarization techniques. We compare standard evaluation metrics based on n-gram matching, contextual embeddings, and Fact Extraction to assess the accuracy and the factual consistency of the generated summaries. To ground these results, we perform an expert-based evaluation using relevant natural language generation criteria and task-specific criteria such as critical omissions, and study the correlation between the automatic metrics and expert judgments. To the best of our knowledge, this study is the first attempt to introduce an open dataset of doctor-patient conversations and clinical notes, with detailed automated and manual evaluations of clinical note generation",
    "volume": "main",
    "checked": true,
    "id": "df9139b3908af17cdb19a3ced717f5060cf92a5f",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.169": {
    "title": "Instruction Clarification Requests in Multimodal Collaborative Dialogue Games: Tasks, and an Analysis of the CoDraw Dataset",
    "abstract": "In visual instruction-following dialogue games, players can engage in repair mechanisms in face of an ambiguous or underspecified instruction that cannot be fully mapped to actions in the world. In this work, we annotate Instruction Clarification Requests (iCRs) in CoDraw, an existing dataset of interactions in a multimodal collaborative dialogue game. We show that it contains lexically and semantically diverse iCRs being produced self-motivatedly by players deciding to clarify in order to solve the task successfully. With 8.8k iCRs found in 9.9k dialogues, CoDraw-iCR (v1) is a large spontaneous iCR corpus, making it a valuable resource for data-driven research on clarification in dialogue. We then formalise and provide baseline models for two tasks: Determining when to make an iCR and how to recognise them, in order to investigate to what extent these tasks are learnable from data",
    "volume": "main",
    "checked": true,
    "id": "8a7e028662345cfb2779ed84a4f516013286d53c",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.170": {
    "title": "Can Synthetic Text Help Clinical Named Entity Recognition? A Study of Electronic Health Records in French",
    "abstract": "In sensitive domains, the sharing of corpora is restricted due to confidentiality, copyrights or trade secrets. Automatic text generation can help alleviate these issues by producing synthetic texts that mimic the linguistic properties of real documents while preserving confidentiality. In this study, we assess the usability of synthetic corpus as a substitute training corpus for clinical information extraction. Our goal is to automatically produce a clinical case corpus annotated with clinical entities and to evaluate it for a named entity recognition (NER) task. We use two auto-regressive neural models partially or fully trained on generic French texts and fine-tuned on clinical cases to produce a corpus of synthetic clinical cases. We study variants of the generation process: (i) fine-tuning on annotated vs. plain text (in that case, annotations are obtained a posteriori) and (ii) selection of generated texts based on models parameters and filtering criteria. We then train NER models with the resulting synthetic text and evaluate them on a gold standard clinical corpus. Our experiments suggest that synthetic text is useful for clinical NER",
    "volume": "main",
    "checked": true,
    "id": "368356a1a99a6ecd04dfe8ac7d97f3308c6128a6",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.171": {
    "title": "IRMA: the 335-million-word Italian coRpus for studying MisinformAtion",
    "abstract": "The dissemination of false information on the internet has received considerable attention over the last decade. Misinformation often spreads faster than mainstream news, thus making manual fact checking inefficient or, at best, labor-intensive. Therefore, there is an increasing need to develop methods for automatic detection of misinformation. Although resources for creating such methods are available in English, other languages are often under-represented in this effort.With this contribution, we present IRMA, a corpus containing over 600,000 Italian news articles (335+ million tokens) collected from 56 websites classified as ‘untrustworthy’ by professional fact-checkers. The corpus is freely available and comprises a rich set of text- and website-level data, representing a turnkey resource to test hypotheses and develop automatic detection algorithms. It contains texts, titles, and dates (from 2004 to 2022), along with three types of semantic measures (i.e., keywords, topics at three different resolutions, and LIWC lexical features). IRMA also includes domain-specific information such as source type (e.g., political, health, conspiracy, etc.), credibility, and higher-level metadata, including several metrics of website incoming traffic that allow to investigate user online behavior. IRMA constitutes the largest corpus of misinformation available today in Italian, making it a valid tool for advancing quantitative research on untrustworthy news detection and ultimately helping limit the spread of misinformation",
    "volume": "main",
    "checked": true,
    "id": "9d19b8ed9ab4278c6852357d7ce24ff749895556",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.172": {
    "title": "Parameter-Efficient Korean Character-Level Language Modeling",
    "abstract": "Character-level language modeling has been shown empirically to perform well on highly agglutinative or morphologically rich languages while using only a small fraction of the parameters required by (sub)word models. Korean fits nicely into this framework, except that, like other CJK languages, it has a very large character vocabulary of 11,172 unique syllables. However, unlike Japanese Kanji and Chinese Hanzi, each Korean syllable can be uniquely factored into a small set of subcharacters, called jamo. We explore a “three-hot” scheme, where we exploit the decomposability of Korean characters to model at the syllable level but using only jamo-level representations. We find that our three-hot embedding and decoding scheme alleviates the two major issues with prior syllable- and jamo-level models. Namely, it requires fewer than 1% of the embedding parameters of a syllable model, and it does not require tripling the sequence length, as with jamo models. In addition, it addresses a theoretical flaw in a prior three-hot modeling scheme. Our experiments show that, even when reducing the number of embedding parameters by 99.6% (from 11.4M to just 36k), our model suffers no loss in translation quality compared to the baseline syllable model",
    "volume": "main",
    "checked": true,
    "id": "18da531f6536bbbcdde1ea2b0d8487fb0d6fa3f5",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.173": {
    "title": "Opportunities and Challenges in Neural Dialog Tutoring",
    "abstract": "Designing dialog tutors has been challenging as it involves modeling the diverse and complex pedagogical strategies employed by human tutors. Although there have been significant recent advances in neural conversational systems using large language models and growth in available dialog corpora, dialog tutoring has largely remained unaffected by these advances. In this paper, we rigorously analyze various generative language models on two dialog tutoring datasets for language learning using automatic and human evaluations to understand the new opportunities brought by these advances as well as the challenges we must overcome to build models that would be usable in real educational settings.We find that although current approaches can model tutoring in constrained learning scenarios when the number of concepts to be taught and possible teacher strategies are small, they perform poorly in less constrained scenarios.Our human quality evaluation shows that both models and ground-truth annotations exhibit low performance in terms of equitable tutoring, which measures learning opportunities for students and how engaging the dialog is.To understand the behavior of our models in a real tutoring setting, we conduct a user study using expert annotators and find a significantly large number of model reasoning errors in 45% of conversations. Finally, we connect our findings to outline future work",
    "volume": "main",
    "checked": true,
    "id": "076d56635311fb75168dc11a094bf7d70e4178a9",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.174": {
    "title": "Evaluating the Robustness of Discrete Prompts",
    "abstract": "Discrete prompts have been used for fine-tuning Pre-trained Language Models for diverse NLP tasks. In particular, automatic methods that generate discrete prompts from a small set of training instances have reported superior performance. However, a closer look at the learnt prompts reveals that they contain noisy and counter-intuitive lexical constructs that would not be encountered in manually-written prompts. This raises an important yet understudied question regarding the robustness of automatically learnt discrete prompts when used in downstream tasks. To address this question, we conduct a systematic study of the robustness of discrete prompts by applying carefully designed perturbations into an application using AutoPrompt and then measure their performance in two Natural Language Inference (NLI) datasets. Our experimental results show that although the discrete prompt-based method remains relatively robust against perturbations to NLI inputs, they are highly sensitive to other types of perturbations such as shuffling and deletion of prompt tokens. Moreover, they generalize poorly across different NLI datasets. We hope our findings will inspire future work on robust discrete prompt learning",
    "volume": "main",
    "checked": true,
    "id": "ce6b0a9877e135c38eb3a6c6705c95422181af78",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.175": {
    "title": "Assessing Out-of-Domain Language Model Performance from Few Examples",
    "abstract": "While pretrained language models have exhibited impressive generalization capabilities, they still behave unpredictably under certain domain shifts. In particular, a model may learn a reasoning process on in-domain training data that does not hold for out-of-domain test data. We address the task of predicting out-of-domain (OOD) performance in a few-shot fashion: given a few target-domain examples and a set of models with similar training performance, can we understand how these models will perform on OOD test data? We benchmark the performance on this task when looking at model accuracy on the few-shot examples, then investigate how to incorporate analysis of the models’ behavior using feature attributions to better tackle this problem. Specifically, we explore a set of factors designed to reveal model agreement with certain pathological heuristics that may indicate worse generalization capabilities. On textual entailment, paraphrase recognition, and a synthetic classification task, we show that attribution-based factors can help rank relative model OOD performance. However, accuracy on a few-shot test set is a surprisingly strong baseline, particularly when the system designer does not have in-depth prior knowledge about the domain shift",
    "volume": "main",
    "checked": true,
    "id": "b1a9104afd48f019aeb98c6be7a15736089959d2",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.176": {
    "title": "Mind the Labels: Describing Relations in Knowledge Graphs With Pretrained Models",
    "abstract": "Pretrained language models (PLMs) for data-to-text (D2T) generation can use human-readable data labels such as column headings, keys, or relation names to generalize to out-of-domain examples. However, the models are well-known in producing semantically inaccurate outputs if these labels are ambiguous or incomplete, which is often the case in D2T datasets. In this paper, we expose this issue on the task of descibing a relation between two entities. For our experiments, we collect a novel dataset for verbalizing a diverse set of 1,522 unique relations from three large-scale knowledge graphs (Wikidata, DBPedia, YAGO). We find that although PLMs for D2T generation expectedly fail on unclear cases, models trained with a large variety of relation labels are surprisingly robust in verbalizing novel, unseen relations. We argue that using data with a diverse set of clear and meaningful labels is key to training D2T generation systems capable of generalizing to novel domains",
    "volume": "main",
    "checked": true,
    "id": "f59f27ae53bc860818cd3363d799a4b85fe6b4f9",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.177": {
    "title": "Shapley Head Pruning: Identifying and Removing Interference in Multilingual Transformers",
    "abstract": "Multilingual transformer-based models demonstrate remarkable zero and few-shot transfer across languages by learning and reusing language-agnostic features. However, as a fixed-size model acquires more languages, its performance across all languages degrades. Those who attribute this interference phenomenon to limited model capacity address the problem by adding additional parameters, despite evidence that transformer-based models are overparameterized. In this work, we show that it is possible to reduce interference by instead identifying and pruning language-specific attention heads. First, we use Shapley Values, a credit allocation metric from coalitional game theory, to identify attention heads that introduce interference. Then, we show that pruning such heads from a fixed model improves performance for a target language on both sentence classification and structural prediction. Finally, we provide insights on language-agnostic and language-specific attention heads using attention visualization",
    "volume": "main",
    "checked": true,
    "id": "c68cb6e59b383d34453fb9b7671fc283fe761a35",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.178": {
    "title": "Why Don't You Do It Right? Analysing Annotators' Disagreement in Subjective Tasks",
    "abstract": "Annotators’ disagreement in linguistic data has been recently the focus of multiple initiatives aimed at raising awareness on issues related to ‘majority voting’ when aggregating diverging annotations. Disagreement can indeed reflect different aspects of linguistic annotation, from annotators’ subjectivity to sloppiness or lack of enough context to interpret a text. In this work we first propose a taxonomy of possible reasons leading to annotators’ disagreement in subjective tasks. Then, we manually label part of a Twitter dataset for offensive language detection in English following this taxonomy, identifying how the different categories are distributed. Finally we run a set of experiments aimed at assessing the impact of the different types of disagreement on classification performance. In particular, we investigate how accurately tweets belonging to different categories of disagreement can be classified as offensive or not, and how injecting data with different types of disagreement in the training set affects performance. We also perform offensive language detection as a multi-task framework, using disagreement classification as an auxiliary task",
    "volume": "main",
    "checked": true,
    "id": "d7b10939d09776b9b10b4555f20577d3dc6697fc",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.179": {
    "title": "Analyzing Challenges in Neural Machine Translation for Software Localization",
    "abstract": "Advancements in Neural Machine Translation (NMT) greatly benefit the software localization industry by decreasing the post-editing time of human annotators. Although the volume of the software being localized is growing significantly, techniques for improving NMT for user interface (UI) texts are lacking. These UI texts have different properties than other collections of texts, presenting unique challenges for NMT. For example, they are often very short, causing them to be ambiguous and needing additional context (button, title text, a table item, etc.) for disambiguation. However, no such UI data sets are readily available with contextual information for NMT models to exploit. This work aims to provide a first step in improving UI translations and highlight its challenges. To achieve this, we provide a novel multilingual UI corpus collection (${sim1.3M$ for English ${leftrightarrow$ German) with a targeted test set and analyze the limitations of state-of-the-art methods on this challenging task. Specifically, we present a targeted test set for disambiguation from English to German to evaluate reliably and emphasize UI translation challenges. Furthermore, we evaluate several state-of-the-art NMT techniques from domain adaptation and document-level NMT on this challenging task. All the scripts to replicate the experiments and data sets are available here.{footnote{{url{https://github.com/saikoneru/NMT{_Localization}}$ˆ{,}${footnote{We crawled this data only for scientific research.}",
    "volume": "main",
    "checked": true,
    "id": "9c6fe8eeab64c1e6841b35eddc600aecf2b4eeea",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.180": {
    "title": "Bootstrapping Multilingual Semantic Parsers using Large Language Models",
    "abstract": "Despite cross-lingual generalization demonstrated by pre-trained multilingual models, the translate-train paradigm of transferring English datasets across multiple languages remains to be a key mechanism for training task-specific multilingual models. However, for many low-resource languages, the availability of a reliable translation service entails significant amounts of costly human-annotated translation pairs. Further, translation services may continue to be brittle due to domain mismatch between task-specific input text and general-purpose text used for training translation models. For multilingual semantic parsing, we demonstrate the effectiveness and flexibility offered by large language models (LLMs) for translating English datasets into several languages via few-shot prompting. Through extensive comparisons on two public datasets, MTOP and MASSIVE, spanning 50 languages and several domains, we show that our method of translating data using LLMs outperforms a strong translate-train baseline on 41 out of 50 languages. We study the key design choices that enable more effective multilingual data translation via prompted LLMs",
    "volume": "main",
    "checked": true,
    "id": "dda0f7f086fc875d583604f8b0cf4a8678bc4de4",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.181": {
    "title": "Modeling Complex Event Scenarios via Simple Entity-focused Questions",
    "abstract": "Event scenarios are often complex and involve multiple event sequences connected through different entity participants. Exploring such complex scenarios requires an ability to branch through different sequences, something that is difficult to achieve with standard event language modeling. To address this, we propose a question-guided generation framework that models events in complex scenarios as answers to questions about participants. At any step in the generation process, the framework uses the previously-generated events as context, but generates the next event as an answer to one of three questions: what else a participant did, what else happened to a participant, or what else happened. The participants and the questions themselves can be sampled or be provided as input from a user, allowing for controllable exploration. Our empirical evaluation shows that this question-guided generation provides better coverage of participants, diverse events within a domain, comparable perplexities for modeling event sequences, and more effective control for interactive schema generation",
    "volume": "main",
    "checked": true,
    "id": "68e7e7c52393fbd7d4d6fb34174df917ef3e8b1e",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.182": {
    "title": "Uncovering Implicit Inferences for Improved Relational Argument Mining",
    "abstract": "Argument mining seeks to extract arguments and their structure from unstructured texts. Identifying relations between arguments (such as attack, support, and neutral) is a challenging task because two arguments may be related to each other via implicit inferences. This task often requires external commonsense knowledge to discover how one argument relates to another. State-of-the-art methods, however, rely on pre-defined knowledge graphs, and thus might not cover target argument pairs well. We introduce a new generative neuro-symbolic approach to finding inference chains that connect the argument pairs by making use of the Commonsense Transformer (COMET). We evaluate our approach on three datasets for both the two-label (attack/support) and three-label (attack/support/neutral) tasks. Our approach significantly outperforms the state-of-the-art, by 2-5% in F1 score, on all three datasets",
    "volume": "main",
    "checked": true,
    "id": "c5b96f13bba2937eca88be6b02fec8fc1d1002b0",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.183": {
    "title": "How people talk about each other: Modeling Generalized Intergroup Bias and Emotion",
    "abstract": "Current studies of bias in NLP rely mainly on identifying (unwanted or negative) bias towards a specific demographic group. While this has led to progress recognizing and mitigating negative bias, and having a clear notion of the targeted group is necessary, it is not always practical. In this work we extrapolate to a broader notion of bias, rooted in social science and psychology literature. We move towards predicting interpersonal group relationship (IGR) - modeling the relationship between the speaker and the target in an utterance - using fine-grained interpersonal emotions as an anchor. We build and release a dataset of English tweets by US Congress members annotated for interpersonal emotion - the first of its kind, and ‘found supervision’ for IGR labels; our analyses show that subtle emotional signals are indicative of different biases. While humans can perform better than chance at identifying IGR given an utterance, we show that neural models perform much better; furthermore, a shared encoding between IGR and interpersonal perceived emotion enabled performance gains in both tasks",
    "volume": "main",
    "checked": true,
    "id": "b4d61b11023c19b13e326e6e0365715b5dbcda2c",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.184": {
    "title": "Semantic Parsing for Conversational Question Answering over Knowledge Graphs",
    "abstract": "In this paper, we are interested in developing semantic parsers which understand natural language questions embedded in a conversation with a user and ground them to formal queries over definitions in a general purpose knowledge graph (KG) with very large vocabularies (covering thousands of concept names and relations, and millions of entities). To this end, we develop a dataset where user questions are annotated with Sparql parses and system answers correspond to execution results thereof. We present two different semantic parsing approaches and highlight the challenges of the task: dealing with large vocabularies, modelling conversation context, predicting queries with multiple entities, and generalising to new questions at test time. We hope our dataset will serve as useful testbed for the development of conversational semantic parsers. Our dataset and models are released at https://github.com/EdinburghNLP/SPICE",
    "volume": "main",
    "checked": true,
    "id": "14fa80cd4533c77b9dd84b05b2c24c0ffce14fb0",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.185": {
    "title": "MAPL: Parameter-Efficient Adaptation of Unimodal Pre-Trained Models for Vision-Language Few-Shot Prompting",
    "abstract": "Large pre-trained models have proved to be remarkable zero- and (prompt-based) few-shot learners in unimodal vision and language tasks. We propose MAPL, a simple and parameter-efficient method that reuses frozen pre-trained unimodal models and leverages their strong generalization capabilities in multimodal vision-language (VL) settings. MAPL learns a lightweight mapping between the representation spaces of unimodal models using aligned image-text data, and can generalize to unseen VL tasks from just a few in-context examples. The small number of trainable parameters makes MAPL effective at low-data and in-domain learning. Moreover, MAPL’s modularity enables easy extension to other pre-trained models. Extensive experiments on several visual question answering and image captioning benchmarks show that MAPL achieves superior or competitive performance compared to similar methods while training orders of magnitude fewer parameters. MAPL can be trained in just a few hours using modest computational resources and public datasets. We release our code and pre-trained model weights at https://github.com/oscmansan/mapl",
    "volume": "main",
    "checked": true,
    "id": "1f86bf1e334200ec0481349255559fbfe7a33caa",
    "citation_count": 2
  },
  "https://aclanthology.org/2023.eacl-main.186": {
    "title": "ComSearch: Equation Searching with Combinatorial Strategy for Solving Math Word Problems with Weak Supervision",
    "abstract": "Previous studies have introduced a weakly-supervised paradigm for solving math word problems requiring only the answer value annotation. While these methods search for correct value equation candidates as pseudo labels, they search among a narrow sub-space of the enormous equation space. To address this problem, we propose a novel search algorithm with combinatorial strategy ComSearch, which can compress the search space by excluding mathematically equivalent equations. The compression allows the searching algorithm to enumerate all possible equations and obtain high-quality data. We investigate the noise in the pseudo labels that hold wrong mathematical logic, which we refer to as the false-matching problem, and propose a ranking model to denoise the pseudo labels. Our approach holds a flexible framework to utilize two existing supervised math word problem solvers to train pseudo labels, and both achieve state-of-the-art performance in the weak supervision task",
    "volume": "main",
    "checked": true,
    "id": "48730654ab43abbf3c18d3ceb86405ac909184c3",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.187": {
    "title": "Towards preserving word order importance through Forced Invalidation",
    "abstract": "Large pre-trained language models such as BERT have been widely used as a framework for natural language understanding (NLU) tasks. However, recent findings have revealed that pre-trained language models are insensitive to word order. The performance on NLU tasks remains unchanged even after randomly permuting the word of a sentence, where crucial syntactic information is destroyed. To help preserve the importance of word order, we propose a simple approach called Forced Invalidation (FI): forcing the model to identify permuted sequences as invalid samples. We perform an extensive evaluation of our approach on various English NLU and QA based tasks over BERT-based and attention-based models over word embeddings. Our experiments demonstrate that FI significantly improves the sensitivity of the models to word order",
    "volume": "main",
    "checked": true,
    "id": "6309aafd90d6659a878afbe8cadb1548557384c7",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.188": {
    "title": "How Many and Which Training Points Would Need to be Removed to Flip this Prediction?",
    "abstract": "We consider the problem of identifying a {emph{minimal subset} of training data ${mathcal{S}_t$ such that if the instances comprising ${mathcal{S}_t$ had been removed prior to training, the categorization of a given test point $x_t$ would have been different.Identifying such a set may be of interest for a few reasons.First, the cardinality of ${mathcal{S}_t$ provides a measure of robustness (if $|{mathcal{S}_t|$ is small for $x_t$, we might be less confident in the corresponding prediction), which we show is correlated with but complementary to predicted probabilities.Second, interrogation of ${mathcal{S}_t$ may provide a novel mechanism for {emph{contesting} a particular model prediction: If one can make the case that the points in ${mathcal{S}_t$ are wrongly labeled or irrelevant, this may argue for overturning the associated prediction. Identifying ${mathcal{S}_t$ via brute-force is intractable.We propose comparatively fast approximation methods to find ${mathcal{S}_t$ based on {emph{influence functions}, and find that—for simple convex text classification models—these approaches can often successfully identify relatively small sets of training examples which, if removed, would flip the prediction",
    "volume": "main",
    "checked": true,
    "id": "4ee5efa9cc781ff933cda7edb3d25b2f3d6931ec",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.189": {
    "title": "Reinforced Sequence Training based Subjective Bias Correction",
    "abstract": "Subjective bias is ubiquitous on news sites, social media, and knowledge resources like Wikipedia. Many existing methods for subjective bias correction have typically focused on making one-word edits and have been trained over a single (often, noisy) domain. In contrast, we propose a novel reinforced sequence training approach for robust subjective bias correction. Three of the unique characteristics of the approach are: (i) it balances bias neutralization with fluency and semantics preservation through reinforcement learning, to broaden the scope to bias beyond a single word; (ii) it is cross-trained over multiple sources of bias to be more robust to new styles of biased writing that are not seen in the training data for a single domain; and (iii) it is used to fine-tune a large pre-trained transformer model to yield state-of-the-art performance in bias text correction task. Extensive experiments show that the proposed approach results in significant improvements in subjective bias correction versus alternatives",
    "volume": "main",
    "checked": true,
    "id": "f6ee3544b8c53ef2f1015e5c29269ae36c428352",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.190": {
    "title": "Detecting Lexical Borrowings from Dominant Languages in Multilingual Wordlists",
    "abstract": "Language contact is a pervasive phenomenon reflected in the borrowing of words from donor to recipient languages. Most computational approaches to borrowing detection treat all languages under study as equally important, even though dominant languages have a stronger impact on heritage languages than vice versa. We test new methods for lexical borrowing detection in contact situations where dominant languages play an important role, applying two classical sequence comparison methods and one machine learning method to a sample of seven Latin American languages which have all borrowed extensively from Spanish. All systems perform well, with the supervised machine learning system outperforming the classical systems. A review of detection errors shows that borrowing detection could be substantially improved by taking into account donor words with divergent meanings from recipient words",
    "volume": "main",
    "checked": true,
    "id": "0ae621bf83cd2161e594fcf10a02e03e55e13f2b",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.191": {
    "title": "Towards Integration of Discriminability and Robustness for Document-Level Relation Extraction",
    "abstract": "Document-level relation extraction (DocRE) predicts relations for entity pairs that rely on long-range context-dependent reasoning in a document. As a typical multi-label classification problem, DocRE faces the challenge of effectively distinguishing a small set of positive relations from the majority of negative ones. This challenge becomes even more difficult to overcome when there exists a significant number of annotation errors in the dataset. In this work, we aim to achieve better integration of both the discriminability and robustness for the DocRE problem. Specifically, we first design an effective loss function to endow high discriminability to both probabilistic outputs and internal representations. We innovatively customize entropy minimization and supervised contrastive learning for the challenging multi-label and long-tailed learning problems. To ameliorate the impact of label errors, we equipped our method with a novel negative label sampling strategy to strengthen the model robustness. In addition, we introduce two new data regimes to mimic more realistic scenarios with annotation errors and evaluate our sampling strategy. Experimental results verify the effectiveness of each component and show that our method achieves new state-of-the-art results on the DocRED dataset, its recently cleaned version, Re-DocRED, and the proposed data regimes",
    "volume": "main",
    "checked": true,
    "id": "26c72910b38b80bde5e27dcfee933796e3c286a2",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.192": {
    "title": "Penguins Don't Fly: Reasoning about Generics through Instantiations and Exceptions",
    "abstract": "Generics express generalizations about the world (e.g., birds can fly) that are not universally true (e.g., newborn birds and penguins cannot fly). Commonsense knowledge bases, used extensively in NLP, encode some generic knowledge but rarely enumerate such exceptions and knowing when a generic statement holds or does not hold true is crucial for developing a comprehensive understanding of generics. We present a novel framework informed by linguistic theory to generate exemplars—specific cases when a generic holds true or false. We generate ~19k exemplars for ~650 generics and show that our framework outperforms a strong GPT-3 baseline by 12.8 precision points. Our analysis highlights the importance of linguistic theory-based controllability for generating exemplars, the insufficiency of knowledge bases as a source of exemplars, and the challenges exemplars pose for the task of natural language inference",
    "volume": "main",
    "checked": true,
    "id": "d89cee8ab8a8b775b49044aae112b3dd910d7338",
    "citation_count": 5
  },
  "https://aclanthology.org/2023.eacl-main.193": {
    "title": "Adding Instructions during Pretraining: Effective way of Controlling Toxicity in Language Models",
    "abstract": "Pretrained large language models have become indispensable for solving various natural language processing (NLP) tasks. However, safely deploying them in real world applications is challenging because they generate toxic content. To address this challenge, we propose two novel pretraining data augmentation strategies that significantly reduce model toxicity without compromising its utility. Our two strategies are: (1) MEDA: adds raw toxicity score as meta-data to the pretraining samples, and (2) INST: adds instructions to those samples indicating their toxicity. Our results indicate that our best performing strategy (INST) substantially reduces the toxicity probability up to 61% while preserving the accuracy on five benchmark NLP tasks as well as improving AUC scores on four bias detection tasks by 1.3%. We also demonstrate the generalizability of our techniques by scaling the number of training samples and the number of model parameters",
    "volume": "main",
    "checked": true,
    "id": "f9338e1b32cb450d95094ce71957375754823e4b",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.194": {
    "title": "Multi2Claim: Generating Scientific Claims from Multi-Choice Questions for Scientific Fact-Checking",
    "abstract": "Training machine learning models to successfully perform scientific fact-checking tasks is challenging due to the expertise bottleneck that limits the availability of appropriate training datasets. In this task, models use textual evidence to confirm scientific claims, which requires data that contains extensive domain-expert annotation. Consequently, the number of existing scientific-fact-checking datasets and the sizes of those datasets are limited. However, these limitations do not apply to multiple-choice question datasets because of the necessity of domain exams in the modern education system. As one of the first steps towards addressing the fact-checking dataset scarcity problem in scientific domains, we propose a pipeline for automatically converting multiple-choice questions into fact-checking data, which we call Multi2Claim. By applying the proposed pipeline, we generated two large-scale datasets for scientific-fact-checking tasks: Med-Fact and Gsci-Fact for the medical and general science domains, respectively. These two datasets are among the first examples of large-scale scientific-fact-checking datasets. We developed baseline models for the verdict prediction task using each dataset. Additionally, we demonstrated that the datasets could be used to improve performance with respect to the F 1 weighted metric on existing fact-checking datasets such as SciFact, HEALTHVER, COVID-Fact, and CLIMATE-FEVER. In some cases, the improvement in performance was up to a 26% increase",
    "volume": "main",
    "checked": true,
    "id": "7d36fd1ca5294d1ce5645cf341462316a7cc3124",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.195": {
    "title": "On Evaluation of Document Classifiers using RVL-CDIP",
    "abstract": "The RVL-CDIP benchmark is widely used for measuring performance on the task of document classification. Despite its widespread use, we reveal several undesirable characteristics of the RVL-CDIP benchmark. These include (1) substantial amounts of label noise, which we estimate to be 8.1% (ranging between 1.6% to 16.9% per document category); (2) presence of many ambiguous or multi-label documents; (3) a large overlap between test and train splits, which can inflate model performance metrics; and (4) presence of sensitive personally-identifiable information like US Social Security numbers (SSNs). We argue that there is a risk in using RVL-CDIP for benchmarking document classifiers, as its limited scope, presence of errors (state-of-the-art models now achieve accuracy error rates that are within our estimated label error rate), and lack of diversity make it less than ideal for benchmarking. We further advocate for the creation of a new document classification benchmark, and provide recommendations for what characteristics such a resource should include",
    "volume": "main",
    "checked": true,
    "id": "014987ed79ddfbf7b82cced49074bde574d054a4",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.196": {
    "title": "Event Linking: Grounding Event Mentions to Wikipedia",
    "abstract": "Comprehending an article requires understanding its constituent events. However, the context where an event is mentioned often lacks the details of this event. A question arises: how can the reader obtain more knowledge about this particular event in addition to what is provided by the local context in the article? This work defines Event Linking, a new natural language understanding task at the event level. Event linking tries to link an event mention appearing in an article to the most appropriate Wikipedia page. This page is expected to provide rich knowledge about what the event mention refers to. To standardize the research in this new direction, we contribute in four-fold. First, this is the first work in the community that formally defines the Event Linking task. Second, we collect a dataset for this new task. Specifically, we automatically gather the training set from Wikipedia, and then create two evaluation sets: one from the Wikipedia domain, reporting the in-domain performance, and a second from the real-world news domain, to evaluate out-of-domain performance. Third, we retrain and evaluate two state-of-the-art (SOTA) entity linking models, showing the challenges of event linking, and we propose an event-specific linking system, EVELINK, to set a competitive result for the new task. Fourth, we conduct a detailed and insightful analysis to help understand the task and the limitations of the current model. Overall, as our analysis shows, Event Linking is a challenging and essential task requiring more effort from the community",
    "volume": "main",
    "checked": true,
    "id": "82d25926f8fe4b4988b8c0a69de3561d41d76080",
    "citation_count": 2
  },
  "https://aclanthology.org/2023.eacl-main.197": {
    "title": "SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains",
    "abstract": "Prompting pre-trained language models leads to promising results across natural language processing tasks but is less effective when applied in low-resource domains, due to the domain gap between the pre-training data and the downstream task. In this work, we bridge this gap with a novel and lightweight prompting methodology called SwitchPrompt for the adaptation of language models trained on datasets from the general domain to diverse low-resource domains. Using domain-specific keywords with a trainable gated prompt, SwitchPrompt offers domain-oriented prompting, that is, effective guidance on the target domains for general-domain language models. Our few-shot experiments on three text classification benchmarks demonstrate the efficacy of the general-domain pre-trained language models when used with SwitchPrompt. They often even outperform their domain-specific counterparts trained with baseline state-of-the-art prompting methods by up to 10.7% performance increase in accuracy. This result indicates that SwitchPrompt effectively reduces the need for domain-specific language model pre-training",
    "volume": "main",
    "checked": true,
    "id": "ef91c31d8aab9fe95fec29149e2fe4568ab2fb32",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.198": {
    "title": "Do dialogue representations align with perception? An empirical study",
    "abstract": "There has been a surge of interest regarding the alignment of large-scale language models with human language comprehension behaviour. The majority of this research investigates comprehension behaviours from reading isolated, written sentences. We propose studying the perception of dialogue, focusing on an intrinsic form of language use: spoken conversations. Using the task of predicting upcoming dialogue turns, we ask whether turn plausibility scores produced by state-of-the-art language models correlate with human judgements. We find a strong correlation for some but not all models: masked language models produce stronger correlations than auto-regressive models. In doing so, we quantify human performance on the response selection task for open-domain spoken conversation. To the best of our knowledge, this is the first such quantification.We find that response selection performance can be used as a coarse proxy for the strength of correlation with human judgements, however humans and models make different response selection mistakes. The model which produces the strongest correlation also outperforms human response selection performance. Through ablation studies, we show that pre-trained language models provide a useful basis for turn representations; however, fine-grained contextualisation, inclusion of dialogue structure information, and fine-tuning towards response selection all boost response selection accuracy by over 30 absolute points",
    "volume": "main",
    "checked": true,
    "id": "124992cbc40be95793318778c043d13d78323573",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.199": {
    "title": "Methods for Measuring, Updating, and Visualizing Factual Beliefs in Language Models",
    "abstract": "Language models can memorize a considerable amount of factual information during pretraining that can be elicited through prompting or finetuning models on tasks like question answering. In this paper, we discuss approaches to measuring model factual beliefs, updating incorrect factual beliefs in models, and visualizing graphical relationships between factual beliefs. Our main contributions include: (1) new metrics for evaluating belief-updating methods focusing on the logical consistency of beliefs, (2) a training objective for Sequential, Local, and Generalizing updates (SLAG) that improves the performance of existing hypernetwork approaches, and (3) the introduction of the belief graph, a new form of visualization for language models that shows relationships between stored model beliefs. Our experiments suggest that models show only limited consistency between factual beliefs, but update methods can both fix incorrect model beliefs and greatly improve their consistency. Although off-the-shelf optimizers are surprisingly strong belief-updating baselines, our learned optimizers can outperform them in more difficult settings than have been considered in past work",
    "volume": "main",
    "checked": true,
    "id": "f727f928e7e179307d8d4a1da2387393f2bd7915",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.200": {
    "title": "Improving Sign Recognition with Phonology",
    "abstract": "We use insights from research on American Sign Language (ASL) phonology to train models for isolated sign language recognition (ISLR), a step towards automatic sign language understanding. Our key insight is to explicitly recognize the role of phonology in sign production to achieve more accurate ISLR than existing work which does not consider sign language phonology. We train ISLR models that take in pose estimations of a signer producing a single sign to predict not only the sign but additionally its phonological characteristics, such as the handshape. These auxiliary predictions lead to a nearly 9% absolute gain in sign recognition accuracy on the WLASL benchmark, with consistent improvements in ISLR regardless of the underlying prediction model architecture. This work has the potential to accelerate linguistic research in the domain of signed languages and reduce communication barriers between deaf and hearing people",
    "volume": "main",
    "checked": true,
    "id": "60adf4789a6768db98aec4a5911cf90e2b659cdf",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.201": {
    "title": "Parameter-efficient Modularised Bias Mitigation via AdapterFusion",
    "abstract": "Large pre-trained language models contain societal biases and carry along these biases to downstream tasks. Current in-processing bias mitigation approaches (like adversarial training) impose debiasing by updating a model’s parameters, effectively transferring the model to a new, irreversible debiased state. In this work, we propose a novel approach to develop stand-alone debiasing functionalities separate from the model, which can be integrated into the model on-demand, while keeping the core model untouched. Drawing from the concept of AdapterFusion in multi-task learning, we introduce DAM (Debiasing with Adapter Modules) – a debiasing approach to first encapsulate arbitrary bias mitigation functionalities into separate adapters, and then add them to the model on-demand in order to deliver fairness qualities. We conduct a large set of experiments on three classification tasks with gender, race, and age as protected attributes. Our results show that DAM improves or maintains the effectiveness of bias mitigation, avoids catastrophic forgetting in a multi-attribute scenario, and maintains on-par task performance, while granting parameter-efficiency and easy switching between the original and debiased models",
    "volume": "main",
    "checked": true,
    "id": "e9c6a95146bf9c1d96c11f0fa9067fc87ed027ce",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.202": {
    "title": "LingMess: Linguistically Informed Multi Expert Scorers for Coreference Resolution",
    "abstract": "Current state-of-the-art coreference systems are based on a single pairwise scoring component, which assigns to each pair of mention spans a score reflecting their tendency to corefer to each other. We observe that different kinds of mention pairs require different information sources to assess their score. We present LingMess, a linguistically motivated categorization of mention-pairs into 6 types of coreference decisions and learn a dedicated trainable scoring function for each category. This significantly improves the accuracy of the pairwise scorer as well as of the overall coreference performance on the English Ontonotes coreference corpus and 5 additional datasets",
    "volume": "main",
    "checked": true,
    "id": "873c3846e26853d300ecf73a6e910ecf6a84e63c",
    "citation_count": 3
  },
  "https://aclanthology.org/2023.eacl-main.203": {
    "title": "Finding the Law: Enhancing Statutory Article Retrieval via Graph Neural Networks",
    "abstract": "Statutory article retrieval (SAR), the task of retrieving statute law articles relevant to a legal question, is a promising application of legal text processing. In particular, high-quality SAR systems can improve the work efficiency of legal professionals and provide basic legal assistance to citizens in need at no cost. Unlike traditional ad-hoc information retrieval, where each document is considered a complete source of information, SAR deals with texts whose full sense depends on complementary information from the topological organization of statute law. While existing works ignore these domain-specific dependencies, we propose a novel graph-augmented dense statute retriever (G-DSR) model that incorporates the structure of legislation via a graph neural network to improve dense retrieval performance. Experimental results show that our approach outperforms strong retrieval baselines on a real-world expert-annotated SAR dataset",
    "volume": "main",
    "checked": true,
    "id": "9a1f956fbe4f189e5d9ea310ab3eec377ff190aa",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.204": {
    "title": "Behavior Cloned Transformers are Neurosymbolic Reasoners",
    "abstract": "In this work, we explore techniques for augmenting interactive agents with information from symbolic modules, much like humans use tools like calculators and GPS systems to assist with arithmetic and navigation. We test our agent’s abilities in text games – challenging benchmarks for evaluating the multi-step reasoning abilities of game agents in grounded, language-based environments. Our experimental study indicates that injecting the actions from these symbolic modules into the action space of a behavior cloned transformer agent increases performance on four text game benchmarks that test arithmetic, navigation, sorting, and common sense reasoning by an average of 22%, allowing an agent to reach the highest possible performance on unseen games. This action injection technique is easily extended to new agents, environments, and symbolic modules",
    "volume": "main",
    "checked": true,
    "id": "c84bb36ab145cf903c1ad404008e0250f688c162",
    "citation_count": 3
  },
  "https://aclanthology.org/2023.eacl-main.205": {
    "title": "Bridging the Gap Between BabelNet and HowNet: Unsupervised Sense Alignment and Sememe Prediction",
    "abstract": "As the minimum semantic units of natural languages, sememes can provide precise representations of concepts. Despite the widespread utilization of lexical resources for semantic tasks, use of sememes is limited by a lack of available sememe knowledge bases. Recent efforts have been made to connect BabelNet with HowNet by automating sememe prediction. However, these methods depend on large manually annotated datasets. We propose to use sense alignment via a novel unsupervised and explainable method. Our method consists of four stages, each relaxing predefined constraints until a complete alignment of BabelNet synsets to HowNet senses is achieved. Experimental results demonstrate the superiority of our unsupervised method over previous supervised ones by an improvement of 12% overall F1 score, setting a new state of the art. Our work is grounded in an interpretable propagation of sememe information between lexical resources, and may benefit downstream applications which can incorporate sememe information",
    "volume": "main",
    "checked": true,
    "id": "65d3dd3dffe91cebebf43274c4a15bb231c9e192",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.206": {
    "title": "The StatCan Dialogue Dataset: Retrieving Data Tables through Conversations with Genuine Intents",
    "abstract": "We introduce the StatCan Dialogue Dataset consisting of 19,379 conversation turns between agents working at Statistics Canada and online users looking for published data tables. The conversations stem from genuine intents, are held in English or French, and lead to agents retrieving one of over 5000 complex data tables. Based on this dataset, we propose two tasks: (1) automatic retrieval of relevant tables based on a on-going conversation, and (2) automatic generation of appropriate agent responses at each turn. We investigate the difficulty of each task by establishing strong baselines. Our experiments on a temporal data split reveal that all models struggle to generalize to future conversations, as we observe a significant drop in performance across both tasks when we move from the validation to the test set. In addition, we find that response generation models struggle to decide when to return a table. Considering that the tasks pose significant challenges to existing models, we encourage the community to develop models for our task, which can be directly used to help knowledge workers find relevant tables for live chat users",
    "volume": "main",
    "checked": true,
    "id": "faedf875dbec7b2fc9b31a8f2aaa64fb7b8d1dec",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.207": {
    "title": "Question Generation Using Sequence-to-Sequence Model with Semantic Role Labels",
    "abstract": "Automatic generation of questions from text has gained increasing attention due to its useful applications. We propose a novel question generation method that combines the benefits of rule-based and neural sequence-to-sequence (Seq2Seq) models. The proposed method can automatically generate multiple questions from an input sentence covering different views of the sentence as in rule-based methods, while more complicated “rules” can be learned via the Seq2Seq model. The method utilizes semantic role labeling to convert training examples into their semantic representations, and then trains a Seq2Seq model over the semantic representations. Our extensive experiments on three real-world data sets show that the proposed method significantly improves the state-of-the-art neural question generation approaches",
    "volume": "main",
    "checked": true,
    "id": "19265db4b61efba7a6a882e89dc43f86ba98b3b6",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.208": {
    "title": "StyLEx: Explaining Style Using Human Lexical Annotations",
    "abstract": "Large pre-trained language models have achieved impressive results on various style classification tasks, but they often learn spurious domain-specific words to make predictions (Hayati et al., 2021). While human explanation highlights stylistic tokens as important features for this task, we observe that model explanations often do not align with them. To tackle this issue, we introduce StyLEx, a model that learns from human annotated explanations of stylistic features and jointly learns to perform the task and predict these features as model explanations. Our experiments show that StyLEx can provide human like stylistic lexical explanations without sacrificing the performance of sentence-level style prediction on both in-domain and out-of-domain datasets. Explanations from StyLEx show significant improvements in explanation metrics (sufficiency, plausibility) and when evaluated with human annotations. They are also more understandable by human judges compared to the widely-used saliency-based explanation baseline",
    "volume": "main",
    "checked": true,
    "id": "1b1b6e429c022fecb524f755e3cf36961566bcb7",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.209": {
    "title": "Comparing Intrinsic Gender Bias Evaluation Measures without using Human Annotated Examples",
    "abstract": "Numerous types of social biases have been identified in pre-trained language models (PLMs), and various intrinsic bias evaluation measures have been proposed for quantifying those social biases.Prior works have relied on human annotated examples to compare existing intrinsic bias evaluation measures.However, this approach is not easily adaptable to different languages nor amenable to large scale evaluations due to the costs and difficulties when recruiting human annotators.To overcome this limitation, we propose a method to compare intrinsic gender bias evaluation measures without relying on human-annotated examples.Specifically, we create multiple bias-controlled versions of PLMs using varying amounts of male vs. female gendered sentences, mined automatically from an unannotated corpus using gender-related word lists. Next, each bias-controlled PLM is evaluated using an intrinsic bias evaluation measure, and the rank correlation between the computed bias scores and the gender proportions used to fine-tune the PLMs is computed.Experiments on multiple corpora and PLMs repeatedly show that the correlations reported by our proposed method that does not require human annotated examples are comparable to those computed using human annotated examples in prior work",
    "volume": "main",
    "checked": true,
    "id": "c21cd6401032f30c4165754d29bd0a7e90582bfd",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.210": {
    "title": "Faithfulness-Aware Decoding Strategies for Abstractive Summarization",
    "abstract": "Despite significant progress in understanding and improving faithfulness in abstractive summarization, the question of how decoding strategies affect faithfulness is less studied. We present a systematic study of the effect of generation techniques such as beam search and nucleus sampling on faithfulness in abstractive summarization. We find a consistent trend where beam search with large beam sizes produces the most faithful summaries while nucleus sampling generates the least faithful ones. We propose two faithfulness-aware generation methods to further improve faithfulness over current generation techniques: (1) ranking candidates generated by beam search using automatic faithfulness metrics and (2) incorporating lookahead heuristics that produce a faithfulness score on the future summary. We show that both generation methods significantly improve faithfulness across two datasets as evaluated by four automatic faithfulness metrics and human evaluation. To reduce computational cost, we demonstrate a simple distillation approach that allows the model to generate faithful summaries with just greedy decoding",
    "volume": "main",
    "checked": true,
    "id": "a4f116e63c30fb8ac11d90ed4d106cc6e9dc30cf",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.211": {
    "title": "Dynamic Benchmarking of Masked Language Models on Temporal Concept Drift with Multiple Views",
    "abstract": "Temporal concept drift refers to the problem of data changing over time. In the field of NLP, that would entail that language (e.g. new expressions, meaning shifts) and factual knowledge (e.g. new concepts, updated facts) evolve over time. Focusing on the latter, we benchmark 11 pretrained masked language models (MLMs) on a series of tests designed to evaluate the effect of temporal concept drift, as it is crucial that widely used language models remain up-to-date with the ever-evolving factual updates of the real world. Specifically, we provide a holistic framework that (1) dynamically creates temporal test sets of any time granularity (e.g. month, quarter, year) of factual data from Wikidata, (2) constructs fine-grained splits of tests (e.g. updated, new, unchanged facts) to ensure comprehensive analysis, and (3) evaluates MLMs in three distinct ways (single-token probing, multi-token generation, MLM scoring). In contrast to prior work, our framework aims to unveil how robust an MLM is over time and thus to provide a signal in case it has become outdated, by leveraging multiple views of evaluation",
    "volume": "main",
    "checked": true,
    "id": "aad8b1ca56aef4a7512789102ce0cc3fc8b064e4",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.212": {
    "title": "Real-Time Visual Feedback to Guide Benchmark Creation: A Human-and-Metric-in-the-Loop Workflow",
    "abstract": "Recent research has shown that language models exploit ‘artifacts’ in benchmarks to solve tasks, rather than truly learning them, leading to inflated model performance. In pursuit of creating better benchmarks, we propose VAIDA, a novel benchmark creation paradigm for NLP, that focuses on guiding crowdworkers, an under-explored facet of addressing benchmark idiosyncrasies. VAIDA facilitates sample correction by providing realtime visual feedback and recommendations to improve sample quality. Our approach is domain, model, task, and metric agnostic, and constitutes a paradigm shift for robust, validated, and dynamic benchmark creation via human-and-metric-in-the-loop workflows. We evaluate via expert review and a user study with NASA TLX. We find that VAIDA decreases effort, frustration, mental, and temporal demands of crowdworkers and analysts, simultaneously increasing the performance of both user groups with a 45.8% decrease in the level of artifacts in created samples. As a by product of our user study, we observe that created samples are adversarial across models, leading to decreases of 31.3% (BERT), 22.5% (RoBERTa), 14.98% (GPT-3 fewshot) in performance",
    "volume": "main",
    "checked": true,
    "id": "1e2839669f61fd99c524690e238f6cbe505e5ffd",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.213": {
    "title": "COMPS: Conceptual Minimal Pair Sentences for testing Robust Property Knowledge and its Inheritance in Pre-trained Language Models",
    "abstract": "A characteristic feature of human semantic cognition is its ability to not only store and retrieve the properties of concepts observed through experience, but to also facilitate the inheritance of properties (can breathe) from superordinate concepts (animal) to their subordinates (dog)—i.e. demonstrate property inheritance. In this paper, we present COMPS, a collection of minimal pair sentences that jointly tests pre-trained language models (PLMs) on their ability to attribute properties to concepts and their ability to demonstrate property inheritance behavior. Analyses of 22 different PLMs on COMPS reveal that they can easily distinguish between concepts on the basis of a property when they are trivially different, but find it relatively difficult when concepts are related on the basis of nuanced knowledge representations. Furthermore, we find that PLMs can show behaviors suggesting successful property inheritance in simple contexts, but fail in the presence of distracting information, which decreases the performance of many models sometimes even below chance. This lack of robustness in demonstrating simple reasoning raises important questions about PLMs’ capacity to make correct inferences even when they appear to possess the prerequisite knowledge",
    "volume": "main",
    "checked": true,
    "id": "6fb0b072c4fcdc0c78218bfd1b181fd562f07cd2",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.214": {
    "title": "Probabilistic Robustness for Data Filtering",
    "abstract": "We introduce our probabilistic robustness rewarded data optimization (PRoDO) approach as a framework to enhance the model’s generalization power by selecting training data that optimizes our probabilistic robustness metrics. We use proximal policy optimization (PPO) reinforcement learning to approximately solve the computationally intractable training subset selection problem. The PPO’s reward is defined as our (${alpha,{epsilon, {gamma$)-Robustness that measures performance consistency over multiple domains by simulating unknown test sets in real-world scenarios using a leaving-one-out strategy. We demonstrate that our PRoDO effectively filters data that lead to significantly higher prediction accuracy and robustness on unknown-domain test sets. Our experiments achieve up to +17.2{% increase of accuracy (+25.5{% relatively) in sentiment analysis, and -28.05 decrease of perplexity (-32.1{% relatively) in language modeling.In addition, our probabilistic (${alpha,{epsilon, {gamma$)-Robustness definition serves as an evaluation metric with higher levels of agreement with human annotations than typical performance-based metrics",
    "volume": "main",
    "checked": true,
    "id": "a08a242e6a90ee3cd0e0540321040986dac85f68",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.215": {
    "title": "Unsupervised Improvement of Factual Knowledge in Language Models",
    "abstract": "Masked language modeling (MLM) plays a key role in pretraining large language models. But the MLM objective is often dominated by high-frequency words that are sub-optimal for learning factual knowledge. In this work, we propose an approach for influencing MLM pretraining in a way that can improve language model performance on a variety of knowledge-intensive tasks. We force the language model to prioritize informative words in a fully unsupervised way. Experiments demonstrate that the proposed approach can significantly improve the performance of pretrained language models on tasks such as factual recall, question answering, sentiment analysis, and natural language inference in a closed-book setting",
    "volume": "main",
    "checked": true,
    "id": "6028780bfb3728292d37c07951e3f463fae0981e",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.216": {
    "title": "Learning to Ignore Adversarial Attacks",
    "abstract": "Despite the strong performance of current NLP models, they can be brittle against adversarial attacks. To enable effective learning against adversarial inputs, we introduce the use of rationale models that can explicitly learn to ignore attack tokens. We find that the rationale models can successfully ignore over 90% of attack tokens. This approach leads to consistent sizable improvements (~10%) over baseline models in robustness on three datasets for both BERT and RoBERTa, and also reliably outperforms data augmentation with adversarial examples alone. In many cases, we find that our method is able to close the gap between model performance on a clean test set and an attacked test set and hence reduce the effect of adversarial attacks",
    "volume": "main",
    "checked": true,
    "id": "b888c632b75631c70bc5323b17fe975f4fc8500e",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.217": {
    "title": "Should You Mask 15% in Masked Language Modeling?",
    "abstract": "Masked language models (MLMs) conventionally mask 15% of tokens due to the belief that more masking would leave insufficient context to learn good representations; this masking rate has been widely used, regardless of model sizes or masking strategies. In this work, we revisit this important choice of MLM pre-training. We first establish that 15% is not universally optimal, and larger models should adopt a higher masking rate. Specifically, we find that masking 40% outperforms 15% for BERT-large size models on GLUE and SQuAD. Interestingly, an extremely high masking rate of 80% can still preserve 95% fine-tuning performance and most of the accuracy in linguistic probing, challenging the conventional wisdom about the role of the masking rate. We then examine the interplay between masking rates and masking strategies and find that uniform masking requires a higher masking rate compared to sophisticated masking strategies such as span or PMI masking. Finally, we argue that increasing the masking rate has two distinct effects: it leads to more corruption, which makes the prediction task more difficult; it also enables more predictions, which benefits optimization. Using this framework, we revisit BERT’s 80-10-10 corruption strategy. Together, our results contribute to a better understanding of MLM pre-training",
    "volume": "main",
    "checked": true,
    "id": "2d439ec2c301d058bd4a8b4743328e3d9939625e",
    "citation_count": 49
  },
  "https://aclanthology.org/2023.eacl-main.218": {
    "title": "How do Words Contribute to Sentence Semantics? Revisiting Sentence Embeddings with a Perturbation Method",
    "abstract": "Understanding sentence semantics requires an interpretation of the main information from a concrete context. To investigate how individual word contributes to sentence semantics, we propose a perturbation method for unsupervised semantic analysis. We next re-examine SOTA sentence embedding models’ ability to capture the main semantics of a sentence by developing a new evaluation metric to adapt sentence compression datasets for automatic evaluation. Results on three datasets show that unsupervised discourse relation recognition can serve as a general inference task that can more effectively aggregate information to essential contents than several SOTA unsupervised sentence embedding models",
    "volume": "main",
    "checked": true,
    "id": "8afe3743913bc8743c75fcb9d8bab5d0c489a6aa",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.219": {
    "title": "AutoTriggER: Label-Efficient and Robust Named Entity Recognition with Auxiliary Trigger Extraction",
    "abstract": "Deep neural models for named entity recognition (NER) have shown impressive results in overcoming label scarcity and generalizing to unseen entities by leveraging distant supervision and auxiliary information such as explanations. However, the costs of acquiring such additional information are generally prohibitive. In this paper, we present a novel two-stage framework (AutoTriggER) to improve NER performance by automatically generating and leveraging “entity triggers” which are human-readable cues in the text that help guide the model to make better decisions. Our framework leverages post-hoc explanation to generate rationales and strengthens a model’s prior knowledge using an embedding interpolation technique. This approach allows models to exploit triggers to infer entity boundaries and types instead of solely memorizing the entity words themselves. Through experiments on three well-studied NER datasets, AutoTriggER shows strong label-efficiency, is capable of generalizing to unseen entities, and outperforms the RoBERTa-CRF baseline by nearly 0.5 F1 points on average",
    "volume": "main",
    "checked": true,
    "id": "a5f22c360f8a85f373f7de7c30a4146176293604",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.220": {
    "title": "Incorporating Task-Specific Concept Knowledge into Script Learning",
    "abstract": "In this paper, we present Tetris, a new task of Goal-Oriented Script Completion. Unlike previous work, it considers a more realistic and general setting, where the input includes not only the goal but also additional user context, including preferences and history. To address this problem, we propose a novel approach, which uses two techniques to improve performance: (1) concept prompting, and (2) script-oriented contrastive learning that addresses step repetition and hallucination problems. On our WikiHow-based dataset, we find that both methods improve performance",
    "volume": "main",
    "checked": true,
    "id": "41c1fcccb6609be71475aee3e2fa9bd4e425e977",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.221": {
    "title": "DeepMaven: Deep Question Answering on Long-Distance Movie/TV Show Videos with Multimedia Knowledge Extraction and Synthesis",
    "abstract": "Long video content understanding poses a challenging set of research questions as it involves long-distance, cross-media reasoning and knowledge awareness. In this paper, we present a new benchmark for this problem domain, targeting the task of deep movie/TV question answering (QA) beyond previous work’s focus on simple plot summary and short video moment settings. We define several baselines based on direct retrieval of relevant context for long-distance movie QA. Observing that real-world QAs may require higher-order multi-hop inferences, we further propose a novel framework, called the DeepMaven, which extracts events, entities, and relations from the rich multimedia content in long videos to pre-construct movie knowledge graphs (movieKGs), and at the time of QA inference, complements general semantics with structured knowledge for more effective information retrieval and knowledge reasoning. We also introduce our recently collected DeepMovieQA dataset, including 1,000 long-form QA pairs from 41 hours of videos, to serve as a new and useful resource for future work. Empirical results show the DeepMaven performs competitively for both the new DeepMovieQA and the pre-existing MovieQA dataset",
    "volume": "main",
    "checked": true,
    "id": "db4063aac6bd51ef08473b986bda71fb3cf16e87",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.222": {
    "title": "Salient Span Masking for Temporal Understanding",
    "abstract": "Salient Span Masking (SSM) has shown itself to be an effective strategy to improve closed-book question answering performance. SSM extends general masked language model pretraining by creating additional unsupervised training sentences that mask a single entity or date span, thus oversampling factual information.Despite the success of this paradigm, the span types and sampling strategies are relatively arbitrary and not widely studied for other tasks. Thus, we investigate SSM from the perspective of temporal tasks, where learning a good representation of various temporal expressions is important. To that end, we introduce Temporal Span Masking (TSM) intermediate training.First, we find that SSM alone improves the downstream performance on three temporal tasks by an avg. +5.8 points. Further, we are able to achieve additional improvements (avg. +0.29 points) by adding the TSM task. These comprise the new best reported results on the targeted tasks. Our analysis suggests that the effectiveness of SSM stems from the sentences chosen in the training data rather than the mask choice: sentences with entities frequently also contain temporal expressions. Nonetheless, the additional targeted spans of TSM can still improve performance, especially in a zero-shot context",
    "volume": "main",
    "checked": true,
    "id": "24ab6356b355b29a3770db56dd1f2200cdd987fa",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.223": {
    "title": "PECO: Examining Single Sentence Label Leakage in Natural Language Inference Datasets through Progressive Evaluation of Cluster Outliers",
    "abstract": "Building natural language inference (NLI) benchmarks that are both challenging for modern techniques, and free from shortcut biases is difficult. Chief among these biases is “single sentence label leakage,” where annotator-introduced spurious correlations yield datasets where the logical relation between (premise, hypothesis) pairs can be accurately predicted from only a single sentence, something that should in principle be impossible. We demonstrate that despite efforts to reduce this leakage, it persists in modern datasets that have been introduced since its 2018 discovery. To enable future amelioration efforts, introduce a novel model-driven technique, the progressive evaluation of cluster outliers (PECO) which enables both the objective measurement of leakage, and the automated detection of subpopulations in the data which maximally exhibit it",
    "volume": "main",
    "checked": true,
    "id": "0ae82f96cb8c1d2da5d8284765dd76a139ba6ff6",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.224": {
    "title": "Weakly-Supervised Questions for Zero-Shot Relation Extraction",
    "abstract": "Zero-Shot Relation Extraction (ZRE) is the task of Relation Extraction where the training and test sets have no shared relation types. This very challenging domain is a good test of a model’s ability to generalize. Previous approaches to ZRE reframed relation extraction as Question Answering (QA), allowing for the use of pre-trained QA models. However, this method required manually creating gold question templates for each new relation. Here, we do away with these gold templates and instead learn a model that can generate questions for unseen relations. Our technique can successfully translate relation descriptions into relevant questions, which are then leveraged to generate the correct tail entity. On tail entity extraction, we outperform the previous state-of-the-art by more than 16 F1 points without using gold question templates. On the RE-QA dataset where no previous baseline for relation extraction exists, our proposed algorithm comes within 0.7 F1 points of a system that uses gold question templates. Our model also outperforms the state-of-the-art ZRE baselines on the FewRel and WikiZSL datasets, showing that QA models no longer need template questions to match the performance of models specifically tailored to the ZRE task. Our implementation is available at https://github.com/fyshelab/QA-ZRE",
    "volume": "main",
    "checked": true,
    "id": "efe94fab86bdc04c7f9eae8d2ef0421ccc846893",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.225": {
    "title": "DiffQG: Generating Questions to Summarize Factual Changes",
    "abstract": "Identifying the difference between two versions of the same article is useful to update knowledge bases and to understand how articles evolve. Paired texts occur naturally in diverse situations: reporters write similar news stories and maintainers of authoritative websites must keep their information up to date. We propose representing factual changes between paired documents as question-answer pairs, where the answer to the same question differs between two versions. We find that question-answer pairs can flexibly and concisely capture the updated contents. Provided with paired documents, annotators identify questions that are answered by one passage but answered differently or cannot be answered by the other. We release DiffQG which consists of 759 QA pairs and 1153 examples of paired passages with no factual change. These questions are intended to be both unambiguous and information-seeking and involve complex edits, pushing beyond the capabilities of current question generation and factual change detection systems. Our dataset summarizes the changes between two versions of the document as questions and answers, studying automatic update summarization in a novel way",
    "volume": "main",
    "checked": true,
    "id": "67c7f7b98cdd80d77000d6f5c0935052b1c1ad50",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.226": {
    "title": "Contextual Dynamic Prompting for Response Generation in Task-oriented Dialog Systems",
    "abstract": "Response generation is one of the critical components in task-oriented dialog systems. Existing studies have shown that large pre-trained language models can be adapted to this task. The typical paradigm of adapting such extremely large language models would be by fine-tuning on the downstream tasks which is not only time-consuming but also involves significant resources and access to fine-tuning data. Prompting (Schick and Schütze, 2020) has been an alternative to fine-tuning in many NLP tasks. In our work, we explore the idea of using prompting for response generation in task-oriented dialog systems. Specifically, we propose an approach that performs contextual dynamic prompting where the prompts are learnt from dialog contexts. We aim to distill useful prompting signals from the dialog context. On experiments with MultiWOZ 2.2 dataset (Zang et al., 2020), we show that contextual dynamic prompts improve response generation in terms of combined score (Mehri et al., 2019) by 3 absolute points, and an additional 17 points when dialog states are incorporated. Furthermore, we carried out human annotation on these conversations and found that agents which incorporate context are preferred over agents with vanilla prefix-tuning",
    "volume": "main",
    "checked": true,
    "id": "66b217600c49195ce01cb24d068ebae5ca7bfd63",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.227": {
    "title": "Why Can't Discourse Parsing Generalize? A Thorough Investigation of the Impact of Data Diversity",
    "abstract": "Recent advances in discourse parsing performance create the impression that, as in other NLP tasks, performance for high-resource languages such as English is finally becoming reliable. In this paper we demonstrate that this is not the case, and thoroughly investigate the impact of data diversity on RST parsing stability. We show that state-of-the-art architectures trained on the standard English newswire benchmark do not generalize well, even within the news domain. Using the two largest RST corpora of English with text from multiple genres, we quantify the impact of genre diversity in training data for achieving generalization to text types unseen during training. Our results show that a heterogeneous training regime is critical for stable and generalizable models, across parser architectures. We also provide error analyses of model outputs and out-of-domain performance. To our knowledge, this study is the first to fully evaluate cross-corpus RST parsing generalizability on complete trees, examine between-genre degradation within an RST corpus, and investigate the impact of genre diversity in training data composition",
    "volume": "main",
    "checked": true,
    "id": "c50ed12a45754221a1e9c65436fec6dd6dddebcc",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.228": {
    "title": "Enriching Biomedical Knowledge for Low-resource Language Through Large-scale Translation",
    "abstract": "Biomedical data and benchmarks are highly valuable yet very limited in low-resource languages other than English, such as Vietnamese. In this paper, we use a state-of-the-art translation model in English-Vietnamese to translate and produce both pretrained and supervised data in the biomedical domains. Thanks to such large-scale translation, we introduce ViPubmedT5, a pretrained Encoder-Decoder Transformer model trained on 20 million translated abstracts from the high-quality public PubMed corpus. ViPubMedT5 demonstrates state-of-the-art results on two different biomedical benchmarks in summarization and acronym disambiguation. Further, we release ViMedNLI - a new NLP task in Vietnamese translated from MedNLI using the recently public En-vi translation model and carefully refined by human experts, with evaluations of existing methods against ViPubmedT5",
    "volume": "main",
    "checked": true,
    "id": "54772ffae642a87b9a6122a6f1bae76b926a7230",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.229": {
    "title": "Syntax-guided Neural Module Distillation to Probe Compositionality in Sentence Embeddings",
    "abstract": "Past work probing compositionality in sentence embedding models faces issues determining the causal impact of implicit syntax representations. Given a sentence, we construct a neural module net based on its syntax parse and train it end-to-end to approximate the sentence’s embedding generated by a transformer model. The distillability of a transformer to a Syntactic NeurAl Module Net (SynNaMoN) then captures whether syntax is a strong causal model of its compositional ability. Furthermore, we address questions about the geometry of semantic composition by specifying individual SynNaMoN modules’ internal architecture & linearity. We find differences in the distillability of various sentence embedding models that broadly correlate with their performance, but observe that distillability doesn’t considerably vary by model size. We also present preliminary evidence that much syntax-guided composition in sentence embedding models is linear, and that non-linearities may serve primarily to handle non-compositional phrases",
    "volume": "main",
    "checked": true,
    "id": "714895fbbf29e4eedc6b30ada3f65541a88f3792",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.230": {
    "title": "Closed-book Question Generation via Contrastive Learning",
    "abstract": "Question Generation (QG) is a fundamental NLP task for many downstream applications. Recent studies on open-book QG, where supportive answer-context pairs are provided to models, have achieved promising progress. However, generating natural questions under a more practical closed-book setting that lacks these supporting documents still remains a challenge. In this work, we propose a new QG model for this closed-book setting that is designed to better understand the semantics of long-form abstractive answers and store more information in its parameters through contrastive learning and an answer reconstruction module. Through experiments, we validate the proposed QG model on both public datasets and a new WikiCQA dataset. Empirical results show that the proposed QG model outperforms baselines in both automatic evaluation and human evaluation. In addition, we show how to leverage the proposed model to improve existing question-answering systems. These results further indicate the effectiveness of our QG model for enhancing closed-book question-answering tasks",
    "volume": "main",
    "checked": true,
    "id": "7b6715f3a7729385456ebde8c91451e85caf19b4",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.231": {
    "title": "A Hybrid Detection and Generation Framework with Separate Encoders for Event Extraction",
    "abstract": "The event extraction task typically consists of event detection and event argument extraction. Most previous work models these two subtasks with shared representation by multiple classification tasks or a unified generative approach. In this paper, we revisit this pattern and propose to use independent encoders to model event detection and event argument extraction, respectively, and use the output of event detection to construct the input of event argument extraction. In addition, we use token-level features to precisely control the fusion between two encoders to achieve joint bridging training rather than directly reusing representations between different tasks. Through a series of careful experiments, we demonstrate the importance of avoiding feature interference of different tasks and the importance of joint bridging training. We achieved competitive results on standard benchmarks (ACE05-E, ACE05-E+, and ERE-EN) and established a solid baseline",
    "volume": "main",
    "checked": true,
    "id": "2d0406440268fe315023587507ad574f571339ea",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.232": {
    "title": "Path Spuriousness-aware Reinforcement Learning for Multi-Hop Knowledge Graph Reasoning",
    "abstract": "Multi-hop reasoning, a prevalent approach for query answering, aims at inferring new facts along reasonable paths over a knowledge graph.Reinforcement learning methods can be adopted by formulating the problem into a Markov decision process.However, common suffering within RL-based reasoning models is that the agent can be biased to spurious paths which coincidentally lead to the correct answer with poor explanation.In this work, we take a deep dive into this phenomenon and define a metric named Path Spuriousness (PS), to quantitatively estimate to what extent a path is spurious.Guided by the definition of PS, we design a model with a new reward that considers both answer accuracy and path reasonableness.We test our method on four datasets and experiments reveal that our method considerably enhances the agent’s capacity to prevent spurious paths while keeping comparable to state-of-the-art performance",
    "volume": "main",
    "checked": true,
    "id": "8e29b969cbe3ec2bd6d26195052f9ebe66649a5a",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.233": {
    "title": "Self-Adaptive Named Entity Recognition by Retrieving Unstructured Knowledge",
    "abstract": "Although named entity recognition (NER) helps us to extract domain-specific entities from text (e.g., artists in the music domain), it is costly to create a large amount of training data or a structured knowledge base to perform accurate NER in the target domain. Here, we propose self-adaptive NER, which retrieves external knowledge from unstructured text to learn the usages of entities that have not been learned well. To retrieve useful knowledge for NER, we design an effective two-stage model that retrieves unstructured knowledge using uncertain entities as queries. Our model predicts the entities in the input and then finds those of which the prediction is not confident. Then, it retrieves knowledge by using these uncertain entities as queries and concatenates the retrieved text to the original input to revise the prediction. Experiments on CrossNER datasets demonstrated that our model outperforms strong baselines by 2.35 points in F1 metric",
    "volume": "main",
    "checked": true,
    "id": "1b654468d8f90e22c42929dbbb67833e6bc334b4",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.234": {
    "title": "When Do Pre-Training Biases Propagate to Downstream Tasks? A Case Study in Text Summarization",
    "abstract": "Large language models (LLMs) are subject to sociocultural and other biases previously identified using intrinsic evaluations. However, when and how these intrinsic biases in pre-trained LM representations propagate to downstream, fine-tuned NLP tasks like summarization is not well understood. In this work, we investigate one type of bias—name-nationality bias—and trace it from the pre-training stage to a downstream summarization task across multiple summarization modeling choices. We show that these biases manifest themselves as hallucinations in summarization, leading to factually incorrect summaries. We also find that this propagation of biases is algorithm-dependent: more abstractive models allow biases to propagate more directly to downstream tasks as hallucinated facts. Building on these observations, we further analyze how changes to the adaptation method and fine-tuning data set affect name nationality biases and show that while they can reduce the overall rate of hallucinations, they do not change the types of biases that do appear",
    "volume": "main",
    "checked": true,
    "id": "f48287e9ed131ff8ffa79b66717887c5af74f203",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.235": {
    "title": "BERT Shows Garden Path Effects",
    "abstract": "Garden path sentences (i.e. “the horse raced past the barn fell”) are sentences that readers initially incorrectly parse, requiring partial or total re-analysis of the sentence structure. Given human difficulty in parsing garden paths, we aim to compare transformer language models’ performance on these sentences. We assess a selection of models from the BERT family which have been fine-tuned on the question-answering task, and evaluate each model’s performance on comprehension questions based on garden path and control sentences. We then further investigate the semantic roles assigned to arguments of verbs in garden path and control sentences by utilizing a probe task to directly assess which semantic role(s) the model assigns. We find that the models have relatively low performance in certain instances of question answering based on garden path contexts, and the model incorrectly assigns semantic roles, aligning for the most part with human performance",
    "volume": "main",
    "checked": true,
    "id": "d536476ea34eb224f9385a78bb6b74efd40de4f6",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.236": {
    "title": "Models Teaching Models: Improving Model Accuracy with Slingshot Learning",
    "abstract": "One significant obstacle to the successful application of machine learning to real-world data is that of labeling: it is often prohibitively expensive to pay an ethical amount for the human labor required to label a dataset successfully. Human-in-the-loop techniques such as active learning can reduce the cost, but the required human time is still significant and many fixed costs remain. Another option is to employ pre-trained transformer models as labelers at scale, which can yield reasonable accuracy and significant cost savings. However, such models can still be expensive to use due to their high computational requirements, and the opaque nature of these models is not always suitable in applied social science and public use contexts. We propose a novel semi-supervised method, named Slingshot Learning, in which we iteratively and selectively augment a small human-labeled dataset with labels from a high-quality “teacher” model to slingshot the performance of a “student” model in a cost-efficient manner. This reduces the accuracy trade-off required to use these simpler algorithms without disrupting their benefits, such as lower compute requirements, better interpretability, and faster inference. We define and discuss the slingshot learning algorithm and demonstrate its effectiveness on several benchmark tasks, using ALBERT to teach a simple Naive Bayes binary classifier. We experimentally demonstrate that Slingshot learning effectively decreases the performance gap between the teacher and student models. We also analyze its performance in several scenarios and compare different variants of the algorithm",
    "volume": "main",
    "checked": true,
    "id": "e9a847bc7934559f8667602adbb1ff5d3207c8e5",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.237": {
    "title": "A Federated Approach for Hate Speech Detection",
    "abstract": "Hate speech detection has been the subject of high research attention, due to the scale of content created on social media. In spite of the attention and the sensitive nature of the task, privacy preservation in hate speech detection has remained under-studied.The majority of research has focused on centralised machine learning infrastructures which risk leaking data. In this paper, we show that using federated machine learning can help address privacy the concerns that are inherent to hate speech detection while obtaining up to 6.81% improvement in terms of F1-score",
    "volume": "main",
    "checked": true,
    "id": "fef39f373e507fcfd867830ccd824cbac561278c",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.238": {
    "title": "Learning the Legibility of Visual Text Perturbations",
    "abstract": "Many adversarial attacks in NLP perturb text in puts to produce visually similar strings (‘ergo’, ‘εrgo’) which are legible to humans but degrade model performance. Although preserving legibility is a necessary condition for text perturbation, little work has been done to systematically characterize it; instead, legibility is typically loosely enforced via intuitions around the nature and extent of perturbations. Particularly, it is unclear to what extent can inputs be perturbed while preserving legibility, or how to quantify the legibility of a perturbed string. In this work, we address this gap by learning models that predict the legibility of a perturbed string, and rank candidate perturbations based on their legibility. To do so, we collect and release LEGIT, a human-annotated dataset comprising the legibility of visually perturbed text. Using this dataset, we build both text- and vision-based models which achieve up to 0.91 F score in predicting whether an input is legible, and an accuracy of 0.86 in predicting which of two given perturbations is more legible. Additionally, we discover that legible perturbations from the LEGIT dataset are more effective at lowering the performance of NLP models than best-known attack strategies, suggesting that current models may be vulnerable to a broad range of perturbations beyond what is captured by existing visual attacks",
    "volume": "main",
    "checked": true,
    "id": "f3d894cf6f7be14a545019f4621ccce41f45b088",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.239": {
    "title": "DyLoRA: Parameter-Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation",
    "abstract": "With the ever-growing size of pretrained models (PMs), fine-tuning them has become more expensive and resource-hungry. As a remedy, low-rank adapters (LoRA) keep the main pretrained weights of the model frozen and just introduce some learnable truncated SVD modules (so-called LoRA blocks) to the model. While LoRA blocks are parameter-efficient, they suffer from two major problems: first, the size of these blocks is fixed and cannot be modified after training (for example, if we need to change the rank of LoRA blocks, then we need to re-train them from scratch); second, optimizing their rank requires an exhaustive search and effort. In this work, we introduce a dynamic low-rank adaptation (DyLoRA) technique to address these two problems together. Our DyLoRA method trains LoRA blocks for a range of ranks instead of a single rank by sorting the representation learned by the adapter module at different ranks during training. We evaluate our solution on different natural language understanding (GLUE benchmark) and language generation tasks (E2E, DART and WebNLG) using different pretrained models such as RoBERTa and GPT with different sizes. Our results show that we can train dynamic search-free models with DyLoRA at least 4 to 7 times (depending to the task) faster than LoRA without significantly compromising performance. Moreover, our models can perform consistently well on a much larger range of ranks compared to LoRA",
    "volume": "main",
    "checked": true,
    "id": "85e959eef45114974c8f8643e88af23936fff3d1",
    "citation_count": 2
  },
  "https://aclanthology.org/2023.eacl-main.240": {
    "title": "Conversational Emotion-Cause Pair Extraction with Guided Mixture of Experts",
    "abstract": "Emotion-Cause Pair Extraction (ECPE) task aims to pair all emotions and corresponding causes in documents.ECPE is an important task for developing human-like responses.However, previous ECPE research is conducted based on news articles, which has different characteristics compared to dialogues.To address this issue, we propose a Pair-Relationship Guided Mixture-of-Experts (PRG-MoE) model, which considers dialogue features (e.g., speaker information).PRG-MoE automatically learns relationship between utterances and advises a gating network to incorporate dialogue features in the evaluation, yielding substantial performance improvement.We employ a new ECPE dataset, which is an English dialogue dataset, with more emotion-cause pairs in documents than news articles.We also propose Cause Type Classification that classifies emotion-cause pairs according to the types of the cause of a detected emotion.For reproducing the results, we make available all our code and data",
    "volume": "main",
    "checked": true,
    "id": "b2ba81a09a549ced2d2696742c4e0dcfe290894f",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.241": {
    "title": "Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey",
    "abstract": "Recent advances in the capacity of large language models to generate human-like text have resulted in their increased adoption in user-facing settings. In parallel, these improvements have prompted a heated discourse around the risks of societal harms they introduce, whether inadvertent or malicious. Several studies have explored these harms and called for their mitigation via development of safer, fairer models. Going beyond enumerating the risks of harms, this work provides a survey of practical methods for addressing potential threats and societal harms from language generation models. We draw on several prior works’ taxonomies of language model risks to present a structured overview of strategies for detecting and ameliorating different kinds of risks/harms of language generators. Bridging diverse strands of research, this survey aims to serve as a practical guide for both LM researchers and practitioners, with explanations of different strategies’ motivations, their limitations, and open problems for future research",
    "volume": "main",
    "checked": true,
    "id": "44e02c4735e2e6cce3214e30bba1e30a92804bdd",
    "citation_count": 5
  },
  "https://aclanthology.org/2023.eacl-main.242": {
    "title": "TraVLR: Now You See It, Now You Don't! A Bimodal Dataset for Evaluating Visio-Linguistic Reasoning",
    "abstract": "Numerous visio-linguistic (V+L) representation learning methods have been developed, yet existing datasets do not adequately evaluate the extent to which they represent visual and linguistic concepts in a unified space. We propose several novel evaluation settings for V+L models, including cross-modal transfer. Furthermore, existing V+L benchmarks often report global accuracy scores on the entire dataset, making it difficult to pinpoint the specific reasoning tasks that models fail and succeed at. We present TraVLR, a synthetic dataset comprising four V+L reasoning tasks. TraVLR’s synthetic nature allows us to constrain its training and testing distributions along task-relevant dimensions, enabling the evaluation of out-of-distribution generalisation. Each example in TraVLR redundantly encodes the scene in two modalities, allowing either to be dropped or added during training or testing without losing relevant information. We compare the performance of four state-of-the-art V+L models, finding that while they perform well on test examples from the same modality, they all fail at cross-modal transfer and have limited success accommodating the addition or deletion of one modality. We release TraVLR as an open challenge for the research community",
    "volume": "main",
    "checked": true,
    "id": "03c4c4ed9a153a36debea5e53a2437a4c5c5972b",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.243": {
    "title": "Paraphrase Acquisition from Image Captions",
    "abstract": "We propose to use image captions from the Web as a previously underutilized resource for paraphrases (i.e., texts with the same “message”) and to create and analyze a corresponding dataset. When an image is reused on the Web, an original caption is often assigned. We hypothesize that different captions for the same image naturally form a set of mutual paraphrases. To demonstrate the suitability of this idea, we analyze captions in the English Wikipedia, where editors frequently relabel the same image for different articles. The paper introduces the underlying mining technology, the resulting Wikipedia-IPC dataset, and compares known paraphrase corpora with respect to their syntactic and semantic paraphrase similarity to our new resource. In this context, we introduce characteristic maps along the two similarity dimensions to identify the style of paraphrases coming from different sources. An annotation study demonstrates the high reliability of the algorithmically determined characteristic maps",
    "volume": "main",
    "checked": true,
    "id": "e1da551bdd8f8c28379b13abe102dfc3590a7123",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.244": {
    "title": "Generation-Based Data Augmentation for Offensive Language Detection: Is It Worth It?",
    "abstract": "Generation-based data augmentation (DA) has been presented in several works as a way to improve offensive language detection. However, the effectiveness of generative DA has been shown only in limited scenarios, and the potential injection of biases when using generated data to classify offensive language has not been investigated. Our aim is that of analyzing the feasibility of generative data augmentation more in-depth with two main focuses. First, we investigate the robustness of models trained on generated data in a variety of data augmentation setups, both novel and already presented in previous work, and compare their performance on four widely-used English offensive language datasets that present inherent differences in terms of content and complexity. In addition to this, we analyze models using the HateCheck suite, a series of functional tests created to challenge hate speech detection systems. Second, we investigate potential lexical bias issues through a qualitative analysis on the generated data. We find that the potential positive impact of generative data augmentation on model performance is unreliable, and generative DA can also have unpredictable effects on lexical bias",
    "volume": "main",
    "checked": true,
    "id": "23005ad965352132ccf397bdd6d4d11d90e283ff",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.245": {
    "title": "Quantifying Context Mixing in Transformers",
    "abstract": "Self-attention weights and their transformed variants have been the main source of information for analyzing token-to-token interactions in Transformer-based models. But despite their ease of interpretation, these weights are not faithful to the models’ decisions as they are only one part of an encoder, and other components in the encoder layer can have considerable impact on information mixing in the output representations. In this work, by expanding the scope of analysis to the whole encoder block, we propose Value Zeroing, a novel context mixing score customized for Transformers that provides us with a deeper understanding of how information is mixed at each encoder layer. We demonstrate the superiority of our context mixing score over other analysis methods through a series of complementary evaluations with different viewpoints based on linguistically informed rationales, probing, and faithfulness analysis",
    "volume": "main",
    "checked": true,
    "id": "9ad02d98c3421cf93be11c81e7b19faab12aea4a",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.246": {
    "title": "KGVL-BART: Knowledge Graph Augmented Visual Language BART for Radiology Report Generation",
    "abstract": "Timely generation of radiology reports and diagnoses is a challenge worldwide due to the enormous number of cases and shortage of radiology specialists. In this paper, we propose a Knowledge Graph Augmented Vision Language BART (KGVL-BART) model that takes as input two chest X-ray images- one frontal and the other lateral- along with tags which are diagnostic keywords, and outputs a report with the patient-specific findings. Our system development effort is divided into 3 stages: i) construction of the Chest X-ray KG (referred to as chestX-KG), ii) image feature extraction, and iii) training a KGVL-BART model using the visual, text, and KG data. The dataset we use is the well-known Indiana University Chest X-ray reports with the train, validation, and test split of 3025 instances, 300 instances, and 500 instances respectively. We construct a Chest X-Ray knowledge graph from these reports by extracting entity1-relation-entity2 triples; the triples get extracted by a rule-based tool of our own. Constructed KG is verified by two experienced radiologists (with experience of 30 years and 8 years, respectively). We demonstrate that our model- KGVL-BART- outperforms State-of-the-Art transformer-based models on standard NLG scoring metrics. We also include a qualitative evaluation of our system by experienced radiologist (with experience of 30 years) on the test data, which showed that 73% of the reports generated were fully correct, only 5.5% are completely wrong and 21.5% have important missing details though overall correct. To the best of our knowledge, ours is the first system to make use of multi-modality and domain knowledge to generate X-ray reports automatically",
    "volume": "main",
    "checked": true,
    "id": "cd1aea1d9221e87879e68f3c3c80e3afd8a54139",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.247": {
    "title": "A simple but effective model for attachment in discourse parsing with multi-task learning for relation labeling",
    "abstract": "In this paper, we present a discourse parsing model for conversation trained on the STAC. We fine-tune a BERT-based model to encode pairs of discourse units and use a simple linear layer to predict discourse attachments. We then exploit a multi-task setting to predict relation labels. The multitask approach effectively aids in the difficult task of relation type prediction; our f1 score of 57 surpasses the state of the art with no loss in performance for attachment, confirming the intuitive interdependence of these two tasks. Our method also improves over previous discourse parsing models in allowing longer input sizes and in permitting attachments in which one node has multiple parents, an important feature of multiparty conversation",
    "volume": "main",
    "checked": true,
    "id": "ce43bd21bdcba34bd3cbe9df1ac42581cc973d1c",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.248": {
    "title": "How Far Can It Go? On Intrinsic Gender Bias Mitigation for Text Classification",
    "abstract": "To mitigate gender bias in contextualized language models, different intrinsic mitigation strategies have been proposed, alongside many bias metrics. Considering that the end use of these language models is for downstream tasks like text classification, it is important to understand how these intrinsic bias mitigation strategies actually translate to fairness in downstream tasks and the extent of this.In this work, we design a probe to investigate the effects that some of the major intrinsic gender bias mitigation strategies have on downstream text classification tasks. We discover that instead of resolving gender bias, intrinsic mitigation techniques and metrics are able to hide it in such a way that significant gender information is retained in the embeddings. Furthermore, we show that each mitigation technique is able to hide the bias from some of the intrinsic bias measures but not all, and each intrinsic bias measure can be fooled by some mitigation techniques, but not all. We confirm experimentally, that none of the intrinsic mitigation techniques used without any other fairness intervention is able to consistently impact extrinsic bias. We recommend that intrinsic bias mitigation techniques should be combined with other fairness interventions for downstream tasks",
    "volume": "main",
    "checked": true,
    "id": "82040a4a1b1bceb5216a9dd09e5d9c8fa3fdd83f",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.249": {
    "title": "Multimodal Event Transformer for Image-guided Story Ending Generation",
    "abstract": "Image-guided story ending generation (IgSEG) is to generate a story ending based on given story plots and ending image. Existing methods focus on cross-modal feature fusion but overlook reasoning and mining implicit information from story plots and ending image. To tackle this drawback, we propose a multimodal event transformer, an event-based reasoning framework for IgSEG. Specifically, we construct visual and semantic event graphs from story plots and ending image, and leverage event-based reasoning to reason and mine implicit information in a single modality. Next, we connect visual and semantic event graphs and utilize cross-modal fusion to integrate different-modality features. In addition, we propose a multimodal injector to adaptive pass essential information to decoder. Besides, we present an incoherence detection to enhance the understanding context of a story plot and the robustness of graph modeling for our model. Experimental results show that our method achieves state-of-the-art performance for the image-guided story ending generation",
    "volume": "main",
    "checked": true,
    "id": "ab5b8f0becb66054b55347829602f730d1a99446",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.250": {
    "title": "Improving Cross-modal Alignment for Text-Guided Image Inpainting",
    "abstract": "Text-guided image inpainting (TGII) aims to restore missing regions based on a given text in a damaged image. Existing methods are based on a strong vision encoder and a cross-modal fusion model to integrate cross-modal features. However, these methods allocate most of the computation to visual encoding, while light computation on modeling modality interactions. Moreover, they take cross-modal fusion for depth features, which ignores a fine-grained alignment between text and image. Recently, vision-language pre-trained models (VLPM), encapsulating rich cross-modal alignment knowledge, have advanced in most multimodal tasks. In this work, we propose a novel model for TGII by improving cross-modal alignment (CMA). CMA model consists of a VLPM as a vision-language encoder, an image generator and global-local discriminators. To explore cross-modal alignment knowledge for image restoration, we introduce cross-modal alignment distillation and in-sample distribution distillation. In addition, we employ adversarial training to enhance the model to fill the missing region in complicated structures effectively. Experiments are conducted on two popular vision-language datasets. Results show that our model achieves state-of-the-art performance compared with other strong competitors",
    "volume": "main",
    "checked": true,
    "id": "433b12f2f635e9c4f5518398dfd74bd6b7966ee4",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.251": {
    "title": "Semantic Specialization for Knowledge-based Word Sense Disambiguation",
    "abstract": "A promising approach for knowledge-based Word Sense Disambiguation (WSD) is to select the sense whose contextualized embeddings computed for its definition sentence are closest to those computed for a target word in a given sentence. This approach relies on the similarity of the sense and context embeddings computed by a pre-trained language model. We propose a semantic specialization for WSD where contextualized embeddings are adapted to the WSD task using solely lexical knowledge. The key idea is, for a given sense, to bring semantically related senses and contexts closer and send different/unrelated senses farther away. We realize this idea as the joint optimization of the Attract-Repel objective for sense pairs and the self-training objective for context-sense pairs while controlling deviations from the original embeddings. The proposed method outperformed previous studies that adapt contextualized embeddings. It achieved state-of-the-art performance on knowledge-based WSD when combined with the reranking heuristic that uses the sense inventory. We found that the similarity characteristics of specialized embeddings conform to the key idea. We also found that the (dis)similarity of embeddings between the related/different/unrelated senses correlates well with the performance of WSD",
    "volume": "main",
    "checked": true,
    "id": "125aeb18a0e4b6eb75b09b2cebddf7b73e062809",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.252": {
    "title": "Concept-based Persona Expansion for Improving Diversity of Persona-Grounded Dialogue",
    "abstract": "A persona-grounded dialogue model aims to improve the quality of responses to promote user engagement. However, because the given personas are mostly short and limited to only a few informative words, it is challenging to utilize them to generate diverse responses. To tackle this problem, we propose a novel persona expansion framework, Concept-based Persona eXpansion (CPX). CPX takes the original persona as input and generates expanded personas that contain conceptually rich content. We constitute CPX with two task modules: 1) Concept Extractor and 2) Sentence Generator. To train these modules, we exploit the duality of two tasks with a commonsense dataset consisting of a concept set and the corresponding sentences which contain the given concepts. Extensive experiments on persona expansion and response generation show that our work sufficiently contributes to improving the quality of responses in diversity and richness",
    "volume": "main",
    "checked": true,
    "id": "8efcd6ca2639a5de99628940f10644308b2e204d",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.253": {
    "title": "RPTCS: A Reinforced Persona-aware Topic-guiding Conversational System",
    "abstract": "Although there has been a plethora of work on open-domain conversational systems, most of the systems lack the mechanism of controlling the concept transitions in a dialogue. For activities like switching from casual chit-chat to task-oriented conversation, an agent with the ability to manage the flow of concepts in a conversation might be helpful. The user would find the dialogue more engaging and be more receptive to such transitions if these concept transitions were made while taking into account the user’s persona. Focusing on persona-aware concept transitions, we propose a Reinforced Persona-aware Topic-guiding Conversational System (RPTCS). Due to the lack of a persona-aware topic transition dataset, we propose a novel conversation dataset creation mechanism in which the conversational agent leads the discourse to drift to a set of target concepts depending on the persona of the speaker and the context of the conversation. To avoid scarcely available expensive human resource, the entire data-creation process is mostly automatic with human-in-loop only for quality checks. This created conversational dataset named PTCD is used to develop the RPTCS in two steps. First, a maximum likelihood estimation loss-based conversational model is trained on PTCD. Then this trained model is fine-tuned in a Reinforcement Learning (RL) framework by employing novel reward functions to assure persona, topic, and context consistency with non-repetitiveness in generated responses. Our experimental results demonstrate the strength of the proposed system with respect to strong baselines",
    "volume": "main",
    "checked": true,
    "id": "931fccfb0ba5a1c278ea8d0edd381cff37a4a3f8",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.254": {
    "title": "What Did You Learn To Hate? A Topic-Oriented Analysis of Generalization in Hate Speech Detection",
    "abstract": "Hate speech has unfortunately become a significant phenomenon on social media platforms, and it can cover various topics (misogyny, sexism, racism, xenophobia, etc.) and targets (e.g., black people, women). Various hate speech detection datasets have been proposed, some annotated for specific topics, and others for hateful speech in general. In either case, they often employ different annotation guidelines, which can lead to inconsistencies, even in datasets focusing on the same topics. This can cause issues in models trying to generalize across more data and more topics in order to improve detection accuracy. In this paper, we propose, for the first time, a topic-oriented approach to study generalization across popular hate speech datasets. We first perform a comparative analysis of the performances of Transformer-based models in capturing topic-generic and topic-specific knowledge when trained on different datasets. We then propose a novel, simple yet effective approach to study more precisely which topics are best captured in implicit manifestations of hate, showing that selecting combinations of datasets with better out-of-domain topical coverage improves the reliability of automatic hate speech detection",
    "volume": "main",
    "checked": true,
    "id": "f0b987396cd1dd6be4dc86837c4f8e2bf3af93bd",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.255": {
    "title": "End-to-end Case-Based Reasoning for Commonsense Knowledge Base Completion",
    "abstract": "Pretrained language models have been shown to store knowledge in their parameters and have achieved reasonable performance in commonsense knowledge base completion (CKBC) tasks. However, CKBC is knowledge-intensive and it is reported that pretrained language models’ performance in knowledge-intensive tasks are limited because of their incapability of accessing and manipulating knowledge. As a result, we hypothesize that providing retrieved passages that contain relevant knowledge as additional input to the CKBC task will improve performance. In particular, we draw insights from Case-Based Reasoning (CBR) – which aims to solve a new problem by reasoning with retrieved relevant cases, and investigate the direct application of it to CKBC. On two benchmark datasets, we demonstrate through automatic and human evaluations that our End-to-end Case-Based Reasoning Framework (ECBRF) generates more valid, informative, and novel knowledge than the state-of-the-art COMET model for CKBC in both the fully supervised and few-shot settings. We provide insights on why previous retrieval-based methods only achieve merely the same performance with COMET. From the perspective of CBR, our framework addresses a fundamental question on whether CBR methodology can be utilized to improve deep learning models",
    "volume": "main",
    "checked": true,
    "id": "d8ef879fe53adc5e2825736ea94aea38f222479c",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.256": {
    "title": "Exploring Segmentation Approaches for Neural Machine Translation of Code-Switched Egyptian Arabic-English Text",
    "abstract": "Data sparsity is one of the main challenges posed by code-switching (CS), which is further exacerbated in the case of morphologically rich languages. For the task of machine translation (MT), morphological segmentation has proven successful in alleviating data sparsity in monolingual contexts; however, it has not been investigated for CS settings. In this paper, we study the effectiveness of different segmentation approaches on MT performance, covering morphology-based and frequency-based segmentation techniques. We experiment on MT from code-switched Arabic-English to English. We provide detailed analysis, examining a variety of conditions, such as data size and sentences with different degrees of CS. Empirical results show that morphology-aware segmenters perform the best in segmentation tasks but under-perform in MT. Nevertheless, we find that the choice of the segmentation setup to use for MT is highly dependent on the data size. For extreme low-resource scenarios, a combination of frequency and morphology-based segmentations is shown to perform the best. For more resourced settings, such a combination does not bring significant improvements over the use of frequency-based segmentation",
    "volume": "main",
    "checked": true,
    "id": "97d63cef7910374b7100f9860a15a9b9fc53e3a4",
    "citation_count": 2
  },
  "https://aclanthology.org/2023.eacl-main.257": {
    "title": "Identifying the limits of transformers when performing model-checking with natural language",
    "abstract": "Can transformers learn to comprehend logical semantics in natural language? Although many strands of work on natural language inference have focussed on transformer models’ ability to perform reasoning on text, the above question has not been answered adequately. This is primarily because the logical problems that have been studied in the context of natural language inference have their computational complexity vary with the logical and grammatical constructs within the sentences. As such, it is difficult to access whether the difference in accuracy is due to logical semantics or the difference in computational complexity. A problem that is much suited to address this issue is that of the model-checking problem, whose computational complexity remains constant (for fragments derived from first-order logic). However, the model-checking problem remains untouched in natural language inference research. Thus, we investigated the problem of model-checking with natural language to adequately answer the question of how the logical semantics of natural language affects transformers’ performance. Our results imply that the language fragment has a significant impact on the performance of transformer models. Furthermore, we hypothesise that a transformer model can at least partially understand the logical semantics in natural language but can not completely learn the rules governing the model-checking algorithm",
    "volume": "main",
    "checked": true,
    "id": "bb3735a0b6b47aa828abcd88123ff6f3090d23bf",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.258": {
    "title": "Improving the Generalizability of Collaborative Dialogue Analysis With Multi-Feature Embeddings",
    "abstract": "Conflict prediction in communication is integral to the design of virtual agents that support successful teamwork by providing timely assistance. The aim of our research is to analyze discourse to predict collaboration success. Unfortunately, resource scarcity is a problem that teamwork researchers commonly face since it is hard to gather a large number of training examples. To alleviate this problem, this paper introduces a multi-feature embedding (MFeEmb) that improves the generalizability of conflict prediction models trained on dialogue sequences. MFeEmb leverages textual, structural, and semantic information from the dialogues by incorporating lexical, dialogue acts, and sentiment features. The use of dialogue acts and sentiment features reduces performance loss from natural distribution shifts caused mainly by changes in vocabulary. This paper demonstrates the performance of MFeEmb on domain adaptation problems in which the model is trained on discourse from one task domain and applied to predict team performance in a different domain. The generalizability of MFeEmb is quantified using the similarity measure proposed by Bontonou et al. (2021). Our results show that MFeEmb serves as an excellent domain-agnostic representation for meta-pretraining a few-shot model on collaborative multiparty dialogues",
    "volume": "main",
    "checked": true,
    "id": "af21369baabf575fa970ec96efb3eba4c7c8b019",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.259": {
    "title": "MetaQA: Combining Expert Agents for Multi-Skill Question Answering",
    "abstract": "The recent explosion of question-answering (QA) datasets and models has increased the interest in the generalization of models across multiple domains and formats by either training on multiple datasets or combining multiple models. Despite the promising results of multi-dataset models, some domains or QA formats may require specific architectures, and thus the adaptability of these models might be limited. In addition, current approaches for combining models disregard cues such as question-answer compatibility. In this work, we propose to combine expert agents with a novel, flexible, and training-efficient architecture that considers questions, answer predictions, and answer-prediction confidence scores to select the best answer among a list of answer predictions. Through quantitative and qualitative experiments, we show that our model i) creates a collaboration between agents that outperforms previous multi-agent and multi-dataset approaches, ii) is highly data-efficient to train, and iii) can be adapted to any QA format. We release our code and a dataset of answer predictions from expert agents for 16 QA datasets to foster future research of multi-agent systems",
    "volume": "main",
    "checked": true,
    "id": "2501cb6222a26a83c70f83a610eb652ba81b3959",
    "citation_count": 4
  },
  "https://aclanthology.org/2023.eacl-main.260": {
    "title": "BERT Is Not The Count: Learning to Match Mathematical Statements with Proofs",
    "abstract": "We introduce a task consisting in matching a proof to a given mathematical statement. The task fits well within current research on Mathematical Information Retrieval and, more generally, mathematical article analysis (Mathematical Sciences, 2014). We present a dataset for the task (the MATcH dataset) consisting of over 180k statement-proof pairs extracted from modern mathematical research articles.We find this dataset highly representative of our task, as it consists of relatively new findings useful to mathematicians. We propose a bilinear similarity model and two decoding methods to match statements to proofs effectively. While the first decoding method matches a proof to a statement without being aware of other statements or proofs, the second method treats the task as a global matching problem. Through a symbol replacement procedure, we analyze the “insights” that pre-trained language models have in such mathematical article analysis and show that while these models perform well on this task with the best performing mean reciprocal rank of 73.7, they follow a relatively shallow symbolic analysis and matching to achieve that performance",
    "volume": "main",
    "checked": true,
    "id": "0eb235ed2eeb381f9536db052350044c03e20fe3",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.261": {
    "title": "Lessons Learned from a Citizen Science Project for Natural Language Processing",
    "abstract": "Many Natural Language Processing (NLP) systems use annotated corpora for training and evaluation. However, labeled data is often costly to obtain and scaling annotation projects is difficult, which is why annotation tasks are often outsourced to paid crowdworkers. Citizen Science is an alternative to crowdsourcing that is relatively unexplored in the context of NLP. To investigate whether and how well Citizen Science can be applied in this setting, we conduct an exploratory study into engaging different groups of volunteers in Citizen Science for NLP by re-annotating parts of a pre-existing crowdsourced dataset. Our results show that this can yield high-quality annotations and at- tract motivated volunteers, but also requires considering factors such as scalability, participation over time, and legal and ethical issues. We summarize lessons learned in the form of guidelines and provide our code and data to aid future work on Citizen Science",
    "volume": "main",
    "checked": true,
    "id": "876f669c31ae7de9a2ec185fb36bd68ba307206c",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.262": {
    "title": "Contrastive Learning with Keyword-based Data Augmentation for Code Search and Code Question Answering",
    "abstract": "The semantic code search is to find code snippets from the collection of candidate code snippets with respect to a user query that describes functionality. Recent work on code search proposes data augmentation of queries for contrastive learning. This data augmentation approach modifies random words in queries. When a user web query for searching code snippet is too brief, the important word that represents the search intent of the query could be undesirably modified. A code snippet has informative components such as function name and documentation that describe its functionality. We propose to utilize these code components to identify important words and preserve them in the data augmentation step. We present KeyDAC (Keyword-based Data Augmentation for Contrastive learning) that identifies important words for code search from queries and code components based on term matching. KeyDAC augments query-code pairs while preserving keywords, and then leverages generated training instances for contrastive learning. We use KeyDAC to fine-tune various pre-trained language models and evaluate the performance of code search and code question answering via CoSQA and WebQueryTest. The experimental results confirm that KeyDAC substantially outperforms the current state-of-the-art performance, and achieves the new state-of-the-arts for both tasks",
    "volume": "main",
    "checked": true,
    "id": "4ade945f1b5fd5625419b3808b41c933016505e8",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.263": {
    "title": "Large Scale Multi-Lingual Multi-Modal Summarization Dataset",
    "abstract": "Significant developments in techniques such as encoder-decoder models have enabled us to represent information comprising multiple modalities. This information can further enhance many downstream tasks in the field of information retrieval and natural language processing; however, improvements in multi-modal techniques and their performance evaluation require large-scale multi-modal data which offers sufficient diversity. Multi-lingual modeling for a variety of tasks like multi-modal summarization, text generation, and translation leverages information derived from high-quality multi-lingual annotated data. In this work, we present the current largest multi-lingual multi-modal summarization dataset (M3LS), and it consists of over a million instances of document-image pairs along with a professionally annotated multi-modal summary for each pair. It is derived from news articles published by British Broadcasting Corporation(BBC) over a decade and spans 20 languages, targeting diversity across five language roots, it is also the largest summarization dataset for 13 languages and consists of cross-lingual summarization data for 2 languages. We formally define the multi-lingual multi-modal summarization task utilizing our dataset and report baseline scores from various state-of-the-art summarization techniques in a multi-lingual setting. We also compare it with many similar datasets to analyze the uniqueness and difficulty of M3LS. The dataset and code used in this work are made available at “https://github.com/anubhav-jangra/M3LS”",
    "volume": "main",
    "checked": true,
    "id": "6efd10f039bc6a22f13f0c43dc67c22778b1719f",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.264": {
    "title": "External Knowledge Acquisition for End-to-End Document-Oriented Dialog Systems",
    "abstract": "End-to-end neural models for conversational AI often assume that a response can be generated by considering only the knowledge acquired by the model during training. Document-oriented conversational models make a similar assumption by conditioning the input on the document and assuming that any other knowledge is captured in the model’s weights. However, a conversation may refer to external knowledge sources. In this work, we present EKo-Doc, an architecture for document-oriented conversations with access to external knowledge: we assume that a conversation is centered around a topic document and that external knowledge is needed to produce responses. EKo-Doc includes a dense passage retriever, a re-ranker, and a response generation model. We train the model end-to-end by using silver labels for the retrieval and re-ranking components that we automatically acquire from the attention signals of the response generation model. We demonstrate with automatic and human evaluations that incorporating external knowledge improves response generation in document-oriented conversations. Our architecture achieves new state-of-the-art results on the Wizard of Wikipedia dataset, outperforming a competitive baseline by 10.3% in Recall@1 and 7.4% in ROUGE-L",
    "volume": "main",
    "checked": true,
    "id": "ebf6c2716b64c10bce3f557ff8a36e073d111ada",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.265": {
    "title": "In-Depth Look at Word Filling Societal Bias Measures",
    "abstract": "Many measures of societal bias in language models have been proposed in recent years. A popular approach is to use a set of word filling prompts to evaluate the behavior of the language models. In this work, we analyze the validity of two such measures – StereoSet and CrowS-Pairs. We show that these measures produce unexpected and illogical results when appropriate control group samples are constructed. Based on this, we believe that they are problematic and using them in the future should be reconsidered. We propose a way forward with an improved testing protocol. Finally, we also introduce a new gender bias dataset for Slovak",
    "volume": "main",
    "checked": true,
    "id": "78665358e27c6c09fbb99b7642b70834666993b9",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.266": {
    "title": "Retrieval-augmented Image Captioning",
    "abstract": "Inspired by retrieval-augmented language generation and pretrained Vision and Language (V&L) encoders, we present a new approach to image captioning that generates sentences given the input image and a set of captions retrieved from a datastore, as opposed to the image alone. The encoder in our model jointly processes the image and retrieved captions using a pretrained V&L BERT, while the decoder attends to the multimodal encoder representations, benefiting from the extra textual evidence from the retrieved captions. Experimental results on the COCO dataset show that image captioning can be effectively formulated from this new perspective. Our model, named EXTRA, benefits from using captions retrieved from the training dataset, and it can also benefit from using an external dataset without the need for retraining. Ablation studies show that retrieving a sufficient number of captions (e.g., k=5) can improve captioning quality. Our work contributes towards using pretrained V&L encoders for generative tasks, instead of standard classification tasks",
    "volume": "main",
    "checked": true,
    "id": "6a4901e3bc255dd9c14d1e0d9e76e46cdd0f0ac0",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.267": {
    "title": "Automatic Evaluation and Analysis of Idioms in Neural Machine Translation",
    "abstract": "A major open problem in neural machine translation (NMT) is the translation of idiomatic expressions, such as “under the weather”. The meaning of these expressions is not composed by the meaning of their constituent words, and NMT models tend to translate them literally (i.e., word-by-word), which leads to confusing and nonsensical translations. Research on idioms in NMT is limited and obstructed by the absence of automatic methods for quantifying these errors. In this work, first, we propose a novel metric for automatically measuring the frequency of literal translation errors without human involvement. Equipped with this metric, we present controlled translation experiments with models trained in different conditions (with/without the test-set idioms) and across a wide range of (global and targeted) metrics and test sets. We explore the role of monolingual pretraining and find that it yields substantial targeted improvements, even without observing any translation examples of the test-set idioms. In our analysis, we probe the role of idiom context. We find that the randomly initialized models are more local or “myopic” as they are relatively unaffected by variations of the idiom context, unlike the pretrained ones",
    "volume": "main",
    "checked": true,
    "id": "c5e0d19735370b95bbf8a366e8268b0df67d3a90",
    "citation_count": 3
  },
  "https://aclanthology.org/2023.eacl-main.268": {
    "title": "Representation biases in sentence transformers",
    "abstract": "Variants of the BERT architecture specialised for producing full-sentence representations often achieve better performance on downstream tasks than sentence embeddings extracted from vanilla BERT. However, there is still little understanding of what properties of inputs determine the properties of such representations. In this study, we construct several sets of sentences with pre-defined lexical and syntactic structures and show that SOTA sentence transformers have a strong nominal-participant-set bias: cosine similarities between pairs of sentences are more strongly determined by the overlap in the set of their noun participants than by having the same predicates, lengthy nominal modifiers, or adjuncts. At the same time, the precise syntactic-thematic functions of the participants are largely irrelevant",
    "volume": "main",
    "checked": true,
    "id": "6079ccace4a95e7cf80abae5caaf7dbfef144f01",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.269": {
    "title": "AbLit: A Resource for Analyzing and Generating Abridged Versions of English Literature",
    "abstract": "Creating an abridged version of a text involves shortening it while maintaining its linguistic qualities. In this paper, we examine this task from an NLP perspective for the first time. We present a new resource, AbLit, which is derived from abridged versions of English literature books. The dataset captures passage-level alignments between the original and abridged texts. We characterize the linguistic relations of these alignments, and create automated models to predict these relations as well as to generate abridgements for new texts. Our findings establish abridgement as a challenging task, motivating future resources and research. The dataset is available at github.com/roemmele/AbLit",
    "volume": "main",
    "checked": true,
    "id": "441f2751da17c1e6056d8b0cdd4065a90f7ea2e7",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.270": {
    "title": "Self-training Reduces Flicker in Retranslation-based Simultaneous Translation",
    "abstract": "In simultaneous translation, the retranslation approach has the advantage of requiring no modifications to the inference engine. However, in order to reduce the undesirable flicker in the output, previous work has resorted to increasing the latency through masking, and introducing specialised inference, thus losing the simplicity of the approach. In this work, we show that self-training improves the flicker-latency tradeoff, while maintaining similar translation quality to the original. Our analysis indicates that self-training reduces flicker by controlling monotonicity. Furthermore, self-training can be combined with biased beam search to further improve the flicker-latency tradeoff",
    "volume": "main",
    "checked": true,
    "id": "c50a3380d38c9db9727424700ad103074b7030b9",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.271": {
    "title": "Social Commonsense for Explanation and Cultural Bias Discovery",
    "abstract": "Social commonsense contains many human biases due to social and cultural influence (Sap et al., 2020; Emelin et al., 2020). We focus on identifying cultural biases in data, specifically causal assumptions and commonsense implications, that strongly influence model decisions for a variety of tasks designed for social impact. This enables us to examine data for bias by making explicit the causal (if-then, inferential) relations in social commonsense knowledge used for decision making, furthering interpretable commonsense reasoning from a dataset perspective. We apply our methods on 2 social tasks: emotion detection and perceived value detection. We identify influential social commonsense knowledge to explain model behavior in the following ways. First, we augment large-scale language models with social knowledge and show improvements for the tasks, indicating the implicit assumptions a model requires to be successful on each dataset. Second, we identify influential events in the datasets by using social knowledge to cluster data and demonstrate the influence that these events have on model behavior via leave-K-out experiments. This allows us to gain a dataset-level understanding of the events and causal commonsense relationships that strongly influence predictions. We then analyze these relationships to detect influential cultural bias in each dataset. Finally, we use our influential event identification for detecting mislabeled examples and improve training and performance through their removal. We support our findings with manual analysis",
    "volume": "main",
    "checked": true,
    "id": "128904f804d206d2b2a512358df4d4385b7d2712",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.272": {
    "title": "Counter-GAP: Counterfactual Bias Evaluation through Gendered Ambiguous Pronouns",
    "abstract": "Bias-measuring datasets play a critical role in detecting biased behavior of language models and in evaluating progress of bias mitigation methods. In this work, we focus on evaluating gender bias through coreference resolution, where previous datasets are either hand-crafted or fail to reliably measure an explicitly defined bias. To overcome these shortcomings, we propose a novel method to collect diverse, natural, and minimally distant text pairs via counterfactual generation, and construct Counter-GAP, an annotated dataset consisting of 4008 instances grouped into 1002 quadruples. We further identify a bias cancellation problem in previous group-level metrics on Counter-GAP, and propose to use the difference between inconsistency across genders and within genders to measure bias at a quadruple level. Our results show that four pre-trained language models are significantly more inconsistent across different gender groups than within each group, and that a name-based counterfactual data augmentation method is more effective to mitigate such bias than an anonymization-based method",
    "volume": "main",
    "checked": true,
    "id": "fd26c019d889b816c28fa2e15e2571faa78592bb",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.273": {
    "title": "The NLP Task Effectiveness of Long-Range Transformers",
    "abstract": "Transformer models cannot easily scale to long sequences due to their O(Nˆ2) time and space complexity. This has led to Transformer variants seeking to lower computational complexity, such as Longformer and Performer. While such models have theoretically greater efficiency, their effectiveness on real NLP tasks has not been well studied. We benchmark 7 variants of Transformer models on 5 difficult NLP tasks and 7 datasets. We design experiments to isolate the effect of pretraining and hyperparameter settings, to focus on their capacity for long-range attention. Moreover, we present various methods to investigate attention behaviors to illuminate model details beyond metric scores. We find that the modified attention in long-range transformers has advantages on content selection and query-guided decoding, but they come with previously unrecognized drawbacks such as insufficient attention to distant tokens and accumulated approximation error",
    "volume": "main",
    "checked": true,
    "id": "8db711adf1beb3e0c2ec492f3936841d827404e9",
    "citation_count": 4
  },
  "https://aclanthology.org/2023.eacl-main.274": {
    "title": "Creation and evaluation of timelines for longitudinal user posts",
    "abstract": "There is increasing interest to work with user generated content in social media, especially textual posts over time. Currently there is no consistent way of segmenting user posts into timelines in a meaningful way that improves the quality and cost of manual annotation. Here we propose a set of methods for segmenting longitudinal user posts into timelines likely to contain interesting moments of change in a user’s behaviour, based on their online posting activity. We also propose a novel framework for evaluating timelines and show its applicability in the context of two different social media datasets. Finally, we present a discussion of the linguistic content of highly ranked timelines",
    "volume": "main",
    "checked": true,
    "id": "54b1eb9d93235642fec4503f4f78bd866526351d",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.275": {
    "title": "Semi-supervised New Event Type Induction and Description via Contrastive Loss-Enforced Batch Attention",
    "abstract": "Most event extraction methods have traditionally relied on an annotated set of event types. However, creating event ontologies and annotating supervised training data are expensive and time-consuming. Previous work has proposed semi-supervised approaches which leverage seen (annotated) types to learn how to automatically discover new event types. State-of-the-art methods, both semi-supervised or fully unsupervised, use a form of reconstruction loss on specific tokens in a context. In contrast, we present a novel approach to semi-supervised new event type induction using a masked contrastive loss, which learns similarities between event mentions by enforcing an attention mechanism over the data minibatch. We further disentangle the discovered clusters by approximating the underlying manifolds in the data, which allows us to achieve an adjusted rand index score of 48.85%. Building on these clustering results, we extend our approach to two new tasks: predicting the type name of the discovered clusters and linking them to FrameNet frames",
    "volume": "main",
    "checked": true,
    "id": "6780df51300f81e355f4d00db7589b8d89ef9525",
    "citation_count": 2
  },
  "https://aclanthology.org/2023.eacl-main.276": {
    "title": "Multilingual Content Moderation: A Case Study on Reddit",
    "abstract": "Content moderation is the process of flagging content based on pre-defined platform rules. There has been a growing need for AI moderators to safeguard users as well as protect the mental health of human moderators from traumatic content. While prior works have focused on identifying hateful/offensive language, they are not adequate for meeting the challenges of content moderation since 1) moderation decisions are based on violation of rules, which subsumes detection of offensive speech, and 2) such rules often differ across communities which entails an adaptive solution. We propose to study the challenges of content moderation by introducing a multilingual dataset of 1.8 Million Reddit comments spanning 56 subreddits in English, German, Spanish and French1. We perform extensive experimental analysis to highlight the underlying challenges and suggest related research problems such as cross-lingual transfer, learning under label noise (human biases), transfer of moderation models, and predicting the violated rule. Our dataset and analysis can help better prepare for the challenges and opportunities of auto moderation",
    "volume": "main",
    "checked": true,
    "id": "78ee30f94bfa9c08306cd2f5bf92e98240aa692d",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-main.277": {
    "title": "GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models",
    "abstract": "Providing natural language instructions in prompts is a useful new paradigm for improving task performance of large language models in a zero-shot setting. Recent work has aimed to improve such prompts via manual rewriting or gradient-based tuning. However, manual rewriting is time-consuming and requires subjective interpretation, while gradient-based tuning can be extremely computationally demanding for large models and may not be feasible for API-based models. In this work, we introduce Gradient-free Instructional Prompt Search (GrIPS), a gradient-free, edit-based search approach for improving task instructions for large language models. GrIPS takes in instructions designed for humans and automatically returns an improved, edited prompt, while allowing for API-based tuning. With InstructGPT models, GrIPS improves the average task performance by up to 4.30 percentage points on eight classification tasks from the Natural Instructions dataset (with similar improvements for OPT, BLOOM, and FLAN-T5). We see improvements for both instruction-only prompts and instruction + k-shot examples prompts. Notably, GrIPS outperforms manual rewriting and purely example-based prompts while controlling for the available compute and data budget. Further, performance of GrIPS is comparable to select gradient-based tuning approaches. Qualitatively, we show our edits can simplify instructions and at times make them incoherent but nonetheless improve accuracy",
    "volume": "main",
    "checked": true,
    "id": "cf934ddd3c852ba9c67cdfd21bf41e7723fc6d9e",
    "citation_count": 24
  },
  "https://aclanthology.org/2023.eacl-main.278": {
    "title": "DiscoScore: Evaluating Text Generation with BERT and Discourse Coherence",
    "abstract": "Recently, there has been a growing interest in designing text generation systems from a discourse coherence perspective, e.g., modeling the interdependence between sentences. Still, recent BERT-based evaluation metrics are weak in recognizing coherence, and thus are not reliable in a way to spot the discourse-level improvements of those text generation systems. In this work, we introduce DiscoScore, a parametrized discourse metric, which uses BERT to model discourse coherence from different perspectives, driven by Centering theory. Our experiments encompass 16 non-discourse and discourse metrics, including DiscoScore and popular coherence models, evaluated on summarization and document-level machine translation (MT). We find that (i) the majority of BERT-based metrics correlate much worse with human rated coherence than early discourse metrics, invented a decade ago; (ii) the recent state-of-the-art BARTScore is weak when operated at system level—which is particularly problematic as systems are typically compared in this manner. DiscoScore, in contrast, achieves strong system-level correlation with human ratings, not only in coherence but also in factual consistency and other aspects, and surpasses BARTScore by over 10 correlation points on average. Further, aiming to understand DiscoScore, we provide justifications to the importance of discourse coherence for evaluation metrics, and explain the superiority of one variant over another. Our code is available at {url{https://github.com/AIPHES/DiscoScore}",
    "volume": "main",
    "checked": true,
    "id": "e6e7d253ace58185acc8b4d8962f7c7b75ab110b",
    "citation_count": 4
  },
  "https://aclanthology.org/2023.eacl-main.279": {
    "title": "Know your audience: specializing grounded language models with listener subtraction",
    "abstract": "Effective communication requires adapting to the idiosyncrasies of each communicative context—such as the common ground shared with each partner. Humans demonstrate this ability to specialize to their audience in many contexts, such as the popular game Dixit. We take inspiration from Dixit to formulate a multi-agent image reference game where a (trained) speaker model is rewarded for describing a target image such that one (pretrained) listener model can correctly identify it among distractors, but another listener cannot. To adapt, the speaker must exploit differences in the knowledge it shares with the different listeners. We show that finetuning an attention-based adapter between a CLIP vision encoder and a large language model in this contrastive, multi-agent setting gives rise to context-dependent natural language specialization from rewards only, without direct supervision. Through controlled experiments, we show that training a speaker with two listeners that perceive differently, using our method, allows the speaker to adapt to the idiosyncracies of the listeners. Furthermore, we show zero-shot transfer of the specialization to real-world data. Our experiments demonstrate a method for specializing grounded language models without direct supervision and highlight the interesting research challenges posed by complex multi-agent communication",
    "volume": "main",
    "checked": true,
    "id": "401c9fcc621873a8c5709c7a5e7224fbedcab821",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-main.280": {
    "title": "Meeting the Needs of Low-Resource Languages: The Value of Automatic Alignments via Pretrained Models",
    "abstract": "Large multilingual models have inspired a new class of word alignment methods, which work well for the model’s pretraining languages. However, the languages most in need of automatic alignment are low-resource and, thus, not typically included in the pretraining data. In this work, we ask: How do modern aligners perform on unseen languages, and are they better than traditional methods? We contribute gold-standard alignments for Bribri–Spanish, Guarani–Spanish, Quechua–Spanish, and Shipibo-Konibo–Spanish. With these, we evaluate state-of-the-art aligners with and without model adaptation to the target language. Finally, we also evaluate the resulting alignments extrinsically through two downstream tasks: named entity recognition and part-of-speech tagging. We find that although transformer-based methods generally outperform traditional models, the two classes of approach remain competitive with each other",
    "volume": "main",
    "checked": true,
    "id": "b12f13f259088de34c72131d3d59f3c5600b8cf1",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-demo.1": {
    "title": "Addressing Issues of Cross-Linguality in Open-Retrieval Question Answering Systems For Emergent Domains",
    "abstract": "Open-retrieval question answering systems are generally trained and tested on large datasets in well-established domains. However, low-resource settings such as new and emerging domains would especially benefit from reliable question answering systems. Furthermore, multilingual and cross-lingual resources in emergent domains are scarce, leading to few or no such systems.In this paper, we demonstrate a cross-lingual open-retrieval question answering system for the emergent domain of COVID-19.Our system adopts a corpus of scientific articles to ensure that retrieved documents are reliable. To address the scarcity of cross-lingual training data in emergent domains, we present a method utilizing automatic translation, alignment, and filtering to produce English-to-all datasets.We show that a deep semantic retriever greatly benefits from training on our English-to-all data and significantly outperforms a BM25 baseline in the cross-lingual setting.We illustrate the capabilities of our system with examples and release all code necessary to train and deploy such a system",
    "volume": "demo",
    "checked": true,
    "id": "1b20cfa7684744595ea9e4cd99bd9618c321dcbf",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-demo.2": {
    "title": "CodeAnno: Extending WebAnno with Hierarchical Document Level Annotation and Automation",
    "abstract": "WebAnno is one of the most popular annotation tools that supports generic annotation types and distributive annotation with multiple user roles. However, WebAnno focuses on annotating span-level mentions and relations among them, making document-level annotation complicated. When it comes to the annotation and analysis of social science materials, it usually involves the creation of codes to categorize a given document. The codes, which are known as codebooks, are typically hierarchical, which enables to code the document either with a general category or more fine-grained subcategories. CodeAnno is forked from WebAnno and designed to solve the coding problems faced by many social science researchers with the following main functionalities. 1) Creation of hierarchical codebooks, with functionality to move and sort categories in the hierarchy 2) an interactive UI for codebook annotation 3) import and export of annotations in CSV format, hence being compatible with existing annotations conducted using spreadsheet applications 4) integration of an external automation component to facilitate coding using machine learning 5) project templating that allows duplicating a project structure without copying the actual documents. We present different use-cases to demonstrate the capability of CodeAnno. A shot demonstration video of the system is available here: https://www.youtube.com/watch?v=RmCdTghBe-s",
    "volume": "demo",
    "checked": true,
    "id": "1877ebdcd06193de80b5e7d0de260ad6cee91edd",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-demo.3": {
    "title": "NLP Workbench: Efficient and Extensible Integration of State-of-the-art Text Mining Tools",
    "abstract": "NLP Workbench is a web-based platform for text mining that allows non-expert users to obtain semantic understanding of large-scale corpora using state-of-the-art text mining models. The platform is built upon latest pre-trained models and open source systems from academia that provide semantic analysis functionalities, including but not limited to entity linking, sentiment analysis, semantic parsing, and relation extraction. Its extensible design enables researchers and developers to smoothly replace an existing model or integrate a new one. To improve efficiency, we employ a microservice architecture that facilitates allocation of acceleration hardware and parallelization of computation. This paper presents the architecture of NLP Workbench and discusses the challenges we faced in designing it. We also discuss diverse use cases of NLP Work- bench and the benefits of using it over other approaches. The platform is under active devel- opment, with its source code released under the MIT license. A website and a short video demonstrating our platform are also available",
    "volume": "demo",
    "checked": true,
    "id": "d6842b08240c342ea6d51b50046d6981254612d6",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-demo.4": {
    "title": "jTLEX: a Java Library for TimeLine EXtraction",
    "abstract": "jTLEX is a programming library that provides a Java implementation of the TimeLine EXtraction algorithm (TLEX; Finlayson et al.,2021), along with utilities for programmatic manipulation of TimeML graphs. Timelines are useful for a number of natural language understanding tasks, such as question answering, cross-document event coreference, and summarization & visualization. jTLEX provides functionality for (1) parsing TimeML annotations into Java objects, (2) construction of TimeML graphs from scratch, (3) partitioning of TimeML graphs into temporally connected subgraphs, (4) transforming temporally connected subgraphs into point algebra (PA) graphs, (5) extracting exact timeline of TimeML graphs, (6) detecting inconsistent subgraphs, and (7) calculating indeterminate sections of the timeline. The library has been tested on the entire TimeBank corpus, and comes with a suite of unit tests. We release the software as open source with a free license for non-commercial use",
    "volume": "demo",
    "checked": true,
    "id": "a4b68000e148fef8554a4ed04bcb51c30e7f967b",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-demo.5": {
    "title": "CovRelex-SE: Adding Semantic Information for Relation Search via Sequence Embedding",
    "abstract": "In recent years, COVID-19 has impacted all aspects of human life. As a result, numerous publications relating to this disease have been issued. Due to the massive volume of publications, some retrieval systems have been developed to provide researchers with useful information. In these systems, lexical searching methods are widely used, which raises many issues related to acronyms, synonyms, and rare keywords. In this paper, we present a hybrid relation retrieval system, CovRelex-SE, based on embeddings to provide high-quality search results. Our system can be accessed through the following URL: https://www.jaist.ac.jp/is/labs/nguyen-lab/systems/covrelex-se/",
    "volume": "demo",
    "checked": true,
    "id": "c08f890d4a8d3bb5a16b6ab1d3e635de26a4ce7e",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-demo.6": {
    "title": "ITMT: Interactive Topic Model Trainer",
    "abstract": "Topic Modeling is a commonly used technique for analyzing unstructured data in various fields, but achieving accurate results and useful models can be challenging, especially for domain experts who lack the knowledge needed to optimize the parameters required by this natural language processing technique. From this perspective, we introduce an Interactive Topic Model Trainer (ITMT) developed within the EU-funded project IntelComp. ITMT is a user-in-the-loop topic modeling tool presented with a graphical user interface that allows the training and curation of different state-of-the-art topic extraction libraries, including some recent neural-based methods, oriented toward the usage by domain experts. This paper reviews ITMT’s functionalities and key implementation aspects in this paper, including a comparison with other tools for topic modeling analysis",
    "volume": "demo",
    "checked": true,
    "id": "271b70641c539c1c6b3bee8432b48d2c36bc53b9",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-demo.7": {
    "title": "FISH: A Financial Interactive System for Signal Highlighting",
    "abstract": "In this system demonstration, we seek to streamline the process of reviewing financial statements and provide insightful information for practitioners. We develop FISH, an interactive system that extracts and highlights crucial textual signals from financial statements efficiently and precisely. To achieve our goal, we integrate pre-trained BERT representations and a fine-tuned BERT highlighting model with a newly-proposed two-stage classify-then-highlight pipeline. We also conduct the human evaluation, showing FISH can provide accurate financial signals. FISH overcomes the limitations of existing research andmore importantly benefits both academics and practitioners in finance as they can leverage state-of-the-art contextualized language models with their newly gained insights. The system is available online at https://fish-web-fish.de.r.appspot.com/, and a short video for introduction is at https://youtu.be/ZbvZQ09i6aw",
    "volume": "demo",
    "checked": true,
    "id": "af4bf4e2d583556ee6e2adbe2feb7af4f5d8967c",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-demo.8": {
    "title": "Yu Sheng: Human-in-Loop Classical Chinese Poetry Generation System",
    "abstract": "The development of poetry generation system mainly focuses on enhancing the capacity of generation model. However, the demands of customization and polishing are generally ignored, which highly reduces the scope of application. In this work, we present Yu Sheng, a web-based poetry generation system that is featured a human-in-loop generation framework, providing various customization options for users with different backgrounds to engage in the process of poetry composition. To this end, we propose two methods and train the models that can perform constrained generation and fine-grained polishing. The automatic and human evaluation results show that our system has a strong ability to generate and polish poetry compared to other vanilla models. Our system is publicly accessible at: https://yusheng.cis.um.edu.mo",
    "volume": "demo",
    "checked": true,
    "id": "103044f64034b73e5e398f55dbca4ab40deac32b",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-demo.9": {
    "title": "PANACEA: An Automated Misinformation Detection System on COVID-19",
    "abstract": "In this demo, we introduce a web-based misinformation detection system PANACEA on COVID-19 related claims, which has two modules, fact-checking and rumour detection. Our fact-checking module, which is supported by novel natural language inference methods with a self-attention network, outperforms state-of-the-art approaches. It is also able to give automated veracity assessment and ranked supporting evidence with the stance towards the claim to be checked. In addition, PANACEA adapts the bi-directional graph convolutional networks model, which is able to detect rumours based on comment networks of related tweets, instead of relying on the knowledge base. This rumour detection module assists by warning the users in the early stages when a knowledge base may not be available",
    "volume": "demo",
    "checked": true,
    "id": "8002b6a4afe4a2b3abc7307e8e9315daddf2cb93",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-demo.10": {
    "title": "NxPlain: A Web-based Tool for Discovery of Latent Concepts",
    "abstract": "The proliferation of deep neural networks in various domains has seen an increased need for the interpretability of these models, especially in scenarios where fairness and trust are as important as model performance. A lot of independent work is being carried out to: i) analyze what linguistic and non-linguistic knowledge is learned within these models, and ii) highlight the salient parts of the input. We present NxPlain, a web-app that provides an explanation of a model’s prediction using latent concepts. NxPlain discovers latent concepts learned in a deep NLP model, provides an interpretation of the knowledge learned in the model, and explains its predictions based on the used concepts. The application allows users to browse through the latent concepts in an intuitive order, letting them efficiently scan through the most salient concepts with a global corpus-level view and a local sentence-level view. Our tool is useful for debugging, unraveling model bias, and for highlighting spurious correlations in a model. A hosted demo is available here: https://nxplain.qcri.org",
    "volume": "demo",
    "checked": true,
    "id": "1088ecaaf9e55864945534834709c675f43422c6",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-demo.11": {
    "title": "Small-Text: Active Learning for Text Classification in Python",
    "abstract": "We introduce small-text, an easy-to-use active learning library, which offers pool-based active learning for single- and multi-label text classification in Python. It features numerous pre-implemented state-of-the-art query strategies, including some that leverage the GPU. Standardized interfaces allow the combination of a variety of classifiers, query strategies, and stopping criteria, facilitating a quick mix and match, and enabling a rapid development of both active learning experiments and applications. With the objective of making various classifiers and query strategies accessible for active learning, small-text integrates several well-known machine learning libraries, namely scikit-learn, Pytorch, and Hugging Face transformers. The latter integrations are optionally installable extensions, so GPUs can be used but are not required. Using this new library, we investigate the performance of the recently published SetFit training paradigm, which we compare to vanilla transformer fine-tuning, finding that it matches the latter in classification accuracy while outperforming it in area under the curve. The library is available under the MIT License at https://github.com/webis-de/small-text, in version 1.3.0 at the time of writing",
    "volume": "demo",
    "checked": true,
    "id": "58cf242af0a9aaea9eccdb33c91603aa422bc233",
    "citation_count": 6
  },
  "https://aclanthology.org/2023.eacl-demo.12": {
    "title": "kogito: A Commonsense Knowledge Inference Toolkit",
    "abstract": "In this paper, we present kogito, an open-source tool for generating commonsense inferences about situations described in text. kogito provides an intuitive and extensible interface to interact with natural language generation models that can be used for hypothesizing commonsense knowledge inference from a textual input. In particular, kogito offers several features for targeted, multi-granularity knowledge generation. These include a standardized API for training and evaluating knowledge models, and generating and filtering inferences from them. We also include helper functions for converting natural language texts into a format ingestible by knowledge models — intermediate pipeline stages such as knowledge head extraction from text, heuristic and model-based knowledge head-relation matching, and an ability to define and use custom knowledge relations. We make the code for kogito available at https://github.com/epfl-nlp/kogito along with thorough documentation at https://kogito.readthedocs.io",
    "volume": "demo",
    "checked": true,
    "id": "8867141e354975e255c93fefa606c6993e33177a",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-demo.13": {
    "title": "Text-Blueprint: An Interactive Platform for Plan-based Conditional Generation",
    "abstract": "While conditional generation models can now generate natural language well enough to create fluent text, it is still difficult to control the generation process, leading to irrelevant, repetitive, and hallucinated content. Recent work shows that planning can be a useful intermediate step to render conditional generation less opaque and more grounded. We present a web browser-based demonstration for query-focused summarization that uses a sequence of question-answer pairs, as a blueprint plan for guiding text generation (i.e., what to say and in what order). We illustrate how users may interact with the generated text and associated plan visualizations, e.g., by editing and modifying the plan in order to improve or control the generated output.A short video demonstrating our system is available at https://goo.gle/text-blueprint-demo",
    "volume": "demo",
    "checked": true,
    "id": "2f3f1a7454dbb1b61a0f7388164fe256edf3bcf7",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-demo.14": {
    "title": "ALAMBIC : Active Learning Automation Methods to Battle Inefficient Curation",
    "abstract": "In this paper, we present ALAMBIC, an open-source dockerized web-based platform for annotating text data through active learning for classification task. Active learning is known to reduce the need of labelling, a time-consuming task, by selecting the most informative instances among the unlabelled instances, reaching an optimal accuracy faster than by just randomly labelling data. ALAMBIC integrates all the steps from data import to customization of the (active) learning process and annotation of the data, with indications of the progress of the trained model that can be downloaded and used in downstream tasks. Its architecture also allows the easy integration of other types of model, features and active learning strategies.The code is available on https://github.com/Trusted-AI-Labs/ALAMBIC and a video demonstration is available on https://youtu.be/4oh8UADfEmY",
    "volume": "demo",
    "checked": true,
    "id": "703e207117388b3ce6da922c078d43393d5690c7",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-demo.15": {
    "title": "SPINDLE: Spinning Raw Text into Lambda Terms with Graph Attention",
    "abstract": "This paper describes SPINDLE, an open source Python module, providing an efficient and accurate parser for written Dutch that transforms raw text input to programs for meaning composition expressed as λ terms. The parser integrates a number of breakthrough advances made in recent years. Its output consists of hi-res derivations of a multimodal type-logical grammar, capturing two orthogonal axes of syntax, namely deep function-argument structures and dependency relations. These are produced by three interdependent systems: a static type-checker asserting the well-formedness of grammatical analyses, a state-of-the-art, structurally-aware supertagger based on heterogeneous graph convolutions, and a massively parallel proof search component based on Sinkhorn iterations. Packed in the software are also handy utilities and extras for proof visualization and inference, intended to facilitate end-user utilization",
    "volume": "demo",
    "checked": true,
    "id": "2951e740d557f56e90df44a7a4587d1dd2c022f7",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-demo.16": {
    "title": "Linguistic Constructs Represent the Domain Model in Intelligent Language Tutoring",
    "abstract": "This paper presents the development of the AI-based language-learning platform, Revita. It is an intelligent online tutor, developed to support learners of multiple languages, from lower-intermediate toward advanced levels. It has been in pilot use with hundreds of students at several universities, whose feedback and needs shape the development. One of the main emerging features of Revita is the system of linguistic constructs to represent the domain knowledge. The system of constructs is developed in collaboration with experts in language pedagogy. Constructs define the types of exercises, the content of the feedback, and enable detailed modeling and evaluation of learner progress",
    "volume": "demo",
    "checked": true,
    "id": "c9acc2be0f78d602f104369329dd092288ec7c40",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-demo.17": {
    "title": "GATE Teamware 2: An open-source tool for collaborative document classification annotation",
    "abstract": "We present GATE Teamware 2: an open-source web-based platform for managing teams of annotators working on document classification tasks. GATE Teamware 2 is an entirely re-engineered successor to GATE Teamware, using contemporary web frameworks. The software allows the management of teams of multiple annotators, project managers and administrators - including the management of annotators - across multiple projects. Projects can be configured to control and monitor the annotation statistics and have a highly flexible JSON-configurable annotation display which can include arbitrary HTML. Optionally, documents can be uploaded with pre-existing annotations and documents are served to annotators in a random order by default to reduce bias. Crucially, annotators can be trained on applying the annotation guidelines correctly and then screened for quality assurance purposes, prior to being cleared for independent annotation. GATE Teamware 2 can be self-deployed, including in container orchestration environments, or provided as private, hosted cloud instances.GATE Teamware 2 is an open-source software and can be downloaded from https://github.com/GATENLP/gate-teamware.A demonstration video of the system has also been made available at https://youtu.be/KoXkuhc4fmM",
    "volume": "demo",
    "checked": true,
    "id": "bc875d3e59a6567fd79dc461569697259abffd9c",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-demo.18": {
    "title": "GameQA: Gamified Mobile App Platform for Building Multiple-Domain Question-Answering Datasets",
    "abstract": "The methods used to create many of the well-known Question-Answering (QA) datasets are hard to replicate for low-resource languages. A commonality amongst these methods is hiring annotators to source answers from the internet by querying a single answer source, such as Wikipedia. Applying these methods for low-resource languages can be problematic since there is no single large answer source for these languages. Consequently, this can result in a high ratio of unanswered questions, since the amount of information in any single source is limited. To address this problem, we developed a novel crowd-sourcing platform to gather multiple-domain QA data for low-resource languages. Our platform, which consists of a mobile app and a web API, gamifies the data collection process. We successfully released the app for Icelandic (a low-resource language with about 350,000 native speakers) to build a dataset which rivals large QA datasets for high-resource languages both in terms of size and ratio of answered questions. We have made the platform open source with instructions on how to localize and deploy it to gather data for other low-resource languages",
    "volume": "demo",
    "checked": true,
    "id": "0f008e07d601e8f21d1df5db3d36e85484840083",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-demo.19": {
    "title": "Towards Speech to Speech Machine Translation focusing on Indian Languages",
    "abstract": "We introduce an SSMT (Speech to Speech Machine Translation, aka Speech to Speech Video Translation) Pipeline(https://ssmt.iiit.ac.in/ssmtiiith), as web application for translating videos from one language to another by cascading multiple language modules. Our speech translation system combines highly accurate speech to text (ASR) for Indian English, pre-possessing modules to bridge ASR-MT gaps such as spoken disfluency and punctuation, robust machine translation (MT) systems for multiple language pairs, SRT module for translated text, text to speech (TTS) module and a module to render translated synthesized audio on the original video. It is user-friendly, flexible, and easily accessible system. We aim to provide a complete configurable speech translation experience to users and researchers with this system. It also supports human intervention where users can edit outputs of different modules and the edited output can then be used for subsequent processing to improve overall output quality. By adopting a human-in-the-loop approach, the aim is to configure technology in such a way where it can assist humans and help to reduce the involved human efforts in speech translation involving English and Indian languages. As per our understanding, this is the first fully integrated system for English to Indian languages (Hindi, Telugu, Gujarati, Marathi and Punjabi) video translation. Our evaluation shows that one can get 3.5+ MOS score using the developed pipeline with human intervention for English to Hindi. A short video demonstrating our system is available at https://youtu.be/MVftzoeRg48",
    "volume": "demo",
    "checked": true,
    "id": "43975af4f577e75b287823a30af26947271a52f2",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-demo.20": {
    "title": "TextWorldExpress: Simulating Text Games at One Million Steps Per Second",
    "abstract": "Text-based games offer a challenging test bed to evaluate virtual agents at language understanding, multi-step problem-solving, and common-sense reasoning. However, speed is a major limitation of current text-based games, capping at 300 steps per second, mainly due to the use of legacy tooling. In this work we present TextWorldExpress, a high-performance simulator that includes implementations of three common text game benchmarks that increases simulation throughput by approximately three orders of magnitude, reaching over one million steps per second on common desktop hardware. This significantly reduces experiment runtime, enabling billion-step-scale experiments in about one day",
    "volume": "demo",
    "checked": true,
    "id": "6cc1f9948df3ae6e82be014856dadee3af2e38aa",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.eacl-demo.21": {
    "title": "TermoUD - a language-independent terminology extraction tool",
    "abstract": "The paper addresses TermoUD — a language-independent terminology extraction tool. Itsprevious version, i.e. TermoPL (Marciniak et al., 2016; Rychlik et al., 2022), uses languagedependent shallow grammar which selects candidate terms. The goal behind the development of TermoUD is to make the procedure as universal as possible, while taking care of the linguistic correctness of selected phrases. The tool is suitable for languages for which the Universal Dependencies (UD) parser exists. We describe a method of candidate term extraction based on UD POS tags and UD relations. The candidate ranking is performed by the C-value metric (contexts counting is adapted to the UD formalism), which doesn’t need any additional language resources. The performance of the tool has been tested on texts in English, French, Dutch, and Slovenian. The results are evaluated on the manually annotated datasets: ACTER, RD-TEC 2.0, GENIA and RSDO5, and compared to those obtained by other tools",
    "volume": "demo",
    "checked": true,
    "id": "94fea94ab93f077a4cf908633e6799ed12f8f018",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-demo.22": {
    "title": "INCOGNITUS: A Toolbox for Automated Clinical Notes Anonymization",
    "abstract": "Automated text anonymization is a classical problem in Natural Language Processing (NLP). The topic has evolved immensely throughout the years, with the first list-search and rule-based solutions evolving to statistical modeling approaches and later to advanced systems that rely on powerful state-of-the-art language models. Even so, these solutions fail to be widely implemented in the most privacy-demanding areas of activity, such as healthcare; none of them is perfect, and most can not guarantee rigorous anonymization. This paper presents INCOGNITUS, a flexible platform for the automated anonymization of clinical notes that offers the possibility of applying different techniques. The available tools include an underexplored yet promising method that guarantees 100% recall by replacing each word with a semantically identical one. In addition, the presented framework incorporates a performance evaluation module to compute a novel metric for information loss assessment in real-time",
    "volume": "demo",
    "checked": true,
    "id": "117e579bc8c04c3d31641c360472a9a8236a498b",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-demo.23": {
    "title": "CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification",
    "abstract": "Chain-of-thought (CoT) prompting enables large language models (LLMs) to solve complex reasoning tasks by generating an explanation before the final prediction. Despite it’s promising ability, a critical downside of CoT prompting is that the performance is greatly affected by the factuality of the generated explanation. To improve the correctness of the explanations, fine-tuning language models with explanation data is needed. However, there exists only a few datasets that can be used for such approaches, and no data collection tool for building them. Thus, we introduce CoTEVer, a tool-kit for annotating the factual correctness of generated explanations and collecting revision data of wrong explanations. Furthermore, we suggest several use cases where the data collected with CoTEVer can be utilized for enhancing the faithfulness of explanations. Our toolkit is publicly available at https://github.com/SeungoneKim/CoTEVer",
    "volume": "demo",
    "checked": true,
    "id": "b9d75f361b5310c6ddcddfe7858bb0416eb78de4",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-demo.24": {
    "title": "OLEA: Tool and Infrastructure for Offensive Language Error Analysis in English",
    "abstract": "State-of-the-art models for identifying offensive language often fail to generalize over more nuanced or implicit cases of offensive and hateful language. Understanding model performance on complex cases is key for building robust models that are effective in real-world settings. To help researchers efficiently evaluate their models, we introduce OLEA, a diagnostic, open-source, extensible Python library that provides easy-to-use tools for error analysis in the context of detecting offensive language in English. OLEA packages analyses and datasets proposed by prior scholarship, empowering researchers to build effective, explainable and generalizable offensive language classifiers",
    "volume": "demo",
    "checked": true,
    "id": "aed981c1a05cb2d2268322652fa23f30a78d92f6",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-demo.25": {
    "title": "TULAP - An Accessible and Sustainable Platform for Turkish Natural Language Processing Resources",
    "abstract": "Access to natural language processing resources is essential for their continuous improvement. This can be especially challenging in educational institutions where the software development effort required to package and release research outcomes may be overwhelming and under-recognized. Access towell-prepared and reliable research outcomes is important both for their developers as well as the greater research community. This paper presents an approach to address this concern with two main goals: (1) to create an open-source easily deployable platform where resources can be easily shared and explored, and (2) to use this platform to publish open-source Turkish NLP resources (datasets and tools) created by a research lab. The Turkish Natural Language Processing (TULAP) was designed and developed as an easy-to-use platform to share dataset and tool resources which supports interactive tool demos. Numerous open access Turkish NLP resources have been shared on TULAP. All tools are containerized to support portability for custom use. This paper describes the design, implementation, and deployment of TULAP with use cases (available at https://tulap.cmpe.boun.edu.tr/). A short video demonstrating our system is available at https://figshare.com/articles/media/TULAP_Demo/22179047",
    "volume": "demo",
    "checked": true,
    "id": "197dd1b2aca34eb545c699cf9ce3d73de3f24b0a",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-demo.26": {
    "title": "ALANNO: An Active Learning Annotation System for Mortals",
    "abstract": "Supervised machine learning has become the cornerstone of today’s data-driven society, increasing the need for labeled data. However, the process of acquiring labels is often expensive and tedious. One possible remedy is to use active learning (AL) – a special family of machine learning algorithms designed to reduce labeling costs. Although AL has been successful in practice, a number of practical challenges hinder its effectiveness and are often overlooked in existing AL annotation tools. To address these challenges, we developed ALANNO, an open-source annotation system for NLP tasks equipped with features to make AL effective in real-world annotation projects. ALANNO facilitates annotation management in a multi-annotator setup and supports a variety of AL methods and underlying models, which are easily configurable and extensible",
    "volume": "demo",
    "checked": true,
    "id": "bd434edefe7a7ef375642f03b60db05052585f69",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-demo.27": {
    "title": "Automatically Summarizing Evidence from Clinical Trials: A Prototype Highlighting Current Challenges",
    "abstract": "In this work we present TrialsSummarizer, a system that aims to automatically summarize evidence presented in the set of randomized controlled trials most relevant to a given query. Building on prior work, the system retrieves trial publications matching a query specifying a combination of condition, intervention(s), and outcome(s), and ranks these according to sample size and estimated study quality.The top-k such studies are passed through a neural multi-document summarization system, yielding a synopsis of these trials. We consider two architectures: A standard sequence-to-sequence model based on BART, and a multi-headed architecture intended to provide greater transparency and controllability to end-users.Both models produce fluent and relevant summaries of evidence retrieved for queries, but their tendency to introduce unsupported statements render them inappropriate for use in this domain at present.The proposed architecture may help users verify outputs allowing users to trace generated tokens back to inputs. The demonstration video can be found at https://vimeo.com/735605060The prototype, source code, and model weights are available at: https://sanjanaramprasad.github.io/trials-summarizer/",
    "volume": "demo",
    "checked": true,
    "id": "475e130cf14644f853836b5e0e0dabfe452c4f9d",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-demo.28": {
    "title": "Corpus Annotation Graph Builder (CAG): An Architectural Framework to Create and Annotate a Multi-source Graph",
    "abstract": "Graphs are a natural representation of complex data as their structure allows users to discover (often implicit) relations among the nodes intuitively. Applications build graphs in an ad-hoc fashion, usually tailored to specific use cases, limiting their reusability. To account for this, we present the Corpus Annotation Graph (CAG) architectural framework based on a create-and-annotate pattern that enables users to build uniformly structured graphs from diverse data sources and extend them with automatically extracted annotations (e.g., named entities, topics). The resulting graphs can be used for further analyses across multiple downstream tasks (e.g., node classification). Code and resources are publicly available on GitHub, and downloadable via PyPi with the command {texttt{pip install cag}",
    "volume": "demo",
    "checked": true,
    "id": "d5318767eb9a8db5e46befa851370b1e0f3d9a4a",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-demo.29": {
    "title": "ferret: a Framework for Benchmarking Explainers on Transformers",
    "abstract": "As Transformers are increasingly relied upon to solve complex NLP problems, there is an increased need for their decisions to be humanly interpretable. While several explainable AI (XAI) techniques for interpreting the outputs of transformer-based models have been proposed, there is still a lack of easy access to using and comparing them.We introduce ferret, a Python library to simplify the use and comparisons of XAI methods on transformer-based classifiers.With ferret, users can visualize and compare transformers-based models output explanations using state-of-the-art XAI methods on any free-text or existing XAI corpora. Moreover, users can also evaluate ad-hoc XAI metrics to select the most faithful and plausible explanations. To align with the recently consolidated process of sharing and using transformers-based models from Hugging Face, ferret interfaces directly with its Python library.In this paper, we showcase ferret to benchmark XAI methods used on transformers for sentiment analysis and hate speech detection. We show how specific methods provide consistently better explanations and are preferable in the context of transformer models",
    "volume": "demo",
    "checked": true,
    "id": "64fc00dd379c1cd176c001794308459b92bb2490",
    "citation_count": 3
  },
  "https://aclanthology.org/2023.eacl-demo.30": {
    "title": "Learn With Martian: A Tool For Creating Assignments That Can Write And Re-Write Themselves",
    "abstract": "In this paper, we propose Learn, a unified, easy-to-use tool to apply question generation and selection in classrooms. The tool lets instructors and TAs create assignments that can write and re-write themselves. Given existing course materials, for example a reference textbook, Learn can generate questions, select the highest quality questions, show the questions to students, adapt question difficulty to student knowledge, and generate new questions based on how effectively old questions help students learn. The modular, composable nature of the tools for handling each sub-task allow instructors to use only the parts of the tool necessary to the course, allowing for integration in a large number of courses with varied teaching styles. We also report on the adoption of the tool in classes at the University of Pennsylvania with over 1000 students. Learn is publicly released at https://learn.withmartian.com",
    "volume": "demo",
    "checked": true,
    "id": "dba8706966c4d24635dee44fea03025717f3c831",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-demo.31": {
    "title": "EVALIGN: Visual Evaluation of Translation Alignment Models",
    "abstract": "This paper presents EvAlign, a visual analytics framework for quantitative and qualitative evaluation of automatic translation alignment models. EvAlign offers various visualization views enabling developers to visualize their models’ predictions and compare the performance of their models with other baseline and state-of-the-art models. Through different search and filter functions, researchers and practitioners can also inspect the frequent alignment errors and their positions. EvAlign hosts nine gold standard datasets and the predictions of multiple alignment models. The tool is extendable, and adding additional datasets and models is straightforward. EvAlign can be deployed and used locally and is available on GitHub",
    "volume": "demo",
    "checked": true,
    "id": "4ef2b09cbb0752d3b685667d7c3f2b9891b5e840",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-demo.32": {
    "title": "ALLECS: A Lightweight Language Error Correction System",
    "abstract": "In this paper, we present ALLECS, a lightweight web application to serve grammatical error correction (GEC) systems so that they can be easily used by the general public. We design ALLECS to be accessible to as many users as possible, including users who have a slow Internet connection and who use mobile phones as their main devices to connect to the Internet. ALLECS provides three state-of-the-art base GEC systems using two approaches (sequence-to-sequence generation and sequence tagging), as well as two state-of-the-art GEC system combination methods using two approaches (edit-based and text-based). ALLECS can be accessed at https://sterling8.d2.comp.nus.edu.sg/gec-demo/",
    "volume": "demo",
    "checked": true,
    "id": "30b5997b4a82c0b90a3ac047e8c75c57cd6fc74d",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-demo.33": {
    "title": "DAVE: Differential Diagnostic Analysis Automation and Visualization from Clinical Notes",
    "abstract": "The Differential Analysis Visualizer for Electronic Medical Records (DAVE) is a tool that utilizes natural language processing and machine learning to help visualize diagnostic algorithms in real-time to help support medical professionals in their clinical decision-making process",
    "volume": "demo",
    "checked": true,
    "id": "b84ebd372f1c34ba20f267231090fbd3e1c48081",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-srw.1": {
    "title": "Revealing Weaknesses of Vietnamese Language Models Through Unanswerable Questions in Machine Reading Comprehension",
    "abstract": "Although the curse of multilinguality significantly restricts the language abilities of multilingual models in monolingual settings, researchers now still have to rely on multilingual models to develop state-of-the-art systems in Vietnamese Machine Reading Comprehension. This difficulty in researching is because of the limited number of high-quality works in developing Vietnamese language models. In order to encourage more work in this research field, we present a comprehensive analysis of language weaknesses and strengths of current Vietnamese monolingual models using the downstream task of Machine Reading Comprehension. From the analysis results, we suggest new directions for developing Vietnamese language models. Besides this main contribution, we also successfully reveal the existence of artifacts in Vietnamese Machine Reading Comprehension benchmarks and suggest an urgent need for new high-quality benchmarks to track the progress of Vietnamese Machine Reading Comprehension. Moreover, we also introduced a minor but valuable modification to the process of annotating unanswerable questions for Machine Reading Comprehension from previous work. Our proposed modification helps improve the quality of unanswerable questions to a higher level of difficulty for Machine Reading Comprehension systems to solve",
    "volume": "student",
    "checked": true,
    "id": "26e74a16644bdb70e66f3e7b3de40337bd270944",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-srw.2": {
    "title": "Incorporating Dropped Pronouns into Coreference Resolution: The case for Turkish",
    "abstract": "Representation of coreferential relations is a challenging and actively studied topic for pro-drop and morphologically rich languages (PD-MRLs) due to dropped pronouns (e.g., null subjects and omitted possessive pronouns). These phenomena require a representation scheme at the morphology level and enhanced evaluation methods.In this paper, we propose a representation & evaluation scheme to incorporate dropped pronouns into coreference resolution and validate it on the Turkish language.Using the scheme, we extend the annotations on the only existing Turkish coreference dataset, which originally did not contain annotations for dropped pronouns. We provide publicly available pre and post processors to enhance the prominent CoNLL coreference scorer also to cover coreferential relations arising from dropped pronouns.As a final step, the paper reports the first neural Turkish coreference resolution results in the literature.Although validated on Turkish, the proposed scheme is language-independent and may be used for other PD-MRLs",
    "volume": "student",
    "checked": true,
    "id": "dffcd859975a43d4b6ebbe40e64a15f3d465829e",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-srw.3": {
    "title": "Towards Generation and Recognition of Humorous Texts in Portuguese",
    "abstract": "Dealing with humor is an important step to develop Natural Language Processing tools capable of handling sophisticated semantic and pragmatic knowledge. In this context, this PhD thesis focuses on the automatic generation and recognition of verbal punning humor in Portuguese, which is still an underdeveloped language when compared to English. One of the main goals of this research is to conciliate Natural Language Generation computational models with existing theories of humor from the Humanities while avoiding mere generation by including contextual information into the generation process. Another point that is of utmost importance is the inclusion of the listener as an active part in the process of understanding and creating humor; we hope to achieve this by using concepts from Recommender Systems in our methods. Ultimately, we want to not only advance the current state-of-the-art in humor generation and recognition, but also to help the general Portuguese-speaking research community with methods, tools and resources that may aid in the development of further techniques for this language. We also expect our systems to provide insightful ideas about how humor is created and perceived by both humans and machines",
    "volume": "student",
    "checked": true,
    "id": "d80d59541093d8b542e64536ab904b362c21437d",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-srw.4": {
    "title": "GAP-Gen: Guided Automatic Python Code Generation",
    "abstract": "Automatic code generation from natural language descriptions can be highly beneficial during the process of software development. In this work, we propose GAP-Gen, a Guided Automatic Python Code Generation method based on Python syntactic constraints and semantic constraints. We first introduce Python syntactic constraints in the form of Syntax-Flow, which is a simplified version of Abstract Syntax Tree (AST) reducing the size and high complexity of Abstract Syntax Tree but maintaining crucial syntactic information of Python code. In addition to Syntax-Flow, we introduce Variable-Flow which abstracts variable and function names consistently through out the code. In our work, rather than pretraining, we focus on modifying the finetuning process which reduces computational requirements but retains high generation performance on automatic Python code generation task. GAP-Gen fine-tunes the transformer based language models T5 and CodeT5 using the Code-to-Docstring datasets CodeSearchNet, CodeSearchNet AdvTest and Code-Docstring Corpus from EdinburghNLP. Our experiments show that GAP-Gen achieves better results on automatic Python code generation task than previous works",
    "volume": "student",
    "checked": true,
    "id": "1e89e1503be350e030e91f635f0f8f967ece7876",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-srw.5": {
    "title": "Development of pre-trained language models for clinical NLP in Spanish",
    "abstract": "Clinical natural language processing aims to tackle language and prediction tasks using text from medical practice, such as clinical notes, prescriptions, and discharge summaries. Several approaches have been tried to deal with these tasks. Since 2017, pre-trained language models (PLMs) have achieved state-of-the-art performance in many tasks. However, most works have been developed in English. This PhD research proposal addresses the development of PLMs for clinical NLP in Spanish. To carry out this study, we will build a clinical corpus big enough to implement a functional PLM. We will test several PLM architectures and evaluate them with language and prediction tasks. The novelty of this work lies in the use of only clinical text, while previous clinical PLMs have used a mix of general, biomedical, and clinical text",
    "volume": "student",
    "checked": true,
    "id": "86e14ee18d99d5877a9ea168019fd4ccdd40db1a",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-srw.6": {
    "title": "Which One Are You Referring To? Multimodal Object Identification in Situated Dialogue",
    "abstract": "The demand for multimodal dialogue systems has been rising in various domains, emphasizing the importance of interpreting multimodal inputs from conversational and situational contexts. One main challenge in multimodal dialogue understanding is multimodal object identification, which constitutes the ability to identify objects relevant to a multimodal user-system conversation. We explore three methods to tackle this problem and evaluate them on the largest situated dialogue dataset, SIMMC 2.1. Our best method, scene-dialogue alignment, improves the performance by ~20% F1-score compared to the SIMMC 2.1 baselines. We provide analysis and discussion regarding the limitation of our methods and the potential directions for future works",
    "volume": "student",
    "checked": true,
    "id": "d5a952d3938e496af0ac67bc0e3d6a34f7040566",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-srw.7": {
    "title": "A Unified Framework for Emotion Identification and Generation in Dialogues",
    "abstract": "Social chatbots have gained immense popularity, and their appeal lies not just in their capacity to respond to the diverse requests from users, but also in the ability to develop an emotional connection with users. To further develop and promote social chatbots, we need to concentrate on increasing user interaction and take into account both the intellectual and emotional quotient in the conversational agents. In this paper, we propose a multi-task framework that jointly identifies the emotion of a given dialogue and generates response in accordance to the identified emotion. We employ a {text{BERT} based network for creating an empathetic system and use a mixed objective function that trains the end-to-end network with both the classification and generation loss. Experimental results show that our proposed framework outperforms current state-of-the-art models",
    "volume": "student",
    "checked": true,
    "id": "fedee3be826b9636068c6bd59159d5a4e7c9a6cc",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-srw.8": {
    "title": "Improving and Simplifying Template-Based Named Entity Recognition",
    "abstract": "With the rise in larger language models, researchers started exploiting them by pivoting the downstream tasks as language modeling tasks using prompts. In this work, we convert the Named Entity Recognition task into a seq2seq task by generating the synthetic sentences using templates. Our main contribution is the conversion framework which provides faster inference. In addition, we test our method’s performance in resource-rich, low resource and domain transfer settings. Results show that our method achieves comparable results in the resource-rich setting and outperforms the current seq2seq paradigm state-of-the-art approach in few-shot settings. Through the experiments, we observed that the negative examples play an important role in model’s performance. We applied our approach over BART and T5-base models, and we notice that the T5 architecture aligns better with our task. The work is performed on the datasets in English language",
    "volume": "student",
    "checked": true,
    "id": "1aae90be8721c07ec76d19c261cb27180690dc7a",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-srw.9": {
    "title": "Polite Chatbot: A Text Style Transfer Application",
    "abstract": "Generating polite responses is essential to build intelligent and engaging dialogue systems. However, this task is far from well-explored due to the difficulties of rendering a particular style in coherent responses, especially when parallel datasets for regular-to-polite pairs are usually unavailable.This paper proposes a polite chatbot that can produce responses that are polite and coherent to the given context.In this study, a politeness transfer model is first used to generate polite synthetic dialogue pairs of contexts and polite utterances. Then, these synthetic pairs are employed to train a dialogue model. Automatic and human evaluations demonstrate that our method outperforms baselines in producing polite dialogue responses while staying competitive in terms of coherent to the given context",
    "volume": "student",
    "checked": true,
    "id": "b378a4c7d69442912ead0b9d9488db251b383ac0",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-srw.10": {
    "title": "Template-guided Grammatical Error Feedback Comment Generation",
    "abstract": "Writing is an important element of language learning, and an increasing amount of learner writing is taking place in online environments. Teachers can provide valuable feedback by commenting on learner text. However, providing relevant feedback for every issue for every student can be time-consuming. To address this, we turn to the NLP subfield of feedback comment generation, the task of automatically generating explanatory notes for learner text with the goal of enhancing learning outcomes. However, freely-generated comments may mix multiple topics seen in the training data or even give misleading advice. In this thesis proposal, we seek to address these issues by categorizing comments and constraining the outputs of noisy classes. We describe an annotation scheme for feedback comment corpora using comment topics with a broader scope than existing typologies focused on error correction. We outline plans for experiments in grouping and clustering, replacing particularly diverse categories with modular templates, and comparing the generation results of using different linguistic features and model architectures with the original dataset versus the newly annotated one. This paper presents the first two years (the master’s component) of a research project for a five-year combined master’s and Ph.D program",
    "volume": "student",
    "checked": true,
    "id": "6e5313ea642fe777c59b8fd545591e8ba5a9ff80",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-srw.11": {
    "title": "Clinical Text Anonymization, its Influence on Downstream NLP Tasks and the Risk of Re-Identification",
    "abstract": "While text-based medical applications have become increasingly prominent, access to clinicaldata remains a major concern. To resolve this issue, further de-identification and anonymization of the data are required. This might, however, alter the contextual information within the clinical texts and therefore influence the learning and performance of possible language models. This paper systematically analyses the potential effects of various anonymization techniques on the performance of state-of-the-art machine learning models based on several datasets corresponding to five different NLP tasks. On this basis, we derive insightful findings and recommendations concerning text anonymization with regard to the performance of machine learning models. In addition, we present a simple re-identification attack applied to the anonymized text data, which can break the anonymization",
    "volume": "student",
    "checked": true,
    "id": "bc0f56a2bc17461f9f761f95dacf6e89890ec21f",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-srw.12": {
    "title": "Automatic Dialog Flow Extraction and Guidance",
    "abstract": "Today, human assistants are often replacedby chatbots, designed to communicate via natural language, however, some disadvantages are notorious with this replacement. This PhD thesis project consists of researching, implementing, and testing a solution for guiding the action of a human in a contact center.It will start with the discovery and creation of datasets in Portuguese.Next, it will go through three main components: Extraction for processing dialogs and using the information todescribe interactions; Representation for discovering the most frequent dialog flowsrepresented by graphs; Guidance for helping the agent during a new dialog. These will be integrated in a single framework. In order to avoid service degradation resulting from the adoption of chatbots, this work aims to explore technologies in order to increase the efficiency of the human’s job without losing human contact",
    "volume": "student",
    "checked": true,
    "id": "4be8c795bf247ac271aea7cb7619c3213f5f21e2",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-srw.13": {
    "title": "Diverse Content Selection for Educational Question Generation",
    "abstract": "Question Generation (QG) systems have shown promising results in reducing the time and effort required to create questions for students. Typically, a first step in QG is to select the content to design a question for. In an educational setting, it is crucial that the resulting questions cover the most relevant/important pieces of knowledge the student should have acquired. Yet, current QG systems either consider just a single sentence or paragraph (thus do not include a selection step), or do not consider this educational viewpoint of content selection. Aiming to fill this research gap with a solution for educational document level QG, we thus propose to select contents for QG based on relevance and topic diversity. We demonstrate the effectiveness of our proposed content selection strategy for QG on 2 educational datasets. In our performance assessment, we also highlight limitations of existing QG evaluation metrics in light of the content selection problem",
    "volume": "student",
    "checked": true,
    "id": "ea4bd6c2b223c5da0a7f37903fc05421a435560a",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-srw.14": {
    "title": "Towards Automatic Grammatical Error Type Classification for Turkish",
    "abstract": "Automatic error type classification is an important process in both learner corpora creation and evaluation of large-scale grammatical error correction systems. Rule-based classifier approaches such as ERRANT have been widely used to classify edits between correct-erroneous sentence pairs into predefined error categories.However, the used error categories are far from being universal yielding many language specific variants of ERRANT.In this paper, we discuss the applicability of the previously introduced grammatical error types to an agglutinative language, Turkish. We suggest changes on current error categories and discuss a hierarchical structure to better suit the inflectional and derivational properties of this morphologically highly rich language. We also introduce ERRANT-TR, the first automatic error type classification toolkit for Turkish. ERRANT-TR currently uses a rule-based error type classification pipeline which relies on word level morphological information. Due to unavailability of learner corpora in Turkish, the proposed system is evaluated on a small set of 106 annotated sentences and its performance is measured as 77.04{% F0.5 score. The next step is to use ERRANT-TR for the development of a Turkish learner corpus",
    "volume": "student",
    "checked": true,
    "id": "8af34a244d7d2745bd853626b4f27acae22ad5a9",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-srw.15": {
    "title": "Theoretical Conditions and Empirical Failure of Bracket Counting on Long Sequences with Linear Recurrent Networks",
    "abstract": "Previous work has established that RNNs with an unbounded activation function have the capacity to count exactly. However, it has also been shown that RNNs are challenging to train effectively and generally do not learn exact counting behaviour. In this paper, we focus on this problem by studying the simplest possible RNN, a linear single-cell network. We conduct a theoretical analysis of linear RNNs and identify conditions for the models to exhibit exact counting behaviour. We provide a formal proof that these conditions are necessary and sufficient. We also conduct an empirical analysis using tasks involving a Dyck-1-like Balanced Bracket language under two different settings. We observe that linear RNNs generally do not meet the necessary and sufficient conditions for counting behaviour when trained with the standard approach. We investigate how varying the length of training sequences and utilising different target classes impacts model behaviour during training and the ability of linear RNN models to effectively approximate the indicator conditions",
    "volume": "student",
    "checked": true,
    "id": "eb87c18dd5bfcc1106086a4c89653ecd02d71f33",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-srw.16": {
    "title": "Addressing Domain Changes in Task-oriented Conversational Agents through Dialogue Adaptation",
    "abstract": "Recent task-oriented dialogue systems are trained on annotated dialogues, which, in turn, reflect certain domain information (e.g., restaurants or hotels in a given region). However, when such domain knowledge changes (e.g., new restaurants open), the initial dialogue model may become obsolete, decreasing the overall performance of the system. Through a number of experiments, we show, for instance, that adding 50{% of new slot-values reduces of about 55{% the dialogue state-tracker performance. In light of such evidence, we suggest that automatic adaptation of training dialogues is a valuable option for re-training obsolete models. We experimented with a dialogue adaptation approach based on fine-tuning a generative language model on domain changes, showing that a significant reduction of performance decrease can be obtained",
    "volume": "student",
    "checked": true,
    "id": "696d3990ad70d71da4e45a6e0fb80e38ddec1bd6",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-tutorials.1": {
    "title": "Mining, Assessing, and Improving Arguments in NLP and the Social Sciences",
    "abstract": "Computational argumentation is an interdisciplinary research field, connecting Natural Language Processing (NLP) to other disciplines such as the social sciences. This tutorial will focus on a task that recently got into the center of attention in the community: argument quality assessment, that is, what makes an argument good or bad? We structure the tutorial along three main coordinates: (1) the notions of argument quality across disciplines (how do we recognize good and bad arguments?), (2) the modeling of subjectivity (who argues to whom; what are their beliefs?), and (3) the generation of improved arguments (what makes an argument better?). The tutorial highlights interdisciplinary aspects of the field, ranging from the collaboration of theory and practice (e.g., in NLP and social sciences), to approaching different types of linguistic structures (e.g., social media versus parliamentary texts), and facing the ethical issues involved (e.g., how to build applications for the social good). A key feature of this tutorial is its interactive nature: We will involve the participants in two annotation studies on the assessment and the improvement of quality, and we will encourage them to reflect on the challenges and potential of these tasks",
    "volume": "tutorial",
    "checked": true,
    "id": "c86b067edebc28f97e845ded1b5edcd5845b3a31",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-tutorials.2": {
    "title": "Emotion Analysis from Texts",
    "abstract": "Emotion analysis in text is an area of research that encompasses a set of various natural language processing (NLP) tasks, including classification and regression settings, as well as structured prediction tasks like role labelling or stimulus detection. In this tutorial, we provide an overview of research from emotion psychology which sets the ground for choosing adequate NLP methodology, and present existing resources and classification methods used for emotion analysis in texts. We further discuss appraisal theories and how events can be interpreted regarding their presumably caused emotion and briefly introduce emotion role labelling. In addition to these technical topics, we discuss the use cases of emotion analysis in text, their societal impact, ethical considerations, as well as the main challenges in the field",
    "volume": "tutorial",
    "checked": true,
    "id": "77d014853c658836f4d21c18ff288f0252739276",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-tutorials.3": {
    "title": "Summarization of Dialogues and Conversations At Scale",
    "abstract": "Conversations are the natural communication format for people. This fact has motivated the large body of question answering and chatbot research as a seamless way for people to interact with machines. The conversations between people however, captured as video, audio or private or public written conversations, largely remain untapped as a source of compelling starting point for developing language technology. Summarizing such conversations can be enormously beneficial: automatic minutes for meetings or meeting highlights sent to relevant people can optimize communication in various groups while minimizing demands on people’s time; similarly analysis of conversations in online support groups can provide valuable information to doctors about the patient concerns. Summarizing written and spoken conversation poses unique research challenges—text reformulation, discourse and meaning analysis beyond the sentence, collecting data, and proper evaluation metrics. All these have been revisited by researchers since the emergence of neural approaches as the dominant approach for solving language processing problems. In this tutorial, we will survey the cutting-edge methods for summarization of conversations, covering key sub-areas whose combination is needed for a successful solution",
    "volume": "tutorial",
    "checked": true,
    "id": "7a77471872e3fd83177ca0d8b2f0a427864a1192",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-tutorials.4": {
    "title": "Understanding Ethics in NLP Authoring and Reviewing",
    "abstract": "With NLP research now quickly being transferred into real-world applications, it is important to be aware of and think through the consequences of our scientific investigation. Such ethical considerations are important in both authoring and reviewing. This tutorial will equip participants with basic guidelines for thinking deeply about ethical issues and review common considerations that recur in NLP research. The methodology is interactive and participatory, including case studies and working in groups. Importantly, the participants will be co-building the tutorial outcomes and will be working to create further tutorial materials to share as public outcomes",
    "volume": "tutorial",
    "checked": true,
    "id": "12902f724619344dfeae330043c4b7b1c9d99bd0",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-tutorials.5": {
    "title": "AutoML for NLP",
    "abstract": "Automated Machine Learning (AutoML) is an emerging field that has potential to impact how we build models in NLP. As an umbrella term that includes topics like hyperparameter optimization and neural architecture search, AutoML has recently become mainstream at major conferences such as NeurIPS, ICML, and ICLR. What does this mean to NLP? Currently, models are often built in an ad hoc process: we might borrow default hyperparameters from previous work and try a few variant architectures, but it is never guaranteed that final trained model is optimal. Automation can introduce rigor in this model-building process. This tutorial will summarize the main AutoML techniques and illustrate how to apply them to improve the NLP model-building process",
    "volume": "tutorial",
    "checked": true,
    "id": "65d42a6ccd87db0a2c7541237abba6ae1fc257c8",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.eacl-tutorials.6": {
    "title": "Privacy-Preserving Natural Language Processing",
    "abstract": "This cutting-edge tutorial will help the NLP community to get familiar with current research in privacy-preserving methods. We will cover topics as diverse as membership inference, differential privacy, homomorphic encryption, or federated learning, all with typical applications to NLP. The goal is not only to draw the interest of the broader community, but also to present some typical use-cases and potential pitfalls in applying privacy-preserving methods to human language technologies",
    "volume": "tutorial",
    "checked": true,
    "id": "e7835090b10c0a9cd0d660a2fd3f25af645529e7",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.1": {
    "title": "Using Punctuation as an Adversarial Attack on Deep Learning-Based NLP Systems: An Empirical Study",
    "abstract": "This work empirically investigates punctuation insertions as adversarial attacks on NLP systems. Data from experiments on three tasks, five datasets, and six models with four attacks show that punctuation insertions, when limited to a few symbols (apostrophes and hyphens), are a superior attack vector compared to character insertions due to 1) a lower after-attack accuracy ($A_{aft-atk}$) than alphabetical character insertions; 2) higher semantic similarity between the resulting and original texts; and 3) a resulting text that is easier and faster to read as assessed with the Test of Word Reading Efficiency (TOWRE)). The tests also indicate that 4) grammar checking does not mitigate punctuation insertions and 5) punctuation insertions outperform word-level attacks in settings with a limited number of word synonyms and queries to the victim’s model. Our findings indicate that inserting a few punctuation types that result in easy-to-read samples is a general attack mechanism. In light of this threat, we assess the impact of punctuation insertions, potential mitigations, the mitigation’s tradeoffs, punctuation insertion’s worst-case scenarios and summarize our findings in a qualitative casual map, so that developers can design safer, more secure systems",
    "volume": "findings",
    "checked": true,
    "id": "9578190d7ba3f61799c1f23b70fb1db6b760bc27",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.2": {
    "title": "Self-Supervised Unimodal Label Generation Strategy Using Recalibrated Modality Representations for Multimodal Sentiment Analysis",
    "abstract": "While multimodal sentiment analysis (MSA) has gained much attention over the last few years, the main focus of most work on MSA has been limited to constructing multimodal representations that capture interactions between different modalities in a single task. This was largely due to a lack of unimodal annotations in MSA benchmark datasets. However, training a model using only multimodal representations can lead to suboptimal performance due to insufficient learning of each uni-modal representation. In this work, to fully optimize learning representations from multimodal data, we propose SUGRM which jointly trains multimodal and unimodal tasks using recalibrated features. The features are recalibrated such that the model learns to weight the features differently based on the features of other modalities. Further, to leverage unimodal tasks, we auto-generate unimodal annotations via a unimodal label generation module (ULGM). The experiment results on two benchmark datasets demonstrate the efficacy of our framework",
    "volume": "findings",
    "checked": true,
    "id": "645934bb7a8d78b5819aa89ef417916c7232c708",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.3": {
    "title": "Fighting FIRe with FIRE: Assessing the Validity of Text-to-Video Retrieval Benchmarks",
    "abstract": "Searching troves of videos with textual descriptions is a core multimodal retrieval task. Owing to the lack of a purpose-built dataset for text-to-video retrieval, video captioning datasets have been re-purposed to evaluate models by (1) treating captions as positive matches to their respective videos and (2) assuming all other videos to be negatives. However, this methodology leads to a fundamental flaw during evaluation: since captions are marked as relevant only to their original video, many alternate videos also match the caption, which introduces false-negative caption-video pairs. We show that when these false negatives are corrected, a recent state-of-the-art model gains 25% recall points—a difference that threatens the validity of the benchmark itself. To diagnose and mitigate this issue, we annotate and release 683K additional caption-video pairs. Using these, we recompute effectiveness scores for three models on two standard benchmarks (MSR-VTT and MSVD). We find that (1) the recomputed metrics are up to 25% recall points higher for the best models, (2) these benchmarks are nearing saturation for Recall@10, (3) caption length (generality) is related to the number of positives, and (4) annotation costs can be mitigated through sampling. We recommend retiring these benchmarks in their current form, and we make recommendations for future text-to-video retrieval benchmarks",
    "volume": "findings",
    "checked": true,
    "id": "3f27a7dad2f93c2b2ea8ad4719f8099bc386e8dc",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.4": {
    "title": "Improving Numeracy by Input Reframing and Quantitative Pre-Finetuning Task",
    "abstract": "Numbers have unique characteristics to words. Teaching models to understand numbers in text is an open-ended research question. Instead of discussing the required calculation skills, this paper focuses on a more fundamental topic: understanding numerals. We point out that innumeracy—the inability to handle basic numeral concepts—exists in most pretrained language models (LMs), and we propose a method to solve this issue by exploring the notation of numbers. Further, we discuss whether changing notation and pre-finetuning along with the comparing-number task can improve performance in three benchmark datasets containing quantitative-related tasks. The results of this study indicate that input reframing and the proposed pre-finetuning task is useful for RoBERTa",
    "volume": "findings",
    "checked": true,
    "id": "d1cbec486a54e846107db42eb6a3a89188c8b06a",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.5": {
    "title": "Visualize Before You Write: Imagination-Guided Open-Ended Text Generation",
    "abstract": "Recent advances in text-to-image synthesis make it possible to visualize machine imaginations for a given context. On the other hand, when generating text, human writers are gifted at creative visualization, which enhances their writings by forming imaginations as blueprints before putting down the stories in words. Inspired by such a cognitive process, we ask the natural question of whether we can endow machines with the same ability to utilize visual information and construct a general picture of the context to guide text generation. In this work, we propose iNLG that uses machine-generated images to guide language models (LM) in open-ended text generation. The experiments and analyses demonstrate the effectiveness of iNLG on open-ended text generation tasks, including text completion, story generation, and concept-to-text generation in both few-shot and full-data scenarios. Both automatic metrics and human evaluations verify that the text snippets generated by our iNLG are coherent and informative while displaying minor degeneration",
    "volume": "findings",
    "checked": true,
    "id": "591627746d0f8c3b642b7c9415bbc8af66e24a0e",
    "citation_count": 2
  },
  "https://aclanthology.org/2023.findings-eacl.6": {
    "title": "ImaginE: An Imagination-Based Automatic Evaluation Metric for Natural Language Generation",
    "abstract": "Automatic evaluations for natural language generation (NLG) conventionally rely on token-level or embedding-level comparisons with text references. This differs from human language processing, for which visual imagination often improves comprehension. In this work, we propose ImaginE, an imagination-based automatic evaluation metric for natural language generation. With the help of StableDiffusion, a state-of-the-art text-to-image generator, we automatically generate an image as the embodied imagination for the text snippet and compute the imagination similarity using contextual embeddings. Experiments spanning several text generation tasks demonstrate that adding machine-generated images with our ImaginE displays great potential in introducing multi-modal information into NLG evaluation, and improves existing automatic metrics’ correlations with human similarity judgments in both reference-based and reference-free evaluation scenarios",
    "volume": "findings",
    "checked": true,
    "id": "55b17a76d8b9f0a9adb5c116450a2cfd2844448c",
    "citation_count": 4
  },
  "https://aclanthology.org/2023.findings-eacl.7": {
    "title": "Entity-Aware Dual Co-Attention Network for Fake News Detection",
    "abstract": "Fake news and misinformation spread rapidly on the Internet. How to identify it and how to interpret the identification results have become important issues. In this paper, we propose a Dual Co-Attention Network (Dual-CAN) for fake news detection, which takes news content, social media replies, and external knowledge into consideration. Our experimental results support that the proposed Dual-CAN outperforms current representative models in two benchmark datasets. We further make in-depth discussions by comparing how models work in both datasets with empirical analysis of attention weights",
    "volume": "findings",
    "checked": true,
    "id": "ad0cb6aefc29e0a36ec3a6027cd642ffa2586ad7",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.8": {
    "title": "CIKQA: Learning Commonsense Inference with a Unified Knowledge-in-the-loop QA Paradigm",
    "abstract": "We propose a new commonsense reasoning benchmark to motivate commonsense reasoning progress from two perspectives: (1) Evaluating whether models can distinguish knowledge quality by predicting if the knowledge is enough to answer the question; (2) Evaluating whether models can develop commonsense inference capabilities that generalize across tasks. We first extract supporting knowledge for each question and ask humans to annotate whether the auto-extracted knowledge is enough to answer the question or not. After that, we convert different tasks into a unified question-answering format to evaluate the models’ generalization capabilities. We name the benchmark Commonsense Inference with Knowledge-in-the-loop Question Answering ({name). Experiments show that with our learning paradigm, models demonstrate encouraging generalization capabilities. At the same time, we also notice that distinguishing knowledge quality remains challenging for current commonsense reasoning models",
    "volume": "findings",
    "checked": true,
    "id": "3b622664d44a57280d3a189fa6475e56b96f1add",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.9": {
    "title": "Data-Efficient Methods For Improving Hate Speech Detection",
    "abstract": "Scarcity of large-scale datasets, especially for resource-impoverished languages motivates exploration of data-efficient methods for hate speech detection. Hateful intents are expressed explicitly (use of cuss, swear, abusive words) and implicitly (indirect and contextual). In this work, we progress implicit and explicit hate speech detection using an input-level data augmentation technique, task reformulation using entailment and cross-learning across five languages. Our proposed data augmentation technique EasyMix, improves the performance across all english datasets by ~1{% and across multilingual datasets by ~1-9{%. We also observe substantial gains of ~2-8% by reformulating hate speech detection as entail problem. We further probe the contextual models and observe that higher layers encode implicit hate while lower layers focus on explicit hate, highlighting the importance of token-level understanding for explicit and context-level for implicit hate speech detection. Code and Dataset splits - https://anonymous.4open.science/r/data_efficient_hatedetect/",
    "volume": "findings",
    "checked": true,
    "id": "e6f8d9ca8f99b7ecdc295cd05dee4c930aa4d886",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.10": {
    "title": "Learning the Effects of Physical Actions in a Multi-modal Environment",
    "abstract": "Large Language Models (LLMs) handle physical commonsense information inadequately. As a result of being trained in a disembodied setting, LLMs often fail to predict an action’s outcome in a given environment. However, predicting the effects of an action before it is executed is crucial in planning, where coherent sequences of actions are often needed to achieve a goal. Therefore, we introduce the multi-modal task of predicting the outcomes of actions solely from realistic sensory inputs (images and text). Next, we extend an LLM to model latent representations of objects to better predict action outcomes in an environment. We show that multi-modal models can capture physical commonsense when augmented with visual information. Finally, we evaluate our model’s performance on novel actions and objects and find that combining modalities help models to generalize and learn physical commonsense reasoning better",
    "volume": "findings",
    "checked": true,
    "id": "fe2f876a33d1f47650408034b74f395485e9034a",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.11": {
    "title": "FVQA 2.0: Introducing Adversarial Samples into Fact-based Visual Question Answering",
    "abstract": "The widely used Fact-based Visual Question Answering (FVQA) dataset contains visually-grounded questions that require information retrieval using common sense knowledge graphs to answer. It has been observed that the original dataset is highly imbalanced and concentrated on a small portion of its associated knowledge graph. We introduce FVQA 2.0 which contains adversarial variants of test questions to address this imbalance. We show that systems trained with the original FVQA train sets can be vulnerable to adversarial samples and we demonstrate an augmentation scheme to reduce this vulnerability without human annotations",
    "volume": "findings",
    "checked": true,
    "id": "80043cc6551a6b677698b6d0a946a4d3d92e8214",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.12": {
    "title": "Revisiting Intermediate Layer Distillation for Compressing Language Models: An Overfitting Perspective",
    "abstract": "Knowledge distillation (KD) is a highly promising method for mitigating the computational problems of pre-trained language models (PLMs). Among various KD approaches, Intermediate Layer Distillation (ILD) has been a de facto standard KD method with its performance efficacy in the NLP field. In this paper, we find that existing ILD methods are prone to overfitting to training datasets, although these methods transfer more information than the original KD. Next, we present the simple observations to mitigate the overfitting of ILD: distilling only the last Transformer layer and conducting ILD on supplementary tasks. Based on our two findings, we propose a simple yet effective consistency-regularized ILD (CR-ILD), which prevents the student model from overfitting the training dataset. Substantial experiments on distilling BERT on the GLUE benchmark and several synthetic datasets demonstrate that our proposed ILD method outperforms other KD techniques. Our code is available at https://github.com/jongwooko/CR-ILD",
    "volume": "findings",
    "checked": true,
    "id": "21a5cd656e6d1426d46c443fb85a41bc2dc53bef",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.13": {
    "title": "Implicit Temporal Reasoning for Evidence-Based Fact-Checking",
    "abstract": "Leveraging contextual knowledge has become standard practice in automated claim verification, yet the impact of temporal reasoning has been largely overlooked. Our study demonstrates that time positively influences the claim verification process of evidence-based fact-checking. The temporal aspects and relations between claims and evidence are first established through grounding on shared timelines, which are constructed using publication dates and time expressions extracted from their text. Temporal information is then provided to RNN-based and Transformer-based classifiers before or after claim and evidence encoding. Our time-aware fact-checking models surpass base models by up to 9% Micro F1 (64.17%) and 15% Macro F1 (47.43%) on the MultiFC dataset. They also outperform prior methods that explicitly model temporal relations between evidence. Our findings show that the presence of temporal information and the manner in which timelines are constructed greatly influence how fact-checking models determine the relevance and supporting or refuting character of evidence documents",
    "volume": "findings",
    "checked": true,
    "id": "36db49716cddec6a51ae620a7c519db7f76df58b",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.14": {
    "title": "Active PETs: Active Data Annotation Prioritisation for Few-Shot Claim Verification with Pattern Exploiting Training",
    "abstract": "To mitigate the impact of the scarcity of labelled data on fact-checking systems, we focus on few-shot claim verification. Despite recent work on few-shot classification by proposing advanced language models, there is a dearth of research in data annotation prioritisation that improves the selection of the few shots to be labelled for optimal model performance. We propose Active PETs, a novel weighted approach that utilises an ensemble of Pattern Exploiting Training (PET) models based on various language models, to actively select unlabelled data as candidates for annotation. Using Active PETs for few-shot data selection shows consistent improvement over the baseline methods, on two technical fact-checking datasets and using six different pretrained language models. We show further improvement with Active PETs-o, which further integrates an oversampling strategy. Our approach enables effective selection of instances to be labelled where unlabelled data is abundant but resources for labelling are limited, leading to consistently improved few-shot claim verification performance. Our code is available",
    "volume": "findings",
    "checked": true,
    "id": "608547338e096bfdf1db170ada341c28f1b7462a",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.findings-eacl.15": {
    "title": "Plan-then-Seam: Towards Efficient Table-to-Text Generation",
    "abstract": "Table-to-text generation aims at automatically generating text to help people conveniently obtain salient information in tables. Recent works explicitly decompose the generation process into content planning and surface generation stages, employing two autoregressive networks for them respectively.However, they are computationally expensive due to the non-parallelizable nature of autoregressive decoding and the redundant parameters of two networks.In this paper, we propose the first totally non-autoregressive table-to-text model (Plan-then-Seam, PTS) that produces its outputs in parallel with one single network.PTS firstly writes and calibrates one plan of the content to be generated with a novel rethinking pointer predictor, and then takes the plan as the context for seaming to decode the description.These two steps share parameters and perform iteratively to capture token inter-dependency while keeping parallel decoding.Experiments on two public benchmarks show that PTS achieves 3.0~5.6 times speedup for inference time, reducing 50% parameters, while maintaining as least comparable performance against strong two-stage table-to-text competitors",
    "volume": "findings",
    "checked": true,
    "id": "e47ae337f787e17b4d3bb817a2667eca430316c1",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.16": {
    "title": "A corpus of metaphors as register markers",
    "abstract": "The paper presents our work on corpus annotationfor metaphor in German. Metaphors denoteentities that are similar to their literal referent,e.g., when *Licht* ‘light’ is used in the senseof ‘hope’. We are interested in the relation betweenmetaphor and register, hence, the corpusincludes material from different registers.We focussed on metaphors that can serve asregister markers and can also be reliably indentifiedfor annotation. Our results show hugedifferences between registers in metaphor usage,which we interpret in terms of specificproperties of the registers",
    "volume": "findings",
    "checked": true,
    "id": "9d5595a45d3cf6153cf70955b895377342af8c17",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.17": {
    "title": "Translate First Reorder Later: Leveraging Monotonicity in Semantic Parsing",
    "abstract": "Prior work in semantic parsing has shown that conventional seq2seq models fail at compositional generalization tasks. This limitation led to a resurgence of methods that model alignments between sentences and their corresponding meaning representations, either implicitly through latent variables or explicitly by taking advantage of alignment annotations. We take the second direction and propose TPol, a two-step approach that first translates input sentences monotonically and then reorders them to obtain the correct output. This is achieved with a modular framework comprising a Translator and a Reorderer component. We test our approach on two popular semantic parsing datasets. Our experiments show that by means of the monotonic translations, TPol can learn reliable lexico-logical patterns from aligned data, significantly improving compositional generalization both over conventional seq2seq models, as well as over other approaches that exploit gold alignments",
    "volume": "findings",
    "checked": true,
    "id": "a19d9fd97cf172e1eb742053770c397d80468448",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.18": {
    "title": "PePe: Personalized Post-editing Model utilizing User-generated Post-edits",
    "abstract": "Incorporating personal preference is crucial in advanced machine translation tasks. Despite the recent advancement of machine translation, it remains a demanding task to properly reflect personal style. In this paper, we introduce a personalized automatic post-editing framework to address this challenge, which effectively generates sentences considering distinct personal behaviors. To build this framework, we first collect post-editing data that connotes the user preference from a live machine translation system. Specifically, real-world users enter source sentences for translation and edit the machine-translated outputs according to the user’s preferred style. We then propose a model that combines a discriminator module and user-specific parameters on the APE framework. Experimental results show that the proposed method outperforms other baseline models on four different metrics (i.e., BLEU, TER, YiSi-1, and human evaluation)",
    "volume": "findings",
    "checked": true,
    "id": "75aa0f2523baa2fa219214d58ba425b5841f8518",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.19": {
    "title": "Infusing Context and Knowledge Awareness in Multi-turn Dialog Understanding",
    "abstract": "In multi-turn dialog understanding, semantic frames are constructed by detecting intents and slots within each user utterance.However, recent works lack the capability of modeling multi-turn dynamics within a dialog in natural language understanding (NLU), instead leaving them for updating dialog states only.Moreover, humans usually associate relevant background knowledge with the current dialog contexts to better illustrate slot semantics revealed from word connotations, where previous works have explored such possibility mostly in knowledge-grounded response generation.In this paper, we propose to amend the research gap by equipping a BERT-based NLU framework with knowledge and context awareness.We first encode dialog contexts with a unidirectional context-aware transformer encoder and select relevant inter-word knowledge with the current word and previous history based on a knowledge attention mechanism. Experimental results in two complicated multi-turn dialog datasets have demonstrated significant improvements of our proposed framework. Attention visualization also demonstrates how our modules leverage knowledge across the utterance",
    "volume": "findings",
    "checked": true,
    "id": "0d8206d58decd57450a76134c111297ff210df25",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.20": {
    "title": "MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages",
    "abstract": "While there has been a recent burgeoning of applications at the intersection of natural and programming languages, such as code generation and code summarization, these applications are usually English-centric. This creates a barrier for program developers who are not proficient in English. To mitigate this gap in technology development across languages, we propose a multilingual dataset, MCoNaLa, to benchmark code generation from natural language commands extending beyond English. Modeled off of the methodology from the English Code/Natural Language Challenge (CoNaLa) dataset, we annotated a total of 896 NL-Code pairs in three languages: Spanish, Japanese, and Russian. We present a systematic evaluation on MCoNaLa by testing state-of-the-art code generation systems. Although the difficulties vary across three languages, all systems lag significantly behind their English counterparts, revealing the challenges in adapting code generation to new languages",
    "volume": "findings",
    "checked": true,
    "id": "d6b7bc4e7968398101d8d9b4da7d4a0186763ff3",
    "citation_count": 9
  },
  "https://aclanthology.org/2023.findings-eacl.21": {
    "title": "Augmenting pre-trained language models with audio feature embedding for argumentation mining in political debates",
    "abstract": "The integration of multimodality in natural language processing (NLP) tasks seeks to exploit the complementary information contained in two or more modalities, such as text, audio and video. This paper investigates the integration of often under-researched audio features with text, using the task of argumentation mining (AM) as a case study. We take a previously reported dataset and present an audio-enhanced version (the Multimodal USElecDeb60To16 dataset). We report the performance of two text models based on BERT and GloVe embeddings, one audio model (based on CNN and Bi-LSTM) and multimodal combinations, on a dataset of 28,850 utterances. The results show that multimodal models do not outperform text-based models when using the full dataset. However, we show that audio features add value in fully supervised scenarios with limited data. We find that when data is scarce (e.g. with 10% of the original dataset) multimodal models yield improved performance, whereas text models based on BERT considerably decrease performance. Finally, we conduct a study with artificially generated voices and an ablation study to investigate the importance of different audio features in the audio models",
    "volume": "findings",
    "checked": true,
    "id": "6fbc948729b5c49c7233f03255f928856708a62d",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.22": {
    "title": "Improving Retrieval Augmented Neural Machine Translation by Controlling Source and Fuzzy-Match Interactions",
    "abstract": "We explore zero-shot adaptation, where a general-domain model has access to customer or domain specific parallel data at inference time, but not during training. We build on the idea of Retrieval Augmented Translation (RAT) where top-k in-domain fuzzy matches are found for the source sentence, and target-language translations of those fuzzy-matched sentences are provided to the translation model at inference time. We propose a novel architecture to control interactions between a source sentence and the top-k fuzzy target-language matches, and compare it to architectures from prior work. We conduct experiments in two language pairs (En-De and En-Fr) by training models on WMT data and testing them with five and seven multi-domain datasets, respectively. Our approach consistently outperforms the alternative architectures, improving BLEU across language pair, domain, and number k of fuzzy matches",
    "volume": "findings",
    "checked": true,
    "id": "23652910a3ad653ea792fbbbabab026d807299c9",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.23": {
    "title": "CALM-Bench: A Multi-task Benchmark for Evaluating Causality-Aware Language Models",
    "abstract": "Causal reasoning is a critical component of human cognition and is required across a range of question-answering (QA) tasks (such as abductive reasoning, commonsense QA, and procedural reasoning). Research on causal QA has been underdefined, task-specific, and limited in complexity. Recent advances in foundation language models (such as BERT, ERNIE, and T5) have shown the efficacy of pre-trained models across diverse QA tasks. However, there is limited research exploring the causal reasoning capabilities of those language models and no standard evaluation benchmark. To unify causal QA research, we propose CALM-Bench, a multi-task benchmark for evaluating causality-aware language models (CALM). We present a standardized definition of causal QA tasks and show empirically that causal reasoning can be generalized and transferred across different QA tasks. Additionally, we share a strong multi-task baseline model which outperforms single-task fine-tuned models on the CALM-Bench tasks",
    "volume": "findings",
    "checked": true,
    "id": "458e17d2c67c8a7474ea5a6375d4a5d56273170c",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.24": {
    "title": "ezCoref: Towards Unifying Annotation Guidelines for Coreference Resolution",
    "abstract": "Large-scale, high-quality corpora are critical for advancing research in coreference resolution. However, existing datasets vary in their definition of coreferences and have been collected via complex and lengthy guidelines that are curated for linguistic experts. These concerns have sparked a growing interest among researchers to curate a unified set of guidelines suitable for annotators with various backgrounds. In this work, we develop a crowdsourcing-friendly coreference annotation methodology, ezCoref, consisting of an annotation tool and an interactive tutorial. We use ezCoref to re-annotate 240 passages from seven existing English coreference datasets (spanning fiction, news, and multiple other domains) while teaching annotators only cases that are treated similarly across these datasets. Surprisingly, we find that reasonable quality annotations were already achievable (90% agreement between the crowd and expert annotations) even without extensive training. On carefully analyzing the remaining disagreements, we identify the presence of linguistic cases that our annotators unanimously agree upon but lack unified treatments (e.g., generic pronouns, appositives) in existing datasets. We propose the research community should revisit these phenomena when curating future unified annotation guidelines",
    "volume": "findings",
    "checked": true,
    "id": "3f7b90f62c2190339c2fe8fe97286634ea00fa65",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.findings-eacl.25": {
    "title": "PREME: Preference-based Meeting Exploration through an Interactive Questionnaire",
    "abstract": "The recent increase in the volume of online meetings necessitates automated tools for organizing the material, especially when an attendee has missed the discussion and needs assistance in quickly exploring it. In this work, we propose a novel end-to-end framework for generating interactive questionnaires for preference-based meeting exploration. As a result, users are supplied with a list of suggested questions reflecting their preferences. Since the task is new, we introduce an automatic evaluation strategy by measuring how much the generated questions via questionnaire are answerable to ensure factual correctness and covers the source meeting for the depth of possible exploration",
    "volume": "findings",
    "checked": true,
    "id": "fd76561584ceeec407707b207f16cf734ebf2f5a",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.26": {
    "title": "Sentence Identification with BOS and EOS Label Combinations",
    "abstract": "The sentence is a fundamental unit in many NLP applications. Sentence segmentation is widely used as the first preprocessing task, where an input text is split into consecutive sentences considering the end of the sentence (EOS) as their boundaries. This task formulation relies on a strong assumption that the input text consists only of sentences, or what we call the sentential units (SUs). However, real-world texts often contain non-sentential units (NSUs) such as metadata, sentence fragments, nonlinguistic markers, etc. which are unreasonable or undesirable to be treated as a part of an SU. To tackle this issue, we formulate a novel task of sentence identification, where the goal is to identify SUs while excluding NSUs in a given text. To conduct sentence identification, we propose a simple yet effective method which combines the beginning of the sentence (BOS) and EOS labels to determine the most probable SUs and NSUs based on dynamic programming. To evaluate this task, we design an automatic, language-independent procedure to convert the Universal Dependencies corpora into sentence identification benchmarks. Finally, our experiments on the sentence identification task demonstrate that our proposed method generally outperforms sentence segmentation baselines which only utilize EOS labels",
    "volume": "findings",
    "checked": true,
    "id": "135d68bdff1971293938c78b17651b7574506b3f",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.27": {
    "title": "Gauging the Gap Between Human and Machine Text Simplification Through Analytical Evaluation of Simplification Strategies and Errors",
    "abstract": "This study presents an analytical evaluation of neural text simplification (TS) systems. Because recent TS models are trained in an end-to-end fashion, it is difficult to grasp their abilities to perform particular simplification operations. For the advancement of TS research and development, we should understand in detail what current TS systems can and cannot perform in comparison with human performance. To that end, we first developed an analytical evaluation framework consisting of fine-grained taxonomies of simplification strategies (at both the surface and content levels) and errors. Using this framework, we annotated TS instances produced by professional human editors and multiple neural TS systems and compared the results. Our analyses concretely and quantitatively revealed a wide gap between humans and systems, specifically indicating that systems tend to perform deletions and local substitutions while excessively omitting important information, and that the systems can hardly perform information addition operations. Based on our analyses, we also provide detailed directions to address these limitations",
    "volume": "findings",
    "checked": true,
    "id": "6a165cba013ef9b1f9f820f038d403a7d87abe5a",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.28": {
    "title": "Bridging the Gap between Pre-Training and Fine-Tuning for Commonsense Generation",
    "abstract": "Commonsense generation aims to generate a plausible sentence containing all given unordered concept words. Previous methods focusing on this task usually directly concatenate these words as the input of a pre-trained language model (PLM). However, in PLMs’ pre-training process, the inputs are often corrupted sentences with correct word order. This input distribution discrepancy between pre-training and fine-tuning makes the model difficult to fully utilize the knowledge of PLMs. In this paper, we propose a two-stage framework to alleviate this issue. Firstly, in pre-training stage, we design a new format of input to endow PLMs the ability to deal with masked sentences with incorrect word order. Secondly, during fine-tuning, we insert the special token [MASK] between two consecutive concept words to make the input distribution more similar to the input distribution in pre-training. We conduct extensive experiments and provide thorough analysis to demonstrate the effectiveness of our proposed method",
    "volume": "findings",
    "checked": true,
    "id": "94e9dabe823ac42620dd719bd0d483873054acde",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.29": {
    "title": "LED: A Dataset for Life Event Extraction from Dialogs",
    "abstract": "Lifelogging has gained more attention due to its wide applications, such as personalized recommendations or memory assistance. The issues of collecting and extracting personal life events have emerged. People often share their life experiences with others through conversations. However, extracting life events from conversations is rarely explored. In this paper, we present Life Event Dialog, a dataset containing fine-grained life event annotations on conversational data. In addition, we initiate a novel Conversational Life Event Extraction task and differentiate the task from the public event extraction or the life event extraction from other sources like microblogs. We explore three information extraction (IE) frameworks to address the Conversational Life Event Extraction task: OpenIE, relation extraction, and event extraction. A comprehensive empirical analysis of the three baselines is established. The results suggest that the current event extraction model still struggles with extracting life events from human daily conversations. Our proposed Life Event Dialog dataset and in-depth analysis of IE frameworks will facilitate future research on life event extraction from conversations",
    "volume": "findings",
    "checked": true,
    "id": "eeef10b15f0c01b31745c7f2b8d8cd76ab933180",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.30": {
    "title": "Reading and Reasoning over Chart Images for Evidence-based Automated Fact-Checking",
    "abstract": "Evidence data for automated fact-checking (AFC) can be in multiple modalities such as text, tables, images, audio, or video. While there is increasing interest in using images for AFC, previous works mostly focus on detecting manipulated or fake images. We propose a novel task, chart-based fact-checking, and introduce ChartBERT as the first model for AFC against chart evidence. ChartBERT leverages textual, structural and visual information of charts to determine the veracity of textual claims. For evaluation, we create ChartFC, a new dataset of 15,886 charts. We systematically evaluate 75 different vision-language (VL) baselines and show that ChartBERT outperforms VL models, achieving 63.8% accuracy. Our results suggest that the task is complex yet feasible, with many challenges ahead",
    "volume": "findings",
    "checked": true,
    "id": "579ebd42633908a72eea4ff2d153b766973e4b84",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.31": {
    "title": "Causal Reasoning of Entities and Events in Procedural Texts",
    "abstract": "Entities and events are crucial to natural language reasoning and common in procedural texts. Existing work has focused either exclusively on entity state tracking (e.g., whether a pan is hot) or on event reasoning (e.g., whether one would burn themselves by touching the pan), while these two tasks are often causally related. We propose CREPE, the first benchmark on causal reasoning of event plausibility and entity states. We show that most language models, including GPT-3, perform close to chance at .35 F1, lagging far behind human at .87 F1. We boost model performance to .59 F1 by creatively representing events as programming languages while prompting language models pretrained on code. By injecting the causal relations between entities and events as intermediate reasoning steps in our representation, we further boost the performance to .67 F1. Our findings indicate not only the challenge that CREPE brings for language models, but also the efficacy of code-like prompting combined with chain-of-thought prompting for multihop event reasoning",
    "volume": "findings",
    "checked": true,
    "id": "c43a4a7b7ea4f4889de051321cb0073fd577f843",
    "citation_count": 2
  },
  "https://aclanthology.org/2023.findings-eacl.32": {
    "title": "Few-Shot Structured Policy Learning for Multi-Domain and Multi-Task Dialogues",
    "abstract": "Reinforcement learning has been widely adopted to model dialogue managers in task-oriented dialogues. However, the user simulator provided by state-of-the-art dialogue frameworks are only rough approximations of human behaviour. The ability to learn from a small number of human interactions is hence crucial, especially on multi-domain and multi-task environments where the action space is large. We therefore propose to use structured policies to improve sample efficiency when learning on these kinds of environments. We also evaluate the impact of learning from human vs simulated experts. Among the different levels of structure that we tested, the graph neural networks (GNNs) show a remarkable superiority by reaching a success rate above 80% with only 50 dialogues when learning from simulated experts. They also show superiority when learning from human experts, although a performance drop was observed. We therefore suggest to concentrate future research efforts on bridging the gap between human data, simulators and automatic evaluators in dialogue frameworks",
    "volume": "findings",
    "checked": true,
    "id": "34924a31cd0264833b026755a4023059c2f27031",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.33": {
    "title": "Transfer Knowledge from Natural Language to Electrocardiography: Can We Detect Cardiovascular Disease Through Language Models?",
    "abstract": "Recent advancements in Large Language Models (LLMs) have drawn increasing attention since the learned embeddings pretrained on large-scale datasets have shown powerful ability in various downstream applications. However, whether the learned knowledge by LLMs can be transferred to clinical cardiology remains unknown. In this work, we aim to bridge this gap by transferring the knowledge of LLMs to clinical Electrocardiography (ECG). We propose an approach for cardiovascular disease diagnosis and automatic ECG diagnosis report generation. We also introduce an additional loss function by Optimal Transport (OT) to align the distribution between ECG and language embedding. The learned embeddings are evaluated on two downstream tasks: (1) automatic ECG diagnosis report generation, and (2) zero-shot cardiovascular disease detection. Our approach is able to generate high-quality cardiac diagnosis reports and also achieves competitive zero-shot classification performance even compared with supervised baselines, which proves the feasibility of transferring knowledge from LLMs to the cardiac domain",
    "volume": "findings",
    "checked": true,
    "id": "7ed237af793f43c442b3e8e1bc9ace906a276b2a",
    "citation_count": 3
  },
  "https://aclanthology.org/2023.findings-eacl.34": {
    "title": "Practical Takes on Federated Learning with Pretrained Language Models",
    "abstract": "Real-world applications of language models entail data privacy constraints when learning from diverse data domains. Federated learning with pretrained language models for language tasks has been gaining attention lately but there are definite confounders that warrants a careful study. Specifically, understanding the limits of federated NLP applications through varying the effects of different aspects (such as data heterogeneity, the trade-off between training time and performance, the effect of different data, and client distributions and sensitivity of the shared model to learning local distributions) is necessary to evaluate whether language models indeed learn to generalize by adapting to the different domains. Towards that, we elaborate different hypotheses over the components in federated NLP architectures and study them in detail with relevant experiments over three tasks: Stanford Sentiment Treebank-2, OntoNotes-5.0 and GigaWord. The experiments with different Transformer inductive biases on the variety of tasks provide a glimpse at the understanding of federated learning at NLP tasks. Specifically, the analysis suggests that regularization due to the ensembling effect may be masquerading as domain adaptation of federated learning in NLP with pre-trained language models",
    "volume": "findings",
    "checked": true,
    "id": "f641773113340ed9c3685609904f2eaf21c58104",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.35": {
    "title": "Paper Bullets: Modeling Propaganda with the Help of Metaphor",
    "abstract": "Propaganda aims to persuade an audience by appealing to emotions and using faulty reasoning, with the purpose of promoting a particular point of view. Similarly, metaphor modifies the semantic frame, thus eliciting a response that can be used to tune up or down the emotional volume of the message. Given the close relationship between them, we hypothesize that, when modeling them computationally, it can be beneficial to do so jointly. In particular, we perform multi-task learning with propaganda identification as the main task and metaphor detection as an auxiliary task. To the best of our knowledge, this is the first work that models metaphor and propaganda together. We experiment with two datasets for identifying propaganda techniques in news articles and in memes shared on social media. We find that leveraging metaphor improves model performance, particularly for the two most common propaganda techniques: loaded language and name-calling",
    "volume": "findings",
    "checked": true,
    "id": "538a02570bf20a6cfa30595667f2dcb76ed07d9d",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.36": {
    "title": "Lexical Semantics with Large Language Models: A Case Study of English \"break",
    "abstract": "Large neural language models (LLMs) can be powerful tools for research in lexical semantics. We illustrate this potential using the English verb “break”, which has numerous senses and appears in a wide range of syntactic frames. We show that LLMs capture known sense distinctions and can be used to identify informative new sense combinations for further analysis. More generally, we argue that LLMs are aligned with lexical semantic theories in providing high-dimensional, contextually modulated representations, but LLMs’ lack of discrete features and dependence on usage-based data offer a genuinely new perspective on traditional problems in lexical semantics",
    "volume": "findings",
    "checked": true,
    "id": "6256ac8e736cfccabac345f191f118a0a0ec2957",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.findings-eacl.37": {
    "title": "SWING: Balancing Coverage and Faithfulness for Dialogue Summarization",
    "abstract": "Missing information is a common issue of dialogue summarization where some information in the reference summaries is not covered in the generated summaries. To address this issue, we propose to utilize natural language inference (NLI) models to improve coverage while avoiding introducing factual inconsistencies. Specifically, we use NLI to compute fine-grained training signals to encourage the model to generate content in the reference summaries that have not been covered, as well as to distinguish between factually consistent and inconsistent generated sentences. Experiments on the DialogSum and SAMSum datasets confirm the effectiveness of the proposed approach in balancing coverage and faithfulness, validated with automatic metrics and human evaluations. Additionally, we compute the correlation between commonly used automatic metrics with human judgments in terms of three different dimensions regarding coverage and factual consistency to provide insight into the most suitable metric for evaluating dialogue summaries",
    "volume": "findings",
    "checked": true,
    "id": "0b0fe9d0809fe2dcd0e22c99e8bd0a3d12e94b9a",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.38": {
    "title": "Language-Aware Multilingual Machine Translation with Self-Supervised Learning",
    "abstract": "Multilingual machine translation (MMT) benefits from cross-lingual transfer but is a challenging multitask optimization problem. This is partly because there is no clear framework to systematically learn language-specific parameters. Self-supervised learning (SSL) approaches that leverage large quantities of monolingual data (where parallel data is unavailable) have shown promise by improving translation performance as complementary tasks to the MMT task. However, jointly optimizing SSL and MMT tasks is even more challenging. In this work, we first investigate how to utilize **intra-distillation** to learn more *language-specific* parameters and then show the importance of these language-specific parameters. Next, we propose a novel but simple SSL task, **concurrent denoising**, that co-trains with the MMT task by concurrently denoising monolingual data on both the encoder and decoder. Finally, we apply **intra-distillation** to this co-training approach. Combining these two approaches significantly improves MMT performance, outperforming three state-of-the-art SSL methods by a large margin, e.g., 11.3{% and 3.7{% improvement on an 8-language and a 15-language benchmark compared with MASS, respectively",
    "volume": "findings",
    "checked": true,
    "id": "501efff0dadd3229f5e0452829ae920e904230c2",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.39": {
    "title": "Cloze Quality Estimation for Language Assessment",
    "abstract": "Cloze tests play an essential role in language assessment and help language learners improve their skills.In this paper, we propose a novel task called Cloze Quality Estimation (CQE) — a zero-shot task of evaluating whether a cloze test is of sufficient “high-quality” for language assessment based on two important factors: reliability and validity.We have taken the first step by creating a new dataset named CELA for the CQE task, which includes English cloze tests and corresponding evaluations about their quality annotated by native English speakers, which includes 2,597 and 1,730 instances in aspects of reliability and validity, respectively.We have tested baseline evaluation methods on the dataset, showing that our method could contribute to the CQE task, but the task is still challenging",
    "volume": "findings",
    "checked": true,
    "id": "41828ba85bde1c52822631f3f37bf80c89e1e0c3",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.40": {
    "title": "Bag of Tricks for In-Distribution Calibration of Pretrained Transformers",
    "abstract": "While pre-trained language models (PLMs) have become a de-facto standard promoting the accuracy of text classification tasks, recent studies find that PLMs often predict over-confidently.Although calibration methods have been proposed, such as ensemble learning and data augmentation, most of the methods have been verified in computer vision benchmarks rather than in PLM-based text classification tasks. In this paper, we present an empirical study on confidence calibration for PLMs, addressing three categories, including confidence penalty losses, data augmentations, and ensemble methods. We find that the ensemble model overfitted to the training set shows sub-par calibration performance and also observe that PLMs trained with confidence penalty loss have a trade-off between calibration and accuracy. Building on these observations, we propose the Calibrated PLM (CALL), a combination of calibration techniques. The CALL complements shortcomings that may occur when utilizing a calibration method individually and boosts both classification and calibration accuracy. Design choices in CALL’s training procedures are extensively studied, and we provide a detailed analysis of how calibration techniques affect the calibration performance of PLMs",
    "volume": "findings",
    "checked": true,
    "id": "ba121a6e2583c5f9b137f04324c25239c63d3473",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.41": {
    "title": "Fine-Tuning Deteriorates General Textual Out-of-Distribution Detection by Distorting Task-Agnostic Features",
    "abstract": "Detecting out-of-distribution (OOD) inputs is crucial for the safe deployment of natural language processing (NLP) models. Though existing methods, especially those based on the statistics in the feature space of fine-tuned pre-trained language models (PLMs), are claimed to be effective, their effectiveness on different types of distribution shifts remains underexplored. In this work, we take the first step to comprehensively evaluate the mainstream textual OOD detection methods for detecting semantic and non-semantic shifts. We find that: (1) no existing method behaves well in both settings; (2) fine-tuning PLMs on in-distribution data benefits detecting semantic shifts but severely deteriorates detecting non-semantic shifts, which can be attributed to the distortion of task-agnostic features. To alleviate the issue, we present a simple yet effective general OOD score named GNOME that integrates the confidence scores derived from the task-agnostic and task-specific representations. Experiments show that GNOME works well in both semantic and non-semantic shift scenarios, and further brings significant improvement on two cross-task benchmarks where both kinds of shifts simultaneously take place. Our code is available at https://github.com/lancopku/GNOME",
    "volume": "findings",
    "checked": true,
    "id": "96a219acb0acdca790f7f9f7f30c507a47a06754",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.42": {
    "title": "A Question of Style: A Dataset for Analyzing Formality on Different Levels",
    "abstract": "Accounting for different degrees of formality is crucial for producing contextually appropriate language. To assist NLP applications concerned with this problem and formality analysis in general, we present the first dataset of sentences from a wide range of genres assessed on a continuous informal-formal scale via comparative judgments. It is the first corpus with a comprehensive perspective on German sentence-level formality overall. We compare machine learning models for formality scoring, a task we treat as a regression problem, on our dataset. Finally, we investigate the relation between sentence- and document-level formality and evaluate leveraging sentence-based annotations for assessing formality on documents",
    "volume": "findings",
    "checked": true,
    "id": "bd1af42e4099d9661bdbaed160bcc4bb65c086e2",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.43": {
    "title": "Task-specific Compression for Multi-task Language Models using Attribution-based Pruning",
    "abstract": "Multi-task language models show outstanding performance for various natural language understanding tasks with only a single model.However, these language models inevitably utilize an unnecessarily large number of model parameters, even when used only for a specific task.In this paper, we propose a novel training-free compression method for multi-task language models using pruning method.Specifically, we use an attribution method to determine which neurons are essential for performing a specific task.We task-specifically prune unimportant neurons and leave only task-specific parameters.Furthermore, we extend our method to be applicable in both low-resource and unsupervised settings. Since our compression method is training-free, it uses little computing resources and does not update the pre-trained parameters of language models, reducing storage space usage.Experimental results on the six widely-used datasets show that our proposed pruning method significantly outperforms baseline pruning methods.In addition, we demonstrate that our method preserves performance even in an unseen domain setting",
    "volume": "findings",
    "checked": true,
    "id": "e4d5f1d8b74f88309be480084830d7883239f989",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.44": {
    "title": "Zero-shot Transfer of Article-aware Legal Outcome Classification for European Court of Human Rights Cases",
    "abstract": "In this paper, we cast Legal Judgment Prediction on European Court of Human Rights cases into an article-aware classification task, where the case outcome is classified from a combined input of case facts and convention articles. This configuration facilitates the model learning some legal reasoning ability in mapping article text to specific case fact text. It also provides an opportunity to evaluate the model’s ability to generalize to zero-shot settings when asked to classify the case outcome with respect to articles not seen during training. We devise zero-shot experiments and apply domain adaptation methods based on domain discrimination and Wasserstein distance. Our results demonstrate that the article-aware architecture outperforms straightforward fact classification. We also find that domain adaptation methods improve zero-shot transfer performance, with article relatedness and encoder pre-training influencing the effect",
    "volume": "findings",
    "checked": true,
    "id": "5315883bd39a4e4af6332e344bb32d29613b3c97",
    "citation_count": 1
  },
  "https://aclanthology.org/2023.findings-eacl.45": {
    "title": "Abstractive Document Summarization with Summary-length Prediction",
    "abstract": "Recently, we can obtain a practical abstractive document summarization model by fine-tuning a pre-trained language model (PLM). Since the pre-training for PLMs does not consider summarization-specific information such as the target summary length, there is a gap between the pre-training and fine-tuning for PLMs in summarization tasks.To fill the gap, we propose a method for enabling the model to understand the summarization-specific information by predicting the summary length in the encoder and generating a summary of the predicted length in the decoder in fine-tuning.Experimental results on the WikiHow, NYT, and CNN/DM datasets showed that our methods improve ROUGE scores from BART by generating summaries of appropriate lengths. Further, we observed about 3.0, 1,5, and 3.1 point improvements for ROUGE-1, -2, and -L, respectively, from GSum on the WikiHow dataset. Human evaluation results also showed that our methods improve the informativeness and conciseness of summaries",
    "volume": "findings",
    "checked": true,
    "id": "499a99c77df968be0da0d72de7bdc62e59fd4d5c",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.46": {
    "title": "Hierarchical Label Generation for Text Classification",
    "abstract": "Hierarchical text classification (HTC) aims to assign the most relevant labels with the hierarchical structure to an input text. However, handling unseen labels with considering a label hierarchy is still an open problem for real-world applications because traditional HTC models employ a pre-defined label set. To deal with this problem, we propose a generation-based classifier that leverages a Seq2Seq framework to capture a label hierarchy and unseen labels explicitly. Because of no available social media datasets that target at HTC, we constructed a new ( Blog ) dataset using pairs of social media posts and their hierarchical topic labels. Experimental results on the Blog dataset showed the effectiveness of our generation-based classifier over state-of-the-art baseline models. Human evaluation results showed that the quality of generated unseen labels outperforms even the gold labels",
    "volume": "findings",
    "checked": true,
    "id": "f2c1b4e69a751ba6ff38a2f258119e6d82f7fb2a",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.47": {
    "title": "Active Learning for Multilingual Semantic Parser",
    "abstract": "Current multilingual semantic parsing (MSP) datasets are almost all collected by translating the utterances in the existing datasets from the resource-rich language to the target language. However, manual translation is costly. To reduce the translation effort, this paper proposes the first active learning procedure for MSP (AL-MSP). AL-MSP selects only a subset from the existing datasets to be translated. We also propose a novel selection method that prioritizes the examples diversifying the logical form structures with more lexical choices, and a novel hyperparameter tuning method that needs no extra annotation cost. Our experiments show that AL-MSP significantly reduces translation costs with ideal selection methods. Our selection method with proper hyperparameters yields better parsing performance than the other baselines on two multilingual datasets",
    "volume": "findings",
    "checked": true,
    "id": "e19a0d4ba1baa8d8291a59a4a32a89ea4433d0c8",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.48": {
    "title": "Joint Word and Morpheme Segmentation with Bayesian Non-Parametric Models",
    "abstract": "Language documentation often requires segmenting transcriptions of utterances collected on the field into words and morphemes. While these two tasks are typically performed in succession, we study here Bayesian models for simultaneously segmenting utterances at these two levels. Our aim is twofold: (a) to study the effect of explicitly introducing a hierarchy of units in joint segmentation models; (b) to further assess whether these two levels can be better identified through weak supervision. For this, we first consider a deterministic coupling between independent models; then design and evaluate hierarchical Bayesian models. Experiments with two under-resourced languages (Japhug and Tsez) allow us to better understand the value of various types of weak supervision. In our analysis, we use these results to revisit the distributional hypotheses behind Bayesian segmentation models and evaluate their validity for language documentation data",
    "volume": "findings",
    "checked": true,
    "id": "128bda0e8dc6c1edd4e6206d50753aae27f94567",
    "citation_count": 0
  },
  "https://aclanthology.org/2023.findings-eacl.49": {
    "title": "Cross-Lingual Transfer of Cognitive Processing Complexity",
    "abstract": "When humans read a text, their eye movements are influenced by the structural complexity of the input sentences. This cognitive phenomenon holds across languages and recent studies indicate that multilingual language models utilize structural similarities between languages to facilitate cross-lingual transfer. We use sentence-level eye-tracking patterns as a cognitive indicator for structural complexity and show that the multilingual model XLM-RoBERTa can successfully predict varied patterns for 13 typologically diverse languages, despite being fine-tuned only on English data. We quantify the sensitivity of the model to structural complexity and distinguish a range of complexity characteristics. Our results indicate that the model develops a meaningful bias towards sentence length but also integrates cross-lingual differences. We conduct a control experiment with randomized word order and find that the model seems to additionally capture more complex structural information",
    "volume": "findings",
    "checked": true,
    "id": "e4b5f2b56b2fe5645baf695f66958d0b35abf6b1",
    "citation_count": 0
  }
}