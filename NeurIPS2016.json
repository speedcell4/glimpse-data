{
  "https://papers.nips.cc/paper_files/paper/2016/hash/018b59ce1fd616d874afad0f44ba338d-Abstract.html": {
    "title": "Eliciting Categorical Data for Optimal Aggregation",
    "abstract": "Models for collecting and aggregating categorical data on crowdsourcing platforms typically fall into two broad categories: those assuming agents honest and consistent but with heterogeneous error rates, and those assuming agents strategic and seek to maximize their expected reward. The former often leads to tractable aggregation of elicited data, while the latter usually focuses on optimal elicitation and does not consider aggregation. In this paper, we develop a Bayesian model, wherein agents have differing quality of information, but also respond to incentives. Our model generalizes both categories and enables the joint exploration of optimal elicitation and aggregation. This model enables our exploration, both analytically and experimentally, of optimal aggregation of categorical data and optimal multiple-choice interface design",
    "volume": "main",
    "checked": true,
    "id": "56b6f8e28a71fc117a53b60246347fe979835f49",
    "citation_count": 11
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/01931a6925d3de09e5f87419d9d55055-Abstract.html": {
    "title": "A Locally Adaptive Normal Distribution",
    "abstract": "The multivariate normal density is a monotonic function of the distance to the mean, and its ellipsoidal shape is due to the underlying Euclidean metric. We suggest to replace this metric with a locally adaptive, smoothly changing (Riemannian) metric that favors regions of high local density. The resulting locally adaptive normal distribution (LAND) is a generalization of the normal distribution to the \"manifold\" setting, where data is assumed to lie near a potentially low-dimensional manifold embedded in R^D. The LAND is parametric, depending only on a mean and a covariance, and is the maximum entropy distribution under the given metric. The underlying metric is, however, non-parametric. We develop a maximum likelihood algorithm to infer the distribution parameters that relies on a combination of gradient descent and Monte Carlo integration. We further extend the LAND to mixture models, and provide the corresponding EM algorithm. We demonstrate the efficiency of the LAND to fit non-trivial probability distributions over both synthetic data, and EEG measurements of human sleep",
    "volume": "main",
    "checked": true,
    "id": "c40c1e1ada32255a39e6065df62775b34d55c8ea",
    "citation_count": 17
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/01eee509ee2f68dc6014898c309e86bf-Abstract.html": {
    "title": "Tagger: Deep Unsupervised Perceptual Grouping",
    "abstract": "We present a framework for efficient perceptual inference that explicitly reasons about the segmentation of its inputs and features.  Rather than being trained for any specific segmentation, our framework learns the grouping process in an unsupervised manner or alongside any supervised task. We enable a neural network to group the representations of different objects in an iterative manner through a differentiable mechanism.  We achieve very fast convergence by allowing the system to amortize the joint iterative inference of the groupings and their representations.  In contrast to many other recently proposed methods for addressing multi-object scenes, our system does not assume the inputs to be images and can therefore directly handle other modalities. We evaluate our method on multi-digit classification of very cluttered images that require texture segmentation. Remarkably our method achieves improved classification performance over convolutional networks despite being fully connected, by making use of the grouping mechanism. Furthermore, we observe that our system greatly improves upon the semi-supervised result of a baseline Ladder network on our dataset. These results are evidence that grouping is a powerful tool that can help to improve sample efficiency",
    "volume": "main",
    "checked": true,
    "id": "66eb9741cee7fdeac0a437cc5e737535b7314f06",
    "citation_count": 128
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/0233f3bb964cf325a30f8b1c2ed2da93-Abstract.html": {
    "title": "Online Bayesian Moment Matching for Topic Modeling with Unknown Number of Topics",
    "abstract": "Latent Dirichlet Allocation (LDA) is a very popular model for topic modeling as well as many other problems with latent groups.  It is both simple and effective.  When the number of topics (or latent groups) is unknown, the Hierarchical Dirichlet Process (HDP) provides an elegant non-parametric extension; however, it is a complex model and it is difficult to incorporate prior knowledge since the distribution over topics is implicit.  We propose two new models that extend LDA in a simple and intuitive fashion by directly expressing a distribution over the number of topics.  We also propose a new online Bayesian moment matching technique to learn the parameters and the number of topics of those models based on streaming data.  The approach achieves higher log-likelihood than batch and online HDP with fixed hyperparameters on several corpora",
    "volume": "main",
    "checked": true,
    "id": "4ca2f8bbff8e54e8f62a6f56e78b86b94d259014",
    "citation_count": 16
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/0245952ecff55018e2a459517fdb40e3-Abstract.html": {
    "title": "Conditional Generative Moment-Matching Networks",
    "abstract": "Maximum mean discrepancy (MMD) has been successfully applied to learn deep generative models for characterizing a joint distribution of variables via kernel mean embedding. In this paper, we present conditional generative moment-matching networks (CGMMN), which learn a conditional distribution given some input variables based on a conditional maximum mean discrepancy (CMMD) criterion. The learning is performed by stochastic gradient descent with the gradient calculated by back-propagation. We evaluate CGMMN on a wide range of tasks, including predictive modeling, contextual generation, and Bayesian dark knowledge, which distills knowledge from a Bayesian model by learning a relatively small CGMMN student network. Our results demonstrate competitive performance in all the tasks",
    "volume": "main",
    "checked": true,
    "id": "41ad41f3aa51fd85a32b1e9b7abbe933e7f4771c",
    "citation_count": 39
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/0266e33d3f546cb5436a10798e657d97-Abstract.html": {
    "title": "Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in the Blanks",
    "abstract": "Hybrid methods that utilize both content and rating information are commonly used in many recommender systems. However, most of them use either handcrafted features or the bag-of-words representation as a surrogate for the content information but they are neither effective nor natural enough. To address this problem, we develop a collaborative recurrent autoencoder (CRAE) which is a denoising recurrent autoencoder (DRAE) that models the generation of content sequences in the collaborative filtering (CF) setting. The model generalizes recent advances in recurrent deep learning from i.i.d. input to non-i.i.d. (CF-based) input and provides a new denoising scheme along with a novel learnable pooling scheme for the recurrent autoencoder. To do this, we first develop a hierarchical Bayesian model for the DRAE and then generalize it to the CF setting. The synergy between denoising and CF enables CRAE to make accurate recommendations while learning to fill in the blanks in sequences. Experiments on real-world datasets from different domains (CiteULike and Netflix) show that, by jointly modeling the order-aware generation of sequences for the content information and performing CF for the ratings, CRAE is able to significantly outperform the state of the art on both the recommendation task based on ratings and the sequence generation task based on content information",
    "volume": "main",
    "checked": true,
    "id": "39afbfe64d83b17368948c6cb3567431580b2a29",
    "citation_count": 92
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/03255088ed63354a54e0e5ed957e9008-Abstract.html": {
    "title": "Bayesian Intermittent Demand Forecasting for Large Inventories",
    "abstract": "We present a scalable and robust Bayesian method for demand forecasting in the context of a large e-commerce platform, paying special attention to intermittent and bursty target statistics. Inference is approximated by the Newton-Raphson algorithm, reduced to linear-time Kalman smoothing, which allows us to operate on several orders of magnitude larger problems than previous related work. In a study on large real-world sales datasets, our method outperforms competing approaches on fast and medium moving items",
    "volume": "main",
    "checked": true,
    "id": "59402496779a6d92afd70a551b64be4356e1a781",
    "citation_count": 75
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/03afdbd66e7929b125f8597834fa83a4-Abstract.html": {
    "title": "Visual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks",
    "abstract": "We study the problem of synthesizing a number of likely future frames from a single input image. In contrast to traditional methods, which have tackled this problem in a deterministic or non-parametric way, we propose a novel approach which models future frames in a probabilistic manner. Our proposed method is therefore able to synthesize multiple possible next frames using the same model. Solving this challenging problem involves low- and high-level image and motion understanding for successful image synthesis. Here, we propose a novel network structure, namely a Cross Convolutional Network, that encodes images as feature maps and motion information as convolutional kernels to aid in synthesizing future frames. In experiments, our model performs well on both synthetic data, such as 2D shapes and animated game sprites, as well as on real-wold video data. We show that our model can also be applied to tasks such as visual analogy-making, and present analysis of the learned network representations",
    "volume": "main",
    "checked": true,
    "id": "397e9b56e46d3cc34af1525493e597facb104570",
    "citation_count": 407
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/03e7ef47cee6fa4ae7567394b99912b7-Abstract.html": {
    "title": "Achieving budget-optimality with adaptive schemes in crowdsourcing",
    "abstract": "Adaptive schemes, where tasks are assigned based on the data collected thus far, are widely used in practical crowdsourcing systems to efficiently allocate the budget. However, existing theoretical analyses of crowdsourcing systems suggest that the gain of adaptive task assignments is minimal. To bridge this gap, we investigate this question under a strictly more general probabilistic model, which has been recently introduced to model practical crowdsourcing data sets. Under this generalized Dawid-Skene model, we characterize the fundamental trade-off between budget and accuracy, and introduce a novel adaptive scheme that matches this fundamental limit. We further quantify the gain of adaptivity, by comparing the trade-off with the one for non-adaptive schemes, and confirm that the gain is significant and can be made arbitrarily large depending on the distribution of the difficulty level of the tasks at hand",
    "volume": "main",
    "checked": true,
    "id": "6b34498066fb9c6d8cbe1d1bc8b70e933823c0fe",
    "citation_count": 72
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/03f544613917945245041ea1581df0c2-Abstract.html": {
    "title": "Stochastic Gradient Richardson-Romberg Markov Chain Monte Carlo",
    "abstract": "Stochastic Gradient Markov Chain Monte Carlo (SG-MCMC) algorithms have become increasingly popular for Bayesian inference in large-scale applications. Even though these methods have proved useful in several scenarios, their performance is often limited by their bias. In this study, we propose a novel sampling algorithm that aims to reduce the bias of SG-MCMC while keeping the variance at a reasonable level. Our approach is based on a numerical sequence acceleration method, namely the Richardson-Romberg extrapolation, which simply boils down   to running almost the same SG-MCMC algorithm twice in parallel with different step sizes. We illustrate our framework on the popular Stochastic Gradient Langevin Dynamics (SGLD) algorithm and propose a novel SG-MCMC algorithm referred to as Stochastic Gradient Richardson-Romberg Langevin Dynamics (SGRRLD). We provide formal theoretical analysis and show that SGRRLD is asymptotically consistent, satisfies a central limit theorem, and its non-asymptotic bias and the mean squared-error can be bounded. Our results show that SGRRLD attains higher rates of convergence than SGLD in both finite-time and asymptotically, and it achieves the theoretical   accuracy of the methods that are based on higher-order integrators. We support our findings using both synthetic and real data experiments",
    "volume": "main",
    "checked": true,
    "id": "25197e54b96ff71f3eeebf7eccbba10c2c5b14a8",
    "citation_count": 33
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/04025959b191f8f9de3f924f0940515f-Abstract.html": {
    "title": "Generating Videos with Scene Dynamics",
    "abstract": "We capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (e.g. action classification) and video generation tasks (e.g. future prediction). We propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene's foreground from the background. Experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images. Moreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning. We believe generative video models can impact many applications in video understanding and simulation",
    "volume": "main",
    "checked": true,
    "id": "ee091ccf24c4f053c5c3dfbefe4a7975ed3447c1",
    "citation_count": 1244
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/046ddf96c233a273fd390c3d0b1a9aa4-Abstract.html": {
    "title": "Approximate maximum entropy principles via Goemans-Williamson with applications to provable variational methods",
    "abstract": "The well known maximum-entropy principle due to Jaynes, which states that given mean parameters, the maximum entropy distribution matching them is in an exponential family has been very popular in machine learning due to its “Occam’s razor” interpretation. Unfortunately, calculating the potentials in the maximum entropy distribution is intractable [BGS14]. We provide computationally efficient versions of this principle when the mean parameters are pairwise moments: we design distributions that approximately match given pairwise moments, while having entropy which is comparable to the maximum entropy distribution matching those moments.  We additionally provide surprising applications of the approximate maximum entropy principle to designing provable variational methods for partition function calculations for Ising models without any assumptions on the potentials of the model. More precisely, we show that we can get approximation guarantees for the log-partition function comparable to those in the low-temperature limit, which is the setting of optimization of quadratic forms over the hypercube. ([AN06])",
    "volume": "main",
    "checked": true,
    "id": "a96873f4f5203419c46cd5dc7c5f8dd645260631",
    "citation_count": 5
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/04df4d434d481c5bb723be1b6df1ee65-Abstract.html": {
    "title": "Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering",
    "abstract": "In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words’ embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs",
    "volume": "main",
    "checked": true,
    "id": "c41eb895616e453dcba1a70c9b942c5063cc656c",
    "citation_count": 5434
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/052335232b11864986bb2fa20fa38748-Abstract.html": {
    "title": "Fast Distributed Submodular Cover: Public-Private Data Summarization",
    "abstract": "In this paper, we introduce the public-private framework of data summarization motivated by privacy concerns in personalized recommender systems and online social services. Such systems have usually access to massive data generated by a large pool of users. A major fraction of the data is public and is visible to (and can be used for) all users. However, each user can also contribute some private data that should not be shared with other users to ensure her privacy. The goal is to provide a succinct summary of massive dataset, ideally as small as possible, from which customized summaries can be built for each user, i.e. it can contain elements from the public data (for diversity) and users' private data (for personalization). To formalize the above challenge, we assume that the scoring function according to which a user evaluates the utility of her summary satisfies submodularity, a widely used notion in data summarization applications. Thus, we model the data summarization targeted to each user as an instance of a submodular cover problem. However, when the data is massive it is infeasible to use the centralized greedy algorithm to find a customized summary even for a single user. Moreover, for a large pool of users, it is too time consuming to find such summaries separately. Instead, we develop a fast distributed algorithm for submodular cover, FASTCOVER, that provides a succinct summary in one shot and for all users. We show that the solution provided by FASTCOVER is competitive with that of the centralized algorithm with the number of rounds that is exponentially smaller than state of the art results. Moreover, we have implemented FASTCOVER with Spark to demonstrate its practical performance on a number of concrete applications, including personalized location recommendation, personalized movie recommendation, and dominating set on tens of millions of data points and varying number of users",
    "volume": "main",
    "checked": true,
    "id": "15d67d6303cd873ede3e695dc180bb44742e6615",
    "citation_count": 44
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/06138bc5af6023646ede0e1f7c1eac75-Abstract.html": {
    "title": "Exponential Family Embeddings",
    "abstract": "Word embeddings are a powerful approach to capturing semantic similarity among terms in a vocabulary. In this paper, we develop exponential family embeddings, which extends the idea of word embeddings to other types of high-dimensional data. As examples, we studied several types of data: neural data with real-valued observations, count data from a market basket analysis, and ratings data from a movie recommendation system. The main idea is that each observation is modeled conditioned on a set of latent embeddings and other observations, called the context, where the way the context is defined depends on the problem. In language the context is the surrounding words; in neuroscience the context is close-by neurons; in market basket data the context is other items in the shopping cart. Each instance of an embedding defines the context, the exponential family of conditional distributions, and how the embedding vectors are shared across data. We infer the embeddings with stochastic gradient descent, with an algorithm that connects closely to generalized linear models. On all three of our applications—neural activity of zebrafish, users’ shopping behavior, and movie ratings—we found that exponential family embedding models are more effective than other dimension reduction methods. They better reconstruct held-out data and find interesting qualitative structure",
    "volume": "main",
    "checked": false,
    "id": "f965abf637f2bf1580e27a59931512147d79704e",
    "citation_count": 1
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/062ddb6c727310e76b6200b7c71f63b5-Abstract.html": {
    "title": "A Non-parametric Learning Method for Confidently Estimating Patient's Clinical State and Dynamics",
    "abstract": "Estimating patient's clinical state from multiple concurrent physiological streams plays an important role in determining if a therapeutic intervention is necessary and for triaging patients in the hospital. In this paper we construct a non-parametric learning algorithm to estimate the clinical state of a patient. The algorithm addresses several known challenges with clinical state estimation such as eliminating bias introduced by therapeutic intervention censoring, increasing the timeliness of state estimation while ensuring a sufficient accuracy, and the ability to detect anomalous clinical states. These benefits are obtained by combining the tools of non-parametric Bayesian inference, permutation testing, and generalizations of the empirical Bernstein inequality. The algorithm is validated using real-world data from a cancer ward in a large academic hospital",
    "volume": "main",
    "checked": true,
    "id": "9f114f6fd6042088f70978953c1f5d2e1470d36b",
    "citation_count": 13
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/06409663226af2f3114485aa4e0a23b4-Abstract.html": {
    "title": "Integrated perception with recurrent multi-task neural networks",
    "abstract": "Modern discriminative predictors have been shown to match natural intelligences in specific perceptual tasks in image classification, object and part detection, boundary extraction, etc. However, a major advantage that natural intelligences still have is that they work well for all perceptual problems together, solving them efficiently and coherently in an integrated manner. In order to capture some of these advantages in machine perception, we ask two questions: whether deep neural networks can learn universal image representations, useful not only for a single task but for all of them, and how the solutions to the different tasks can be integrated in this framework. We answer by proposing a new architecture, which we call multinet, in which not only deep image features are shared between tasks, but where tasks can interact in a recurrent manner by encoding the results of their analysis in a common shared representation of the data. In this manner, we show that the performance of individual tasks in standard benchmarks can be improved first by sharing features between them and then, more significantly, by integrating their solutions in the common representation",
    "volume": "main",
    "checked": true,
    "id": "02c270487bd68afb0cc9224d2102118f42ee1d59",
    "citation_count": 75
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/07563a3fe3bbe7e3ba84431ad9d055af-Abstract.html": {
    "title": "Dialog-based Language Learning",
    "abstract": "A long-term goal of machine learning research is to build an intelligent dialog agent. Most research in natural language understanding has focused on learning from fixed training sets of labeled data, with supervision either at the word level (tagging, parsing tasks) or sentence level (question answering, machine translation). This kind of supervision is not realistic of how humans learn, where language is both learned by, and used for, communication. In this work, we study dialog-based language learning, where supervision is given naturally and implicitly in the response of the dialog partner during the conversation. We study this setup in two domains: the bAbI dataset of (Weston et al., 2015) and large-scale question answering from (Dodge et al., 2015). We evaluate a set of baseline learning strategies on these tasks, and show that a novel model incorporating predictive lookahead is a promising approach for learning from a teacher's response. In particular, a surprising result is that it can learn to answer questions correctly without any reward-based supervision at all",
    "volume": "main",
    "checked": true,
    "id": "3bbf2ee642ed311e500017def1f54df453a935c1",
    "citation_count": 96
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/076a0c97d09cf1a0ec3e19c7f2529f2b-Abstract.html": {
    "title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks",
    "abstract": "Recurrent neural networks (RNNs) stand at the forefront of many recent developments in deep learning. Yet a major difficulty with these models is their tendency to overfit, with dropout shown to fail when applied to recurrent layers. Recent results at the intersection of Bayesian modelling and deep learning offer a Bayesian interpretation of common deep learning techniques such as dropout. This grounding of dropout in approximate Bayesian inference suggests an extension of the theoretical results, offering insights into the use of dropout with RNN models. We apply this new variational inference based dropout technique in LSTM and GRU models, assessing it on language modelling and sentiment analysis tasks. The new approach outperforms existing techniques, and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank (73.4 test perplexity). This extends our arsenal of variational tools in deep learning",
    "volume": "main",
    "checked": true,
    "id": "0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652",
    "citation_count": 1469
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/0771fc6f0f4b1d7d1bb73bbbe14e0e31-Abstract.html": {
    "title": "Automatic Neuron Detection in Calcium Imaging Data Using Convolutional Networks",
    "abstract": "Calcium imaging is an important technique for monitoring the activity of thousands of neurons simultaneously. As calcium imaging datasets grow in size, automated detection of individual neurons is becoming important. Here we apply a supervised learning approach to this problem and show that convolutional networks can achieve near-human accuracy and superhuman speed. Accuracy is superior to the popular PCA/ICA method based on precision and recall relative to ground truth annotation by a human expert. These results suggest that convolutional networks are an efficient and flexible tool for the analysis of large-scale calcium imaging data",
    "volume": "main",
    "checked": true,
    "id": "e275384296a4dd669d3a5470354a78be1df0b84e",
    "citation_count": 79
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/07811dc6c422334ce36a09ff5cd6fe71-Abstract.html": {
    "title": "Convolutional Neural Fabrics",
    "abstract": "Despite the success of CNNs, selecting the optimal architecture for a given task remains an open problem. Instead of aiming to select a single optimal architecture, we propose a ``fabric'' that embeds an exponentially large number of architectures. The fabric consists of a 3D trellis that connects response maps at different layers, scales, and channels  with a sparse homogeneous local connectivity pattern. The only hyper-parameters of a fabric are the number of channels and layers. While individual architectures can be recovered as paths, the fabric can in addition ensemble all embedded architectures together, sharing their weights where their  paths overlap. Parameters can be learned using standard methods based on back-propagation, at a cost that scales linearly in the fabric size. We present benchmark results competitive with the state of the art for image classification on MNIST and CIFAR10, and for semantic segmentation on the Part Labels dataset",
    "volume": "main",
    "checked": true,
    "id": "197c8988ef21d0b58d363c21bafe1900c3089e3e",
    "citation_count": 216
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/07cdfd23373b17c6b337251c22b7ea57-Abstract.html": {
    "title": "Budgeted stream-based active learning via adaptive submodular maximization",
    "abstract": "Active learning enables us to reduce the annotation cost by adaptively selecting unlabeled instances to be labeled. For pool-based active learning, several effective methods with theoretical guarantees have been developed through maximizing some utility function satisfying adaptive submodularity. In contrast, there have been few methods for stream-based active learning based on adaptive submodularity. In this paper, we propose a new class of utility functions, policy-adaptive submodular functions, and prove this class includes many existing adaptive submodular functions appearing in real world problems. We provide a general framework based on policy-adaptive submodularity that makes it possible to convert existing pool-based methods to stream-based methods and give theoretical guarantees on their performance. In addition we empirically demonstrate their effectiveness comparing with existing heuristics on common benchmark datasets",
    "volume": "main",
    "checked": true,
    "id": "311ac557c96daa6adbc81f0d9e7b9cccba571d74",
    "citation_count": 34
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/08e6bea8e90ba87af3c9554d94db6579-Abstract.html": {
    "title": "An equivalence between high dimensional Bayes optimal inference and M-estimation",
    "abstract": "Due to the computational difficulty of performing MMSE (minimum mean squared error) inference, maximum a posteriori (MAP) is often used as a surrogate. However, the accuracy of MAP is suboptimal for high dimensional inference, where the number of model parameters is of the same order as the number of samples. In this work we demonstrate how MMSE performance is asymptotically achievable via optimization with an appropriately selected convex penalty and regularization function which are a smoothed version of the widely applied MAP algorithm. Our findings provide a new derivation and interpretation for recent optimal M-estimators discovered by El Karoui, et. al. PNAS 2013 as well as extending to non-additive noise models. We demonstrate the performance of these optimal M-estimators with numerical simulations.  Overall, at the heart of our work is the revelation of a remarkable equivalence between two seemingly very different computational problems: namely that of high dimensional Bayesian integration, and high dimensional convex optimization.  In essence we show that the former computationally difficult integral may be computed by solving the latter, simpler optimization problem",
    "volume": "main",
    "checked": true,
    "id": "6563131e8ee5aa69ba6365b9754fc2e6a7fc19a6",
    "citation_count": 16
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/093b60fd0557804c8ba0cbf1453da22f-Abstract.html": {
    "title": "A Sparse Interactive Model for Matrix Completion with Side Information",
    "abstract": "Matrix completion methods can benefit from side information besides the partially observed matrix. The use of side features describing the row and column entities of a matrix has been shown to reduce the sample complexity for completing the matrix. We propose a novel sparse formulation that explicitly models the interaction between the row and column side features to approximate the matrix entries. Unlike early methods, this model does not require the low-rank condition on the model parameter matrix. We prove that when the side features can span the latent feature space of the matrix to be recovered, the number of observed entries needed for an exact recovery is $O(\\log N)$ where $N$ is the size of the matrix. When the side features are corrupted latent features of the matrix with a small perturbation, our method can achieve an $\\epsilon$-recovery with $O(\\log N)$ sample complexity, and maintains a $\\O(N^{3/2})$ rate similar to classfic methods with no side information. An efficient linearized Lagrangian algorithm is developed with a strong guarantee of convergence. Empirical results show that our approach outperforms three state-of-the-art methods both in simulations and on real world datasets",
    "volume": "main",
    "checked": true,
    "id": "124a37515c7da761916311789d95aaed303f3eec",
    "citation_count": 29
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/0966289037ad9846c5e994be2a91bafa-Abstract.html": {
    "title": "Bi-Objective Online Matching and Submodular Allocations",
    "abstract": "Online allocation problems have been widely studied due to their numerous practical applications (particularly to Internet advertising), as well as considerable theoretical interest. The main challenge in such problems is making assignment decisions in the face of uncertainty about future input; effective algorithms need to predict which constraints are most likely to bind, and learn the balance between short-term gain and the value of long-term resource availability.  In many important applications, the algorithm designer is faced with multiple objectives to optimize. In particular, in online advertising it is fairly common to optimize multiple metrics, such as clicks, conversions, and impressions, as well as other metrics which may be largely uncorrelated such as ‘share of voice’, and ‘buyer surplus’. While there has been considerable work on multi-objective offline optimization (when the entire input is known in advance), very little is known about the online case, particularly in the case of adversarial input. In this paper, we give the first results for bi-objective online submodular optimization, providing almost matching upper and lower bounds for allocating items to agents with two submodular value functions. We also study practically relevant special cases of this problem related to Internet advertising, and obtain improved results. All our algorithms are nearly best possible, as well as being efficient and easy to implement in practice",
    "volume": "main",
    "checked": true,
    "id": "6c0d5ffbaced1565580f3fb2c9b6dce50bb5871b",
    "citation_count": 8
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/0a09c8844ba8f0936c20bd791130d6b6-Abstract.html": {
    "title": "Interpretable Distribution Features with Maximum Testing Power",
    "abstract": "Two semimetrics on probability distributions are proposed, given as the sum of differences of expectations of analytic functions evaluated at spatial or frequency locations (i.e, features). The features are chosen so as to maximize the distinguishability of the distributions, by optimizing a lower bound on test power for a statistical test using these features. The result is a parsimonious and interpretable indication of how and where two distributions differ locally. An empirical estimate of the test power criterion converges with increasing sample size, ensuring the quality of the returned features. In real-world benchmarks on high-dimensional text and image data, linear-time tests using the proposed semimetrics achieve comparable performance to the state-of-the-art quadratic-time maximum mean discrepancy test, while returning human-interpretable features that explain the test results",
    "volume": "main",
    "checked": true,
    "id": "b126f2344e97cb97b1014e9686a5496cb4982011",
    "citation_count": 94
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/0a0a0c8aaa00ade50f74a3f0ca981ed7-Abstract.html": {
    "title": "Finding significant combinations of features in the presence of categorical covariates",
    "abstract": "In high-dimensional settings, where the number of features p is typically much larger than the number of samples n, methods which can systematically examine arbitrary combinations of features, a huge 2^p-dimensional space, have recently begun to be explored. However, none of the current methods is able to assess the association between feature combinations and a target variable while conditioning on a categorical covariate, in order to correct for potential confounding effects.  We propose the Fast Automatic Conditional Search (FACS) algorithm, a significant discriminative itemset mining method which conditions on categorical covariates and only scales as O(k log k), where k is the number of states of the categorical covariate. Based on the Cochran-Mantel-Haenszel Test, FACS demonstrates superior speed and statistical power on simulated and real-world datasets compared to the state of the art, opening the door to numerous applications in biomedicine",
    "volume": "main",
    "checked": true,
    "id": "a37509504f92e8f8c53a25b86e8fc86c77916a21",
    "citation_count": 29
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/0a113ef6b61820daa5611c870ed8d5ee-Abstract.html": {
    "title": "A Non-convex One-Pass Framework for Generalized Factorization Machine and Rank-One Matrix Sensing",
    "abstract": "We develop an efficient alternating framework for learning a generalized version of Factorization Machine (gFM) on steaming data with provable guarantees. When the instances are sampled from $d$ dimensional random Gaussian vectors and the target second order coefficient matrix in gFM is of rank $k$, our algorithm converges linearly, achieves $O(\\epsilon)$ recovery error after retrieving $O(k^{3}d\\log(1/\\epsilon))$ training instances, consumes $O(kd)$ memory in one-pass of dataset and only requires matrix-vector product operations in each iteration. The key ingredient of our framework is a construction of an estimation sequence  endowed with a so-called Conditionally Independent RIP condition (CI-RIP). As special cases of gFM, our framework can be applied to symmetric or asymmetric rank-one matrix sensing problems, such as inductive matrix completion and phase retrieval",
    "volume": "main",
    "checked": true,
    "id": "1935cdb485247a1f7cb867320aa9b5592ea1253d",
    "citation_count": 19
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/0a87257e5308197df43230edf4ad1dae-Abstract.html": {
    "title": "Avoiding Imposters and Delinquents: Adversarial Crowdsourcing and Peer Prediction",
    "abstract": "We consider a crowdsourcing model in which n workers are asked to rate the quality of n items previously generated by other workers. An unknown set of $\\alpha n$ workers generate reliable ratings, while the remaining workers may behave arbitrarily and possibly adversarially. The manager of the experiment can also manually evaluate the quality of a small number of items, and wishes to curate together almost all of the high-quality items with at most an fraction of low-quality items. Perhaps surprisingly, we show that this is possible with an amount of work required of the manager, and each worker, that does not scale with n: the dataset can be curated with $\\tilde{O}(1/\\beta\\alpha\\epsilon^4)$ ratings per worker, and $\\tilde{O}(1/\\beta\\epsilon^2)$ ratings by the manager, where $\\beta$ is the fraction of high-quality items. Our results extend to the more general setting of peer prediction, including peer grading in online classrooms",
    "volume": "main",
    "checked": true,
    "id": "e2d78d2800910060696c3a376043b88b92aa5665",
    "citation_count": 36
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/0bf727e907c5fc9d5356f11e4c45d613-Abstract.html": {
    "title": "Threshold Bandits, With and Without Censored Feedback",
    "abstract": "We consider the \\emph{Threshold Bandit} setting, a variant of the classical multi-armed bandit problem in which the reward on each round depends on a piece of side information known as a \\emph{threshold value}. The learner selects one of $K$ actions (arms), this action generates a random sample from a fixed distribution, and the action then receives a unit payoff in the event that this sample exceeds the threshold value. We consider two versions of this problem, the \\emph{uncensored} and \\emph{censored} case, that determine whether the sample is always observed or only when the threshold is not met. Using new tools to understand the popular UCB algorithm, we show that the uncensored case is essentially no more difficult than the classical multi-armed bandit setting. Finally we show that the censored case exhibits more challenges, but we give guarantees in the event that the sequence of threshold values is generated optimistically",
    "volume": "main",
    "checked": true,
    "id": "e56a77de71d367b45f368de4dcfed3c83bc0acb6",
    "citation_count": 16
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/0c9ebb2ded806d7ffda75cd0b95eb70c-Abstract.html": {
    "title": "Variational Bayes on Monte Carlo Steroids",
    "abstract": "Variational approaches are often used to approximate intractable posteriors or normalization constants in hierarchical latent variable models. While often effective in practice, it is known that the approximation error can be arbitrarily large.  We propose a new class of bounds on the marginal log-likelihood of directed latent variable models. Our approach relies on random projections to simplify the posterior. In contrast to standard variational methods, our bounds are guaranteed to be tight with high probability. We provide a new approach for learning latent variable models based on optimizing our new bounds on the log-likelihood. We demonstrate empirical improvements on benchmark datasets in vision and language for sigmoid belief networks, where a neural network is used to approximate the posterior",
    "volume": "main",
    "checked": true,
    "id": "1fd2a94b4e42c00b40770cb16518ad6b2756f431",
    "citation_count": 5
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/0d4f4805c36dc6853edfa4c7e1638b48-Abstract.html": {
    "title": "Finite-Dimensional BFRY Priors and Variational Bayesian Inference for Power Law Models",
    "abstract": "Bayesian nonparametric  methods based on the Dirichlet process (DP), gamma process and beta process, have proven effective in capturing aspects of various datasets arising in machine learning.  However, it is now recognized that such processes have their limitations in terms of the ability to capture power law behavior. As such there is now considerable interest in models based on the Stable Processs (SP), Generalized Gamma process (GGP) and Stable-beta process (SBP). These models present new challenges in terms of practical statistical implementation. In analogy to tractable processes such as the finite-dimensional Dirichlet process, we describe a class of random processes, we call iid finite-dimensional BFRY processes, that enables one to begin to develop efficient posterior inference algorithms such as variational Bayes that readily scale to massive datasets. For illustrative purposes, we describe a simple variational Bayes algorithm for normalized SP mixture models, and demonstrate its usefulness with experiments on synthetic and real-world datasets",
    "volume": "main",
    "checked": true,
    "id": "39de5abf8f3ed609e18d8e3e10d49dacfc4e839a",
    "citation_count": 12
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/0d73a25092e5c1c9769a9f3255caa65a-Abstract.html": {
    "title": "Maximal Sparsity with Deep Networks?",
    "abstract": "The iterations of many sparse estimation algorithms are comprised of a fixed linear filter cascaded with a thresholding nonlinearity, which collectively resemble a typical neural network layer.  Consequently, a lengthy sequence of algorithm iterations can be viewed as a deep network with shared, hand-crafted layer weights.  It is therefore quite natural to examine the degree to which a learned network model might act as a viable surrogate for traditional sparse estimation in domains where ample training data is available.  While the possibility of a reduced computational budget is readily apparent when a ceiling is imposed on the number of layers, our work primarily focuses on estimation accuracy.  In particular, it is well-known that when a signal dictionary has coherent columns, as quantified by a large RIP constant, then most tractable iterative algorithms are unable to find maximally sparse representations.  In contrast, we demonstrate both theoretically and empirically the potential for a trained deep network to recover minimal $\\ell_0$-norm representations in regimes where existing methods fail.  The resulting system, which can effectively learn novel iterative sparse estimation algorithms, is deployed on a practical photometric stereo estimation problem, where the goal is to remove sparse outliers that can disrupt the estimation of surface normals from a 3D scene",
    "volume": "main",
    "checked": true,
    "id": "7c8479100dc6f950ddc52d76eed9027870130f2a",
    "citation_count": 147
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/0deb1c54814305ca9ad266f53bc82511-Abstract.html": {
    "title": "Single-Image Depth Perception in the Wild",
    "abstract": "This paper studies single-image depth perception in the wild, i.e., recovering depth from a single image taken in unconstrained settings. We introduce a new dataset “Depth in the Wild” consisting of images in the wild annotated with relative depth between pairs of random points. We also propose a new algorithm that learns to estimate metric depth using annotations of relative depth. Compared to the state of the art, our algorithm is simpler and performs better. Experiments show that our algorithm, combined with existing RGB-D data and our new relative depth annotations, significantly improves single-image depth perception in the wild",
    "volume": "main",
    "checked": true,
    "id": "b17e61972e674f8f734bd428cb882a9bb797abe2",
    "citation_count": 398
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/0e55666a4ad822e0e34299df3591d979-Abstract.html": {
    "title": "Single Pass PCA of Matrix Products",
    "abstract": "In this paper we present a new algorithm for computing a low rank approximation of the product $A^TB$ by taking only a single pass of the two matrices $A$ and $B$. The straightforward way to do this is to (a) first sketch $A$ and $B$ individually, and then (b) find the top components using PCA on the sketch. Our algorithm in contrast retains additional summary information about $A,B$ (e.g. row and column norms etc.) and uses this additional information to obtain an improved approximation from the sketches. Our main analytical result establishes a comparable spectral norm guarantee to existing two-pass methods; in addition we also provide results from an Apache Spark implementation that shows better computational and statistical performance on real-world and synthetic evaluation datasets",
    "volume": "main",
    "checked": true,
    "id": "75f4ece734651bde5faea2cfdcf9a97a90215d1d",
    "citation_count": 8
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/0e65972dce68dad4d52d063967f0a705-Abstract.html": {
    "title": "Optimal Sparse Linear Encoders and Sparse PCA",
    "abstract": "Principal components analysis~(PCA) is the optimal linear  encoder of data. Sparse linear encoders (e.g., sparse PCA) produce more interpretable features that  can promote better generalization. (\\rn{1}) Given a level of sparsity, what is the best approximation to PCA?  (\\rn{2}) Are there efficient algorithms which can achieve this optimal  combinatorial tradeoff? We answer both questions by  providing the first polynomial-time algorithms to construct \\emph{optimal} sparse linear auto-encoders; additionally, we demonstrate the performance of our algorithms on real data",
    "volume": "main",
    "checked": true,
    "id": "874bb571b46dc0abb13953837f7374541fce4455",
    "citation_count": 5
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/0e9fa1f3e9e66792401a6972d477dcc3-Abstract.html": {
    "title": "Measuring the reliability of MCMC inference with bidirectional Monte Carlo",
    "abstract": "Markov chain Monte Carlo (MCMC) is one of the main workhorses of probabilistic inference, but it is notoriously hard to measure the quality of approximate posterior samples. This challenge is particularly salient in black box inference methods, which can hide details and obscure inference failures. In this work, we extend the recently introduced bidirectional Monte Carlo technique to evaluate MCMC-based posterior inference algorithms. By running annealed importance sampling (AIS) chains both from prior to posterior and vice versa on simulated data, we upper bound in expectation the symmetrized KL divergence between the true posterior distribution and the distribution of approximate samples. We integrate our method into two probabilistic programming languages, WebPPL and Stan, and validate it on several models and datasets. As an example of how our method be used to guide the design of inference algorithms, we apply it to study the effectiveness of different model representations in WebPPL and Stan",
    "volume": "main",
    "checked": true,
    "id": "fb3818bccddf09ab2f88fe05253dcf88d557a325",
    "citation_count": 18
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/0ed9422357395a0d4879191c66f4faa2-Abstract.html": {
    "title": "Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections",
    "abstract": "In this paper, we propose a very deep fully convolutional encoding-decoding framework for image restoration such as denoising and super-resolution. The network is composed of multiple layers of convolution and deconvolution operators, learning end-to-end mappings from corrupted images to the original ones. The convolutional layers act as the feature extractor, which capture the abstraction of image contents while eliminating noises/corruptions. Deconvolutional layers are then used to recover the image details. We propose to symmetrically link convolutional and deconvolutional layers with skip-layer connections, with which the training converges much faster and attains a higher-quality local optimum. First, the skip connections allow the signal to be back-propagated to bottom layers directly, and thus tackles the problem of gradient vanishing, making training deep networks easier and achieving restoration performance gains consequently. Second, these skip connections pass image details from convolutional layers to deconvolutional layers, which is beneficial in recovering the original image. Significantly, with the large capacity, we can handle different levels of noises using a single model. Experimental results show that our network achieves better performance than recent state-of-the-art methods",
    "volume": "main",
    "checked": true,
    "id": "18168aea48a22f6fe2fe407c0ff70083cba225a7",
    "citation_count": 1288
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/0efe32849d230d7f53049ddc4a4b0c60-Abstract.html": {
    "title": "On Valid Optimal Assignment Kernels and Applications to Graph Classification",
    "abstract": "The success of kernel methods has initiated the design of novel positive semidefinite functions, in particular for structured data. A leading design paradigm for this is the convolution kernel, which decomposes structured objects into their parts and sums over all pairs of parts. Assignment kernels, in contrast, are obtained from an optimal bijection between parts, which can provide a more valid notion of similarity. In general however, optimal assignments yield indefinite functions, which complicates their use in kernel methods. We characterize a class of base kernels used to compare parts that guarantees positive semidefinite optimal assignment kernels. These base kernels give rise to hierarchies from which the optimal assignment kernels are computed in linear time by histogram intersection. We apply these results by developing the Weisfeiler-Lehman optimal assignment kernel for graphs. It provides high classification accuracy on widely-used benchmark data sets improving over the original Weisfeiler-Lehman kernel",
    "volume": "main",
    "checked": true,
    "id": "da2e04453b6f0d89ee75e6f68d619d936cd9c0b5",
    "citation_count": 162
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/0f96613235062963ccde717b18f97592-Abstract.html": {
    "title": "Multi-view Anomaly Detection via Robust Probabilistic Latent Variable Models",
    "abstract": "We propose probabilistic latent variable models for multi-view anomaly detection, which is the task of finding instances that have inconsistent views given multi-view data. With the proposed model, all views of a non-anomalous instance are assumed to be generated from a single latent vector. On the other hand, an anomalous instance is assumed to have multiple latent vectors, and its different views are generated from different latent vectors. By inferring the number of latent vectors used for each instance with Dirichlet process priors, we obtain multi-view anomaly scores. The proposed model can be seen as a robust extension of probabilistic canonical correlation analysis for noisy multi-view data. We present Bayesian inference procedures for the proposed model based on a stochastic EM algorithm. The effectiveness of the proposed model is demonstrated in terms of performance when detecting multi-view anomalies",
    "volume": "main",
    "checked": true,
    "id": "170105988a29ae15cc8ff5fe0e24fde455c8a3f6",
    "citation_count": 28
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/0fe473396242072e84af286632d3f0ff-Abstract.html": {
    "title": "Optimal Architectures in a Solvable Model of Deep Networks",
    "abstract": "Deep neural networks have received a considerable attention due to the success of their training for real world machine learning applications. They are also of great interest to the understanding of sensory processing in cortical sensory hierarchies. The purpose of this work is to advance our theoretical understanding of the computational benefits of these architectures. Using a simple model of clustered noisy inputs and a simple learning rule, we provide analytically derived recursion relations describing the propagation of the signals along the deep network. By analysis of these equations, and defining performance measures, we show that these model networks have optimal depths. We further explore the dependence of the optimal architecture on the system parameters",
    "volume": "main",
    "checked": true,
    "id": "8f6de8e7a74d93114af9b6ab025e72cdbbfa94ac",
    "citation_count": 26
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/10907813b97e249163587e6246612e21-Abstract.html": {
    "title": "Efficient state-space modularization for planning: theory, behavioral and neural signatures",
    "abstract": "Even in state-spaces of modest size, planning is plagued by the “curse of dimensionality”. This problem is particularly acute in human and animal cognition given the limited capacity of working memory, and the time pressures under which planning often occurs in the natural environment. Hierarchically organized modular representations have long been suggested to underlie the capacity of biological systems to efficiently and flexibly plan in complex environments. However, the principles underlying efficient modularization remain obscure, making it difficult to identify its behavioral and neural signatures. Here, we develop a normative theory of efficient state-space representations which partitions an environment into distinct modules by minimizing the average (information theoretic) description length of planning within the environment, thereby optimally trading off the complexity of planning across and within modules. We show that such optimal representations provide a unifying account for a diverse range of hitherto unrelated phenomena at multiple levels of behavior and neural representation",
    "volume": "main",
    "checked": true,
    "id": "1840e7a05d0ff8465ace15a4a2e3ef4bad32c5c4",
    "citation_count": 20
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/10a5ab2db37feedfdeaab192ead4ac0e-Abstract.html": {
    "title": "A Communication-Efficient Parallel Algorithm for Decision Tree",
    "abstract": "Decision tree (and its extensions such as Gradient Boosting Decision Trees and Random Forest) is a widely used machine learning algorithm, due to its practical effectiveness and model interpretability. With the emergence of big data, there is an increasing need to parallelize the training process of decision tree. However, most existing attempts along this line suffer from high communication costs. In this paper, we propose a new algorithm, called \\emph{Parallel Voting Decision Tree (PV-Tree)}, to tackle this challenge. After partitioning the training data onto a number of (e.g., $M$) machines, this algorithm performs both local voting and global voting in each iteration. For local voting, the top-$k$ attributes are selected from each machine according to its local data. Then, the indices of these top attributes are aggregated by a server, and the globally top-$2k$ attributes are determined by a majority voting among these local candidates. Finally, the full-grained histograms of the globally top-$2k$ attributes are collected from local machines in order to identify the best (most informative) attribute and its split point. PV-Tree can achieve a very low communication cost (independent of the total number of attributes) and thus can scale out very well. Furthermore, theoretical analysis shows that this algorithm can learn a near optimal decision tree, since it can find the best attribute with a large probability. Our experiments on real-world datasets show that PV-Tree significantly outperforms the existing parallel decision tree algorithms in the tradeoff between accuracy and efficiency",
    "volume": "main",
    "checked": true,
    "id": "21237fed21499bf55b56be65e0ef7a582dd57cfc",
    "citation_count": 87
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/10c66082c124f8afe3df4886f5e516e0-Abstract.html": {
    "title": "Supervised Word Mover's Distance",
    "abstract": "Accurately measuring the similarity between text documents lies at the core of many real world applications of machine learning. These include web-search ranking, document recommendation, multi-lingual document matching, and article categorization. Recently, a new document metric, the word mover's distance (WMD), has been proposed with unprecedented results on kNN-based document classification. The WMD elevates high quality word embeddings to document metrics by formulating the distance between two documents as an optimal transport problem between the embedded words. However, the document distances are entirely unsupervised and lack a mechanism to incorporate supervision when available. In this paper we propose an efficient technique to learn a supervised metric, which we call the Supervised WMD (S-WMD) metric. Our algorithm learns document distances that measure the underlying semantic differences between documents by leveraging semantic differences between individual words discovered during supervised training. This is achieved with an linear transformation of the underlying word embedding space and tailored word-specific weights, learned to minimize the stochastic leave-one-out nearest neighbor classification error on a per-document level. We evaluate our metric on eight real-world text classification tasks on which S-WMD consistently  outperforms almost all of our 26 competitive baselines",
    "volume": "main",
    "checked": true,
    "id": "a896b4fd83aeb55a58b2a7313ee6e10576dbf28a",
    "citation_count": 145
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/1145a30ff80745b56fb0cecf65305017-Abstract.html": {
    "title": "Fast and accurate spike sorting of high-channel count probes with KiloSort",
    "abstract": "New silicon technology is enabling large-scale electrophysiological recordings in vivo from hundreds to thousands of channels. Interpreting these recordings requires scalable and accurate automated methods for spike sorting, which should minimize the time required for manual curation of the results. Here we introduce KiloSort, a new integrated spike sorting framework that uses template matching both during spike detection and during spike clustering. KiloSort models the electrical voltage as a sum of template waveforms triggered on the spike times, which allows overlapping spikes to be identified and resolved. Unlike previous algorithms that compress the data with PCA, KiloSort operates on the raw data which allows it to construct a more accurate model of the waveforms. Processing times are faster than in previous algorithms thanks to batch-based optimization on GPUs. We compare KiloSort to an established algorithm and show favorable performance, at much reduced processing times. A novel post-clustering merging step based on the continuity of the templates further reduced substantially the number of manual operations required on this data, for the neurons with near-zero error rates, paving the way for fully automated spike sorting of multichannel electrode recordings",
    "volume": "main",
    "checked": true,
    "id": "2b9785e12b82baf67b185b6c2b0220514797c442",
    "citation_count": 277
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/130f1a8e9e102707f3f91b010f151b0b-Abstract.html": {
    "title": "Learning brain regions via large-scale online structured sparse dictionary learning",
    "abstract": "We propose a multivariate online dictionary-learning method for obtaining decompositions of brain images with structured and sparse components (aka atoms). Sparsity is to be understood in the usual sense: the dictionary atoms are constrained to contain mostly zeros. This is imposed via an $\\ell_1$-norm constraint. By \"structured\", we mean that the atoms are piece-wise smooth and compact, thus making up blobs, as opposed to scattered patterns of activation. We propose to use a Sobolev (Laplacian) penalty to impose this type of structure. Combining the two penalties, we obtain decompositions that properly delineate brain structures from functional images. This non-trivially extends the online dictionary-learning  work of Mairal et al. (2010), at the price of only a factor of 2 or 3 on the overall running time. Just like the Mairal et al. (2010) reference method, the online nature of our proposed algorithm allows it to scale to arbitrarily sized datasets. Experiments on brain data show that our proposed method extracts structured and denoised dictionaries that are more intepretable and better capture inter-subject variability in small medium, and large-scale regimes alike, compared to state-of-the-art models",
    "volume": "main",
    "checked": true,
    "id": "3594dbce8e9547b2766d9c95650b515444cd42e4",
    "citation_count": 19
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/139f0874f2ded2e41b0393c4ac5644f7-Abstract.html": {
    "title": "Improving PAC Exploration Using the Median Of Means",
    "abstract": "We present the first application of the median of means in a PAC exploration algorithm for MDPs. Using the median of means allows us to significantly reduce the dependence of our bounds on the range of values that the value function can take, while introducing a dependence on the (potentially much smaller) variance of the Bellman operator. Additionally, our algorithm is the first algorithm with PAC bounds that can be applied to MDPs with unbounded rewards",
    "volume": "main",
    "checked": true,
    "id": "38c96f8e8432b4ad029b078d27d315313f75d5f0",
    "citation_count": 11
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/13f320e7b5ead1024ac95c3b208610db-Abstract.html": {
    "title": "Active Nearest-Neighbor Learning in Metric Spaces",
    "abstract": "We propose a pool-based non-parametric active learning algorithm for general metric spaces, called MArgin Regularized Metric Active Nearest Neighbor (MARMANN), which outputs a nearest-neighbor classifier. We give prediction error guarantees that depend on the noisy-margin properties of the input sample, and are competitive with those obtained by previously proposed passive learners. We prove that the label complexity of MARMANN is significantly lower than that of any passive learner with similar error guarantees. Our algorithm is based on a generalized sample compression scheme and a new label-efficient active model-selection procedure",
    "volume": "main",
    "checked": true,
    "id": "f4902d60dd70aaf41862615a1ed97a8a36be56a2",
    "citation_count": 34
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/140f6969d5213fd0ece03148e62e461e-Abstract.html": {
    "title": "Learning from Small Sample Sets by Combining Unsupervised Meta-Training with CNNs",
    "abstract": "This work explores CNNs for the recognition of novel categories from few examples. Inspired by the transferability properties of CNNs, we introduce an additional unsupervised meta-training stage that exposes multiple top layer units to a large amount of unlabeled real-world images. By encouraging these units to learn diverse sets of low-density separators across the unlabeled data, we capture a more generic, richer description of the visual world, which decouples these units from ties to a specific set of categories. We propose an unsupervised margin maximization that jointly estimates compact high-density regions and infers low-density separators. The low-density separator (LDS) modules can be plugged into any or all of the top layers of a standard CNN architecture. The resulting CNNs significantly improve the performance in scene classification, fine-grained recognition, and action recognition with small training samples",
    "volume": "main",
    "checked": true,
    "id": "774ae9c6b2a83c6891b5aeeb169cfd462d45f715",
    "citation_count": 68
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/144a3f71a03ab7c4f46f9656608efdb2-Abstract.html": {
    "title": "Learning Bayesian networks with ancestral constraints",
    "abstract": "We consider the problem of learning Bayesian networks optimally, when subject to background knowledge in the form of ancestral constraints. Our approach is based on a recently proposed framework for optimal structure learning based on non-decomposable scores, which is general enough to accommodate ancestral constraints. The proposed framework exploits oracles for learning structures using decomposable scores, which cannot accommodate ancestral constraints since they are non-decomposable. We show how to empower these oracles by passing them decomposable constraints that they can handle, which are inferred from ancestral constraints that they cannot handle. Empirically, we demonstrate that our approach can be orders-of-magnitude more efficient than alternative frameworks, such as those based on integer linear programming",
    "volume": "main",
    "checked": true,
    "id": "1364a54bb4017b83d168ff7b5f84060128c85461",
    "citation_count": 25
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/148510031349642de5ca0c544f31b2ef-Abstract.html": {
    "title": "Exponential expressivity in deep neural networks through transient chaos",
    "abstract": "We combine Riemannian geometry with the mean field theory of high dimensional chaos to study the nature of signal propagation in deep neural networks with random weights. Our results reveal a phase transition in the expressivity of random deep networks, with networks in the chaotic phase computing nonlinear functions whose global curvature grows exponentially with depth, but not with width. We prove that this generic class of random functions cannot be efficiently computed by any shallow network, going beyond prior work that restricts their analysis to single functions. Moreover, we formally quantify and demonstrate the long conjectured idea that deep networks can disentangle exponentially curved manifolds in input space into flat manifolds in hidden space.  Our theoretical framework for analyzing the expressive power of deep networks is broadly applicable and provides a basis for quantifying previously abstract notions about the geometry of deep functions",
    "volume": "main",
    "checked": true,
    "id": "6e997fec1412abb4b630d0e6d4df95813a01e093",
    "citation_count": 474
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/14cfdb59b5bda1fc245aadae15b1984a-Abstract.html": {
    "title": "MetaGrad: Multiple Learning Rates in Online Learning",
    "abstract": "In online convex optimization it is well known that certain subclasses of objective functions are much easier than arbitrary convex functions. We are interested in designing adaptive methods that can automatically get fast rates in as many such subclasses as possible, without any manual tuning. Previous adaptive methods are able to interpolate between strongly convex and general convex functions. We present a new method, MetaGrad, that adapts to a much broader class of functions, including exp-concave and strongly convex functions, but also various types of stochastic and non-stochastic functions without any curvature. For instance, MetaGrad can achieve logarithmic regret on the unregularized hinge loss, even though it has no curvature, if the data come from a favourable probability distribution. MetaGrad's main feature is that it simultaneously considers multiple learning rates. Unlike all previous methods with provable regret guarantees, however, its learning rates are not monotonically decreasing over time and are not tuned based on a theoretically derived bound on the regret. Instead, they are weighted directly proportional to their empirical performance on the data using a tilted exponential weights master algorithm",
    "volume": "main",
    "checked": true,
    "id": "3fcece80931e21003096b94454c9da42f585273e",
    "citation_count": 69
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/14d9e8007c9b41f57891c48e07c23f57-Abstract.html": {
    "title": "Learning under uncertainty: a comparison between R-W and Bayesian approach",
    "abstract": "Accurately differentiating between what are truly unpredictably random and systematic changes that occur at random can have profound effect on affect and cognition. To examine the underlying computational principles that guide different learning behavior in an uncertain environment, we compared an R-W model and a Bayesian approach in a visual search task with different volatility levels. Both R-W model and the Bayesian approach reflected an individual's estimation of the environmental volatility, and there is a strong correlation between the learning rate in R-W model and the belief of stationarity in the Bayesian approach in different volatility conditions. In a low volatility condition, R-W model indicates that learning rate positively correlates with lose-shift rate, but not choice optimality (inverted U shape). The Bayesian approach indicates that the belief of environmental stationarity positively correlates with choice optimality, but not lose-shift rate (inverted U shape). In addition, we showed that comparing to Expert learners, individuals with high lose-shift rate (sub-optimal learners) had significantly higher learning rate estimated from R-W model and lower belief of stationarity from the Bayesian model",
    "volume": "main",
    "checked": true,
    "id": "16f92d9a90c2c1b744db284673532c208ca0c41d",
    "citation_count": 2
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/1579779b98ce9edb98dd85606f2c119d-Abstract.html": {
    "title": "End-to-End Goal-Driven Web Navigation",
    "abstract": "We propose a goal-driven web navigation as a benchmark task for evaluating an agent with abilities to understand natural language and plan on partially observed environments. In this challenging task, an agent navigates through a website, which is represented as a graph consisting of web pages as nodes and hyperlinks as directed edges, to find a web page in which a query appears. The agent is required to have sophisticated high-level reasoning based on natural languages and efficient sequential decision-making capability to succeed. We release a software tool, called WebNav, that automatically transforms a website into this goal-driven web navigation task, and as an example, we make WikiNav, a dataset constructed from the English Wikipedia. We extensively evaluate different variants of neural net based artificial agents on WikiNav and observe that the proposed goal-driven web navigation well reflects the advances in models, making it a suitable benchmark for evaluating future progress. Furthermore, we extend the WikiNav with question-answer pairs from Jeopardy! and test the proposed agent based on recurrent neural networks against strong inverted index based search engines. The artificial agents trained on WikiNav outperforms the engined based approaches, demonstrating the capability of the proposed goal-driven navigation as a good proxy for measuring the progress in real-world tasks such as focused crawling and question-answering",
    "volume": "main",
    "checked": true,
    "id": "9b41a7f3e4a70eb5bc4113cbc6237b6120b7efc4",
    "citation_count": 28
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/158fc2ddd52ec2cf54d3c161f2dd6517-Abstract.html": {
    "title": "Higher-Order Factorization Machines",
    "abstract": "Factorization machines (FMs) are a supervised learning approach that can use second-order feature combinations even when the data is very high-dimensional. Unfortunately, despite increasing interest in FMs, there exists to date no efficient training algorithm for higher-order FMs (HOFMs). In this paper, we present the first generic yet efficient algorithms for training arbitrary-order HOFMs. We also present new variants of HOFMs with shared parameters, which greatly reduce model size and prediction times while maintaining similar accuracy.  We demonstrate the proposed approaches on four different link prediction tasks",
    "volume": "main",
    "checked": true,
    "id": "3f3b3dc86415ebda1043d2be55e75a29ffe2bd95",
    "citation_count": 149
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/15de21c670ae7c3f6f3f1f37029303c9-Abstract.html": {
    "title": "Efficient Second Order Online Learning by Sketching",
    "abstract": "We propose Sketched Online Newton (SON), an online second order learning algorithm that enjoys substantially improved regret guarantees for ill-conditioned data. SON is an enhanced version of the Online Newton Step, which, via sketching techniques enjoys a running time linear in the dimension and sketch size.  We further develop sparse forms of the sketching methods (such as Oja's rule), making the computation linear in the sparsity of features. Together, the algorithm eliminates all computational obstacles in previous second order online learning approaches",
    "volume": "main",
    "checked": true,
    "id": "329012f816422c57ed6ed28c1d8a358dd361d79d",
    "citation_count": 78
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/16026d60ff9b54410b3435b403afd226-Abstract.html": {
    "title": "Professor Forcing: A New Algorithm for Training Recurrent Networks",
    "abstract": "The Teacher Forcing algorithm trains recurrent networks by supplying observed sequence values as inputs during training and using the network’s own one-step-ahead predictions to do multi-step sampling. We introduce the Professor Forcing algorithm, which uses adversarial domain adaptation to encourage the dynamics of the recurrent network to be the same when training the network and when sampling from the network over multiple time steps. We apply Professor Forcing to language modeling, vocal synthesis on raw waveforms, handwriting generation, and image generation. Empirically we find that Professor Forcing acts as a regularizer, improving test likelihood on character level Penn Treebank and sequential MNIST. We also find that the model qualitatively improves samples, especially when sampling for a large number of time steps.  This is supported by human evaluation of sample quality.  Trade-offs between Professor Forcing and Scheduled Sampling are discussed. We produce T-SNEs showing that Professor Forcing successfully makes the dynamics of the network during training and sampling more similar",
    "volume": "main",
    "checked": true,
    "id": "db38edba294b7d2fd8ca3aad65721bd9dce32619",
    "citation_count": 461
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/1679091c5a880faf6fb5e6087eb1b2dc-Abstract.html": {
    "title": "Deep ADMM-Net for Compressive Sensing MRI",
    "abstract": "Compressive Sensing (CS) is an effective approach for fast Magnetic Resonance Imaging (MRI). It aims at reconstructing MR image from a small number of  under-sampled data in k-space, and accelerating the data acquisition in MRI.  To improve the current MRI system in reconstruction accuracy and computational speed,  in this paper, we propose a novel deep architecture, dubbed ADMM-Net.  ADMM-Net is defined over a data flow graph, which is derived from the iterative  procedures in Alternating Direction Method of Multipliers (ADMM) algorithm for optimizing a CS-based MRI model. In the training phase, all parameters of the net, e.g., image transforms, shrinkage functions, etc., are discriminatively trained end-to-end using L-BFGS algorithm. In the testing phase, it has computational overhead similar to ADMM but uses optimized parameters learned from the  training data for CS-based reconstruction task. Experiments on MRI image  reconstruction under different sampling ratios in k-space demonstrate that it significantly improves the baseline ADMM algorithm and achieves high reconstruction  accuracies with fast computational speed",
    "volume": "main",
    "checked": true,
    "id": "3435491a6ccfd5cab1f3378c10636e9130610b9e",
    "citation_count": 837
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/1714726c817af50457d810aae9d27a2e-Abstract.html": {
    "title": "Adaptive Averaging in Accelerated Descent Dynamics",
    "abstract": "We study accelerated descent dynamics for constrained convex optimization. This dynamics can be described naturally as a coupling of a dual variable accumulating gradients at a given rate $\\eta(t)$, and a primal variable obtained as the weighted average of the mirrored dual trajectory, with weights $w(t)$. Using a Lyapunov argument, we give sufficient conditions on $\\eta$ and $w$ to achieve a desired convergence rate. As an example, we show that the replicator dynamics (an example of mirror descent on the simplex) can be accelerated using a simple averaging scheme. We then propose an adaptive averaging heuristic which adaptively computes the weights to speed up the decrease of the Lyapunov function. We provide guarantees on adaptive averaging in continuous-time, prove that it preserves the quadratic convergence rate of accelerated first-order methods in discrete-time, and give numerical experiments to compare it with existing heuristics, such as adaptive restarting. The experiments indicate that adaptive averaging performs at least as well as adaptive restarting, with significant improvements in some cases",
    "volume": "main",
    "checked": true,
    "id": "3e54ec5dd108867180fde93bf9170d9b79e414b8",
    "citation_count": 14
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/1728efbda81692282ba642aafd57be3a-Abstract.html": {
    "title": "Dynamic Mode Decomposition with Reproducing Kernels for Koopman Spectral Analysis",
    "abstract": "A spectral analysis of the Koopman operator, which is an infinite dimensional linear operator on an observable, gives a (modal) description of the global behavior of a nonlinear dynamical system without any explicit prior knowledge of its governing equations. In this paper, we consider a spectral analysis of the Koopman operator in a reproducing kernel Hilbert space (RKHS). We propose a modal decomposition algorithm to perform the analysis using finite-length data sequences generated from a nonlinear system. The algorithm is in essence reduced to the calculation of a set of orthogonal bases for the Krylov matrix in RKHS and the eigendecomposition of the projection of the Koopman operator onto the subspace spanned by the bases. The algorithm returns a decomposition of the dynamics into a finite number of modes, and thus it can be thought of as a feature extraction procedure for a nonlinear dynamical system. Therefore, we further consider applications in machine learning using extracted features with the presented analysis. We illustrate the method on the applications using synthetic and real-world data",
    "volume": "main",
    "checked": true,
    "id": "aaa1989f559d12061456ac6ff730406cdb5138fe",
    "citation_count": 76
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/17ed8abedc255908be746d245e50263a-Abstract.html": {
    "title": "Total Variation Classes Beyond 1d: Minimax Rates, and the Limitations of Linear Smoothers",
    "abstract": "We consider the problem of estimating a function defined over $n$ locations on a $d$-dimensional grid (having all side lengths equal to $n^{1/d}$).  When the function is constrained to have discrete total variation bounded by $C_n$, we derive the minimax optimal (squared) $\\ell_2$ estimation error rate, parametrized by $n, C_n$. Total variation denoising, also known as the fused lasso, is seen to be rate optimal.  Several simpler estimators exist, such as Laplacian smoothing and Laplacian eigenmaps.  A natural question is: can these simpler estimators perform just as well?  We prove that these estimators, and more broadly all estimators given by linear transformations of the input data, are suboptimal over the class of functions with bounded variation. This extends fundamental findings of Donoho and Johnstone (1998) on 1-dimensional total variation spaces to higher dimensions.  The implication is that the computationally simpler methods cannot be used for such sophisticated denoising tasks, without sacrificing statistical accuracy. We also derive minimax rates for discrete Sobolev spaces over $d$-dimensional grids, which are, in some sense, smaller than the total variation function spaces.  Indeed, these are small enough spaces that linear estimators can be optimal---and a few well-known ones are, such as Laplacian smoothing and Laplacian eigenmaps, as we show.  Lastly, we investigate the adaptivity of the total variation denoiser to these smaller Sobolev function spaces",
    "volume": "main",
    "checked": true,
    "id": "7ae148184061f3c88e8b9b2f630f12624a19064e",
    "citation_count": 63
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/184260348236f9554fe9375772ff966e-Abstract.html": {
    "title": "Hardness of Online Sleeping Combinatorial Optimization Problems",
    "abstract": "We show that several online combinatorial optimization problems that admit efficient no-regret algorithms become computationally hard in the sleeping setting where a subset of actions becomes unavailable in each round. Specifically, we show that the sleeping versions of these problems are at least as hard as PAC learning DNF expressions, a long standing open problem. We show hardness for the sleeping versions of Online Shortest Paths, Online Minimum Spanning Tree, Online k-Subsets, Online k-Truncated Permutations, Online Minimum Cut, and Online Bipartite Matching. The hardness result for the sleeping version of the Online Shortest Paths problem resolves an open problem presented at COLT 2015 [Koolen et al., 2015]",
    "volume": "main",
    "checked": true,
    "id": "ebf015f3c43daed7647f1a6d20946f09df16ce84",
    "citation_count": 13
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/185c29dc24325934ee377cfda20e414c-Abstract.html": {
    "title": "Density Estimation via Discrepancy Based Adaptive Sequential Partition",
    "abstract": "Given $iid$ observations from an unknown continuous distribution defined on some domain $\\Omega$, we propose a nonparametric method to learn a piecewise constant function to approximate the underlying probability density function. Our density estimate is a piecewise constant function defined on a binary partition of $\\Omega$.  The key ingredient of the algorithm is to use discrepancy, a concept originates from Quasi Monte Carlo analysis, to control the partition process. The resulting algorithm is simple, efficient, and has  provable convergence rate. We demonstrate empirically its efficiency as a density estimation method. We also show how it can be utilized to find good initializations for k-means",
    "volume": "main",
    "checked": true,
    "id": "5a36ef0f588a4cfcca31a1a5803b21532f40bdf5",
    "citation_count": 23
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/186a157b2992e7daed3677ce8e9fe40f-Abstract.html": {
    "title": "Quantized Random Projections and Non-Linear Estimation of Cosine Similarity",
    "abstract": "Random projections constitute a simple, yet effective technique for dimensionality reduction with applications in learning and search problems. In the present paper, we consider the problem of estimating cosine similarities when the projected data undergo scalar quantization to $b$ bits. We here argue that the maximum likelihood estimator (MLE) is a principled approach to deal with the non-linearity resulting from quantization, and subsequently study its computational and statistical properties. A specific focus is on the on the trade-off between bit depth and the number of projections given a fixed budget of bits for storage or transmission. Along the way, we also touch upon the existence of a qualitative counterpart to the Johnson-Lindenstrauss lemma in the presence of quantization",
    "volume": "main",
    "checked": true,
    "id": "f90ff5c71e9ce877de465a5a3be78ab88b92a00e",
    "citation_count": 5
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/186fb23a33995d91ce3c2212189178c8-Abstract.html": {
    "title": "Algorithms and matching lower bounds for approximately-convex optimization",
    "abstract": "In recent years, a rapidly increasing number of applications in practice requires solving non-convex objectives, like training neural networks, learning graphical models, maximum likelihood estimation etc. Though simple heuristics such as gradient descent with very few modifications tend to work well, theoretical understanding is very weak.   We consider possibly the most natural class of non-convex functions where one could hope to obtain provable guarantees: functions that are ``approximately convex'', i.e. functions $\\tf: \\Real^d \\to \\Real$ for which there exists a \\emph{convex function} $f$ such that for all $x$, $|\\tf(x) - f(x)| \\le \\errnoise$ for a fixed value $\\errnoise$. We then want to minimize $\\tf$, i.e. output a point $\\tx$ such that $\\tf(\\tx) \\le \\min_{x} \\tf(x) + \\err$.   It is quite natural to conjecture that for fixed $\\err$, the problem gets harder for larger $\\errnoise$, however, the exact dependency of $\\err$ and $\\errnoise$ is not known. In this paper, we strengthen the known \\emph{information theoretic} lower bounds on the trade-off between $\\err$ and $\\errnoise$ substantially, and exhibit an algorithm that matches these lower bounds for a large class of convex bodies",
    "volume": "main",
    "checked": true,
    "id": "d24ee1c435558b7e88f2df10778f5aa762d89a50",
    "citation_count": 28
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/18d10dc6e666eab6de9215ae5b3d54df-Abstract.html": {
    "title": "The Parallel Knowledge Gradient Method for Batch Bayesian Optimization",
    "abstract": "In many applications of black-box optimization, one can evaluate multiple points simultaneously, e.g. when evaluating the performances of several different neural network architectures in a parallel computing environment.  In this paper, we develop a novel batch Bayesian optimization algorithm --- the parallel knowledge gradient method. By construction, this method provides the one-step Bayes optimal batch of points to sample. We provide an efficient strategy for computing this Bayes-optimal batch of points, and we demonstrate that the parallel knowledge gradient method finds global optima significantly faster than previous batch Bayesian optimization algorithms on both synthetic test functions and when tuning hyperparameters of practical machine learning algorithms, especially when function evaluations are noisy",
    "volume": "main",
    "checked": true,
    "id": "b3e321ce08da3044340101b842338798a6a987c1",
    "citation_count": 170
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/1a0a283bfe7c549dee6c638a05200e32-Abstract.html": {
    "title": "Edge-exchangeable graphs and sparsity",
    "abstract": "Many popular network models rely on the assumption of (vertex) exchangeability, in which the distribution of the graph is invariant to relabelings of the vertices. However, the Aldous-Hoover theorem guarantees that these graphs are dense or empty with probability one, whereas many real-world graphs are sparse. We present an alternative notion of exchangeability for random graphs, which we call edge exchangeability, in which the distribution of a graph sequence is invariant to the order of the edges. We demonstrate that edge-exchangeable models, unlike models that are traditionally vertex exchangeable, can exhibit sparsity. To do so, we outline a general framework for graph generative models; by contrast to the pioneering work of Caron and Fox (2015), models within our framework are stationary across steps of the graph sequence. In particular, our model grows the graph by instantiating more latent atoms of a single random measure as the dataset size increases, rather than adding new atoms to the measure",
    "volume": "main",
    "checked": true,
    "id": "f9250c0deb3a7176549b13ebd205efb0a3e57b13",
    "citation_count": 73
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/1aa48fc4880bb0c9b8a3bf979d3b917e-Abstract.html": {
    "title": "Stochastic Variance Reduction Methods for Saddle-Point Problems",
    "abstract": "We consider convex-concave saddle-point problems where the objective functions may be split in many components, and extend recent stochastic variance reduction methods (such as SVRG or SAGA) to provide the first  large-scale linearly convergent algorithms for this class of problems which are common in machine learning. While the algorithmic extension is straightforward, it comes with challenges and opportunities: (a) the convex minimization analysis does not apply and we use the notion of monotone operators to prove convergence, showing in particular that the same algorithm applies to a larger class of problems, such as variational inequalities,  (b) there are two notions of splits, in terms of functions, or in terms of partial derivatives, (c) the split does need to be done with convex-concave terms, (d) non-uniform sampling is key to an efficient algorithm, both in theory and practice, and (e)  these incremental algorithms can be easily accelerated using a simple extension of the \"catalyst\" framework,  leading to an algorithm which is always superior to accelerated batch algorithms",
    "volume": "main",
    "checked": true,
    "id": "75b27ad55d57428d0c0590398534569e8f8959ec",
    "citation_count": 167
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/1abb1e1ea5f481b589da52303b091cbb-Abstract.html": {
    "title": "A Probabilistic Model of Social Decision Making based on Reward Maximization",
    "abstract": "A fundamental problem in cognitive neuroscience is how humans make decisions, act, and behave in relation to other humans. Here we adopt the hypothesis that when we are in an interactive social setting, our brains perform Bayesian inference of the intentions and cooperativeness of others using probabilistic representations. We employ the framework of partially observable Markov decision processes (POMDPs) to model human decision making in a social context, focusing specifically on the volunteer's dilemma in a version of the classic Public Goods Game. We show that the POMDP model explains both the behavior of subjects as well as neural activity recorded using fMRI during the game. The decisions of subjects can be modeled across all trials using two interpretable parameters. Furthermore, the expected reward predicted by the model for each subject was correlated with the activation of brain areas related to reward expectation in social interactions. Our results suggest a probabilistic basis for human social decision making within the framework of expected reward maximization",
    "volume": "main",
    "checked": true,
    "id": "3c433fcfc81f0a84081f5de4101e42e485b08999",
    "citation_count": 11
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/1ce927f875864094e3906a4a0b5ece68-Abstract.html": {
    "title": "Bootstrap Model Aggregation for Distributed Statistical Learning",
    "abstract": "In distributed, or privacy-preserving learning, we are often given a set of probabilistic models estimated from different local repositories, and asked to combine them into a single model that gives efficient statistical estimation. A simple method is to linearly average the parameters of the local models, which, however, tends to be degenerate or not applicable on non-convex models, or models with different parameter dimensions. One more practical strategy is to generate bootstrap samples from the local models, and then learn a joint model based on the combined bootstrap set. Unfortunately, the bootstrap procedure introduces additional noise and can significantly deteriorate the performance. In this work, we propose two variance reduction methods to correct the bootstrap noise, including a weighted M-estimator that is both statistically efficient and practically powerful. Both theoretical and empirical analysis is provided to demonstrate our methods",
    "volume": "main",
    "checked": true,
    "id": "bf1b3461b137433465eb7b04d7728e0b40af8cd8",
    "citation_count": 8
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/1d94108e907bb8311d8802b48fd54b4a-Abstract.html": {
    "title": "Unsupervised Learning of 3D Structure from Images",
    "abstract": "A key goal of computer vision is to recover the underlying 3D structure that gives rise to 2D observations of the world. If endowed with 3D understanding, agents can abstract away from the complexity of the rendering process to form stable, disentangled representations of scene elements. In this paper we learn strong deep generative models of 3D structures, and recover these structures from 2D images via probabilistic inference. We demonstrate high-quality samples and report log-likelihoods on several datasets, including ShapeNet, and establish the first benchmarks in the literature. We also show how these models and their inference networks can be trained jointly, end-to-end, and directly from 2D images without any use of ground-truth 3D labels. This demonstrates for the first time the feasibility of learning to infer 3D representations of the world in a purely unsupervised manner",
    "volume": "main",
    "checked": true,
    "id": "09d2f3467f171f253499773d8c1d71f21ac4f983",
    "citation_count": 365
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/1e8c391abfde9abea82d75a2d60278d4-Abstract.html": {
    "title": "beta-risk: a New Surrogate Risk for Learning from Weakly Labeled Data",
    "abstract": "During the past few years, the machine learning community has paid attention to developping new methods for learning from weakly labeled data. This field covers different settings like semi-supervised learning, learning with label proportions, multi-instance learning, noise-tolerant learning, etc. This paper presents a generic framework to deal with these weakly labeled scenarios. We introduce the beta-risk as a generalized formulation of the standard empirical risk based on surrogate margin-based loss functions. This risk allows us to express the reliability on the labels and to derive different kinds of learning algorithms. We specifically focus on SVMs and propose a soft margin beta-svm algorithm  which behaves better that the state of the art",
    "volume": "main",
    "checked": true,
    "id": "0c27fd237306f9a65a300cdf1402e0b34a68be55",
    "citation_count": 6
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/1f34004ebcb05f9acda6016d5cc52d5e-Abstract.html": {
    "title": "Learning Supervised PageRank with Gradient-Based and Gradient-Free Optimization Methods",
    "abstract": "In this paper, we consider a non-convex loss-minimization problem of learning Supervised PageRank models, which can account for features of nodes and edges. We propose gradient-based and random gradient-free methods to solve this problem. Our algorithms are based on the concept of an inexact oracle and unlike the state-of-the-art gradient-based method we manage to provide theoretically the convergence rate guarantees for both of them. Finally, we compare the performance of the proposed optimization methods with the state of the art applied to a ranking task",
    "volume": "main",
    "checked": true,
    "id": "0329dbd27f51f1e0586baa122f860efef6779d5b",
    "citation_count": 50
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/1f4477bad7af3616c1f933a02bfabe4e-Abstract.html": {
    "title": "Globally Optimal Training of Generalized Polynomial Neural Networks with Nonlinear Spectral Methods",
    "abstract": "The optimization problem behind neural networks is highly non-convex. Training with stochastic gradient descent and variants requires careful parameter tuning and provides no guarantee to achieve the global optimum. In contrast we show under quite weak assumptions on the data that a particular class of feedforward  neural networks can be trained globally optimal with a linear convergence rate. Up to our knowledge this is the first practically feasible method which achieves such a guarantee. While the method can in principle be applied to deep networks, we restrict ourselves for simplicity in this paper to one- and two hidden layer networks. Our experiments confirms that these models are already rich enough to achieve good performance on a series of real-world datasets",
    "volume": "main",
    "checked": true,
    "id": "9e850a82adbec3941b4ff2d4ededbc47b45285b5",
    "citation_count": 30
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/1f50893f80d6830d62765ffad7721742-Abstract.html": {
    "title": "Optimal Black-Box Reductions Between Optimization Objectives",
    "abstract": "The diverse world of machine learning applications has given rise to a plethora of algorithms and optimization methods, finely tuned to the specific regression or classification task at hand.  We reduce the complexity of algorithm design for machine learning by reductions:  we develop reductions that take a method developed for one setting and apply it to the entire spectrum of smoothness and strong-convexity in applications.  Furthermore, unlike existing results, our new reductions are OPTIMAL and more PRACTICAL. We show how these new reductions give rise to new and faster running times on training linear classifiers for various families of loss functions, and conclude with experiments showing their successes also in practice",
    "volume": "main",
    "checked": true,
    "id": "6bbf543c5b2bab13bb1808c23d2227f0a6416240",
    "citation_count": 83
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/208e43f0e45c4c78cafadb83d2888cb6-Abstract.html": {
    "title": "Sequential Neural Models with Stochastic Layers",
    "abstract": "How can we efficiently propagate uncertainty in a latent state representation with recurrent neural networks?  This paper introduces stochastic recurrent neural networks which glue a deterministic recurrent neural network and a state space model together to form a stochastic and sequential neural generative model. The clear separation of deterministic and stochastic layers allows a structured variational inference network to track the factorization of the model’s posterior distribution. By retaining both the nonlinear recursive structure of a recurrent neural network and averaging over the uncertainty in a latent path, like a state space model, we improve the state of the art results on the Blizzard and TIMIT speech modeling data sets by a large margin, while achieving comparable performances to competing methods on polyphonic music modeling",
    "volume": "main",
    "checked": true,
    "id": "f05d8eacc1469439bb04f2768fd68878c982e636",
    "citation_count": 336
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/20c9f5700da1088260df60fcc5df2b53-Abstract.html": {
    "title": "Iterative Refinement of the Approximate Posterior for Directed Belief Networks",
    "abstract": "Variational methods that rely on a recognition network to approximate the posterior of directed graphical models offer better inference and learning than previous methods. Recent advances that exploit the capacity and flexibility in this approach have expanded what kinds of models can be trained. However, as a proposal for the posterior, the capacity of the recognition network is limited, which can constrain the representational power of the generative model and increase the variance of Monte Carlo estimates. To address these issues, we introduce an iterative refinement procedure for improving the approximate posterior of the recognition network and show that training with the refined posterior is competitive with state-of-the-art methods. The advantages of refinement are further evident in an increased effective sample size, which implies a lower variance of gradient estimates",
    "volume": "main",
    "checked": true,
    "id": "dfd80cf0e4f0c5e3030da2fe6b2f408bb226075b",
    "citation_count": 35
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/20d135f0f28185b84a4cf7aa51f29500-Abstract.html": {
    "title": "Stochastic Multiple Choice Learning for Training Diverse Deep Ensembles",
    "abstract": "Many practical perception systems exist within larger processes which often include interactions with users or additional components that are capable of evaluating the quality of predicted solutions. In these contexts, it is beneficial to provide these oracle mechanisms with multiple highly likely hypotheses rather than a single prediction. In this work, we pose the task of producing multiple outputs as a learning problem over an ensemble of deep networks -- introducing a novel stochastic gradient descent based approach to minimize the loss with respect to an oracle. Our method is simple to implement, agnostic to both architecture and loss function, and parameter-free. Our approach achieves lower oracle error compared to existing methods on a wide range of tasks and deep architectures. We also show qualitatively that solutions produced from our approach often provide interpretable representations of task ambiguity",
    "volume": "main",
    "checked": true,
    "id": "c659f4ca9b7e75abf77133c54ad830ed4ebc0ff0",
    "citation_count": 141
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/228499b55310264a8ea0e27b6e7c6ab6-Abstract.html": {
    "title": "Learning shape correspondence with anisotropic convolutional neural networks",
    "abstract": "Convolutional neural networks have achieved extraordinary results in many computer vision and pattern recognition applications; however, their adoption in the computer graphics and geometry processing communities is limited due to the non-Euclidean structure of their data.  In this paper, we propose Anisotropic Convolutional Neural Network (ACNN), a generalization of classical CNNs to non-Euclidean domains, where classical convolutions are replaced by projections over a set of oriented anisotropic diffusion kernels. We use ACNNs to effectively learn intrinsic dense correspondences between deformable shapes, a fundamental problem in geometry processing, arising in a wide variety of applications. We tested ACNNs performance in very challenging settings, achieving state-of-the-art results on some of the most difficult recent correspondence benchmarks",
    "volume": "main",
    "checked": true,
    "id": "ba8d37f2e98d70f917d0d3a49c387cef6867e65e",
    "citation_count": 450
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/22ac3c5a5bf0b520d281c122d1490650-Abstract.html": {
    "title": "Learning Tree Structured Potential Games",
    "abstract": "Many real phenomena, including behaviors, involve strategic interactions that can be learned from data. We focus on learning tree structured potential games where equilibria are represented by local maxima of an underlying potential function. We cast the learning problem within a max margin setting and show that the problem is NP-hard even when the strategic interactions form a tree. We develop a variant of dual decomposition to estimate the underlying game and demonstrate with synthetic and real decision/voting data that the game theoretic perspective (carving out local maxima) enables meaningful recovery",
    "volume": "main",
    "checked": true,
    "id": "98d473f95b9bed40a6cfd6e3b591fd9322680f17",
    "citation_count": 15
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/231141b34c82aa95e48810a9d1b33a79-Abstract.html": {
    "title": "RETAIN: An Interpretable Predictive Model for Healthcare using Reverse Time Attention Mechanism",
    "abstract": "Accuracy and interpretability are two dominant features of successful predictive models. Typically, a choice must be made in favor of complex black box models such as recurrent neural networks (RNN) for accuracy versus less accurate but more interpretable traditional models such as logistic regression. This tradeoff poses challenges in medicine where both accuracy and interpretability are important. We addressed this challenge by developing the REverse Time AttentIoN model (RETAIN) for application to Electronic Health Records (EHR) data. RETAIN achieves high accuracy while remaining clinically interpretable and is based on a two-level neural attention model that detects influential past visits and significant clinical variables within those visits (e.g. key diagnoses). RETAIN mimics physician practice by attending the EHR data in a reverse time order so that recent clinical visits are likely to receive higher attention. RETAIN was tested on a large health system EHR dataset with 14 million visits completed by 263K patients over an 8 year period and demonstrated predictive accuracy and computational scalability comparable to state-of-the-art methods such as RNN, and ease of interpretability comparable to traditional models",
    "volume": "main",
    "checked": true,
    "id": "e8e9125704edbcf73999f2f452fe4a701163d6b6",
    "citation_count": 849
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/2387337ba1e0b0249ba90f55b2ba2521-Abstract.html": {
    "title": "PAC Reinforcement Learning with Rich Observations",
    "abstract": "We propose and study a new model for reinforcement learning with rich observations, generalizing contextual bandits to sequential decision making.  These models require an agent to take actions based on observations (features) with the goal of achieving long-term performance competitive with a large set of policies.  To avoid barriers to sample-efficient learning associated with large observation spaces and general POMDPs, we focus on problems that can be summarized by a small number of hidden states and have long-term rewards that are predictable by a reactive function class.  In this setting, we design and analyze a new reinforcement learning algorithm, Least Squares Value Elimination by Exploration. We prove that the algorithm learns near optimal behavior after a number of episodes that is polynomial in all relevant parameters, logarithmic in the number of policies, and independent of the size of the observation space. Our result provides theoretical justification for reinforcement learning with function approximation",
    "volume": "main",
    "checked": true,
    "id": "ae778db334e4e3a6f1d3ffceac70ca40bf164482",
    "citation_count": 118
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/23ad3e314e2a2b43b4c720507cec0723-Abstract.html": {
    "title": "Generative Shape Models: Joint Text Recognition and Segmentation with Very Little Training Data",
    "abstract": "We demonstrate that a generative model for object shapes can achieve state of the art results on challenging scene text recognition tasks, and with orders of magnitude fewer training images than required for competing discriminative methods. In addition to transcribing text from challenging images, our method performs fine-grained instance segmentation of characters. We show that our model is more robust to both affine transformations and non-affine deformations compared to previous approaches",
    "volume": "main",
    "checked": true,
    "id": "e9fe3e026ff06de5b226d5d8f4d9cdca627c5b1b",
    "citation_count": 9
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/23c97e9cb93576e45d2feaf00d0e8502-Abstract.html": {
    "title": "Probabilistic Linear Multistep Methods",
    "abstract": "We present a derivation and theoretical investigation of the Adams-Bashforth and Adams-Moulton family of linear multistep methods for solving ordinary differential equations, starting from a Gaussian process (GP) framework. In the limit, this formulation coincides with the classical deterministic methods, which have been used as higher-order initial value problem solvers for over a century. Furthermore, the natural probabilistic framework provided by the GP formulation allows us to derive probabilistic versions of these methods, in the spirit of a number of other probabilistic ODE solvers presented in the recent literature. In contrast to higher-order Runge-Kutta methods, which require multiple intermediate function evaluations per step, Adams family methods make use of previous function evaluations, so that increased accuracy arising from a higher-order multistep approach comes at very little additional computational cost. We show that through a careful choice of covariance function for the GP, the posterior mean and standard deviation over the numerical solution can be made to exactly coincide with the value given by the deterministic method and its local truncation error respectively. We provide a rigorous proof of the convergence of these new methods, as well as an empirical investigation (up to fifth order) demonstrating their convergence rates in practice",
    "volume": "main",
    "checked": true,
    "id": "91eba9bd8084fa570e92b5d29ca9b6e95e3a69c9",
    "citation_count": 29
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/2421fcb1263b9530df88f7f002e78ea5-Abstract.html": {
    "title": "Computational and Statistical Tradeoffs in Learning to Rank",
    "abstract": "For massive and heterogeneous modern  data sets, it is of fundamental interest to provide guarantees on the accuracy of estimation when computational resources are limited. In the application of learning to rank, we provide a hierarchy of rank-breaking mechanisms ordered by the complexity in thus generated sketch of the data. This allows the number of data points collected to be gracefully traded off against computational resources available, while guaranteeing the desired level of accuracy. Theoretical guarantees on the proposed generalized rank-breaking implicitly provide such trade-offs, which can be explicitly characterized under certain canonical scenarios on the structure of the data",
    "volume": "main",
    "checked": true,
    "id": "78821e86799c1c360cc49b029d53bf88282eaa97",
    "citation_count": 14
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/2451041557a22145b3701b0184109cab-Abstract.html": {
    "title": "Split LBI: An Iterative Regularization Path with Structural Sparsity",
    "abstract": "An iterative regularization path with structural sparsity is proposed in this paper based on variable splitting and the Linearized Bregman Iteration, hence called \\emph{Split LBI}. Despite its simplicity, Split LBI outperforms the popular generalized Lasso in both theory and experiments. A theory of path consistency is presented that equipped with a proper early stopping, Split LBI may achieve model selection consistency under a family of Irrepresentable Conditions which can be weaker than the necessary and sufficient condition for generalized Lasso. Furthermore, some $\\ell_2$ error bounds are also given at the minimax optimal rates. The utility and benefit of the algorithm are illustrated by applications on both traditional image denoising and a novel example on partial order ranking",
    "volume": "main",
    "checked": true,
    "id": "e8971a0ab91a2e443e00b85096f82e6dd24098bf",
    "citation_count": 18
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/2596a54cdbb555cfd09cd5d991da0f55-Abstract.html": {
    "title": "Incremental Variational Sparse Gaussian Process Regression",
    "abstract": "Recent work on scaling up Gaussian process regression (GPR) to large datasets has primarily focused on sparse GPR, which leverages a small set of basis functions to approximate the full Gaussian process during inference.  However, the majority of these approaches are batch methods that operate on the entire training dataset at once, precluding the use of datasets that are streaming or too large to fit into memory. Although previous work has considered incrementally solving variational sparse GPR, most algorithms fail to update the basis functions and therefore perform suboptimally. We propose a novel incremental learning algorithm for variational sparse GPR based on stochastic mirror ascent of probability densities in reproducing kernel Hilbert space. This new formulation allows our algorithm to update basis functions online in accordance with the manifold structure of probability densities for fast convergence. We conduct several experiments and show that our proposed approach achieves better empirical performance in terms of prediction error than  the recent state-of-the-art incremental solutions to variational sparse GPR",
    "volume": "main",
    "checked": true,
    "id": "c4c67f66438cb08c9c527b80be34a67a181252db",
    "citation_count": 31
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/25ddc0f8c9d3e22e03d3076f98d83cb2-Abstract.html": {
    "title": "Sublinear Time Orthogonal Tensor Decomposition",
    "abstract": "A recent work (Wang et. al., NIPS 2015) gives the fastest known algorithms for orthogonal tensor decomposition with provable guarantees. Their algorithm is based on computing sketches of the input tensor, which requires reading the entire input. We show in a number of cases one can achieve the same theoretical guarantees in sublinear time, i.e., even without reading most of the input tensor. Instead of using sketches to estimate inner products in tensor decomposition algorithms, we use importance sampling. To achieve sublinear time, we need to know the norms of tensor slices, and we show how to do this in a number of important cases. For symmetric tensors $ T = \\sum_{i=1}^k \\lambda_i u_i^{\\otimes p}$ with $\\lambda_i > 0$ for all i, we estimate such norms in sublinear time whenever p is even. For the important case of p = 3 and small values of k, we can also estimate such norms. For asymmetric tensors sublinear time is not possible in general, but we show if the tensor slice norms are just slightly below $\\| T \\|_F$ then sublinear time is again possible. One of the main strengths of our work is empirical - in a number of cases our algorithm is orders of magnitude faster than existing methods with the same accuracy",
    "volume": "main",
    "checked": true,
    "id": "0791b4dd7070a1c8b48396323e21ebc2f7330b7b",
    "citation_count": 37
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/26f5bd4aa64fdadf96152ca6e6408068-Abstract.html": {
    "title": "Mapping Estimation for Discrete Optimal Transport",
    "abstract": "We are interested in the computation of the transport map of an Optimal Transport problem. Most of the computational approaches of Optimal Transport use the Kantorovich relaxation of the problem to learn a probabilistic coupling $\\mgamma$ but do not address the problem of learning the underlying transport map $\\funcT$ linked to the original Monge problem. Consequently, it lowers the potential usage of such methods in contexts where out-of-samples computations are mandatory. In this paper we propose a new way to jointly learn the coupling and an approximation of the transport map. We use a jointly convex formulation which can be efficiently optimized. Additionally, jointly learning the coupling and the transport map allows to smooth the result of the Optimal Transport and generalize it to out-of-samples examples. Empirically, we show the interest and the relevance of our method in two tasks: domain adaptation and image editing",
    "volume": "main",
    "checked": true,
    "id": "7a9e4178c1884a0d2608626435d0f3d40ef2fca6",
    "citation_count": 90
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/277a78fc05c8864a170e9a56ceeabc4c-Abstract.html": {
    "title": "Greedy Feature Construction",
    "abstract": "We present an effective method for supervised feature construction. The main goal of the approach is to construct a feature representation for which a set of linear hypotheses is of sufficient capacity -- large enough to contain a satisfactory solution to the considered problem and small enough to allow good generalization from a small number of training examples. We achieve this goal with a greedy procedure that constructs features by empirically fitting squared error residuals. The proposed constructive procedure is consistent and can output a rich set of features. The effectiveness of the approach is evaluated empirically by fitting a linear ridge regression model in the constructed feature space and our empirical results indicate a superior performance of our approach over competing methods",
    "volume": "main",
    "checked": true,
    "id": "68b3f7d6120d251fbf72503fc535e994ff2c1607",
    "citation_count": 11
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/2823f4797102ce1a1aec05359cc16dd9-Abstract.html": {
    "title": "Dynamic Network Surgery for Efficient DNNs",
    "abstract": "Deep learning has become a ubiquitous technology to improve machine intelligence. However, most of the existing deep models are structurally very complex, making them difficult to be deployed on the mobile platforms with limited computational power. In this paper, we propose a novel network compression method called dynamic network surgery, which can remarkably reduce the network complexity by making on-the-fly connection pruning. Unlike the previous methods which accomplish this task in a greedy way, we properly incorporate connection splicing into the whole process to avoid incorrect pruning and make it as a continual network maintenance. The effectiveness of our method is proved with experiments. Without any accuracy loss, our method can efficiently compress the number of parameters in LeNet-5 and AlexNet by a factor of $\\bm{108}\\times$ and $\\bm{17.7}\\times$ respectively, proving that it outperforms the recent pruning method by considerable margins. Code and some models are available at https://github.com/yiwenguo/Dynamic-Network-Surgery",
    "volume": "main",
    "checked": true,
    "id": "c220cdbcec6f92e4bc0f58c5fa6c1183105be1f9",
    "citation_count": 865
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/286674e3082feb7e5afb92777e48821f-Abstract.html": {
    "title": "Graph Clustering: Block-models and model free results",
    "abstract": "Clustering graphs under the Stochastic Block Model (SBM) and extensions are well studied. Guarantees of correctness exist under the assumption that the data is sampled from a model. In this paper, we propose a framework, in which we obtain \"correctness\" guarantees without assuming the data comes from a model. The guarantees we obtain depend instead on the statistics of the data that can be checked. We also show that this framework ties in with the existing model-based framework, and that we can exploit results in model-based recovery, as well as strengthen the results existing in that area of research",
    "volume": "main",
    "checked": true,
    "id": "c8d88a209a1e7fe1b7567f0c454b8269ece9c2bf",
    "citation_count": 6
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/289dff07669d7a23de0ef88d2f7129e7-Abstract.html": {
    "title": "CMA-ES with Optimal Covariance Update and Storage Complexity",
    "abstract": "The covariance matrix adaptation evolution strategy (CMA-ES) is arguably one of the most powerful real-valued derivative-free optimization algorithms, finding many applications in machine learning. The CMA-ES is a Monte Carlo method, sampling from a sequence of multi-variate Gaussian distributions. Given the function values at the sampled points, updating and storing the covariance matrix dominates the time and space complexity in each iteration of the algorithm. We propose a numerically stable quadratic-time covariance matrix update scheme with minimal memory requirements based on maintaining triangular Cholesky factors. This requires a modification of the cumulative step-size adaption (CSA) mechanism in the CMA-ES, in which we replace the inverse of the square root of the covariance matrix by the inverse of the triangular Cholesky factor. Because the triangular Cholesky factor changes smoothly with the matrix square root, this modification does not change the behavior of the CMA-ES in terms of required objective function evaluations as verified empirically. Thus, the described algorithm can and should replace the standard CMA-ES if updating and storing the covariance matrix matters",
    "volume": "main",
    "checked": true,
    "id": "c4d795bfc1f153176a9e6a20bbad5b1cb03c52df",
    "citation_count": 31
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/28b60a16b55fd531047c0c958ce14b95-Abstract.html": {
    "title": "Feature selection in functional data classification with recursive maxima hunting",
    "abstract": "Dimensionality reduction is one of the key issues in the design of effective machine learning methods for automatic induction.  In this work, we introduce recursive maxima hunting (RMH) for variable selection in classification problems with functional data. In this context, variable selection techniques are especially attractive because they reduce the dimensionality, facilitate the interpretation and can improve the accuracy of the predictive models. The method, which is a recursive extension of maxima hunting (MH), performs variable selection by identifying the maxima of a relevance function, which measures the strength of the correlation of the predictor functional variable with the class label. At each stage, the information associated with the selected variable is removed by subtracting the conditional expectation of the process. The results of an extensive empirical evaluation are used to illustrate that, in the problems investigated, RMH has comparable or higher predictive accuracy than standard simensionality reduction techniques, such as PCA and PLS, and state-of-the-art feature selection methods for functional data, such as maxima hunting",
    "volume": "main",
    "checked": true,
    "id": "af3bd8e60a997c3407d19f795e11e0aedb3333ae",
    "citation_count": 3
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/28e209b61a52482a0ae1cb9f5959c792-Abstract.html": {
    "title": "Cyclades: Conflict-free Asynchronous Machine Learning",
    "abstract": "We present Cyclades, a general framework for parallelizing stochastic optimization algorithms in a shared memory setting. Cyclades is asynchronous during model updates, and requires no memory locking mechanisms, similar to Hogwild!-type algorithms. Unlike Hogwild!, Cyclades introduces no conflicts during parallel execution, and offers a black-box analysis for provable speedups across a large family of algorithms.  Due to its inherent cache locality and conflict-free nature,  our multi-core implementation of Cyclades consistently outperforms Hogwild!-type algorithms on sufficiently sparse datasets, leading to up to 40% speedup gains compared to Hogwild!, and up to 5\\times gains over asynchronous implementations of variance reduction algorithms",
    "volume": "main",
    "checked": true,
    "id": "db77ef399dbf5d71b70119d0842885efd906d282",
    "citation_count": 61
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/291597a100aadd814d197af4f4bab3a7-Abstract.html": {
    "title": "Proximal Stochastic Methods for Nonsmooth Nonconvex Finite-Sum Optimization",
    "abstract": "We analyze stochastic algorithms for optimizing nonconvex, nonsmooth finite-sum problems, where the nonsmooth part is convex.  Surprisingly, unlike the smooth case, our knowledge of this fundamental problem is very limited. For example, it is not known whether the proximal stochastic gradient method with constant minibatch converges to a stationary point. To tackle this issue, we develop fast stochastic algorithms that provably converge to a stationary point for constant minibatches. Furthermore, using a variant of these algorithms, we obtain provably faster convergence than batch proximal gradient descent. Our results are based on the recent variance reduction techniques for convex optimization but with a novel analysis for handling nonconvex and nonsmooth functions. We also prove global linear convergence rate for an interesting subclass of nonsmooth nonconvex functions, which subsumes several recent works",
    "volume": "main",
    "checked": true,
    "id": "8fc759bc88c38e951e65223ee771479fcdcf0618",
    "citation_count": 157
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/296472c9542ad4d4788d543508116cbc-Abstract.html": {
    "title": "Spectral Learning of Dynamic Systems from Nonequilibrium Data",
    "abstract": "Observable operator models (OOMs) and related models are one of the most important and powerful tools for modeling and analyzing stochastic systems. They exactly describe dynamics of finite-rank systems and can be efficiently and consistently estimated through spectral learning under the assumption of identically distributed data. In this paper, we investigate the properties of spectral learning without this assumption due to the requirements of analyzing large-time scale systems, and show that the equilibrium dynamics of a system can be extracted from nonequilibrium observation data by imposing an equilibrium constraint. In addition, we propose a binless extension of spectral learning for continuous data. In comparison with the other continuous-valued spectral algorithms, the binless algorithm can achieve consistent estimation of equilibrium dynamics with only linear complexity",
    "volume": "main",
    "checked": true,
    "id": "37ce5ed363e4d1284e430d9fc31fa1e669b1c15c",
    "citation_count": 4
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/299570476c6f0309545110c592b6a63b-Abstract.html": {
    "title": "Dimension-Free Iteration Complexity of Finite Sum Optimization Problems",
    "abstract": "Many canonical machine learning problems boil down to a convex optimization problem with a finite sum structure. However, whereas much progress has been made in developing faster algorithms for this setting, the inherent limitations of these problems are not satisfactorily addressed by existing lower bounds. Indeed, current bounds focus on first-order optimization algorithms, and only apply in the often unrealistic regime where the number of iterations is less than $\\cO(d/n)$ (where $d$ is the dimension and $n$ is the number of samples). In this work, we extend the framework of Arjevani et al. \\cite{arjevani2015lower,arjevani2016iteration} to provide new lower bounds, which are dimension-free, and go beyond the assumptions of current bounds, thereby covering standard finite sum optimization methods, e.g., SAG, SAGA, SVRG, SDCA without duality, as well as stochastic coordinate-descent methods, such as SDCA and accelerated proximal SDCA",
    "volume": "main",
    "checked": true,
    "id": "2a357eb4e88669bee3c7f313433041109ff87d8a",
    "citation_count": 27
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/299a23a2291e2126b91d54f3601ec162-Abstract.html": {
    "title": "Hierarchical Object Representation for Open-Ended Object Category Learning and Recognition",
    "abstract": "Most robots lack the ability to learn new objects from past experiences. To migrate a robot to a new environment one must often completely re-generate the knowledge- base that it is running with. Since in open-ended domains the set of categories to be learned is not predefined, it is not feasible to assume that one can pre-program all object categories required by robots. Therefore, autonomous robots must have the ability to continuously execute learning and recognition in a concurrent and interleaved fashion. This paper proposes an open-ended 3D object recognition system which concurrently learns both the object categories and the statistical features for encoding objects. In particular, we propose an extension of Latent Dirichlet Allocation to learn structural semantic features (i.e. topics) from low-level feature co-occurrences for each category independently. Moreover, topics in each category are discovered in an unsupervised fashion and are updated incrementally using new object views. The approach contains similarities with the organization of the visual cortex and builds a hierarchy of increasingly sophisticated representations. Results show the fulfilling performance of this approach on different types of objects. Moreover, this system demonstrates the capability of learning from few training examples and competes with state-of-the-art systems",
    "volume": "main",
    "checked": true,
    "id": "7a8fdad6c4832010bd0b5fcf265614b75b0c3be4",
    "citation_count": 19
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/299fb2142d7de959380f91c01c3a293c-Abstract.html": {
    "title": "Active Learning with Oracle Epiphany",
    "abstract": "We present a theoretical analysis of active learning with more realistic interactions with human oracles. Previous empirical studies have shown oracles abstaining on difficult queries until accumulating enough information to make label decisions. We formalize this phenomenon with an “oracle epiphany model” and analyze active learning query complexity under such oracles for both the realizable and the agnos- tic cases. Our analysis shows that active learning is possible with oracle epiphany, but incurs an additional cost depending on when the epiphany happens. Our results suggest new, principled active learning approaches with realistic oracles",
    "volume": "main",
    "checked": true,
    "id": "69e8a9a196663b50b0139e0cf0a0e02364faff86",
    "citation_count": 7
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/2a27b8144ac02f67687f76782a3b5d8f-Abstract.html": {
    "title": "Stochastic Optimization for Large-scale Optimal Transport",
    "abstract": "Optimal transport (OT) defines a powerful framework to compare probability distributions in a geometrically faithful way. However, the practical impact of OT is still limited because of its computational burden. We propose a new class of stochastic optimization algorithms to cope with large-scale problems routinely encountered in machine learning applications. These methods are able to manipulate arbitrary distributions (either discrete or continuous) by simply requiring to be able to draw samples from them, which is the typical setup in high-dimensional learning problems. This alleviates the need to discretize these densities, while giving access to provably convergent methods that output the correct distance without discretization error. These algorithms rely on two main ideas: (a) the dual OT problem can be re-cast as the maximization of an expectation; (b) entropic regularization of the primal OT problem results in a smooth dual optimization optimization which can be addressed with algorithms that have a provably faster convergence. We instantiate these ideas in three different computational setups: (i) when comparing a discrete distribution to another, we show that incremental stochastic optimization schemes can beat the current state of the art finite dimensional OT solver (Sinkhorn's algorithm) ; (ii) when comparing a discrete distribution to a continuous density, a re-formulation (semi-discrete) of the dual program is amenable to averaged stochastic gradient descent, leading to better performance than approximately solving the problem by discretization ; (iii) when dealing with two continuous densities, we propose a stochastic gradient descent over a reproducing kernel Hilbert space (RKHS). This is currently the only known method to solve this problem, and is more efficient than discretizing beforehand the two densities. We backup these claims on a set of discrete, semi-discrete and continuous benchmark problems",
    "volume": "main",
    "checked": true,
    "id": "6f840ce7c8373c11f89ea1693d0cab170627d31f",
    "citation_count": 346
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/2a79ea27c279e471f4d180b08d62b00a-Abstract.html": {
    "title": "The Sound of APALM Clapping: Faster Nonsmooth Nonconvex Optimization with Stochastic Asynchronous PALM",
    "abstract": "We introduce the Stochastic Asynchronous Proximal Alternating Linearized Minimization (SAPALM) method, a block coordinate stochastic proximal-gradient method for solving nonconvex, nonsmooth optimization problems. SAPALM is the first asynchronous parallel optimization method that provably converges on a large class of nonconvex, nonsmooth problems. We prove that SAPALM matches the best known rates of convergence --- among synchronous or asynchronous methods --- on this problem class. We provide upper bounds on the number of workers for which we can expect to see a linear speedup, which match the best bounds known for less complex problems, and show that in practice SAPALM achieves this linear speedup. We demonstrate state-of-the-art performance on several matrix factorization problems",
    "volume": "main",
    "checked": true,
    "id": "1494d53ddbeea4eedeb31c2ccda49f68486ecb98",
    "citation_count": 36
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/2b0f658cbffd284984fb11d90254081f-Abstract.html": {
    "title": "Coresets for Scalable Bayesian Logistic Regression",
    "abstract": "The use of Bayesian methods in large-scale data settings is attractive because of the rich hierarchical models, uncertainty quantification, and prior specification they provide. Standard Bayesian inference algorithms are computationally expensive, however, making their direct application to large datasets difficult or infeasible. Recent work on scaling Bayesian inference has focused on modifying the underlying algorithms to, for example, use only a random data subsample at each iteration. We leverage the insight that data is often redundant to instead obtain a weighted subset of the data (called a coreset) that is much smaller than the original dataset. We can then use this small coreset in any number of existing posterior inference algorithms without modification. In this paper, we develop an efficient coreset construction algorithm for Bayesian logistic regression models. We provide theoretical guarantees on the size and approximation quality of the coreset -- both for fixed, known datasets, and in expectation for a wide class of data generative models. Crucially, the proposed approach also permits efficient construction of the coreset in both streaming and parallel settings, with minimal additional effort. We demonstrate the efficacy of our approach on a number of synthetic and real-world datasets, and find that, in practice, the size of the coreset is independent of the original dataset size. Furthermore, constructing the coreset takes a negligible amount of time compared to that required to run MCMC on it",
    "volume": "main",
    "checked": true,
    "id": "51449a536eccb6952a2d3c9171bf6dfcd334ecb5",
    "citation_count": 175
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/2b24d495052a8ce66358eb576b8912c8-Abstract.html": {
    "title": "Sorting out typicality with the inverse moment matrix SOS polynomial",
    "abstract": "We study a surprising phenomenon related to the representation of a cloud of data points using polynomials. We start with the previously unnoticed empirical observation that, given a collection (a cloud) of data points, the sublevel sets of a certain distinguished polynomial capture the shape of the cloud very accurately. This distinguished polynomial is a sum-of-squares (SOS) derived in a simple manner from the inverse of the empirical moment matrix. In fact, this SOS polynomial is directly related to orthogonal polynomials and the Christoffel function. This allows to generalize and interpret extremality properties of orthogonal polynomials and to provide a mathematical rationale for the observed phenomenon. Among diverse potential applications, we illustrate the relevance of our results on a network intrusion detection task for which we obtain performances similar to existing dedicated methods reported in the literature",
    "volume": "main",
    "checked": true,
    "id": "0570d0d8b0582630a2e90f84a653cfb9564a7cd1",
    "citation_count": 47
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/2ba596643cbbbc20318224181fa46b28-Abstract.html": {
    "title": "The Multi-fidelity Multi-armed Bandit",
    "abstract": "We study a variant of the classical stochastic $K$-armed bandit where observing the outcome of each arm is expensive, but cheap approximations to this outcome are available. For example, in online advertising the performance of an ad can be approximated by displaying it for shorter time periods or to narrower audiences. We formalise this task as a \\emph{multi-fidelity} bandit, where, at each time step, the forecaster may choose to play an arm at any one of $M$ fidelities. The highest fidelity (desired outcome) expends cost $\\costM$. The $m$\\ssth fidelity (an approximation) expends $\\costm < \\costM$ and returns a biased estimate of the highest fidelity. We develop \\mfucb, a novel upper confidence bound procedure for this setting and prove that it naturally adapts to the sequence of available approximations and costs thus attaining better regret than naive strategies which ignore the approximations. For instance, in the above online advertising example, \\mfucbs would use the lower fidelities to quickly eliminate suboptimal ads and reserve the larger expensive experiments on a small set of promising candidates. We complement this result with a lower bound and show that \\mfucbs is nearly optimal under certain conditions",
    "volume": "main",
    "checked": true,
    "id": "2d5c177ebfd8b8e20363e4781f3c3729e96b8b3b",
    "citation_count": 24
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/2bb232c0b13c774965ef8558f0fbd615-Abstract.html": {
    "title": "Joint Line Segmentation and Transcription for End-to-End Handwritten Paragraph Recognition",
    "abstract": "Offline handwriting recognition systems require cropped text line images for both training and recognition. On the one hand, the annotation of position and transcript at line level is costly to obtain. On the other hand, automatic line segmentation algorithms are prone to errors, compromising the subsequent recognition.  In this paper, we propose a modification of the popular and efficient Multi-Dimensional Long Short-Term Memory Recurrent Neural Networks (MDLSTM-RNNs) to enable end-to-end processing of handwritten paragraphs. More particularly, we replace the collapse layer transforming the two-dimensional representation into a sequence of predictions by a recurrent version which can select one line at a time.  In the proposed model, a neural network performs a kind of implicit line segmentation by computing attention weights on the image representation. The experiments on paragraphs of Rimes and IAM databases yield results that are competitive with those of networks trained at line level, and constitute a significant step towards end-to-end transcription of full documents",
    "volume": "main",
    "checked": true,
    "id": "2c4c976fdf3d8191caeb097c07f64e500b2f6712",
    "citation_count": 145
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/2c6ae45a3e88aee548c0714fad7f8269-Abstract.html": {
    "title": "k*-Nearest Neighbors: From Global to Local",
    "abstract": "The weighted k-nearest neighbors  algorithm is one of the most fundamental non-parametric methods in pattern recognition and machine learning.  The question of setting the optimal number of neighbors as well as the optimal weights has received much attention throughout the years, nevertheless this problem seems to  have remained unsettled. In this paper we offer a simple approach to locally weighted regression/classification, where we make the bias-variance tradeoff explicit.  Our formulation enables us to phrase a notion of optimal weights, and to efficiently find these weights as well as the optimal number of neighbors  efficiently and adaptively, for each data point whose value we wish to estimate. The applicability of our approach is demonstrated on several datasets, showing superior performance over standard locally weighted methods",
    "volume": "main",
    "checked": true,
    "id": "a5d121a5d262c99612afcc2a6eb05baf49ec514d",
    "citation_count": 56
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/2cad8fa47bbef282badbb8de5374b894-Abstract.html": {
    "title": "Protein contact prediction from amino acid co-evolution using convolutional networks for graph-valued images",
    "abstract": "Proteins are the \"building blocks of life\", the most abundant organic molecules, and the central focus of most areas of biomedicine. Protein structure is strongly related to protein function, thus structure prediction is a crucial task on the way to solve many biological questions. A contact map is a compact representation of the three-dimensional structure of a protein via the pairwise contacts between the amino acid constituting the protein. We use a convolutional network to calculate protein contact maps from inferred statistical coupling between positions in the protein sequence. The input to the network has an image-like structure amenable to convolutions, but every \"pixel\" instead of color channels contains a bipartite undirected edge-weighted graph. We propose several methods for treating such \"graph-valued images\" in a convolutional network. The proposed method outperforms state-of-the-art methods by a large margin. It also allows for a great flexibility with regard to the input data, which makes it useful for studying a wide range of problems",
    "volume": "main",
    "checked": true,
    "id": "56fbaf38a74f5c59e4b21460473d2e28796e4515",
    "citation_count": 41
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/2d405b367158e3f12d7c1e31a96b3af3-Abstract.html": {
    "title": "Learnable Visual Markers",
    "abstract": "We propose a new approach to designing visual markers (analogous to QR-codes, markers for augmented reality, and robotic fiducial tags) based on the advances in deep generative networks. In our approach, the markers are obtained as color images synthesized by a deep network from input bit strings, whereas another deep network is trained to recover the bit strings back from the photos of these markers. The two networks are trained simultaneously in a joint backpropagation process that takes characteristic photometric and geometric distortions associated with marker fabrication and capture into account. Additionally, a stylization loss based on statistics of activations in a pretrained classification network can be inserted into the learning in order to shift the marker appearance towards some texture prototype. In the experiments, we demonstrate that the markers obtained using our approach are capable of retaining bit strings that are long enough to be practical. The ability to automatically adapt markers according to the usage scenario and the desired capacity as well as the ability to combine information encoding with artistic stylization are the unique properties of our approach. As a byproduct, our approach provides an insight on the structure of patterns that are most suitable for recognition by ConvNets and on their ability to distinguish composite patterns",
    "volume": "main",
    "checked": true,
    "id": "d8c1ff897190a46602c54a6b6f6fdd66d9b97813",
    "citation_count": 11
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/2dea61eed4bceec564a00115c4d21334-Abstract.html": {
    "title": "Finite-Sample Analysis of Fixed-k Nearest Neighbor Density Functional Estimators",
    "abstract": "We provide finite-sample analysis of a general framework for using k-nearest neighbor statistics to estimate functionals of a nonparametric continuous probability density, including entropies and divergences. Rather than plugging a consistent density estimate (which requires k → ∞ as the sample size n → ∞) into the functional of interest, the estimators we consider fix k and perform a bias correction. This can be more efficient computationally, and, as we show, statistically, leading to faster convergence rates. Our framework unifies several previous estimators, for most of which ours are the first finite sample guarantees",
    "volume": "main",
    "checked": true,
    "id": "4d489ff43d7f2ca30155fd05b02bf71a99c15091",
    "citation_count": 47
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/2df45244f09369e16ea3f9117ca45157-Abstract.html": {
    "title": "Maximizing Influence in an Ising Network: A Mean-Field Optimal Solution",
    "abstract": "Influence maximization in social networks has typically been studied in the context of contagion models and irreversible processes. In this paper, we consider an alternate model that treats individual opinions as spins in an Ising system at dynamic equilibrium. We formalize the \\textit{Ising influence maximization} problem, which has a natural physical interpretation as maximizing the magnetization given a budget of external magnetic field. Under the mean-field (MF) approximation, we present a gradient ascent algorithm that uses the susceptibility to efficiently calculate local maxima of the magnetization, and we develop a number of sufficient conditions for when the MF magnetization is concave and our algorithm converges to a global optimum. We apply our algorithm on random and real-world networks, demonstrating, remarkably, that the MF optimal external fields (i.e., the external fields which maximize the MF magnetization) exhibit a phase transition from focusing on high-degree individuals at high temperatures to focusing on low-degree individuals at low temperatures. We also establish a number of novel results about the structure of steady-states in the ferromagnetic MF Ising model on general graphs, which are of independent interest",
    "volume": "main",
    "checked": true,
    "id": "99676466189005a2d57084e218272f86c1107342",
    "citation_count": 21
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/2dffbc474aa176b6dc957938c15d0c8b-Abstract.html": {
    "title": "Regret Bounds for Non-decomposable Metrics with Missing Labels",
    "abstract": "We consider the problem of recommending relevant labels (items) for a given data point (user). In particular, we are interested in the practically important setting where the evaluation is with respect to non-decomposable (over labels) performance metrics like the $F_1$ measure, \\emph{and} training data has missing labels. To this end, we propose a generic framework that given a performance metric $\\Psi$, can devise a regularized objective function and a threshold such that all the values in the predicted score vector above and only above the threshold are selected to be positive.  We show that the regret or generalization error in the given metric $\\Psi$ is bounded ultimately by estimation error of certain underlying parameters. In particular, we derive regret bounds under three popular settings: a) collaborative filtering, b) multilabel classification, and c) PU (positive-unlabeled) learning.  For each of the above problems, we can obtain precise non-asymptotic regret bound which is small even when a large fraction of labels is missing. Our empirical results on synthetic and benchmark datasets demonstrate that by explicitly modeling for missing labels and optimizing the desired performance metric, our algorithm indeed achieves significantly better performance (like $F_1$ score) when compared to methods that do not model missing label information carefully",
    "volume": "main",
    "checked": true,
    "id": "571ae64ae7faa5744ee157b661e38d660880f1e3",
    "citation_count": 3
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/2e65f2f2fdaf6c699b223c61b1b5ab89-Abstract.html": {
    "title": "Adaptive Concentration Inequalities for Sequential Decision Problems",
    "abstract": "A key challenge in sequential decision problems is to determine how many samples are needed for an agent to make reliable decisions with good probabilistic guarantees.   We introduce Hoeffding-like concentration inequalities that hold for a random, adaptively chosen number of samples. Our inequalities are tight under natural assumptions and can greatly simplify the analysis of common sequential decision problems. In particular, we apply them to sequential hypothesis testing, best arm identification, and sorting. The resulting algorithms rival or exceed the state of the art both theoretically and empirically",
    "volume": "main",
    "checked": true,
    "id": "9d68c8b56a5994ee9488c66a0125af053d4f03ba",
    "citation_count": 42
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/2f37d10131f2a483a8dd005b3d14b0d9-Abstract.html": {
    "title": "Refined Lower Bounds for Adversarial Bandits",
    "abstract": "We provide new lower bounds on the regret that must be suffered by adversarial bandit algorithms. The new results show that recent upper bounds that either (a) hold with high-probability or (b) depend on the total loss of the best arm or (c) depend on the quadratic variation of the losses, are close to tight. Besides this we prove two impossibility results. First, the existence of a single arm that is optimal in every round cannot improve the regret in the worst case. Second, the regret cannot scale with the effective range of the losses. In contrast, both results are possible in the full-information setting",
    "volume": "main",
    "checked": true,
    "id": "0bf8deac125bbc0b74a9963e224d30d0a29e50e2",
    "citation_count": 57
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/2f4fe03d77724a7217006e5d16728874-Abstract.html": {
    "title": "Structure-Blind Signal Recovery",
    "abstract": "We consider the problem of recovering a signal observed in Gaussian noise. If the set of signals is convex and compact, and can be specified beforehand, one can use classical linear estimators that achieve a risk within a constant factor of the minimax risk. However, when the set is unspecified, designing an estimator that is blind to the hidden structure of the signal remains a challenging problem. We propose a new family of estimators to recover signals observed in Gaussian noise. Instead of specifying the set where the signal lives, we assume the existence of a well-performing linear estimator. Proposed estimators enjoy exact oracle inequalities and can be efficiently computed through convex optimization. We present several numerical illustrations that show the potential of the approach",
    "volume": "main",
    "checked": true,
    "id": "dfa82e2db04a4d1f2969760e89deb89a238d4653",
    "citation_count": 3
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/2f885d0fbe2e131bfc9d98363e55d1d4-Abstract.html": {
    "title": "Reward Augmented Maximum Likelihood for Neural Structured Prediction",
    "abstract": "A key problem in structured output prediction is enabling direct optimization of the task reward function that matters for test evaluation. This paper presents a simple and computationally efficient method that incorporates task reward into maximum likelihood training. We establish a connection between maximum likelihood and regularized expected reward, showing that they are approximately equivalent in the vicinity of the optimal solution. Then we show how maximum likelihood can be generalized by optimizing the conditional probability of auxiliary outputs that are sampled proportional to their exponentiated scaled rewards. We apply this framework to optimize edit distance in the output space, by sampling from edited targets. Experiments on speech recognition and machine translation for neural sequence to sequence models show notable improvements over maximum likelihood baseline by simply sampling from target output augmentations",
    "volume": "main",
    "checked": true,
    "id": "1d9600229a4cf8ba5e8f5ad4d05b41af9c8f80a6",
    "citation_count": 209
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/30ef30b64204a3088a26bc2e6ecf7602-Abstract.html": {
    "title": "Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning",
    "abstract": "Effective convolutional neural networks are trained on large sets of labeled data. However, creating large labeled datasets is a very costly and time-consuming task. Semi-supervised learning uses unlabeled data to train a model with higher accuracy when there is a limited set of labeled data available. In this paper, we consider the problem of semi-supervised learning with convolutional neural networks. Techniques such as randomized data augmentation, dropout and random max-pooling provide better generalization and stability for classifiers that are trained using gradient descent. Multiple passes of an individual sample through the network might lead to different predictions due to the non-deterministic behavior of these techniques. We propose an unsupervised loss function that takes advantage of the stochastic nature of these methods and minimizes the difference between the predictions of multiple passes of a training sample through the network. We evaluate the proposed method on several benchmark datasets",
    "volume": "main",
    "checked": true,
    "id": "4c20e7f95448ca3c1042a6d7fa5fa15ec27e9aeb",
    "citation_count": 736
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/312351bff07989769097660a56395065-Abstract.html": {
    "title": "An Online Sequence-to-Sequence Model Using Partial Conditioning",
    "abstract": "Sequence-to-sequence models have achieved impressive results on various tasks. However, they are unsuitable for tasks that require incremental predictions to be made as more data arrives or tasks that have long input sequences and output sequences. This is because they generate an output sequence conditioned on an entire input sequence. In this paper, we present a Neural Transducer that can make incremental predictions as more input arrives, without redoing the entire computation. Unlike sequence-to-sequence models, the Neural Transducer computes the next-step distribution conditioned on the partially observed input sequence and the partially generated sequence. At each time step, the transducer can decide to emit zero to many output symbols. The data can be processed using an encoder and presented as input to the transducer. The discrete decision to emit a symbol at every time step makes it difficult to learn with conventional backpropagation. It is however possible to train the transducer by using a dynamic programming algorithm to generate target discrete decisions. Our experiments show that the Neural Transducer works well in settings where it is required to produce output predictions as data come in. We also find that the Neural Transducer performs well for long sequences even when attention mechanisms are not used",
    "volume": "main",
    "checked": true,
    "id": "a418524a3576afff4dc2178ed169e692915bd46b",
    "citation_count": 103
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/3147da8ab4a0437c15ef51a5cc7f2dc4-Abstract.html": {
    "title": "Interaction Networks for Learning about Objects, Relations and Physics",
    "abstract": "Reasoning about objects, relations, and physics is central to human intelligence, and a key goal of artificial intelligence. Here we introduce the interaction network, a model which can reason about how objects in complex systems interact, supporting dynamical predictions, as well as inferences about the abstract properties of the system. Our model takes graphs as input, performs object- and relation-centric reasoning in a way that is analogous to a simulation, and is implemented using deep neural networks. We evaluate its ability to reason about several challenging physical domains: n-body problems, rigid-body collision, and non-rigid dynamics. Our results show it can be trained to accurately simulate the physical trajectories of dozens of objects over thousands of time steps, estimate abstract quantities such as energy, and generalize automatically to systems with different numbers and configurations of objects and relations. Our interaction network implementation is the first general-purpose, learnable physics engine, and a powerful general framework for reasoning about object and relations in a wide variety of complex real-world domains",
    "volume": "main",
    "checked": true,
    "id": "ae42c0cff384495683192b06bd985cdd7a54632a",
    "citation_count": 1089
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/31839b036f63806cba3f47b93af8ccb5-Abstract.html": {
    "title": "Bayesian optimization under mixed constraints with a slack-variable augmented Lagrangian",
    "abstract": "An augmented Lagrangian (AL) can convert a constrained optimization problem into a sequence of simpler (e.g., unconstrained) problems which are then usually solved with local solvers. Recently, surrogate-based Bayesian optimization (BO) sub-solvers have been successfully deployed in the AL framework for a more global search in the presence of inequality constraints; however a drawback was that expected improvement (EI) evaluations relied on Monte Carlo. Here we introduce an alternative slack variable AL, and show that in this formulation the EI may be evaluated with library routines. The slack variables furthermore facilitate equality as well as inequality constraints, and mixtures thereof. We show our new slack \"ALBO\" compares favorably to the original. Its superiority over conventional alternatives is reinforced on several new mixed constraint examples",
    "volume": "main",
    "checked": true,
    "id": "392a0a019aea59268a7ab3a551870e97dc56c373",
    "citation_count": 62
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/31857b449c407203749ae32dd0e7d64a-Abstract.html": {
    "title": "Combinatorial Energy Learning for Image Segmentation",
    "abstract": "We introduce a new machine learning approach for image segmentation that uses a neural network to model the conditional energy of a segmentation given an image.  Our approach, combinatorial energy learning for image segmentation (CELIS) places a particular emphasis on modeling the inherent combinatorial nature of dense image segmentation problems.  We propose efficient algorithms for learning deep neural networks to model the energy function, and for local optimization of this energy in the space of supervoxel agglomerations.  We extensively evaluate our method on a publicly available 3-D microscopy dataset with 25 billion voxels of ground truth data. On an 11 billion voxel test set, we find that our method improves volumetric reconstruction accuracy by more than 20% as compared to two state-of-the-art baseline methods: graph-based segmentation of the output of a 3-D convolutional neural network trained to predict boundaries, as well as a random forest classifier trained to agglomerate supervoxels that were generated by a 3-D convolutional neural network",
    "volume": "main",
    "checked": true,
    "id": "bf5d32a05e6898a853d11732a40e64f449654041",
    "citation_count": 25
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/31fefc0e570cb3860f2a6d4b38c6490d-Abstract.html": {
    "title": "Bayesian Optimization for Probabilistic Programs",
    "abstract": "We present the first general purpose framework for marginal maximum a posteriori estimation of probabilistic program variables. By using a series of code transformations, the evidence of any probabilistic program, and therefore of any graphical model, can be optimized with respect to an arbitrary subset of its sampled variables.  To carry out this optimization, we develop the first Bayesian optimization package to directly exploit the source code of its target, leading to innovations in problem-independent hyperpriors, unbounded optimization, and implicit constraint satisfaction; delivering significant performance improvements over prominent existing packages.  We present applications of our method to a number of tasks including engineering design and parameter optimization",
    "volume": "main",
    "checked": true,
    "id": "ae49bab4af8d994306204ca3586809b07681d74a",
    "citation_count": 26
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/320722549d1751cf3f247855f937b982-Abstract.html": {
    "title": "Coin Betting and Parameter-Free Online Learning",
    "abstract": "In the recent years, a number of parameter-free algorithms have been developed for online linear optimization over Hilbert spaces and for learning with expert advice.  These algorithms achieve optimal regret bounds that depend on the unknown competitors, without having to tune the learning rates with oracle choices.  We present a new intuitive framework to design parameter-free algorithms for both online linear optimization over Hilbert spaces and for learning with expert advice, based on reductions to betting on outcomes of adversarial coins. We instantiate it using a betting algorithm based on the Krichevsky-Trofimov estimator.  The resulting algorithms are simple, with no parameters to be tuned, and they improve or match previous results in terms of regret guarantee and per-round complexity",
    "volume": "main",
    "checked": true,
    "id": "06bb9abf54e4234f58ebd54d3fe7b4f55f0c4d76",
    "citation_count": 102
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/325995af77a0e8b06d1204a171010b3a-Abstract.html": {
    "title": "Learning Deep Embeddings with Histogram Loss",
    "abstract": "We suggest a new loss for learning deep embeddings. The key characteristics of the new loss is the absence of tunable parameters and very good results obtained across a range of datasets and problems. The loss is computed by estimating two distribution of similarities for positive (matching) and negative (non-matching) point pairs, and then computing the probability of a positive pair to have a lower similarity score than a negative pair based on these probability estimates. We show that these operations can be performed in a simple and piecewise-differentiable manner using 1D histograms with soft assignment operations. This makes the proposed loss suitable for learning deep embeddings using stochastic optimization. The experiments reveal favourable results compared to recently proposed loss functions",
    "volume": "main",
    "checked": true,
    "id": "ece3b623232c90bb8a9021a3eb25223c4fde7069",
    "citation_count": 308
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/329e6581efbc90bd92a1f22c4ba2103d-Abstract.html": {
    "title": "An Efficient Streaming Algorithm for the Submodular Cover Problem",
    "abstract": "We initiate the study of the classical Submodular Cover (SC) problem in the data streaming model which we refer to as the Streaming Submodular Cover (SSC). We show that any single pass streaming algorithm using sublinear memory in the size of the stream will fail to provide any non-trivial approximation guarantees for SSC. Hence, we consider a relaxed version of SSC, where we only seek to find a partial cover. We design the first Efficient bicriteria Submodular Cover Streaming (ESC-Streaming) algorithm for this problem, and provide theoretical guarantees for its performance supported by numerical evidence. Our algorithm finds solutions that are competitive with the near-optimal offline greedy algorithm despite requiring only a single pass over the data stream. In our numerical experiments, we evaluate the performance of ESC-Streaming on active set selection and large-scale graph cover problems",
    "volume": "main",
    "checked": true,
    "id": "9d8006c385529107b2514af0770d1d36ebadcb15",
    "citation_count": 13
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/339a18def9898dd60a634b2ad8fbbd58-Abstract.html": {
    "title": "Fundamental Limits of Budget-Fidelity Trade-off in Label Crowdsourcing",
    "abstract": "Digital crowdsourcing (CS) is a modern approach to perform certain large projects using small contributions of a large crowd. In CS, a taskmaster typically breaks down the project into small batches of tasks and assigns them to so-called workers with imperfect skill levels. The crowdsourcer then collects and analyzes the results for inference and serving the purpose of the project. In this work, the CS problem, as a human-in-the-loop computation problem, is modeled and analyzed in an information theoretic rate-distortion framework. The purpose is to identify the ultimate fidelity that one can achieve by any form of query from the crowd and any decoding (inference) algorithm with a given budget. The results are established by a joint source channel (de)coding scheme, which represent the query scheme and inference, over parallel noisy channels, which model workers with imperfect skill levels. We also present and analyze a query scheme dubbed k-ary incidence coding and study optimized query pricing in this setting",
    "volume": "main",
    "checked": true,
    "id": "60b44f01bd7be40ad9f56836beba45592cafc749",
    "citation_count": 7
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/33bb83720ba9d2b6da87114380314af5-Abstract.html": {
    "title": "Beyond Exchangeability: The Chinese Voting Process",
    "abstract": "Many online communities present user-contributed responses, such as reviews of products and answers to questions. User-provided helpfulness votes can highlight the most useful responses, but voting is a social process that can gain momentum based on the popularity of responses and the polarity of existing votes. We propose the Chinese Voting Process (CVP) which models the evolution of helpfulness votes as a self-reinforcing process dependent on position and presentation biases. We evaluate this model on Amazon product reviews and more than 80 StackExchange forums, measuring the intrinsic quality of individual responses and behavioral coefficients of different communities",
    "volume": "main",
    "checked": true,
    "id": "75d09f5bbca31ab6933d6a64fecce8af869a966f",
    "citation_count": 4
  },
  "https://papers.nips.cc/paper_files/paper/2016/hash/34ed066df378efacc9b924ec161e7639-Abstract.html": {
    "title": "Robust Spectral Detection of Global Structures in the Data by Learning a Regularization",
    "abstract": "Spectral methods are popular in detecting global structures in the given data that can be represented as a matrix. However when the data matrix is sparse or noisy, classic spectral methods usually fail to work, due to localization of eigenvectors (or singular vectors) induced by the sparsity or noise. In this work, we propose a general method to solve the localization problem by learning a regularization matrix from the localized eigenvectors. Using matrix perturbation analysis, we demonstrate that the learned  regularizations suppress down the eigenvalues associated with localized  eigenvectors and enable us to recover the informative eigenvectors representing the global structure. We show applications of our method in several inference problems: community detection in networks, clustering from pairwise similarities, rank estimation and matrix completion problems. Using extensive experiments, we illustrate that our method solves the localization problem and works down to the  theoretical detectability limits in different kinds of synthetic data. This is in contrast with existing spectral algorithms based on data matrix, non-backtracking matrix, Laplacians and those with rank-one regularizations, which perform poorly in the sparse case with noise",
    "volume": "main",
    "checked": true,
    "id": "a864e0ea3709a2f1b7dcc8752dfef67478c52082",
    "citation_count": 10
  }
}