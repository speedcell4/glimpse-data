{
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Object-Centric_Video_Representation_for_Long-Term_Action_Anticipation_WACV_2024_paper.html": {
    "title": "Object-Centric Video Representation for Long-Term Action Anticipation",
    "volume": "main",
    "abstract": "This paper focuses on building object-centric representations for long-term action anticipation in videos. Our key motivation is that objects provide important cues to recognize and predict human-object interactions, especially when the predictions are longer term, as an observed \"background\" object could be used by the human actor in the future. We observe that existing object-based video recognition frameworks either assume the existence of in-domain supervised object detectors or follow a fully weakly-supervised pipeline to infer object locations from action labels. We propose to build object-centric video representations by leveraging visual-language pretrained models. This is achieved by \"object prompts\", an approach to extract task-specific object-centric representations from general-purpose pretrained models without finetuning. To recognize and predict human-object interactions, we use a Transformer-based neural architecture which allows the \"retrieval\" of relevant objects for action anticipation at various time scales. We conduct extensive evaluations on the Ego4D, 50Salads, and EGTEA Gaze+ benchmarks. Both quantitative and qualitative results confirm the effectiveness of our proposed method",
    "checked": true,
    "id": "1fe7a8fdba3d5300e1a7a6a72b1efcb8b4a66d95",
    "semantic_title": "object-centric video representation for long-term action anticipation",
    "citation_count": 0,
    "authors": [
      "Ce Zhang",
      "Changcheng Fu",
      "Shijie Wang",
      "Nakul Agarwal",
      "Kwonjoon Lee",
      "Chiho Choi",
      "Chen Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Honda_CLRerNet_Improving_Confidence_of_Lane_Detection_With_LaneIoU_WACV_2024_paper.html": {
    "title": "CLRerNet: Improving Confidence of Lane Detection With LaneIoU",
    "volume": "main",
    "abstract": "Lane marker detection is a crucial component of the autonomous driving and driver assistance systems. Modern deep lane detection methods with anchor-based lane representation exhibit excellent performance on lane detection benchmarks. Through preliminary oracle experiments, we firstly disentangle the lane representation components to determine the direction of our approach. We show that correct lane positions are already among the predictions of an existing anchor-based detector, and the confidence scores that accurately represent intersection-over-union (IoU) with ground truths are the most beneficial. Based on the finding, we propose LaneIoU that better correlates with the metric, by taking the local lane angles into consideration. We develop a novel detector coined CLRerNet featuring LaneIoU for the target assignment cost and loss functions aiming at the improved quality of confidence scores. Through careful and fair benchmark including cross validation, we demonstrate that CLRerNet outperforms the state-of-the-art by a large margin - enjoying F1 score of 81.43% compared with 80.47% of the existing method on CULane, and 86.47% compared with 86.10% on CurveLanes",
    "checked": true,
    "id": "13f1b60bbb44e0e33e3fab8e9c39077e2d918287",
    "semantic_title": "clrernet: improving confidence of lane detection with laneiou",
    "citation_count": 2,
    "authors": [
      "Hiroto Honda",
      "Yusuke Uchida"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Stojnic_Training_Ensembles_With_Inliers_and_Outliers_for_Semi-Supervised_Active_Learning_WACV_2024_paper.html": {
    "title": "Training Ensembles With Inliers and Outliers for Semi-Supervised Active Learning",
    "volume": "main",
    "abstract": "Deep active learning in the presence of outlier examples poses a realistic yet challenging scenario. Acquiring unlabeled data for annotation requires a delicate balance between avoiding outliers to conserve the annotation budget and prioritizing useful inlier examples for effective training. In this work, we present an approach that leverages three highly synergistic components, which are identified as key ingredients: joint classifier training with inliers and outliers, semi-supervised learning through pseudo-labeling, and model ensembling. Our work demonstrates that ensembling significantly enhances the accuracy of pseudo-labeling and improves the quality of data acquisition. By enabling semi-supervision through the joint training process, where outliers are properly handled, we observe a substantial boost in classifier accuracy through the use of all available unlabeled examples. Notably, we reveal that the integration of joint training renders explicit outlier detection unnecessary; a conventional component for acquisition in prior work. The three key components align seamlessly with numerous existing approaches. Through empirical evaluations, we showcase that their combined use leads to a performance increase. Remarkably, despite its simplicity, our proposed approach outperforms all other methods in terms of performance. Code: https://github.com/vladan-stojnic/active-outliers",
    "checked": true,
    "id": "6890bf5890f95d5f3d2a819f150f54d0cf304a03",
    "semantic_title": "training ensembles with inliers and outliers for semi-supervised active learning",
    "citation_count": 0,
    "authors": [
      "Vladan Stojnić",
      "Zakaria Laskar",
      "Giorgos Tolias"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Li_Robust_Source-Free_Domain_Adaptation_for_Fundus_Image_Segmentation_WACV_2024_paper.html": {
    "title": "Robust Source-Free Domain Adaptation for Fundus Image Segmentation",
    "volume": "main",
    "abstract": "Unsupervised Domain Adaptation (UDA) is a learning technique that transfers knowledge learned in the source domain from labelled training data to the target domain with only unlabelled data. It is of significant importance to medical image segmentation because of the usual lack of labelled training data. Although extensive efforts have been made to optimize UDA techniques to improve the accuracy of segmentation models in the target domain, few studies have addressed the robustness of these models under UDA. In this study, we propose a two-stage training strategy for robust domain adaptation. In the source training stage, we utilize adversarial sample augmentation to enhance the robustness and generalization capability of the source model. And in the target training stage, we propose a novel robust pseudo-label and pseudo-boundary (PLPB) method, which effectively utilizes unlabeled target data to generate pseudo labels and pseudo boundaries that enable model self-adaptation without requiring source data. Extensive experimental results on cross-domain fundus image segmentation confirm the effectiveness and versatility of our method. Source code of this study is openly accessible at https://github.com/LinGrayy/PLPB",
    "checked": true,
    "id": "2314a9c503d2c4fd8d8b555dc7d7730f7399b3a8",
    "semantic_title": "robust source-free domain adaptation for fundus image segmentation",
    "citation_count": 0,
    "authors": [
      "Lingrui Li",
      "Yanfeng Zhou",
      "Ge Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Iwai_Controlling_Rate_Distortion_and_Realism_Towards_a_Single_Comprehensive_Neural_WACV_2024_paper.html": {
    "title": "Controlling Rate, Distortion, and Realism: Towards a Single Comprehensive Neural Image Compression Model",
    "volume": "main",
    "abstract": "In recent years, neural network-driven image compression (NIC) has gained significant attention. Some works adopt deep generative models such as GANs and diffusion models to enhance perceptual quality (realism). A critical obstacle of these generative NIC methods is that each model is optimized for a single bit rate. Consequently, multiple models are required to compress images to different bit rates, which is impractical for real-world applications. To tackle this issue, we propose a variable-rate generative NIC model. Specifically, we explore several discriminator designs tailored for the variable-rate approach and introduce a novel adversarial loss. Moreover, by incorporating the newly proposed multi-realism technique, our method allows the users to adjust the bit rate, distortion, and realism with a single model, achieving ultra-controllability. Unlike existing variable-rate generative NIC models, our method matches or surpasses the performance of state-of-the-art single-rate generative NIC models while covering a wide range of bit rates using just one model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shoma Iwai",
      "Tomo Miyazaki",
      "Shinichiro Omachi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Lim_MetaVers_Meta-Learned_Versatile_Representations_for_Personalized_Federated_Learning_WACV_2024_paper.html": {
    "title": "MetaVers: Meta-Learned Versatile Representations for Personalized Federated Learning",
    "volume": "main",
    "abstract": "One of the daunting challenges in federated learning (FL) is the heterogeneity across clients that hinders the successful federation of a global model. When the heterogeneity becomes worse, personalized federated learning (PFL) pursues to detour the hardship of capturing the commonality across clients by allowing the personalization of models built upon the federation. In the scope of PFL for visual models, on the contrary, the recent effort for aggregating an effective global representation rather than chasing further personalization draws great attention. Along the same lines, we aim to train a large-margin global representation with a strong generalization across clients by adopting the meta-learning framework and margin-based loss, which are widely accepted to be effective in handling multiple visual tasks. Our method called MetVers achieves state-of-the-art accuracies for the PFL benchmarks with the CIFAR-10, CIFAR-100, and CINIC-10 datasets while showing robustness against data reconstruction attacks. Noteworthy, the versatile representation of MetaVers exhibits a strong generalization when tested on new clients with novel classes",
    "checked": true,
    "id": "6f22e99f67adbc8cf39b4c53fd3b875012473fb7",
    "semantic_title": "metavers: meta-learned versatile representations for personalized federated learning",
    "citation_count": 0,
    "authors": [
      "Jin Hyuk Lim",
      "SeungBum Ha",
      "Sung Whan Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wallin_Improving_Open-Set_Semi-Supervised_Learning_With_Self-Supervision_WACV_2024_paper.html": {
    "title": "Improving Open-Set Semi-Supervised Learning With Self-Supervision",
    "volume": "main",
    "abstract": "Open-set semi-supervised learning (OSSL) embodies a practical scenario within semi-supervised learning, wherein the unlabeled training set encompasses classes absent from the labeled set. Many existing OSSL methods assume that these out-of-distribution data are harmful and put effort into excluding data belonging to unknown classes from the training objective. In contrast, we propose an OSSL framework that facilitates learning from all unlabeled data through self-supervision. Additionally, we utilize an energy-based score to accurately recognize data belonging to the known classes, making our method well-suited for handling uncurated data in deployment. We show through extensive experimental evaluations that our method yields state-of-the-art results on many of the evaluated benchmark problems in terms of closed-set accuracy and open-set recognition when compared with existing methods for OSSL. Our code is available at https://github.com/walline/ssl-tf2-sefoss",
    "checked": true,
    "id": "efb9cb7a0576a36c16297fe0992ddcb91e520ce9",
    "semantic_title": "improving open-set semi-supervised learning with self-supervision",
    "citation_count": 1,
    "authors": [
      "Erik Wallin",
      "Lennart Svensson",
      "Fredrik Kahl",
      "Lars Hammarstrand"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Barsellotti_FOSSIL_Free_Open-Vocabulary_Semantic_Segmentation_Through_Synthetic_References_Retrieval_WACV_2024_paper.html": {
    "title": "FOSSIL: Free Open-Vocabulary Semantic Segmentation Through Synthetic References Retrieval",
    "volume": "main",
    "abstract": "Unsupervised Open-Vocabulary Semantic Segmentation aims to segment an image into regions referring to an arbitrary set of concepts described by text, without relying on dense annotations that are available only for a subset of the categories. Previous works relied on inducing pixel-level alignment in a multi-modal space through contrastive training over vast corpora of image-caption pairs. However, representing a semantic category solely through its textual embedding is insufficient to encompass the wide-ranging variability in the visual appearances of the images associated with that category. In this paper, we propose FOSSIL, a pipeline that enables a self-supervised backbone to perform open-vocabulary segmentation relying only on the visual modality. In particular, we decouple the task into two components: (1) we leverage text-conditioned diffusion models to generate a large collection of visual embeddings, starting from a set of captions. These can be retrieved at inference time to obtain a support set of references for the set of textual concepts. Further, (2) we exploit self-supervised dense features to partition the image into semantically coherent regions. We demonstrate that our approach provides strong performance on different semantic segmentation datasets, without requiring any additional training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Barsellotti",
      "Roberto Amoroso",
      "Lorenzo Baraldi",
      "Rita Cucchiara"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Rani_Activity-Based_Early_Autism_Diagnosis_Using_a_Multi-Dataset_Supervised_Contrastive_Learning_WACV_2024_paper.html": {
    "title": "Activity-Based Early Autism Diagnosis Using a Multi-Dataset Supervised Contrastive Learning Approach",
    "volume": "main",
    "abstract": "Autism Spectrum Disorder (ASD) is a neurological disorder. Its primary symptoms include difficulty in verbal/non-verbal communication and rigid/repetitive behavior. Traditional methods of autism diagnosis require multiple visits to a human specialist. However, this process is generally time-consuming and may result in a delayed (early) intervention. In this paper, we present a data-driven approach to automate autism diagnosis using video clips of subjects performing simple activities recorded in a weakly constrained environment. This task is particularly challenging since the available training data is small, videos from the two categories (\"ASD\" and \"Control\") are generally perceptually indistinguishable, and there is no clear understanding of what features would be beneficial in this task. To address these, we present a novel multi-dataset supervised contrastive learning technique to learn discriminative features simultaneously from multiple video datasets with significantly diverse distributions. Extensive empirical analyses demonstrate the promise of our approach compared to competing techniques on this challenging task",
    "checked": false,
    "id": "f4ed8030292fff494b591868fb8c1019cd054347",
    "semantic_title": "early-stage autism diagnosis using action videos and contrastive feature learning",
    "citation_count": 0,
    "authors": [
      "Asha Rani",
      "Yashaswi Verma"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ye_Label_Shift_Estimation_for_Class-Imbalance_Problem_A_Bayesian_Approach_WACV_2024_paper.html": {
    "title": "Label Shift Estimation for Class-Imbalance Problem: A Bayesian Approach",
    "volume": "main",
    "abstract": "As a type of distribution shift, label shift occurs when the source and target domains have different label distributions P(Y) but identical conditional distributions of data given labels P(X | Y). Under a Bayesian framework, we propose a novel Maximum A Posteriori (MAP) model and a novel posterior sampling model for the label shift problem. We prove the MAP objective admits a unique optimum and derive an EM algorithm that converges to the global optimum. We propose a novel Adaptive Prior Learning (APL) model to adaptively select prior parameters given data. We use the Markov Chain Monte Carlo (MCMC) method in our posterior sampling model to estimate and correct for label shift. Our methods can effectively resolve class imbalance problems on large-scale datasets without fine-tuning the classifier. Experiments show that our model outperforms existing methods on a variety of label shift settings. Our code is available at https://github.com/ChangkunYe/MAPLS/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changkun Ye",
      "Russell Tsuchida",
      "Lars Petersson",
      "Nick Barnes"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Adam_SeaTurtleID2022_A_Long-Span_Dataset_for_Reliable_Sea_Turtle_Re-Identification_WACV_2024_paper.html": {
    "title": "SeaTurtleID2022: A Long-Span Dataset for Reliable Sea Turtle Re-Identification",
    "volume": "main",
    "abstract": "This paper introduces the first public large-scale, long-span dataset with sea turtle photographs captured in the wild - SeaTurtleID2022. The dataset contains 8729 photographs of 438 unique individuals collected within 13 years, making it the longest-spanned dataset for animal re-identification. Each photograph includes various annotations, e.g., identity, encounter timestamp, and body parts segmentation masks. Instead of a standard \"random\" split, the dataset allows for two realistic and ecologically motivated splits: (i) time-aware: a closed-set with training, validation, and test data from different days/years, and (ii) open-set: with new unknown individuals in test and validation sets. We show that time-aware splits are essential for benchmarking methods for re-identification, as random splits lead to performance overestimation. Furthermore, a baseline instance segmentation and re-identification performance over various body parts is provided. At last, an end-to-end system for sea turtle re-identification is proposed and evaluated. The proposed system based on Hybrid Task Cascade for head instance segmentation and ArcFace-trained feature-extractor achieved an accuracy of 86.8%",
    "checked": true,
    "id": "47c5259bedc4e41aa99de1242752f36275c84d53",
    "semantic_title": "seaturtleid2022: a long-span dataset for reliable sea turtle re-identification",
    "citation_count": 1,
    "authors": [
      "Lukáš Adam",
      "Vojtěch Čermák",
      "Kostas Papafitsoros",
      "Lukas Picek"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Xu_Self-Supervised_Edge_Detection_Reconstruction_for_Topology-Informed_3D_Axon_Segmentation_and_WACV_2024_paper.html": {
    "title": "Self-Supervised Edge Detection Reconstruction for Topology-Informed 3D Axon Segmentation and Centerline Detection",
    "volume": "main",
    "abstract": "Many machine learning-based axon tracing methods rely on image datasets with segmentation labels. This requires manual annotation from domain experts, which is labor-intensive and not practical for large-scale brain mapping on hemisphere or whole brain tissue at cellular or sub-cellular resolution. Additionally, preserving axon structure topology is crucial to understanding neural connections and brain function. Self-supervised learning (SSL) is a machine learning framework that allows models to learn an auxiliary task on unannotated data to aid performance on a supervised target task. In this work, we propose a novel SSL auxiliary task of reconstructing an edge detector for the target task of topology-oriented axon segmentation and centerline detection. We pretrained 3D U-Nets on three different SSL tasks using a mouse brain dataset: our proposed task, predicting the order of permuted slices, and playing a Rubik's cube. We then evaluated these U-Nets and a baseline model on a different mouse brain dataset. Across all experiments, the U-Net pretrained on our proposed task improved the baseline's segmentation, topology-preservation, and centerline detection by up to 5.03%, 4.65%, and 5.41%, respectively. In contrast, there was no consistent improvement over the baseline observed with the slice-permutation and Rubik's cube pretrained U-Nets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alec S. Xu",
      "Nina I. Shamsi",
      "Lars A. Gjesteby",
      "Laura J. Brattain"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Liu_Bi-Directional_Training_for_Composed_Image_Retrieval_via_Text_Prompt_Learning_WACV_2024_paper.html": {
    "title": "Bi-Directional Training for Composed Image Retrieval via Text Prompt Learning",
    "volume": "main",
    "abstract": "Composed image retrieval searches for a target image based on a multi-modal user query comprised of a reference image and modification text describing the desired changes. Existing approaches to solving this challenging task learn a mapping from the (reference image, modification text)-pair to an image embedding that is then matched against a large image corpus. One area that has not yet been explored is the reverse direction, which asks the question, what reference image when modified as described by the text would produce the given target image? In this work we propose a bi-directional training scheme that leverages such reversed queries and can be applied to existing composed image retrieval architectures with minimum changes, which improves the performance of the model. To encode the bi-directional query we prepend a learnable token to the modification text that designates the direction of the query and then finetune the parameters of the text embedding module. We make no other changes to the network architecture. Experiments on two standard datasets show that our novel approach achieves improved performance over a baseline BLIP-based model that itself already achieves competitive performance. Our code is released at https://github.com/Cuberick-Orion/Bi-Blip4CIR",
    "checked": true,
    "id": "690d87c9055c5a42ba25380603a82931a77e932c",
    "semantic_title": "bi-directional training for composed image retrieval via text prompt learning",
    "citation_count": 5,
    "authors": [
      "Zheyuan Liu",
      "Weixuan Sun",
      "Yicong Hong",
      "Damien Teney",
      "Stephen Gould"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Jing_iBARLE_imBalance-Aware_Room_Layout_Estimation_WACV_2024_paper.html": {
    "title": "iBARLE: imBalance-Aware Room Layout Estimation",
    "volume": "main",
    "abstract": "Room layout estimation predicts layouts from a single panorama. It requires datasets with large-scale and diverse room shapes to well train the models. However, there are significant imbalances in real-world datasets including the dimensions of layout complexity, camera locations, and variation in scene appearance. These issues considerably influence the model training performance. In this work, we propose imBalance-Aware Room Layout Estimation (iBARLE) framework to address these issues. iBARLE consists of: (1) Appearance Variation Generation (AVG) module, which promotes visual appearance domain generalization, (2) Complex Structure Mix-up (CSMix) module, which enhances generalizability w.r.t. room structure, and (3) a gradient-based layout objective function, which allows more effective accounting for occlusions in complex layouts. All modules are jointly trained and help each other to achieve the best performance. Experiments and ablation studies based on ZInD dataset illustrate that iBARLE has state-of-the-art performance compared with other layout estimation baselines",
    "checked": true,
    "id": "c2743e5addf9ff0a41295e8535884eb0e3436c12",
    "semantic_title": "ibarle: imbalance-aware room layout estimation",
    "citation_count": 0,
    "authors": [
      "Taotao Jing",
      "Lichen Wang",
      "Naji Khosravan",
      "Zhiqiang Wan",
      "Zachary Bessinger",
      "Zhengming Ding",
      "Sing Bing Kang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Liu_FarSight_A_Physics-Driven_Whole-Body_Biometric_System_at_Large_Distance_and_WACV_2024_paper.html": {
    "title": "FarSight: A Physics-Driven Whole-Body Biometric System at Large Distance and Altitude",
    "volume": "main",
    "abstract": "Whole-body biometric recognition is an important area of research due to its vast applications in law enforcement, border security, and surveillance. This paper presents the end-to-end design, development and evaluation of FarSight, an innovative software system designed for whole-body (fusion of face, gait and body shape) biometric recognition. FarSight accepts videos from elevated platforms and drones as input and outputs a candidate list of identities from a gallery. The system is designed to address several challenges, including (i) low-quality imagery, (ii) large yaw and pitch angles, (iii) robust feature extraction to accommodate large intra-person variabilities and large inter-person similarities, and (iv) the large domain gap between training and test sets. FarSight combines the physics of imaging and deep learning models to enhance image restoration and biometric feature encoding. We test FarSight's effectiveness using the newly acquired IARPA Biometric Recognition and Identification at Altitude and Range (BRIAR) dataset. Notably, FarSight demonstrated a substantial performance increase on the BRIAR dataset, with gains of +11.82% Rank-20 identification and +11.3% TAR@1%FAR",
    "checked": true,
    "id": "018fd91891b8c3779ea195e2d4deb1933df53e54",
    "semantic_title": "farsight: a physics-driven whole-body biometric system at large distance and altitude",
    "citation_count": 4,
    "authors": [
      "Feng Liu",
      "Ryan Ashbaugh",
      "Nicholas Chimitt",
      "Najmul Hassan",
      "Ali Hassani",
      "Ajay Jaiswal",
      "Minchul Kim",
      "Zhiyuan Mao",
      "Christopher Perry",
      "Zhiyuan Ren",
      "Yiyang Su",
      "Pegah Varghaei",
      "Kai Wang",
      "Xingguang Zhang",
      "Stanley Chan",
      "Arun Ross",
      "Humphrey Shi",
      "Zhangyang Wang",
      "Anil Jain",
      "Xiaoming Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Rothmeier_Time_To_Shine_Fine-Tuning_Object_Detection_Models_With_Synthetic_Adverse_WACV_2024_paper.html": {
    "title": "Time To Shine: Fine-Tuning Object Detection Models With Synthetic Adverse Weather Images",
    "volume": "main",
    "abstract": "The detection of vehicles, pedestrians, and obstacles plays an important role in the decision-making process of autonomous vehicles. While existing methods achieve high detection accuracy under good environmental conditions, they often fail in adverse weather conditions due to limited visibility, blurred contours, and low contrast. These \"edge-case\" scenarios are not well represented in existing datasets and are not handled properly by object detection algorithms. In our work, we propose a novel approach to synthesising photorealistic and highly diverse scenarios that can be used to fine-tune object detection algorithms in adverse weather conditions such as snow, fog, and rain. The approach uses the Midjourney text-to-image model to create accurate synthetic images of desired weather conditions. Our experiments show that training with our dataset significantly improves detection accuracy in harsh weather conditions. Our results are compared to baseline models and models fine-tuned on augmented clear weather images",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Rothmeier",
      "Werner Huber",
      "Alois C. Knoll"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Chakraborty_Unsupervised_and_Semi-Supervised_Co-Salient_Object_Detection_via_Segmentation_Frequency_Statistics_WACV_2024_paper.html": {
    "title": "Unsupervised and Semi-Supervised Co-Salient Object Detection via Segmentation Frequency Statistics",
    "volume": "main",
    "abstract": "In this paper, we address the detection of co-occurring salient objects (CoSOD) in an image group using frequency statistics in an unsupervised manner, which further enable us to develop a semi-supervised method. While previous works have mostly focused on fully supervised CoSOD, less attention has been allocated to detecting co-salient objects when limited segmentation annotations are available for training. Our simple yet effective unsupervised method US-CoSOD combines the object co-occurrence frequency statistics of unsupervised single-image semantic segmentations with salient foreground detections using self-supervised feature learning. For the first time, we show that a large unlabeled dataset e.g. ImageNet-1k can be effectively leveraged to significantly improve unsupervised CoSOD performance. Our unsupervised model is a great pre-training initialization for our semi-supervised model SS-CoSOD, especially when very limited labeled data is available for training. To avoid propagating erroneous signals from predictions on unlabeled data, we propose a confidence estimation module to guide our semi-supervised training. Extensive experiments on three CoSOD benchmark datasets show that both of our unsupervised and semi-supervised models outperform the corresponding state-of-the-art models by a significant margin (e.g., on the Cosal2015 dataset, our US-CoSOD model has an 8.8% F-measure gain over a SOTA unsupervised co-segmentation model and our SS-CoSOD model has an 11.81% F-measure gain over a SOTA semi-supervised CoSOD model)",
    "checked": true,
    "id": "8f7bd404e9968798d57a02314a74690bf5fd8faf",
    "semantic_title": "unsupervised and semi-supervised co-salient object detection via segmentation frequency statistics",
    "citation_count": 0,
    "authors": [
      "Souradeep Chakraborty",
      "Shujon Naha",
      "Muhammet Bastan",
      "Amit Kumar K. C.",
      "Dimitris Samaras"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Yasarla_3SD_Self-Supervised_Saliency_Detection_With_No_Labels_WACV_2024_paper.html": {
    "title": "3SD: Self-Supervised Saliency Detection With No Labels",
    "volume": "main",
    "abstract": "We present a conceptually simple self-supervised method for saliency detection. Our method generates and uses pseudo-ground truth labels for training. The generated pseudo-GT labels don't require any kind of human annotations (e.g., pixel-wise labels or weak labels like scribbles). Recent works show that features extracted from classification tasks provide important saliency cues like structure and semantic information of salient objects in the image. Our method, called 3SD, exploits this idea by adding a branch for a self-supervised classification task in parallel with salient object detection, to obtain class activation maps (CAM maps). These CAM maps along with the edges of the input image are used to generate the pseudo-GT saliency maps to train our 3SD network. Specifically, we propose a contrastive learning-based training on multiple image patches for the classification task. We show the multi-patch classification with contrastive loss improves the quality of the CAM maps compared to naive classification on the entire image. Experiments on six benchmark datasets demonstrate that without any labels, our 3SD method outperforms all existing weakly supervised and unsupervised methods, and its performance is on par with the fully-supervised methods",
    "checked": true,
    "id": "2a78e1c0412cbcc851ba60224c15c501debe2049",
    "semantic_title": "3sd: self-supervised saliency detection with no labels",
    "citation_count": 3,
    "authors": [
      "Rajeev Yasarla",
      "Renliang Weng",
      "Wongun Choi",
      "Vishal M. Patel",
      "Amir Sadeghian"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Chen_Pixel_Matching_Network_for_Cross-Domain_Few-Shot_Segmentation_WACV_2024_paper.html": {
    "title": "Pixel Matching Network for Cross-Domain Few-Shot Segmentation",
    "volume": "main",
    "abstract": "Few-Shot Segmentation (FSS) aims to segment the novel class images with a few annotated samples. In the past, numerous studies have concentrated on cross-category tasks, where the training and testing sets are derived from the same dataset, while these methods face significant difficulties in domain-shift scenarios. To better tackle the cross-domain tasks, we propose a pixel matching network (PMNet) to extract the domain-agnostic pixel-level affinity matching with a frozen backbone and capture both the pixel-to-pixel and pixel-to-patch relations in each support-query pair with the bidirectional 3D convolutions. Different from the existing methods that remove the support background, we design a hysteretic spatial filtering module (HSFM) to filter the background-related query features and retain the foreground-related query features with the assistance of the support background, which is beneficial for eliminating interference objects in the query background. We comprehensively evaluate our PMNet on ten benchmarks under cross-category, cross-dataset, and cross-domain FSS tasks. Experimental results demonstrate that PMNet performs very competitively under different settings with only 0.68M parameters, especially under cross-domain FSS tasks, showing its effectiveness and efficiency",
    "checked": false,
    "id": "97979cec79adf3cec0295189f572830771dc40cf",
    "semantic_title": "pseudo-interacting guided network for few-shot segmentation",
    "citation_count": 0,
    "authors": [
      "Hao Chen",
      "Yonghan Dong",
      "Zheming Lu",
      "Yunlong Yu",
      "Jungong Han"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Tan_Cross-Domain_Few-Shot_Incremental_Learning_for_Point-Cloud_Recognition_WACV_2024_paper.html": {
    "title": "Cross-Domain Few-Shot Incremental Learning for Point-Cloud Recognition",
    "volume": "main",
    "abstract": "Sensing 3D objects is critical when 2D object recognition is not accessible. A robot pre-trained on a large point-cloud dataset will encounter unseen classes of 3D objects after deploying it. Therefore, the robot should be able to learn continuously in real-world scenarios. Few-shot class-incremental learning (FSCIL) requires the model to learn from few-shot new examples continually and not forget past classes. However, there is an implicit but strong assumption in the FSCIL that the distribution of the base and incremental classes is the same. In this paper, we focus on cross-domain FSCIL for point-cloud recognition. We decompose the catastrophic forgetting into base class forgetting and incremental class forgetting and alleviate them separately. We utilize the base model to discriminate base samples and new samples by treating base samples as in-distribution samples, and new objects as out-of-distribution samples. We retain the base model to avoid catastrophic forgetting of base classes and train an extra domain-specific module for all new samples to adapt to new classes. At inference, we first discriminate whether the sample belongs to the base class or the new class. Once classified at the model level, test samples are then passed to the corresponding model for class-level classification. To better mitigate the forgetting of new classes, we adopt the soft label and hard label replay together. Extensive experiments on synthetic-to-real incremental 3D datasets show that our proposed method can balance the performance between the base and new objects and outperforms the previous state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuwen Tan",
      "Xiang Xiang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Jang_Robust_Unsupervised_Domain_Adaptation_Through_Negative-View_Regularization_WACV_2024_paper.html": {
    "title": "Robust Unsupervised Domain Adaptation Through Negative-View Regularization",
    "volume": "main",
    "abstract": "In the realm of Unsupervised Domain Adaptation (UDA), Vision Transformers (ViTs) have recently demonstrated remarkable adaptability surpassing that of traditional Convolutional Neural Networks (CNNs). Nevertheless, the patch-based structure of ViTs heavily relies on local features within image patches, potentially leading to reduced robustness when confronted with out-of-distribution (OOD) samples. To address this concern, we introduce a novel regularizer tailored specifically for UDA. By leveraging negative views, i.e. target-domain samples applied by negative augmentations, we make the learning process more intricate, thereby preventing models from taking shortcuts in spatial context recognition. We present a novel loss function, rooted in contrastive principles, to effectively distinguish between the negative views and original target samples. By integrating this novel regularizer with existing UDA methodologies, we guide ViTs to prioritize context relationships among local patches, thereby enhancing the robustness of ViTs. Our proposed Negative View-based Contrastive (NVC) regularizer substantially boosts the performance of baseline UDA methods across diverse benchmark datasets. Furthermore, we release new dataset, Retail-71, comprising 71 classes of images commonly encountered in retail stores. Through comprehensive experimentation, we showcase the effectiveness of our approach on traditional benchmarks as well as the novel retail domain. These results substantiate the robust adaptation capabilities of our proposed method. Our method is implemented at our repository",
    "checked": true,
    "id": "071ff892c2a02a23a39fb66bbd8fb455cc20d680",
    "semantic_title": "robust unsupervised domain adaptation through negative-view regularization",
    "citation_count": 0,
    "authors": [
      "Joonhyeok Jang",
      "Sunhyeok Lee",
      "Seonghak Kim",
      "Jung-un Kim",
      "Seonghyun Kim",
      "Daeshik Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Katsumata_Soft_Curriculum_for_Learning_Conditional_GANs_With_Noisy-Labeled_and_Uncurated_WACV_2024_paper.html": {
    "title": "Soft Curriculum for Learning Conditional GANs With Noisy-Labeled and Uncurated Unlabeled Data",
    "volume": "main",
    "abstract": "Label-noise or curated unlabeled data are used to compensate for the assumption of clean labeled data in training the conditional generative adversarial network; however, satisfying such an extended assumption is occasionally laborious or impractical. As a step towards generative modeling accessible to everyone, we introduce a novel conditional image generation framework that accepts noisy-labeled and uncurated unlabeled data during training: (i) closed-set and open-set label noise in labeled data and (ii) closed-set and open-set unlabeled data. To combat it, we propose soft curriculum learning, which assigns instance-wise weights for adversarial training while assigning new labels for unlabeled data and correcting wrong labels for labeled data. Unlike popular curriculum learning, which uses a threshold to pick the training samples, our soft curriculum controls the effect of each training instance by using the weights predicted by the auxiliary classifier, resulting in the preservation of useful samples while ignoring harmful ones. Our experiments show that our approach outperforms existing semi-supervised and label-noise robust methods in terms of both quantitative and qualitative performance. In particular, the proposed approach matches the performance of (semi-)supervised GANs even with less than half the labeled data",
    "checked": true,
    "id": "0d12bb148b7b8490d2c79ba73296bd2107e9488a",
    "semantic_title": "soft curriculum for learning conditional gans with noisy-labeled and uncurated unlabeled data",
    "citation_count": 0,
    "authors": [
      "Kai Katsumata",
      "Duc Minh Vo",
      "Tatsuya Harada",
      "Hideki Nakayama"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Duran_HMP_Hand_Motion_Priors_for_Pose_and_Shape_Estimation_From_WACV_2024_paper.html": {
    "title": "HMP: Hand Motion Priors for Pose and Shape Estimation From Video",
    "volume": "main",
    "abstract": "Understanding how humans interact with the world necessitates accurate 3D hand pose estimation, a task complicated by the hand's high degree of articulation, frequent occlusions, self-occlusions, and rapid motions. While most existing methods rely on single-image inputs, videos have useful cues to address aforementioned issues. However, existing video-based 3D hand datasets are insufficient for training feedforward models to generalize to in-the-wild scenarios. On the other hand, we have access to large human motion capture datasets which also include hand motions, e.g. AMASS. Therefore, we develop a generative motion prior specific for hands, trained on the AMASS dataset which features diverse and high-quality hand motions. This motion prior is then employed for video-based 3D hand motion estimation following a latent optimization approach. Our integration of a robust motion prior significantly enhances performance, especially in occluded scenarios. It produces stable, temporally consistent results that surpass conventional single-frame methods. We demonstrate our method's efficacy via qualitative and quantitative evaluations on the HO3D and DexYCB datasets, with special emphasis on an occlusion-focused subset of HO3D. Code is available at https://hmp.is.tue.mpg.de",
    "checked": true,
    "id": "9da11bcdea67a605aa4bcbb5d17df977f5694d6e",
    "semantic_title": "hmp: hand motion priors for pose and shape estimation from video",
    "citation_count": 0,
    "authors": [
      "Enes Duran",
      "Muhammed Kocabas",
      "Vasileios Choutas",
      "Zicong Fan",
      "Michael J. Black"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ao_Amodal_Intra-Class_Instance_Segmentation_Synthetic_Datasets_and_Benchmark_WACV_2024_paper.html": {
    "title": "Amodal Intra-Class Instance Segmentation: Synthetic Datasets and Benchmark",
    "volume": "main",
    "abstract": "Images of realistic scenes often contain intra-class objects that are heavily occluded from each other, making the amodal perception task that requires parsing the occluded parts of the objects challenging. Although important for downstream tasks such as robotic grasping systems, the lack of large-scale amodal datasets with detailed annotations makes it difficult to model intra-class occlusions explicitly. This paper introduces two new amodal datasets for image amodal completion tasks, which contain a total of over 267K images of intra-class occlusion scenarios, annotated with multiple masks, amodal bounding boxes, dual order relations and full appearance for instances and background. We also present a point-supervised scheme with layer priors for amodal instance segmentation specifically designed for intra-class occlusion scenarios. Experiments show that our weakly supervised approach outperforms the SOTA fully supervised methods, while our layer priors design exhibits remarkable performance improvements in the case of intra-class occlusion in both synthetic and real images",
    "checked": true,
    "id": "0fa9e3398b9e177a088b96ea30074f623d75069d",
    "semantic_title": "amodal intra-class instance segmentation: synthetic datasets and benchmark",
    "citation_count": 0,
    "authors": [
      "Jiayang Ao",
      "Qiuhong Ke",
      "Krista A. Ehinger"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Cho_RMFER_Semi-Supervised_Contrastive_Learning_for_Facial_Expression_Recognition_With_Reaction_WACV_2024_paper.html": {
    "title": "RMFER: Semi-Supervised Contrastive Learning for Facial Expression Recognition With Reaction Mashup Video",
    "volume": "main",
    "abstract": "Facial expression recognition (FER) has greatly benefited from deep learning but still faces challenges in dataset collection due to the nuanced nature of facial expressions. In this study, we present a novel unlabeled dataset and semi-supervised contrastive learning framework that utilizes Reaction Mashup (RM) videos, a video that includes multiple individuals reacting to the same film. We created a Reaction Mashup dataset (RMset) from these videos. Our framework integrates three distinct modules: A classification module for supervised facial expression categorization, an attention module for inter-sample attention learning, and a contrastive module for attention-based contrastive learning using RMset. We utilize both the classification and attention modules for the initial training, subsequently incorporating the contrastive module to enhance the learning process. Our experiments demonstrate that our method improves feature learning and outperforms state-of-the-art models on three benchmark FER datasets. Codes are available at https://github.com/yunseongcho/RMFER",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunseong Cho",
      "Chanwoo Kim",
      "Hoseong Cho",
      "Yunhoe Ku",
      "Eunseo Kim",
      "Muhammadjon Boboev",
      "Joonseok Lee",
      "Seungryul Baek"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Banerjee_AMEND_Adaptive_Margin_and_Expanded_Neighborhood_for_Efficient_Generalized_Category_WACV_2024_paper.html": {
    "title": "AMEND: Adaptive Margin and Expanded Neighborhood for Efficient Generalized Category Discovery",
    "volume": "main",
    "abstract": "Generalized Category Discovery aims to discover and cluster images from previously unseen classes, in addition to classifying images from seen classes correctly. In this work, we propose a simple, yet effective framework for this task, which not only performs on-par or better with the current approaches but is also significantly more efficient in terms of computational requirements. Our first contribution is to use expanded neighborhood information in contrastive learning to generate robust and generalizable features. To generate more discriminative feature representations, especially for fine-grained datasets and confusing classes, we propose a class-wise adaptive margin regularizer that aims at increasing the angular separation among the prototypes of all classes. Extensive experiments on three generic as well as four fine-grained benchmark datasets show the usefulness of the proposed Adaptive Margin and Expanded Neighborhood (AMEND) framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anwesha Banerjee",
      "Liyana Sahir Kallooriyakath",
      "Soma Biswas"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Siddiquee_Brainomaly_Unsupervised_Neurologic_Disease_Detection_Utilizing_Unannotated_T1-Weighted_Brain_MR_WACV_2024_paper.html": {
    "title": "Brainomaly: Unsupervised Neurologic Disease Detection Utilizing Unannotated T1-Weighted Brain MR Images",
    "volume": "main",
    "abstract": "Harnessing the power of deep neural networks in the medical imaging domain is challenging due to the difficulties in acquiring large annotated datasets, especially for rare diseases, which involve high costs, time, and effort for annotation. Unsupervised disease detection methods, such as anomaly detection, can significantly reduce human effort in these scenarios. While anomaly detection typically focuses on learning from images of healthy subjects only, real-world situations often present unannotated datasets with a mixture of healthy and diseased subjects. Recent studies have demonstrated that utilizing such unannotated images can improve unsupervised disease and anomaly detection. However, these methods do not utilize knowledge specific to registered neuroimages, resulting in a subpar performance in neurologic disease detection. To address this limitation, we propose Brainomaly, a GAN-based image-to-image translation method specifically designed for neurologic disease detection. Brainomaly not only offers tailored image-to-image translation suitable for neuroimages but also leverages unannotated mixed images to achieve superior neurologic disease detection. Additionally, we address the issue of model selection for inference without annotated samples by proposing a pseudo-AUC metric, further enhancing Brainomaly's detection performance. Extensive experiments and ablation studies demonstrate that Brainomaly outperforms existing state-of-the-art unsupervised disease and anomaly detection methods by significant margins in Alzheimer's disease detection using a publicly available dataset and headache detection using an institutional dataset. The code is available from https://github.com/mahfuzmohammad/Brainomaly",
    "checked": true,
    "id": "191e94ac1aa0aed64d024f8abc7212fef4b99d09",
    "semantic_title": "brainomaly: unsupervised neurologic disease detection utilizing unannotated t1-weighted brain mr images",
    "citation_count": 2,
    "authors": [
      "Md Mahfuzur Rahman Siddiquee",
      "Jay Shah",
      "Teresa Wu",
      "Catherine Chong",
      "Todd J. Schwedt",
      "Gina Dumkrieger",
      "Simona Nikolova",
      "Baoxin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/De_Plaen_Contrastive_Learning_for_Multi-Object_Tracking_With_Transformers_WACV_2024_paper.html": {
    "title": "Contrastive Learning for Multi-Object Tracking With Transformers",
    "volume": "main",
    "abstract": "The DEtection TRansformer (DETR) opened new possibilities for object detection by modeling it as a translation task: converting image features into object-level representations. Previous works typically add expensive modules to DETR to perform Multi-Object Tracking (MOT), resulting in more complicated architectures. We instead show how DETR can be turned into a MOT model by employing an instance-level contrastive loss, a revised sampling strategy and a lightweight assignment method. Our training scheme learns object appearances while preserving detection capabilities and with little overhead. Its performance surpasses the previous state-of-the-art by +2.6 mMOTA on the challenging BDD100K dataset and is comparable to existing transformer-based methods on the MOT17 dataset",
    "checked": true,
    "id": "a48cd2f7dbcaec476ff330ab32e9afa8dddceb43",
    "semantic_title": "contrastive learning for multi-object tracking with transformers",
    "citation_count": 0,
    "authors": [
      "Pierre-François De Plaen",
      "Nicola Marinello",
      "Marc Proesmans",
      "Tinne Tuytelaars",
      "Luc Van Gool"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Chang_BEVMap_Map-Aware_BEV_Modeling_for_3D_Perception_WACV_2024_paper.html": {
    "title": "BEVMap: Map-Aware BEV Modeling for 3D Perception",
    "volume": "main",
    "abstract": "In autonomous driving applications, there is a strong preference for modeling the world in Bird's-Eye View (BEV), as it leads to improved accuracy and performance. BEV features are widely used in perception tasks since they allow fusing information from multiple views in an efficient manner. However, BEV features generated from camera images are prone to be imprecise due to the difficulty of estimating depth in the perspective view. Improper placement of BEV features limits the accuracy of downstream tasks. We introduce a method for incorporating map information to improve perspective depth estimation from 2D camera images and thereby producing geometrically- and semantically-robust BEV features. We show that augmenting the camera images with the BEV map and map-to-camera projections can compensate for the depth uncertainty. Experiments on the nuScenes dataset demonstrate that our method outperforms previous approaches using only camera images in segmentation and detection tasks",
    "checked": false,
    "id": "89bdd8f7cdbc622b1a43eee102049126d19f4c99",
    "semantic_title": "occlubev: occlusion aware spatiotemporal modeling for multi-view 3d object detection",
    "citation_count": 0,
    "authors": [
      "Mincheol Chang",
      "Seokha Moon",
      "Reza Mahjourian",
      "Jinkyu Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Clemmer_PreciseDebias_An_Automatic_Prompt_Engineering_Approach_for_Generative_AI_To_WACV_2024_paper.html": {
    "title": "PreciseDebias: An Automatic Prompt Engineering Approach for Generative AI To Mitigate Image Demographic Biases",
    "volume": "main",
    "abstract": "Recent years have witnessed growing concerns over demographic biases in image-centric applications, including image search engines and generative systems. While the advent of generative AI offers a pathway to mitigate these biases by producing underrepresented images, existing solutions still fail to precisely generate images that reflect specified demographic distributions. In this paper, we propose PreciseDebias, a comprehensive end-to-end framework that can rectify demographic bias in image generation. By leveraging fine-tuned Large Language Models (LLMs) coupled with text-to-image generative models, PreciseDebias transforms generic text prompts to produce images in line with specified demographic distributions. The core component of PreciseDebias is our novel instruction-following LLM, meticulously designed with an emphasis on model bias assessment and balanced model training. Extensive experiments demonstrate the effectiveness of PreciseDebias in rectifying biases pertaining to both ethnicity and gender in images. Furthermore, when compared with two baselines, PreciseDebias illustrates its robustness and capability to capture demographic intricacies. The generalization of PreciseDebias is further illuminated by the diverse images it produces across multiple professions and demographic attributes. To ensure reproducibility, we will make PreciseDebias openly accessible to the broader research community by releasing all models and code",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Colton Clemmer",
      "Junhua Ding",
      "Yunhe Feng"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Sarkar_Benchmark_Generation_Framework_With_Customizable_Distortions_for_Image_Classifier_Robustness_WACV_2024_paper.html": {
    "title": "Benchmark Generation Framework With Customizable Distortions for Image Classifier Robustness",
    "volume": "main",
    "abstract": "We present a novel framework for generating adversarial benchmarks to evaluate the robustness of image classification models. The RLAB framework allows users to customize the types of distortions to be optimally applied to images, which helps address the specific distortions relevant to their deployment. The benchmark can generate datasets at various distortion levels to assess the robustness of different image classifiers. Our results show that the adversarial samples generated by our framework with any of the image classification models, like ResNet-50, Inception-V3, and VGG-16, are effective and transferable to other models causing them to fail. These failures happen even when these models are adversarially retrained using state-of-the-art techniques, demonstrating the generalizability of our adversarial samples. Our framework also allows the creation of adversarial samples for non-ground truth classes at different levels of intensity, enabling tunable benchmarks for the evaluation of false positives. We achieve competitive performance in terms of net L_2 distortion compared to state-of-the-art benchmark techniques on CIFAR-10 and ImageNet; however, we demonstrate our framework achieves such results with simple distortions like Gaussian noise without introducing unnatural artifacts or color bleeds. This is made possible by a model-based reinforcement learning (RL) agent and a technique that reduces a deep tree search of the image for model sensitivity to perturbations, to a one-level analysis and action. The flexibility of choosing distortions and setting classification probability thresholds for multiple classes makes our framework suitable for algorithmic audits",
    "checked": true,
    "id": "257bab6daed684561398565894087128ff7a5857",
    "semantic_title": "benchmark generation framework with customizable distortions for image classifier robustness",
    "citation_count": 0,
    "authors": [
      "Soumyendu Sarkar",
      "Ashwin Ramesh Babu",
      "Sajad Mousavi",
      "Zachariah Carmichael",
      "Vineet Gundecha",
      "Sahand Ghorbanpour",
      "Ricardo Luna Gutierrez",
      "Antonio Guillen",
      "Avisek Naug"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Qiu_Shape-Biased_CNNs_Are_Not_Always_Superior_in_Out-of-Distribution_Robustness_WACV_2024_paper.html": {
    "title": "Shape-Biased CNNs Are Not Always Superior in Out-of-Distribution Robustness",
    "volume": "main",
    "abstract": "In recent years, Out-of-Distribution (o.o.d) Robustness has garnered increasing attention in Deep Learning, and shape-biased Convolutional Neural Networks (CNNs) are believed to exhibit higher robustness, attributed to the inherent shape-based decision rule of human cognition. In this work, we delve deeper into the intricate relationship between shape/texture information and o.o.d robustness by leveraging a carefully curated \"Category-Balanced ImageNet\" dataset. We find that shape information is not always superior in distinguishing distinct categories and shape-biased model is not always superior across various o.o.d scenarios. Motivated by these insightful findings, we design a novel method named Shape-Texture Adaptive Recombination (STAR) to achieve higher o.o.d robustness. A category-balanced dataset is firstly used to pretrain a debiased backbone and three specialized heads, each adept at robustly extracting shape, texture, and debiased features. Subsequently, an instance-adaptive recombination head is trained to adaptively adjust the contributions of these distinctive features for each given instance. Through comprehensive experiments, our proposed method achieves state-of-the-art o.o.d robustness across various scenarios such as image corruptions, adversarial attacks, style shifts, and dataset shifts, demonstrating its effectiveness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinkuan Qiu",
      "Meina Kan",
      "Yongbin Zhou",
      "Yanchao Bi",
      "Shiguang Shan"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Lu_Towards_Visual_Saliency_Explanations_of_Face_Verification_WACV_2024_paper.html": {
    "title": "Towards Visual Saliency Explanations of Face Verification",
    "volume": "main",
    "abstract": "In the past years, deep convolutional neural networks have been pushing the frontier of face recognition (FR) techniques in both verification and identification scenarios. Despite the high accuracy, they are often criticized for lacking explainability. There has been an increasing demand for understanding the decision-making process of deep face recognition systems. Recent studies have investigated the usage of visual saliency maps as an explanation, but they often lack a discussion and analysis in the context of face recognition. This paper concentrates on explainable face verification tasks and conceives a new explanation framework. Firstly, a definition of the saliency-based explanation method is provided, which focuses on the decisions made by the deep FR model. Secondly, a new model-agnostic explanation method named CorrRISE is proposed to produce saliency maps, which reveal both the similar and dissimilar regions of any given pair of face images. Then, an evaluation methodology is designed to measure the performance of general visual saliency explanation methods in face verification. Finally, substantial visual and quantitative results have shown that the proposed CorrRISE method demonstrates promising results in comparison with other state-of-the-art explainable face verification approaches",
    "checked": true,
    "id": "8ac875015cbefbec05a0d829202ced456bd9222f",
    "semantic_title": "towards visual saliency explanations of face verification",
    "citation_count": 1,
    "authors": [
      "Yuhang Lu",
      "Zewei Xu",
      "Touradj Ebrahimi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Huber_Bias_and_Diversity_in_Synthetic-Based_Face_Recognition_WACV_2024_paper.html": {
    "title": "Bias and Diversity in Synthetic-Based Face Recognition",
    "volume": "main",
    "abstract": "Synthetic data is emerging as a substitute for authentic data to solve ethical and legal challenges in handling authentic face data. The current models can create real-looking face images of people who do not exist. However, it is a known and sensitive problem that face recognition systems are susceptible to bias, i.e. performance differences between different demographic and non-demographics attributes, which can lead to unfair decisions. In this work, we investigate how the diversity of synthetic face recognition datasets compares to authentic datasets, and how the distribution of the training data of the generative models affects the distribution of the synthetic data. To do this, we looked at the distribution of gender, ethnicity, age, and head position. Furthermore, we investigated the concrete bias of three recent synthetic-based face recognition models on the studied attributes in comparison to a baseline model trained on authentic data. Our results show that the generator generate a similar distribution as the used training data in terms of the different attributes. With regard to bias, it can be seen that the synthetic-based models share a similar bias behavior with the authentic-based models. However, with the uncovered lower intra-identity attribute consistency seems to be beneficial in reducing bias",
    "checked": true,
    "id": "3fd4c19b49143e5a72877028f028b679fbdb5445",
    "semantic_title": "bias and diversity in synthetic-based face recognition",
    "citation_count": 0,
    "authors": [
      "Marco Huber",
      "Anh Thi Luu",
      "Fadi Boutros",
      "Arjan Kuijper",
      "Naser Damer"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Deshmukh_Textual_Alchemy_CoFormer_for_Scene_Text_Understanding_WACV_2024_paper.html": {
    "title": "Textual Alchemy: CoFormer for Scene Text Understanding",
    "volume": "main",
    "abstract": "The paper presents CoFormer (Convolutional Fourier Transformer), a robust and adaptable transformer architecture designed for a range of scene text tasks. CoFormer integrates convolution and Fourier operations into the transformer architecture. Thus, it leverages convolution properties such as shared weights, local receptive fields, and spatial subsampling, while the Fourier operation emphasizes composite characteristics from the frequency domain. The research further proposes the first pretraining datasets, named Textverse10M-E and Textverse10M-H. Using these datasets, we demonstrate the efficacy of pretraining for scene text understanding. CoFormer achieves state-of-theart results with and without pretraining on two downstream tasks: scene text recognition and scene text style transfer. The paper presents LISTNet (Language Invariant Style Transfer), a novel framework for bi-lingual scene text style transfer. It also introduces three datasets, viz., TST500K for scene text style transfer, CSTR2.5M and Akshara550 for scene text recognition",
    "checked": true,
    "id": "2ca72400a4026e9fe649e9c5e2458167b56f8452",
    "semantic_title": "textual alchemy: coformer for scene text understanding",
    "citation_count": 0,
    "authors": [
      "Gayatri Deshmukh",
      "Onkar Susladkar",
      "Dhruv Makwana",
      "Sparsh Mittal",
      "Sai Chandra Teja R."
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Singla_Data-Centric_Debugging_Mitigating_Model_Failures_via_Targeted_Image_Retrieval_WACV_2024_paper.html": {
    "title": "Data-Centric Debugging: Mitigating Model Failures via Targeted Image Retrieval",
    "volume": "main",
    "abstract": "Deep neural networks can be unreliable in the real world when the training set does not adequately cover all the settings where they are deployed. Focusing on image classification, we consider the setting where we have an error distribution E representing a deployment scenario where the model fails. We have access to a small set of samples E_sample from E and it can be expensive to obtain additional samples. In the traditional model development framework, mitigating failures of the model in E can be challenging and is often done in an ad hoc manner. In this paper, we propose a general methodology for model debugging that can systemically improve model performance on E while maintaining its performance on the original test set. Our key assumption is that we have access to a large pool of weakly (noisily) labeled data F. However, naively adding F to the training would hurt model performance due to the large extent of label noise. Our Data-Centric Debugging (DCD) framework carefully creates a debug-train set by selecting images from F that are perceptually similar to the images in E_sample. To do this, we use the l_2 distance in the feature space (penultimate layer activations) of various models including ResNet, Robust ResNet and DINO where we observe DINO ViTs are significantly better at discovering similar images compared to Resnets. Compared to the baselines that maintain model performance on the test set, we achieve significantly (+9.45%) improved results on the debug-heldout sets",
    "checked": false,
    "id": "8aa9f90d8bb3cebc6362eae1e4a764b44f697b60",
    "semantic_title": "data-centric debugging: mitigating model failures via targeted data collection",
    "citation_count": 4,
    "authors": [
      "Sahil Singla",
      "Atoosa Malemir Chegini",
      "Mazda Moayeri",
      "Soheil Feizi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Fujitake_DTrOCR_Decoder-Only_Transformer_for_Optical_Character_Recognition_WACV_2024_paper.html": {
    "title": "DTrOCR: Decoder-Only Transformer for Optical Character Recognition",
    "volume": "main",
    "abstract": "Typical text recognition methods rely on an encoder-decoder structure, in which the encoder extracts features from an image, and the decoder produces recognized text from these features. In this study, we propose a simpler and more effective method for text recognition, known as the Decoder-only Transformer for Optical Character Recognition (DTrOCR). This method uses a decoder-only Transformer to take advantage of a generative language model that is pre-trained on a large corpus. We examined whether a generative language model that has been successful in natural language processing can also be effective for text recognition in computer vision. Our experiments demonstrated that DTrOCR outperforms current state-of-the-art methods by a large margin in the recognition of printed, handwritten, and scene text in both English and Chinese",
    "checked": true,
    "id": "01d7d75440c36c7d7ea5f781664662b6b143299f",
    "semantic_title": "dtrocr: decoder-only transformer for optical character recognition",
    "citation_count": 2,
    "authors": [
      "Masato Fujitake"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wang_Efficient_Transferability_Assessment_for_Selection_of_Pre-Trained_Detectors_WACV_2024_paper.html": {
    "title": "Efficient Transferability Assessment for Selection of Pre-Trained Detectors",
    "volume": "main",
    "abstract": "Large-scale pre-training followed by downstream fine-tuning is an effective solution for transferring deep-learning-based models. Since finetuning all possible pre-trained models is computational costly, we aim to predict the transferability performance of these pre-trained models in a computational efficient manner. Different from previous work that seek out suitable models for downstream classification and segmentation tasks, this paper studies the efficient transferability assessment of pre-trained object detectors. To this end, we build up a detector transferability benchmark which contains a large and diverse zoo of pre-trained detectors with various architectures, source datasets and training schemes. Given this zoo, we adopt 6 target datasets from 5 diverse domains as the downstream target tasks for evaluation. Further, we propose to assess classification and regression sub-tasks simultaneously in a unified framework. Additionally, we design a complementary metric for evaluating tasks with varying objects. Experimental results demonstrate that our method outperforms other state-of-the-art approaches in assessing transferability under different target domains while efficiently reducing wall-clock time 32x and requiring a mere 5.2% memory footprint compared to brute-force fine-tuning of all pre-trained detectors. Our assessment code and benchmark will be publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhao Wang",
      "Aoxue Li",
      "Zhenguo Li",
      "Qi Dou"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Pham_NVAutoNet_Fast_and_Accurate_360deg_3D_Visual_Perception_for_Self_WACV_2024_paper.html": {
    "title": "NVAutoNet: Fast and Accurate 360deg 3D Visual Perception for Self Driving",
    "volume": "main",
    "abstract": "Achieving robust and real-time 3D perception is fundamental for autonomous vehicles. While most existing 3D perception methods prioritize detection accuracy, they often overlook critical aspects such as computational efficiency, onboard chip deployment friendliness, resilience to sensor mounting deviations, and adaptability to various vehicle types. To address these challenges, we present NVAutoNet: a specialized Bird's-Eye-View (BEV) perception network tailored explicitly for automated vehicles. NVAutoNet takes synchronized camera images as input and predicts 3D signals like obstacles, freespaces, and parking spaces. The core of NVAutoNet's architecture (image and BEV backbones) relies on efficient convolutional networks, optimized for high performance using TensorRT. Our image-to-BEV transformation employs simple linear layers and BEV look-up tables, ensuring rapid inference speed. Trained on an extensive proprietary dataset, NVAutoNet consistently achieves elevated perception accuracy, operating remarkably at 53 frames per second on the NVIDIA DRIVE Orin SoC. Notably, NVAutoNet demonstrates resilience to sensor mounting deviations arising from diverse car models. Moreover, NVAutoNet excels in adapting to varied vehicle types, facilitated by inexpensive model fine-tuning procedures that expedite compatibility adjustments",
    "checked": false,
    "id": "164e278a392e84ceda654f25c7fe5105e802f342",
    "semantic_title": "nvautonet: fast and accurate 360° 3d visual perception for self driving",
    "citation_count": 1,
    "authors": [
      "Trung Pham",
      "Mehran Maghoumi",
      "Wanli Jiang",
      "Bala Siva Sashank Jujjavarapu",
      "Mehdi Sajjadi",
      "Xin Liu",
      "Hsuan-Chu Lin",
      "Bor-Jeng Chen",
      "Giang Truong",
      "Chao Fang",
      "Junghyun Kwon",
      "Minwoo Park"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Nguyen_VideoFACT_Detecting_Video_Forgeries_Using_Attention_Scene_Context_and_Forensic_WACV_2024_paper.html": {
    "title": "VideoFACT: Detecting Video Forgeries Using Attention, Scene Context, and Forensic Traces",
    "volume": "main",
    "abstract": "Fake videos represent an important misinformation threat. While existing forensic networks have demonstrated strong performance on image forgeries, recent results reported on the Adobe VideoSham dataset show that these networks fail to identify fake content in videos. In response, we propose VideoFACT - a new network that is able to detect and localize a wide variety of video forgeries and manipulations. To overcome challenges that existing networks face when analyzing videos, our network utilizes both forensic embeddings to capture traces left by manipulation, context embeddings to control for variation in forensic traces introduced by video coding, and a deep self-attention mechanism to estimate the quality and relative importance of local forensic embeddings. We create several new video forgery datasets and use these, along with publicly available data, to experimentally evaluate our network's performance. These results show that our proposed network is able to identify a diverse set of video forgeries, including those not encountered during training. Furthermore, we show that our network can be fine-tuned to achieve even stronger performance on challenging AI-based manipulations",
    "checked": true,
    "id": "7105378af541b2b6df10bc9a9c7f569a1a2852ab",
    "semantic_title": "videofact: detecting video forgeries using attention, scene context, and forensic traces",
    "citation_count": 2,
    "authors": [
      "Tai D. Nguyen",
      "Shengbang Fang",
      "Matthew C. Stamm"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Vinod_TEGLO_High_Fidelity_Canonical_Texture_Mapping_From_Single-View_Images_WACV_2024_paper.html": {
    "title": "TEGLO: High Fidelity Canonical Texture Mapping From Single-View Images",
    "volume": "main",
    "abstract": "Recent work in Neural Fields (NFs) learn 3D representations from class-specific single view image collections. However, they are unable to reconstruct the input data preserving high-frequency details. Further, these methods do not disentangle appearance from geometry and hence are not suitable for tasks such as texture transfer and editing. In this work, we propose TEGLO (Textured EG3D-GLO) for learning 3D representations from single view in-the-wild image collections for a given class of objects. We accomplish this by training a conditional Neural Radiance Field (NeRF) without any explicit 3D supervision. We equip our method with editing capabilities by creating a dense correspondence mapping to a 2D canonical space. We demonstrate that such mapping enables texture transfer and texture editing without requiring meshes with shared topology. Our key insight is that by mapping the input image pixels onto the texture space we can achieve near perfect reconstruction (>74 dB PSNR at 1024^2 resolution). Our formulation allows for high quality 3D consistent novel view synthesis with high-frequency details even at megapixel image resolutions",
    "checked": true,
    "id": "03f050da6d7207ca6e8d7fdfa00cfacba5d19515",
    "semantic_title": "teglo: high fidelity canonical texture mapping from single-view images",
    "citation_count": 1,
    "authors": [
      "Vishal Vinod",
      "Tanmay Shah",
      "Dmitry Lagun"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Nogueira_Prototypical_Contrastive_Network_for_Imbalanced_Aerial_Image_Segmentation_WACV_2024_paper.html": {
    "title": "Prototypical Contrastive Network for Imbalanced Aerial Image Segmentation",
    "volume": "main",
    "abstract": "Binary segmentation is the main task underpinning several remote sensing applications, which are particularly interested in identifying and monitoring a specific category/object. Although extremely important, such a task has several challenges, including huge intra-class variance for the background and data imbalance. Furthermore, most works tackling this task partially or completely ignore one or both of these challenges and their developments. In this paper, we propose a novel method to perform imbalanced binary segmentation of remote sensing images based on deep networks, prototypes, and contrastive loss. The proposed approach allows the model to focus on learning the foreground class while alleviating the class imbalance problem by allowing it to concentrate on the most difficult background examples. The results demonstrate that the proposed method outperforms state-of-the-art techniques for imbalanced binary segmentation of remote sensing images while taking much less training time",
    "checked": false,
    "id": "4be82db0438adfc5f0db37a56e8471a9c7861888",
    "semantic_title": "multiscale prototype contrast network for high-resolution aerial imagery semantic segmentation",
    "citation_count": 0,
    "authors": [
      "Keiller Nogueira",
      "Mayara Maezano Faita-Pinheiro",
      "Ana Paula Marques Ramos",
      "Wesley Nunes Gonçalves",
      "José Marcato Junior",
      "Jefersson A. dos Santos"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Haitman_BoostRad_Enhancing_Object_Detection_by_Boosting_Radar_Reflections_WACV_2024_paper.html": {
    "title": "BoostRad: Enhancing Object Detection by Boosting Radar Reflections",
    "volume": "main",
    "abstract": "Automotive radars have an important role in autonomous driving systems. The main challenge in automotive radar detection is the radar's wide point spread function (PSF) in the angular domain that causes blurriness and clutter in the radar image. Numerous studies suggest employing an 'end-to-end' learning strategy using a Deep Neural Network (DNN) to directly detect objects from radar images. This approach implicitly addresses the PSF's impact on objects of interest. In this paper, we propose an alternative approach, which we term \"Boosting Radar Reflections\" (BoostRad). In BoostRad, a first DNN is trained to narrow the PSF for all the reflection points in the scene. The output of the first DNN is a boosted reflection image with higher resolution and reduced clutter, resulting in a sharper and cleaner image. Subsequently, a second DNN is employed to detect objects within the boosted reflection image. We develop a novel method for training the boosting DNN that incorporates domain knowledge of radar's PSF characteristics. BoostRad's performance is evaluated using the RADDet and CARRADA datasets, revealing its superiority over reference methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuval Haitman",
      "Oded Bialer"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Pham_Frequency_Attention_for_Knowledge_Distillation_WACV_2024_paper.html": {
    "title": "Frequency Attention for Knowledge Distillation",
    "volume": "main",
    "abstract": "Knowledge distillation is an attractive approach for learning compact deep neural networks, which learns a lightweight student model by distilling knowledge from a complex teacher model. Attention-based knowledge distillation is a specific form of intermediate feature-based knowledge distillation that uses attention mechanisms to encourage the student to better mimic the teacher. However, most of the previous attention-based distillation approaches perform attention in the spatial domain, which primarily affects local regions in the input image. This may not be sufficient when we need to capture the broader context or global information necessary for effective knowledge transfer. In frequency domain, since each frequency is determined from all pixels of the image in spatial domain, it can contain global information about the image. Inspired by the benefits of the frequency domain, we propose a novel module that functions as an attention mechanism in the frequency domain. The module consists of a learnable global filter that can adjust the frequencies of student's features under the guidance of the teacher's features, which encourages the student's features to have patterns similar to the teacher's features. We then propose an enhanced knowledge review-based distillation model by leveraging the proposed frequency attention module. The extensive experiments with various teacher and student architectures on image classification and object detection benchmark datasets show that the proposed approach outperforms other knowledge distillation methods",
    "checked": false,
    "id": "79ac49735c1e50c5c17b7e688306bf30a0eaa561",
    "semantic_title": "few-shot radar jamming recognition network via time-frequency self-attention and global knowledge distillation",
    "citation_count": 5,
    "authors": [
      "Cuong Pham",
      "Van-Anh Nguyen",
      "Trung Le",
      "Dinh Phung",
      "Gustavo Carneiro",
      "Thanh-Toan Do"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Jang_Lost_Your_Style_Navigating_With_Semantic-Level_Approach_for_Text-To-Outfit_Retrieval_WACV_2024_paper.html": {
    "title": "Lost Your Style? Navigating With Semantic-Level Approach for Text-To-Outfit Retrieval",
    "volume": "main",
    "abstract": "Fashion stylists have historically bridged the gap between consumers' desires and perfect outfits, which involve intricate combinations of colors, patterns, and materials. Although recent advancements in fashion recommendation systems have made strides in outfit compatibility prediction and complementary item retrieval, these systems rely heavily on pre-selected customer choices. Therefore, we introduce a groundbreaking approach to fashion recommendations: text-to-outfit retrieval task that generates a complete outfit set based solely on textual descriptions given by users. Our model is devised at three semantic levels--item, style, and outfit--where each level progressively aggregates data to form a coherent outfit recommendation based on textual input. Here, we leverage strategies similar to those in the contrastive language-image pretraining model to address the intricate-style matrix within the outfit sets. Using the Maryland Polyvore and Polyvore Outfit datasets, our approach significantly outperformed state-of-the-art models in text-video retrieval tasks, solidifying its effectiveness in the fashion recommendation domain. This research not only pioneers a new facet of fashion recommendation systems, but also introduces a method that captures the essence of individual style preferences through textual descriptions",
    "checked": true,
    "id": "9dae57e2a3856b243194acd1778aa8abe726d25e",
    "semantic_title": "lost your style? navigating with semantic-level approach for text-to-outfit retrieval",
    "citation_count": 0,
    "authors": [
      "Junkyu Jang",
      "Eugene Hwang",
      "Sung-Hyuk Park"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Bashirov_MoRF_Mobile_Realistic_Fullbody_Avatars_From_a_Monocular_Video_WACV_2024_paper.html": {
    "title": "MoRF: Mobile Realistic Fullbody Avatars From a Monocular Video",
    "volume": "main",
    "abstract": "We present a system to create Mobile Realistic Fullbody (MoRF) avatars. MoRF avatars are rendered in real-time on mobile devices, learned from monocular videos, and have high realism. We use SMPL-X as a proxy geometry and render it with DNR (neural texture and image-2-image network). We improve on prior work, by overfitting per-frame warping fields in the neural texture space, allowing to better align the training signal between different frames. We also refine SMPL-X mesh fitting procedure to improve the overall avatar quality. In the comparisons to other monocular video-based avatar systems, MoRF avatars achieve higher image sharpness and temporal consistency. Participants of our user study also preferred avatars generated by MoRF",
    "checked": true,
    "id": "3990ccfe8f432b4815efbf69b00875770338ab87",
    "semantic_title": "morf: mobile realistic fullbody avatars from a monocular video",
    "citation_count": 1,
    "authors": [
      "Renat Bashirov",
      "Alexey Larionov",
      "Evgeniya Ustinova",
      "Mikhail Sidorenko",
      "David Svitov",
      "Ilya Zakharkin",
      "Victor Lempitsky"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Flotzinger_dacl10k_Benchmark_for_Semantic_Bridge_Damage_Segmentation_WACV_2024_paper.html": {
    "title": "dacl10k: Benchmark for Semantic Bridge Damage Segmentation",
    "volume": "main",
    "abstract": "Reliably identifying reinforced concrete defects (RCDs) plays a crucial role in assessing the structural integrity, traffic safety, and long-term durability of concrete bridges, which represent the most common bridge type worldwide. Nevertheless, available datasets for the recognition of RCDs are small in terms of size and class variety, which questions their usability in real-world scenarios and their role as a benchmark. Our contribution to this problem is \"dacl10k\", an exceptionally diverse RCD dataset for multi-label semantic segmentation comprising 9,920 images deriving from real-world bridge inspections. dacl10k distinguishes 12 damage classes as well as 6 bridge components that play a key role in the building assessment and recommending actions, such as restoration works, traffic load limitations or bridge closures. In addition, we examine baseline models for dacl10k which are subsequently evaluated. The best model achieves a mean intersection-over-union of 0.42 on the test set. dacl10k, along with our baselines, will be openly accessible to researchers and practitioners, representing the currently biggest dataset regarding number of images and class diversity for semantic segmentation in the bridge inspection domain",
    "checked": true,
    "id": "fc3c9624b60ff9b12b66bd979deb7d78a90b2a56",
    "semantic_title": "dacl10k: benchmark for semantic bridge damage segmentation",
    "citation_count": 1,
    "authors": [
      "Johannes Flotzinger",
      "Philipp J. Rösch",
      "Thomas Braml"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Bernhard_Whats_Outside_the_Intersection_Fine-Grained_Error_Analysis_for_Semantic_Segmentation_WACV_2024_paper.html": {
    "title": "What's Outside the Intersection? Fine-Grained Error Analysis for Semantic Segmentation Beyond IoU",
    "volume": "main",
    "abstract": "Semantic segmentation represents a fundamental task in computer vision with various application areas such as autonomous driving, medical imaging, or remote sensing. For evaluating and comparing semantic segmentation models, the mean intersection over union (mIoU) is currently the gold standard. However, while mIoU serves as a valuable benchmark, it does not offer insights into the types of errors incurred by a model. Moreover, different types of errors may have different impacts on downstream applications. To address this issue, we propose an intuitive method for the systematic categorization of errors, thereby enabling a fine-grained analysis of semantic segmentation models. Since we assign each erroneous pixel to precisely one error type, our method seamlessly extends the popular IoU-based evaluation by shedding more light on the false positive and false negative predictions. Our approach is model- and dataset-agnostic, as it does not rely on additional information besides the predicted and ground-truth segmentation masks. In our experiments, we demonstrate that our method accurately assesses model strengths and weaknesses on a quantitative basis, thus reducing the dependence on time-consuming qualitative model inspection. We analyze a variety of state-of-the-art semantic segmentation models, revealing systematic differences across various architectural paradigms. Exploiting the gained insights, we showcase that combining two models with complementary strengths in a straightforward way is sufficient to consistently improve mIoU, even for models setting the current state of the art on ADE20K. We release a toolkit for our evaluation method at https://github.com/mxbh/beyond-iou",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maximilian Bernhard",
      "Roberto Amoroso",
      "Yannic Kindermann",
      "Lorenzo Baraldi",
      "Rita Cucchiara",
      "Volker Tresp",
      "Matthias Schubert"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ghaleb_Co-Speech_Gesture_Detection_Through_Multi-Phase_Sequence_Labeling_WACV_2024_paper.html": {
    "title": "Co-Speech Gesture Detection Through Multi-Phase Sequence Labeling",
    "volume": "main",
    "abstract": "Gestures are integral components of face-to-face communication. They unfold over time, often following predictable movement phases of preparation, stroke, and retraction. Yet, the prevalent approach to automatic gesture detection treats the problem as binary classification, classifying a segment as either containing a gesture or not, thus failing to capture its inherently sequential and contextual nature. To address this, we introduce a novel framework that reframes the task as a multi-phase sequence labeling problem rather than binary classification. Our model processes sequences of skeletal movements over time windows, uses Transformer encoders to learn contextual embeddings, and leverages Conditional Random Fields to perform sequence labeling. We evaluate our proposal on a large dataset of diverse co-speech gestures in task-oriented face-to-face dialogues. The results consistently demonstrate that our method significantly outperforms strong baseline models in detecting gesture strokes. Furthermore, applying Transformer encoders to learn contextual embeddings from movement sequences substantially improves gesture unit detection. These results highlight our framework's capacity to capture the fine-grained dynamics of co-speech gesture phases, paving the way for more nuanced and accurate gesture detection and analysis",
    "checked": true,
    "id": "8772fac45cd1b1292147421687725a85f33cad0a",
    "semantic_title": "co-speech gesture detection through multi-phase sequence labeling",
    "citation_count": 1,
    "authors": [
      "Esam Ghaleb",
      "Ilya Burenko",
      "Marlou Rasenberg",
      "Wim Pouw",
      "Peter Uhrig",
      "Judith Holler",
      "Ivan Toni",
      "Aslı Özyürek",
      "Raquel Fernández"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Maheshwari_Missing_Modality_Robustness_in_Semi-Supervised_Multi-Modal_Semantic_Segmentation_WACV_2024_paper.html": {
    "title": "Missing Modality Robustness in Semi-Supervised Multi-Modal Semantic Segmentation",
    "volume": "main",
    "abstract": "Using multiple spatial modalities has been proven helpful in improving semantic segmentation performance. However, there are several real-world challenges that have yet to be addressed: (a) improving label efficiency and (b) enhancing robustness in realistic scenarios where modalities are missing at the test time. To address these challenges, we first propose a simple yet efficient multi-modal fusion mechanism Linear Fusion, that performs better than the state-of-the-art multi-modal models even with limited supervision. Second, we propose M3L: Multi-modal Teacher for Masked Modality Learning, a semi-supervised framework that not only improves the multi-modal performance but also makes the model robust to the realistic missing modality scenario using unlabeled data. We create the first benchmark for semi-supervised multi-modal semantic segmentation and also report the robustness to missing modalities. Our proposal shows an absolute improvement of up to 5% on robust mIoU above the most competitive baselines. Our project page is at https://harshm121.github.io/projects/m3l.html",
    "checked": true,
    "id": "b767a2193c6ba98e11d482ad96aa7d860e492593",
    "semantic_title": "missing modality robustness in semi-supervised multi-modal semantic segmentation",
    "citation_count": 2,
    "authors": [
      "Harsh Maheshwari",
      "Yen-Cheng Liu",
      "Zsolt Kira"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ben-Dov_Adversarial_Likelihood_Estimation_With_One-Way_Flows_WACV_2024_paper.html": {
    "title": "Adversarial Likelihood Estimation With One-Way Flows",
    "volume": "main",
    "abstract": "Generative Adversarial Networks (GANs) can produce high-quality samples, but do not provide an estimate of the probability density around the samples. However, it has been noted that maximizing the log-likelihood within an energy-based setting can lead to an adversarial framework where the discriminator provides unnormalized density (often called energy). We further develop this perspective, incorporate importance sampling, and show that 1) Wasserstein GAN performs a biased estimate of the partition function, and we propose instead to use an unbiased estimator; and 2) when optimizing for likelihood, one must maximize generator entropy. This is hypothesized to provide a better mode coverage. Different from previous works, we explicitly compute the density of the generated samples. This is the key enabler to designing an unbiased estimator of the partition function and computation of the generator entropy term. The generator density is obtained via a new type of flow network, called one-way flow network, that is less constrained in terms of architecture, as it does not require a tractable inverse function. Our experimental results show that our method converges faster, produces comparable sample quality to GANs with similar architecture, successfully avoids over-fitting to commonly used datasets and produces smooth low-dimensional latent representations of the training data",
    "checked": true,
    "id": "d6d6b1e6b4579c4005e024b964eecca04f190e05",
    "semantic_title": "adversarial likelihood estimation with one-way flows",
    "citation_count": 0,
    "authors": [
      "Omri Ben-Dov",
      "Pravir Singh Gupta",
      "Victoria Abrevaya",
      "Michael J. Black",
      "Partha Ghosh"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Chang_Fast_Sun-Aligned_Outdoor_Scene_Relighting_Based_on_TensoRF_WACV_2024_paper.html": {
    "title": "Fast Sun-Aligned Outdoor Scene Relighting Based on TensoRF",
    "volume": "main",
    "abstract": "In this work, we introduce our method of outdoor scene relighting for Neural Radiance Fields (NeRF) named Sun-aligned Relighting TensoRF (SR-TensoRF). SR-TensoRF offers a lightweight and rapid pipeline aligned with the sun, thereby achieving a simplified workflow that eliminates the need for environment maps. Our sun-alignment strategy is motivated by the insight that shadows, unlike viewpoint-dependent albedo, are determined by light direction. We directly use the sun direction as an input during shadow generation, simplifying the requirements of the inference process significantly. Moreover, SR-TensoRF leverages the training efficiency of TensoRF by incorporating our proposed cubemap concept, resulting in notable acceleration in both training and rendering processes compared to existing methods",
    "checked": true,
    "id": "53f85ab058fbc33006399558f5f581eeb2dfbc8b",
    "semantic_title": "fast sun-aligned outdoor scene relighting based on tensorf",
    "citation_count": 0,
    "authors": [
      "Yeonjin Chang",
      "Yearim Kim",
      "Seunghyeon Seo",
      "Jung Yi",
      "Nojun Kwak"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Hong_Robust_Eye_Blink_Detection_Using_Dual_Embedding_Video_Vision_Transformer_WACV_2024_paper.html": {
    "title": "Robust Eye Blink Detection Using Dual Embedding Video Vision Transformer",
    "volume": "main",
    "abstract": "Eye blink detection serves as a crucial biomarker for evaluating both physical and mental states, garnering considerable attention in biometric and video-based studies. Among various methods, video-based eye blink detection has been particularly favored due to its non-invasive nature, enabling broader applications. However, capturing eye blinks from different camera angles poses significant challenges, primarily because the eye region is relatively small and eye blinks occur rapidly, necessitating a robust detection algorithm. To address these challenges, we introduce Dual Embedding Video Vision Transformer (DE-ViViT), a novel approach for eye blink detection that employs two different embedding strategies: (i) tubelet embedding and (ii) residual embedding. Each embedding can capture large and subtle changes within the eye movement sequence respectively. We rigorously evaluate our proposed method using HUST-LEBW, a publicly available dataset, as well as our newly collected multi-angle eye blink dataset (MAEB). The results indicate that the proposed model consistently outperforms existing methods across both datasets, with notably minor performance variations depending on the camera angles",
    "checked": true,
    "id": "729e6c1b6021879f2084c8a920299769df25f8e7",
    "semantic_title": "robust eye blink detection using dual embedding video vision transformer",
    "citation_count": 0,
    "authors": [
      "Jeongmin Hong",
      "Joseph Shin",
      "Juhee Choi",
      "Minsam Ko"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Nguyen_Domain_Generalisation_via_Risk_Distribution_Matching_WACV_2024_paper.html": {
    "title": "Domain Generalisation via Risk Distribution Matching",
    "volume": "main",
    "abstract": "We propose a novel approach for domain generalisation (DG) leveraging risk distributions to characterise domains, thereby achieving domain invariance. In our findings, risk distributions effectively highlight differences between training domains and reveal their inherent complexities. In testing, we may observe similar, or potentially intensifying in magnitude, divergences between risk distributions. Hence, we propose a compelling proposition: Minimising the divergences between risk distributions across training domains leads to robust invariance for DG. The key rationale behind this concept is that a model, trained on domain-invariant or stable features, may consistently produce similar risk distributions across various domains. Building upon this idea, we propose Risk Distribution Matching (RDM). Using the maximum mean discrepancy (MMD) distance, RDM aims to minimise the variance of risk distributions across training domains. However, when the number of domains increases, the direct optimisation of variance leads to linear growth in MMD computations, resulting in inefficiency. Instead, we propose an approximation that requires only one MMD computation, by aligning just two distributions: that of the worst-case domain and the aggregated distribution from all domains. Notably, this method empirically outperforms optimising distributional variance while being computationally more efficient. Unlike conventional DG matching algorithms, RDM stands out for its enhanced efficacy by concentrating on scalar risk distributions, sidestepping the pitfalls of high-dimensional challenges seen in feature or gradient matching. Our extensive experiments on standard benchmark datasets demonstrate that RDM shows superior generalisation capability over state-of-the-art DG methods",
    "checked": true,
    "id": "82ee2ef27b9738e5f7bbf884acc975ff19be43bd",
    "semantic_title": "domain generalisation via risk distribution matching",
    "citation_count": 0,
    "authors": [
      "Toan Nguyen",
      "Kien Do",
      "Bao Duong",
      "Thin Nguyen"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Chen_Panelformer_Sewing_Pattern_Reconstruction_From_2D_Garment_Images_WACV_2024_paper.html": {
    "title": "Panelformer: Sewing Pattern Reconstruction From 2D Garment Images",
    "volume": "main",
    "abstract": "In this paper, we present a novel approach for reconstructing garment sewing patterns from 2D garment images. Our method addresses the challenge of handling occlusion in 2D images by leveraging the symmetric and correlated nature of garment panels. We introduce a transformer-based deep neural network called Panelformer that learns the parametric space of garment sewing patterns. The network comprises two components: the panel transformer and the stitch predictor. The panel transformer estimates the parametric panel shapes, including the occluded panels, by learning from the visible ones. The stitch predictor determines the stitching information among the predicted panels, enabling the reconstruction of the complete garment. To mitigate the overfitting problem caused by strong panel correlations, we propose two tailor-made data augmentation techniques: panel masking and garment mixing. These techniques generate a wider variety of panel combinations, enhancing the model's robustness and generalization capability. We evaluate the effectiveness of Panelformer using a synthetic dataset with diverse garment types. The experimental results demonstrate that our method outperforms competing baselines and achieves comparable performance to NeuralTailor, which operates on 3D point cloud data. This validates the efficacy of our approach in the context of garment sewing pattern reconstruction. By utilizing 2D images as input, our method expands the potential applications of garment modeling and offers easy accessibility to end users. Our code is available online",
    "checked": false,
    "id": "aaedb11eac8c65e9d4e8f56e6004b0d3010ddf3d",
    "semantic_title": "panelformer: sewing pattern reconstruction from 2d garment images supplemental material",
    "citation_count": 0,
    "authors": [
      "Cheng-Hsiu Chen",
      "Jheng-Wei Su",
      "Min-Chun Hu",
      "Chih-Yuan Yao",
      "Hung-Kuo Chu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Omidi_Unsupervised_Domain_Adaptation_of_MRI_Skull-Stripping_Trained_on_Adult_Data_WACV_2024_paper.html": {
    "title": "Unsupervised Domain Adaptation of MRI Skull-Stripping Trained on Adult Data to Newborns",
    "volume": "main",
    "abstract": "Skull-stripping is an important first step when analyzing brain Magnetic Resonance Imaging (MRI) data. Deep learning-based supervised segmentation models, such as the U-net model, have shown promising results in automating this segmentation task. However, when it comes to newborn MRI data, there are no publicly available brain MRI datasets that come with manually annotated segmentation masks to be used as labels during the training of these models. Manual segmentation of brain MR images is time-consuming, labor-intensive, and requires expertise. Furthermore, using a segmentation model trained on adult brain MR images for segmenting newborn brain images is not effective due to a large domain shift between adult and newborn data. As a result, there is a need for more efficient and accurate skull-stripping methods for newborns' brain MRIs. In this paper, we present an unsupervised approach to adapt a U-net skull-stripping model trained on adult MRI to work effectively on newborns. Our results demonstrate the effectiveness of our novel unsupervised approach in enhancing segmentation accuracy. Our proposed method achieved an overall Dice coefficient of 0.916 +- 0.032 (mean +- std), and our ablation studies confirmed the effectiveness of our proposal. Remarkably, despite being unsupervised, our model's performance stands in close proximity to that of the current state-of-the-art supervised models against which we conducted our comparisons. These findings indicate the potential of this method as a valuable, easier, and faster tool for supporting healthcare professionals in the examination of MR images of newborn brains. All the codes are available at: https://github.com/abbasomidi77/DAUnet",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abbas Omidi",
      "Aida Mohammadshahi",
      "Neha Gianchandani",
      "Regan King",
      "Lara Leijser",
      "Roberto Souza"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Generated_Distributions_Are_All_You_Need_for_Membership_Inference_Attacks_WACV_2024_paper.html": {
    "title": "Generated Distributions Are All You Need for Membership Inference Attacks Against Generative Models",
    "volume": "main",
    "abstract": "Generative models have demonstrated revolutionary success in various visual creation tasks, but in the meantime, they have been exposed to the threat of leaking private information of their training data. Several membership inference attacks (MIAs) have been proposed to exhibit the privacy vulnerability of generative models by classifying a query image as a training dataset member or nonmember. However, these attacks suffer from major limitations, such as requiring shadow models and white-box access, and either ignoring or only focusing on the unique property of diffusion models, which block their generalization to multiple generative models. In contrast, we propose the first generalized membership inference attack against a variety of generative models such as generative adversarial networks, [variational] autoencoders, implicit functions, and the emerging diffusion models. We leverage only generated distributions from target generators and auxiliary non-member datasets, therefore regarding target generators as black boxes and agnostic to their architectures or application scenarios. Experiments validate that all the generative models are vulnerable to our attack. For instance, our work achieves attack AUC > 0.99 against DDPM, DDIM, and FastDPM trained on CIFAR-10 and CelebA. And the attack against VQGAN, LDM (for the text-conditional generation), and LIIF achieves AUC > 0.90. As a result, we appeal to our community to be aware of such privacy leakage risks when designing and publishing generative models",
    "checked": true,
    "id": "5e1015f87ccc0ef57c0a098395ebecc9bc6b2379",
    "semantic_title": "generated distributions are all you need for membership inference attacks against generative models",
    "citation_count": 0,
    "authors": [
      "Minxing Zhang",
      "Ning Yu",
      "Rui Wen",
      "Michael Backes",
      "Yang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Shen_Multitask_Vision-Language_Prompt_Tuning_WACV_2024_paper.html": {
    "title": "Multitask Vision-Language Prompt Tuning",
    "volume": "main",
    "abstract": "Prompt Tuning, conditioning on task-specific learned prompt vectors, has emerged as a data-efficient and parameter-efficient method for adapting large pretrained vision-language models to multiple downstream tasks. However, existing approaches usually consider learning prompt vectors for each task independently from scratch, thereby failing to exploit the rich shareable knowledge across different vision-language tasks. In this paper, we propose multitask vision-language prompt tuning (MVLPT), which incorporates cross-task knowledge into prompt tuning for vision-language models. Specifically, (i) we demonstrate the effectiveness of learning a single transferable prompt from multiple source tasks to initialize the prompt for each target task; (ii) we show many target tasks can benefit each other from sharing prompt vectors and thus can be jointly learned via multitask prompt tuning. We benchmark the proposed MVLPT using three representative prompt tuning methods, namely text prompt tuning, visual prompt tuning, and the unified vision-language prompt tuning. Results in 20 vision tasks demonstrate that the proposed approach outperforms all single-task baseline prompt tuning methods, setting the new state-of-the-art on the few-shot ELEVATER benchmarks and cross-task generalization benchmarks. To understand where the cross-task knowledge is most effective, we also conduct a large-scale study on task transferability with 20 vision tasks in 400 combinations for each prompt tuning method. It shows that the most performant MVLPT for each prompt tuning method prefers different task combinations and many tasks can benefit each other, depending on their visual similarity and label similarity",
    "checked": true,
    "id": "fd8c1b8741163d8737652fbcd3507bcd7d6225c7",
    "semantic_title": "multitask vision-language prompt tuning",
    "citation_count": 19,
    "authors": [
      "Sheng Shen",
      "Shijia Yang",
      "Tianjun Zhang",
      "Bohan Zhai",
      "Joseph E. Gonzalez",
      "Kurt Keutzer",
      "Trevor Darrell"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Barbany_ProcSim_Proxy-Based_Confidence_for_Robust_Similarity_Learning_WACV_2024_paper.html": {
    "title": "ProcSim: Proxy-Based Confidence for Robust Similarity Learning",
    "volume": "main",
    "abstract": "Deep Metric Learning (DML) methods aim at learning an embedding space in which distances are closely related to the inherent semantic similarity of the inputs. Previous studies have shown that popular benchmark datasets often contain numerous wrong labels, and DML methods are susceptible to them. Intending to study the effect of realistic noise, we create an ontology of the classes in a dataset and use it to simulate semantically coherent labeling mistakes. To train robust DML models, we propose ProcSim, a simple framework that assigns a confidence score to each sample using the normalized distance to its class representative. The experimental results show that the proposed method achieves state-of-the-art performance on the DML benchmark datasets injected with uniform and the proposed semantically coherent noise",
    "checked": true,
    "id": "1d64d37f1ebc80098ef4a5c321345f33ab1eaf99",
    "semantic_title": "procsim: proxy-based confidence for robust similarity learning",
    "citation_count": 0,
    "authors": [
      "Oriol Barbany",
      "Xiaofan Lin",
      "Muhammet Bastan",
      "Arnab Dhua"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Park_Hard-Label_Based_Small_Query_Black-Box_Adversarial_Attack_WACV_2024_paper.html": {
    "title": "Hard-Label Based Small Query Black-Box Adversarial Attack",
    "volume": "main",
    "abstract": "We consider the hard-label based black-box adversarial attack setting which solely observes the target model's predicted class. Most of the attack methods in this setting suffer from impractical number of queries required to achieve a successful attack. One approach to tackle this drawback is utilising the adversarial transferability between white-box surrogate models and black-box target model. However, the majority of the methods adopting this approach are soft-label based to take the full advantage of zeroth-order optimisation. Unlike mainstream methods, we propose a new practical setting of hard-label based attack with an optimisation process guided by a pre-trained surrogate model. Experiments show the proposed method significantly improves the query efficiency of the hard-label based black-box attack across various target model architectures. We find the proposed method achieves approximately 5 times higher attack success rate compared to the benchmarks, especially at the small query budgets as 100 and 250",
    "checked": false,
    "id": "2268170f52722f20d295d7532b8a8ef569ed00b6",
    "semantic_title": "boundary defense against black-box adversarial attacks",
    "citation_count": 4,
    "authors": [
      "Jeonghwan Park",
      "Paul Miller",
      "Niall McLaughlin"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kwon_Learning_to_Detour_Shortcut_Mitigating_Augmentation_for_Weakly_Supervised_Semantic_WACV_2024_paper.html": {
    "title": "Learning to Detour: Shortcut Mitigating Augmentation for Weakly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "Weakly supervised semantic segmentation (WSSS) employing weak forms of labels has been actively studied to alleviate the annotation cost of acquiring pixel-level labels. However, classifiers trained on biased datasets tend to exploit shortcut features and make predictions based on spurious correlations between certain backgrounds and objects, leading to a poor generalization performance. In this paper, we propose shortcut mitigating augmentation (SMA) for WSSS, which generates synthetic representations of object-background combinations not seen in the training data to reduce the use of shortcut features. Our approach disentangles the object-relevant and background features. We then shuffle and combine the disentangled representations to create synthetic features of diverse object-background combinations. SMA-trained classifier depends less on contexts and focuses more on the target object when making predictions. In addition, we analyzed the behavior of the classifier on shortcut usage after applying our augmentation using an attribution method-based metric. The proposed method achieved the improved performance of semantic segmentation result on PASCAL VOC 2012 and MS COCO 2014 datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JuneHyoung Kwon",
      "Eunju Lee",
      "Yunsung Cho",
      "YoungBin Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Trinh_3D_Super-Resolution_Model_for_Vehicle_Flow_Field_Enrichment_WACV_2024_paper.html": {
    "title": "3D Super-Resolution Model for Vehicle Flow Field Enrichment",
    "volume": "main",
    "abstract": "In vehicle shape design from aerodynamic performance perspective, deep learning methods enable us to estimate the flow field in a short period. However, the estimated flow fields are generally coarse and of low resolution. Therefore, a super-resolution model is required to enrich them. In this study, we propose a novel super-resolution model to enrich the flow fields around the vehicle to a higher resolution. To deal with the complex flow fields of vehicles, we apply the residual-in-residual dense block (RRDB) as the basic network-building unit in the generator without batch normalization. We then apply the relativistic discriminator to provide better feedback regarding the lack of high-frequency components. In addition, we propose a distance-weighted loss to obtain better estimation in wake regions and regions near the vehicle surface. Physics-informed loss is used to help the model generate data that satisfies the physical governing equations. We also propose a new training strategy to improve the leaning effectiveness and avoid instability during training. Experimental results demonstrate that the proposed method outperforms the previous study in vehicle flow field enrichment tasks by a significant margin",
    "checked": false,
    "id": "4a73c896a0cb841c35ddecfb9211a109100e2dd8",
    "semantic_title": "step into geological samples digital twins",
    "citation_count": 0,
    "authors": [
      "Thanh Luan Trinh",
      "Fangge Chen",
      "Takuya Nanri",
      "Kei Akasaka"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Liao_Multi-View_3D_Object_Reconstruction_and_Uncertainty_Modelling_With_Neural_Shape_WACV_2024_paper.html": {
    "title": "Multi-View 3D Object Reconstruction and Uncertainty Modelling With Neural Shape Prior",
    "volume": "main",
    "abstract": "3D object reconstruction is important for semantic scene understanding. It is challenging to reconstruct detailed 3D shapes from monocular images directly due to a lack of depth information, occlusion and noise. Most current methods generate deterministic object models without any awareness of the uncertainty of the reconstruction. We tackle this problem by leveraging a neural object representation which learns an object shape distribution from large dataset of 3d object models and maps it into a latent space. We propose a method to model uncertainty as part of the representation and define an uncertainty-aware encoder which generates latent codes with uncertainty directly from individual input images. Further, we propose a method to propagate the uncertainty in the latent code to SDF values and generate a 3d object mesh with local uncertainty for each mesh component. Finally, we propose an incremental fusion method under a Bayesian framework to fuse the latent codes from multi-view observations. We evaluate the system in both synthetic and real datasets to demonstrate the effectiveness of uncertainty-based fusion to improve 3D object reconstruction accuracy",
    "checked": true,
    "id": "d2e2bba282438864c1ed6dfa5bd4d1ea4ee3f82d",
    "semantic_title": "multi-view 3d object reconstruction and uncertainty modelling with neural shape prior",
    "citation_count": 1,
    "authors": [
      "Ziwei Liao",
      "Steven L. Waslander"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Djilali_Do_VSR_Models_Generalize_Beyond_LRS3_WACV_2024_paper.html": {
    "title": "Do VSR Models Generalize Beyond LRS3?",
    "volume": "main",
    "abstract": "The Lip Reading Sentences-3 (LRS3) benchmark has primarily been the focus of intense research in visual speech recognition (VSR) during the last few years. As a result, there is an increased risk of overfitting to its excessively used test set, which is only one hour duration. To alleviate this issue, we build a new VSR test set by closely following the LRS3 dataset creation processes. We then evaluate and analyse the extent to which the current VSR models generalize to the new test data. We evaluate a broad range of publicly available VSR models and find significant drops in performance on our test set, compared to their corresponding LRS3 results. Our results suggest that the increase in word error rates is caused by the models' inability to generalize to slightly \"harder\" and more realistic lip sequences than those found in the LRS3 test set. Our new test benchmark will be made public in order to enable future research towards more robust VSR models",
    "checked": true,
    "id": "f08076332559023bb8665875f9b7769c399b8836",
    "semantic_title": "do vsr models generalize beyond lrs3?",
    "citation_count": 0,
    "authors": [
      "Yasser Abdelaziz Dahou Djilali",
      "Sanath Narayan",
      "Eustache LeBihan",
      "Haithem Boussaid",
      "Ebtesam Almazrouei",
      "Merouane Debbah"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Dessalene_Context_in_Human_Action_Through_Motion_Complementarity_WACV_2024_paper.html": {
    "title": "Context in Human Action Through Motion Complementarity",
    "volume": "main",
    "abstract": "Motivated by Goldman's Theory of Human Action - a framework in which action decomposes into 1) base physical movements, and 2) the context in which they occur - we propose a novel learning formulation for motion and context, where context is derived as the complement to motion. More specifically, we model physical movement through the adoption of Therbligs, a set of elemental physical motions centered around object manipulation. Context is modeled through the use of a contrastive mutual information loss that formulates context information as the action information not contained within movement information. We empirically prove the utility brought by this separation of representation, showing sizable improvements in action recognition and action anticipation accuracies for a variety of models. We present results over two object manipulation datasets: EPIC Kitchens 100, and 50 Salads",
    "checked": true,
    "id": "5ca21164fc4b035f2f95c65ffe4453918cee2f3d",
    "semantic_title": "context in human action through motion complementarity",
    "citation_count": 0,
    "authors": [
      "Eadom Dessalene",
      "Michael Maynord",
      "Cornelia Fermüller",
      "Yiannis Aloimonos"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Hooda_D4_Detection_of_Adversarial_Diffusion_Deepfakes_Using_Disjoint_Ensembles_WACV_2024_paper.html": {
    "title": "D4: Detection of Adversarial Diffusion Deepfakes Using Disjoint Ensembles",
    "volume": "main",
    "abstract": "Detecting diffusion-generated deepfake images remains an open problem. Current detection methods fail against an adversary who adds imperceptible adversarial perturbations to the deepfake to evade detection. In this work, we propose Disjoint Diffusion Deepfake Detection (D4), a deepfake detector designed to improve black-box adversarial robustness beyond de facto solutions such as adversarial training. D4 uses an ensemble of models over disjoint subsets of the frequency spectrum to significantly improve adversarial robustness. Our key insight is to leverage a redundancy in the frequency domain and apply a saliency partitioning technique to disjointly distribute frequency components across multiple models. We formally prove that these disjoint ensembles lead to a reduction in the dimensionality of the input subspace where adversarial deepfakes lie, thereby making adversarial deepfakes harder to find for black-box attacks. We then empirically validate the D4 method against several black-box attacks and find that D4 significantly outperforms existing state-of-the-art defenses applied to diffusion-generated deepfake detection. We also demonstrate that D4 provides robustness against adversarial deepfakes from unseen data distributions as well as unseen generative techniques",
    "checked": true,
    "id": "d22030f2211d274187f6bf8b70736ec0ebb91e34",
    "semantic_title": "d4: detection of adversarial diffusion deepfakes using disjoint ensembles",
    "citation_count": 1,
    "authors": [
      "Ashish Hooda",
      "Neal Mangaokar",
      "Ryan Feng",
      "Kassem Fawaz",
      "Somesh Jha",
      "Atul Prakash"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Di_ProS_Facial_Omni-Representation_Learning_via_Prototype-Based_Self-Distillation_WACV_2024_paper.html": {
    "title": "ProS: Facial Omni-Representation Learning via Prototype-Based Self-Distillation",
    "volume": "main",
    "abstract": "This paper presents a novel approach, called Prototype-based Self-Distillation (ProS), for unsupervised face representation learning. The existing supervised methods heavily rely on a large amount of annotated training facial data, which poses challenges in terms of data collection and privacy concerns. To address these issues, we propose ProS, which leverages a vast collection of unlabeled face images to learn a comprehensive facial omni-representation. In particular, ProS consists of two vision-transformers (teacher and student models) that are trained with different augmented images (cropping, blurring, coloring, etc.). Besides, we build a face-aware retrieval system along with augmentations to obtain the curated images comprising predominantly facial areas. To enhance the discrimination of learned features, we introduce a prototype-based matching loss that aligns the similarity distributions between features (teacher or student) and a set of learnable prototypes. After pre-training, the teacher vision transformer serves as a backbone for downstream tasks, including attribute estimation, expression recognition, and landmark alignment, achieved through simple fine-tuning with additional layers. Extensive experiments demonstrate that our method achieves state-of-the-art performance on various tasks, both in full and few-shot settings. Furthermore, we investigate pre-training with synthetic face images, and ProS exhibits promising performance in this scenario as well",
    "checked": true,
    "id": "08071e1175b2da1fd796589ae112e40342693c65",
    "semantic_title": "pros: facial omni-representation learning via prototype-based self-distillation",
    "citation_count": 0,
    "authors": [
      "Xing Di",
      "Yiyu Zheng",
      "Xiaoming Liu",
      "Yu Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Li_TCP_Triplet_Contrastive-Relationship_Preserving_for_Class-Incremental_Learning_WACV_2024_paper.html": {
    "title": "TCP: Triplet Contrastive-Relationship Preserving for Class-Incremental Learning",
    "volume": "main",
    "abstract": "In class-incremental learning (CIL), when deep neural networks learn new classes, their recognition performance in old classes will drop significantly. This phenomenon is widely known as catastrophic forgetting. To alleviate catastrophic forgetting, existing methods store a small portion of old class data with a memory buffer and replay it while learning new classes. These methods suffer from a severe imbalance problem between old and new classes. In this paper, we discover that the imbalance problem in CIL makes it difficult to preserve the feature relation of old classes and hard to learn the feature relation between old and new classes. To mitigate the above two issues, we design a triplet contrastive preserving (TCP) loss to preserve old knowledge, and propose an asymmetric augmented contrastive learning (A2CL) method to learn new classes. Comprehensive experiments demonstrate the effectiveness of our method, which increases the average accuracies by 1.26% and 0.95% on CIFAR-100 and ImageNet. Especially under smaller memory buffer settings where the imbalance problem is more severe, our method can surpass the baselines by a large margin (up to 3.2%). We also show that TCP can be easily plugged into other methods and further improve their performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiyao Li",
      "Xuefei Ning",
      "Shanghang Zhang",
      "Lidong Guo",
      "Tianchen Zhao",
      "Huazhong Yang",
      "Yu Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Musallam_Self-Supervised_Learning_for_Place_Representation_Generalization_Across_Appearance_Changes_WACV_2024_paper.html": {
    "title": "Self-Supervised Learning for Place Representation Generalization Across Appearance Changes",
    "volume": "main",
    "abstract": "Visual place recognition is a key to unlocking spatial navigation for animals, humans and robots. While state-of-the-art approaches are trained in a supervised manner and, therefore, hardly capture the information needed for generalizing to unusual conditions. We argue that self-supervised learning may help abstracting the place representation so that it can be foreseen, irrespective of the conditions. More precisely, in this paper, we investigate learning features that are robust to appearance modifications while sensitive to geometric transformations in a self-supervised manner. This dual-purpose training is made possible by combining the two self-supervision main paradigms, i.e. contrastive and predictive learning. Our results on standard benchmarks reveal that jointly learning such appearance-robust and geometry-sensitive image descriptors leads to competitive visual place recognition results across adverse seasonal and illumination conditions without requiring any humanannotated labels",
    "checked": true,
    "id": "6df2d8ce11107b1d4a84a5a76744a98a1cf29d15",
    "semantic_title": "self-supervised learning for place representation generalization across appearance changes",
    "citation_count": 0,
    "authors": [
      "Mohamed Adel Musallam",
      "Vincent Gaudillière",
      "Djamila Aouada"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Patch-Based_Selection_and_Refinement_for_Early_Object_Detection_WACV_2024_paper.html": {
    "title": "Patch-Based Selection and Refinement for Early Object Detection",
    "volume": "main",
    "abstract": "Early object detection (OD) is a crucial task for the safety of many dynamic systems. Current OD algorithms have limited success for small objects at a long distance. To improve the accuracy and efficiency of such a task, we propose a novel set of algorithms that divide the image into patches, select patches with objects at various scales, elaborate the details of a small object, and detect it as early as possible. Our approach is built upon a transformer-based network and integrates the diffusion model to improve the detection accuracy. As demonstrated on BDD100K, our algorithms enhance the mAP for small objects from 1.03 to 8.93, and reduce the data volume in computation by more than 77%",
    "checked": true,
    "id": "f966c89d2f63061daed90706f243682a2e59af51",
    "semantic_title": "patch-based selection and refinement for early object detection",
    "citation_count": 1,
    "authors": [
      "Tianyi Zhang",
      "Kishore Kasichainula",
      "Yaoxin Zhuo",
      "Baoxin Li",
      "Jae-Sun Seo",
      "Yu Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Berrada_Guided_Distillation_for_Semi-Supervised_Instance_Segmentation_WACV_2024_paper.html": {
    "title": "Guided Distillation for Semi-Supervised Instance Segmentation",
    "volume": "main",
    "abstract": "Although instance segmentation methods have improved considerably, the dominant paradigm is to rely on fully annotated training images, which are tedious to obtain. To alleviate this reliance, and boost results, semi-supervised approaches leverage unlabeled data as an additional training signal that limits overfitting to the labeled samples. In this context, we present novel design choices to significantly improve teacher-student distillation models. In particular, we (i) improve the distillation approach by introducing a novel \"guided burn-in\" stage, and (ii) evaluate different instance segmentation architectures, as well as backbone networks and pre-training strategies. Contrary to previous work which uses only supervised data for the burn-in period of the student model, we also use guidance of the teacher model to exploit unlabeled data in the burn-in period. Our improved distillation approach leads to substantial improvements over previous state-of-the-art results. For example, on the Cityscapes dataset we improve mask-AP from 23.7 to 33.9 when using labels for 10% of images, and on the COCO dataset we improve mask-AP from 18.3 to 34.1 when using labels for only 1% of the training data",
    "checked": true,
    "id": "a9b1414974257253222a88ddf23688aae0d8941d",
    "semantic_title": "guided distillation for semi-supervised instance segmentation",
    "citation_count": 0,
    "authors": [
      "Tariq Berrada",
      "Camille Couprie",
      "Karteek Alahari",
      "Jakob Verbeek"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Albanese_Optimizing_Long-Term_Robot_Tracking_With_Multi-Platform_Sensor_Fusion_WACV_2024_paper.html": {
    "title": "Optimizing Long-Term Robot Tracking With Multi-Platform Sensor Fusion",
    "volume": "main",
    "abstract": "Monitoring a fleet of robots requires stable long-term tracking with re-identification, which is yet an unsolved challenge in many scenarios. One application of this is the analysis of autonomous robotic soccer games at RoboCup. Tracking in these games requires handling of identically looking players, strong occlusions, and non-professional video recordings, but also offers state information estimated by the robots. In order to make effective use of the information coming from the robot sensors, we propose a robust tracking and identification pipeline. It fuses external non-calibrated camera data with the robots' internal states using quadratic optimization for tracklet matching. The approach is validated using game recordings from previous RoboCup World Cup tournaments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giuliano Albanese",
      "Arka Mitra",
      "Jan-Nico Zaech",
      "Yupeng Zhao",
      "Ajad Chhatkuli",
      "Luc Van Gool"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Mehta_HyperMix_Out-of-Distribution_Detection_and_Classification_in_Few-Shot_Settings_WACV_2024_paper.html": {
    "title": "HyperMix: Out-of-Distribution Detection and Classification in Few-Shot Settings",
    "volume": "main",
    "abstract": "Out-of-distribution (OOD) detection is an important topic for real-world machine learning systems, but settings with limited in-distribution samples have been underexplored. Such few-shot OOD settings are challenging, as models have scarce opportunities to learn the data distribution before being tasked with identifying OOD samples. Indeed, we demonstrate that recent state-of-the-art OOD methods fail to outperform simple baselines in the few-shot setting. We thus propose a hypernetwork framework called HyperMix, using Mixup on the generated classifier parameters, as well as a natural out-of-episode outlier exposure technique that does not require an additional outlier dataset. We conduct experiments on CIFAR-FS and MiniImageNet, significantly outperforming other OOD methods in the few-shot regime",
    "checked": true,
    "id": "05c8494cdeef84764f6a46a0802a74a12fdb70bb",
    "semantic_title": "hypermix: out-of-distribution detection and classification in few-shot settings",
    "citation_count": 0,
    "authors": [
      "Nikhil Mehta",
      "Kevin J. Liang",
      "Jing Huang",
      "Fu-Jen Chu",
      "Li Yin",
      "Tal Hassner"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Bhattarai_TriPlaneNet_An_Encoder_for_EG3D_Inversion_WACV_2024_paper.html": {
    "title": "TriPlaneNet: An Encoder for EG3D Inversion",
    "volume": "main",
    "abstract": "Recent progress in NeRF-based GANs has introduced a number of approaches for high-resolution and high-fidelity generative modeling of human heads with a possibility for novel view rendering. At the same time, one must solve an inverse problem to be able to re-render or modify an existing image or video. Despite the success of universal optimization-based methods for 2D GAN inversion, those applied to 3D GANs may fail to extrapolate the result onto the novel view, whereas optimization-based 3D GAN inversion methods are time-consuming and can require at least several minutes per image. Fast encoder-based techniques, such as those developed for StyleGAN, may also be less appealing due to the lack of identity preservation. Our work introduces a fast technique that bridges the gap between the two approaches by directly utilizing the tri-plane representation presented for the EG3D generative model. In particular, we build upon a feed-forward convolutional encoder for the latent code and extend it with a fully-convolutional predictor of tri-plane numerical offsets. The renderings are similar in quality to the ones produced by optimization-based techniques and outperform the ones by encoder-based methods. As we empirically prove, this is a consequence of directly operating in the tri-plane space, not in the GAN parameter space, while making use of an encoder-based trainable approach. Finally, we demonstrate significantly more correct embedding of a face image in 3D than for all the baselines, further strengthened by a probably symmetric prior enabled during training",
    "checked": true,
    "id": "e996c09708a56ce074f4e72d1ae910a9a39f8b4f",
    "semantic_title": "triplanenet: an encoder for eg3d inversion",
    "citation_count": 11,
    "authors": [
      "Ananta R. Bhattarai",
      "Matthias Nießner",
      "Artem Sevastopolsky"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Anderson_Elusive_Images_Beyond_Coarse_Analysis_for_Fine-Grained_Recognition_WACV_2024_paper.html": {
    "title": "Elusive Images: Beyond Coarse Analysis for Fine-Grained Recognition",
    "volume": "main",
    "abstract": "While the community has seen many advances in recent years to address the challenging problem of Finegrained Visual Categorization (FGVC), progress seems to be slowing--new state-of-the-art methods often distinguish themselves by improving top-1 accuracy by mere tenths of a percent. However, across all of the now-standard FGVC datasets, there remain sizeable portions of the test data that none of the current state-of-the-art (SOTA) models can successfully predict. This paper provides a framework for identifying and studying the errors that current methods make across diverse fine-grained datasets. Three models of difficulty--Prediction Overlap, Prediction Rank and Pairwise Class Confusion--are employed to highlight the most challenging sets of images and classes. Extensive experiments apply a range of standard and SOTA methods, evaluating them on multiple FGVC domains and datasets. Insights acquired from coupling these difficulty paradigms with the careful analysis of experimental results suggest crucial areas for future FGVC research, focusing critically on the set of elusive images that none of the current models can correctly classify. Code is available at catalys1.github.io/elusive-images-fgvc",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Connor Anderson",
      "Matt Gwilliam",
      "Evelyn Gaskin",
      "Ryan Farrell"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Dunnhofer_Tracking_Skiers_From_the_Top_to_the_Bottom_WACV_2024_paper.html": {
    "title": "Tracking Skiers From the Top to the Bottom",
    "volume": "main",
    "abstract": "Skiing is a popular winter sport discipline with a long history of competitive events. In this domain, computer vision has the potential to enhance the understanding of athletes' performance, but its application lags behind other sports due to limited studies and datasets. This paper makes a step forward in filling such gaps. A thorough investigation is performed on the task of skier tracking in a video capturing his/her complete performance. Obtaining continuous and accurate skier localization is preemptive for further higher-level performance analyses. To enable the study, the largest and most annotated dataset for computer vision in skiing, SkiTB, is introduced. Several visual object tracking algorithms, including both established methodologies and a newly introduced skier-optimized baseline algorithm, are tested using the dataset. The results provide valuable insights into the applicability of different tracking methods for vision-based skiing analysis. SkiTB, code, and results are available at https://machinelearning.uniud.it/datasets/skitb",
    "checked": true,
    "id": "1788dd2188df73639ee0a3f932e82eb0dd918114",
    "semantic_title": "tracking skiers from the top to the bottom",
    "citation_count": 0,
    "authors": [
      "Matteo Dunnhofer",
      "Luca Sordi",
      "Niki Martinel",
      "Christian Micheloni"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Liu_BPKD_Boundary_Privileged_Knowledge_Distillation_for_Semantic_Segmentation_WACV_2024_paper.html": {
    "title": "BPKD: Boundary Privileged Knowledge Distillation for Semantic Segmentation",
    "volume": "main",
    "abstract": "Current knowledge distillation approaches in semantic segmentation tend to adopt a holistic approach that treats all spatial locations equally. However, for dense prediction, students' predictions on edge regions are highly uncertain due to contextual information leakage, requiring higher spatial sensitivity knowledge than the body regions. To address this challenge, this paper proposes a novel approach called boundary-privileged knowledge distillation (BPKD). it distils the knowledge from the teacher model's body and edges separately to the compact student model. Specifically, we employ two distinct loss functions: (i) edge loss, which aims to distinguish between ambiguous classes at the pixel level in edge regions; (ii) body loss, which utilizes shape constraints and selectively attends to the inner-semantic regions. Our experiments demonstrate that the proposed BPKD method provides extensive refinements and aggregation for edge and body regions. Additionally, the method achieves state-of-the-art distillation performance for semantic segmentation on three popular benchmark datasets, highlighting its effectiveness and generalization ability. BPKD shows consistent improvements across a diverse array of lightweight segmentation structures, including both CNNs and transformers, underscoring its architecture-agnostic adaptability",
    "checked": true,
    "id": "d88bad322c0b043ae0f2f00ccec1e3e10a55d68a",
    "semantic_title": "bpkd: boundary privileged knowledge distillation for semantic segmentation",
    "citation_count": 0,
    "authors": [
      "Liyang Liu",
      "Zihan Wang",
      "Minh Hieu Phan",
      "Bowen Zhang",
      "Jinchao Ge",
      "Yifan Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Xia_DREAM_Visual_Decoding_From_Reversing_Human_Visual_System_WACV_2024_paper.html": {
    "title": "DREAM: Visual Decoding From Reversing Human Visual System",
    "volume": "main",
    "abstract": "In this work we present DREAM, an fMRI-to-image method for reconstructing viewed images from brain activities, grounded on fundamental knowledge of the human visual system. We craft reverse pathways that emulate the hierarchical and parallel nature of how humans perceive the visual world. These tailored pathways are specialized to decipher semantics, color, and depth cues from fMRI data, mirroring the forward pathways from visual stimuli to fMRI recordings. To do so, two components mimic the inverse processes within the human visual system: the Reverse Visual Association Cortex (R-VAC) which reverses pathways of this brain region, extracting semantics from fMRI data; the Reverse Parallel PKM (R-PKM) component simultaneously predicting color and depth from fMRI signals. The experiments indicate that our method outperforms the current state-of-the-art models in terms of the consistency of appearance, structure, and semantics. Code will be available at https://github.com/weihaox/DREAM",
    "checked": true,
    "id": "5d998aed5a1b5143d4e79806cdb614281989587b",
    "semantic_title": "dream: visual decoding from reversing human visual system",
    "citation_count": 1,
    "authors": [
      "Weihao Xia",
      "Raoul de Charette",
      "Cengiz Oztireli",
      "Jing-Hao Xue"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Felt_Seeing_Stars_Learned_Star_Localization_for_Narrow-Field_Astrometry_WACV_2024_paper.html": {
    "title": "Seeing Stars: Learned Star Localization for Narrow-Field Astrometry",
    "volume": "main",
    "abstract": "Star localization in astronomical imagery is a computer vision task that underpins satellite tracking. Astronomical star extraction techniques often struggle to detect stars when applied to satellite tracking imagery due to the narrower fields of view and rate track observational modes of satellite tracking telescopes. We present a large dataset of real narrow-field rate-tracked imagery with ground truth stars, created using a combination of existing star detection techniques, an astrometric engine, and a star catalog. We train three state of the art object detection, instance segmentation, and line segment detection models on this dataset and evaluate them with object-wise, pixel-wise, and astrometric metrics. Our proposed approaches require no metadata; when paired with a lost-in-space astrometric engine, they find astrometric fits based solely on uncorrected image pixels. Experimental results on real data indicate the effectiveness of learned star detection: we report astrometric fit rates over double that of classical star detection algorithms, improved dim star recall, and comparable star localization residuals",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Violet Felt",
      "Justin Fletcher"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Demir_How_Do_Deepfakes_Move_Motion_Magnification_for_Deepfake_Source_Detection_WACV_2024_paper.html": {
    "title": "How Do Deepfakes Move? Motion Magnification for Deepfake Source Detection",
    "volume": "main",
    "abstract": "With the proliferation of deep generative models, deepfakes are improving in quality and quantity everyday. However, there are subtle authenticity signals in pristine videos, not replicated by current generative models. We contrast the movement in deepfakes and authentic videos by motion magnification towards building a generalized deepfake source detector. The sub-muscular motion in faces has different interpretations per different generative models, which is reflected in their generative residue. Our approach exploits the difference between real motion and the amplified generative artifacts, by combining deep and traditional motion magnification, to detect whether a video is fake and its source generator if so. Evaluating our approach on two multi-source datasets, we obtain 97.77% and 94.03% for video source detection. Our approach performs at least 4.08% better than the prior deepfake source detector and other complex architectures. We also analyze magnification amount, phase extraction window, backbone network, sample counts, and sample lengths. Finally, we report our results on skin tones and genders to assess the model bias",
    "checked": true,
    "id": "649b3575b51c8f22afcb725707957111eaa92d7c",
    "semantic_title": "how do deepfakes move? motion magnification for deepfake source detection",
    "citation_count": 2,
    "authors": [
      "Ilke Demir",
      "Umur Aybars Çiftçi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Gopal_Separable_Self_and_Mixed_Attention_Transformers_for_Efficient_Object_Tracking_WACV_2024_paper.html": {
    "title": "Separable Self and Mixed Attention Transformers for Efficient Object Tracking",
    "volume": "main",
    "abstract": "The deployment of transformers for visual object tracking has shown state-of-the-art results on several benchmarks. However, the transformer-based models are under-utilized for Siamese lightweight tracking due to the computational complexity of their attention blocks. This paper proposes an efficient self and mixed attention transformer-based architecture for lightweight tracking. The proposed backbone utilizes the separable mixed attention transformers to fuse the template and search regions during feature extraction to generate superior feature encoding. Our prediction head performs global contextual modeling of the encoded features by leveraging efficient self-attention blocks for robust target state estimation. With these contributions, the proposed lightweight tracker deploys a transformer-based backbone and head module concurrently for the first time. Our ablation study testifies to the effectiveness of the proposed combination of backbone and head modules. Simulations show that our Separable Self and Mixed Attention-based Tracker, SMAT, surpasses the performance of related lightweight trackers on GOT10k, TrackingNet, LaSOT, NfS30, UAV123, and AVisT datasets, while running at 37 fps on CPU, 158 fps on GPU, and having 3.8M parameters. For example, it significantly surpasses the closely related trackers E.T.Track and MixFormerV2-S on GOT10k-test by a margin of 7.9% and 5.8%, respectively, in the AO metric. The tracker code and model is available at https://github.com/goutamyg/SMAT",
    "checked": true,
    "id": "c56f3121ccd1e2468ae797c757f2b29d91d70f85",
    "semantic_title": "separable self and mixed attention transformers for efficient object tracking",
    "citation_count": 0,
    "authors": [
      "Goutam Yelluru Gopal",
      "Maria A. Amer"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ganz_CLIPAG_Towards_Generator-Free_Text-to-Image_Generation_WACV_2024_paper.html": {
    "title": "CLIPAG: Towards Generator-Free Text-to-Image Generation",
    "volume": "main",
    "abstract": "Perceptually Aligned Gradients (PAG) refer to an intriguing property observed in robust image classification models, wherein their input gradients align with human perception and pose semantic meanings. While this phenomenon has gained significant research attention, it was solely studied in the context of unimodal vision-only architectures. In this work, we extend the study of PAG to Vision-Language architectures, which form the foundations for diverse image-text tasks and applications. Through an adversarial robustification finetuning of CLIP, we demonstrate that robust Vision-Language models exhibit PAG in contrast to their vanilla counterparts. This work reveals the merits of CLIP with PAG (CLIPAG) in several vision-language generative tasks. Notably, we show that seamlessly integrating CLIPAG in a \"plug-n-play\" manner leads to substantial improvements in vision-language generative applications. Furthermore, leveraging its PAG property, CLIPAG enables text-to-image generation without any generative model, which typically requires huge generators",
    "checked": true,
    "id": "291d92da53d182c0fdf7eea465c9b519ef1fc1f3",
    "semantic_title": "clipag: towards generator-free text-to-image generation",
    "citation_count": 1,
    "authors": [
      "Roy Ganz",
      "Michael Elad"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Rami_Source-Guided_Similarity_Preservation_for_Online_Person_Re-Identification_WACV_2024_paper.html": {
    "title": "Source-Guided Similarity Preservation for Online Person Re-Identification",
    "volume": "main",
    "abstract": "Online Unsupervised Domain Adaptation (OUDA) for person Re-Identification (Re-ID) is the task of continuously adapting a model trained on a well-annotated source-domain dataset to a target domain observed as a data stream. In OUDA, person Re-ID models face two main challenges: catastrophic forgetting and domain shift. In this work, we propose a new Source-guided Similarity Preservation (S2P) framework to alleviate these two problems. Our framework is based on the extraction of a support set composed of source images that maximizes the similarity with the target data. This support set is used to identify feature similarities that must be preserved during the learning process. S2P can incorporate multiple existing UDA methods to mitigate catastrophic forgetting. Our experiments show that S2P outperforms previous state-of-the-art methods on multiple real-to-real and synthetic-to-real challenging OUDA benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hamza Rami",
      "Jhony H. Giraldo",
      "Nicolas Winckler",
      "Stéphane Lathuilière"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Maag_Uncertainty-Weighted_Loss_Functions_for_Improved_Adversarial_Attacks_on_Semantic_Segmentation_WACV_2024_paper.html": {
    "title": "Uncertainty-Weighted Loss Functions for Improved Adversarial Attacks on Semantic Segmentation",
    "volume": "main",
    "abstract": "State-of-the-art deep neural networks have been shown to be extremely powerful in a variety of perceptual tasks like semantic segmentation. However, these networks are vulnerable to adversarial perturbations of the input which are imperceptible for humans but lead to incorrect predictions. Treating image segmentation as a sum of pixel-wise classifications, adversarial attacks developed for classification models were shown to be applicable to segmentation models as well. In this work, we present simple uncertainty-based weighting schemes for the loss functions of such attacks that (i) put higher weights on pixel classifications which can more easily perturbed and (ii) zero-out the pixel-wise losses corresponding to those pixels that are already confidently misclassified. The weighting schemes can be easily integrated into the loss function of a range of well-known adversarial attackers with minimal additional computational overhead, but lead to significant improved perturbation performance, as we demonstrate in our empirical analysis on several datasets and models",
    "checked": true,
    "id": "d3fe3635e3288bd378b8d6228ef10536d854cbb0",
    "semantic_title": "uncertainty-weighted loss functions for improved adversarial attacks on semantic segmentation",
    "citation_count": 0,
    "authors": [
      "Kira Maag",
      "Asja Fischer"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Rai_Towards_Realistic_Generative_3D_Face_Models_WACV_2024_paper.html": {
    "title": "Towards Realistic Generative 3D Face Models",
    "volume": "main",
    "abstract": "In recent years, there has been significant progress in 2D generative face models fueled by applications such as animation, synthetic data generation, and digital avatars. However, due to the absence of 3D information, these 2D models often struggle to accurately disentangle facial attributes like pose, expression, and illumination, limiting their editing capabilities. To address this limitation, this paper proposes a 3D controllable generative face model to produce high-quality albedo and precise 3D shapes by leveraging existing 2D generative models. By combining 2D face generative models with semantic face manipulation, this method enables editing of detailed 3D rendered faces. The proposed framework utilizes an alternating descent optimization approach over shape and albedo. Differentiable rendering is used to train high-quality shapes and albedo without 3D supervision. Moreover, this approach outperforms most state-of-the-art (SOTA) methods in the well-known NoW and REALY benchmarks for 3D face reconstruction. It also outperforms the SOTA reconstruction models in recovering rendered faces' identities across novel poses. Additionally, the paper demonstrates direct control of expressions in 3D faces by exploiting latent space leading to text-based editing of 3D faces",
    "checked": true,
    "id": "95ef8ca0c2d50e3d3d523812985d4dfcc9137651",
    "semantic_title": "towards realistic generative 3d face models",
    "citation_count": 5,
    "authors": [
      "Aashish Rai",
      "Hiresh Gupta",
      "Ayush Pandey",
      "Francisco Vicente Carrasco",
      "Shingo Jason Takagi",
      "Amaury Aubel",
      "Daeil Kim",
      "Aayush Prakash",
      "Fernando De la Torre"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Aminbeidokhti_Domain_Generalization_by_Rejecting_Extreme_Augmentations_WACV_2024_paper.html": {
    "title": "Domain Generalization by Rejecting Extreme Augmentations",
    "volume": "main",
    "abstract": "Data augmentation is one of the most powerful techniques for regularizing deep learning models and improving their recognition performance in a variety of tasks and domains. However, this holds for standard in-domain settings, in which the training and test data follow the same distribution. For the out-domain, in which the test data follows a different and unknown distribution, the best recipe for data augmentation is not clear. In this paper, we show that also for out-domain or domain generalization settings, data augmentation can bring a conspicuous and robust improvement in performance. For doing that, we propose a simple procedure: i) use uniform sampling on standard data augmentation transformations ii) increase transformations strength to adapt to the higher data variance expected when working out of domain iii) devise a new reward function to reject extreme transformations that can harm the training. With this simple formula, our data augmentation scheme achieves comparable or better results to state-of-the-art performance on most domain generalization datasets",
    "checked": true,
    "id": "1ce3e2440dd53beaaaeb0677483a9af5310e4e5b",
    "semantic_title": "domain generalization by rejecting extreme augmentations",
    "citation_count": 0,
    "authors": [
      "Masih Aminbeidokhti",
      "Fidel A. Guerrero Peña",
      "Heitor Rapela Medeiros",
      "Thomas Dubail",
      "Eric Granger",
      "Marco Pedersoli"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Prashanth_Towards_Accurate_Disease_Segmentation_in_Plant_Images_A_Comprehensive_Dataset_WACV_2024_paper.html": {
    "title": "Towards Accurate Disease Segmentation in Plant Images: A Comprehensive Dataset Creation and Network Evaluation",
    "volume": "main",
    "abstract": "Automated disease segmentation in plant images plays a crucial role in identifying and mitigating the impact of plant diseases on agricultural productivity. In this study, we address the problem of Northern Leaf Blight (NLB) disease segmentation in maize plants. We present a comprehensive dataset of 1000 plant images annotated with NLB disease regions. We employ the Mask R-CNN and Cascaded Mask R-CNN models with various backbone architectures to perform NLB disease segmentation. The experimental results demonstrate the effectiveness of the models in accurately delineating NLB disease regions. Specifically, the ResNet Strikes Back-50 backbone architecture achieves the highest mean average precision (mAP) score, indicating its ability to capture intricate details of NLB disease spots. Additionally, the cascaded approach enhances segmentation accuracy compared to the single-stage Mask R-CNN models. Our findings provide valuable insights into the performance of different backbone architectures and contribute to the development of automated NLB disease segmentation methods in plant images. The generated dataset and experimental results serve as a resource for further research in plant disease segmentation and management",
    "checked": true,
    "id": "bec5a34ae3cd081daf2f213892cdb58755453a02",
    "semantic_title": "towards accurate disease segmentation in plant images: a comprehensive dataset creation and network evaluation",
    "citation_count": 0,
    "authors": [
      "Komuravelli Prashanth",
      "Jaladi Sri Harsha",
      "Sivapuram Arun Kumar",
      "Jaladi Srilekha"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhao_Deep_Subdomain_Alignment_for_Cross-Domain_Image_Classification_WACV_2024_paper.html": {
    "title": "Deep Subdomain Alignment for Cross-Domain Image Classification",
    "volume": "main",
    "abstract": "Unsupervised domain adaptation (UDA), which aims to transfer knowledge learned from a labeled source domain to an unlabeled target domain, is useful for various cross-domain image classification scenarios. A commonly used approach for UDA is to minimize the distribution differences between two domains, and subdomain alignment is found to be an effective method. However, most of the existing subdomain alignment methods are based on adversarial learning and focus on subdomain alignment procedures without considering the discriminability among individual subdomains, resulting in slow convergence and unsatisfactory adaptation results. To address these issues, we propose a novel deep subdomain alignment method for UDA in image classification, which consists of a Union Subdomain Contrastive Learning (USCL) module and a Multi-view Subdomain Alignment (MvSA) strategy. USCL can create discriminative and dispersed subdomains by bringing samples from the same subdomain closer while pushing away samples from different subdomains. MvSA makes use of labeled source domain data and easy target domain data to perform target-to-source and target-to-target alignment. Experimental results on three image classification datasets (Office-31, Office-Home, Visda-17) demonstrate that our proposed method is effective for UDA and achieves promising results in several cross-domain image classification tasks",
    "checked": false,
    "id": "7d79423db4d157e89291a3f9e05ece37837272c4",
    "semantic_title": "correlation subdomain alignment network based cross-domain hyperspectral image classification method",
    "citation_count": 0,
    "authors": [
      "Yewei Zhao",
      "Hu Han",
      "Shiguang Shan",
      "Xilin Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Chien_Classifying_Cable_Tendency_With_Semantic_Segmentation_by_Utilizing_Real_and_WACV_2024_paper.html": {
    "title": "Classifying Cable Tendency With Semantic Segmentation by Utilizing Real and Simulated RGB Data",
    "volume": "main",
    "abstract": "Cable tendency is the potential shape or characteristic that a cable may possess while being manipulated, of which some are considered erroneous and should be identified as a part of anomaly detection during an automatic manipulation. This research explores the ability of deep-learning models in learning the cable tendencies that, contrary to typical classification tasks of multi-object scenarios, is to differentiate the multiple states displayable by the same object -- in this case, cables. By training multiple models with different combinations of self-collected real-world data and self-generated simulation data, a comparative study is carried out to compare the performance of each approach. In conclusion, the effectiveness of detecting three abnormal states and shapes of cables, and using simulation data is certificated in experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pei-Chun Chien",
      "Powei Liao",
      "Eiji Fukuzawa",
      "Jun Ohya"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Springstein_Visual_Narratives_Large-Scale_Hierarchical_Classification_of_Art-Historical_Images_WACV_2024_paper.html": {
    "title": "Visual Narratives: Large-Scale Hierarchical Classification of Art-Historical Images",
    "volume": "main",
    "abstract": "Iconography refers to the methodical study and interpretation of thematic content in the visual arts, distinguishing it, e.g., from purely formal or aesthetic considerations. In iconographic studies, Iconclass is a widely used taxonomy that encapsulates historical, biblical, and literary themes, among others. However, given the hierarchical nature and inherent complexity of such a taxonomy, it is highly desirable to use automated methods for (Iconclass-based) image classification. Previous studies either focused narrowly on certain subsets of narratives or failed to exploit Iconclass's hierarchical structure. In this paper, we propose a novel approach for Hierarchical Multi-label Classification (HMC) of iconographic concepts in images. We present three strategies, including Large Language Models (LLMs), for the generation of textual image descriptions using keywords extracted from Iconclass. These descriptions are utilized to pre-train a Vision-Language Model (VLM) based on a newly introduced data set of 477,569 images with more than 20,000 Iconclass concepts, far more than considered in previous studies. Furthermore, we present five approaches to multi-label classification, including a novel transformer decoder that leverages hierarchical information from the Iconclass taxonomy. Experimental results show the superiority of this approach over reasonable baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthias Springstein",
      "Stefanie Schneider",
      "Javad Rahnama",
      "Julian Stalter",
      "Maximilian Kristen",
      "Eric Müller-Budack",
      "Ralph Ewerth"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Karim_Real-Time_Weakly_Supervised_Video_Anomaly_Detection_WACV_2024_paper.html": {
    "title": "Real-Time Weakly Supervised Video Anomaly Detection",
    "volume": "main",
    "abstract": "Weakly supervised video anomaly detection is an important problem in many real-world applications where during training there are some anomalous videos, in addition to nominal videos, without labelled frames to indicate when the anomaly happens. State-of-the-art methods in this domain typically focus on offline anomaly detection without any concern for real-time detection. Most of these methods rely on ad hoc feature aggregation techniques and the use of metric learning losses, which limit the ability of the models to detect anomalies in real-time. In line with the premise of deep neural networks, there also has been a growing interest in developing end-to-end approaches that can automatically learn effective features directly from the raw data. We propose the first real-time and end-to-end trained algorithm for weakly supervised video anomaly detection. Our training procedure builds upon recent action recognition literature and uses a trainable video model to learn visual features. This is in contrast to existing approaches which largely depend on pre-trained feature extractors. The proposed method significantly improves the anomaly detection speed and AUC performance compared to the existing methods. Specifically, on the UCF-Crime dataset, our method achieves 86.94% AUC with a decision period of 6.4 seconds while the competing methods achieve at most 85.92% AUC with a decision period of 273 seconds",
    "checked": false,
    "id": "a80ffd49a32f575028888e3b91092ac1e5a857b4",
    "semantic_title": "batchnorm-based weakly supervised video anomaly detection",
    "citation_count": 0,
    "authors": [
      "Hamza Karim",
      "Keval Doshi",
      "Yasin Yilmaz"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kulkarni_C2AIR_Consolidated_Compact_Aerial_Image_Haze_Removal_WACV_2024_paper.html": {
    "title": "C2AIR: Consolidated Compact Aerial Image Haze Removal",
    "volume": "main",
    "abstract": "Aerial image haze removal deals with improving the visibility and quality of images captured from aerial platforms, such as drones and satellites. Aerial images are commonly used in various applications such as environmental monitoring, and disaster response. These applications usually require cleaner data for accurate functioning. However, atmospheric conditions such as haze or fog can significantly degrade the quality of these images, reducing their contrast, color saturation, and sharpness, making it difficult to extract meaningful information from them. Existing methods rely on computationally heavy and haze density (light, moderate, dense) specific architectures for aerial image dehazing. In light of these limitations, we propose a novel lightweight and consolidated approach for aerial image dehazing. In this approach, we propose Density Aware Query Modulated Block for learning weather degradations in input features and guiding the restoration process. Further, we propose Cross Collaborative Feed-Forward Block for learning to restore varying sizes of the structures in the input images. Finally, we propose Gated Adaptive Feature Fusion block to achieve inter-scale and intra-feature attentive fusion, effective for aerial image restoration. Extensive analysis on benchmark aerial image dehazing datasets and real-world images, along with detailed ablation studies validate the effectiveness of the proposed approach. Further, we have analysed our method for other restoration task such as underwater image enhancement to experiment its wide applicability. The code is available at https: //github.com/AshutoshKulkarni4998/C2AIR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashutosh Kulkarni",
      "Shruti S. Phutke",
      "Santosh Kumar Vipparthi",
      "Subrahmanyam Murala"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Tran_Permutation-Aware_Activity_Segmentation_via_Unsupervised_Frame-To-Segment_Alignment_WACV_2024_paper.html": {
    "title": "Permutation-Aware Activity Segmentation via Unsupervised Frame-To-Segment Alignment",
    "volume": "main",
    "abstract": "This paper presents an unsupervised transformer-based framework for temporal activity segmentation which leverages not only frame-level cues but also segment-level cues. This is in contrast with previous methods which often rely on frame-level information only. Our approach begins with a frame-level prediction module which estimates framewise action classes via a transformer encoder. The frame-level prediction module is trained in an unsupervised manner via temporal optimal transport. To exploit segment-level information, we utilize a segment-level prediction module and a frame-to-segment alignment module. The former includes a transformer decoder for estimating video transcripts, while the latter matches frame-level features with segment-level features, yielding permutation-aware segmentation results. Moreover, inspired by temporal optimal transport, we introduce simple-yet-effective pseudo labels for unsupervised training of the above modules. Our experiments on four public datasets, i.e., 50 Salads, YouTube Instructions, Breakfast, and Desktop Assembly show that our approach achieves comparable or better performance than previous methods in unsupervised activity segmentation",
    "checked": false,
    "id": "9e941646b124aa5ff508ba97ed0549620d224df5",
    "semantic_title": "permutation-aware action segmentation via unsupervised frame-to-segment alignment",
    "citation_count": 2,
    "authors": [
      "Quoc-Huy Tran",
      "Ahmed Mehmood",
      "Muhammad Ahmed",
      "Muhammad Naufil",
      "Anas Zafar",
      "Andrey Konin",
      "Zeeshan Zia"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Hesse_Prototype_Learning_for_Explainable_Brain_Age_Prediction_WACV_2024_paper.html": {
    "title": "Prototype Learning for Explainable Brain Age Prediction",
    "volume": "main",
    "abstract": "The lack of explainability of deep learning models limits the adoption of such models in clinical practice. Prototype-based models can provide inherent explainable predictions, but these have predominantly been designed for classification tasks, despite many important tasks in medical imaging being continuous regression problems. Therefore, in this work, we present ExPeRT: an explainable prototype-based model specifically designed for regression tasks. Our proposed model makes a sample prediction from the distances to a set of learned prototypes in latent space, using a weighted mean of prototype labels. The distances in latent space are regularized to be relative to label differences, and each of the prototypes can be visualized as a sample from the training set. The image-level distances are further constructed from patch-level distances, in which the patches of both images are structurally matched using optimal transport. This thus provides an example-based explanation with patch-level detail at inference time. We demonstrate our proposed model for brain age prediction on two imaging datasets: adult MR and fetal ultrasound. Our approach achieved state-of-the-art prediction performance while providing insight into the model's reasoning process",
    "checked": true,
    "id": "0756a9a3765c9982e4b02f4530efaea031c233a1",
    "semantic_title": "prototype learning for explainable brain age prediction",
    "citation_count": 0,
    "authors": [
      "Linde S. Hesse",
      "Nicola K. Dinsdale",
      "Ana I. L. Namburete"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wan_Exploiting_CLIP_for_Zero-Shot_HOI_Detection_Requires_Knowledge_Distillation_at_WACV_2024_paper.html": {
    "title": "Exploiting CLIP for Zero-Shot HOI Detection Requires Knowledge Distillation at Multiple Levels",
    "volume": "main",
    "abstract": "In this paper, we investigate the task of zero-shot human-object interaction (HOI) detection, a novel paradigm for identifying HOIs without the need for task-specific annotations. To address this challenging task, we employ CLIP, a large-scale pre-trained vision-language model (VLM), for knowledge distillation on multiple levels. To this end, we design a multi-branch neural network that leverages CLIP for learning HOI representations at various levels, including global images, local union regions encompassing human-object pairs, and individual instances of humans or objects. To train our model, CLIP is utilized to generate HOI scores for both global images and local union regions that serve as supervision signals. The extensive experiments demonstrate the effectiveness of our novel multi-level CLIP knowledge integration strategy. Notably, the model achieves strong performance, which is even comparable with some fully-supervised and weakly-supervised methods on the public HICO-DET benchmark",
    "checked": true,
    "id": "99905ba4f5c462b0026f3cc1b59d4f4d0cfd7155",
    "semantic_title": "exploiting clip for zero-shot hoi detection requires knowledge distillation at multiple levels",
    "citation_count": 0,
    "authors": [
      "Bo Wan",
      "Tinne Tuytelaars"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Li_SDNet_An_Extremely_Efficient_Portrait_Matting_Model_via_Self-Distillation_WACV_2024_paper.html": {
    "title": "SDNet: An Extremely Efficient Portrait Matting Model via Self-Distillation",
    "volume": "main",
    "abstract": "Most existing portrait matting models either require expensive auxiliary information or try to decompose the task into sub-tasks that are usually resource-hungry. These challenges limit its application on low-power computing devices. In addition, mobile networks tend to be less powerful than those cumbersome ones in feature representation mining. In this paper, we propose an extremely efficient portrait matting model via self-distillation (SDNet), that aims to provide a solution to performing accurate and effective portrait matting with limited computing resources. Our SDNet contains only 2M parameters, 2.2% of the parameters of MGM, and 1.5% of that of Matteformer. We introduce the training pipeline of self-distillation that can improve our lightweight baseline model without any parameter addition, network modification, or over-parameterized teacher models which need well-pretraining. Extensive experiments demonstrate the effectiveness of our self-distillation method and the lightweight SDNet network. Our SDNet outperforms the state-of-the-art (SOTA) lightweight approaches on both synthetic and real-world images",
    "checked": false,
    "id": "05abddfc2dcc0d975bb69766ebf125485cffb912",
    "semantic_title": "sdnet: an extremely efﬁcient portrait matting model via self-distillation",
    "citation_count": 0,
    "authors": [
      "Ziwen Li",
      "Bo Xu",
      "Jiake Xie",
      "Yong Tang",
      "Cheng Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Han_Hybrid_Neural_Diffeomorphic_Flow_for_Shape_Representation_and_Generation_via_WACV_2024_paper.html": {
    "title": "Hybrid Neural Diffeomorphic Flow for Shape Representation and Generation via Triplane",
    "volume": "main",
    "abstract": "Deep Implicit Functions (DIFs) have gained popularity in 3D computer vision due to their compactness and continuous representation capabilities. However, addressing dense correspondences and semantic relationships across DIF-encoded shapes remains a critical challenge, limiting their applications in texture transfer and shape analysis. Moreover, recent endeavors in 3D shape generation using DIFs often neglect correspondence and topology preservation. This paper presents HNDF (Hybrid Neural Diffeomorphic Flow), a method that implicitly learns the underlying representation and decomposes intricate dense correspondences into explicitly axis-aligned triplane features. To avoid suboptimal representations trapped in local minima, we propose hybrid supervision that captures both local and global correspondences. Unlike conventional approaches that directly generate new 3D shapes, we further explore the idea of shape generation with deformed template shape via diffeomorphic flows, where the deformation is encoded by the generated triplane features. Leveraging a pre-existing 2D diffusion model, we produce high-quality and diverse 3D diffeomorphic flows through generated triplanes features, ensuring topological consistency with the template shape. Extensive experiments on medical image organ segmentation datasets evaluate the effectiveness of HNDF in 3D shape representation and generation",
    "checked": true,
    "id": "416f1b4c76a75bf893e927c51adbeb4c94856e24",
    "semantic_title": "hybrid neural diffeomorphic flow for shape representation and generation via triplane",
    "citation_count": 0,
    "authors": [
      "Kun Han",
      "Shanlin Sun",
      "Thanh-Tung Le",
      "Xiangyi Yan",
      "Haoyu Ma",
      "Chenyu You",
      "Xiaohui Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Benaim_Volumetric_Disentanglement_for_3D_Scene_Manipulation_WACV_2024_paper.html": {
    "title": "Volumetric Disentanglement for 3D Scene Manipulation",
    "volume": "main",
    "abstract": "Recently, advances in differential volumetric rendering enabled significant breakthroughs in the photo-realistic and fine-detailed reconstruction of complex 3D scenes, which is key for many virtual reality applications. However, in the context of augmented reality, one may also wish to effect semantic manipulations or augmentations of objects within a scene. To this end, we propose a volumetric framework for (i) disentangling or separating, the volumetric representation of a given foreground object from the background, and (ii) semantically manipulating the foreground object, as well as the background. Our method enables the separate control of pixel color and depth as well as 3D similarity transformations of both the foreground and background objects. We subsequently demonstrate our framework's applicability on several downstream manipulation tasks, going beyond the placement and movement of foreground objects. These tasks include object camouflage, non-negative 3D object inpainting, 3D object translation, 3D object inpainting, and 3D text-based object manipulation. Our framework takes as input a set of 2D masks specifying the desired foreground object for training views, together with the associated 2D views and poses, and produces a foreground-background disentanglement that respects the surrounding illumination, reflections, and partial occlusions, which can be applied to both training and novel views",
    "checked": true,
    "id": "e855b7797de4a9d78121f03adb66d89d239217d2",
    "semantic_title": "volumetric disentanglement for 3d scene manipulation",
    "citation_count": 11,
    "authors": [
      "Sagie Benaim",
      "Frederik Warburg",
      "Peter Ebert Christensen",
      "Serge Belongie"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zheng_CAILA_Concept-Aware_Intra-Layer_Adapters_for_Compositional_Zero-Shot_Learning_WACV_2024_paper.html": {
    "title": "CAILA: Concept-Aware Intra-Layer Adapters for Compositional Zero-Shot Learning",
    "volume": "main",
    "abstract": "In this paper, we study the problem of Compositional Zero-Shot Learning (CZSL), which is to recognize novel attribute-object combinations with pre-existing concepts. Recent researchers focus on applying large-scale Vision-Language Pre-trained (VLP) models like CLIP with strong generalization ability. However, these methods treat the pre-trained model as a black box and focus on pre- and post-CLIP operations, which do not inherently mine the semantic concept between the layers inside CLIP. We propose to dive deep into the architecture and insert adapters, a parameter-efficient technique proven to be effective among large language models, into each CLIP encoder layer. We further equip adapters with concept awareness so that concept-specific features of \"object\", \"attribute\", and \"composition\" can be extracted. We assess our method on four popular CZSL datasets, MIT-States, C-GQA, UT-Zappos, and VAW-CZSL, which shows state-of-the-art performance compared to existing methods on all of them",
    "checked": true,
    "id": "3e23df3d723e35a1eead16c8131cae3feec92343",
    "semantic_title": "caila: concept-aware intra-layer adapters for compositional zero-shot learning",
    "citation_count": 0,
    "authors": [
      "Zhaoheng Zheng",
      "Haidong Zhu",
      "Ram Nevatia"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Capitani_ClusterFix_A_Cluster-Based_Debiasing_Approach_Without_Protected-Group_Supervision_WACV_2024_paper.html": {
    "title": "ClusterFix: A Cluster-Based Debiasing Approach Without Protected-Group Supervision",
    "volume": "main",
    "abstract": "The failures of Deep Networks can sometimes be ascribed to biases in the data or algorithmic choices. Existing debiasing approaches exploit prior knowledge to avoid unintended solutions; we acknowledge that, in real-world settings, it could be unfeasible to gather enough prior information to characterize the bias, or it could even raise ethical considerations. We hence propose a novel debiasing approach, termed ClusterFix, which does not require any external hint about the nature of biases. Such an approach alters the standard empirical risk minimization and introduces a per-example weight, encoding how critical and far from the majority an example is. Notably, the weights consider how difficult it is for the model to infer the correct pseudo-label, which is obtained in a self-supervised manner by dividing examples into multiple clusters. Extensive experiments show that the misclassification error incurred in identifying the correct cluster allows for identifying examples prone to bias-related issues. As a result, our approach outperforms existing methods on standard benchmarks for bias removal and fairness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giacomo Capitani",
      "Federico Bolelli",
      "Angelo Porrello",
      "Simone Calderara",
      "Elisa Ficarra"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Cohen_Simple_Post-Training_Robustness_Using_Test_Time_Augmentations_and_Random_Forest_WACV_2024_paper.html": {
    "title": "Simple Post-Training Robustness Using Test Time Augmentations and Random Forest",
    "volume": "main",
    "abstract": "Although Deep Neural Networks (DNNs) achieve excellent performance on many real-world tasks, they are highly vulnerable to adversarial attacks. A leading defense against such attacks is adversarial training, a technique in which a DNN is trained to be robust to adversarial attacks by introducing adversarial noise to its input. This procedure is effective but must be done during the training phase. In this work, we propose Augmented Random Forest (ARF), a simple and easy-to-use strategy for robustifying an existing pretrained DNN without modifying its weights. For every image, we generate randomized test time augmentations by applying diverse color, blur, noise, and geometric transforms. Then we use the DNN's logits output to train a simple random forest to predict the real class label. Our method achieves state-of-the-art adversarial robustness on a diversity of white and black box attacks with minimal compromise on the natural images' classification. We test ARF also against numerous adaptive white-box attacks and it shows excellent results when combined with adversarial training",
    "checked": false,
    "id": "de8ac5752362608da615bc695792a9839422d5c9",
    "semantic_title": "a deep learning model using geostationary satellite data for forest fire detection with reduced detection latency",
    "citation_count": 19,
    "authors": [
      "Gilad Cohen",
      "Raja Giryes"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Mazumder_Learning_Low-Rank_Latent_Spaces_With_Simple_Deterministic_Autoencoder_Theoretical_and_WACV_2024_paper.html": {
    "title": "Learning Low-Rank Latent Spaces With Simple Deterministic Autoencoder: Theoretical and Empirical Insights",
    "volume": "main",
    "abstract": "The autoencoder is an unsupervised learning paradigm that aims to create a compact latent representation of data by minimizing the reconstruction loss. However, it tends to overlook the fact that most data (images) are embedded in a lower-dimensional latent space, which is crucial for effective data representation. To address this limitation, we propose a novel approach called Low-Rank Autoencoder (LoRAE). In LoRAE, we incorporated a low-rank regularizer to adaptively learn a low-dimensional latent space while preserving the basic objective of an autoencoder. This helps embed the data in a lower-dimensional latent space while preserving important information. It is a simple autoencoder extension that learns low-rank latent space. Theoretically, we establish a tighter error bound for our model. Empirically, our model's superiority shines through various tasks such as image generation and downstream classification. Both theoretical and practical outcomes highlight the importance of acquiring low-dimensional embeddings",
    "checked": true,
    "id": "2da2a1c650eb93a7d3989dfef0fddca65e8f15c1",
    "semantic_title": "learning low-rank latent spaces with simple deterministic autoencoder: theoretical and empirical insights",
    "citation_count": 0,
    "authors": [
      "Alokendu Mazumder",
      "Tirthajit Baruah",
      "Bhartendu Kumar",
      "Rishab Sharma",
      "Vishwajeet Pattanaik",
      "Punit Rathore"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Khan_A_Hybrid_Graph_Network_for_Complex_Activity_Detection_in_Video_WACV_2024_paper.html": {
    "title": "A Hybrid Graph Network for Complex Activity Detection in Video",
    "volume": "main",
    "abstract": "Interpretation and understanding of video presents a challenging computer vision task in numerous fields - e.g. autonomous driving and sports analytics. Existing approaches to interpreting the actions taking place within a video clip are based upon Temporal Action Localisation (TAL), which typically identifies short-term actions. The emerging field of Complex Activity Detection (CompAD) extends this analysis to long-term activities, with a deeper understanding obtained by modelling the internal structure of a complex activity taking place within the video. We address the CompAD problem using a hybrid graph neural network which combines attention applied to a graph encoding the local (short-term) dynamic scene with a temporal graph modelling the overall long-duration activity. Our approach is as follows: i) Firstly, we propose a novel feature extraction technique which, for each video snippet, generates spatiotemporal 'tubes' for the active elements ('agents') in the (local) scene by detecting individual objects, tracking them and then extracting 3D features from all the agent tubes as well as the overall scene. ii) Next, we construct a local scene graph where each node (representing either an agent tube or the scene) is connected to all other nodes. Attention is then applied to this graph to obtain an overall representation of the local dynamic scene. iii) Finally, all local scene graph representations are interconnected via a temporal graph, to estimate the complex activity class together with its start and end time. The proposed framework outperforms all previous state-of-the-art methods on all three datasets including ActivityNet-1.3, Thumos-14, and ROAD",
    "checked": true,
    "id": "a28b64e8c3bba33a9caaff21a56480e2a3f9a1c7",
    "semantic_title": "a hybrid graph network for complex activity detection in video",
    "citation_count": 0,
    "authors": [
      "Salman Khan",
      "Izzeddin Teeti",
      "Andrew Bradley",
      "Mohamed Elhoseiny",
      "Fabio Cuzzolin"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Movie_Genre_Classification_by_Language_Augmentation_and_Shot_Sampling_WACV_2024_paper.html": {
    "title": "Movie Genre Classification by Language Augmentation and Shot Sampling",
    "volume": "main",
    "abstract": "Video-based movie genre classification has garnered considerable attention due to its various applications in recommendation systems. Prior work has typically addressed this task by adapting models from traditional video classification tasks, such as action recognition or event detection. However, these models often neglect language elements (e.g., narrations or conversations) present in videos, which can implicitly convey high-level semantics of movie genres, like storylines or background context. Additionally, existing approaches are primarily designed to encode the entire content of the input video, leading to inefficiencies in predicting movie genres. Movie genre prediction may require only a few shots to accurately determine the genres, rendering a comprehensive understanding of the entire video unnecessary. To address these challenges, we propose a Movie genre Classification method based on Language augmentatIon and shot samPling (Movie-CLIP). Movie-CLIP mainly consists of two parts: a language augmentation module to recognize language elements from the input audio, and a shot sampling module to select representative shots from the entire video. We evaluate our method on MovieNet and Condensed Movies datasets, achieving approximate 6-9% improvement in mean Average Precision (mAP) over the baselines. We also generalize Movie-CLIP to the scene boundary detection task, achieving 1.1% improvement in Average Precision (AP) over the state-of-the-art. We release our implementation at github.com/Zhongping-Zhang/Movie-CLIP",
    "checked": true,
    "id": "0ff06122ea3359871652a1486fddf785b061b765",
    "semantic_title": "movie genre classification by language augmentation and shot sampling",
    "citation_count": 0,
    "authors": [
      "Zhongping Zhang",
      "Yiwen Gu",
      "Bryan A. Plummer",
      "Xin Miao",
      "Jiayi Liu",
      "Huayan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/DAmicantonio_Automated_Camera_Calibration_via_Homography_Estimation_With_GNNs_WACV_2024_paper.html": {
    "title": "Automated Camera Calibration via Homography Estimation With GNNs",
    "volume": "main",
    "abstract": "Over the past few decades, a significant rise of camera-based applications for traffic monitoring has occurred. Governments and local administrations are increasingly relying on the data collected from these cameras to enhance road safety and optimize traffic conditions. However, for effective data utilization, it is imperative to ensure accurate and automated calibration of the involved cameras. This paper proposes a novel approach to address this challenge by leveraging the topological structure of intersections. We propose a framework involving the generation of a set of synthetic intersection viewpoint images from a bird's-eye-view image, framed as a graph of virtual cameras to model these images. Using the capabilities of Graph Neural Networks, we effectively learn the relationships within this graph, thereby facilitating the estimation of a homography matrix. This estimation leverages the neighbourhood representation for any real-world camera and is enhanced by exploiting multiple images instead of a single match. In turn, the homography matrix allows the retrieval of extrinsic calibration parameters. As a result, the proposed framework demonstrates superior performance on both synthetic datasets and real-world cameras, setting a new state-of-the-art benchmark",
    "checked": true,
    "id": "24b0a75621f792f46eedcbe0e4a2ebf091c85366",
    "semantic_title": "automated camera calibration via homography estimation with gnns",
    "citation_count": 0,
    "authors": [
      "Giacomo D'Amicantonio",
      "Egor Bondarev",
      "Peter H.N. de With"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kim_Randomized_Adversarial_Style_Perturbations_for_Domain_Generalization_WACV_2024_paper.html": {
    "title": "Randomized Adversarial Style Perturbations for Domain Generalization",
    "volume": "main",
    "abstract": "We propose a novel domain generalization technique, referred to as Randomized Adversarial Style Perturbation (RASP), which is motivated by the observation that the characteristics of each domain are captured by the feature statistics corresponding to its style. The proposed algorithm perturbs the style of a feature in an adversarial direction towards a randomly selected class, and prevents the model from being misled by the unexpected styles observed in unseen target domains. While RASP is effective for handling domain shifts, its naive integration into the training procedure is prone to degrade the capability of learning knowledge from source domains due to the feature distortions caused by style perturbation. This challenge is alleviated by Normalized Feature Mixup (NFM) during training, which facilitates learning the original features while achieving robustness to perturbed representations. We evaluate the proposed algorithm via extensive experiments on various benchmarks and show that our approach improves domain generalization performance, especially in large-scale benchmarks",
    "checked": true,
    "id": "3892cba7b2d00a235e4ee168d90b9d55894c1d0f",
    "semantic_title": "randomized adversarial style perturbations for domain generalization",
    "citation_count": 1,
    "authors": [
      "Taehoon Kim",
      "Bohyung Han"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Theisen_C-CLIP_Contrastive_Image-Text_Encoders_To_Close_the_Descriptive-Commentative_Gap_WACV_2024_paper.html": {
    "title": "C-CLIP: Contrastive Image-Text Encoders To Close the Descriptive-Commentative Gap",
    "volume": "main",
    "abstract": "The interplay between the image and comment on a social media post is one of high importance for understanding its overall message. Recent strides in multimodal embedding models, namely CLIP, have provided an avenue forward in relating image and text. However the current training regime for CLIP models is insufficient for matching content found on social media, regardless of site or language. Current CLIP training data is based on what we call \"descriptive\" text: text in which an image is merely described. This is something rarely seen on social media, where the vast majority of text content is \"commentative\" in nature. The captions provide commentary and broader context related to the image, rather than describing what is in it. Current CLIP models perform poorly on retrieval tasks where image-caption pairs display a commentative relationship. Closing this gap would be beneficial for several important application areas related to social media. For instance, it would allow groups focused on Open-Source Intelligence Operations (OSINT) to further aid efforts during disaster events, such as the ongoing Russian invasion of Ukraine, by easily exposing data to non-technical users for discovery and analysis. In order to close this gap we demonstrate that training contrastive image-text encoders on explicitly commentative pairs results in large improvements in retrieval results, with the results extending across a variety of non-English languages",
    "checked": true,
    "id": "3deea3cfd5ca9ff1decd74dd62523a1e5121088f",
    "semantic_title": "c-clip: contrastive image-text encoders to close the descriptive-commentative gap",
    "citation_count": 0,
    "authors": [
      "William Theisen",
      "Walter J. Scheirer"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Hardy_LInKs_Lifting_Independent_Keypoints_-_Partial_Pose_Lifting_for_Occlusion_WACV_2024_paper.html": {
    "title": "LInKs \"Lifting Independent Keypoints\" - Partial Pose Lifting for Occlusion Handling With Improved Accuracy in 2D-3D Human Pose Estimation",
    "volume": "main",
    "abstract": "We present LInKs, a novel unsupervised learning method to recover 3D human poses from 2D kinematic skeletons obtained from a single image, even when occlusions are present. Our approach follows a unique two-step process, which involves first lifting the occluded 2D pose to the 3D domain, followed by filling in the occluded parts using the partially reconstructed 3D coordinates. This lift-then-fill approach leads to significantly more accurate results compared to models that complete the pose in 2D space alone. Additionally, we improve the stability and likelihood estimation of normalising flows through a custom sampling function replacing PCA dimensionality reduction used in prior work. Furthermore, we are the first to investigate if different parts of the 2D kinematic skeleton can be lifted independently which we find by itself reduces the error of current lifting approaches. We attribute this to the reduction of long-range keypoint correlations. In our detailed evaluation, we quantify the error under various realistic occlusion scenarios, showcasing the versatility and applicability of our model. Our results consistently demonstrate the superiority of handling all types of occlusions in 3D space when compared to others that complete the pose in 2D space. Our approach also exhibits consistent accuracy in scenarios without occlusion, as evidenced by a 7.9% reduction in reconstruction error compared to prior works on the Human3.6M dataset. Furthermore, our method excels in accurately retrieving complete 3D poses even in the presence of occlusions, making it highly applicable in situations where complete 2D pose information is unavailable",
    "checked": true,
    "id": "dbbfa4d4f92d297daf823e07f7f59d212a9402d8",
    "semantic_title": "links \"lifting independent keypoints\" - partial pose lifting for occlusion handling with improved accuracy in 2d-3d human pose estimation",
    "citation_count": 0,
    "authors": [
      "Peter Hardy",
      "Hansung Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Popordanoska_Beyond_Classification_Definition_and_Density-Based_Estimation_of_Calibration_in_Object_WACV_2024_paper.html": {
    "title": "Beyond Classification: Definition and Density-Based Estimation of Calibration in Object Detection",
    "volume": "main",
    "abstract": "Despite their impressive predictive performance in various computer vision tasks, deep neural networks (DNNs) tend to make overly confident predictions, which hinders their widespread use in safety-critical applications. While there have been recent attempts to calibrate DNNs, most of these efforts have primarily been focused on classification tasks, thus neglecting DNN-based object detectors. Although several recent works addressed calibration for object detection and proposed differentiable penalties, none of them are consistent estimators of established concepts in calibration. In this work, we tackle the challenge of defining and estimating calibration error specifically for this task. In particular, we adapt the definition of classification calibration error to handle the nuances associated with object detection, and predictions in structured output spaces more generally. Furthermore, we propose a consistent and differentiable estimator of the detection calibration error, utilizing kernel density estimation. Our experiments demonstrate the effectiveness of our estimator against competing train-time and post-hoc calibration methods, while maintaining similar detection performance",
    "checked": true,
    "id": "ad697aefab3d5ebc932c8143ce501188f12e4923",
    "semantic_title": "beyond classification: definition and density-based estimation of calibration in object detection",
    "citation_count": 0,
    "authors": [
      "Teodora Popordanoska",
      "Aleksei Tiulpin",
      "Matthew B. Blaschko"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Tay_PrivObfNet_A_Weakly_Supervised_Semantic_Segmentation_Model_for_Data_Protection_WACV_2024_paper.html": {
    "title": "PrivObfNet: A Weakly Supervised Semantic Segmentation Model for Data Protection",
    "volume": "main",
    "abstract": "The use of social media has made it easy to communicate and share information over the internet. However, it also brings issues such as data privacy leakage, which can be exploited by recipients with malicious intentions to harm the sender. In this paper, we propose a deep neural network that analyzes the user's image for privacy sensitive content and automatically locates sensitive regions for obfuscation. Our approach relies solely on image level annotations and learns to (a) predict an overall privacy score, (b) detect sensitive attributes and (c) demarcate the sensitive regions for obfuscation, in a given input image. We validated the performance of our proposed method on three large datasets, VISPR, PASCAL VOC 2012 and MS COCO 2014, in terms of privacy score, attribute prediction and obfuscation performance. On the VISPR dataset, we achieved a Pearson correlation of 0.88 and a Spearman correlation of 0.86, outperforming previous methods. On PASCAL VOC 2012 and MS COCO 2014, our model achieved a mean IOU of 71.5% and 43.9% respectively, and is among the state-of-the-art techniques using weakly supervised semantic segmentation learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ChiatPin Tay",
      "Vigneshwaran Subbaraju",
      "Thivya Kandappu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Vuong_Toward_Planet-Wide_Traffic_Camera_Calibration_WACV_2024_paper.html": {
    "title": "Toward Planet-Wide Traffic Camera Calibration",
    "volume": "main",
    "abstract": "Despite the widespread deployment of outdoor cameras, their potential for automated analysis remains largely untapped due, in part, to calibration challenges. The absence of precise camera calibration data, including intrinsic and extrinsic parameters, hinders accurate real-world distance measurements from captured videos. To address this, we present a scalable framework that utilizes street-level imagery to reconstruct a metric 3D model, facilitating precise calibration of in-the-wild traffic cameras. Notably, our framework achieves 3D scene reconstruction and accurate localization of over 100 global traffic cameras and is scalable to any camera with sufficient street-level imagery. For evaluation, we introduce a dataset of 20 fully calibrated traffic cameras, demonstrating our method's significant enhancements over existing automatic calibration techniques. Furthermore, we highlight our approach's utility in traffic analysis by extracting insights via 3D vehicle reconstruction and speed measurement, thereby opening up the potential of using outdoor cameras for automated analysis",
    "checked": true,
    "id": "53e9acc6d551fed3ff6f423ec88da0da1a9d39db",
    "semantic_title": "toward planet-wide traffic camera calibration",
    "citation_count": 0,
    "authors": [
      "Khiem Vuong",
      "Robert Tamburo",
      "Srinivasa G. Narasimhan"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wang_3D_Human_Pose_Estimation_With_Two-Step_Mixed-Training_Strategy_WACV_2024_paper.html": {
    "title": "3D Human Pose Estimation With Two-Step Mixed-Training Strategy",
    "volume": "main",
    "abstract": "In monocular 3D human pose estimation, target motions are generally stable and continuous, which indicates that joint velocity can provide valuable information for better estimation. Therefore, it is critical to learn the joint motion trajectory and spatio-temporal information from velocity. Previous works have shown that Transformers are effective in capturing the relationship between tokens. However, in practice, only 2D position is available and 3D velocity has not been explicitly used as a model input. To address this challenge, we propose TMT (Two-step Mixed-Training strategy), a transformer-based approach that effectively incorporates 3D velocity into the input vector during training, allowing for better learning of relevant features in the shallow layers. Extensive experiments demonstrate that TMT significantly improves the performance of state-of-the-art models, such as MixSTE, MHFormer, and PoseFomer, on two datasets: Human3.6M and MPI-INF-3DHP. TMT out performs the state-of-the-art approach by up to 13.8% on the Human3.6M dataset",
    "checked": false,
    "id": "80d61d2a099c14deb0268b643374753dc1a13177",
    "semantic_title": "upcoming oberseminars",
    "citation_count": 0,
    "authors": [
      "Yingfeng Wang",
      "Zhengwei Wang",
      "Muyu Li",
      "Hong Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Chandran_Learning-Based_Spotlight_Position_Optimization_for_Non-Line-of-Sight_Human_Localization_and_Posture_WACV_2024_paper.html": {
    "title": "Learning-Based Spotlight Position Optimization for Non-Line-of-Sight Human Localization and Posture Classification",
    "volume": "main",
    "abstract": "Non-line-of-sight imaging (NLOS) is the process of estimating information about a scene that is hidden from the direct line of sight of the camera. NLOS imaging typically requires time-resolved detectors and a laser source for illumination, which are both expensive and computationally intensive to handle. In this paper, we propose an NLOS-based localization and posture classification technique that works on a system of an off-the-shelf projector and camera. We leverage a message-passing neural network to learn a scene geometry and predict the best position to be spotlighted by the projector that can maximize the NLOS signal. The training of the neural network is performed in an end-to-end manner. Therefore, the ground truth spotlighted position is unnecessary during the training, and the network parameters are optimized to maximize the NLOS performance. Unlike prior deep-learning-based NLOS techniques that assume planar relay walls, our system allows us to handle line-of-sight scenes where scene geometries are more arbitrary. Our method demonstrates state-of-the-art performance in object localization and position classification using both synthetic and real scenes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sreenithy Chandran",
      "Tatsuya Yatagawa",
      "Hiroyuki Kubo",
      "Suren Jayasuriya"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Niemeijer_Generalization_by_Adaptation_Diffusion-Based_Domain_Extension_for_Domain-Generalized_Semantic_Segmentation_WACV_2024_paper.html": {
    "title": "Generalization by Adaptation: Diffusion-Based Domain Extension for Domain-Generalized Semantic Segmentation",
    "volume": "main",
    "abstract": "When models, e.g., for semantic segmentation, are applied to images that are vastly different from training data, the performance will drop significantly. Domain adaptation methods try to overcome this issue, but need samples from the target domain. However, this might not always be feasible for various reasons and therefore domain generalization methods are useful as they do not require any target data. We present a new diffusion-based domain extension (DIDEX) method and employ a diffusion model to generate a pseudo-target domain with diverse text prompts. In contrast to existing methods, this allows to control the style and content of the generated images and to introduce a high diversity. In a second step, we train a generalizing model by adapting towards this pseudo-target domain. We outperform previous approaches by a large margin across various datasets and architectures without using any real data. For the generalization from GTA5, we improve state-of-the-art mIoU performance by 3.8% absolute on average and for SYNTHIA by 11.8% absolute, marking a big step for the generalization performance on these benchmarks. Code is available at https://github.com/JNiemeijer/DIDEX",
    "checked": true,
    "id": "aa242811d5fb3446f00f861eb5dfca9bdd05def3",
    "semantic_title": "generalization by adaptation: diffusion-based domain extension for domain-generalized semantic segmentation",
    "citation_count": 0,
    "authors": [
      "Joshua Niemeijer",
      "Manuel Schwonberg",
      "Jan-Aike Termöhlen",
      "Nico M. Schmidt",
      "Tim Fingscheidt"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Baghbaderani_Temporally-Consistent_Video_Semantic_Segmentation_With_Bidirectional_Occlusion-Guided_Feature_Propagation_WACV_2024_paper.html": {
    "title": "Temporally-Consistent Video Semantic Segmentation With Bidirectional Occlusion-Guided Feature Propagation",
    "volume": "main",
    "abstract": "Despite recent progress in static image segmentation, video segmentation is still challenging due to the need for an accurate, fast, and temporally consistent model. Conducting per-frame static image segmentation is not acceptable since it is computationally prohibitive and prone to temporal inconsistency. In this paper, we present bidirectional occlusion-guided feature propagation (BOFP) method with the goal of improving temporal consistency of segmentation results without sacrificing segmentation accuracy, while at the same time keeping the operations at a low computation cost. It leverages temporal coherence in the video by feature propagation from keyframes to other frames along the motion paths in both forward and backward directions. We propose an occlusion-based attention network to estimate the distorted areas based on bidirectional optical flows, and utilize them as cues for correcting and fusing the propagated features. Extensive experiments on benchmark datasets demonstrate that the proposed BOFP method achieves superior performance in terms of temporal consistency while maintaining comparable level of segmentation accuracy at a low computation cost, striking a great balance among the three metrics essential to evaluate video segmentation solutions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Razieh Kaviani Baghbaderani",
      "Yuanxin Li",
      "Shuangquan Wang",
      "Hairong Qi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kim_MICS_Midpoint_Interpolation_To_Learn_Compact_and_Separated_Representations_for_WACV_2024_paper.html": {
    "title": "MICS: Midpoint Interpolation To Learn Compact and Separated Representations for Few-Shot Class-Incremental Learning",
    "volume": "main",
    "abstract": "Few-shot class-incremental learning (FSCIL) aims to learn a classification model for continually accepting novel classes with a few samples. The key of FSCIL is the joint success of the following two training stages: Base training stage to classify base classes and Incremental training stage with sequential learning of novel classes. However, recent efforts show a tendency to focus on one of the stages, or separately design strategies for each stage, so that less effort has been paid to devise a consistent strategy across the consecutive stages. In this paper, we first emphasize the particular aspects of the successful FSCIL algorithm that are worthwhile to consistently pursue during both stages, i.e., intra-class compactness and inter-class separability of the representation, which allows a model to reserve feature space in between current classes for preparing the acceptance of novel classes in the future. To achieve these aspects, we propose a mixup-based FSCIL method called MICS, which theoretically guarantees to enlarge the thickness of the margin space between different classes, leading to outstanding performance on the existing benchmarks. Code is available at https://github.com/solangii/MICS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Solang Kim",
      "Yuho Jeong",
      "Joon Sung Park",
      "Sung Whan Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Abou-Chakra_ParticleNeRF_A_Particle-Based_Encoding_for_Online_Neural_Radiance_Fields_WACV_2024_paper.html": {
    "title": "ParticleNeRF: A Particle-Based Encoding for Online Neural Radiance Fields",
    "volume": "main",
    "abstract": "While existing Neural Radiance Fields (NeRFs) for dynamic scenes are offline methods with an emphasis on visual fidelity, our paper addresses the online use case that prioritises real-time adaptability. We present ParticleNeRF, a new approach that dynamically adapts to changes in the scene geometry by learning an up-to-date representation online, every 200ms. ParticleNeRF achieves this using a novel particle-based parametric encoding. We couple features to particles in space and backpropagate the photometric reconstruction loss into the particles' position gradients, which are then interpreted as velocity vectors. Governed by a lightweight physics system to handle collisions, this lets the features move freely with the changing scene geometry. We demonstrate ParticleNeRF on various dynamic scenes containing translating, rotating, articulated, and deformable objects. ParticleNeRF is the first online dynamic NeRF and achieves fast adaptability with better visual fidelity than brute-force online InstantNGP and other baseline approaches on dynamic scenes with online constraints",
    "checked": false,
    "id": "89835c5adbca97df6ef4216f55136431174de0ce",
    "semantic_title": "particlenerf: a particle-based encoding for online neural radiance fields in dynamic scenes",
    "citation_count": 10,
    "authors": [
      "Jad Abou-Chakra",
      "Feras Dayoub",
      "Niko Sünderhauf"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Chen_Residual_Graph_Convolutional_Network_for_Birds-Eye-View_Semantic_Segmentation_WACV_2024_paper.html": {
    "title": "Residual Graph Convolutional Network for Bird's-Eye-View Semantic Segmentation",
    "volume": "main",
    "abstract": "Retrieving spatial information and understanding the semantic information of the surroundings are important for Bird's-Eye-View (BEV) semantic segmentation. In the application of autonomous driving, autonomous vehicles need to be aware of their surroundings to drive safely. However, current BEV semantic segmentation techniques, deep Convolutional Neural Networks (CNNs) and transformers, have difficulties in efficiently obtaining the global semantic relationships of the surroundings. In this paper, we propose to incorporate a novel Residual Graph Convolutional (RGC) module in deep CNNs to acquire both the global information and the region-level semantic relationship in the multi-view image domain. Specifically, the RGC module employs a non-overlapping graph space projection to efficiently project the complete BEV information into graph space. It then builds interconnected spatial and channel graphs to extract spatial information between each node and channel information within each node (i.e., extract contextual relationships of the global features). Furthermore, it uses a downsample residual process to enhance the coordinate feature reuse to maintain the global information. The segmentation data augmentation and alignment module helps to simultaneously augment and align BEV features and ground truth to geometrically preserve their alignment to achieve better segmentation results. Our experimental results on the nuScenes benchmark dataset demonstrate that the RGC network outperforms four state-of-the-art networks and its four variants in terms of IoU and mIoU. The proposed RGC network achieves a higher mIoU of 3.1% than the best state-of-the-art network, BEVFusion. Code and models will be released",
    "checked": true,
    "id": "ea92cb1b3452575975430149a867efad6138e3a0",
    "semantic_title": "residual graph convolutional network for bird's-eye-view semantic segmentation",
    "citation_count": 0,
    "authors": [
      "Qiuxiao Chen",
      "Xiaojun Qi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Yap_Group-Wise_Contrastive_Bottleneck_for_Weakly-Supervised_Visual_Representation_Learning_WACV_2024_paper.html": {
    "title": "Group-Wise Contrastive Bottleneck for Weakly-Supervised Visual Representation Learning",
    "volume": "main",
    "abstract": "Coarse or weak labels can serve as a cost-effective solution to the problem of visual representation learning. When fine-grained labels are unavailable, weak labels can provide some form of supervisory signals to guide the representation learning process. Some examples of weak labels include image captions, visual attributes and coarse-grained object categories. In this work, we consider the semantic grouping relationship that exists within certain types of weak labels and propose a group-wise contrastive bottleneck module to leverage this relationship. The semantic group may contain labels that are related to a general concept, such as the colour or shape of objects. Using the group-wise bottleneck module, we disentangle the global image features into multiple group features and apply contrastive learning in a group-wise manner to maximize the similarity of positive pairs within each semantic group. The positive pairs are defined based on the similarity of the labels captured by each group. To learn a more robust representation, we introduce a reconstruction objective where an image feature is reconstructed back from the disentangled features, and this reconstruction is encouraged to be consistent with the feature obtained from a different augmented view of the same image. We empirically verify the efficacy of the proposed method on several datasets in the context of visual attribute learning, fair representation learning and hierarchical label learning. The experimental results indicate that our proposed method outperforms prior weakly-supervised methods and is flexible in adapting to different representation learning settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boon Peng Yap",
      "Beng Koon Ng"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kerim_Leveraging_Synthetic_Data_To_Learn_Video_Stabilization_Under_Adverse_Conditions_WACV_2024_paper.html": {
    "title": "Leveraging Synthetic Data To Learn Video Stabilization Under Adverse Conditions",
    "volume": "main",
    "abstract": "Stabilization plays a central role in improving the quality of videos. However, current methods perform poorly under adverse conditions. In this paper, we propose a synthetic-aware adverse weather video stabilization algorithm that dispenses real data for training, relying solely on synthetic data. Our approach leverages specially generated synthetic data to avoid the feature extraction issues faced by current methods. To achieve this, we present a novel data generator to produce the required training data with an automatic ground-truth extraction procedure. We also propose a new dataset, VSAC105Real, and compare our method to five recent video stabilization algorithms using two benchmarks. Our method generalizes well on real-world videos across all weather conditions and does not require large-scale synthetic training data. Implementations for our proposed video stabilization algorithm, generator, and datasets are available at https://github.com/A-Kerim/SyntheticData4VideoStabilization_WACV_2024",
    "checked": true,
    "id": "71c952915aeb5b63dd978c144933803700175e53",
    "semantic_title": "leveraging synthetic data to learn video stabilization under adverse conditions",
    "citation_count": 0,
    "authors": [
      "Abdulrahman Kerim",
      "Washington L. S. Ramos",
      "Leandro Soriano Marcolino",
      "Erickson R. Nascimento",
      "Richard Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Liu_Generation_of_Upright_Panoramic_Image_From_Non-Upright_Panoramic_Image_WACV_2024_paper.html": {
    "title": "Generation of Upright Panoramic Image From Non-Upright Panoramic Image",
    "volume": "main",
    "abstract": "The inclination of a spherical camera results in nonupright panoramic images. To carry out upright adjustment, traditional methods estimate camera inclination angles firstly, and then resample the image in terms of the estimated rotation to generate upright image. Since sampling an image is a time-consuming processing, a lookup table is usually used to achieve a high processing speed; however, the content of a lookup table depends on the rotational angles and needs extra memory to store also. In this paper we propose a new approach for panorama upright adjustment, which directly generates an upright panoramic image from an input nonupright one without rotation estimation and lookup tables as an intermediate processing. The proposed approach formulates panorama upright adjustment as a pixelwise image-to-image mapping problem, and the mapping is directly generated from an input nonupright panoramic image via an end-to-end neural network. As shown in the experiment of this paper, the proposed method results in a lightweight network, as less as 163MB, with high processing speed, as great as 9ms, for a 256x512 pixel panoramic image",
    "checked": false,
    "id": "c51d603302d965855ca390181dbb7495017b2ca7",
    "semantic_title": "an end-to-end network for upright adjustment of panoramic images",
    "citation_count": 0,
    "authors": [
      "Jingguo Liu",
      "Heyu Chen",
      "Shigang Li",
      "Jianfeng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Lee_RADIO_Reference-Agnostic_Dubbing_Video_Synthesis_WACV_2024_paper.html": {
    "title": "RADIO: Reference-Agnostic Dubbing Video Synthesis",
    "volume": "main",
    "abstract": "One of the most challenging problems in audio-driven talking head generation is achieving high-fidelity detail while ensuring precise synchronization. Given only a single reference image, extracting meaningful identity attributes becomes even more challenging, often causing the network to mirror the facial and lip structures too closely. To address these issues, we introduce RADIO, a framework engineered to yield high-quality dubbed videos regardless of the pose or expression in reference images. The key is to modulate the decoder layers using latent space composed of audio and reference features. Additionally, we incorporate ViT blocks into the decoder to emphasize high-fidelity details, especially in the lip region. Our experimental results demonstrate that RADIO displays high synchronization without the loss of fidelity. Especially in harsh scenarios where the reference frame deviates significantly from the ground truth, our method outperforms state-of-the-art methods, highlighting its robustness",
    "checked": true,
    "id": "5bb94c5faf2e0802caabaafce45c4cfd33c1e7c1",
    "semantic_title": "radio: reference-agnostic dubbing video synthesis",
    "citation_count": 0,
    "authors": [
      "Dongyeun Lee",
      "Chaewon Kim",
      "Sangjoon Yu",
      "Jaejun Yoo",
      "Gyeong-Moon Park"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Al-lahham_A_Coarse-To-Fine_Pseudo-Labeling_C2FPL_Framework_for_Unsupervised_Video_Anomaly_Detection_WACV_2024_paper.html": {
    "title": "A Coarse-To-Fine Pseudo-Labeling (C2FPL) Framework for Unsupervised Video Anomaly Detection",
    "volume": "main",
    "abstract": "Detection of anomalous events in videos is an important problem in applications such as surveillance. Video anomaly detection (VAD) is well-studied in the one-class classification (OCC) and weakly supervised (WS) settings. However, fully unsupervised (US) video anomaly detection methods, which learn a complete system without any annotation or human supervision, have not been explored in depth. This is because the lack of any ground truth annotations significantly increases the magnitude of the VAD challenge. To address this challenge, we propose a simple-but-effective two-stage pseudo-label generation framework that produces segment-level (normal/anomaly) pseudo-labels, which can be further used to train a segment-level anomaly detector in a supervised manner. The proposed coarse-to-fine pseudo-label (C2FPL) generator employs carefully-designed hierarchical divisive clustering and statistical hypothesis testing to identify anomalous video segments from a set of completely unlabeled videos. The trained anomaly detector can be directly applied on segments of an unseen test video to obtain segment-level, and subsequently, frame-level anomaly predictions. Extensive studies on two large-scale public-domain datasets, UCF-Crime and XD-Violence, demonstrate that the proposed unsupervised approach achieves superior performance compared to all existing OCC and US methods, while yielding comparable performance to the state-of-the-art WS methods",
    "checked": true,
    "id": "a35b0178653ef2477f7b4cfa614f9daa0d66a79f",
    "semantic_title": "a coarse-to-fine pseudo-labeling (c2fpl) framework for unsupervised video anomaly detection",
    "citation_count": 0,
    "authors": [
      "Anas Al-lahham",
      "Nurbek Tastan",
      "Muhammad Zaigham Zaheer",
      "Karthik Nandakumar"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Valois_Occlusion_Sensitivity_Analysis_With_Augmentation_Subspace_Perturbation_in_Deep_Feature_WACV_2024_paper.html": {
    "title": "Occlusion Sensitivity Analysis With Augmentation Subspace Perturbation in Deep Feature Space",
    "volume": "main",
    "abstract": "Deep Learning of neural networks has gained prominence in multiple life-critical applications like medical diagnoses and autonomous vehicle accident investigations. However, concerns about model transparency and biases persist. Explainable methods are viewed as the solution to address these challenges. In this study, we introduce the Occlusion Sensitivity Analysis with Deep Feature Augmentation Subspace (OSA-DAS), a novel perturbation-based interpretability approach for computer vision. While traditional perturbation methods make only use of occlusions to explain the model predictions, OSA-DAS extends standard occlusion sensitivity analysis by enabling the integration with diverse image augmentations. Distinctly, our method utilizes the output vector of a DNN to build low-dimensional subspaces within the deep feature vector space, offering a more precise explanation of the model prediction. The structural similarity between these subspaces encompasses the influence of diverse augmentations and occlusions. We test extensively on the ImageNet-1k, and our class- and model-agnostic approach outperforms commonly used interpreters, setting it apart in the realm of explainable AI",
    "checked": true,
    "id": "97d628444a00fbb9c7e1435cf0765392142326f2",
    "semantic_title": "occlusion sensitivity analysis with augmentation subspace perturbation in deep feature space",
    "citation_count": 0,
    "authors": [
      "Pedro H. V. Valois",
      "Koichiro Niinuma",
      "Kazuhiro Fukui"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Chandrasekar_PhISH-Net_Physics_Inspired_System_for_High_Resolution_Underwater_Image_Enhancement_WACV_2024_paper.html": {
    "title": "PhISH-Net: Physics Inspired System for High Resolution Underwater Image Enhancement",
    "volume": "main",
    "abstract": "Underwater imaging presents numerous challenges due to refraction, light absorption, and scattering, resulting in color degradation, low contrast, and blurriness. Enhancing underwater images is crucial for high-level computer vision tasks, but existing methods either neglect the physics-based image formation process or require expensive computations. In this paper, we propose an effective framework that combines a physics-based Underwater Image Formation Model (UIFM) with a deep image enhancement approach based on the retinex model. Firstly, we remove backscatter by estimating attenuation coefficients using depth information. Then, we employ a retinex model-based deep image enhancement module to enhance the images. To ensure adherence to the UIFM, we introduce a novel Wideband Attenuation prior. The proposed PhISH-Net framework achieves real-time processing of high-resolution underwater images using a lightweight neural network and a bilateral-grid-based upsampler. Extensive experiments on two underwater image datasets demonstrate the superior performance of our method compared to state-of-the-art techniques. Additionally, qualitative evaluation on a cross-dataset scenario confirms its generalization capability. Our contributions lie in combining the physics-based UIFM with deep image enhancement methods, introducing the wideband attenuation prior, and achieving superior performance and efficiency",
    "checked": false,
    "id": "3232535521e60b91cd8de042756730d438051db4",
    "semantic_title": "supplementary for phish-net: physics inspired system for high resolution underwater image enhancement",
    "citation_count": 0,
    "authors": [
      "Aditya Chandrasekar",
      "Manogna Sreenivas",
      "Soma Biswas"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Pham_MixtureGrowth_Growing_Neural_Networks_by_Recombining_Learned_Parameters_WACV_2024_paper.html": {
    "title": "MixtureGrowth: Growing Neural Networks by Recombining Learned Parameters",
    "volume": "main",
    "abstract": "Most deep neural networks are trained under fixed network architectures and require retraining when the architecture changes. If expanding the network's size is needed, it is necessary to retrain from scratch, which is expensive. To avoid this, one can grow from a small network by adding random weights over time to gradually achieve the target network size. However, this naive approach falls short in practice as it brings too much noise to the growing process. Prior work tackled this issue by leveraging the already learned weights and training data for generating new weights through conducting a computationally expensive analysis step. In this paper, we introduce MixtureGrowth, a new approach to growing networks that circumvents the initialization overhead in prior work. Before growing, each layer in our model is generated with a linear combination of parameter templates. Newly grown layer weights are generated by using a new linear combination of existing templates for a layer. On one hand, these templates are already trained for the task, providing a strong initialization. On the other, the new coefficients provide flexibility for the added layer weights to learn something new. We show that our approach boosts top-1 accuracy over the state-of-the-art by 2-2.5% on CIFAR-100 and ImageNet datasets, while achieving comparable performance with fewer FLOPs to a larger network trained from scratch. Code is available at https://github.com/chaudatascience/mixturegrowth",
    "checked": true,
    "id": "fd203cd888185b2a0d4962b70b52b298f2875bf1",
    "semantic_title": "mixturegrowth: growing neural networks by recombining learned parameters",
    "citation_count": 0,
    "authors": [
      "Chau Pham",
      "Piotr Teterwak",
      "Soren Nelson",
      "Bryan A. Plummer"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Pan_Zero-Shot_Building_Attribute_Extraction_From_Large-Scale_Vision_and_Language_Models_WACV_2024_paper.html": {
    "title": "Zero-Shot Building Attribute Extraction From Large-Scale Vision and Language Models",
    "volume": "main",
    "abstract": "Modern building recognition methods, exemplified by the BRAILS framework, utilize supervised learning to extract information from satellite and street-view images for image classification and semantic segmentation tasks. However, each task module requires human-annotated data, hindering the scalability and robustness to regional variations and annotation imbalances. In response, we propose a new zero-shot workflow for building attribute extraction that utilizes large-scale vision and language models to mitigate reliance on external annotations. The proposed workflow contains two key components: image-level captioning and segment-level captioning for the building images based on the vocabularies pertinent to structural and civil engineering. These two components generate descriptive captions by computing feature representations of the image and the vocabularies, and facilitating a semantic match between the visual and textual representations. Consequently, our framework offers a promising avenue to enhance AI-driven captioning for building attribute extraction in the structural and civil engineering domains, ultimately reducing reliance on human annotations while bolstering performance and adaptability",
    "checked": true,
    "id": "10c6f1a2a1ae24595e7472a2092e652c329862ab",
    "semantic_title": "zero-shot building attribute extraction from large-scale vision and language models",
    "citation_count": 0,
    "authors": [
      "Fei Pan",
      "Sangryul Jeon",
      "Brian Wang",
      "Frank Mckenna",
      "Stella X. Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Koohpayegani_SimA_Simple_Softmax-Free_Attention_for_Vision_Transformers_WACV_2024_paper.html": {
    "title": "SimA: Simple Softmax-Free Attention for Vision Transformers",
    "volume": "main",
    "abstract": "Recently, vision transformers have become very popular. However, deploying them in many applications is computationally expensive partly due to the Softmax layer in the attention block. We introduce a simple yet effective, Softmax-free attention block, SimA, which normalizes query and key matrices with simple l1-norm instead of using Softmax layer. Then, the attention block in SimA is a simple multiplication of three matrices, so SimA can dynamically change the ordering of the computation at the test time to achieve linear computation on the number of tokens or the number of channels. We empirically show that SimA applied to three SOTA variations of transformers, DeiT, XCiT, and CvT, results in on-par accuracy compared to the SOTA models, without any need for Softmax layer. Interestingly, changing SimA from multi-head to single-head has only a small effect on the accuracy, which further simplifies the attention block. Moreover, we show that SimA is much faster on small edge devices, e.g., Raspberry Pi, which we believe is due to higher complexity of Softmax layer on those devices. The code is available here: https://github.com/UCDvision/sima",
    "checked": true,
    "id": "1966c4df2cda0fb8daf7f36366d909a021b6d5c1",
    "semantic_title": "sima: simple softmax-free attention for vision transformers",
    "citation_count": 6,
    "authors": [
      "Soroush Abbasi Koohpayegani",
      "Hamed Pirsiavash"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Sahu_POP-VQA_-_Privacy_Preserving_On-Device_Personalized_Visual_Question_Answering_WACV_2024_paper.html": {
    "title": "POP-VQA - Privacy Preserving, On-Device, Personalized Visual Question Answering",
    "volume": "main",
    "abstract": "The next generation of device smartness needs to go beyond being able to understand basic user commands. As our systems become more efficient, they need to be taught to understand user interactions and intents from all possible input modalities. This is where the recent advent of large scale multi-modal models can form the foundation for next-gen technologies. However, the true power of such interactive systems can only be realized with privacy conserving personalization. In this paper, we propose an on-device visual question answering system that generates personalized answers using on-device user knowledge graph. These systems have the potential to serve as a fundamental groundwork for the development of genuinely intelligent and tailored assistants, targeted specifically to the needs and preferences of each individual. We validate our model performance on both in-realm, public datasets and personal user data. Our results show consistent performance increase across both tasks, with an absolute improvement of 36% with KVQA data-set on 1-hop inferences and 6% improvement on user personal data. We also conduct and showcase user-study results to validate our hypothesis of the need and relevance of proposed system",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pragya Paramita Sahu",
      "Abhishek Raut",
      "Jagdish Singh Samant",
      "Mahesh Gorijala",
      "Vignesh Lakshminarayanan",
      "Pinaki Bhaskar"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Tejero-de-Pablos_Complementary-Contradictory_Feature_Regularization_Against_Multimodal_Overfitting_WACV_2024_paper.html": {
    "title": "Complementary-Contradictory Feature Regularization Against Multimodal Overfitting",
    "volume": "main",
    "abstract": "Understanding multimodal learning is essential to design intelligent systems that can effectively combine various data types (visual, audio, etc.). Multimodal learning is not trivial, as adding new modalities does not always result in a significant improvement in performance, i.e., multimodal overfitting. To tackle this, several works proposed regularizing each modality's learning speed and feature distribution. However, in these methods, characterizing quantitatively and qualitatively multimodal overfitting is not intuitive. We hypothesize that, rather than regularizing abstract hyperparameters, regularizing the features learned is a more straightforward methodology against multimodal overfitting. For the given input modalities and task, we constrain \"complementary\" (useful) and \"contradictory\" (obstacle) features via a masking operation on the multimodal latent space. In addition, we leverage latent discretization so the size of the complementary-contradictory spaces becomes learnable, allowing the estimation of a modal complementarity measure. Our method successfully improves the performance of datasets with modality overfitting in different tasks, providing insight into \"what\" and \"how much\" is learned from each modality. Furthermore, it facilitates transfer learning to new datasets. Our code and a detailed manual are available at https://github.com/CyberAgentAILab/CM-VQVAE",
    "checked": false,
    "id": "e7fc1fb294fa4ff294a68fbc19d7d67dd17c3278",
    "semantic_title": "multimodal neuroimage data fusion based on multikernel learning in personalized medicine",
    "citation_count": 0,
    "authors": [
      "Antonio Tejero-de-Pablos"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Tanaka_Appearance-Based_Curriculum_for_Semi-Supervised_Learning_With_Multi-Angle_Unlabeled_Data_WACV_2024_paper.html": {
    "title": "Appearance-Based Curriculum for Semi-Supervised Learning With Multi-Angle Unlabeled Data",
    "volume": "main",
    "abstract": "We propose an appearance-based curriculum (ABC) for a semi-supervised learning scenario where labeled images taken from limited angles and unlabeled ones taken from various angles are available for training. A common approach to semi-supervised learning relies on pseudo-labeling and data augmentation, but it struggles with large visual variations that cannot be covered by data augmentation. To solve this problem, ABC incrementally expands the pool of unlabeled images fed to a base semi-supervised learner so that newly added data are the ones most similar to those already in the pool. This way, the learner can assign pseudo-labels to the new data with high accuracy, keeping the quality of pseudo-labels higher than that when all the unlabeled data are processed at once, as customarily done in existing semi-supervised learning methods. We conducted extensive experiments and confirmed that our method outperforms the state-of-the-art semi-supervised learning methods in our scenario",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuki Tanaka",
      "Shuhei M. Yoshida",
      "Takashi Shibata",
      "Makoto Terao",
      "Takayuki Okatani",
      "Masashi Sugiyama"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Incorporating_Physics_Principles_for_Precise_Human_Motion_Prediction_WACV_2024_paper.html": {
    "title": "Incorporating Physics Principles for Precise Human Motion Prediction",
    "volume": "main",
    "abstract": "A variety of real-world applications rely on accurate predictions of 3D human motion from their past observations. While existing methods have made notable progress, their predictions over subsecond horizons can still be off by many centimeters. In this paper, we argue that achieving precise human motion prediction requires characterizing the fundamental physics principles governing body movements. We introduce PhysMoP, a novel framework that incorporates Physics for human Motion Prediction. PhysMoP estimates the body configuration of the next frame by solving the Euler-Lagrange equations, a set of Ordinary Different Equations describing the physical motion rules. To limit the inherent problem of error accumulation over time, PhysMoP leverages a data-driven model and iteratively guides the physics-based prediction via a fusion model. Through extensive experiments, we demonstrate that PhysMoP significantly outperforms existing approaches at subsecond prediction horizons. For example, at a prediction horizon of 80 msec, PhysMoP outperforms traditional data-driven approaches by a factor of 10 or more",
    "checked": false,
    "id": "02dddceeb9c3937d4ed25b9cc94ba9526dd2fedf",
    "semantic_title": "predicting multi-joint kinematics of the upper limb from emg signals across varied loads with a physics-informed neural network",
    "citation_count": 0,
    "authors": [
      "Yufei Zhang",
      "Jeffrey O. Kephart",
      "Qiang Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ren_MuSHRoom_Multi-Sensor_Hybrid_Room_Dataset_for_Joint_3D_Reconstruction_and_WACV_2024_paper.html": {
    "title": "MuSHRoom: Multi-Sensor Hybrid Room Dataset for Joint 3D Reconstruction and Novel View Synthesis",
    "volume": "main",
    "abstract": "Metaverse technologies demand accurate, real-time, and immersive modeling on consumer-grade hardware for both non-human perception (e.g., drone/robot/autonomous car navigation) and immersive technologies like AR/VR, requiring both structural accuracy and photorealism. However, there exists a knowledge gap in how to apply geometric reconstruction and photorealism modeling (novel view synthesis) in a unified framework. To address this gap and promote the development of robust and immersive modeling and rendering with consumer-grade devices, first, we propose a real-world Multi-Sensor Hybrid Room Dataset (MuSHRoom). Our dataset presents exciting challenges and requires state-of-the-art methods to be cost-effective, robust to noisy data and devices, and can jointly learn 3D reconstruction and novel view synthesis, instead of treating them as separate tasks, making them ideal for real-world applications. Second, we benchmark several famous pipelines on our dataset for joint 3D mesh reconstruction and novel view synthesis. Finally, in order to further improve the overall performance, we propose a new method that achieves a good trade-off between the two tasks. Our dataset and benchmark show great potential in promoting the improvements for fusing 3D reconstruction and high-quality rendering in a robust and computationally efficient end-to-end fashion. The dataset and code is available at the project webpate: https://xuqianren.github. io/publications/MuSHRoom/",
    "checked": true,
    "id": "c0d0970cd453f851a80bc86a7f4bb3cfc54819a4",
    "semantic_title": "mushroom: multi-sensor hybrid room dataset for joint 3d reconstruction and novel view synthesis",
    "citation_count": 0,
    "authors": [
      "Xuqian Ren",
      "Wenjia Wang",
      "Dingding Cai",
      "Tuuli Tuominen",
      "Juho Kannala",
      "Esa Rahtu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Dutta_POISE_Pose_Guided_Human_Silhouette_Extraction_Under_Occlusions_WACV_2024_paper.html": {
    "title": "POISE: Pose Guided Human Silhouette Extraction Under Occlusions",
    "volume": "main",
    "abstract": "Human silhouette extraction is a fundamental task in computer vision with applications in various downstream tasks. However, occlusions pose a significant challenge, leading to distorted silhouettes. To address this challenge, we introduce POISE : Pose Guided Human Silhouette Extraction under Occlusions, a fusion framework that enhances accuracy and robustness in human silhouette prediction. By combining initial silhouette estimates from a segmentation model with human joint predictions from a 2D pose estimation model, POISE leverages the complementary strengths of both approaches, effectively integrating precise body shape information and spatial information to tackle occlusions. Furthermore, the unsupervised nature of POISE eliminates the need for costly annotations, making it scalable and practical. Extensive experimental results demonstrate its superiority in improving silhouette extraction under occlusions, with promising results in downstream tasks such as gait recognition",
    "checked": true,
    "id": "565573c0979ecf864a8fb6ef541e4649bc3e44cf",
    "semantic_title": "poise: pose guided human silhouette extraction under occlusions",
    "citation_count": 0,
    "authors": [
      "Arindam Dutta",
      "Rohit Lal",
      "Dripta S. Raychaudhuri",
      "Calvin-Khang Ta",
      "Amit K. Roy-Chowdhury"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Park_Shape-Guided_Diffusion_With_Inside-Outside_Attention_WACV_2024_paper.html": {
    "title": "Shape-Guided Diffusion With Inside-Outside Attention",
    "volume": "main",
    "abstract": "We introduce precise object silhouette as a new form of user control in text-to-image diffusion models, which we dub Shape-Guided Diffusion. Our training-free method uses an Inside-Outside Attention mechanism during the inversion and generation process to apply a shape constraint to the cross- and self-attention maps. Our mechanism designates which spatial region is the object (inside) vs. background (outside) then associates edits to the correct region. We demonstrate the efficacy of our method on the shape-guided editing task, where the model must replace an object according to a text prompt and object mask. We curate a new ShapePrompts benchmark derived from MS-COCO and achieve SOTA results in shape faithfulness without a degradation in text alignment or image realism according to both automatic metrics and annotator ratings. Our data and code will be made available at https://shape-guided-diffusion.github.io",
    "checked": true,
    "id": "074c9b84e86b70c02cd1fa0d65204fe07ecdd849",
    "semantic_title": "shape-guided diffusion with inside-outside attention",
    "citation_count": 19,
    "authors": [
      "Dong Huk Park",
      "Grace Luo",
      "Clayton Toste",
      "Samaneh Azadi",
      "Xihui Liu",
      "Maka Karalashvili",
      "Anna Rohrbach",
      "Trevor Darrell"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Pang_Learning_Visual_Body-Shape-Aware_Embeddings_for_Fashion_Compatibility_WACV_2024_paper.html": {
    "title": "Learning Visual Body-Shape-Aware Embeddings for Fashion Compatibility",
    "volume": "main",
    "abstract": "Body shape is a crucial factor in outfit recommendation. Previous studies that directly used body measurement data to investigate the relationship between body shape and outfit have achieved limited performance due to oversimplified body shape representations. This paper proposes a Visual Body-shape-Aware Network (ViBA-Net) to improve the fashion compatibility model's awareness of human body shape through visual-level information. Specifically, ViBA-Net consists of three modules: a body-shape embedding module, which extracts visual and anthropometric features of body shape from a newly introduced large-scale body shape dataset; an outfit embedding module, which learns the outfit representation based on visual features extracted from a try-on image and textual features extracted from fashion attributes; and a joint embedding module, which jointly models the relationship between the representations of body shape and outfit. ViBA-Net is designed to generate attribute-level explanations for the evaluation results based on the computed attention weights. The effectiveness of ViBA-Net is evaluated on two mainstream datasets through qualitative and quantitative analysis. Data and code are released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaicheng Pang",
      "Xingxing Zou",
      "Waikeung Wong"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ye_Unsupervised_Exemplar-Based_Image-to-Image_Translation_and_Cascaded_Vision_Transformers_for_Tagged_WACV_2024_paper.html": {
    "title": "Unsupervised Exemplar-Based Image-to-Image Translation and Cascaded Vision Transformers for Tagged and Untagged Cardiac Cine MRI Registration",
    "volume": "main",
    "abstract": "Multi-modal registration between tagged and untagged cardiac cine magnetic resonance (MR) images remains difficult, due to the domain gap and large deformations between the two modalities. Recent work using an image-to-image translation (I2I) module to overcome the domain gap can convert the multi-modal into a mono-modal registration task and take advantage of advanced mono-modal registration architectures. However, they often ignore two issues: the sample-specific style of each image to be registered during I2I and large hybrid rigid and non-rigid deformations between modalities. We first propose an exemplar-based I2I module capable of unsupervised cross-domain correspondence learning to enforce the style consistency between the fake image and the image to be registered. Then we propose an efficient cascaded vision transformer-based registration network to predict both the affine and non-rigid deformations, in which a single feature embedding subnetwork is shared by the two stages of deformation prediction. We validated our method on a clinical cardiac MR dataset with paired but unaligned untagged and tagged MR images. The results show that our method outperforms traditional methods significantly in terms of the I2I quality and multi-modal image registration accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meng Ye",
      "Mikael Kanski",
      "Dong Yang",
      "Leon Axel",
      "Dimitris Metaxas"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Khan_Spectroformer_Multi-Domain_Query_Cascaded_Transformer_Network_for_Underwater_Image_Enhancement_WACV_2024_paper.html": {
    "title": "Spectroformer: Multi-Domain Query Cascaded Transformer Network for Underwater Image Enhancement",
    "volume": "main",
    "abstract": "Underwater images often suffer from color distortion, haze, and limited visibility due to light refraction and absorption in water. These challenges significantly impact autonomous underwater vehicle applications, necessitating efficient image enhancement techniques. To address these challenges, we propose a Multi-Domain Query Cascaded Transformer Network for underwater image enhancement. Our approach includes a novel Multi-Domain Query Cascaded Attention mechanism that integrates localized transmission features and global illumination features. To improve feature propagation from the encoder to the decoder, we propose a Spatio-Spectro Fusion-Based Attention Block. Additionally, we introduce a Hybrid Fourier-Spatial Upsampling Block, which uniquely combines Fourier and spatial upsampling techniques to enhance feature resolution effectively. We evaluate our method on benchmark synthetic and real-world underwater image datasets, demonstrating its superiority through extensive ablation studies and comparative analysis. The testing code is available at: https: //github.com/Mdraqibkhan/Spectroformer",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raqib Khan",
      "Priyanka Mishra",
      "Nancy Mehta",
      "Shruti S. Phutke",
      "Santosh Kumar Vipparthi",
      "Sukumar Nandi",
      "Subrahmanyam Murala"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Huang_Removing_the_Quality_Tax_in_Controllable_Face_Generation_WACV_2024_paper.html": {
    "title": "Removing the Quality Tax in Controllable Face Generation",
    "volume": "main",
    "abstract": "3DMM conditioned face generation has gained traction due to its well-defined controllability; however, the trade-off is lower sample quality: Previous works such as DiscoFaceGAN and 3D-FM GAN show a significant FID gap compared to the unconditional StyleGAN, suggesting that there is a quality tax to pay for controllability. In this paper, we challenge the assumption that quality and controllability cannot coexist. To pinpoint the previous issues, we mathematically formalize the problem of 3DMM conditioned face generation. Then, we devise simple solutions to the problem under our proposed framework. This results in a new model that effectively removes the quality tax between 3DMM conditioned face GANs and the unconditional StyleGAN. Project webpage: https://visual.cs.brown.edu/taxfreegan",
    "checked": false,
    "id": "48c06cb4c47cf5ad4c0af3c87fe77e1b5c2af213",
    "semantic_title": "tax-free' 3dmm conditional face generation",
    "citation_count": 0,
    "authors": [
      "Yiwen Huang",
      "Zhiqiu Yu",
      "Xinjie Yi",
      "Yue Wang",
      "James Tompkin"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Santoso_On_Manipulating_Scene_Text_in_the_Wild_With_Diffusion_Models_WACV_2024_paper.html": {
    "title": "On Manipulating Scene Text in the Wild With Diffusion Models",
    "volume": "main",
    "abstract": "Diffusion models have gained attention for image editing yielding impressive results in text-to-image tasks. On the downside, one might notice that generated images of stable diffusion models suffer from deteriorated details. This pitfall impacts image editing tasks that require information preservation e.g., scene text editing. As a desired result, the model must show the capability to replace the text on the source image to the target text while preserving the details e.g., color, font size, and background. To leverage the potential of diffusion models, in this work, we introduce Diffusion-BasEd Scene Text manipulation network so-called DBEST. Specifically, we design two adaptation strategies, namely one-shot style adaptation and text-recognition guidance. In experiments, we thoroughly assess and compare our proposed method against state-of-the-arts on various scene text datasets, then provide extensive ablation studies for each granularity to analyze our performance gain. Also, we demonstrate the effectiveness of our proposed method to synthesize scene text indicated by competitive Optical Character Recognition (OCR) accuracy. Our method achieves 94.15% and 98.12% on COCO-text and ICDAR2013 datasets for character-level evaluation",
    "checked": true,
    "id": "2236807f5483b80cf28391f4d0b707d3f307885d",
    "semantic_title": "on manipulating scene text in the wild with diffusion models",
    "citation_count": 0,
    "authors": [
      "Joshua Santoso",
      "Christian Simon",
      "Williem"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Sun_Improved_Techniques_for_Quantizing_Deep_Networks_With_Adaptive_Bit-Widths_WACV_2024_paper.html": {
    "title": "Improved Techniques for Quantizing Deep Networks With Adaptive Bit-Widths",
    "volume": "main",
    "abstract": "Quantizing deep networks with adaptive bit-widths is a promising technique for efficient inference across many devices and resource constraints. In contrast to static methods that repeat the quantization process and train different models for different constraints, adaptive quantization enables us to flexibly adjust the bit-widths of a single deep network during inference for instant adaptation in different scenarios. While existing research shows encouraging results on common image classification benchmarks, this paper investigates how to train such adaptive networks more effectively. Specifically, we present two novel techniques for quantizing deep neural networks with adaptive bit-widths of weights and activations. First, we propose a collaborative strategy to choose a high-precision \"teacher\" for transferring knowledge to the low-precision \"student\" while jointly optimizing the model with all bit-widths. Second, to effectively transfer knowledge, we develop a dynamic block swapping method by randomly replacing the blocks in the lower-precision student network with the corresponding blocks in the higher-precision teacher network. Extensive experiments on multiple image classification datasets and novel video classification experiments, well demonstrate the efficacy of our approach over state-of-the-art methods",
    "checked": false,
    "id": "bf5f5e3075e70d2e3b07560e715de84cd645c6e9",
    "semantic_title": "aq2pnn: enabling two-party privacy-preserving deep neural network inference with adaptive quantization",
    "citation_count": 0,
    "authors": [
      "Ximeng Sun",
      "Rameswar Panda",
      "Chun-Fu Richard Chen",
      "Naigang Wang",
      "Bowen Pan",
      "Aude Oliva",
      "Rogerio Feris",
      "Kate Saenko"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Duan_Mining_and_Unifying_Heterogeneous_Contrastive_Relations_for_Weakly-Supervised_Actor-Action_Segmentation_WACV_2024_paper.html": {
    "title": "Mining and Unifying Heterogeneous Contrastive Relations for Weakly-Supervised Actor-Action Segmentation",
    "volume": "main",
    "abstract": "We introduce a novel weakly-supervised video actor-action segmentation (VAAS) framework, where only video-level tags are available. Previous VAAS methods follow a synthesize-and-refine scheme, i.e., they first synthesize the pseudo-segmentation and recursively refine the segmentation. However, this process requires significant time costs and heavily relies on the quality of the initial segmentation. Unlike existing works, our method hierarchically mines contrastive relations to supplement each other for learning a visually-plausible segmentation model. Specifically, three contrastive relations are abstracted from the pixel-level and frame-level, i.e., low-level edge-aware, class-activation map aware, and semantic tag-aware relations. Then, the discovered contrastive relations are unified into a universal objective for training the segmentation model, regardless of their heterogeneity. Moreover, we incorporate motion cues and unlabeled samples to increase the discriminative power and robustness of the segmentation model. Extensive experiments indicate that our proposed method produces reasonable segmentation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Duan",
      "Hao Tang",
      "Changchang Sun",
      "Ye Zhu",
      "Yan Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Liu_Rethinking_Knowledge_Distillation_With_Raw_Features_for_Semantic_Segmentation_WACV_2024_paper.html": {
    "title": "Rethinking Knowledge Distillation With Raw Features for Semantic Segmentation",
    "volume": "main",
    "abstract": "Most existing knowledge distillation methods for semantic segmentation focus on extracting various sophisticated knowledge from raw features. However, such knowledge is usually manually designed and relies on prior knowledge as in traditional feature engineering. In this paper, we aim to propose a simple and effective feature distillation method using raw features. To this end, we revisit the pioneering work in feature distillation, FitNets, which simply minimizes the mean squared error (MSE) loss between the teacher and student features. Our experiments show that this naive method yields good results, even surpassing some well-designed methods in some cases. However, it requires carefully tuning the weight of distillation loss. By decomposing the loss function of FitNets into a magnitude difference term and an angular difference term, we find the weight of the angular difference term is affected by the magnitudes of the teacher features and the student features. We experimentally show that the angular difference term plays a crucial role in feature distillation and the magnitude of the features produced by different models may vary significantly. Therefore, it is hard to determine a suitable loss weight for various models. To avoid the weight of the angular distillation term being affected by the magnitude of the features, we propose Angular Distillation and explore distilling angular information along different feature dimensions for semantic segmentation. Extensive experiments show that our simple method exhibits great robustness to hyper-parameters and achieves state-of-the-art distillation performance for semantic segmentation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Liu",
      "Chenshu Chen",
      "Xi Yang",
      "Wenming Tan"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Park_Fully-Automatic_Reflection_Removal_for_360-Degree_Images_WACV_2024_paper.html": {
    "title": "Fully-Automatic Reflection Removal for 360-Degree Images",
    "volume": "main",
    "abstract": "Reflection removal (RR) is a technique to reconstruct the transmitted scene behind the glass from a mixed image taken through glass. In 360-degree images, the mixed image region and the reference image region capturing the reflected scene exist together, and the mixed image is often restored by using the information of reference image. In this paper, we first propose a fully-automatic end-to-end RR framework for 360-degree images which automatically detects the mixed and reference image regions and removes the reflection artifacts in the mixed image by using the reference information simultaneously. We devise a transformer based U-Net architecture with horizontal windowing scheme to capture the long-range dependencies between the mixed and reference images via the self-attention mechanism and suppress the reflection artifacts by using the reference information. We also construct a training dataset of 360-degree images by synthesizing realistic reflection artifacts considering diverse geometric relation and photometric variation between the mixed and reference images. The experimental results show that the proposed method detects the mixed and reference image regions reliably without user-annotation and achieves better performance of RR compared with the state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonghyuk Park",
      "Hyeona Kim",
      "Eunpil Park",
      "Jae-Young Sim"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Xian_MITFAS_Mutual_Information_Based_Temporal_Feature_Alignment_and_Sampling_for_WACV_2024_paper.html": {
    "title": "MITFAS: Mutual Information Based Temporal Feature Alignment and Sampling for Aerial Video Action Recognition",
    "volume": "main",
    "abstract": "We present a novel approach for action recognition in UAV videos. Our formulation is designed to handle occlusion and viewpoint changes caused by the movement of a UAV. We use the concept of mutual information to compute and align the regions corresponding to human action or motion in the temporal domain. This enables our recognition model to learn from the key features associated with the motion. We also propose a novel frame sampling method that uses joint mutual information to acquire the most informative frame sequence in UAV videos. We have integrated our approach with X3D and evaluated the performance on multiple datasets. In practice, we achieve 18.9% improvement in Top-1 accuracy over current state-of-the-art methods on UAV-Human, 7.3% improvement on Drone-Action, and 7.16% improvement on NEC Drones. The code is available at https://github.com/Ricky-Xian/MITFAS",
    "checked": true,
    "id": "5bf7813028e533989dba3ba59d61f0e14fc0a8de",
    "semantic_title": "mitfas: mutual information based temporal feature alignment and sampling for aerial video action recognition",
    "citation_count": 2,
    "authors": [
      "Ruiqi Xian",
      "Xijun Wang",
      "Dinesh Manocha"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ziaratnia_Multimodal_Deep_Learning_for_Remote_Stress_Estimation_Using_CCT-LSTM_WACV_2024_paper.html": {
    "title": "Multimodal Deep Learning for Remote Stress Estimation Using CCT-LSTM",
    "volume": "main",
    "abstract": "Stress estimation is key to the early detection and mitigation of health problems, enhancing driving safety through driver stress monitoring, and improving human-robot interaction efficiency by adapting to user's stress levels. In this paper, we present a novel method for video-based remote stress estimation and categorization, which involves two separate experiments: one for stress task classification and another for multilevel stress classification. The method combines two deep learning approaches, the Compact Convolutional Transformer (CCT) and Long Short-Term Memory (LSTM), to form a CCT-LSTM pipeline. For each modality (facial expression and rPPG), a CCT model is used to extract features, followed by an LSTM block for temporal pattern recognition. In stress task classification, T1, T2, and T3 tasks from the UBFC-Phys dataset are used, utilizing sevenfold cross-validation. The results indicated a mean accuracy of 83.2% and an F1 score of 83.4%. For multilevel stress classification, the control (lower stress) and test (higher stress) groups from the same dataset were used with fivefold cross-validation, achieving a mean accuracy of 80.5% and an F1 score of 80.3%. The results suggest that our proposed model surpasses existing stress estimation methods by effectively using multimodal deep learning and the CCT-LSTM pipeline for precise, non-invasive stress detection and categorization, with applications in health monitoring, safety, and interactive technologies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sayyedjavad Ziaratnia",
      "Tipporn Laohakangvalvit",
      "Midori Sugaya",
      "Peeraya Sripian"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Liu_Let_the_Beat_Follow_You_-_Creating_Interactive_Drum_Sounds_WACV_2024_paper.html": {
    "title": "Let the Beat Follow You - Creating Interactive Drum Sounds From Body Rhythm",
    "volume": "main",
    "abstract": "It is often the case that human body movements include rhythmic patterns. A video camera system that captures these patterns and responds to them with rhythmic sounds or music as these happen could create a unique interactive experience. Creating such an experience is challenging and cannot be achieved with existing methods since it requires a real-time translation of related visual cues into in-rhythm sounds. In this work, we propose a novel learning-based system, called 'InteractiveBeat', which generates an evolving interactive soundtrack for a camera input that captures person's movements. InteractiveBeat infers body skeleton keypoints and translates them into drum rhythms using a series of sequence models. It then implements a conditional drum generation network for generating polyphonic drum sounds based on the rhythms. To guarantee real-time function, these models are integrated into a time-evolving pipeline with update rules. For training and evaluation of InteractiveBeat, in addition to training on well-annotated large-scale dance database, we collected a dataset of in-the-wild videos with people performing movements of various activities that correspond to background music. We evaluate InteractiveBeat in two scenarios: i) laboratory setting, ii) prerecorded videos of movements from in-the-wild videos, and develop 'live' demo prototype of the system. Our results on evaluations show that the system can generate interactive rhythmic drums with higher accuracy than existing methods and achieves non-cumulative latency of 34ms. This allows InteractiveBeat to be synchronized with the video stream and to react to movements in real-time",
    "checked": false,
    "id": "816c38546a771ee53fe6d3ea68f7513a3f8878c4",
    "semantic_title": "sound as an object. considerations in the design of gestural musical instruments and vibro-acoustic furniture",
    "citation_count": 0,
    "authors": [
      "Xiulong Liu",
      "Kun Su",
      "Eli Shlizerman"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Sarkar_A_Visual_Active_Search_Framework_for_Geospatial_Exploration_WACV_2024_paper.html": {
    "title": "A Visual Active Search Framework for Geospatial Exploration",
    "volume": "main",
    "abstract": "Many problems can be viewed as forms of geospatial search aided by aerial imagery, with examples ranging from detecting poaching activity to human trafficking. We model this class of problems in a visual active search (VAS) framework, which has three key inputs: (1) an image of the entire search area, which is subdivided into regions, (2) a local search function, which determines whether a previously unseen object class is present in a given region, and (3) a fixed search budget, which limits the number of times the local search function can be evaluated. The goal is to maximize the number of objects found within the search budget. We propose a reinforcement learning approach for VAS that learns a meta-search policy from a collection of fully annotated search tasks. This meta-search policy is then used to dynamically search for a novel target-object class, leveraging the outcome of any previous queries to determine where to query next. Through extensive experiments on several large-scale satellite imagery datasets, we show that the proposed approach significantly outperforms several strong baselines. We also propose novel domain adaptation techniques that improve the policy at decision time when there is a significant domain gap with the training data. Code is publicly available",
    "checked": true,
    "id": "f1d6474fd8bdac189d513a5d65e5bb4d0d88ec8d",
    "semantic_title": "a visual active search framework for geospatial exploration",
    "citation_count": 1,
    "authors": [
      "Anindya Sarkar",
      "Michael Lanier",
      "Scott Alfeld",
      "Jiarui Feng",
      "Roman Garnett",
      "Nathan Jacobs",
      "Yevgeniy Vorobeychik"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhu_ShARc_Shape_and_Appearance_Recognition_for_Person_Identification_In-the-Wild_WACV_2024_paper.html": {
    "title": "ShARc: Shape and Appearance Recognition for Person Identification In-the-Wild",
    "volume": "main",
    "abstract": "Identifying individuals in unconstrained video settings is a valuable yet challenging task in biometric analysis due to variations in appearances, environments, degradations, and occlusions. In this paper, we present ShARc, a multimodal approach for video-based person identification in uncontrolled environments that emphasizes 3-D body shape, pose, and appearance. We introduce two encoders: a Pose and Shape Encoder (PSE) and an Aggregated Appearance Encoder (AAE). PSE encodes the body shape via binarized silhouettes, skeleton motions, and 3-D body shape, while AAE provides two levels of temporal appearance feature aggregation: attention-based feature aggregation and averaging aggregation. For attention-based feature aggregation, we employ spatial and temporal attention to focus on key areas for person distinction. For averaging aggregation, we introduce a novel flattening layer after averaging to extract more distinguishable information and reduce overfitting of attention. We utilize centroid feature averaging for gallery registration. We demonstrate significant improvements over existing state-of-the-art methods on public datasets, including CCVID, MEVID, and BRIAR",
    "checked": true,
    "id": "4eb16a5ff7b2f644d191a88edddf1c951c45d661",
    "semantic_title": "sharc: shape and appearance recognition for person identification in-the-wild",
    "citation_count": 2,
    "authors": [
      "Haidong Zhu",
      "Wanrong Zheng",
      "Zhaoheng Zheng",
      "Ram Nevatia"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Yu_DocReal_Robust_Document_Dewarping_of_Real-Life_Images_via_Attention-Enhanced_Control_WACV_2024_paper.html": {
    "title": "DocReal: Robust Document Dewarping of Real-Life Images via Attention-Enhanced Control Point Prediction",
    "volume": "main",
    "abstract": "Document image dewarping is a crucial task in computer vision with numerous practical applications. The control point method, as a popular image dewarping approach, has attracted attention due to its simplicity and efficiency. However, inaccurate control point prediction due to varying background noises and deformation types can result in unsatisfactory performance. To address these issues, we propose a robust document dewarping approach for real-life images, namely DocReal, which utilizes Enet to effectively remove background noise and an attention-enhanced control point (AECP) module to better capture local deformations. Moreover, we augment the training data by synthesizing 2D images with 3D deformations and additional deformation types. Our proposed method achieves state-of-the-art performance on the DocUNet benchmark and a newly proposed benchmark of 200 Chinese distorted images, exhibiting superior dewarping accuracy, OCR performance, and robustness to various types of image distortion",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangchen Yu",
      "Yina Xie",
      "Lei Wu",
      "Yafei Wen",
      "Guozhi Wang",
      "Shuai Ren",
      "Xiaoxin Chen",
      "Jianfeng Mao",
      "Wenye Li"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Pidaparthy_Multi-Level_Attention_Aggregation_for_Aesthetic_Face_Relighting_WACV_2024_paper.html": {
    "title": "Multi-Level Attention Aggregation for Aesthetic Face Relighting",
    "volume": "main",
    "abstract": "Face relighting is the challenging task of estimating the illumination cast on portrait images by a light source varying in both position and intensity. As shadows are an important aspect of relighting, many prior works focus on estimating accurate shadows using either a shadow mask or face geometry. While these work well, the rendered images do not look aesthetic/photo-realistic. We propose a novel method that combines the features from attention maps at higher resolutions with the lighting information to estimate aesthetic relit images with accurate shadows. We created a new relighting dataset using a synthetic One-Light-At-a-Time (OLAT) lighting rig in Blender software that captures most of the variations encountered in face relighting. Through extensive experimental validation, we show that the performance of our model is better than the current state-of-art face relighting models despite training on a significantly smaller dataset of only synthetic images. We also demonstrate unsupervised domain adaptation from synthetic to real images. We show that our model is able to adapt very well to significantly different out-of-training light source positions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hemanth Pidaparthy",
      "Abhay Chauhan",
      "Pavan Sudheendra"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kim_Learning_Residual_Elastic_Warps_for_Image_Stitching_Under_Dirichlet_Boundary_WACV_2024_paper.html": {
    "title": "Learning Residual Elastic Warps for Image Stitching Under Dirichlet Boundary Condition",
    "volume": "main",
    "abstract": "Trendy suggestions for learning-based elastic warps enable the deep image stitchings to align images exposed to large parallax errors. Despite the remarkable alignments, the methods struggle with occasional holes or discontinuity between overlapping and non-overlapping regions of a target image as the applied training strategy mostly focuses on overlap region alignment. As a result, they require additional modules such as seam finder and image inpainting for hiding discontinuity and filling holes, respectively. In this work, we suggest Recurrent Elastic Warps (REwarp) that address the problem with Dirichlet boundary condition and boost performances by residual learning for recurrent misalign correction. Specifically, REwarp predicts a homography and a Thin-plate Spline (TPS) under the boundary constraint for discontinuity and hole-free image stitching. Our experiments show the favorable aligns and the competitive computational costs of REwarp compared to the existing stitching methods. Our source code is available at https://github.com/minshu-kim/REwarp",
    "checked": true,
    "id": "83675d2a8206a38c0bc06e30698b83a70f9b95c1",
    "semantic_title": "learning residual elastic warps for image stitching under dirichlet boundary condition",
    "citation_count": 0,
    "authors": [
      "Minsu Kim",
      "Yongjun Lee",
      "Woo Kyoung Han",
      "Kyong Hwan Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Myers-Dean_Interactive_Segmentation_for_Diverse_Gesture_Types_Without_Context_WACV_2024_paper.html": {
    "title": "Interactive Segmentation for Diverse Gesture Types Without Context",
    "volume": "main",
    "abstract": "Interactive segmentation entails a human marking an image to guide how a model either creates or edits a segmentation. Our work addresses limitations of existing methods: they either only support one gesture type for marking an image (e.g., either clicks or scribbles) or require knowledge of the gesture type being employed, and require specifying whether marked regions should be included versus excluded in the final segmentation. We instead propose a simplified interactive segmentation task where a user only must mark an image, where the input can be of any gesture type without specifying the gesture type. We support this new task by introducing the first interactive segmentation dataset with multiple gesture types as well as a new evaluation metric capable of holistically evaluating interactive segmentation algorithms. We then analyze numerous interactive segmentation algorithms, including ones adapted for our novel task. While we observe promising performance overall, we also highlight areas for future improvement. To facilitate further extensions of this work, we publicly share our new dataset at https://github.com/joshmyersdean/dig",
    "checked": true,
    "id": "c66f8bf0c79246693774d85dbbab49eb61c8f59f",
    "semantic_title": "interactive segmentation for diverse gesture types without context",
    "citation_count": 0,
    "authors": [
      "Josh Myers-Dean",
      "Yifei Fan",
      "Brian Price",
      "Wilson Chan",
      "Danna Gurari"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wang_Customizing_360-Degree_Panoramas_Through_Text-to-Image_Diffusion_Models_WACV_2024_paper.html": {
    "title": "Customizing 360-Degree Panoramas Through Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "Personalized text-to-image (T2I) synthesis based on diffusion models has attracted significant attention in recent research. However, existing methods primarily concentrate on customizing subjects or styles, neglecting the exploration of global geometry. In this study, we propose an approach that focuses on the customization of 360-degree panoramas, which inherently possess global geometric properties, using a T2I diffusion model. To achieve this, we curate a paired image-text dataset specifically designed for the task and subsequently employ it to fine-tune a pre-trained T2I diffusion model with LoRA. Nevertheless, the fine-tuned model alone does not ensure the continuity between the leftmost and rightmost sides of the synthesized images, a crucial characteristic of 360-degree panoramas. To address this issue, we propose a method called StitchDiffusion. Specifically, we perform pre-denoising operations twice at each time step of the denoising process on the stitch block consisting of the leftmost and rightmost image regions. Furthermore, a global cropping is adopted to synthesize seamless 360-degree panoramas. Experimental results demonstrate the effectiveness of our customized model combined with the proposed StitchDiffusion in generating high-quality 360-degree panoramic images. Moreover, our customized model exhibits exceptional generalization ability in producing scenes unseen in the fine-tuning dataset. Code is available at https://github.com/littlewhitesea/StitchDiffusion",
    "checked": true,
    "id": "3e5e06bc5624b2408fec03c301cfe6c3a48747fa",
    "semantic_title": "customizing 360-degree panoramas through text-to-image diffusion models",
    "citation_count": 2,
    "authors": [
      "Hai Wang",
      "Xiaoyu Xiang",
      "Yuchen Fan",
      "Jing-Hao Xue"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Hu_Temporal_Context_Enhanced_Referring_Video_Object_Segmentation_WACV_2024_paper.html": {
    "title": "Temporal Context Enhanced Referring Video Object Segmentation",
    "volume": "main",
    "abstract": "The goal of Referring Video Object Segmentation is to extract an object from a video clip based on a given expression. While previous methods have utilized the transformer's multi-modal learning capabilities to aggregate information from different modalities, they have mainly focused on spatial information and paid less attention to temporal information. To enhance the learning of temporal information, we propose TCE-RVOS with a novel frame token fusion (FTF) structure and a novel instance query transformer (IQT). Our technical innovations maximize the potential information gain of videos over single images. Our contributions also include a new classification of two widely used validation datasets for investigation of challenging cases. Our experimental results demonstrate that TCE-RVOS effectively captures temporal information and outperforms the previous state-of-the-art methods by increasing the J&F score by 4.0 and 1.9 points using ResNet-50 and VSwin-Tiny as the backbone on Ref-Youtube-VOS, respectively, and +2.0 mAP on A2D-Sentences dataset by using VSwin-Tiny backbone. The code is available at https://github.com/haliphinx/TCE-RVOS",
    "checked": false,
    "id": "674683805bf3acce27b08c650bbe2b09701840dd",
    "semantic_title": "temporal collection and distribution for referring video object segmentation",
    "citation_count": 3,
    "authors": [
      "Xiao Hu",
      "Basavaraj Hampiholi",
      "Heiko Neumann",
      "Jochen Lang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Liu_Revisiting_Token_Pruning_for_Object_Detection_and_Instance_Segmentation_WACV_2024_paper.html": {
    "title": "Revisiting Token Pruning for Object Detection and Instance Segmentation",
    "volume": "main",
    "abstract": "Vision Transformers (ViTs) have shown impressive performance in computer vision, but their high computational cost, quadratic in the number of tokens, limits their adoption in computation-constrained applications. However, this large number of tokens may not be necessary, as not all tokens are equally important. In this paper, we investigate token pruning to accelerate inference for object detection and instance segmentation, extending prior works from image classification. Through extensive experiments, we offer four insights for dense tasks: (i) tokens should not be completely pruned and discarded, but rather preserved in the feature maps for later use. (ii) reactivating previously pruned tokens can further enhance model performance. (iii) a dynamic pruning rate based on images is better than a fixed pruning rate. (iv) a lightweight, 2-layer MLP can effectively prune tokens, achieving accuracy comparable with complex gating networks with a simpler design. We assess the effects of these design decisions on the COCO dataset and introduce an approach that incorporates these findings, showing a reduction in performance decline from 1.5 mAP to 0.3 mAP in both boxes and masks, compared to existing token pruning methods. In relation to the dense counterpart that utilizes all tokens, our method realizes an increase in inference speed, achieving up to 34% faster performance for the entire network and 46% for the backbone. Code will be publicly available",
    "checked": true,
    "id": "8f94725de737761336a268504ed3269d88942594",
    "semantic_title": "revisiting token pruning for object detection and instance segmentation",
    "citation_count": 0,
    "authors": [
      "Yifei Liu",
      "Mathias Gehrig",
      "Nico Messikommer",
      "Marco Cannici",
      "Davide Scaramuzza"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Gaarsdal_AssemblyNet_A_Point_Cloud_Dataset_and_Benchmark_for_Predicting_Part_WACV_2024_paper.html": {
    "title": "AssemblyNet: A Point Cloud Dataset and Benchmark for Predicting Part Directions in an Exploded Layout",
    "volume": "main",
    "abstract": "Exploded views are powerful tools for visualizing the assembly and disassembly of complex objects, widely used in technical illustrations, assembly instructions, and product presentations. Previous methods for automating the creation of exploded views are either slow and computationally costly or compromise on accuracy. Therefore, the construction of exploded views is typically a manual process. In this paper, we propose a novel approach for automatically predicting the direction of parts in an exploded view using deep learning. To achieve this, we introduce a new dataset, AssemblyNet, which contains point cloud data sampled from 3D models of real-world assemblies, including water pumps, mixed industrial assemblies, and LEGO models. The AssemblyNet dataset includes a total of 44 assemblies, separated into 495 subassemblies with a total of 5420 parts. We provide ground truth labels for regression and classification, representing the directions in which the parts are moved in the exploded views. We also provide performance benchmarks using various state-of-the-art models for shape classification on point clouds and propose a novel two-path network architecture. Project page available at https://github.com/jgaarsdal/AssemblyNet",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jesper Gaarsdal",
      "Joakim Bruslund Haurum",
      "Sune Wolff",
      "Claus Brøndgaard Madsen"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Caron_Location-Aware_Self-Supervised_Transformers_for_Semantic_Segmentation_WACV_2024_paper.html": {
    "title": "Location-Aware Self-Supervised Transformers for Semantic Segmentation",
    "volume": "main",
    "abstract": "Pixel-level labels are particularly expensive to acquire. Hence, pretraining is a critical step to improve models on a task like semantic segmentation. However, prominent algorithms for pretraining neural networks use image-level objectives, e.g. image classification, image-text alignment a la CLIP, or self-supervised contrastive learning. These objectives do not model spatial information, which might be sub-optimal when finetuning on downstream tasks with spatial reasoning. In this work, we pretrain networks with a location-aware (LOCA) self-supervised method which fosters the emergence of strong dense features. Specifically, we use both a patch-level clustering scheme to mine dense pseudo-labels and a relative location prediction task to encourage learning about object parts and their spatial arrangement. Our experiments show that LOCA pretraining leads to representations that transfer competitively to challenging and diverse semantic segmentation datasets",
    "checked": false,
    "id": "81bccb97ab141760cbcc165e3b8f3891c40e5bb0",
    "semantic_title": "location-aware self-supervised transformers",
    "citation_count": 8,
    "authors": [
      "Mathilde Caron",
      "Neil Houlsby",
      "Cordelia Schmid"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Anastasakis_Self-Supervised_Learning_for_Visual_Relationship_Detection_Through_Masked_Bounding_Box_WACV_2024_paper.html": {
    "title": "Self-Supervised Learning for Visual Relationship Detection Through Masked Bounding Box Reconstruction",
    "volume": "main",
    "abstract": "We present a novel self-supervised approach for representation learning, particularly for the task of Visual Relationship Detection (VRD). Motivated by the effectiveness of Masked Image Modeling (MIM), we propose Masked Bounding Box Reconstruction (MBBR), a variation of MIM where a percentage of the entities/objects within a scene are masked and subsequently reconstructed based on the unmasked objects. The core idea is that, through object-level masked modeling, the network learns context-aware representations that capture the interaction of objects within a scene and thus are highly predictive of visual object relationships. We extensively evaluate learned representations, both qualitatively and quantitatively, in a few-shot setting and demonstrate the efficacy of MBBR for learning robust visual representations, particularly tailored for VRD. The proposed method is able to surpass state-of-the-art VRD methods on the Predicate Detection (PredDet) evaluation setting, using only a few annotated samples. We make our code available at https://github.com/deeplab-ai/SelfSupervisedVRD",
    "checked": true,
    "id": "4ab629a146c73f5dbfd592852a53d44cfe83da82",
    "semantic_title": "self-supervised learning for visual relationship detection through masked bounding box reconstruction",
    "citation_count": 0,
    "authors": [
      "Zacharias Anastasakis",
      "Dimitrios Mallis",
      "Markos Diomataris",
      "George Alexandridis",
      "Stefanos Kollias",
      "Vassilis Pitsikalis"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ebmer_Real-Time_6-DoF_Pose_Estimation_by_an_Event-Based_Camera_Using_Active_WACV_2024_paper.html": {
    "title": "Real-Time 6-DoF Pose Estimation by an Event-Based Camera Using Active LED Markers",
    "volume": "main",
    "abstract": "Real-time applications for autonomous operations depend largely on fast and robust vision-based localization systems. Since image processing tasks require processing large amounts of data, the computational resources often limit the performance of other processes. To overcome this limitation, traditional marker-based localization systems are widely used since they are easy to integrate and achieve reliable accuracy. However, classical marker-based localization systems significantly depend on standard cameras with low frame rates, which often lack accuracy due to motion blur. In contrast, event-based cameras provide high temporal resolution and a high dynamic range, which can be utilized for fast localization tasks, even under challenging visual conditions. This paper proposes a simple but effective event-based pose estimation system using active LED markers (ALM) for fast and accurate pose estimation. The proposed algorithm is able to operate in real time with a latency below 0.5 ms while maintaining output rates of 3 kHz. Experimental results in static and dynamic scenarios are presented to demonstrate the performance of the proposed approach in terms of computational speed and absolute accuracy, using the OptiTrack system as the basis for measurement. Moreover, we demonstrate the feasibility of the proposed approach by deploying the hardware, i.e., the event-based camera and ALM, and the software in a real quadcopter application",
    "checked": true,
    "id": "00b7af46731d220461dfe66ace7df2742415a7f7",
    "semantic_title": "real-time 6-dof pose estimation by an event-based camera using active led markers",
    "citation_count": 0,
    "authors": [
      "Gerald Ebmer",
      "Adam Loch",
      "Minh Nhat Vu",
      "Roberto Mecca",
      "Germain Haessig",
      "Christian Hartl-Nesic",
      "Markus Vincze",
      "Andreas Kugi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ali_P-Age_Pexels_Dataset_for_Robust_Spatio-Temporal_Apparent_Age_Classification_WACV_2024_paper.html": {
    "title": "P-Age: Pexels Dataset for Robust Spatio-Temporal Apparent Age Classification",
    "volume": "main",
    "abstract": "Age estimation is a challenging task that has numerous applications. In this paper, we propose a new direction for age classification that utilizes a video-based model to address challenges such as occlusions, low-resolution, and lighting conditions. To address these challenges, we propose AgeFormer which utilizes spatio-temporal information on the dynamics of the entire body dominating face-based methods for age classification. Our novel two-stream architecture uses TimeSformer and EfficientNet as backbones, to effectively capture both facial and body dynamics information for efficient and accurate age estimation in videos. Furthermore, to fill the gap in predicting age in real-world situations from videos, we construct a video dataset called Pexels Age (P-Age) for age classification. The proposed method achieves superior results compared to existing face-based age estimation methods and is evaluated in situations where the face is highly occluded, blurred, or masked. The method is also cross-tested on a variety of challenging video datasets such as Charades, Smarthome, and Thumos-14",
    "checked": true,
    "id": "dac367687b5e1298a2d887c7731eac8dea07c709",
    "semantic_title": "p-age: pexels dataset for robust spatio-temporal apparent age classification",
    "citation_count": 0,
    "authors": [
      "Abid Ali",
      "Ashish Marisetty",
      "François Brémond"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Mahmud_SSVOD_Semi-Supervised_Video_Object_Detection_With_Sparse_Annotations_WACV_2024_paper.html": {
    "title": "SSVOD: Semi-Supervised Video Object Detection With Sparse Annotations",
    "volume": "main",
    "abstract": "Despite significant progress in semi-supervised learning for image object detection, several key issues are yet to be addressed for video object detection: (1) Achieving good performance for supervised video object detection greatly depends on the availability of annotated frames. (2) Despite having large inter-frame correlations in a video, collecting annotations for a large number of frames per video is expensive, time-consuming, and often redundant. (3) Existing semi-supervised techniques on static images can hardly exploit the temporal motion dynamics inherently present in videos. In this paper, we introduce SSVOD, an end-to-end semi-supervised video object detection framework that exploits motion dynamics of videos to utilize large-scale unlabeled frames with sparse annotations. To selectively assemble robust pseudo-labels across groups of frames, we introduce flow-warped predictions from nearby frames for temporal-consistency estimation. In particular, we introduce cross-IoU and cross-divergence based selection methods over a set of estimated predictions to include robust pseudo-labels for bounding boxes and class labels, respectively. To strike a balance between confirmation bias and uncertainty noise in pseudo-labels, we propose confidence threshold based combination of hard and soft pseudo-labels. Our method achieves significant performance improvements over existing methods on ImageNet-VID, Epic-KITCHENS, and YouTube-VIS datasets. Codes are available at https://github.com/enyac-group/SSVOD.git",
    "checked": true,
    "id": "c1261eb6ea4cc8e1c583733003b9d0e2dd135b71",
    "semantic_title": "ssvod: semi-supervised video object detection with sparse annotations",
    "citation_count": 0,
    "authors": [
      "Tanvir Mahmud",
      "Chun-Hao Liu",
      "Burhaneddin Yaman",
      "Diana Marculescu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Fletcher_Deep_Optics_for_Optomechanical_Control_Policy_Design_WACV_2024_paper.html": {
    "title": "Deep Optics for Optomechanical Control Policy Design",
    "volume": "main",
    "abstract": "An emerging class of Fizeau optical telescopes have the potential to upend prior cost scaling models, substantially improving the angular resolution and contrast attainable by ground-based astronomical instruments. However, this design introduces a challenging visual control problem that must be solved to compensate for wavefront aberrations induced by the flexible substructure it employs. We subvert this problem with a deep optics approach to policy design and image recovery that exploits, rather than corrects, aberrations to obtain domain-specific object recovery performance exceeding that of more costly filled aperture designs",
    "checked": false,
    "id": "cf208298360461f91c8b52f108b95ce24323cd57",
    "semantic_title": "developing subsea communication using fiber optics",
    "citation_count": 0,
    "authors": [
      "Justin Fletcher"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Laczko_A_Generative_Multi-Resolution_Pyramid_and_Normal-Conditioning_3D_Cloth_Draping_WACV_2024_paper.html": {
    "title": "A Generative Multi-Resolution Pyramid and Normal-Conditioning 3D Cloth Draping",
    "volume": "main",
    "abstract": "RGB cloth generation has been deeply studied in the related literature, however, 3D garment generation remains an open problem. In this paper, we build a conditional variational autoencoder for 3D garment generation and draping. We propose a pyramid network to add garment details progressively in a canonical space, i.e. unposing and unshaping the garments w.r.t. the body. We study conditioning the network on surface normal UV maps, as an intermediate representation, which is an easier problem to optimize than 3D coordinates. Our results on two public datasets, CLOTH3D and CAPE, show that our model is robust, controllable in terms of detail generation by the use of multi-resolution pyramids, and achieves state-of-the-art results that can highly generalize to unseen garments, poses, and shapes even when training with small amounts of data. The code can be found at: https://github.com/HunorLaczko/pyramid-drape",
    "checked": true,
    "id": "5b550dadd308dbba5b43b16a8940956d1fa70f93",
    "semantic_title": "a generative multi-resolution pyramid and normal-conditioning 3d cloth draping",
    "citation_count": 0,
    "authors": [
      "Hunor Laczkó",
      "Meysam Madadi",
      "Sergio Escalera",
      "Jordi Gonzalez"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Sagar_MAdVerse_A_Hierarchical_Dataset_of_Multi-Lingual_Ads_From_Diverse_Sources_WACV_2024_paper.html": {
    "title": "MAdVerse: A Hierarchical Dataset of Multi-Lingual Ads From Diverse Sources and Categories",
    "volume": "main",
    "abstract": "The convergence of computer vision and advertising has sparked substantial interest lately. Existing advertisement datasets often derive from subsets of established data with highly specialized annotations or feature diverse annotations without a cohesive taxonomy among ad images. Notably, no datasets encompass diverse advertisement styles or semantic grouping at various levels of granularity for a better understanding of ads. Our work addresses this gap by introducing MAdVerse, an extensive, multilingual compilation of more than 50,000 ads from the web, social media websites and e-newspapers. Advertisements are hierarchically grouped with uniform granularity into 11 categories, divided into 51 sub-categories, and 524 fine-grained brands at leaf level, each featuring ads in various languages. Furthermore, we provide comprehensive baseline classification results for various pertinent prediction tasks within the realm of advertising analysis. Specifically, these tasks include hierarchical ad classification, source classification, multilingual classification and inducing hierarchy in existing ad datasets. The dataset, code and models are available on the project page https://madverse24.github.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amruth Sagar",
      "Rishabh Srivastava",
      "Rakshitha R. T.",
      "Venkata Kesav Venna",
      "Ravi Kiran Sarvadevabhatla"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Schubert_Identifying_Label_Errors_in_Object_Detection_Datasets_by_Loss_Inspection_WACV_2024_paper.html": {
    "title": "Identifying Label Errors in Object Detection Datasets by Loss Inspection",
    "volume": "main",
    "abstract": "Labeling datasets for supervised object detection is a dull and time-consuming task. Errors can be easily introduced during annotation and overlooked during review, yielding inaccurate benchmarks and performance degradation of deep neural networks trained on noisy labels. In this work, we introduce a benchmark for label error detection methods on object detection datasets as well as a theoretically underpinned label error detection method and a number of baselines. We simulate four different types of randomly introduced label errors on train and test sets of well-labeled object detection datasets. For our label error detection method we assume a two-stage object detector to be given and consider the sum of both stages' classification and regression losses. The losses are computed with respect to the predictions and the noisy labels including simulated label errors, aiming at detecting the latter. We compare our method to four baselines: a naive one without deep learning, the object detector's score, the entropy of the classification softmax distribution and a probability margin based method from related work. We outperform all baselines and demonstrate that among the considered methods, ours is the only one that detects label errors of all four types efficiently, which we also derive theoretically. Furthermore, we detect real label errors a) on commonly used test datasets in object detection and b) on a proprietary dataset. In both cases we achieve low false positives rates, i.e., we detect label errors with a precision for a) of up to 71.5% and for b) with 97%",
    "checked": true,
    "id": "5936053a05256cfef2db446e65420110a3af76e6",
    "semantic_title": "identifying label errors in object detection datasets by loss inspection",
    "citation_count": 1,
    "authors": [
      "Marius Schubert",
      "Tobias Riedlinger",
      "Karsten Kahl",
      "Daniel Kröll",
      "Sebastian Schoenen",
      "Siniša Šegvić",
      "Matthias Rottmann"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Agnolucci_Reference-Based_Restoration_of_Digitized_Analog_Videotapes_WACV_2024_paper.html": {
    "title": "Reference-Based Restoration of Digitized Analog Videotapes",
    "volume": "main",
    "abstract": "Analog magnetic tapes have been the main video data storage device for several decades. Videos stored on analog videotapes exhibit unique degradation patterns caused by tape aging and reader device malfunctioning that are different from those observed in film and digital video restoration tasks. In this work, we present a reference-based approach for the resToration of digitized Analog videotaPEs (TAPE). We leverage CLIP for zero-shot artifact detection to identify the cleanest frames of each video through textual prompts describing different artifacts. Then, we select the clean frames most similar to the input ones and employ them as references. We design a transformer-based Swin-UNet network that exploits both neighboring and reference frames via our Multi-Reference Spatial Feature Fusion (MRSFF) blocks. MRSFF blocks rely on cross-attention and attention pooling to take advantage of the most useful parts of each reference frame. To address the absence of ground truth in real-world videos, we create a synthetic dataset of videos exhibiting artifacts that closely resemble those commonly found in analog videotapes. Both quantitative and qualitative experiments show the effectiveness of our approach compared to other state-of-the-art methods. The code, the model, and the synthetic dataset are publicly available at https://github.com/miccunifi/TAPE",
    "checked": true,
    "id": "d781fbe0ea499f501a386bf1cc01c31991bbefa0",
    "semantic_title": "reference-based restoration of digitized analog videotapes",
    "citation_count": 0,
    "authors": [
      "Lorenzo Agnolucci",
      "Leonardo Galteri",
      "Marco Bertini",
      "Alberto Del Bimbo"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Narayanswamy_BigSmall_Efficient_Multi-Task_Learning_for_Disparate_Spatial_and_Temporal_Physiological_WACV_2024_paper.html": {
    "title": "BigSmall: Efficient Multi-Task Learning for Disparate Spatial and Temporal Physiological Measurements",
    "volume": "main",
    "abstract": "Understanding of human visual perception has historically inspired the design of computer vision architectures. As an example, perception occurs at different scales both spatially and temporally, suggesting that the extraction of salient visual information may be made more effective by attending to specific features at varying scales. Visual changes in the body due to physiological processes also occur at varying scales and with modality-specific characteristic properties. Inspired by this, we present BigSmall, an efficient architecture for physiological and behavioral measurement. We present the first joint camera-based facial action, cardiac, and pulmonary measurement model. We propose a multi-branch network with wrapping temporal shift modules that yields both accuracy and efficiency gains. We observe that fusing low-level features leads to suboptimal performance, but that fusing high level features enables efficiency gains with negligible losses in accuracy. Experimental results demonstrate that BigSmall significantly reduces the computational costs. Furthermore, compared to existing task-specific models, BigSmall achieves comparable or better results on multiple physiological measurement tasks simultaneously with a unified model",
    "checked": true,
    "id": "a1fcb781005e3aacc378f451ccad0ff5dff07236",
    "semantic_title": "bigsmall: efficient multi-task learning for disparate spatial and temporal physiological measurements",
    "citation_count": 4,
    "authors": [
      "Girish Narayanswamy",
      "Yujia Liu",
      "Yuzhe Yang",
      "Chengqian Ma",
      "Xin Liu",
      "Daniel McDuff",
      "Shwetak Patel"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kalla_Robust_Feature_Learning_and_Global_Variance-Driven_Classifier_Alignment_for_Long-Tail_WACV_2024_paper.html": {
    "title": "Robust Feature Learning and Global Variance-Driven Classifier Alignment for Long-Tail Class Incremental Learning",
    "volume": "main",
    "abstract": "This paper introduces a two-stage framework designed to enhance long-tail class incremental learning, enabling the model to progressively learn new classes, while mitigating catastrophic forgetting in the context of long-tailed data distributions. Addressing the challenge posed by the under-representation of tail classes in long-tail class incremental learning, our approach achieves classifier alignment by leveraging global variance as an informative measure and class prototypes in the second stage. This process effectively captures class properties and eliminates the need for data balancing or additional layer tuning. Alongside traditional class incremental learning losses in the first stage, the proposed approach incorporates mixup classes to learn robust feature representations, ensuring smoother boundaries. The proposed framework can seamlessly integrate as a module with any class incremental learning method to effectively handle long-tail class incremental learning scenarios. Extensive experimentation on the CIFAR-100 and ImageNet-Subset datasets validates the approach's efficacy, showcasing its superiority over state-of-the-art techniques across various long-tail CIL settings",
    "checked": true,
    "id": "3a74c2f6f3ac2b156f2f5465193e83a448f81f8e",
    "semantic_title": "robust feature learning and global variance-driven classifier alignment for long-tail class incremental learning",
    "citation_count": 0,
    "authors": [
      "Jayateja Kalla",
      "Soma Biswas"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Fischer_MagneticPillars_Efficient_Point_Cloud_Registration_Through_Hierarchized_Birds-Eye-View_Cell_Correspondence_WACV_2024_paper.html": {
    "title": "MagneticPillars: Efficient Point Cloud Registration Through Hierarchized Birds-Eye-View Cell Correspondence Refinement",
    "volume": "main",
    "abstract": "Recent point cloud registration approaches often deal with a consecutive determination of coarse and fine feature correspondences for hierarchical pose refinement. Due to the unordered nature of point clouds, a common way to generate a subsampled representation for the coarse matching step is by applying 3D-sensitive convolution approaches. However, expensive grouping mechanisms such as nearest neighbour search have to be used to determine the associated fine features, generating individual associations for each point cloud and leading to an increased overall runtime. Furthermore current methods often tend to predict deficient point correspondences and rely on additional filtering by expensive registration backends like RANSAC impeding their application in time critical systems. To overcome these challenges, we present MagneticPillars utilizing a Birds-Eye-View (BEV) grid representation, entailing fixed affiliations between coarse and fine feature cells. We show that by extracting correspondences in this manner, a small amount of key points is already sufficient to achieve an accurate pose estimation without external optimization methods like RANSAC. We evaluate our approach on two autonomous driving datasets for the task of point cloud registration by applying SVD as the backend, where we outperform recent state-of-the-art methods, reducing the rotation and translation error by 12% and 40%, respectively, and to top it all off, cutting runtime in half",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Fischer",
      "Martin Simon",
      "Stefan Milz",
      "Patrick Mäder"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Laroche_Fast_Diffusion_EM_A_Diffusion_Model_for_Blind_Inverse_Problems_WACV_2024_paper.html": {
    "title": "Fast Diffusion EM: A Diffusion Model for Blind Inverse Problems With Application to Deconvolution",
    "volume": "main",
    "abstract": "Using diffusion models to solve inverse problems is a growing field of research. Current methods assume the degradation to be known and provide impressive results in terms of restoration quality and diversity. In this work, we leverage the efficiency of those models to jointly estimate the restored image and unknown parameters of the degradation model such as blur kernel. In particular, we designed an algorithm based on the well-known Expectation-Minimization (EM) estimation method and diffusion models. Our method alternates between approximating the expected log-likelihood of the inverse problem using samples drawn from a diffusion model and a maximization step to estimate unknown model parameters. For the maximization step, we also introduce a novel blur kernel regularization based on a Plug & Play denoiser. Diffusion models are long to run, thus we provide a fast version of our algorithm. Extensive experiments on blind image deblurring demonstrate the effectiveness of our method when compared to other state-of-the-art approaches",
    "checked": true,
    "id": "93ec0424a9b48d25b9805b209ff6af5a88b0941b",
    "semantic_title": "fast diffusion em: a diffusion model for blind inverse problems with application to deconvolution",
    "citation_count": 2,
    "authors": [
      "Charles Laroche",
      "Andrés Almansa",
      "Eva Coupeté"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Taketsugu_Active_Transfer_Learning_for_Efficient_Video-Specific_Human_Pose_Estimation_WACV_2024_paper.html": {
    "title": "Active Transfer Learning for Efficient Video-Specific Human Pose Estimation",
    "volume": "main",
    "abstract": "Human Pose (HP) estimation is actively researched because of its wide range of applications. However, even estimators pre-trained on large datasets may not perform satisfactorily due to a domain gap between the training and test data. To address this issue, we present our approach combining Active Learning (AL) and Transfer Learning (TL) to adapt HP estimators to individual video domains efficiently. For efficient learning, our approach quantifies (i) the estimation uncertainty based on the temporal changes in the estimated heatmaps and (ii) the unnaturalness in the estimated full-body HPs. These quantified criteria are then effectively combined with the state-of-the-art representativeness criterion to select uncertain and diverse samples for efficient HP estimator learning. Furthermore, we reconsider the existing Active Transfer Learning (ATL) method to introduce novel ideas related to the retraining methods and Stopping Criteria (SC). Experimental results demonstrate that our method enhances learning efficiency and outperforms comparative methods. Our code is publicly available at: https://github.com/ImIntheMiddle/VATL4Pose-WACV2024",
    "checked": true,
    "id": "f6cf740acc538bfc3b764f2354f81f0a241ea212",
    "semantic_title": "active transfer learning for efficient video-specific human pose estimation",
    "citation_count": 0,
    "authors": [
      "Hiromu Taketsugu",
      "Norimichi Ukita"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Chen_Training-Free_Layout_Control_With_Cross-Attention_Guidance_WACV_2024_paper.html": {
    "title": "Training-Free Layout Control With Cross-Attention Guidance",
    "volume": "main",
    "abstract": "Recent diffusion-based generators can produce high-quality images from textual prompts. However, they often disregard textual instructions that specify the spatial layout of the composition. We propose a simple approach that achieves robust layout control without the need for training or fine-tuning of the image generator. Our technique manipulates the cross-attention layers that the model uses to interface textual and visual information and steers the generation in the desired direction given, e.g., a user-specified layout. To determine how to best guide attention, we study the role of attention maps and explore two alternative strategies, forward and backward guidance. We thoroughly evaluate our approach on three benchmarks and provide several qualitative examples and a comparative analysis of the two strategies that demonstrate the superiority of backward guidance compared to forward guidance, as well as prior work. We further demonstrate the versatility of layout guidance by extending it to applications such as editing the layout and context of real images",
    "checked": true,
    "id": "6a195df2f7611aa75d5734b2efb32a408d2f8348",
    "semantic_title": "training-free layout control with cross-attention guidance",
    "citation_count": 44,
    "authors": [
      "Minghao Chen",
      "Iro Laina",
      "Andrea Vedaldi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/He_Learning_Transferable_Representations_for_Image_Anomaly_Localization_Using_Dense_Pretraining_WACV_2024_paper.html": {
    "title": "Learning Transferable Representations for Image Anomaly Localization Using Dense Pretraining",
    "volume": "main",
    "abstract": "Image anomaly localization (IAL) is widely applied in fault detection and industrial inspection domains to discover anomalous patterns in images at the pixel level. The unique challenge of this task is the lack of comprehensive anomaly samples for model training. The state-of-the-art methods train end-to-end models that leverage outlier exposure to simulate pseudo anomalies, but they show poor transferability to new datasets due to the inherent bias to the synthesized outliers during training. Recently, two-stage instance-level self-supervised learning (SSL) has shown potential in learning generic representations for IAL. However, we hypothesize that dense-level SSL is more compatible as IAL requires pixel-level prediction. In this paper, we bridge these gaps by proposing a two-stage, dense pre-training model tailored for the IAL task. More specifically, our model utilizes dual positive-pair selection criteria and dual feature scales to learn more effective representations. Through extensive experiments, we show that our learned representations achieve significantly better anomaly localization performance among two-stage models, while requiring almost half the convergence time. Moreover, our learned representations have better transferability to unseen datasets. Code is available at https://github. com/terrlo/DS2",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haitian He",
      "Sarah Erfani",
      "Mingming Gong",
      "Qiuhong Ke"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Peven_Embedding_Task_Structure_for_Action_Detection_WACV_2024_paper.html": {
    "title": "Embedding Task Structure for Action Detection",
    "volume": "main",
    "abstract": "We present a straightforward, flexible method to enhance the accuracy and quality of action detection by expressing temporal and structural relationships of actions in the loss function of a deep network. We describe ways to represent otherwise implicit structure in video data and demonstrate how these structures reflect natural biases that improve network training. Our experiments show that our approach improves both accuracy and edit-distance of action recognition and detection models over a baseline. Our framework leads to improvements over prior work and obtains state-of-the-art results on multiple benchmarks",
    "checked": false,
    "id": "5e83efad4c51f9913e66e07407e6ce73a779e5f5",
    "semantic_title": "an improved deep-learning network for abnormal action detection",
    "citation_count": 0,
    "authors": [
      "Michael Peven",
      "Gregory D. Hager"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Shakibajahromi_RIMeshGNN_A_Rotation-Invariant_Graph_Neural_Network_for_Mesh_Classification_WACV_2024_paper.html": {
    "title": "RIMeshGNN: A Rotation-Invariant Graph Neural Network for Mesh Classification",
    "volume": "main",
    "abstract": "Shape analysis tasks, including mesh classification, segmentation, and retrieval demonstrate symmetries in Euclidean space and should be invariant to geometric transformations such as rotation and translation. However, existing methods in mesh analysis often rely on extensive data augmentation and more complex analysis models to handle 3D rotations. Despite these efforts, rotation invariance is not guaranteed, which can significantly reduce accuracy when test samples undergo arbitrary rotations, because the analysis method struggles to generalize to the unknown orientations of the test samples. To address these challenges, our work presents a novel approach that employs graph neural networks (GNNs) to analyze mesh-structured data. Our proposed GNN layer, aggregation function, and local pooling layer are equivariant to the rotation, reflection and translation of 3D shapes, making them suitable building blocks for our proposed rotation-invariant network for the classification of mesh models. Therefore, our proposed approach does not need rotation augmentation, and we can maintain accuracy even when test samples undergo arbitrary rotations. Extensive experiments on various datasets demonstrate that our methods achieve state-of-the-art performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bahareh Shakibajahromi",
      "Edward Kim",
      "David E. Breen"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Cheng_Stereo_Matching_in_Time_100_FPS_Video_Stereo_Matching_for_WACV_2024_paper.html": {
    "title": "Stereo Matching in Time: 100+ FPS Video Stereo Matching for Extended Reality",
    "volume": "main",
    "abstract": "Real-time Stereo Matching is a cornerstone task for Extended Reality (XR) applications, such as 3D scene understanding, video pass-through, and mixed-reality games. Despite significant advancements, getting accurate depth information in real time on a low-power mobile device remains a challenge. One of the main difficulties is the lack of high-quality indoor video stereo data captured by head-mounted VR or AR glasses. To address this, we introduce a novel video stereo synthetic dataset that comprises photorealistic renderings of various indoor scenes and realistic camera motion captured by a moving VR/AR head-mounted display (HMD). Our newly proposed dataset enables one to develop a novel framework for continuous video-rate stereo matching. As another contribution, we also propose a new video-based stereo matching approach tailored for XR applications, which achieves real-time inference at an impressive 134fps on a standard desktop computer, or 30fps on a battery-powered HMD. Our key insight is that disparity and contextual information are highly correlated and redundant between consecutive stereo frames. By unrolling an iterative cost aggregation in time (i.e. in temporal dimension), we are able to distribute and reuse the aggregated features over time. This leads to a substantial reduction in computation without sacrificing accuracy. We conducted extensive evaluations and demonstrated that our method achieves superior performance compared to the current state-of-the-art, making it a strong contender for real-time stereo matching in VR/AR applications",
    "checked": true,
    "id": "cd8f2e24263be5065849ee2c278f028027a1d3e1",
    "semantic_title": "stereo matching in time: 100+ fps video stereo matching for extended reality",
    "citation_count": 0,
    "authors": [
      "Ziang Cheng",
      "Jiayu Yang",
      "Hongdong Li"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Delatolas_Learning_the_What_and_How_of_Annotation_in_Video_Object_WACV_2024_paper.html": {
    "title": "Learning the What and How of Annotation in Video Object Segmentation",
    "volume": "main",
    "abstract": "Video Object Segmentation (VOS) is crucial for several applications, from video editing to video data generation. Training a VOS model requires an abundance of manually labeled training videos. The de-facto traditional way of annotating objects requires humans to draw detailed segmentation masks on the target objects at each video frame. This annotation process, however, is tedious and time-consuming. To reduce this annotation cost, in this paper, we propose EVA-VOS, a human-in-the-loop annotation framework for video object segmentation. Unlike the traditional approach, we introduce an agent that predicts iteratively both which frame (\"What\") to annotate and which annotation type (\"How\") to use. Then, the annotator annotates only the selected frame that is used to update a VOS module, leading to significant gains in annotation time. We conduct experiments on the MOSE and the DAVIS datasets and we show that: (a) EVA-VOS leads to masks with accuracy close to the human agreement 3.5x faster than the standard way of annotating videos; (b) our frame selection achieves state-of-the-art performance; (c) EVA-VOS yields significant performance gains in terms of annotation time compared to all other methods and baselines",
    "checked": true,
    "id": "eb119d5987506828aba1df3656886c25ef0cde56",
    "semantic_title": "learning the what and how of annotation in video object segmentation",
    "citation_count": 0,
    "authors": [
      "Thanos Delatolas",
      "Vicky Kalogeiton",
      "Dim P. Papadopoulos"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Nasser_Reverse_Knowledge_Distillation_Training_a_Large_Model_Using_a_Small_WACV_2024_paper.html": {
    "title": "Reverse Knowledge Distillation: Training a Large Model Using a Small One for Retinal Image Matching on Limited Data",
    "volume": "main",
    "abstract": "Retinal image matching (RIM) plays a crucial role in monitoring disease progression and treatment response as retina is the only tissue where blood vessels can be directly observed. However, datasets with matched keypoints between temporally separated pairs of images are not available in abundance to train transformer-based models. Firstly, we release keypoint annotations for retinal images from multiple datasets to aid further research on RIM. Secondly, we propose a novel approach based on reverse knowledge distillation to train large models with limited data while preventing overfitting. We propose architectural modifications to a CNN-based semi-supervised method called SuperRetina [22] that helps improve its results on a publicly available dataset. We train a computationally heavier model based on a vision transformer encoder, utilizing the lighter CNN-based model. This approach, which we call reverse knowledge distillation (RKD), further improves the matching results even though it contrasts with the conventional knowledge distillation where lighter models are trained based on heavier ones is the norm. Further, we show that our technique generalizes to other domains, such as facial landmark matching",
    "checked": true,
    "id": "e28f1c71732126180dcb65187ea562b4a6374a6b",
    "semantic_title": "reverse knowledge distillation: training a large model using a small one for retinal image matching on limited data",
    "citation_count": 1,
    "authors": [
      "Sahar Almahfouz Nasser",
      "Nihar Gupte",
      "Amit Sethi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Schaefer_Edge_Inference_With_Fully_Differentiable_Quantized_Mixed_Precision_Neural_Networks_WACV_2024_paper.html": {
    "title": "Edge Inference With Fully Differentiable Quantized Mixed Precision Neural Networks",
    "volume": "main",
    "abstract": "The large computing and memory cost of deep neural networks (DNNs) often precludes their use in resource-constrained devices. Quantizing the parameters and operations to lower bit-precision offers substantial memory and energy savings for neural network inference, facilitating the use of DNNs on edge computing platforms. Recent efforts at quantizing DNNs have employed a range of techniques encompassing progressive quantization, step-size adaptation, and gradient scaling. This paper proposes a new quantization approach for mixed precision convolutional neural networks (CNNs) targeting edge-computing. Our method establishes a new pareto frontier in model accuracy and memory footprint demonstrating a range of pre-trained quantized models, delivering best-in-class accuracy below 4.3 MB of weights and activations without modifying the model architecture. Our main contributions are: (i) a method for tensor-sliced learned precision with a hardware-aware cost function for heterogeneous differentiable quantization, (ii) targeted gradient modification for weights and activations to mitigate quantization errors, and (iii) a multi-phase learning schedule to address instability in learning arising from updates to the learned quantizer and model parameters. We demonstrate the effectiveness of our techniques on the ImageNet dataset across a range of models including EfficientNet-Lite0 (e.g., 4.14MB of weights and activations at 67.66% accuracy) and MobileNetV2 (e.g., 3.51MB weights and activations at 65.39% accuracy)",
    "checked": true,
    "id": "0792b5ebfb6ca62e8b4b475fc7705b0f78f746ff",
    "semantic_title": "edge inference with fully differentiable quantized mixed precision neural networks",
    "citation_count": 5,
    "authors": [
      "Clemens JS Schaefer",
      "Siddharth Joshi",
      "Shan Li",
      "Raul Blazquez"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Nadeem_CAD_-_Contextual_Multi-Modal_Alignment_for_Dynamic_AVQA_WACV_2024_paper.html": {
    "title": "CAD - Contextual Multi-Modal Alignment for Dynamic AVQA",
    "volume": "main",
    "abstract": "In the context of Audio Visual Question Answering (AVQA) tasks, the audio and visual modalities could be learnt on three levels: 1) Spatial, 2) Temporal, and 3) Semantic. Existing AVQA methods suffer from two major shortcomings; the audio-visual (AV) information passing through the network isn't aligned on Spatial and Temporal levels; and, inter-modal (audio and visual) Semantic information is often not balanced within a context; this results in poor performance. In this paper, we propose a novel end-to-end Contextual Multi-modal Alignment (CAD) network that addresses the challenges in AVQA methods by i) introducing a parameter-free stochastic Contextual block that ensures robust audio and visual alignment on the Spatial level; ii) proposing a pre-training technique for dynamic audio and visual alignment on Temporal level in a self-supervised setting, and iii) introducing a cross-attention mechanism to balance audio and visual information on Semantic level. The proposed novel CAD network improves the overall performance over the state-of-the-art methods on average by 9.4% on the MUSIC-AVQA dataset. We also demonstrate that our proposed contributions to AVQA can be added to the existing methods to improve their performance without additional complexity requirements",
    "checked": true,
    "id": "8daab8525c92b5c4e2655050890fcfdb5f153c1e",
    "semantic_title": "cad - contextual multi-modal alignment for dynamic avqa",
    "citation_count": 0,
    "authors": [
      "Asmar Nadeem",
      "Adrian Hilton",
      "Robert Dawes",
      "Graham Thomas",
      "Armin Mustafa"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Singh_Discriminator-Free_Unsupervised_Domain_Adaptation_for_Multi-Label_Image_Classification_WACV_2024_paper.html": {
    "title": "Discriminator-Free Unsupervised Domain Adaptation for Multi-Label Image Classification",
    "volume": "main",
    "abstract": "In this paper, a discriminator-free adversarial-based Unsupervised Domain Adaptation (UDA) for Multi-Label Image Classification (MLIC) referred to as DDA-MLIC is proposed. Recently, some attempts have been made for introducing adversarial-based UDA methods in the context of MLIC. However, these methods which rely on an additional discriminator subnet present one major shortcoming. The learning of domain-invariant features may harm their task-specific discriminative power, since the classification and discrimination tasks are decoupled. Herein, we propose to overcome this issue by introducing a novel adversarial critic that is directly deduced from the task-specific classifier. Specifically, a two-component Gaussian Mixture Model (GMM) is fitted on the source and target predictions in order to distinguish between two clusters. This allows extracting a Gaussian distribution for each component. The resulting Gaussian distributions are then used for formulating an adversarial loss based on a Frechet distance. The proposed method is evaluated on several multi-label image datasets covering three different types of domain shift. The obtained results demonstrate that DDA-MLIC outperforms existing state-of-the-art methods in terms of precision while requiring a lower number of parameters. The code is publicly available at github.com/cvi2snt/DDA-MLIC",
    "checked": true,
    "id": "af14f426ecbba6a38fee1dd3cabfc5d823ab53c2",
    "semantic_title": "discriminator-free unsupervised domain adaptation for multi-label image classification",
    "citation_count": 0,
    "authors": [
      "Inder Pal Singh",
      "Enjie Ghorbel",
      "Anis Kacem",
      "Arunkumar Rathinam",
      "Djamila Aouada"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wang_Continual_Test-Time_Domain_Adaptation_via_Dynamic_Sample_Selection_WACV_2024_paper.html": {
    "title": "Continual Test-Time Domain Adaptation via Dynamic Sample Selection",
    "volume": "main",
    "abstract": "The objective of Continual Test-time Domain Adaptation (CTDA) is to gradually adapt a pre-trained model to a sequence of target domains without accessing the source data. This paper proposes a Dynamic Sample Selection (DSS) method for CTDA. DSS consists of dynamic thresholding, positive learning, and negative learning processes. Traditionally, models learn from unlabeled unknown environment data and equally rely on all samples' pseudo-labels to update their parameters through self-training. However, noisy predictions exist in these pseudo-labels, so all samples are not equally trustworthy. Therefore, in our method, a dynamic thresholding module is first designed to select suspected low-quality from high-quality samples. The selected low-quality samples are more likely to be wrongly predicted. Therefore, we apply joint positive and negative learning on both high- and low-quality samples to reduce the risk of using wrong information. We conduct extensive experiments that demonstrate the effectiveness of our proposed method for CTDA in the image domain, outperforming the state-of-the-art results. Furthermore, our approach is also evaluated in the 3D point cloud domain, showcasing its versatility and potential for broader applicability",
    "checked": true,
    "id": "998eca275572b8c77df0665dc89222405548f83a",
    "semantic_title": "continual test-time domain adaptation via dynamic sample selection",
    "citation_count": 0,
    "authors": [
      "Yanshuo Wang",
      "Jie Hong",
      "Ali Cheraghian",
      "Shafin Rahman",
      "David Ahmedt-Aristizabal",
      "Lars Petersson",
      "Mehrtash Harandi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Rotstein_FuseCap_Leveraging_Large_Language_Models_for_Enriched_Fused_Image_Captions_WACV_2024_paper.html": {
    "title": "FuseCap: Leveraging Large Language Models for Enriched Fused Image Captions",
    "volume": "main",
    "abstract": "The advent of vision-language pre-training techniques enhanced substantial progress in the development of models for image captioning. However, these models frequently produce generic captions and may omit semantically important image details. This limitation can be traced back to the image-text datasets; while their captions typically offer a general description of image content, they frequently omit salient details. Considering the magnitude of these datasets, manual reannotation is impractical, emphasizing the need for an automated approach. To address this challenge, we leverage existing captions and explore augmenting them with visual details using \"frozen\" vision experts including an object detector, an attribute recognizer, and an Optical Character Recognizer (OCR). Our proposed method, FuseCap, fuses the outputs of such vision experts with the original captions using a large language model (LLM), yielding comprehensive image descriptions. We automatically curate a training set of 12M image-enriched caption pairs. These pairs undergo extensive evaluation through both quantitative and qualitative analyses. Subsequently, this data is utilized to train a captioning generation BLIP-based model. This model outperforms current state-of-the-art approaches, producing more precise and detailed descriptions, demonstrating the effectiveness of the proposed data-centric approach. We release this large-scale dataset of enriched image-caption pairs for the community",
    "checked": true,
    "id": "28870aedc2f1653e6d69f11fb792bc87537414cb",
    "semantic_title": "fusecap: leveraging large language models for enriched fused image captions",
    "citation_count": 2,
    "authors": [
      "Noam Rotstein",
      "David Bensaïd",
      "Shaked Brody",
      "Roy Ganz",
      "Ron Kimmel"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Hu_Learning_To_Adapt_CLIP_for_Few-Shot_Monocular_Depth_Estimation_WACV_2024_paper.html": {
    "title": "Learning To Adapt CLIP for Few-Shot Monocular Depth Estimation",
    "volume": "main",
    "abstract": "Pre-trained Visual-Language Models (VLMs), such as CLIP, have shown enhanced performance across a range of tasks that involve the integration of visual and linguistic elements. When CLIP is used for depth estimation tasks, the patches, divided from the input images, can be combined with a series of semantic descriptions of the depth information to obtain similarity results. The coarse estimation of depth is then achieved by weighting and summing the depth values, called depth bins, corresponding to the predefined semantic descriptions. The zero-shot approach circumvents the computational and time-intensive nature of traditional fully-supervised depth estimation methods. However, this method, utilizing fixed depth bins, may not effectively generalize as images from different scenes may exhibit distinct depth distributions. To address this challenge, we propose a few-shot-based method which learns to adapt the VLMs for monocular depth estimation to balance training costs and generalization capabilities. Specifically, it assigns different depth bins for different scenes, which can be selected by the model during inference. Additionally, we incorporate learnable prompts to preprocess the input text to convert the easily human-understood text into easily model-understood vectors and further enhance the performance. With only one image per scene for training, our extensive experiment results on the NYU V2 dataset demonstrate that our method outperforms the previous state-of-the-art method by up to 10.6% in terms of MARE",
    "checked": true,
    "id": "dd473ed4bca0c946557e8d29beeb6f06870bfa17",
    "semantic_title": "learning to adapt clip for few-shot monocular depth estimation",
    "citation_count": 1,
    "authors": [
      "Xueting Hu",
      "Ce Zhang",
      "Yi Zhang",
      "Bowen Hai",
      "Ke Yu",
      "Zhihai He"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Shoshan_Asymmetric_Image_Retrieval_With_Cross_Model_Compatible_Ensembles_WACV_2024_paper.html": {
    "title": "Asymmetric Image Retrieval With Cross Model Compatible Ensembles",
    "volume": "main",
    "abstract": "The asymmetrical retrieval setting is a well suited solution for resource constrained applications such as face recognition and image retrieval. In this setting, a large model is used for indexing the gallery while a lightweight model is used for querying. The key principle in such systems is ensuring that both models share the same embedding space. Most methods in this domain are based on knowledge distillation. While useful, they suffer from several drawbacks: they are upper-bounded by the performance of the single best model found and cannot be extended to use an ensemble of models in a straightforward manner. In this paper we present an approach that does not rely on knowledge distillation, rather it utilizes embedding transformation models. This allows the use of N independently trained and diverse gallery models (e.g., trained on different datasets or having a different architecture) and a single query model. As a result, we improve the overall accuracy beyond that of any single model while maintaining a low computational budget for querying. Additionally, we propose a gallery image rejection method that utilizes the diversity between multiple transformed embeddings to estimate the uncertainty of gallery images",
    "checked": true,
    "id": "032119cbd0dffbed60d4c5f7e1a114c498dd15b3",
    "semantic_title": "asymmetric image retrieval with cross model compatible ensembles",
    "citation_count": 0,
    "authors": [
      "Alon Shoshan",
      "Ori Linial",
      "Nadav Bhonker",
      "Elad Hirsch",
      "Lior Zamir",
      "Igor Kviatkovsky",
      "Gérard Medioni"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Liao_Progressive_Hypothesis_Transformer_for_3D_Human_Mesh_Recovery_WACV_2024_paper.html": {
    "title": "Progressive Hypothesis Transformer for 3D Human Mesh Recovery",
    "volume": "main",
    "abstract": "Recent advancements in Transformer-based human mesh reconstruction (HMR) are commendable. However, these models often lift 2D images directly to 3D vertices without explicit intermediate guidance. In addition, the global attention mechanism tends to spread attention across larger body areas and even unrelated background regions during human mesh estimation, rather than focusing on critical local regions such as human body joints. This tendency leads to inaccurate and unrealistic results for complex activities. To address these challenges, we introduce the Progressive Hypotheses Transformer, which employs 2D and 3D pose predictions to progressively guide our model. Moreover, we propose a mechanism that generates multiple plausible hypotheses for both 2D and 3D poses to mitigate potential inaccuracies arising from intermediate pose estimations. Our model also incorporates inter-intra attention to capture correlations between joints and hypotheses. Experimental results demonstrate that our method surpasses existing imagebased approaches on Human3.6M [13] and 3DPW [36] with fewer parameters and relatively lower computational costs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huang-Ru Liao",
      "Jen-Chun Lin",
      "Chun-Yi Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Lin_MPT_Mesh_Pre-Training_With_Transformers_for_Human_Pose_and_Mesh_WACV_2024_paper.html": {
    "title": "MPT: Mesh Pre-Training With Transformers for Human Pose and Mesh Reconstruction",
    "volume": "main",
    "abstract": "Traditional methods of reconstructing 3D human pose and mesh from single images rely on paired image-mesh datasets, which can be difficult and expensive to obtain. Due to this limitation, model scalability is constrained as well as reconstruction performance. Towards addressing the challenge, we introduce Mesh Pre-Training (MPT), an effective pre-training strategy that leverages large amounts of MoCap data to effectively perform pre-training at scale. We introduce the use of MoCap-generated heatmaps as input representations to the mesh regression transformer and propose a Masked Heatmap Modeling approach for improving pre-training performance. This study demonstrates that pre-training using the proposed MPT allows our models to perform effective inference without requiring fine-tuning. We further show that fine-tuning the pre-trained MPT model considerably improves the accuracy of human mesh reconstruction from single images. Experimental results show that MPT outperforms previous state-of-the-art methods on Human3.6M and 3DPW datasets. As a further application, we benchmark and study MPT on the task of 3D hand reconstruction, showing that our generic pre-training scheme generalizes well to hand pose estimation and achieves promising reconstruction performance",
    "checked": true,
    "id": "075a38f1e525e237f16f8971e36e39850dc159c8",
    "semantic_title": "mpt: mesh pre-training with transformers for human pose and mesh reconstruction",
    "citation_count": 5,
    "authors": [
      "Kevin Lin",
      "Chung-Ching Lin",
      "Lin Liang",
      "Zicheng Liu",
      "Lijuan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Jeong_Training-Free_Content_Injection_Using_H-Space_in_Diffusion_Models_WACV_2024_paper.html": {
    "title": "Training-Free Content Injection Using H-Space in Diffusion Models",
    "volume": "main",
    "abstract": "Diffusion models (DMs) synthesize high-quality images in various domains. However, controlling their generative process is still hazy because the intermediate variables in the process are not rigorously studied. Recently, the bottleneck feature of the U-Net, namely h-space, is found to convey the semantics of the resulting image. It enables StyleCLIP-like latent editing within DMs. In this paper, we explore further usage of h-space beyond attribute editing, and introduce a method to inject the content of one image into another image by combining their features in the generative processes. Briefly, given the original generative process of the other image, 1) we gradually blend the bottleneck feature of the content with proper normalization, and 2) we calibrate the skip connections to match the injected content. Unlike custom-diffusion approaches, our method does not require time-consuming optimization or fine-tuning. Instead, our method manipulates intermediate features within a feed-forward generative process. Furthermore, our method does not require supervision from external networks",
    "checked": true,
    "id": "6bdfd57293e318bd07d2d3b6929f4a57ccbfeb46",
    "semantic_title": "training-free content injection using h-space in diffusion models",
    "citation_count": 13,
    "authors": [
      "Jaeseok Jeong",
      "Mingi Kwon",
      "Youngjung Uh"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Lee_Hard_Sample-Aware_Consistency_for_Low-Resolution_Facial_Expression_Recognition_WACV_2024_paper.html": {
    "title": "Hard Sample-Aware Consistency for Low-Resolution Facial Expression Recognition",
    "volume": "main",
    "abstract": "Facial expression recognition (FER) plays a pivotal role in computer vision applications, encompassing video understanding and human-computer interaction. Despite notable advancements in FER, performance still falters when handling low-resolution facial images encountered in real-world scenarios and datasets. While consistency constraint techniques have garnered attention for generating robust convolutional neural network models that accommodate input variations through augmentation, their efficacy is diminished in the realm of low-resolution FER. This decline in performance can be attributed to augmented samples that networks struggle to extract expressive features. In this paper, we identify hard samples that cause an overfitting problem when considering various degrees of resolution and propose novel hard sample-aware consistency (HSAC) loss functions, which include combined attention consistency and label distribution learning. The combined attention consistency aligns an attention map from multi-scale low-resolution images with an appropriate target attention map by combining activation maps from high-resolution and flipped low-resolution images. We measure the classification difficulty for low-resolution face images and adaptively apply label distribution learning by combining the original target and predictions of high-resolution input. Our HSAC empowers the network to achieve generalization by effectively managing hard samples. Extensive experiments on various FER datasets demonstrate the superiority of our proposed method over existing approaches for multi-scale low-resolution images. Furthermore, we achieved a new state-of-the-art performance of 90.97% on the original RAF-DB dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bokyeung Lee",
      "Kyungdeuk Ko",
      "Jonghwan Hong",
      "Hanseok Ko"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Phan_ZEETAD_Adapting_Pretrained_Vision-Language_Model_for_Zero-Shot_End-to-End_Temporal_Action_WACV_2024_paper.html": {
    "title": "ZEETAD: Adapting Pretrained Vision-Language Model for Zero-Shot End-to-End Temporal Action Detection",
    "volume": "main",
    "abstract": "Temporal action detection (TAD) involves the localization and classification of action instances within untrimmed videos. While standard TAD follows fully supervised learning with closed-set setting on large training data, recent zero-shot TAD methods showcase the promising openset setting by leveraging large-scale contrastive visuallanguage (ViL) pretrained models. However, existing zeroshot TAD methods have limitations on how to properly construct the strong relationship between two Interdependent tasks of localization and classification and adapt ViL model to video understanding. In this work, we present ZEETAD, featuring two modules: dual-localization and zeroshot proposal classification. The former is a Transformerbased module that detects action events while selectively collecting crucial semantic embeddings for later Recognition. The latter one, CLIP-based module, generates semantic embeddings from text and frame inputs for each temporal unit. Additionally, we enhance discriminative capability on unseen classes by minimally updating the frozen CLIP encoder with lightweight adapters. Extensive experiments on THUMOS14 and ActivityNet-1.3 datasets demonstrate our approach's superior performance in zero-shot TAD and effective knowledge transfer from ViL models to unseen action categories. Code is available at https: //github.com/UARK-AICV/ZEETAD",
    "checked": true,
    "id": "01fec258f2d20744f0bfde4d0560f2eeccad0fc9",
    "semantic_title": "zeetad: adapting pretrained vision-language model for zero-shot end-to-end temporal action detection",
    "citation_count": 0,
    "authors": [
      "Thinh Phan",
      "Khoa Vo",
      "Duy Le",
      "Gianfranco Doretto",
      "Donald Adjeroh",
      "Ngan Le"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Jindal_Army_of_Thieves_Enhancing_Black-Box_Model_Extraction_via_Ensemble_Based_WACV_2024_paper.html": {
    "title": "Army of Thieves: Enhancing Black-Box Model Extraction via Ensemble Based Sample Selection",
    "volume": "main",
    "abstract": "Machine Learning (ML) models become vulnerable to Model Stealing Attacks (MSA) when they are deployed as a service. In such attacks, the deployed model is queried repeatedly to build a labelled dataset. This dataset allows the attacker to train a thief model that mimics the original model. To maximize query efficiency, the attacker has to select the most informative subset of data points from the pool of available data. Existing attack strategies utilize approaches like Active Learning and Semi-Supervised learning to minimize costs. However, in the black-box setting, these approaches may select sub-optimal samples as they train only one thief model. Depending on the thief model's capacity and the data it was pretrained on, the model might even select noisy samples that harm the learning process. In this work, we explore the usage of an ensemble of deep learning models as our thief model. We call our attack Army of Thieves(AOT) as we train multiple models with varying complexities to leverage the crowd's wisdom. Based on the ensemble's collective decision, uncertain samples are selected for querying, while the most confident samples are directly included in the training data. Our approach is the first one to utilize an ensemble of thief models to perform model extraction. We outperform the base approaches of existing state-of-the-art methods by at least 3% and achieve a 21% higher adversarial sample transferability than previous work for models trained on the CIFAR-10 dataset",
    "checked": true,
    "id": "3f7a77e8437be83b4cb2810f1de0a6e6b45fe6d7",
    "semantic_title": "army of thieves: enhancing black-box model extraction via ensemble based sample selection",
    "citation_count": 0,
    "authors": [
      "Akshit Jindal",
      "Vikram Goyal",
      "Saket Anand",
      "Chetan Arora"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Vats_GC-MVSNet_Multi-View_Multi-Scale_Geometrically-Consistent_Multi-View_Stereo_WACV_2024_paper.html": {
    "title": "GC-MVSNet: Multi-View, Multi-Scale, Geometrically-Consistent Multi-View Stereo",
    "volume": "main",
    "abstract": "Traditional multi-view stereo (MVS) methods rely heavily on photometric and geometric consistency constraints, but newer machine learning-based MVS methods check geometric consistency across multiple source views only as a post-processing step. In this paper, we present a novel approach that explicitly encourages geometric consistency of reference view depth maps across multiple source views at different scales during learning (see Fig. 1). We find that adding this geometric consistency loss significantly accelerates learning by explicitly penalizing geometrically inconsistent pixels, reducing the training iteration requirements to nearly half that of other MVS methods. Our extensive experiments show that our approach achieves a new state-of-the-art on the DTU and BlendedMVS datasets, and competitive results on the Tanks and Temples benchmark. To the best of our knowledge, GC-MVSNet is the first attempt to enforce multi-view, multi-scale geometric consistency during learning",
    "checked": true,
    "id": "d08a33018b45c7337ff36bc0592a95bf8b44dce3",
    "semantic_title": "gc-mvsnet: multi-view, multi-scale, geometrically-consistent multi-view stereo",
    "citation_count": 0,
    "authors": [
      "Vibhas K. Vats",
      "Sripad Joshi",
      "David J. Crandall",
      "Md. Alimoor Reza",
      "Soon-heung Jung"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Goswami_Active_Batch_Sampling_for_Multi-Label_Classification_With_Binary_User_Feedback_WACV_2024_paper.html": {
    "title": "Active Batch Sampling for Multi-Label Classification With Binary User Feedback",
    "volume": "main",
    "abstract": "Multi-label classification is a generalization of multi-class classification, where a single data sample can have multiple labels. While deep neural networks have depicted commendable performance for multi-label learning, they require a large amount of manually annotated training data to attain good generalization capability. However, annotating a multi-label data sample requires a human oracle to consider the presence/absence of every single class individually, which is extremely laborious. Active learning algorithms automatically identify the salient and exemplar instances from large amounts of unlabeled data and are effective in reducing human annotation effort in inducing a machine learning model. In this paper, we propose a novel active learning framework for multi-label learning, which queries a batch of (image-label) pairs and for each pair, poses the question whether the queried label is present in the corresponding image; the human annotators merely need to provide a binary feedback (yes / no) in response to each query, which involves much less manual work. We pose the image and label selection as a constrained optimization problem and derive a linear programming relaxation to select a batch of (image-label) pairs, which are maximally informative to the underlying deep neural network. Our extensive empirical studies on three challenging datasets corroborate the potential of our method for real-world multi-label classification applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Debanjan Goswami",
      "Shayok Chakraborty"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Han_Efficient_MAE_Towards_Large-Scale_Vision_Transformers_WACV_2024_paper.html": {
    "title": "Efficient MAE Towards Large-Scale Vision Transformers",
    "volume": "main",
    "abstract": "Masked Autoencoder (MAE) has demonstrated superb pre-training efficiency for vision Transformer, thanks to its partial input paradigm and high mask ratio (0.75). However, MAE often suffers from severe performance drop under higher mask ratios, which hinders its potential toward larger-scale vision Transformers. In this work, we identify that the performance drop is largely attributed to the over-dominance of difficult reconstruction targets, as higher mask ratios lead to more sparse visible patches and fewer visual clues for reconstruction. To mitigate this issue, we design Efficient MAE that introduces a novel Difficulty-Flatten Loss and a decoder masking strategy, enabling a higher mask ratio for more efficient pre-training. The Difficulty-Flatten Loss provides balanced supervision on reconstruction targets of different difficulties, mitigating the performance drop under higher mask ratios effectively. Additionally, the decoder masking strategy discards the most difficult reconstruction targets, which further alleviates the optimization difficulty and accelerates the pre-training clearly. Our proposed Efficient MAE introduces 27% and 30% pre-training runtime accelerations for the ViT-Large and ViT-Huge models, provides valuable insights into MAE's optimization, and paves the way for larger-scale vision Transformer pre-training. Code and pre-trained models will be released",
    "checked": false,
    "id": "526aec044f43f5bfbba5f3307f08bdfe0dd14750",
    "semantic_title": "parameter-efficient fine-tuning for vision transformers",
    "citation_count": 30,
    "authors": [
      "Qiu Han",
      "Gongjie Zhang",
      "Jiaxing Huang",
      "Peng Gao",
      "Zhang Wei",
      "Shijian Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Jamal_M33D_Learning_3D_Priors_Using_Multi-Modal_Masked_Autoencoders_for_2D_WACV_2024_paper.html": {
    "title": "M33D: Learning 3D Priors Using Multi-Modal Masked Autoencoders for 2D Image and Video Understanding",
    "volume": "main",
    "abstract": "We present a new pre-training strategy called M^ 3 3D (Multi-Modal Masked 3D) built based on Multi-modal masked autoencoders that can leverage 3D priors and learned cross-modal representations in RGB-D data. We integrate two major self-supervised learning frameworks; Masked Image Modeling (MIM) and contrastive learning; aiming to effectively embed masked 3D priors and modality complementary features to enhance the correspondence between modalities. In contrast to recent approaches which are either focusing on specific downstream tasks or require multi-view correspondence, we show that our pre-training strategy is ubiquitous, enabling improved representation learning that can transfer into improved performance on various downstream tasks such as video action recognition, video action detection, 2D semantic segmentation and depth estimation. Experiments show that M^ 3 3D outperforms the existing state-of-the-art approaches on ScanNet, NYUv2, UCF-101 and OR-AR, particularly with an improvement of +1.3% mIoU against Mask3D on ScanNet semantic segmentation. We further evaluate our method on low-data regime and demonstrate its superior data efficiency compared to current state-of-the-art approaches",
    "checked": true,
    "id": "d1ad9705b5eb345eb391ece02c815cc5a38f971a",
    "semantic_title": "m33d: learning 3d priors using multi-modal masked autoencoders for 2d image and video understanding",
    "citation_count": 0,
    "authors": [
      "Muhammad Abdullah Jamal",
      "Omid Mohareri"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Thakur_GraphGraph_A_Nested_Graph-Based_Framework_for_Early_Accident_Anticipation_WACV_2024_paper.html": {
    "title": "Graph(Graph): A Nested Graph-Based Framework for Early Accident Anticipation",
    "volume": "main",
    "abstract": "Anticipating traffic accidents early using dashcam videos is an important task for ensuring road safety and building reliable intelligent autonomous vehicles. However, factors like high traffic on the roads, different types of accidents, limited angles of vision, etc. make this task very challenging. Using the early frames, a lot of existing methods predict a large number of false positives which poses a huge risk for all vehicles on the road. In this paper, we propose a novel end-to-end learning, nested graph-based framework named Graph(Graph) for early accident anticipation. It uses interactions between the objects in the same as well as the neighboring frames along with the global features to make precise predictions as early as possible. This way it is able to embed the local as well as global temporal information into the extracted features. Graph(Graph) outperforms state-of-the-art methods on different datasets by a large margin demonstrating its effectiveness. With empirical evidence, we highlight the importance of each component in Graph(Graph) and show their effect on the final performance. Our code is available at https://github.com/thakurnupur/Graph-Graph",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nupur Thakur",
      "PrasanthSai Gouripeddi",
      "Baoxin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Joseph_Iterative_Multi-Granular_Image_Editing_Using_Diffusion_Models_WACV_2024_paper.html": {
    "title": "Iterative Multi-Granular Image Editing Using Diffusion Models",
    "volume": "main",
    "abstract": "Recent advances in text-guided image synthesis has dramatically changed how creative professionals generate artistic and aesthetically pleasing visual assets. To fully support such creative endeavors, the process should possess the ability to: 1) iteratively edit the generations and 2) control the spatial reach of desired changes (global, local or anything in between). We formalize this pragmatic problem setting as Iterative Multi-granular Editing. While there has been substantial progress with diffusion-based models for image synthesis and editing, they are all one shot (i.e., no iterative editing capabilities) and do not naturally yield multi-granular control (i.e., covering the full spectrum of local-to-global edits). To overcome these drawbacks, we propose EMILIE: Iterative Multi-granular Image Editor. EMILIE introduces a novel latent iteration strategy, which re-purposes a pre-trained diffusion model to facilitate iterative editing. This is complemented by a gradient control operation for multi-granular control. We introduce a new benchmark dataset to evaluate our newly proposed setting. We conduct exhaustive quantitatively and qualitatively evaluation against recent state-of-the-art approaches adapted to our task, to being out the mettle of EMILIE. We hope our work would attract attention to this newly identified, pragmatic problem setting",
    "checked": true,
    "id": "f9d6afa4c38981307e7579d3469ad81147b222e4",
    "semantic_title": "iterative multi-granular image editing using diffusion models",
    "citation_count": 3,
    "authors": [
      "K. J. Joseph",
      "Prateksha Udhayanan",
      "Tripti Shukla",
      "Aishwarya Agarwal",
      "Srikrishna Karanam",
      "Koustava Goswami",
      "Balaji Vasan Srinivasan"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Liu_Efficient_Feature_Distillation_for_Zero-Shot_Annotation_Object_Detection_WACV_2024_paper.html": {
    "title": "Efficient Feature Distillation for Zero-Shot Annotation Object Detection",
    "volume": "main",
    "abstract": "We propose a new setting for detecting unseen objects called Zero-shot Annotation object Detection (ZAD). It expands the zero-shot object detection setting by allowing the novel objects to exist in the training images and restricts the additional information the detector uses to novel category names. Recently, to detect unseen objects, largescale vision-language models (e.g., CLIP) are leveraged by different methods. The distillation-based methods have good overall performance but suffer from a long training schedule caused by two factors. First, existing work creates distillation regions biased to the base categories, which limits the distillation of novel category information. Second, directly using the raw feature from CLIP for distillation neglects the domain gap between the training data of CLIP and the detection datasets, which makes it difficult to learn the mapping from the image region to the vision-language feature space. To solve these problems, we propose Efficient feature distillation for Zero-shot Annotation object Detection (EZAD). Firstly, EZAD adapts the CLIP's feature space to the target detection domain by re-normalizing CLIP; Secondly, EZAD uses CLIP to generate distillation proposals with potential novel category names to avoid the distillation being overly biased toward the base categories. Finally, EZAD takes advantage of semantic meaning for regression to further improve the model performance. As a result, EZAD outperforms the previous distillation-based methods in COCO by 4% with a much shorter training schedule and achieves a 3% improvement on the LVIS dataset. Our code is available at https://github.com/dragonlzm/EZAD",
    "checked": true,
    "id": "4a9bd207a26aa2b18d15f72e252852eeb326db72",
    "semantic_title": "efficient feature distillation for zero-shot annotation object detection",
    "citation_count": 0,
    "authors": [
      "Zhuoming Liu",
      "Xuefeng Hu",
      "Ram Nevatia"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Xu_SpectralCLIP_Preventing_Artifacts_in_Text-Guided_Style_Transfer_From_a_Spectral_WACV_2024_paper.html": {
    "title": "SpectralCLIP: Preventing Artifacts in Text-Guided Style Transfer From a Spectral Perspective",
    "volume": "main",
    "abstract": "Owing to the power of vision-language foundation models, e.g., CLIP, the area of image synthesis has seen recent important advances. Particularly, for style transfer, CLIP enables transferring more general and abstract styles without collecting the style images in advance, as the style can be efficiently described with natural language, and the result is optimized by minimizing the CLIP similarity between the text description and the stylized image. However, directly using CLIP to guide style transfer leads to undesirable artifacts (mainly written words and unrelated visual entities) spread over the image. In this paper, we propose SpectralCLIP, which is based on a spectral representation of the CLIP embedding sequence, where most of the common artifacts occupy specific frequencies. By masking the band including these frequencies, we can condition the generation process to adhere to the target style properties (e.g., color, texture, paint stroke, etc.) while excluding the generation of larger-scale structures corresponding to the artifacts. Experimental results show that SpectralCLIP prevents the generation of artifacts effectively in quantitative and qualitative terms, without impairing the stylisation quality. We also apply SpectralCLIP to text-conditioned image generation and show that it prevents written words in the generated images. Our code is available at https://github.com/zipengxuc/SpectralCLIP",
    "checked": true,
    "id": "7b9590440e15c21c9638c78499c643284a6d19b8",
    "semantic_title": "spectralclip: preventing artifacts in text-guided style transfer from a spectral perspective",
    "citation_count": 0,
    "authors": [
      "Zipeng Xu",
      "Songlong Xing",
      "Enver Sangineto",
      "Nicu Sebe"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Das_Harnessing_the_Power_of_Multi-Lingual_Datasets_for_Pre-Training_Towards_Enhancing_WACV_2024_paper.html": {
    "title": "Harnessing the Power of Multi-Lingual Datasets for Pre-Training: Towards Enhancing Text Spotting Performance",
    "volume": "main",
    "abstract": "The adaptation capability to a wide range of domains is crucial for scene text spotting models when deployed to real-world conditions. However, existing state-of-the-art approaches usually incorporate scene text detection and recognition simply by pretraining on natural scene image datasets, which do not directly exploit the feature interaction between multiple domains. In this work, we investigate the problem of domain-adapted scene text spotting, i.e., training a model on multi-domain source data such that it can directly adapt to target domains rather than being specialized for a specific domain or scenario. Further, we investigate a transformer baseline called Swin-TESTR to focus on solving scene-text spotting for both regular (ICDAR2015) and arbitrary-shaped scene text (CTW1500, TotalText) along with an exhaustive evaluation. The results clearly demonstrate the potential of intermediate representations on text spotting benchmarks across multiple domains (e.g. language, synth to real, and documents) both in terms of accuracy and model efficiency",
    "checked": true,
    "id": "35d408016c91ac101f77923f68ea0f222aa85e02",
    "semantic_title": "harnessing the power of multi-lingual datasets for pre-training: towards enhancing text spotting performance",
    "citation_count": 0,
    "authors": [
      "Alloy Das",
      "Sanket Biswas",
      "Ayan Banerjee",
      "Josep Lladós",
      "Umapada Pal",
      "Saumik Bhattacharya"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Xu_Rethink_Cross-Modal_Fusion_in_Weakly-Supervised_Audio-Visual_Video_Parsing_WACV_2024_paper.html": {
    "title": "Rethink Cross-Modal Fusion in Weakly-Supervised Audio-Visual Video Parsing",
    "volume": "main",
    "abstract": "Existing works on weakly-supervised audio-visual video parsing adopt hybrid attention network (HAN) as the multi-modal embedding to capture the cross-modal context. It embeds the audio and visual modalities with a shared network, where the cross-attention is performed at the input. However, such an early fusion method highly entangles the two non-fully correlated modalities and leads to sub-optimal performance in detecting single-modality events. To deal with this problem, we propose the messenger-guided mid-fusion transformer to reduce the uncorrelated cross-modal context in the fusion. The messengers condense the full cross-modal context into a compact representation to only preserve useful cross-modal information. Furthermore, due to the fact that microphones capture audio events from all directions, while cameras only record visual events within a restricted field of view, there is a more frequent occurrence of unaligned cross-modal context from audio streams for visual event predictions. We thus propose cross-audio prediction consistency to suppress the impact of irrelevant audio information on visual event prediction. Experiments consistently illustrate the superior performance of our framework compared to existing state-of-the-art methods",
    "checked": true,
    "id": "1e8f72969dc7189bd01e7f84356798baac0aff11",
    "semantic_title": "rethink cross-modal fusion in weakly-supervised audio-visual video parsing",
    "citation_count": 0,
    "authors": [
      "Yating Xu",
      "Conghui Hu",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zang_Refine_and_Redistribute_Multi-Domain_Fusion_and_Dynamic_Label_Assignment_for_WACV_2024_paper.html": {
    "title": "Refine and Redistribute: Multi-Domain Fusion and Dynamic Label Assignment for Unbiased Scene Graph Generation",
    "volume": "main",
    "abstract": "Scene Graph Generation (SGG) plays an important role in enhancing visual image comprehension. However, existing approaches often struggle to represent implicit relationship features, resulting in a limited ability to distinguish predicates. Meanwhile, they are vulnerable to skewed instance distributions, which impairs effective training for fine-grained predicates. To address these problems, we propose a novel feature refinement and data redistribution framework (RAR). Specifically, a multi-domain fusion (MDF) module is designed to acquire comprehensive predicate representations, integrating global knowledge from the contextual domain and local details in the spatial-frequency domains. Then, we introduce a dynamic label assignment (DLA) strategy to tackle the long-tailed problem. Different predicate categories are adaptively grouped, accommodating varying training conditions. Guided by this strategy, we leverage a hierarchical auto-encoder to generate siamese samples, expanding the label cardinality. Furthermore, we explore the updated sample space to derive reliable samples and assign tailored labels, ultimately achieving the data rebalancing. Experiments on VG and GQA demonstrate that our model contributes to correcting prediction bias and achieves a significant improvement of approximately 10% in mean recall compared to baseline models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujie Zang",
      "Yaochen Li",
      "Yuan Gao",
      "Yimou Guo",
      "Wenneng Tang",
      "Yanxue Li",
      "Meklit Atlaw"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Semantic_Transfer_From_Head_to_Tail_Enlarging_Tail_Margin_for_WACV_2024_paper.html": {
    "title": "Semantic Transfer From Head to Tail: Enlarging Tail Margin for Long-Tailed Visual Recognition",
    "volume": "main",
    "abstract": "Deep neural networks excel in visual recognition tasks,but their success hinges on access to balanced datasets. Yet, real-world datasets often exhibit a long-tailed distribution, compromising network efficiency and hampering generalization on unseen data. To enhance the model's generalization in long-tailed scenarios, we present a novel feature augmentation approach termed SeMAntic tRansfer from head to Tail (SMART), which enriches the feature patterns for tail samples by transferring semantic covariance from the head classes to the tail classes along semantically correlating dimensions. This strategy boosts the model's generalization ability by implicitly and adaptively weighting the logits, thereby widening the classification margin of tail classes. Inspired by the success of this weighting, we further incorporate a semantic-aware weighting strategy for the loss tied to tail samples. This amplifies the effect of enlarging the margin for tail classes. We are the first to provide theoretical analysis that demonstrates a large semantic diversity in tail samples can increase class margins during the training stage, leading to improved generalization. Empirical observations support our theory. Notably, with no need for extra data or learnable parameters, SMART achieves state-of-the-art results on five long-tailed benchmark datasets: CIFAR-10/100-LT, Places-LT, ImageNet-LT, and iNaturalist 2018",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shan Zhang",
      "Yao Ni",
      "Jinhao Du",
      "Yanxia Liu",
      "Piotr Koniusz"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Lee_PoseDiff_Pose-Conditioned_Multimodal_Diffusion_Model_for_Unbounded_Scene_Synthesis_From_WACV_2024_paper.html": {
    "title": "PoseDiff: Pose-Conditioned Multimodal Diffusion Model for Unbounded Scene Synthesis From Sparse Inputs",
    "volume": "main",
    "abstract": "Novel view synthesis has been heavily driven by NeRF-based models, but these models often hold limitations with the requirement of dense coverage of input views and expensive computations. NeRF models designed for scenarios with a few sparse input views face difficulty in being generalizable to complex or unbounded scenes, where multiple scene content can be at any distance from a multi-directional camera, and thus generate unnatural and low quality images with blurry or floating artifacts. To accommodate the lack of dense information in sparse view scenarios and the computational burden of NeRF-based models in novel view synthesis, our approach adopts diffusion models. In this paper, we present PoseDiff, which combines the fast and plausible generation ability of diffusion models and 3D-aware view consistency of pose parameters from NeRF-based models. Specifically, PoseDiff is a multimodal pose-conditioned diffusion model applicable for novel view synthesis of unbounded scenes as well as bounded or forward-facing scenes with sparse views. PoseDiff renders plausible novel views for given pose parameters while maintaining high-frequency geometric details in significantly less time than conventional NeRF-based methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seoyoung Lee",
      "Joonseok Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Sadhu_Leveraging_Task-Specific_Pre-Training_To_Reason_Across_Images_and_Videos_WACV_2024_paper.html": {
    "title": "Leveraging Task-Specific Pre-Training To Reason Across Images and Videos",
    "volume": "main",
    "abstract": "We explore the Reasoning Across Images and Video (RAIV) task, which requires models to reason on a pair of visual inputs comprising various combinations of images and/or videos. Previous work in this area has been limited to image pairs focusing primarily on the existence and/or cardinality of objects. To address this, we leverage existing datasets with rich annotations to generate semantically meaningful queries about actions, objects, and their relationships. We introduce new datasets that encompass visually similar inputs, reasoning over images, across images and videos, or across videos. Recognizing the distinct nature of RAIV compared to existing pre-training objectives which work on single image-text pairs, we explore task-specific pre-training, wherein a pre-trained model is trained on an objective similar to downstream tasks without utilizing fine-tuning datasets. Experiments with several state-of-the-art pre-trained image-language models reveal that task-specific pre-training significantly enhances performance on downstream datasets, even in the absence of additional pre-training data. We provide further ablative studies to guide future work",
    "checked": false,
    "id": "474043deeca83742be2c6ac7d36d279b90080c36",
    "semantic_title": "unifying tracking and image-video object detection",
    "citation_count": 0,
    "authors": [
      "Arka Sadhu",
      "Ram Nevatia"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Rodriguez_Recognition_of_Unseen_Bird_Species_by_Learning_From_Field_Guides_WACV_2024_paper.html": {
    "title": "Recognition of Unseen Bird Species by Learning From Field Guides",
    "volume": "main",
    "abstract": "We exploit field guides to learn bird species recognition, in particular zero-shot recognition of unseen species. Illustrations contained in field guides deliberately focus on discriminative properties of each species, and can serve as side information to transfer knowledge from seen to unseen bird species. We study two approaches: (1) a contrastive encoding of illustrations, which can be fed into standard zero-shot learning schemes; and (2) a novel method that leverages the fact that illustrations are also images and as such structurally more similar to photographs than other kinds of side information. Our results show that illustrations from field guides, which are readily available for a wide range of species, are indeed a competitive source of side information for zero-shot learning. On a subset of the iNaturalist2021 dataset with 749 seen and 739 unseen species, we obtain a classification accuracy of unseen bird species of 12% @top-1 and 38% @top-10, which shows the potential of field guides for challenging real-world scenarios with many species. Our code is available at https://github.com/ac-rodriguez/zsl_billow",
    "checked": true,
    "id": "b173041c6df7a3fa491a42d89a33b459564ec1d7",
    "semantic_title": "recognition of unseen bird species by learning from field guides",
    "citation_count": 0,
    "authors": [
      "Andrés C. Rodríguez",
      "Stefano D'Aronco",
      "Rodrigo Caye Daudt",
      "Jan D. Wegner",
      "Konrad Schindler"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Hess_LidarCLIP_or_How_I_Learned_To_Talk_to_Point_Clouds_WACV_2024_paper.html": {
    "title": "LidarCLIP or: How I Learned To Talk to Point Clouds",
    "volume": "main",
    "abstract": "Research connecting text and images has recently seen several breakthroughs, with models like CLIP, DALL*E 2, and Stable Diffusion. However, the connection between text and other visual modalities, such as lidar data, has received less attention, prohibited by the lack of text-lidar datasets. In this work, we propose LidarCLIP, a mapping from automotive point clouds to a pre-existing CLIP embedding space. Using image-lidar pairs, we supervise a point cloud encoder with the image CLIP embeddings, effectively relating text and lidar data with the image domain as an intermediary. We show the effectiveness of LidarCLIP by demonstrating that lidar-based retrieval is generally on par with image-based retrieval, but with complementary strengths and weaknesses. By combining image and lidar features, we improve upon both single-modality methods and enable a targeted search for challenging detection scenarios under adverse sensor conditions. We also explore zero-shot classification and show that LidarCLIP outperforms existing attempts to use CLIP for point clouds by a large margin. Finally, we leverage our compatibility with CLIP to explore a range of applications, such as point cloud captioning and lidar-to-image generation, without any additional training. Code and pre-trained models at https://github.com/atonderski/lidarclip",
    "checked": true,
    "id": "791afcb97eb31f25d0899c3e6de761168909c8e4",
    "semantic_title": "lidarclip or: how i learned to talk to point clouds",
    "citation_count": 8,
    "authors": [
      "Georg Hess",
      "Adam Tonderski",
      "Christoffer Petersson",
      "Kalle Åström",
      "Lennart Svensson"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Sahin_Enhancing_Multimodal_Compositional_Reasoning_of_Visual_Language_Models_With_Generative_WACV_2024_paper.html": {
    "title": "Enhancing Multimodal Compositional Reasoning of Visual Language Models With Generative Negative Mining",
    "volume": "main",
    "abstract": "Contemporary large-scale visual language models (VLMs) exhibit strong representation capacities, making them ubiquitous for enhancing the image and text understanding tasks. They are often trained in a contrastive manner on a large and diverse corpus of images and corresponding text captions scraped from the internet. Despite this, VLMs often struggle with compositional reasoning tasks which require a fine-grained understanding of the complex interactions of objects and their attributes. This failure can be attributed to two main factors: 1) Contrastive approaches have traditionally focused on mining negative examples from existing datasets. However, the mined negative examples might not be difficult for the model to discriminate from the positive. An alternative to mining would be negative sample generation 2) But existing generative approaches primarily focus on generating hard negative texts associated with a given image. Mining in the other direction, i.e., generating negative image samples associated with a given text has been ignored. To overcome both these limitations, we propose a framework that not only mines in both directions but also generates challenging negative samples in both modalities, i.e., images and texts. Leveraging these generative hard negative samples, we significantly enhance VLMs' performance in tasks involving multimodal compositional reasoning. Our code and dataset are released at https://ugorsahin.github.io/enhancing-multimodal-compositional-reasoning-of-vlm.html",
    "checked": true,
    "id": "034b8304e57d154cfcd3224f6b5705bb8157033a",
    "semantic_title": "enhancing multimodal compositional reasoning of visual language models with generative negative mining",
    "citation_count": 0,
    "authors": [
      "Ugur Sahin",
      "Hang Li",
      "Qadeer Khan",
      "Daniel Cremers",
      "Volker Tresp"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Sung-Bin_LaughTalk_Expressive_3D_Talking_Head_Generation_With_Laughter_WACV_2024_paper.html": {
    "title": "LaughTalk: Expressive 3D Talking Head Generation With Laughter",
    "volume": "main",
    "abstract": "Laughter is a unique expression, essential to affirmative social interactions of humans. Although current 3D talking head generation methods produce convincing verbal articulations, they often fail to capture the vitality and subtleties of laughter and smiles despite their importance in social context. In this paper, we introduce a novel task to generate 3D talking heads capable of both articulate speech and authentic laughter. Our newly curated dataset comprises 2D laughing videos paired with pseudo-annotated and human-validated 3D FLAME parameters and vertices. Given our proposed dataset, we present a strong baseline with a two-stage training scheme: the model first learns to talk and then acquires the ability to express laughter. Extensive experiments demonstrate that our method performs favorably compared to existing approaches in both talking head generation and expressing laughter signals. We further explore potential applications on top of our proposed method for rigging realistic avatars",
    "checked": true,
    "id": "468afb2e981a55f6ab22412193f711a11b98a63c",
    "semantic_title": "laughtalk: expressive 3d talking head generation with laughter",
    "citation_count": 3,
    "authors": [
      "Kim Sung-Bin",
      "Lee Hyun",
      "Da Hye Hong",
      "Suekyeong Nam",
      "Janghoon Ju",
      "Tae-Hyun Oh"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Rosskamp_Effects_of_Markers_in_Training_Datasets_on_the_Accuracy_of_WACV_2024_paper.html": {
    "title": "Effects of Markers in Training Datasets on the Accuracy of 6D Pose Estimation",
    "volume": "main",
    "abstract": "Collecting training data for pose estimation methods on images is a time-consuming task and usually involves some kind of manual labeling of the 6D pose of objects. This time could be reduced considerably by using marker-based tracking that would allow for automatic labeling of training images. However, images containing markers may reduce the accuracy of pose estimation due to a bias introduced by the markers. In this paper, we analyze the influence of markers in training images on pose estimation accuracy. We investigate the accuracy of estimated poses for three different cases: i) training on images with markers, ii) removing markers by inpainting, and iii) augmenting the dataset with randomly generated markers to reduce spatial learning of marker features. Our results demonstrate that utilizing marker-based techniques is an effective strategy for collecting large amounts of ground truth data for pose prediction. Moreover, our findings suggest that the usage of inpainting techniques do not reduce prediction accuracy. Additionally, we investigate the effect of inaccuracies of labeling in training data on prediction accuracy. We show that the precise ground truth data obtained through marker tracking proves to be superior compared to markerless datasets if labeling errors of 6D ground truth exist. Our data generation tools are available online: https://github.com/JHRosskamp/6DPoseDataGenTools",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Janis Rosskamp",
      "Rene Weller",
      "Gabriel Zachmann"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Alleviating_Foreground_Sparsity_for_Semi-Supervised_Monocular_3D_Object_Detection_WACV_2024_paper.html": {
    "title": "Alleviating Foreground Sparsity for Semi-Supervised Monocular 3D Object Detection",
    "volume": "main",
    "abstract": "Monocular 3D object detection (M3OD) is a significant yet inherently challenging task in autonomous driving due to absence of explicit depth cues in a single RGB image. In this paper, we strive to boost currently underperforming monocular 3D object detectors by leveraging an abundance of unlabelled data via semi-supervised learning. Our proposed ODM3D framework entails cross-modal knowledge distillation at various levels to inject LiDAR-domain knowledge into a monocular detector during training. By identifying object sparsity as the main culprit behind existing methods' suboptimal training, we exploit the precise localisation information embedded in LiDAR points to enable more foreground-attentive and efficient distillation via the proposed BEV occupancy guidance mask, leading to notably improved knowledge transfer and M3OD performance. Besides, motivated by insights into why existing cross-modal GT-sampling techniques fail on our task at hand, we further design a novel cross-modal object-wise data augmentation strategy for effective RGB-LiDAR joint learning. Our method ranks 1st in both KITTI validation and test benchmarks, significantly surpassing all existing monocular methods, supervised or semi-supervised, on both BEV and 3D detection metrics",
    "checked": false,
    "id": "f4159a4c13919f0cf66e0a9ae7ed767289357fe0",
    "semantic_title": "odm3d: alleviating foreground sparsity for semi-supervised monocular 3d object detection",
    "citation_count": 0,
    "authors": [
      "Weijia Zhang",
      "Dongnan Liu",
      "Chao Ma",
      "Weidong Cai"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Neoral_MFT_Long-Term_Tracking_of_Every_Pixel_WACV_2024_paper.html": {
    "title": "MFT: Long-Term Tracking of Every Pixel",
    "volume": "main",
    "abstract": "We propose MFT -- Multi-Flow dense Tracker -- a novel method for dense, pixel-level, long-term tracking. The approach exploits optical flows estimated not only between consecutive frames, but also for pairs of frames at logarithmically spaced intervals. It selects the most reliable sequence of flows on the basis of estimates of its geometric accuracy and the probability of occlusion, both provided by a pre-trained CNN. We show that MFT achieves competitive performance on the TAP-Vid benchmark, outperforming baselines by a significant margin, and tracking densely orders of magnitude faster than the state-of-the-art point-tracking methods. The method is insensitive to medium-length occlusions and it is robustified by estimating flow with respect to the reference frame, which reduces drift",
    "checked": true,
    "id": "19d6f6763c27c5e8847dad33ef366d509b3057dd",
    "semantic_title": "mft: long-term tracking of every pixel",
    "citation_count": 3,
    "authors": [
      "Michal Neoral",
      "Jonáš Šerých",
      "Jiří Matas"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kirchheim_Out-of-Distribution_Detection_With_Logical_Reasoning_WACV_2024_paper.html": {
    "title": "Out-of-Distribution Detection With Logical Reasoning",
    "volume": "main",
    "abstract": "Machine Learning models often only generalize reliably to samples from the training distribution. Consequentially, detecting when input data is out-of-distribution (OOD) is crucial, especially in safety-critical applications. Current OOD detection methods, however, tend to be domain agnostic and often fail to incorporate valuable prior knowledge about the structure of the training distribution. To address this limitation, we introduce a novel, hybrid OOD detection algorithm that combines a deep learning-based perception system with a first-order logic-based knowledge representation. A logical reasoning system uses this knowledge base at run-time to infer whether inputs are consistent with prior knowledge about the training distribution. In contrast to purely neural systems, the structured knowledge representation allows humans to inspect and modify the rules that govern the OOD detectors' behavior. This not only enhances performance but also fosters a level of explainability that is particularly beneficial in safety-critical contexts. We demonstrate the effectiveness of our method through experiments on several datasets and discuss advantages and limitations. Our code is available online",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Konstantin Kirchheim",
      "Tim Gonschorek",
      "Frank Ortmeier"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhang_WalkFormer_Point_Cloud_Completion_via_Guided_Walks_WACV_2024_paper.html": {
    "title": "WalkFormer: Point Cloud Completion via Guided Walks",
    "volume": "main",
    "abstract": "Point clouds are often sparse and incomplete in real-world scenarios. The prevailing methods for point cloud completion typically rely on encoding the partial points and then decoding complete points from a global feature vector, which might lose the existing patterns and elaborate structures. To address these issues, we propose WalkFormer, a novel approach to predict complete point clouds through a partial deformation process. Concretely, our method samples locally dominant points based on feature similarity and moves the points to form the missing part. Since these points maintain representative information of the surrounding structures, they are appropriately selected as the starting points for multiple guided walks. Furthermore, we design a Route Transformer module to exploit and aggregate the walk information with topological relations. These guided walks facilitate the learning of long-range dependencies for predicting shape deformation. Qualitative and quantitative evaluations demonstrate that our proposed approach achieves superior performance compared to state-of-the-art methods in the 3D point cloud completion task",
    "checked": false,
    "id": "882306e404bb4dbfccab8192ef547ba4f1cf8d53",
    "semantic_title": "point cloud completion guided by prior knowledge via causal inference",
    "citation_count": 0,
    "authors": [
      "Mohang Zhang",
      "Yushi Li",
      "Rong Chen",
      "Yushan Pan",
      "Jia Wang",
      "Yunzhe Wang",
      "Rong Xiang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Echterhoff_Driving_Through_the_Concept_Gridlock_Unraveling_Explainability_Bottlenecks_in_Automated_WACV_2024_paper.html": {
    "title": "Driving Through the Concept Gridlock: Unraveling Explainability Bottlenecks in Automated Driving",
    "volume": "main",
    "abstract": "Concept bottleneck models have been successfully used for explainable machine learning by encoding information within the model with a set of human-defined concepts. In the context of human-assisted or autonomous driving, explainability models can help user acceptance and understanding of decisions made by the autonomous vehicle, which can be used to rationalize and explain driver or vehicle behavior. We propose a new approach using concept bottlenecks as visual features for control command predictions and explanations of user and vehicle behavior. We learn a human understandable concept layer that we use to explain sequential driving scenes while learning vehicle control commands. This approach can then be used to determine whether a change in a preferred gap or steering commands from a human (or autonomous vehicle) is led by an external stimulus or change in preferences. We achieve competitive performance to latent visual features while gaining interpretability within our model setup",
    "checked": true,
    "id": "16ce83baff50ddca81a23210933f269fd3fbdb74",
    "semantic_title": "driving through the concept gridlock: unraveling explainability bottlenecks in automated driving",
    "citation_count": 0,
    "authors": [
      "Jessica Echterhoff",
      "An Yan",
      "Kyungtae Han",
      "Amr Abdelraouf",
      "Rohit Gupta",
      "Julian McAuley"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Spetlik_Single-Image_Deblurring_Trajectory_and_Shape_Recovery_of_Fast_Moving_Objects_WACV_2024_paper.html": {
    "title": "Single-Image Deblurring, Trajectory and Shape Recovery of Fast Moving Objects With Denoising Diffusion Probabilistic Models",
    "volume": "main",
    "abstract": "Blurry appearance of fast moving objects in video frames was successfully used to reconstruct the object appearance and motion in both 2D and 3D domains. The proposed method addresses the novel, severely ill-posed, task of single-image fast moving object deblurring, shape, and trajectory recovery -- previous approaches require at least three consecutive video frames. Given a single image, the method outputs the object 2D appearance and position in a series of sub-frames as if captured by a high-speed camera (i.e. temporal super-resolution). The proposed SI-DDPM-FMO method is trained end-to-end on a synthetic dataset with various moving objects, yet it generalizes well to real-world data from several publicly available datasets. SI-DDPM-FMO performs similarly to or better than recent multi-frame methods and a carefully designed baseline method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Radim Spetlik",
      "Denys Rozumnyi",
      "Jiří Matas"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Shaik_IDD-AW_A_Benchmark_for_Safe_and_Robust_Segmentation_of_Drive_WACV_2024_paper.html": {
    "title": "IDD-AW: A Benchmark for Safe and Robust Segmentation of Drive Scenes in Unstructured Traffic and Adverse Weather",
    "volume": "main",
    "abstract": "Large-scale deployment of fully autonomous vehicles requires a very high degree of robustness to unstructured traffic, weather conditions, and should prevent unsafe mispredictions. While there are several datasets and benchmarks focusing on segmentation for drive scenes, they are not specifically focused on safety and robustness issues. We introduce the IDD-AW dataset, which provides 5000 pairs of high-quality images with pixel-level annotations, captured under rain, fog, low light, and snow in unstructured driving conditions. As compared to other adverse weather datasets, we provide i.) more annotated images, ii.) paired Near-Infrared (NIR) image for each frame, iii.) larger label set with a 4-level label hierarchy to capture unstructured traffic conditions. We benchmark state-of-the-art models for semantic segmentation in IDD-AW. We also propose a new metric called \"Safe mean Intersection over Union (Safe mIoU)\" for hierarchical datasets which penalizes dangerous mispredictions that are not captured in the traditional definition of mean Intersection over Union (mIoU). The results show that IDD-AW is one of the most challenging datasets to date for these tasks. The dataset and code will be available here: https://iddaw.github.io",
    "checked": true,
    "id": "e3b278970a25cfc2a305eedca5cfbb9355c99577",
    "semantic_title": "idd-aw: a benchmark for safe and robust segmentation of drive scenes in unstructured traffic and adverse weather",
    "citation_count": 0,
    "authors": [
      "Furqan Ahmed Shaik",
      "Abhishek Reddy",
      "Nikhil Reddy Billa",
      "Kunal Chaudhary",
      "Sunny Manchanda",
      "Girish Varma"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Doubinsky_Semantic_Generative_Augmentations_for_Few-Shot_Counting_WACV_2024_paper.html": {
    "title": "Semantic Generative Augmentations for Few-Shot Counting",
    "volume": "main",
    "abstract": "With the availability of powerful text-to-image diffusion models, recent works have explored the use of synthetic data to improve image classification performances. These works show that it can effectively augment or even replace real data. In this work, we investigate how synthetic data can benefit few-shot class-agnostic counting. This requires to generate images that correspond to a given input number of objects. However, text-to-image models struggle to grasp the notion of count. We propose to rely on a double conditioning of Stable Diffusion with both a prompt and a density map in order to augment a training dataset for few-shot counting. Due to the small dataset size, the fine-tuned model tends to generate images close to the training images. We propose to enhance the diversity of synthesized images by exchanging captions between images thus creating unseen configurations of object types and spatial layout. Our experiments show that our diversified generation strategy significantly improves the counting accuracy of two recent and performing few-shot counting models on FSC147 and CARPK",
    "checked": true,
    "id": "01843a69f5c8a66da06ca883aabaa86ffa2b3fd8",
    "semantic_title": "semantic generative augmentations for few-shot counting",
    "citation_count": 0,
    "authors": [
      "Perla Doubinsky",
      "Nicolas Audebert",
      "Michel Crucianu",
      "Hervé Le Borgne"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Jeanneret_Text-to-Image_Models_for_Counterfactual_Explanations_A_Black-Box_Approach_WACV_2024_paper.html": {
    "title": "Text-to-Image Models for Counterfactual Explanations: A Black-Box Approach",
    "volume": "main",
    "abstract": "This paper addresses the challenge of generating Counterfactual Explanations (CEs), involving the identification and modification of the fewest necessary features to alter a classifier's prediction for a given image. Our proposed method, Text-to-Image Models for Counterfactual Explanations (TIME), is a black-box counterfactual technique based on distillation. Unlike previous methods, this approach requires solely the image and its prediction, omitting the need for the classifier's structure, parameters, or gradients. Before generating the counterfactuals, TIME introduces two distinct biases into Stable Diffusion in the form of textual embeddings: the context bias, associated with the image's structure, and the class bias, linked to class-specific features learned by the target classifier. After learning these biases, we find the optimal latent code applying the classifier's predicted class token and regenerate the image using the target embedding as conditioning, producing the counterfactual explanation. Extensive empirical studies validate that TIME can generate explanations of comparable effectiveness even when operating within a black-box setting",
    "checked": true,
    "id": "b0e77071555eb1a49d66893310afff20741976e7",
    "semantic_title": "text-to-image models for counterfactual explanations: a black-box approach",
    "citation_count": 0,
    "authors": [
      "Guillaume Jeanneret",
      "Loïc Simon",
      "Frédéric Jurie"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Dong_Physical-Space_Multi-Body_Mesh_Detection_Achieved_by_Local_Alignment_and_Global_WACV_2024_paper.html": {
    "title": "Physical-Space Multi-Body Mesh Detection Achieved by Local Alignment and Global Dense Learning",
    "volume": "main",
    "abstract": "From monocular RGB images captured in the wild, detecting multi-body 3D meshes in physical sizes and locations is notoriously difficult due to the diverse visual ambiguity and lack of explicit depth measurement. Modern DNN approaches made numerous advances based on either two-stage Region-of-Interests(RoI)-Align or single-stage fixed Field-of-View (FoV) detector frameworks for two main subtasks: local pelvis-centered mesh regression and global body-to-camera translation regression. However, sub-meter-level physical-space monocular mesh detection is still out of reach by existing solutions. In this paper, we recognize two common drawbacks: (1) The local meshes are usually estimated without explicitly aligning body features under image-space scaling, occlusion, and truncation; (2) The global translations are estimated based on a weak-perspective assumption, which tricks the network into prioritizing image-space (front-view) mesh alignment and leads to inaccurate mesh depth. We introduce Physical-space Multi-body Mesh Detection (PMMD), in which (1) Locally, we preserve the body aspect ratio, align the body-to-RoI layout, and densely refine the person-wise RoI features for robustness; (2) Globally, we learn dense-depth-guided features to amend the body-wise local feature for physical depth estimation. With the cleaned local features and explicit local-global associations, PMMD achieves the best centimeter-level local mesh metrics and the first sub-meter-level global mesh metrics from monocular images in 3DPW and AGORA datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoye Dong",
      "Tiange Xiang",
      "Sravan Chittupalli",
      "Jun Liu",
      "Dong Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Otholt_Guided_Cluster_Aggregation_A_Hierarchical_Approach_to_Generalized_Category_Discovery_WACV_2024_paper.html": {
    "title": "Guided Cluster Aggregation: A Hierarchical Approach to Generalized Category Discovery",
    "volume": "main",
    "abstract": "Despite advances in image recognition, recognizing novel categories in unlabeled data remains challenging for machine learning methods, even though humans can perform this task with ease. A recently developed setting to tackle this problem is Generalized Category Discovery (GCD), in which the task is to, given a labeled dataset, classify an unlabeled dataset, where the unlabeled dataset contains both known classes and novel classes that do not appear in the labeled data. Existing GCD methods mostly focus on learning strong image representations, on which they then apply a clustering algorithm such as k-means. Despite obtaining good performance, they do not fully exploit the potential of the learned features due to the simple nature of the clustering mechanism. To address this issue, we make use of the fact that local neighborhoods in self-supervised feature spaces are highly homogeneous. We leverage this observation to develop Guided Cluster Aggregation (GCA), a hierarchical approach that first groups the data into small clusters of high purity, then aggregates them into larger clusters. Experiments show that GCA outperforms semi-supervised k-means in most cases, especially in fine-grained classification tasks. Code available at https://github.com/J- L- O/guided-cluster-aggregation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jona Otholt",
      "Christoph Meinel",
      "Haojin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Klenk_Masked_Event_Modeling_Self-Supervised_Pretraining_for_Event_Cameras_WACV_2024_paper.html": {
    "title": "Masked Event Modeling: Self-Supervised Pretraining for Event Cameras",
    "volume": "main",
    "abstract": "Event cameras asynchronously capture brightness changes with low latency, high temporal resolution, and high dynamic range. However, annotation of event data is a costly and laborious process, which limits the use of deep learning methods for classification and other semantic tasks with the event modality. To reduce the dependency on labeled event data, we introduce Masked Event Modeling (MEM), a self-supervised framework for events. Our method pretrains a neural network on unlabeled events, which can originate from any event camera recording. Subsequently, the pretrained model is finetuned on a downstream task, leading to a consistent improvement of the task accuracy. For example, our method reaches state-of-the-art classification accuracy across three datasets, N-ImageNet, N-Cars, and N-Caltech101, increasing the top-1 accuracy of previous work by significant margins. When tested on real-world event data, MEM is even superior to supervised RGB-based pretraining. The models pretrained with MEM are also label-efficient and generalize well to the dense task of semantic image segmentation",
    "checked": true,
    "id": "2a76dbc610a66a2d2087fc327987b6f975c9a5a8",
    "semantic_title": "masked event modeling: self-supervised pretraining for event cameras",
    "citation_count": 6,
    "authors": [
      "Simon Klenk",
      "David Bonello",
      "Lukas Koestler",
      "Nikita Araslanov",
      "Daniel Cremers"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Yoo_Real-Time_Polyp_Detection_in_Colonoscopy_Using_Lightweight_Transformer_WACV_2024_paper.html": {
    "title": "Real-Time Polyp Detection in Colonoscopy Using Lightweight Transformer",
    "volume": "main",
    "abstract": "Colorectal cancer (CRC) represents a major global health challenge, and early detection of polyps is crucial in preventing its progression. Although colonoscopy is the gold standard for polyp detection, it has limitations, such as human error and missed detection rates. In response, computer-aided detection (CADe) systems have been developed to enhance the efficiency and accuracy of polyp detection. As deep learning gained prominence, the incorporation of Convolutional Neural Networks (CNNs) into CADe systems emerged as a breakthrough approach. However, CADe systems based on CNNs often demand significant computational resources, making them unsuitable for deployment in resource-constrained environments. To mitigate this, we propose a novel and lightweight polyp detection model that integrates a Transformer layer into the You Only Look Once (YOLO) architecture, focusing on optimizing the neck part responsible for feature fusion and rescaling. Our model demonstrates a substantial reduction in computational complexity and the number of parameters, without compromising detection performances. The lightweight model makes it accessible and feasibly deployable in medically underserved regions, serving a significant public interest by potentially expanding the reach of critical diagnostic tools for CRC prevention. By optimizing the architecture to reduce resource requirements while maintaining performance, our model becomes a practical solution to assist healthcare professionals in the real-time identification of polyps, even with resource-constraint devices",
    "checked": false,
    "id": "c25c7d13d24018b0760e3201a1198bbc4651b795",
    "semantic_title": "transresu-net: transformer based resu-net for real-time colonoscopy polyp segmentation",
    "citation_count": 9,
    "authors": [
      "Youngbeom Yoo",
      "Jae Young Lee",
      "Dong-Jae Lee",
      "Jiwoon Jeon",
      "Junmo Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kolodiazhnyi_Top-Down_Beats_Bottom-Up_in_3D_Instance_Segmentation_WACV_2024_paper.html": {
    "title": "Top-Down Beats Bottom-Up in 3D Instance Segmentation",
    "volume": "main",
    "abstract": "Most 3D instance segmentation methods exploit a bottom-up strategy, typically including resource-exhaustive post-processing. For point grouping, bottom-up methods rely on prior assumptions about the objects in the form of hyperparameters, which are domain-specific and need to be carefully tuned. On the contrary, we address 3D instance segmentation with a TD3D: the pioneering cluster-free, fully-convolutional and entirely data-driven approach trained in an end-to-end manner. This is the first top-down method outperforming bottom-up approaches in 3D domain. With its straightforward pipeline, it performs outstandingly well on the standard benchmarks: ScanNet v2, its extension ScanNet200, and S3DIS. Besides, our method is much faster on inference than the current state-of-the-art grouping-based approaches: our flagship modification is 1.9x faster than the most accurate bottom-up method, while being more accurate, and our faster modification shows state-of-the-art accuracy running at 2.6x speed. Code is available at https://github.com/SamsungLabs/td3d",
    "checked": true,
    "id": "5539648bdbee84403d03576fc63d72b7267cf29e",
    "semantic_title": "top-down beats bottom-up in 3d instance segmentation",
    "citation_count": 7,
    "authors": [
      "Maksim Kolodiazhnyi",
      "Anna Vorontsova",
      "Anton Konushin",
      "Danila Rukhovich"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Sarkar_Open-Set_Object_Detection_by_Aligning_Known_Class_Representations_WACV_2024_paper.html": {
    "title": "Open-Set Object Detection by Aligning Known Class Representations",
    "volume": "main",
    "abstract": "Open Set Object Detection (OSOD) has emerged as a contemporary research direction to address the detection of unknown objects. Recently, few works have achieved remarkable performance in the OSOD task by employing contrastive clustering to separate unknown classes. In contrast, we propose a new semantic clustering-based approach to facilitate a meaningful alignment of clusters in semantic space and introduce a class decorrelation module to enhance inter-cluster separation. Our approach further incorporates an object focus module to predict objectness scores, which enhances the detection of unknown objects. Further, we employ i) an evaluation technique that penalizes low-confidence outputs to mitigate the risk of misclassification of the unknown objects and ii) a new metric called HMP that combines known and unknown precision using harmonic mean. Our extensive experiments demonstrate that the proposed model achieves significant improvement on the MS-COCO & PASCAL VOC dataset for the OSOD task",
    "checked": false,
    "id": "7b234d67bbf768b05af6bbaeaae8bd749bf346fc",
    "semantic_title": "spectral-spatial latent reconstruction for open-set hyperspectral image classification",
    "citation_count": 23,
    "authors": [
      "Hiran Sarkar",
      "Vishal Chudasama",
      "Naoyuki Onoe",
      "Pankaj Wasnik",
      "Vineeth N. Balasubramanian"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhang_DR2_Disentangled_Recurrent_Representation_Learning_for_Data-Efficient_Speech_Video_Synthesis_WACV_2024_paper.html": {
    "title": "DR2: Disentangled Recurrent Representation Learning for Data-Efficient Speech Video Synthesis",
    "volume": "main",
    "abstract": "Although substantial progress has been made in audio-driven talking video synthesis, there still remain two major difficulties: existing works 1) need a long sequence of training dataset (>1h) to synthesize co-speech gestures, which causes a significant limitation on their applicability; 2) usually fail to generate long sequences, or can only generate long sequences without enough diversity. To solve these challenges, we propose a Disentangled Recurrent Representation Learning framework to synthesize long diversified gesture sequences with a short training video of around 2 minutes. In our framework, we first make a disentangled latent space assumption to encourage unpaired audio and pose combinations, which results in diverse \"one-to-many\" mappings in pose generation. Next, we apply a recurrent inference module to feed back the last generation as initial guidance to the next phase, enhancing the long-term video generation of full continuity and diversity. Comprehensive experimental results verify that our model can generate realistic synchronized full-body talking videos with training data efficiency",
    "checked": false,
    "id": "3616bb9c44a5dd64acdc4f591874c773fd472ed1",
    "semantic_title": "dr 2 : disentangled recurrent representation learning for data-efficient speech video synthesis",
    "citation_count": 0,
    "authors": [
      "Chenxu Zhang",
      "Chao Wang",
      "Yifan Zhao",
      "Shuo Cheng",
      "Linjie Luo",
      "Xiaohu Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Bhattacharya_EvDNeRF_Reconstructing_Event_Data_With_Dynamic_Neural_Radiance_Fields_WACV_2024_paper.html": {
    "title": "EvDNeRF: Reconstructing Event Data With Dynamic Neural Radiance Fields",
    "volume": "main",
    "abstract": "We present EvDNeRF, a pipeline for generating event data and training an event-based dynamic NeRF, for the purpose of faithfully reconstructing eventstreams on scenes with rigid and non-rigid deformations that may be too fast to capture with a standard camera. Event cameras register asynchronous per-pixel brightness changes at MHz rates with high dynamic range, making them ideal for observing fast motion with almost no motion blur. Neural radiance fields (NeRFs) offer visual-quality geometric-based learnable rendering, but prior work with events has only considered reconstruction of static scenes. Our EvDNeRF can predict eventstreams of dynamic scenes from a static or moving viewpoint between any desired timestamps, thereby allowing it to be used as an event-based simulator for a given scene. We show that by training on varied batch sizes of events, we can improve test-time predictions of events at fine time resolutions, outperforming baselines that pair standard dynamic NeRFs with event generators. We release our simulated and real datasets, as well as code for multi-view event-based data generation and the training and evaluation of EvDNeRF models",
    "checked": true,
    "id": "0d6169a33ddef781017db0088135f4041c1c7fc1",
    "semantic_title": "evdnerf: reconstructing event data with dynamic neural radiance fields",
    "citation_count": 0,
    "authors": [
      "Anish Bhattacharya",
      "Ratnesh Madaan",
      "Fernando Cladera",
      "Sai Vemprala",
      "Rogerio Bonatti",
      "Kostas Daniilidis",
      "Ashish Kapoor",
      "Vijay Kumar",
      "Nikolai Matni",
      "Jayesh K. Gupta"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Qin_DISCO_Distributed_Inference_With_Sparse_Communications_WACV_2024_paper.html": {
    "title": "DISCO: Distributed Inference With Sparse Communications",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) have great potential to solve many real-world problems, but they usually require an extensive amount of computation and memory. It is of great difficulty to deploy a large DNN model to a single resource-limited device with small memory capacity. Distributed computing is a common approach to reduce single-node memory consumption and to accelerate the inference of DNN models. In this paper, we explore the \"within-layer model parallelism\", which distributes the inference of each layer into multiple nodes. In this way, the memory requirement can be distributed to many nodes, making it possible to use several edge devices to infer a large DNN model. Due to the dependency within each layer, data communications between nodes during this parallel inference can be a bottleneck when the communication bandwidth is limited. We propose a framework to train DNN models for Distributed Inference with Sparse Communications (DISCO). We convert the problem of selecting which subset of data to transmit between nodes into a model optimization problem, and derive models with both computation and communication reduction when each layer is inferred on multiple nodes. We show the benefit of the DISCO framework on a variety of CV tasks such as image classification, object detection, semantic segmentation, and image super resolution. The corresponding models include important DNN building blocks such as convolutions and transformers. For example, each layer of a ResNet-50 model can be distributively inferred across two nodes with 5x less data communications, almost half overall computations and less than half memory requirement for a single node, and achieve comparable accuracy to the original ResNet-50 model",
    "checked": true,
    "id": "ba05ff59a27bb5ded2d407c264fac3a6901d432a",
    "semantic_title": "disco: distributed inference with sparse communications",
    "citation_count": 0,
    "authors": [
      "Minghai Qin",
      "Chao Sun",
      "Jaco Hofmann",
      "Dejan Vucinic"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Azari_EmoStyle_One-Shot_Facial_Expression_Editing_Using_Continuous_Emotion_Parameters_WACV_2024_paper.html": {
    "title": "EmoStyle: One-Shot Facial Expression Editing Using Continuous Emotion Parameters",
    "volume": "main",
    "abstract": "Recent studies have achieved impressive results in face generation and editing of facial expressions. However, existing approaches either generate a discrete number of facial expressions or have limited control over the emotion of the output image. To overcome this limitation, we introduced EmoStyle, a method to edit facial expressions based on valence and arousal, two continuous emotional parameters that can specify a broad range of emotions. EmoStyle is designed to separate emotions from other facial characteristics and to edit the face to display a desired emotion. We employ the pre-trained generator from StyleGAN2, taking advantage of its rich latent space. We also proposed an adapted inversion method to be able to apply our system on out-of-StyleGAN2 domain images in a one-shot manner. The qualitative and quantitative evaluations show that our approach has the capability to synthesize a wide range of expressions to output high-resolution images",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bita Azari",
      "Angelica Lim"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Harithas_FinderNet_A_Data_Augmentation_Free_Canonicalization_Aided_Loop_Detection_and_WACV_2024_paper.html": {
    "title": "FinderNet: A Data Augmentation Free Canonicalization Aided Loop Detection and Closure Technique for Point Clouds in 6-DOF Separation",
    "volume": "main",
    "abstract": "We focus on the problem of LiDAR point cloud based loop detection (or Finding) and closure (LDC) for mobile robots. State-of-the-art (SOTA) methods directly generate learned embeddings from a given point cloud, require large data augmentation, and are not robust to wide viewpoint variations in 6 Degrees-of-Freedom (DOF). Moreover, the absence of strong priors in an unstructured point cloud leads to highly inaccurate LDC. In this original approach, we propose independent roll and pitch canonicalization of point clouds using a common dominant ground plane. We discretize the canonicalized point clouds along the axis perpendicular to the ground plane leads to images simi- lar to digital elevation maps (DEMs), which expose strong spatial priors in the scene. Our experiments show that LDC based on learnt embeddings from such DEMs is not only data efficient but also significantly more robust, and generalizable than the current SOTA. We report an (aver- age precision for loop detection, mean absolute transla- tion/rotation error) improvement of (8.4, 16.7/5.43)% on the KITTI08 sequence, and (11.0, 34.0/25.4)% on GPR10 sequence, over the current SOTA. To further test the ro- bustness of our technique on point clouds in 6-DOF motion we create and opensource a custom dataset called Lidar- UrbanFly Dataset (LUF) which consists of point clouds ob- tained from a LiDAR mounted on a quadrotor. More details on our website https://gsc2001.github.io/FinderNet/",
    "checked": true,
    "id": "ac4235dd9ade1691066eb2d2d840680c905bc95b",
    "semantic_title": "findernet: a data augmentation free canonicalization aided loop detection and closure technique for point clouds in 6-dof separation",
    "citation_count": 2,
    "authors": [
      "Sudarshan S. Harithas",
      "Gurkirat Singh",
      "Aneesh Chavan",
      "Sarthak Sharma",
      "Suraj Patni",
      "Chetan Arora",
      "Madhava Krishna"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wang_Distortion-Disentangled_Contrastive_Learning_WACV_2024_paper.html": {
    "title": "Distortion-Disentangled Contrastive Learning",
    "volume": "main",
    "abstract": "Self-supervised learning is well known for its remarkable performance in representation learning and various downstream computer vision tasks. Recently, Positive-pair-Only Contrastive Learning (POCL) has achieved reliable performance without the need to construct positive-negative training sets. It reduces memory requirements by lessening the dependency on the batch size. The POCL method typically uses a single objective function to extract the distortion invariant representation (DIR), which describes the proximity of positive-pair representations affected by different distortions. This objective function implicitly enables the model to filter out or ignore the distortion variant representation (DVR) affected by different distortions. However, some recent studies have shown that proper use of DVR in contrastive can optimize the performance of models in some downstream domain-specific tasks. In addition, these POCL methods have been observed to be sensitive to augmentation strategies. To address these limitations, we propose a novel POCL framework named Distortion-Disentangled Contrastive Learning (DDCL) and a Distortion-Disentangled Loss (DDL). Our approach is the first to explicitly and adaptively disentangle and exploit the DVR inside the model and feature stream to improve the overall representation utilization efficiency, robustness, and representation ability. Experiments demonstrate our framework's superiority to Barlow Twins and Simsiam in terms of convergence, representation quality (Including transferability and generality), and robustness on several benchmark datasets",
    "checked": true,
    "id": "5cf639111785cb48f5175dd98d513642520a42ab",
    "semantic_title": "distortion-disentangled contrastive learning",
    "citation_count": 0,
    "authors": [
      "Jinfeng Wang",
      "Sifan Song",
      "Jionglong Su",
      "S. Kevin Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Gungor_Boosting_Weakly_Supervised_Object_Detection_Using_Fusion_and_Priors_From_WACV_2024_paper.html": {
    "title": "Boosting Weakly Supervised Object Detection Using Fusion and Priors From Hallucinated Depth",
    "volume": "main",
    "abstract": "Despite recent attention and exploration of depth for various tasks, it is still an unexplored modality for weakly-supervised object detection (WSOD). We propose an amplifier method for enhancing the performance of WSOD by integrating depth information. Our approach can be applied to any WSOD method based on multiple instance learning, without necessitating additional annotations or inducing large computational expenses. Our proposed method employs a monocular depth estimation technique to obtain hallucinated depth information, which is then incorporated into a Siamese WSOD network using contrastive loss and fusion. By analyzing the relationship between language context and depth, we calculate depth priors to identify the bounding box proposals that may contain an object of interest. These depth priors are then utilized to update the list of pseudo ground-truth boxes, or adjust the confidence of perbox predictions. Our proposed method is evaluated on six datasets (COCO, PASCAL VOC, Conceptual Captions, Clipart1k, Watercolor2k, and Comic2k) by implementing it on top of two state-of-the-art WSOD methods, and we demonstrate a substantial enhancement in performance",
    "checked": true,
    "id": "a1454ac0d7b4d839f13884ca9c3c71e878f1974e",
    "semantic_title": "boosting weakly supervised object detection using fusion and priors from hallucinated depth",
    "citation_count": 0,
    "authors": [
      "Cagri Gungor",
      "Adriana Kovashka"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Himmi_MS-EVS_Multispectral_Event-Based_Vision_for_Deep_Learning_Based_Face_Detection_WACV_2024_paper.html": {
    "title": "MS-EVS: Multispectral Event-Based Vision for Deep Learning Based Face Detection",
    "volume": "main",
    "abstract": "Event-based sensing is a relatively new imaging modality that enables low latency, low power, high temporal resolution and high dynamic range acquisition. These properties make it a highly desirable sensor for edge applications and in high dynamic range environments. As of today, most event-based sensors are monochromatic (grayscale), capturing light from a wide spectral range over the visible, in a single channel. In this paper, we introduce multispectral events and study their advantages. In particular, we consider multiple bands in the visible and near-infrared range, and explore their potential compared to monochromatic events and conventional multispectral imaging for the face detection task. We further release the first large scale bimodal face detection datasets, with RGB videos and their simulated color events, N-MobiFace and N-YoutubeFaces, and a smaller dataset with multispectral videos and events, N-SpectralFace. We find that early fusion of multispectral events significantly improves the face detection performance, compared to the early fusion of conventional multispectral images. This result shows that polychromatic events carry relatively more useful information about the scene than conventional multispectral/color images do, with respect to their monochromatic equivalent. To the best of our knowledge, our proposed method is the first exploratory research on multispectral events, specifically including near infrared data",
    "checked": true,
    "id": "68bf3f0717be1318bbb603f393a94795670d7efc",
    "semantic_title": "ms-evs: multispectral event-based vision for deep learning based face detection",
    "citation_count": 0,
    "authors": [
      "Saad Himmi",
      "Vincent Parret",
      "Ajad Chhatkuli",
      "Luc Van Gool"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kim_Adaptive_Latent_Diffusion_Model_for_3D_Medical_Image_to_Image_WACV_2024_paper.html": {
    "title": "Adaptive Latent Diffusion Model for 3D Medical Image to Image Translation: Multi-Modal Magnetic Resonance Imaging Study",
    "volume": "main",
    "abstract": "Multi-modal images play a crucial role in comprehensive evaluations in medical image analysis providing complementary information for identifying clinically important biomarkers. However, in clinical practice, acquiring multiple modalities can be challenging due to reasons such as scan cost, limited scan time, and safety considerations. In this paper, we propose a model based on the latent diffusion model (LDM) that leverages switchable blocks for image-to-image translation in 3D medical images without patch cropping. The 3D LDM combined with conditioning using the target modality allows generating high-quality target modality in 3D overcoming the shortcoming of the missing out-of-slice information in 2D generation methods. The switchable block, noted as multiple switchable spatially adaptive normalization (MS-SPADE), dynamically transforms source latents to the desired style of the target latents to help with the diffusion process. The MS-SPADE block allows us to have one single model to tackle many translation tasks of one source modality to various targets removing the need for many translation models for different scenarios. Our model exhibited successful image synthesis across different source-target modality scenarios and surpassed other models in quantitative evaluations tested on multi-modal brain magnetic resonance imaging datasets of four different modalities. Our model demonstrated successful image synthesis across various modalities even allowing for one-to-many modality translations. Furthermore, it outperformed other one-to-one translation models in quantitative evaluations",
    "checked": true,
    "id": "3c36d53b230445176027dceb7cae297e794ee0ee",
    "semantic_title": "adaptive latent diffusion model for 3d medical image to image translation: multi-modal magnetic resonance imaging study",
    "citation_count": 0,
    "authors": [
      "Jonghun Kim",
      "Hyunjin Park"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Thakare_Lets_Observe_Them_Over_Time_An_Improved_Pedestrian_Attribute_Recognition_WACV_2024_paper.html": {
    "title": "Let's Observe Them Over Time: An Improved Pedestrian Attribute Recognition Approach",
    "volume": "main",
    "abstract": "Despite poor image quality, occlusions, and small training datasets, recent pedestrian attribute recognition (PAR) methods have achieved considerable performance. However, leveraging only spatial information of different attributes limits their reliability and generalizability. This paper introduces a multi-perspective approach to reduce over-dependence on spatial clues of a single perspective and exploits other aspects available in multiple perspectives. In order to tackle image quality and occlusions, we exploit different spatial clues present across images and handpick the best attribute-specific features to classify. Precisely, we extract the class-activation energy of each attribute and correlate it with the corresponding energy present across other images using the proposed Self-Attentive Cross Relation Module. In the next stage, we fuse this correlation information with similar clues accumulated from the other images. Lastly, we train a classification neural network using combined correlation information with two different losses. We have validated our method on four widely used PAR datasets, namely Market1501, PETA, PA-100k, and Duke. Our method achieves superior performance over most existing methods, demonstrating the effectiveness of a multi-perspective approach in PAR",
    "checked": false,
    "id": "48e0cc0a9a0274d894a676442ed115e58001b133",
    "semantic_title": "machine intelligence and soft computing: expert foresight reliability analysis for aerospace applications: reducing over-conservative expert estimates in the presence of limited data challenges, approaches and of the integrity, purposefulness and adaptivity principle and",
    "citation_count": 0,
    "authors": [
      "Kamalakar Vijay Thakare",
      "Debi Prosad Dogra",
      "Heeseung Choi",
      "Haksub Kim",
      "Ig-Jae Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Dey_AnyStar_Domain_Randomized_Universal_Star-Convex_3D_Instance_Segmentation_WACV_2024_paper.html": {
    "title": "AnyStar: Domain Randomized Universal Star-Convex 3D Instance Segmentation",
    "volume": "main",
    "abstract": "Star-convex shapes arise across bio-microscopy and radiology in the form of nuclei, nodules, metastases, and other units. Existing instance segmentation networks for such structures train on densely labeled instances for each dataset, which requires substantial and often impractical manual annotation effort. Further, significant reengineering or finetuning is needed when presented with new datasets and imaging modalities due to changes in contrast, shape, orientation, resolution, and density. We present AnyStar, a domain-randomized generative model that simulates synthetic training data of blob-like objects with randomized appearance, environments, and imaging physics to train general-purpose star-convex instance segmentation networks. As a result, networks trained using our generative model do not require annotated images from unseen datasets. A single network trained on our synthesized data accurately 3D segments C. elegans and P. dumerilii nuclei in fluorescence microscopy, mouse cortical nuclei in micro-CT, zebrafish brain nuclei in EM, and placental cotyledons in human fetal MRI, all without any retraining, finetuning, transfer learning, or domain adaptation. Code is available at https://github.com/neel-dey/AnyStar",
    "checked": true,
    "id": "1d851de632015df478e8a055fd8f11543acdfa0f",
    "semantic_title": "anystar: domain randomized universal star-convex 3d instance segmentation",
    "citation_count": 2,
    "authors": [
      "Neel Dey",
      "Mazdak Abulnaga",
      "Benjamin Billot",
      "Esra Abaci Turk",
      "Ellen Grant",
      "Adrian V. Dalca",
      "Polina Golland"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Haalck_Solving_the_Plane-Sphere_Ambiguity_in_Top-Down_Structure-From-Motion_WACV_2024_paper.html": {
    "title": "Solving the Plane-Sphere Ambiguity in Top-Down Structure-From-Motion",
    "volume": "main",
    "abstract": "Drone-based land surveys and tracking applications with a moving camera require three-dimensional reconstructions from videos recorded using a downward facing camera and are usually generated by Structure-from-Motion (SfM) algorithms. Unfortunately, monocular SfM pipelines can fail in the presence of lens distortion due to a critical configuration resulting in a plane-sphere ambiguity which is characterized by severe curvatures of the reconstructions and erroneous relative camera pose estimations. We propose a 4-point minimal solver for the relative pose estimation for two views sharing the same radial distortion parameters (i.e. from the same camera) with a viewing direction perpendicular to the ground plane. To extract 3D reconstructions from continuous videos, the relative pose of pairwise frames is estimated by using the solver with RANSAC and the Sampson error where globally consistent distortion parameters are determined by taking the medial of all values. Moreover, we propose an additional regularizer for the final bundle adjustment to remove any remaining curvature of the reconstruction if necessary. We tested our methods on synthetic and real-world data and our results demonstrate a significant reduction of curvature and more accurate relative pose estimations. Our algorithm can be easily integrated into existing pipelines and is therefore a practical solution to resolve the plane-sphere ambiguity in a variety of top-down SfM applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lars Haalck",
      "Benjamin Risse"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Herzig_PromptonomyViT_Multi-Task_Prompt_Learning_Improves_Video_Transformers_Using_Synthetic_Scene_WACV_2024_paper.html": {
    "title": "PromptonomyViT: Multi-Task Prompt Learning Improves Video Transformers Using Synthetic Scene Data",
    "volume": "main",
    "abstract": "Action recognition models have achieved impressive results by incorporating scene-level annotations, such as objects, their relations, 3D structure, and more. However, obtaining annotations of scene structure for videos requires a significant amount of effort to gather and annotate, making these methods expensive to train. In contrast, synthetic datasets generated by graphics engines provide powerful alternatives for generating scene-level annotations across multiple tasks. In this work, we propose an approach to leverage synthetic scene data for improving video understanding. We present a multi-task prompt learning approach for video transformers, where a shared video transformer backbone is enhanced by a small set of specialized parameters for each task. Specifically, we add a set of \"task prompts\", each corresponding to a different task, and let each prompt predict task-related annotations. This design allows the model to capture information shared among synthetic scene tasks as well as information shared between synthetic scene tasks and a real video downstream task throughout the entire network. We refer to this approach as \"Promptonomy\", since the prompts model task-related structure. We propose the PromptonomyViT model (PViT), a video transformer that incorporates various types of scene-level information from synthetic data using the \"Promptonomy\" approach. PViT shows strong performance improvements on multiple video understanding tasks and datasets. Project page: https://ofir1080.github.io/PromptonomyViT/",
    "checked": true,
    "id": "12d362946dbe9bc26e1b6902cf96e354587644d3",
    "semantic_title": "promptonomyvit: multi-task prompt learning improves video transformers using synthetic scene data",
    "citation_count": 7,
    "authors": [
      "Roei Herzig",
      "Ofir Abramovich",
      "Elad Ben Avraham",
      "Assaf Arbelle",
      "Leonid Karlinsky",
      "Ariel Shamir",
      "Trevor Darrell",
      "Amir Globerson"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Improving_the_Leaking_of_Augmentations_in_Data-Efficient_GANs_via_Adaptive_WACV_2024_paper.html": {
    "title": "Improving the Leaking of Augmentations in Data-Efficient GANs via Adaptive Negative Data Augmentation",
    "volume": "main",
    "abstract": "Data augmentation (DA) has shown its effectiveness in training Data-Efficient GANs (DE-GANs). However, applying DA in DE-GANs results in transforming the distributions of generated data and real data to augmented distributions of generated data and real data. This augmentation process could produce some out-of-distribution samples, known as the leaking of augmentations problem, which is highly undesirable in DE-GANs training. Although some methods propose \"leaking-free\" DAs for DE-GANs, we theoretically and practically argue that the leaking of augmentations problem still exists in these methods. To alleviate the leaking of augmentations in DE-GANs, in this paper, we propose a simple yet effective method called adaptive negative data augmentation (ANDA) for DE-GANs, with a negligible computational cost increase. Specifically, ANDA adaptively augments the augmented distribution of generated data using the augmented distribution of negative real data, where the negative real data is produced by applying negative data augmentation (NDA) on the real data. In this case, potential leaking samples can be presented as \"fake\" instances to the discriminator adaptively, which avoids the generator (G) learning such samples, thus resulting in better performance. Extensive experiments on several datasets with different DE-GANs demonstrate that ANDA can effectively alleviate the leaking of augmentations problem during training and achieve better performance. Codes are available at https://github.com/zzhang05/ANDA",
    "checked": false,
    "id": "594fffdacf2b0f35dd83fdac8de7c38d4b4d533a",
    "semantic_title": "supplementary material of improving the leaking of augmentations in data-efficient gans via adaptive negative data augmentation",
    "citation_count": 0,
    "authors": [
      "Zhaoyu Zhang",
      "Yang Hua",
      "Guanxiong Sun",
      "Hui Wang",
      "Seán McLoone"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kim_Enhancing_Diverse_Intra-Identity_Representation_for_Visible-Infrared_Person_Re-Identification_WACV_2024_paper.html": {
    "title": "Enhancing Diverse Intra-Identity Representation for Visible-Infrared Person Re-Identification",
    "volume": "main",
    "abstract": "Visible-Infrared person Re-Identification (VI-ReID) is a challenging task due to modality discrepancy. To reduce modality-gap, existing methods primarily focus on sample diversity, such as data augmentation or generating intermediate modality between Visible and Infrared. However, these methods do not consider the increase in intra-instance variance caused by sample diversity, and they focus on dominant features, which results in a remaining modality gap for hard samples. This limitation hinders performance improvement. We propose Intra-identity Representation Diversification (IRD) based metric learning to handle the intra-instance variance. Specifically IRD method enlarge the Intra-modality Intra-identity Representation Space (IIRS) for each modality within the same identity to learn diverse feature representation abilities. This enables the formation of a shared space capable of representing common features across hetero-modality, thereby reducing the modality gap more effectively. In addition, we introduce a HueGray (HG) data augmentation method, which increases sample diversity simply and effectively. Finally, we propose the Diversity Enhancement Network (DEN) for robustly handling intra-instance variance. The proposed method demonstrates superior performance compared to the state-of-the-art methods on the SYSU-MM01 and RegDB datasets. Notably, on the challenging SYSU-MM01 dataset, our approach achieves remarkable results with a Rank-1 accuracy of 76.36% and a mean Average Precision (mAP) of 71.30%",
    "checked": false,
    "id": "295065a4d1afeb001460ae2d76e6285e1e3fc070",
    "semantic_title": "exploring invariant representation for visible-infrared person re-identification",
    "citation_count": 3,
    "authors": [
      "Sejun Kim",
      "Soonyong Gwon",
      "Kisung Seo"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Katageri_Synergizing_Contrastive_Learning_and_Optimal_Transport_for_3D_Point_Cloud_WACV_2024_paper.html": {
    "title": "Synergizing Contrastive Learning and Optimal Transport for 3D Point Cloud Domain Adaptation",
    "volume": "main",
    "abstract": "Recently, the fundamental problem of unsupervised domain adaptation (UDA) on 3D point clouds has been motivated by a wide variety of applications in robotics, virtual reality, and scene understanding, to name a few. The point cloud data acquisition procedures manifest themselves as significant domain discrepancies and geometric variations among both similar and dissimilar classes. The standard domain adaptation methods developed for images do not directly translate to point cloud data because of their complex geometric nature. To address this challenge, we leverage the idea of multimodality and alignment between distributions. We propose a new UDA architecture for point cloud classification that benefits from multimodal contrastive learning to get better class separation in both domains individually. Further, the use of optimal transport (OT) aims at learning source and target data distributions jointly to reduce the cross-domain shift and provide a better alignment. We conduct a comprehensive empirical study on PointDA-10 and GraspNetPC-10 and show that our method achieves state-of the-art performance on GraspNetPC-10 (with approx. 4-12% margin) and best average performance on PointDA-10. Our ablation studies and decision boundary analysis also validate the significance of our contrastive learning module and OT alignment",
    "checked": true,
    "id": "951d9d4188d58755bf8679e5c5d8e2e85dd206ac",
    "semantic_title": "synergizing contrastive learning and optimal transport for 3d point cloud domain adaptation",
    "citation_count": 0,
    "authors": [
      "Siddharth Katageri",
      "Arkadipta De",
      "Chaitanya Devaguptapu",
      "VSSV Prasad",
      "Charu Sharma",
      "Manohar Kaul"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Li_Video_Instance_Matting_WACV_2024_paper.html": {
    "title": "Video Instance Matting",
    "volume": "main",
    "abstract": "Conventional video matting outputs one alpha matte for all instances appearing in a video frame so that individual instances are not distinguished. While video instance segmentation provides time-consistent instance masks, results are unsatisfactory for matting applications, especially due to applied binarization. To remedy this deficiency, we propose Video Instance Matting (VIM), that is, estimating the alpha mattes of each instance at each frame of a video sequence. To tackle this challenging problem, we present MSG-VIM, a Mask Sequence Guided Video Instance Matting neural network, as a novel baseline model for VIM. MSG-VIM leverages a mixture of mask augmentations to make predictions robust to inaccurate and inconsistent mask guidance. It incorporates temporal mask and temporal feature guidance to improve the temporal consistency of alpha matte predictions. Furthermore, we build a new benchmark for VIM, called VIM50, which comprises 50 video clips with multiple human instances as foreground objects. To evaluate performances on the VIM task, we introduce a suitable metric called Video Instance-aware Matting Quality (VIMQ). Our proposed model MSG-VIM sets a strong baseline on the VIM50 benchmark and outperforms existing methods by a large margin",
    "checked": true,
    "id": "c72e353325da567c680b21e31257439c785df46e",
    "semantic_title": "video instance matting",
    "citation_count": 1,
    "authors": [
      "Jiachen Li",
      "Roberto Henschel",
      "Vidit Goel",
      "Marianna Ohanyan",
      "Shant Navasardyan",
      "Humphrey Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Xu_DPPMask_Masked_Image_Modeling_With_Determinantal_Point_Processes_WACV_2024_paper.html": {
    "title": "DPPMask: Masked Image Modeling With Determinantal Point Processes",
    "volume": "main",
    "abstract": "Masked Image Modeling (MIM) has achieved impressive representative performance with the aim of reconstructing randomly masked images. Despite the empirical success, most previous works have neglected the important fact that it is unreasonable to force the model to reconstruct something beyond recovery, such as those masked objects. In this work, we show that uniformly random masking widely used in previous works unavoidably loses some key objects and changes original semantic information, resulting in a misalignment problem and hurting the representative learning eventually. To address this issue, we augment MIM with a new masking strategy namely the DPPMask by substituting the random process with Determinantal Point Process (DPPs) to reduce the semantic change of the image after masking. Our method is simple yet effective and requires no extra learnable parameters when implemented within various frameworks. In particular, we evaluate our method on two representative MIM frameworks, MAE and iBOT. We show that DPPMask surpassed random sampling under both lower and higher masking ratios, indicating that DPPMask makes the reconstruction task more reasonable. We further test our method on the background challenge and multi-class classification tasks, showing that our method is more robust at various tasks",
    "checked": true,
    "id": "e03f581b575ef5ee8eab90e9700b357d0f84cb6a",
    "semantic_title": "dppmask: masked image modeling with determinantal point processes",
    "citation_count": 0,
    "authors": [
      "Junde Xu",
      "Zikai Lin",
      "Donghao Zhou",
      "Yaodong Yang",
      "Xiangyun Liao",
      "Qiong Wang",
      "Bian Wu",
      "Guangyong Chen",
      "Pheng-Ann Heng"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kapil_ShadowSense_Unsupervised_Domain_Adaptation_and_Feature_Fusion_for_Shadow-Agnostic_Tree_WACV_2024_paper.html": {
    "title": "ShadowSense: Unsupervised Domain Adaptation and Feature Fusion for Shadow-Agnostic Tree Crown Detection From RGB-Thermal Drone Imagery",
    "volume": "main",
    "abstract": "Accurate detection of individual tree crowns from remote sensing data poses a significant challenge due to the dense nature of forest canopy and the presence of diverse environmental variations, e.g., overlapping canopies, occlusions, and varying lighting conditions. Additionally, the lack of data for training robust models adds another limitation in effectively studying complex forest conditions. This paper presents a novel method for detecting shadowed tree crowns and provides a challenging dataset comprising roughly 50k paired RGB-thermal images to facilitate future research for illumination-invariant detection. The proposed method (ShadowSense) is entirely self-supervised, leveraging domain adversarial training without source domain annotations for feature extraction and foreground feature alignment for feature pyramid networks to adapt domain-invariant representations by focusing on visible foreground regions, respectively. It then fuses complementary information of both modalities to effectively improve upon the predictions of an RGB-trained detector and boost the overall accuracy. Extensive experiments demonstrate the superiority of the proposed method over both the baseline RGB-trained detector and state-of-the-art techniques that rely on unsupervised domain adaptation or early image fusion. Our code and data are available: https://github.com/rudrakshkapil/ShadowSense",
    "checked": true,
    "id": "4ca82bc640df99fcf8060bba76e59a3403419a68",
    "semantic_title": "shadowsense: unsupervised domain adaptation and feature fusion for shadow-agnostic tree crown detection from rgb-thermal drone imagery",
    "citation_count": 0,
    "authors": [
      "Rudraksh Kapil",
      "Seyed Mojtaba Marvasti-Zadeh",
      "Nadir Erbilgin",
      "Nilanjan Ray"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Lee_Pruning_From_Scratch_via_Shared_Pruning_Module_and_Nuclear_Norm-Based_WACV_2024_paper.html": {
    "title": "Pruning From Scratch via Shared Pruning Module and Nuclear Norm-Based Regularization",
    "volume": "main",
    "abstract": "Most pruning methods focus on determining redundant channels from the pre-trained model. However, they overlook the cost of training large networks and the significance of selecting channels for effective reconfiguration. In this paper, we present a \"pruning from scratch\" framework that considers reconfiguration and expression capacity. Our Shared Pruning Module (SPM) handles a channel alignment problem in residual blocks for lossless reconfiguration after pruning. Moreover, we introduce nuclear norm-based regularization to preserve the representability of large networks during the pruning process. By combining it with MACs-based regularization, we achieve an efficient and powerful pruned network while compressing towards target MACs. The experimental results demonstrate that our method prunes redundant channels effectively to enhance representation capacity of the network. Our approach compresses ResNet50 on ImageNet without requiring additional resources, achieving a top-1 accuracy of 75.25% with only 41% of the original model's MACs. Code is available at https://github.com/jsleeg98/NuSPM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donghyeon Lee",
      "Eunho Lee",
      "Youngbae Hwang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Jobin_Semantic_Labels-Aware_Transformer_Model_for_Searching_Over_a_Large_Collection_WACV_2024_paper.html": {
    "title": "Semantic Labels-Aware Transformer Model for Searching Over a Large Collection of Lecture-Slides",
    "volume": "main",
    "abstract": "Massive Open Online Courses (MOOCs) enable easy access to many educational materials, particularly lecture slides, on the web. Searching through them based on user queries becomes an essential problem due to the availability of such vast information. To address this, we present Lecture Slide Deck Search Engine -- a model that supports natural language queries and hand-drawn sketches and performs searches on a large collection of slide images on computer science topics. This search engine is trained using a novel semantic label-aware transformer model that extracts the semantic labels in the slide images and seamlessly encodes them with the visual cues from the slide images and textual cues from the natural language query. Further, to study the problem in a challenging setting, we introduce a novel dataset, namely the Lecture Slide Deck (LecSD) Dataset containing 54K slide images from the Data Structure, computer networks, and optimization courses and provide associated manual annotation for the query in the form of natural language or hand-drawn sketch. The proposed Lecture Slide Deck Search Engine outperforms the competitive baselines and achieves nearly 4% superior Recall@1 on an absolute scale compared to the state-of-the-art approach. We firmly believe that this work will open up promising directions for improving the accessibility and usability of educational resources, enabling students and educators to find and utilize lecture materials more effectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "K. V. Jobin",
      "Anand Mishra",
      "C. V. Jawahar"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Multimodal_Channel-Mixing_Channel_and_Spatial_Masked_AutoEncoder_on_Facial_Action_WACV_2024_paper.html": {
    "title": "Multimodal Channel-Mixing: Channel and Spatial Masked AutoEncoder on Facial Action Unit Detection",
    "volume": "main",
    "abstract": "Recent studies have focused on utilizing multi-modal data to develop robust models for facial Action Unit (AU) detection. However, the heterogeneity of multi-modal data poses challenges in learning effective representations. One such challenge is extracting relevant features from multiple modalities using a single feature extractor. Moreover, previous studies have not fully explored the potential of multi-modal fusion strategies. In contrast to the extensive work on late fusion, there are limited investigations on early fusion for channel information exploration. This paper presents a novel multi-modal reconstruction network, named Multimodal Channel-Mixing (MCM), as a pre-trained model to learn robust representation for facilitating multi-modal fusion. The approach follows an early fusion setup, integrating a Channel-Mixing module, where two out of five channels are randomly dropped. The dropped channels then are reconstructed from the remaining channels using masked autoencoder. This module not only reduces channel redundancy, but also facilitates multi-modal learning and reconstruction capabilities, resulting in robust feature learning. The encoder is fine-tuned on a downstream task of automatic facial action unit detection. Pretraining experiments were conducted on BP4D+, followed by fine-tuning on BP4D and DISFA to assess the effectiveness and robustness of the proposed framework. The results demonstrate that our method meets and surpasses the performance of state-of-the-art baseline method",
    "checked": true,
    "id": "7e687d3449d15906061ad6ca8058347ea0ad4bbc",
    "semantic_title": "multimodal channel-mixing: channel and spatial masked autoencoder on facial action unit detection",
    "citation_count": 0,
    "authors": [
      "Xiang Zhang",
      "Huiyuan Yang",
      "Taoyue Wang",
      "Xiaotian Li",
      "Lijun Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ko_ZIGNeRF_Zero-Shot_3D_Scene_Representation_With_Invertible_Generative_Neural_Radiance_WACV_2024_paper.html": {
    "title": "ZIGNeRF: Zero-Shot 3D Scene Representation With Invertible Generative Neural Radiance Fields",
    "volume": "main",
    "abstract": "Generative Neural Radiance Fields (NeRFs) have demonstrated remarkable proficiency in synthesizing multi-view images by learning the distribution of a set of unposed images. Despite the aptitude of existing Generative NeRFs in generating 3D-consistent high-quality random samples within data distribution, the creation of a 3D representation of a singular input image remains a formidable challenge. In this manuscript, we introduce ZIGNeRF, an innovative model that executes zero-shot Generative Adversarial Network (GAN) inversion for the generation of multi-view images from a single out-of-distribution image. The model is underpinned by a novel inverter that maps out-of-domain images into the latent code of the generator manifold. Notably, ZIGNeRF is capable of disentangling the object from the background and executing 3D operations such as 360-degree rotation or depth and horizontal translation. The efficacy of our model is validated using multiple real-image datasets: Cats, AFHQ, CelebA, CelebA-HQ, and CompCars",
    "checked": true,
    "id": "417929477a0e3c2f6c44836942e938b3cef61fc2",
    "semantic_title": "zignerf: zero-shot 3d scene representation with invertible generative neural radiance fields",
    "citation_count": 1,
    "authors": [
      "Kanghyeok Ko",
      "Minhyeok Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Lu_SLoSH_Set_Locality_Sensitive_Hashing_via_Sliced-Wasserstein_Embeddings_WACV_2024_paper.html": {
    "title": "SLoSH: Set Locality Sensitive Hashing via Sliced-Wasserstein Embeddings",
    "volume": "main",
    "abstract": "Learning from set-structured data is an essential problem with many applications in machine learning and computer vision. This paper focuses on non-parametric and data-independent learning from set-structured data using approximate nearest neighbor (ANN) solutions, particularly locality-sensitive hashing. We consider the problem of set retrieval from an input set query. Such a retrieval problem requires: 1) an efficient mechanism to calculate the distances/dissimilarities between sets, and 2) an appropriate data structure for fast nearest-neighbor search. To that end, we propose to use Sliced-Wasserstein embedding as a computationally efficient set-2-vector operator that enables downstream ANN, with theoretical guarantees. The set elements are treated as samples from an unknown underlying distribution, and the Sliced-Wasserstein distance is used to compare sets. We demonstrate the effectiveness of our algorithm, denoted as Set Locality Sensitive Hashing (SLoSH), on various set retrieval datasets and compare our proposed embedding with standard set embedding approaches, including Generalized Mean (GeM) embedding/pooling, Featurewise Sort Pooling (FSPool), Covariance Pooling, and Wasserstein embedding and show consistent improvement in retrieval results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhe Lu",
      "Xinran Liu",
      "Andrea Soltoggio",
      "Soheil Kolouri"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Yuan_StreamMapNet_Streaming_Mapping_Network_for_Vectorized_Online_HD_Map_Construction_WACV_2024_paper.html": {
    "title": "StreamMapNet: Streaming Mapping Network for Vectorized Online HD Map Construction",
    "volume": "main",
    "abstract": "High-Definition (HD) maps are essential for the safety of autonomous driving systems. While existing techniques employ camera images and onboard sensors to generate vectorized high-precision maps, they are constrained by their reliance on single-frame input. This approach limits their stability and performance in complex scenarios such as occlusions, largely due to the absence of temporal information. Moreover, their performance diminishes when applied to broader perception ranges. In this paper, we present StreamMapNet, a novel online mapping pipeline adept at long-sequence temporal modeling of videos. StreamMapNet employs multi-point attention and temporal information which empowers the construction of large-range local HD maps with high stability and further addresses the limitations of existing methods. Furthermore, we critically examine widely used online HD Map construction benchmark and datasets, Argoverse2 and nuScenes, revealing significant bias in the existing evaluation protocols. We propose to resplit the benchmarks according to geographical spans, promoting fair and precise evaluations. Experimental results validate that StreamMapNet significantly outperforms existing methods across all settings while maintaining an online inference speed of 14.2 FPS. Our code is available at https://github.com/yuantianyuan01/StreamMapNet",
    "checked": true,
    "id": "4d1d0d01ab55cd37c65e4d29fcd6786e45f153b7",
    "semantic_title": "streammapnet: streaming mapping network for vectorized online hd map construction",
    "citation_count": 4,
    "authors": [
      "Tianyuan Yuan",
      "Yicheng Liu",
      "Yue Wang",
      "Yilun Wang",
      "Hang Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Argaw_Blurry_Video_Compression_A_Trade-Off_Between_Visual_Enhancement_and_Data_WACV_2024_paper.html": {
    "title": "Blurry Video Compression: A Trade-Off Between Visual Enhancement and Data Compression",
    "volume": "main",
    "abstract": "Existing video compression (VC) methods primarily aim to reduce the spatial and temporal redundancies between consecutive frames in a video while preserving its quality. In this regard, previous works have achieved remarkable results on videos acquired under specific settings such as instant (known) exposure time and shutter speed which often result in sharp videos. However, when these methods are evaluated on videos captured under different temporal priors, which lead to degradations like motion blur and low frame rate, they fail to maintain the quality of the contents. In this work, we tackle the VC problem in a general scenario where a given video can be blurry due to predefined camera settings or dynamics in the scene. By exploiting the natural trade-off between visual enhancement and data compression, we formulate VC as a min-max optimization problem and propose an effective framework and training strategy to tackle the problem. Extensive experimental results on several benchmark datasets confirm the effectiveness of our method compared to several state-of-the-art VC approaches",
    "checked": true,
    "id": "6252e510c0e5f2cb257a755712df6a4835c9f15c",
    "semantic_title": "blurry video compression: a trade-off between visual enhancement and data compression",
    "citation_count": 0,
    "authors": [
      "Dawit Mureja Argaw",
      "Junsik Kim",
      "In So Kweon"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wu_Correlation-Aware_Active_Learning_for_Surgery_Video_Segmentation_WACV_2024_paper.html": {
    "title": "Correlation-Aware Active Learning for Surgery Video Segmentation",
    "volume": "main",
    "abstract": "Semantic segmentation is a complex task that relies heavily on large amounts of annotated image data. How- ever, annotating such data can be time-consuming and resource-intensive, especially in the medical domain. Active Learning (AL) is a popular approach that can help to reduce this burden by iteratively selecting images for annotation to improve the model performance. In the case of video data, it is important to consider the model uncertainty and the temporal nature of the sequences when selecting images for annotation. This work proposes a novel AL strategy for surgery video segmentation, COWAL, COrrelation aWare Active Learning. Our approach involves projecting images into a latent space that has been fine-tuned using contrastive learning and then selecting a fixed number of representative images from local clusters of video frames. We demonstrate the effectiveness of this approach on two video datasets of surgical instruments and three real-world video datasets. The datasets and code will be made publicly available upon receiving necessary approvals",
    "checked": true,
    "id": "33bedf6b1bc105103b61aea7592f43a8b3041f1d",
    "semantic_title": "correlation-aware active learning for surgery video segmentation",
    "citation_count": 0,
    "authors": [
      "Fei Wu",
      "Pablo Márquez-Neila",
      "Mingyi Zheng",
      "Hedyeh Rafii-Tari",
      "Raphael Sznitman"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Jeong_EResFD_Rediscovery_of_the_Effectiveness_of_Standard_Convolution_for_Lightweight_WACV_2024_paper.html": {
    "title": "EResFD: Rediscovery of the Effectiveness of Standard Convolution for Lightweight Face Detection",
    "volume": "main",
    "abstract": "This paper analyzes the design choices of face detection architecture that improve efficiency of computation cost and accuracy. Specifically, we re-examine the effectiveness of the standard convolutional block as a lightweight backbone architecture for face detection. Unlike the current tendency of lightweight architecture design, which heavily utilizes depthwise separable convolution layers, we show that heavily channel-pruned standard convolution layers can achieve better accuracy and inference speed when using a similar parameter size. This observation is supported by the analyses concerning the characteristics of the target data domain, faces. Based on our observation, we propose to employ ResNet with a highly reduced channel, which surprisingly allows high efficiency compared to other mobile-friendly networks (e.g., MobileNetV1, V2, V3). From the extensive experiments, we show that the proposed backbone can replace that of the state-of-the-art face detector with a faster inference speed. Also, we further propose a new feature aggregation method to maximize the detection performance. Our proposed detector EResFD obtained 80.4% mAP on WIDER FACE Hard subset which only takes 37.7 ms for VGA image inference on CPU. Code is available at https://github.com/clovaai/EResFD",
    "checked": false,
    "id": "f20366ca194f8baf4c21bbbc3bd3a9ab843f59ce",
    "semantic_title": "rediscovery of the effectiveness of standard convolution for lightweight face detection",
    "citation_count": 2,
    "authors": [
      "Joonhyun Jeong",
      "Beomyoung Kim",
      "Joonsang Yu",
      "YoungJoon Yoo"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Babaiee_Neural_Echos_Depthwise_Convolutional_Filters_Replicate_Biological_Receptive_Fields_WACV_2024_paper.html": {
    "title": "Neural Echos: Depthwise Convolutional Filters Replicate Biological Receptive Fields",
    "volume": "main",
    "abstract": "In this study, we present evidence suggesting that depthwise convolutional kernels are effectively replicating the structural intricacies of the biological receptive fields observed in the mammalian retina. We provide analytics of trained kernels from various state-of-the-art models substantiating this evidence. Inspired by this intriguing discovery, we propose an initialization scheme that draws inspiration from the biological receptive fields. Experimental analysis of the ImageNet dataset with multiple CNN architectures featuring depthwise convolutions reveals a marked enhancement in the accuracy of the learned model when initialized with biologically derived weights. This underlies the potential for biologically inspired computational models to further our understanding of vision processing systems and to improve the efficacy of convolutional networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zahra Babaiee",
      "Peyman M. Kiasari",
      "Daniela Rus",
      "Radu Grosu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Keshtkaran_Estimating_Blood_Alcohol_Level_Through_Facial_Features_for_Driver_Impairment_WACV_2024_paper.html": {
    "title": "Estimating Blood Alcohol Level Through Facial Features for Driver Impairment Assessment",
    "volume": "main",
    "abstract": "Drunk driving-related road accidents contribute significantly to the global burden of road injuries. Addressing alcohol-related harm, particularly during safety-critical activities like driving, requires real-time monitoring of an individual's blood alcohol concentration (BAC). We devise an in-vehicle machine learning system that harnesses standard commercial RGB cameras to predict critical levels of BAC. Our system can detect instances of alcohol intoxication impairment as subtle as 0.05 g/dL (WHO recommended legal limit for driving), with an accuracy of 75%, by leveraging the physiological manifestations of alcohol intoxication on a driver's face. This system holds great promise for improving road safety. In tandem, we have compiled a data set of 60 subjects engaged in simulated driving scenarios, spanning three levels of alcohol intoxication. These scenarios were captured and divided into video segments labeled \"sober\", \"low\", and \"severe\" Alcohol Intoxication Impairment (AII), constituting the basis for evaluating our system's performance. To the best of our knowledge, this study is the first to create a large-scale real-life dataset of alcohol intoxication and assess intoxication levels using an off-the-shelf RGB camera to detect drunk driving",
    "checked": true,
    "id": "20bdc6dfc90347c5a1b880766d58d3b10f3fd456",
    "semantic_title": "estimating blood alcohol level through facial features for driver impairment assessment",
    "citation_count": 0,
    "authors": [
      "Ensiyeh Keshtkaran",
      "Brodie von Berg",
      "Grant Regan",
      "David Suter",
      "Syed Zulqarnain Gilani"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Saffi_Auto-BPA_An_Enhanced_Ball-Pivoting_Algorithm_With_Adaptive_Radius_Using_Contextual_WACV_2024_paper.html": {
    "title": "Auto-BPA: An Enhanced Ball-Pivoting Algorithm With Adaptive Radius Using Contextual Bandits",
    "volume": "main",
    "abstract": "The Ball-Pivoting Algorithm (BPA) is a notable technique for 3D surface reconstruction from point clouds, heavily reliant on the ball radius. In practical application, determining the optimal radius for BPA often necessitates iterative experimentation to achieve better reconstruction quality. BPA entails geometric computations like iterative pivoting, inherently lacking differentiability. In this paper, we tackle the dual challenges of radius selection and non-differentiability in BPA. Inspired by contextual bandits, we propose an innovative approach that learns the optimal radius based on local geometric features within point clouds. We validate our method on the ModelNet10 and ShapeNet datasets, showcasing superior surface reconstruction compared to manual tuning and other classic methods both for low and high point cloud densities. Our code is available at https://github.com/houda-pixel/Auto-BPA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Houda Saffi",
      "Naima Otberdout",
      "Youssef Hmamouche",
      "Amal El Fallah Seghrouchni"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kawamura_MIDAS_Mixing_Ambiguous_Data_With_Soft_Labels_for_Dynamic_Facial_WACV_2024_paper.html": {
    "title": "MIDAS: Mixing Ambiguous Data With Soft Labels for Dynamic Facial Expression Recognition",
    "volume": "main",
    "abstract": "Dynamic facial expression recognition (DFER) is an important task in the field of computer vision. To apply automatic DFER in practice, it is necessary to accurately recognize ambiguous facial expressions, which often appear in data in the wild. In this paper, we propose MIDAS, a data augmentation method for DFER, which augments ambiguous facial expression data with soft labels consisting of probabilities for multiple emotion classes. In MIDAS, the training data are augmented by convexly combining pairs of video frames and their corresponding emotion class labels, which can also be regarded as an extension of mixup to soft-labeled video data. This simple extension is remarkably effective in DFER with ambiguous facial expression data. To evaluate MIDAS, we conducted experiments on the DFEW dataset. The results demonstrate that the model trained on the data augmented by MIDAS outperforms the existing state-of-the-art method trained on the original dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryosuke Kawamura",
      "Hideaki Hayashi",
      "Noriko Takemura",
      "Hajime Nagahara"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/van_Rozendaal_MobileNVC_Real-Time_1080p_Neural_Video_Compression_on_a_Mobile_Device_WACV_2024_paper.html": {
    "title": "MobileNVC: Real-Time 1080p Neural Video Compression on a Mobile Device",
    "volume": "main",
    "abstract": "Neural video codecs have recently become competitive with standard codecs such as HEVC in the low-delay setting. However, most neural codecs are large floating-point networks that use pixel-dense warping operations for temporal modeling, making them too computationally expensive for deployment on mobile devices. Recent work has demonstrated that running a neural decoder in real time on mobile is feasible, but shows this only for 720p RGB video. This work presents the first neural video codec that decodes 1080p YUV420 video in real time on a mobile device. Our codec relies on two major contributions. First, we design an efficient codec that uses a block-based motion compensation algorithm available on the warping core of the mobile accelerator, and we show how to quantize this model to integer precision. Second, we implement a fast decoder pipeline that concurrently runs neural network components on the neural signal processor, parallel entropy coding on the mobile GPU, and warping on the warping core. Our codec outperforms the previous on-device codec by a large margin with up to 48 % BD-rate savings, while reducing the MAC count on the receiver side by 10x. We perform a careful ablation to demonstrate the effect of the introduced motion compensation scheme, and ablate the effect of model quantization",
    "checked": true,
    "id": "1244296d5d10604519637fb30c9e34a1976686ec",
    "semantic_title": "mobilenvc: real-time 1080p neural video compression on a mobile device",
    "citation_count": 1,
    "authors": [
      "Ties van Rozendaal",
      "Tushar Singhal",
      "Hoang Le",
      "Guillaume Sautiere",
      "Amir Said",
      "Krishna Buska",
      "Anjuman Raha",
      "Dimitris Kalatzis",
      "Hitarth Mehta",
      "Frank Mayer",
      "Liang Zhang",
      "Markus Nagel",
      "Auke Wiggers"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wang_Improving_the_Effectiveness_of_Deep_Generative_Data_WACV_2024_paper.html": {
    "title": "Improving the Effectiveness of Deep Generative Data",
    "volume": "main",
    "abstract": "Recent deep generative models (DGMs) such as generative adversarial networks (GANs) and diffusion probabilistic models (DPMs) have shown their impressive ability in generating high-fidelity photorealistic images. Although looking appealing to human eyes, training a model on purely synthetic images for downstream image processing tasks like image classification often results in an undesired performance drop compared to training on real data. Previous works have demonstrated that enhancing a real dataset with synthetic images from DGMs can be beneficial. However, the improvements were subjected to certain circumstances and yet were not comparable to adding the same number of real images. In this work, we propose a new taxonomy to describe factors contributing to this commonly observed phenomenon and investigate it on the popular CIFAR-10 dataset. We hypothesize that the Content Gap accounts for a large portion of the performance drop when using synthetic images from DGM and propose strategies to better utilize them in downstream tasks. Extensive experiments on multiple datasets showcase that our method outperforms baselines on downstream classification tasks both in case of training on synthetic only (Synthetic-to-Real) and training on a mix of real and synthetic data (Data Augmentation), particularly in the data-scarce scenario",
    "checked": true,
    "id": "61553e2b71489bf80f7c9b662d8e56843e537538",
    "semantic_title": "improving the effectiveness of deep generative data",
    "citation_count": 0,
    "authors": [
      "Ruyu Wang",
      "Sabrina Schmedding",
      "Marco F. Huber"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wu_Learning_Better_Keypoints_for_Multi-Object_6DoF_Pose_Estimation_WACV_2024_paper.html": {
    "title": "Learning Better Keypoints for Multi-Object 6DoF Pose Estimation",
    "volume": "main",
    "abstract": "We address the problem of keypoint selection, and find that the performance of 6DoF pose estimation methods can be improved when pre-defined keypoint locations are learned, rather than being heuristically selected as has been the standard approach. We found that accuracy and efficiency can be improved by training a graph network to select a set of disperse keypoints with similarly distributed votes. These votes, learned by a regression network to accumulate evidence for the keypoint locations, can be regressed more accurately compared to previous heuristic keypoint algorithms. The proposed KeyGNet, supervised by a combined loss measuring both Wasserstein distance and dispersion, learns the color and geometry features of the target objects to estimate optimal keypoint locations. Experiments demonstrate the keypoints selected by KeyGNet improved the accuracy for all evaluation metrics of all seven datasets tested, for three keypoint voting methods. The challenging Occlusion LINEMOD dataset notably improved ADD(S) by +16.4% on PVN3D, and all core BOP datasets showed an AR improvement for all objects, of between +1% and +21.5%. There was also a notable increase in performance when transitioning from single object to multiple object training using KeyGNet keypoints, essentially eliminating the SISO-MIMO gap for Occlusion LINEMOD",
    "checked": true,
    "id": "8e16083ecd6d166f2eb667852e72c7fe913f18a3",
    "semantic_title": "learning better keypoints for multi-object 6dof pose estimation",
    "citation_count": 2,
    "authors": [
      "Yangzheng Wu",
      "Michael Greenspan"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhu_Unsupervised_Graphic_Layout_Grouping_With_Transformers_WACV_2024_paper.html": {
    "title": "Unsupervised Graphic Layout Grouping With Transformers",
    "volume": "main",
    "abstract": "Graphic design conveys messages through the combination of text, images and other visual elements. Unstructured designs such as overloaded social media graphics may fail to communicate their intended messages effectively. To address this issue, layout grouping offers a solution by organizing design elements into perceptual groups. While most methods rely on heuristic Gestalt principles, they often lack the context modeling ability needed to handle complex layouts. In this work, we reformulate the layout grouping task as a set prediction problem. It uses Transformers to learn a set of group tokens at various hierarchies, enabling it to reason the membership of the elements more effectively. The self-attention mechanism in Transformers boosts its context modeling ability, which enables it to handle complex layouts more accurately. To reduce annotation costs, we also propose an unsupervised learning strategy that pre-trains on noisy pseudo-labels induced by a novel heuristic algorithm. This approach then bootstraps to self-refine the noisy labels, further improving the accuracy of our model. Our extensive experiments demonstrate the effectiveness of our method, which outperforms existing state-of-the-art approaches in terms of accuracy and efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialiang Zhu",
      "Danqing Huang",
      "Chunyu Wang",
      "Mingxi Cheng",
      "Ji Li",
      "Han Hu",
      "Xin Geng",
      "Baining Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Can_Vision-Language_Models_Be_a_Good_Guesser_Exploring_VLMs_for_WACV_2024_paper.html": {
    "title": "Can Vision-Language Models Be a Good Guesser? Exploring VLMs for Times and Location Reasoning",
    "volume": "main",
    "abstract": "Vision-Language Models (VLMs) are expected to be capable of reasoning with commonsense knowledge as human beings. One example is that humans can reason where and when an image is taken based on their knowledge. This makes us wonder if, based on visual cues, Vision-Language Models that are pre-trained with large-scale image-text resources can achieve and even surpass human capability in reasoning times and location. To address this question, we propose a two-stage Recognition & Reasoning probing task applied to discriminative and generative VLMs to uncover whether VLMs can recognize times and location-relevant features and further reason about it. To facilitate the studies, we introduce WikiTiLo, a well-curated image dataset compromising images with rich socio-cultural cues. In extensive evaluation experiments, we find that although VLMs can effectively retain times and location-relevant features in visual encoders, they still fail to make perfect reasoning with context-conditioned visual features. The dataset is available at https://github.com/gengyuanmax/WikiTiLo",
    "checked": true,
    "id": "a865f897eacb220c85dbc16977e5192e44556e31",
    "semantic_title": "can vision-language models be a good guesser? exploring vlms for times and location reasoning",
    "citation_count": 0,
    "authors": [
      "Gengyuan Zhang",
      "Yurui Zhang",
      "Kerui Zhang",
      "Volker Tresp"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Cao_What_Decreases_Editing_Capability_Domain-Specific_Hybrid_Refinement_for_Improved_GAN_WACV_2024_paper.html": {
    "title": "What Decreases Editing Capability? Domain-Specific Hybrid Refinement for Improved GAN Inversion",
    "volume": "main",
    "abstract": "Recently, inversion methods have been exploring the incorporation of additional high-rate information from pretrained generators (such as weights or intermediate features) to improve the refinement of inversion and editing results from embedded latent codes. While such techniques have shown reasonable improvements in reconstruction, they often lead to a decrease in editing capability, especially when dealing with complex images that contain occlusions, detailed backgrounds, and artifacts. A vital crux is refining inversion results, avoiding editing capability degradation. To address this problem, we propose a novel refinement mechanism called Domain-Specific Hybrid Refinement (DHR), which draws on the advantages and disadvantages of two mainstream refinement techniques. We find that the weight modulation can gain favorable editing results but is vulnerable to these complex image areas and feature modulation is efficient at reconstructing. Hence, we divide the image into two domains and process them with these two methods separately. We first propose a Domain-Specific Segmentation module to automatically segment images into in-domain and out-of-domain parts according to their invertibility and editability without additional data annotation, where our hybrid refinement process aims to maintain the editing capability for in-domain areas and improve fidelity for both of them. We achieve this through Hybrid Modulation Refinement, which respectively refines these two domains by weight modulation and feature modulation. Our proposed method is compatible with all latent code embedding methods. Extension experiments demonstrate that our approach achieves state-of-the-art in real image inversion and editing. Code is available at https://github.com/caopulan/Domain-Specific_Hybrid_Refinement_Inversion",
    "checked": true,
    "id": "259cf9356e90d536aba42c026b1192586a6a6cab",
    "semantic_title": "what decreases editing capability? domain-specific hybrid refinement for improved gan inversion",
    "citation_count": 2,
    "authors": [
      "Pu Cao",
      "Lu Yang",
      "Dongxv Liu",
      "Xiaoya Yang",
      "Tianrui Huang",
      "Qing Song"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Chen_Longformer_Longitudinal_Transformer_for_Alzheimers_Disease_Classification_With_Structural_MRIs_WACV_2024_paper.html": {
    "title": "Longformer: Longitudinal Transformer for Alzheimer's Disease Classification With Structural MRIs",
    "volume": "main",
    "abstract": "Structural magnetic resonance imaging (sMRI), especially longitudinal sMRI, is often used to monitor and capture disease progression during the clinical diagnosis of Alzheimer's Disease (AD). However, current methods neglect AD's progressive nature and have mostly relied on a single image for recognizing AD. In this paper, we consider the problem of leveraging the longitudinal MRIs of a subject for AD classification. To address the challenges of missing data, data demand, and subtle changes over time in learning longitudinal 3D MRIs, we propose a novel model LongFormer, which is a hybrid 3D CNN and transformer design to learn from image and longitudinal flow pairs. Our model can fully leverage all images in a dataset and effectively fuse spatiotemporal features for classification. We evaluate our model on three datasets, i.e., ADNI, OASIS, and AIBL, and compare it to eight baseline algorithms. Our proposed LongFormer achieves state-of-the-art performance in classifying AD and NC subjects from all these three public datasets. Our source code is available online",
    "checked": true,
    "id": "13c4f0a7c25c94075f0e9fec6d2b8d019b8d3b47",
    "semantic_title": "longformer: longitudinal transformer for alzheimer's disease classification with structural mris",
    "citation_count": 1,
    "authors": [
      "Qiuhui Chen",
      "Qiang Fu",
      "Hao Bai",
      "Yi Hong"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Park_Grafting_Vision_Transformers_WACV_2024_paper.html": {
    "title": "Grafting Vision Transformers",
    "volume": "main",
    "abstract": "Vision Transformers (ViTs) have recently become the state-of-the-art across many computer vision tasks. In contrast to convolutional networks (CNNs), ViTs enable global information sharing even within shallow layers of a network, i.e., among high-resolution features. However, this perk was later overlooked with the success of pyramid architectures such as Swin Transformer, which show better performance-complexity trade-offs. In this paper, we present a simple and efficient add-on component (termed GrafT) that considers global dependencies and multi-scale information throughout the network, in both high- and low-resolution features alike. It has the flexibility of branching out at arbitrary depths and shares most of the parameters and computations of the backbone. GrafT shows consistent gains over various well-known models which includes both hybrid and pure Transformer types, both homogeneous and pyramid structures, and various self-attention methods. In particular, it largely benefits mobile-size models by providing high-level semantics. On the ImageNet-1k dataset, GrafT delivers +3.9%, +1.4%, and +1.9% top-1 accuracy improvement to DeiT-T, Swin-T, and MobileViT-XXS, respectively. The code and models are at https://github.com/jongwoopark7978/Grafting-Vision-Transformer",
    "checked": true,
    "id": "b6115e422e59c464db2d8257724c66827202b6c8",
    "semantic_title": "grafting vision transformers",
    "citation_count": 0,
    "authors": [
      "Jongwoo Park",
      "Kumara Kahatapitiya",
      "Donghyun Kim",
      "Shivchander Sudalairaj",
      "Quanfu Fan",
      "Michael S. Ryoo"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Sinha_Hardware_Aware_Evolutionary_Neural_Architecture_Search_Using_Representation_Similarity_Metric_WACV_2024_paper.html": {
    "title": "Hardware Aware Evolutionary Neural Architecture Search Using Representation Similarity Metric",
    "volume": "main",
    "abstract": "Hardware-aware Neural Architecture Search (HW-NAS) is a technique used to automatically design the architecture of a neural network for a specific task and target hardware. However, evaluating the performance of candidate architectures is a key challenge in HW-NAS, as it requires significant computational resources. To address this challenge, we propose an efficient hardware-aware evolution-based NAS approach called HW-EvRSNAS. Our approach re-frames the neural architecture search problem as finding an architecture with performance similar to that of a reference model for a target hardware, while adhering to a cost constraint for that hardware. This is achieved through a representation similarity metric known as Representation Mutual Information (RMI) employed as a proxy performance evaluator. It measures the mutual information between the hidden layer representations of a reference model and those of sampled architectures using a single training batch. We also use a penalty term that penalizes the search process in proportion to how far an architecture's hardware cost is from the desired hardware cost threshold. This resulted in a significantly reduced search time compared to the literature that reached up to 8000x speedups resulting in lower CO2 emissions. The proposed approach is evaluated on two different search spaces while using lower computational resources. Furthermore, our approach is thoroughly examined on six different edge devices under various hardware cost constraints",
    "checked": true,
    "id": "3a39cf5351a9609c81dd3f59b3b05835d252dbf7",
    "semantic_title": "hardware aware evolutionary neural architecture search using representation similarity metric",
    "citation_count": 0,
    "authors": [
      "Nilotpal Sinha",
      "Abd El Rahman Shabayek",
      "Anis Kacem",
      "Peyman Rostami",
      "Carl Shneider",
      "Djamila Aouada"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhang_DECDM_Document_Enhancement_Using_Cycle-Consistent_Diffusion_Models_WACV_2024_paper.html": {
    "title": "DECDM: Document Enhancement Using Cycle-Consistent Diffusion Models",
    "volume": "main",
    "abstract": "The performance of optical character recognition (OCR) heavily relies on document image quality, which is crucial for automatic document processing and document intelligence. However, most existing document enhancement methods require supervised data pairs, which raises concerns about data separation and privacy protection, and makes it challenging to adapt these methods to new domain pairs. To address these issues, we propose DECDM, an end-to-end document-level image translation method inspired by recent advances in diffusion models. Our method overcomes the limitations of paired training by independently training the source (noisy input) and target (clean output) models, making it possible to apply domain-specific diffusion models to other pairs. DECDM trains on one dataset at a time, eliminating the need to scan both datasets concurrently, and effectively preserving data privacy from the source or target domain. We also introduce simple data augmentation strategies to improve character-glyph conservation during translation. We compare DECDM with state-of-the-art methods on multiple synthetic data and benchmark datasets, such as document denoising and shadow removal, and demonstrate the superiority of performance quantitatively and qualitatively",
    "checked": true,
    "id": "3169b6cb12b0412c3dcbd5fdd6e2052100f793a1",
    "semantic_title": "decdm: document enhancement using cycle-consistent diffusion models",
    "citation_count": 0,
    "authors": [
      "Jiaxin Zhang",
      "Joy Rimchala",
      "Lalla Mouatadid",
      "Kamalika Das",
      "Sricharan Kumar"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Habib_Watch_Where_You_Head_A_View-Biased_Domain_Gap_in_Gait_WACV_2024_paper.html": {
    "title": "Watch Where You Head: A View-Biased Domain Gap in Gait Recognition and Unsupervised Adaptation",
    "volume": "main",
    "abstract": "Gait Recognition is a computer vision task aiming to identify people by their walking patterns. Although existing methods often show high performance on specific datasets, they lack the ability to generalize to unseen scenarios. Unsupervised Domain Adaptation (UDA) tries to adapt a model, pre-trained in a supervised manner on a source domain, to an unlabelled target domain. There are only a few works on UDA for gait recognition proposing solutions to limited scenarios. In this paper, we reveal a fundamental phenomenon in adaptation of gait recognition models, caused by the bias in the target domain to viewing angle or walking direction. We then suggest a remedy to reduce this bias with a novel triplet selection strategy combined with curriculum learning. To this end, we present Gait Orientation-based method for Unsupervised Domain Adaptation (GOUDA). We provide extensive experiments on four widely-used gait datasets, CASIA-B, OU-MVLP, GREW, and Gait3D, and on three backbones, GaitSet, GaitPart, and GaitGL, justifying the view bias and showing the superiority of our proposed method over prior UDA works",
    "checked": true,
    "id": "1f3d4f4a051e80ef4c16eec95bc7abf030cdc624",
    "semantic_title": "watch where you head: a view-biased domain gap in gait recognition and unsupervised adaptation",
    "citation_count": 0,
    "authors": [
      "Gavriel Habib",
      "Noa Barzilay",
      "Or Shimshi",
      "Rami Ben-Ari",
      "Nir Darshan"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Chen_Show_Your_Face_Restoring_Complete_Facial_Images_From_Partial_Observations_WACV_2024_paper.html": {
    "title": "Show Your Face: Restoring Complete Facial Images From Partial Observations for VR Meeting",
    "volume": "main",
    "abstract": "Virtual Reality (VR) headsets allow users to interact with the virtual world. However, the device physically blocks visual connections among users, causing huge inconveniences for VR meetings. To address this issue, studies have been conducted to restore human faces from images captured by Headset Mounted Cameras (HMC). Unfortunately, existing approaches heavily rely on high-resolution person-specific 3D models which are prohibitively expensive to apply to large-scale scenarios. Our goal is to design an efficient framework for restoring users' facial data in VR meetings. Specifically, we first build a new dataset, named Facial Image Composition (FIC) data which approximates the real HMC images from a VR headset. By leveraging the heterogeneity of the HMC images, we decompose the restoration problem into a local geometry transformation and global color/style fusion. Then we propose a 2D light-weight facial image composition network (FIC-Net), where three independent local models are responsible for transforming raw HMC patches and the global model performs a fusion of the transformed HMC patches with a pre-recorded reference image. Finally, we also propose a stage-wise training strategy to optimize the generalization of our FIC-Net. We have validated the effectiveness of our proposed FIC-Net through extensive experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Chen",
      "Zhiqi Zhang",
      "Junsong Yuan",
      "Yi Xu",
      "Lantao Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Chaudhury_Shape_From_Shading_for_Robotic_Manipulation_WACV_2024_paper.html": {
    "title": "Shape From Shading for Robotic Manipulation",
    "volume": "main",
    "abstract": "Controlling illumination can generate high quality information about object surface normals and depth discontinuities at a low computational cost. In this work we demonstrate a robot workspace-scaled controlled illumination approach that generates high quality information for table top scale objects for robotic manipulation. With our low angle of incidence directional illumination approach, we can precisely capture surface normals and depth discontinuities of monochromatic Lambertian objects. We show that this approach to shape estimation is 1) valuable for general purpose grasping with a single point vacuum gripper, 2) can measure the deformation of known objects, and 3) can estimate pose of known objects and track unknown objects in the robot's workspace",
    "checked": true,
    "id": "d1e132ab2e7b992083269171fb26cc9fbc852651",
    "semantic_title": "shape from shading for robotic manipulation",
    "citation_count": 0,
    "authors": [
      "Arkadeep Narayan Chaudhury",
      "Leonid Keselman",
      "Christopher G. Atkeson"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Yasarla_Self-Supervised_Denoising_Transformer_With_Gaussian_Process_WACV_2024_paper.html": {
    "title": "Self-Supervised Denoising Transformer With Gaussian Process",
    "volume": "main",
    "abstract": "Convolutional neural network (CNN) based methods have been the main focus of recent developments for image denoising. However, these methods lack majorly in two ways: 1) They require a large amount of labeled data to perform well. 2) They do not have a good global understanding due to convolutional inductive biases. Recent emergence of Transformers and self-supervised learning methods have focused on tackling these issues. In this work, we address both these issues for image denoising and propose a new method: Self-Supervised denoising Transformer (SST-GP) with Gaussian Process. Our novelties are two fold: First, we propose a new way of doing self-supervision by incorporating Gaussian Processes (GP). Given a noisy image, we generate multiple noisy down-sampled images with random cyclic shifts. Using GP, we formulate a joint Gaussian distribution between these down-sampled images and learn the relation between their corresponding denoising function mappings to predict the pseudo-Ground truth (pseudo-GT) for each of the down-sampled images. This enables the network to learn noise present in the down-sampled images and achieve better denoising performance by using the joint relationship between down-sampled images with help of GP. Second, we propose a new transformer architecture - Denoising Transformer (Den-T) which is tailor-made for denoising application. Den-T has two transformer encoder branches - one which focuses on extracting fine context details and another to extract coarse context details. This helps Den-T to attend to both local and global information to effectively denoise the image. Finally, we train Den-T using the proposed self-supervised strategy using GP and achieve a better performance over recent unsupervised/self-supervised denoising approaches when validated on various denoising datasets like Kodak, BSD, Set-14 and SIDD. Codes will be made public after review",
    "checked": false,
    "id": "cd87298895146633e4578ee21ef4fa022b8c75b3",
    "semantic_title": "a self-supervised denoising method based on deep noise estimation",
    "citation_count": 1,
    "authors": [
      "Rajeev Yasarla",
      "Jeya Maria Jose Valanarasu",
      "Vishwanath Sindagi",
      "Vishal M. Patel"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhao_SemST_Semantically_Consistent_Multi-Scale_Image_Translation_via_Structure-Texture_Alignment_WACV_2024_paper.html": {
    "title": "SemST: Semantically Consistent Multi-Scale Image Translation via Structure-Texture Alignment",
    "volume": "main",
    "abstract": "Unsupervised image-to-image translation learns cross-domain image mapping that transfers input from the source domain to output in the target domain while preserving its semantics. One challenge is that different semantic statistics in source and target domains result in content discrepancy known as semantic distortion. To address this problem, a novel I2I method that maintains semantic consistency in translation is proposed and named SemST in this work. SemST reduces semantic distortion by employing contrastive learning and aligning the structural and textural properties of input and output by maximizing their mutual information. Furthermore, a multi-scale approach is introduced to enhance translation performance, thereby enabling the applicability of SemST to domain adaptation in high-resolution images. Experiments show that SemST effectively mitigates semantic distortion and achieves state-of-the-art performance. Also, the application of SemST to domain adaptation is explored. It is demonstrated by preliminary experiments that SemST can be utilized as a beneficial pre-training for the semantic segmentation task",
    "checked": true,
    "id": "f733abce20bafb5c5090e18a4217c56aa5bae892",
    "semantic_title": "semst: semantically consistent multi-scale image translation via structure-texture alignment",
    "citation_count": 0,
    "authors": [
      "Ganning Zhao",
      "Wenhui Cui",
      "Suya You",
      "C.-C. Jay Kuo"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Mounsaveng_Bag_of_Tricks_for_Fully_Test-Time_Adaptation_WACV_2024_paper.html": {
    "title": "Bag of Tricks for Fully Test-Time Adaptation",
    "volume": "main",
    "abstract": "Fully Test-Time Adaptation (TTA), which aims at adapting models to data drifts, has recently attracted wide interest. Numerous tricks and techniques have been proposed to ensure robust learning on arbitrary streams of unlabeled data. However, assessing the true impact of each individual technique and obtaining a fair comparison still constitutes a significant challenge. To help consolidate the community's knowledge, we present a categorization of selected orthogonal TTA techniques, including small batch normalization, stream rebalancing, reliable sample selection, and network confidence calibration. We meticulously dissect the effect of each approach on different scenarios of interest. Through our analysis, we shed light on trade-offs induced by those techniques between accuracy, the computational power required, and model complexity. We also uncover the synergy that arises when combining techniques and are able to establish new state-of-the-art results",
    "checked": true,
    "id": "02719390fe42401f3f8facd949d35bb2572b377e",
    "semantic_title": "bag of tricks for fully test-time adaptation",
    "citation_count": 2,
    "authors": [
      "Saypraseuth Mounsaveng",
      "Florent Chiaroni",
      "Malik Boudiaf",
      "Marco Pedersoli",
      "Ismail Ben Ayed"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Majhi_OE-CTST_Outlier-Embedded_Cross_Temporal_Scale_Transformer_for_Weakly-Supervised_Video_Anomaly_WACV_2024_paper.html": {
    "title": "OE-CTST: Outlier-Embedded Cross Temporal Scale Transformer for Weakly-Supervised Video Anomaly Detection",
    "volume": "main",
    "abstract": "Video anomaly detection in real-world scenarios is challenging due to the complex temporal blending of long and short-length anomalies with normal ones. Further, it is more difficult to detect those due to : (i) Distinctive features characterizing the short and long anomalies with sharp and progressive temporal cues respectively; (ii) Lack of precise temporal information (i.e. weak-supervision) limits the temporal dynamics modeling of anomalies from normal events. In this paper, we propose a novel 'temporal transformer' framework for weakly-supervised anomaly detection: OE-CTST. The proposed framework has two major components: (i) Outlier Embedder (OE) and (ii) Cross Temporal Scale Transformer (CTST). First, OE generates anomaly-aware temporal position encoding to allow the transformer to effectively model the temporal dynamics among the anomalies and normal events. Second, CTST encodes the cross-correlation between multi-temporal scale features to benefit short and long length anomalies by modeling the global temporal relations. The proposed OE-CTST is validated on three publicly available datasets i.e. UCF-Crime, XD-Violence, and IITB-Corridor, outperforming recently reported state-of-the-art approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Snehashis Majhi",
      "Rui Dai",
      "Quan Kong",
      "Lorenzo Garattoni",
      "Gianpiero Francesca",
      "François Brémond"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Haslum_Bridging_Generalization_Gaps_in_High_Content_Imaging_Through_Online_Self-Supervised_WACV_2024_paper.html": {
    "title": "Bridging Generalization Gaps in High Content Imaging Through Online Self-Supervised Domain Adaptation",
    "volume": "main",
    "abstract": "High Content Imaging (HCI) plays a vital role in modern drug discovery and development pipelines, facilitating various stages from hit identification to candidate drug characterization. Applying machine learning models to these datasets can prove challenging as they typically consist of multiple batches, affected by experimental variation, especially if different imaging equipment have been used. Moreover, as new data arrive, it is preferable that they are analyzed in an online fashion. To overcome this, we propose CODA, an online self-supervised domain adaptation approach. CODA divides the classifier's role into a generic feature extractor and a task-specific model. We adapt the feature extractor's weights to the new domain using cross-batch self-supervision while keeping the task-specific model unchanged. Our results demonstrate that this strategy significantly reduces the generalization gap, achieving up to a 300% improvement when applied to data from different labs utilizing different microscopes. CODA can be applied to new, unlabeled out-of-domain data sources of different sizes, from a single plate to multiple experimental batches",
    "checked": true,
    "id": "bc1d536ff8d2065c70db32f750dc3db8c49681e1",
    "semantic_title": "bridging generalization gaps in high content imaging through online self-supervised domain adaptation",
    "citation_count": 0,
    "authors": [
      "Johan Fredin Haslum",
      "Christos Matsoukas",
      "Karl-Johan Leuchowius",
      "Kevin Smith"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Tiwari_Using_Early_Readouts_To_Mediate_Featural_Bias_in_Distillation_WACV_2024_paper.html": {
    "title": "Using Early Readouts To Mediate Featural Bias in Distillation",
    "volume": "main",
    "abstract": "Deep networks tend to learn spurious feature-label correlations in real-world supervised learning tasks. This vulnerability is aggravated in distillation, where a student model may have lesser representational capacity than the corresponding teacher model. Often, knowledge of specific spurious correlations is used to reweight instances & rebalance the learning process. We propose a novel early readout mechanism whereby we attempt to predict the label using representations from earlier network layers. We show that these early readouts automatically identify problem instances or groups in the form of confident, incorrect predictions. Leveraging these signals to modulate the distillation loss on an instance level allows us to substantially improve not only group fairness measures across benchmark datasets, but also overall accuracy of the student model. We also provide secondary analyses that bring insight into the role of feature learning in supervision and distillation",
    "checked": true,
    "id": "fe9546f728066c543850311ff0ebe4391a4e8951",
    "semantic_title": "using early readouts to mediate featural bias in distillation",
    "citation_count": 0,
    "authors": [
      "Rishabh Tiwari",
      "Durga Sivasubramanian",
      "Anmol Mekala",
      "Ganesh Ramakrishnan",
      "Pradeep Shenoy"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Atanyan_Continuous_Adaptation_for_Interactive_Segmentation_Using_Teacher-Student_Architecture_WACV_2024_paper.html": {
    "title": "Continuous Adaptation for Interactive Segmentation Using Teacher-Student Architecture",
    "volume": "main",
    "abstract": "Interactive segmentation is the task of segmenting objects or regions of interest from images based on user annotations. While most current methods perform effectively on images from the same distribution as the training dataset, they suffer to generalize on unseen domains. To address this issue some approaches incorporate test-time adaptation techniques which, on the other hand, may lead to catastrophic forgetting (i.e. degrading the performance on the previously seen domains) when applied on datasets from various domains sequentially.In this paper, we propose a novel domain adaptation approach leveraging a teacher-student learning framework to tackle the catastrophic forgetting issue. Continuously updating the student and teacher models based on user clicks results in improved segmentation accuracy on unseen domains, while preserving comparable performance on previous domains.Our approach is evaluated on a sequence of datasets from unseen domains (i.e. medical, aerial images, etc.), and, after adaptation, on the source domain demonstrating a significant decline of catastrophic forgetting (e.g. from 55% to 4% on Berkeley dataset)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Barsegh Atanyan",
      "Levon Khachatryan",
      "Shant Navasardyan",
      "Yunchao Wei",
      "Humphrey Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Venkataramani_Causal_Feature_Alignment_Learning_To_Ignore_Spurious_Background_Features_WACV_2024_paper.html": {
    "title": "Causal Feature Alignment: Learning To Ignore Spurious Background Features",
    "volume": "main",
    "abstract": "Deep neural networks are susceptible to spurious features strongly correlating with the target. This phenomenon leads to sub-optimal performance during real-world deployment where the spurious correlations do not exist, leading to deployment challenges in safety-critical environments like healthcare, autonomous navigation etc. While spurious features can correlate with causal features in myriad ways, we propose a solution for a common manifestation in computer vision where the background corresponds to a spurious feature. In contrast to previous works, we do not require apriori knowledge of different sub-groups in the data induced by the presence/absence of spurious features and the corresponding access to samples from these sub-groups. Our proposed method, Causal Feature Alignment (CFA), utilizes segmentation of foreground (a proxy for the causal component) on a small subset of training examples to align the representations of the original images to match words from only causal elements. We first demonstrate the validity of the proposed method on semi-synthetic data. Subsequently, we obtain state-of-the-art results on worst-group accuracy (93%) on the benchmark dataset of Waterbirds using CFA. Furthermore, we demonstrate significant gains of 6% on the Backgrounds Challenge. Finally, we show that utilizing the recently released foundational methods can alleviate the requirement of dense segmentation and can be substituted with weaker modes of human input like bounding boxes, clicks etc., without any performance loss compared to the original CFA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rahul Venkataramani",
      "Parag Dutta",
      "Vikram Melapudi",
      "Ambedkar Dukkipati"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Abdessaied_VD-GR_Boosting_Visual_Dialog_With_Cascaded_Spatial-Temporal_Multi-Modal_Graphs_WACV_2024_paper.html": {
    "title": "VD-GR: Boosting Visual Dialog With Cascaded Spatial-Temporal Multi-Modal Graphs",
    "volume": "main",
    "abstract": "We propose VD-GR -- a novel visual dialog model that combines pre-trained language models (LMs) with graph neural networks (GNNs). Prior works mainly focused on one class of models at the expense of the other, thus missing out on the opportunity of combining their respective benefits. At the core of VD-GR is a novel integration mechanism that alternates between spatial-temporal multi-modal GNNs and BERT layers, and that covers three distinct contributions: First, we use multi-modal GNNs to process the features of each modality (image, question, and dialog history) and exploit their local structures before performing BERT global attention. Second, we propose hub-nodes that link to all other nodes within one modality graph, allowing the model to propagate information from one GNN (modality) to the other in a cascaded manner. Third, we augment the BERT hidden states with fine-grained multi-modal GNN features before passing them to the next VD-GR layer. Evaluations on VisDial v1.0, VisDial v0.9, VisDialConv, and VisPro show that VD-GR achieves new state-of-the-art results on all datasets",
    "checked": true,
    "id": "38518ea139671aaf5269f7a7f1d974a20b829e8a",
    "semantic_title": "vd-gr: boosting visual dialog with cascaded spatial-temporal multi-modal graphs",
    "citation_count": 0,
    "authors": [
      "Adnen Abdessaied",
      "Lei Shi",
      "Andreas Bulling"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ramachandra_Fingervein_Verification_Using_Convolutional_Multi-Head_Attention_Network_WACV_2024_paper.html": {
    "title": "Fingervein Verification Using Convolutional Multi-Head Attention Network",
    "volume": "main",
    "abstract": "Biometric verification systems are deployed in various security-based access-control applications that require user-friendly and reliable user verification. Among the different biometric characteristics, fingervein biometrics have been extensively studied owing to their reliable verification performance. Furthermore, fingervein patterns reside inside the skin and are not visible outside; therefore, they possess inherent resistance to presentation attacks and degradation due to external factors. In this study, we introduce a novel fingervein verification technique using a convolutional multihead attention network, VeinAtnNet. The proposed VeinAtnNet is designed to achieve light weight with a smaller number of learnable parameters while extracting discriminant information from both normal and enhanced fingervein images. The proposed VeinAtnNet was trained on the newly constructed fingervein dataset with 300 unique fingervein patterns that were captured in multiple sessions to obtain 92 samples per unique fingervein. Extensive experiments were performed on the newly collected dataset FV-300 and the publicly available FV-USM fingervein dataset. The performance of the proposed method was compared with five state-of-the-art fingervein verification systems, indicating the efficacy of the proposed VeinAtnNet",
    "checked": true,
    "id": "46d510ed47931939063936f74433f92800f7b2f5",
    "semantic_title": "fingervein verification using convolutional multi-head attention network",
    "citation_count": 0,
    "authors": [
      "Raghavendra Ramachandra",
      "Sushma Venkatesh"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Yang_Foundation_Model_Assisted_Weakly_Supervised_Semantic_Segmentation_WACV_2024_paper.html": {
    "title": "Foundation Model Assisted Weakly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "This work aims to leverage pre-trained foundation models, such as contrastive language-image pre-training (CLIP) and segment anything model (SAM), to address weakly supervised semantic segmentation (WSSS) using image-level labels. To this end, we propose a coarse-to-fine framework based on CLIP and SAM for generating high-quality segmentation seeds. Specifically, we construct an image classification task and a seed segmentation task, which are jointly performed by CLIP with frozen weights and two sets of learnable task-specific prompts. A SAM-based seeding (SAMS) module is designed and applied to each task to produce either coarse or fine seed maps. Moreover, we design a multi-label contrastive loss supervised by image-level labels and a CAM activation loss supervised by the generated coarse seed map. These losses are used to learn the prompts, which are the only parts need to be learned in our framework. Once the prompts are learned, we input each image along with the learned segmentation-specific prompts into CLIP and the SAMS module to produce high-quality segmentation seeds. These seeds serve as pseudo labels to train an off-the-shelf segmentation network like other two-stage WSSS methods. Experiments show that our method achieves the state-of-the-art performance on PASCAL VOC 2012 and competitive results on MS COCO 2014. Our code will be released upon acceptance",
    "checked": true,
    "id": "85944e5d0f93772cbbe851ae5f2e6b08928f0d3d",
    "semantic_title": "foundation model assisted weakly supervised semantic segmentation",
    "citation_count": 0,
    "authors": [
      "Xiaobo Yang",
      "Xiaojin Gong"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ruan_Describe_Images_in_a_Boring_Way_Towards_Cross-Modal_Sarcasm_Generation_WACV_2024_paper.html": {
    "title": "Describe Images in a Boring Way: Towards Cross-Modal Sarcasm Generation",
    "volume": "main",
    "abstract": "Sarcasm generation has been investigated in previous studies by considering it as a text-to-text generation problem, i.e., generating a sarcastic sentence for an input sentence. In this paper, we study a new problem of cross-modal sarcasm generation (CMSG), i.e., generating a sarcastic description for a given image. CMSG is challenging as models need to satisfy the characteristics of sarcasm, as well as the correlation between different modalities. In addition, there should be some inconsistency between the two modalities, which requires imagination. Moreover, high-quality training data is insufficient. To address these problems, we take a step toward generating sarcastic descriptions from images without paired training data and propose an Extraction-Generation-Ranking based Modular method (EGRM) for CMSG. Specifically, EGRM first extracts diverse information from an image at different levels and uses the obtained image tags, sentimental descriptive caption, and commonsense-based consequence to generate candidate sarcastic texts. Then, a comprehensive ranking algorithm, which considers image-text relation, sarcasticness, and grammaticality, is proposed to select a final text from the candidate texts. Human evaluation at five criteria on a total of 2100 generated image-text pairs and auxiliary automatic evaluation show the superiority of our method. Code and data will be publicly available",
    "checked": false,
    "id": "0848b72bef2f566b1a35deae2428c9a4f621c617",
    "semantic_title": "how to describe images in a more funny way? towards a modular approach to cross-modal sarcasm generation",
    "citation_count": 1,
    "authors": [
      "Jie Ruan",
      "Yue Wu",
      "Xiaojun Wan",
      "Yuesheng Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kim_Offline-to-Online_Knowledge_Distillation_for_Video_Instance_Segmentation_WACV_2024_paper.html": {
    "title": "Offline-to-Online Knowledge Distillation for Video Instance Segmentation",
    "volume": "main",
    "abstract": "In this paper, we present offline-to-online knowledge distillation (OOKD) for video instance segmentation (VIS), which transfers a wealth of video knowledge from an offline model to an online model for consistent prediction. Unlike previous methods that have adopted either an online or offline model, our single online model takes advantage of both models by distilling offline knowledge. To transfer knowledge correctly, we propose query filtering and association (QFA), which filters irrelevant queries to exact instances. Our KD with QFA increases the robustness of feature matching by encoding object-centric features from a single frame supplemented by long-range global information. We also propose a simple data augmentation scheme for knowledge distillation in the VIS task that fairly transfers the knowledge of all classes into the online model. Extensive experiments show that our method significantly improves the performance in video instance segmentation, especially for challenging datasets, including long, dynamic sequences. Our method also achieves state-of-the-art performance on YTVIS-21, YTVIS-22, and OVIS datasets, with mAP scores of 46.1%, 43.6%, and 31.1%, respectively",
    "checked": true,
    "id": "83c4e4c26301c1ad1647ed96f542c7eb432ec6f3",
    "semantic_title": "offline-to-online knowledge distillation for video instance segmentation",
    "citation_count": 0,
    "authors": [
      "Hojin Kim",
      "Seunghun Lee",
      "Hyeon Kang",
      "Sunghoon Im"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Yuan_Rethinking_Multimodal_Content_Moderation_From_an_Asymmetric_Angle_With_Mixed-Modality_WACV_2024_paper.html": {
    "title": "Rethinking Multimodal Content Moderation From an Asymmetric Angle With Mixed-Modality",
    "volume": "main",
    "abstract": "There is a rapidly growing need for multimodal content moderation (CM) as more and more content on social media is multimodal in nature. Existing unimodal CM systems may fail to catch harmful content that crosses modalities (e.g., memes or videos), which may lead to severe consequences. In this paper, we present a novel CM model, Asymmetric Mixed-Modal Moderation (AM3), to target multimodal and unimodal CM tasks. Specifically, to address the asymmetry in semantics between vision and language, AM3 has a novel asymmetric fusion architecture that is designed to not only fuse the common knowledge in both modalities but also to exploit the unique information in each modality. Unlike pre- vious works that focus on representing the two modalities in similar feature space while overlooking the intrinsic difference between the information conveyed in multimodality and in unimodality (asymmetry in modalities), we propose a novel cross-modality contrastive loss to learn the unique knowledge that only appears in multimodality. This is critical as some harmful intent may only be conveyed through the intersection of both modalities. With extensive experiments, we show that AM3 outperforms all existing state-of-the-art methods on both multimodal and unimodal CM benchmarks",
    "checked": true,
    "id": "ef773b93f1480112fbc85f78d3364f86e9641022",
    "semantic_title": "rethinking multimodal content moderation from an asymmetric angle with mixed-modality",
    "citation_count": 0,
    "authors": [
      "Jialin Yuan",
      "Ye Yu",
      "Gaurav Mittal",
      "Matthew Hall",
      "Sandra Sajeev",
      "Mei Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Hekimoglu_Active_Learning_With_Task_Consistency_and_Diversity_in_Multi-Task_Networks_WACV_2024_paper.html": {
    "title": "Active Learning With Task Consistency and Diversity in Multi-Task Networks",
    "volume": "main",
    "abstract": "Multi-task networks demonstrate state-of-the-art performance across various vision tasks. However, their performance relies on large-scale annotated datasets, demanding extensive labeling efforts, especially as the number of tasks to label increases. In this paper, we introduce an active learning framework consisting of a data selection strategy that identifies the most informative unlabeled samples and a training strategy that ensures balanced training across multiple tasks. Our selection strategy leverages the inconsistency between initial and refined task predictions generated by recent two-stage multi-task networks. We further enhance our selection by incorporating task-specific sample diversity through a novel feature extraction mechanism. Our method captures task features for all tasks and distills them into a unified representation, which is used to curate a training set encapsulating diverse task-specific scenarios. In our training strategy, we introduce a sample-specific loss weighting mechanism based on the individual task selection scores. This facilitates the individual prioritization of samples for each task, effectively simulating the sample ordering process inherent in single-task active learning. Extensive experimentation on the PASCAL and NYUD-v2 datasets demonstrates that our approach outperforms existing state-of-the-art methods. Our approach reaches the loss of the network trained with all the available data using only 50% of the data, corresponding to 10% fewer labels compared to the state-of-the-art selection strategy. Our code is available at https://github.com/aralhekimoglu/mtal",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aral Hekimoglu",
      "Michael Schmidt",
      "Alvaro Marcos-Ramiro"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Chuah_Single_Domain_Generalization_via_Normalised_Cross-Correlation_Based_Convolutions_WACV_2024_paper.html": {
    "title": "Single Domain Generalization via Normalised Cross-Correlation Based Convolutions",
    "volume": "main",
    "abstract": "Deep learning techniques often perform poorly in the presence of domain shift, where the test data follows a different distribution than the training data. The most practically desirable approach to address this issue is Single Domain Generalization (S-DG), which aims to train robust models using data from a single source. Prior work on S-DG has primarily focused on using data augmentation techniques to generate diverse training data. In this paper, we explore an alternative approach by investigating the robustness of linear operators, such as convolution and dense layers commonly used in deep learning. We propose a novel operator called XCNorm that computes the normalized cross-correlation between weights and an input feature patch. This approach is invariant to both affine shifts and changes in energy within a local feature patch and eliminates the need for commonly used non-linear activation functions. We show that deep neural networks composed of this operator are robust to common semantic distribution shifts. Furthermore, our empirical results on single-domain generalization benchmarks demonstrate that our proposed technique performs comparably to the state-of-the-art methods",
    "checked": true,
    "id": "44b67e5b016d21e307033e794a52d14723debecf",
    "semantic_title": "single domain generalization via normalised cross-correlation based convolutions",
    "citation_count": 0,
    "authors": [
      "WeiQin Chuah",
      "Ruwan Tennakoon",
      "Reza Hoseinnezhad",
      "David Suter",
      "Alireza Bab-Hadiashar"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kalshetti_Intrinsic_Hand_Avatar_Illumination-Aware_Hand_Appearance_and_Shape_Reconstruction_From_WACV_2024_paper.html": {
    "title": "Intrinsic Hand Avatar: Illumination-Aware Hand Appearance and Shape Reconstruction From Monocular RGB Video",
    "volume": "main",
    "abstract": "Reconstructing a user-specific hand avatar is essential for a personalized experience in augmented and virtual reality systems. Current state-of-the-art avatar reconstruction methods use implicit representations to capture detailed geometry and appearance combined with neural rendering. However, these methods rely on a complicated multi-view setup, do not explicitly handle environment lighting leading to baked-in illumination and self-shadows, and require long hours for training. We present a method to reconstruct a hand avatar from a monocular RGB video of a user's hand in arbitrary hand poses captured under real-world environment lighting. Specifically, our method jointly optimizes shape, appearance, and lighting parameters using a realistic shading model in a differentiable rendering framework incorporating Monte Carlo path tracing. Despite relying on physically-based rendering, our method can complete the reconstruction within minutes. In contrast to existing work, our method disentangles intrinsic properties of the underlying appearance and environment lighting, leading to realistic self-shadows. We compare our method with state-of-the-art hand avatar reconstruction methods and observe that it outperforms them on all commonly used metrics. We also evaluate our method on our captured dataset to emphasize its generalization capability. Finally, we demonstrate applications of our intrinsic hand avatar on novel pose synthesis and relighting. We plan to release our code to aid further research",
    "checked": false,
    "id": "aa103cad5fa6d804f6a4e4361a01506e07a96754",
    "semantic_title": "reconstructing hand shape and appearance for accurate tracking from monocular video",
    "citation_count": 0,
    "authors": [
      "Pratik Kalshetti",
      "Parag Chaudhuri"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Therien_Object_Re-Identification_From_Point_Clouds_WACV_2024_paper.html": {
    "title": "Object Re-Identification From Point Clouds",
    "volume": "main",
    "abstract": "Object re-identification (ReID) from images plays a critical role in application domains of image retrieval (surveillance, retail analytics, etc.) and multi-object tracking (autonomous driving, robotics, etc.). However, systems that additionally or exclusively perceive the world from depth sensors are becoming more commonplace without any corresponding methods for object ReID. In this work, we fill the gap by providing the first large-scale study of object ReID from point clouds and establishing its performance relative to image ReID. To enable such a study, we create two large-scale ReID datasets with paired image and LiDAR observations and propose a lightweight matching head that can be concatenated to any set or sequence processing backbone (e.g., PointNet or ViT), creating a family of comparable object ReID networks for both modalities. Run in Siamese style, our proposed point cloud ReID networks can make thousands of pairwise comparisons in real-time (10 hz). Our findings demonstrate that their performance increases with higher sensor resolution and approaches that of image ReID when observations are sufficiently dense. Our strongest network trained at the largest scale achieves ReID accuracy exceeding 90% for rigid objects and 85% for deformable objects (without any explicit skeleton normalization). To our knowledge, we are the first to study object re-identification from real point cloud observations",
    "checked": true,
    "id": "d7b6db68185c6c00aa282c7081d318cee225c6a2",
    "semantic_title": "object re-identification from point clouds",
    "citation_count": 0,
    "authors": [
      "Benjamin Thérien",
      "Chengjie Huang",
      "Adrian Chow",
      "Krzysztof Czarnecki"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ribeiro-Gomes_MotionGPT_Human_Motion_Synthesis_With_Improved_Diversity_and_Realism_via_WACV_2024_paper.html": {
    "title": "MotionGPT: Human Motion Synthesis With Improved Diversity and Realism via GPT-3 Prompting",
    "volume": "main",
    "abstract": "There are numerous applications for human motion synthesis, including animation, gaming, robotics, or sports science. In recent years, human motion generation from natural language has emerged as a promising alternative to costly and labor-intensive data collection methods relying on motion capture or wearable sensors (e.g., suits). Despite this, generating human motion from textual descriptions remains a challenging and intricate task, primarily due to the scarcity of large-scale supervised datasets capable of capturing the full diversity of human activity. This study proposes a new approach, called MotionGPT, to address the limitations of previous text-based human motion generation methods by utilizing the extensive semantic information available in large language models (LLMs). We first pretrain a doubly text-conditional motion diffusion model on both coarse (\"high-level\") and detailed (\"low-level\") ground truth text data. Then during inference, we improve motion diversity and alignment with the training set, by zero-shot prompting GPT-3 for additional \"low-level\" details. Our method achieves new state-of-the-art quantitative results in terms of Frechet Inception Distance (FID) and motion diversity metrics, and improves all considered metrics. Furthermore, it has strong qualitative performance, producing natural results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jose Ribeiro-Gomes",
      "Tianhui Cai",
      "Zoltán Á. Milacski",
      "Chen Wu",
      "Aayush Prakash",
      "Shingo Takagi",
      "Amaury Aubel",
      "Daeil Kim",
      "Alexandre Bernardino",
      "Fernando De la Torre"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Marvasti-Zadeh_Training-Based_Model_Refinement_and_Representation_Disagreement_for_Semi-Supervised_Object_Detection_WACV_2024_paper.html": {
    "title": "Training-Based Model Refinement and Representation Disagreement for Semi-Supervised Object Detection",
    "volume": "main",
    "abstract": "Semi-supervised object detection (SSOD) aims to improve the performance and generalization of existing object detectors by utilizing limited labeled data and extensive unlabeled data. Despite many advances, recent SSOD methods are still challenged by inadequate model refinement using the classical exponential moving average (EMA) strategy, the consensus of Teacher-Student models in the latter stages of training (i.e., losing their distinctiveness), and noisy/misleading pseudo-labels. This paper proposes a novel training-based model refinement (TMR) stage and a simple yet effective representation disagreement (RD) strategy to address the limitations of classical EMA and the consensus problem. The TMR stage of Teacher-Student models optimizes the lightweight scaling operation to refine the model's weights and prevent overfitting or forgetting learned patterns from unlabeled data. Meanwhile, the RD strategy helps keep these models diverged to encourage the student model to explore additional patterns in unlabeled data. Our approach can be integrated into established SSOD methods and is empirically validated using two baseline methods, with and without cascade regression, to generate more reliable pseudo-labels. Extensive experiments demonstrate the superior performance of our approach over state-of-the-art SSOD methods. Specifically, the proposed approach outperforms the baseline Unbiased-Teacher-v2 (& Unbiased-Teacher-v1) method by an average mAP margin of 2.23, 2.1, and 3.36 (& 2.07, 1.9, and 3.27) on COCO-standard, COCO-additional, and Pascal VOC datasets, respectively",
    "checked": false,
    "id": "abd146378c079c4054fd534597342cdc13d6f061",
    "semantic_title": "tmr-rd: training-based model refinement and representation disagreement for semi-supervised object detection",
    "citation_count": 0,
    "authors": [
      "Seyed Mojtaba Marvasti-Zadeh",
      "Nilanjan Ray",
      "Nadir Erbilgin"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Li_Efficient_Layout-Guided_Image_Inpainting_for_Mobile_Use_WACV_2024_paper.html": {
    "title": "Efficient Layout-Guided Image Inpainting for Mobile Use",
    "volume": "main",
    "abstract": "The layout guidance, which specifies the pixel-wise object distribution, is beneficial to preserving the object boundaries in image inpainting while not hurting model's generalization capability. We aim to design an efficient and robust layout-guided image inpainting method for mobile use, which can achieve the robustness in presence of the mixed scenes where objects with the delicate shape reside next to the hole. Our method is made up of two sub-models, which restore the pixel-information for the hole from coarse to fine, and support each other to overcome the practical challenges encountered when making the whole method lightweight. The layout mask guides the two sub-models, which thus enables the robustness of our method in mixed scenes. We demonstrate the efficiency and robustness of our method via both the experiments and a mobile demo",
    "checked": false,
    "id": "03e3cd9e9c8d34b7b83cc20c5333a63b346fe1b5",
    "semantic_title": "delivered lectures on ‘ bending rules: augmenting post-digital architecture practice', exploring various aspects of digital design and architectural practice,",
    "citation_count": 0,
    "authors": [
      "Wenbo Li",
      "Yi Wei",
      "Yilin Shen",
      "Hongxia Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Giazitzis_SigmML_Metric_Meta-Learning_for_Writer_Independent_Offline_Signature_Verification_in_WACV_2024_paper.html": {
    "title": "SigmML: Metric Meta-Learning for Writer Independent Offline Signature Verification in the Space of SPD Matrices",
    "volume": "main",
    "abstract": "The handwritten signature has been identified as one of the most popular biometric means of human consent and/or presence for transactions held by any number of physical or legal entities. Automated signature verification (ASV), merge popular scientific branches such as computer vision, pattern recognition and/or data-driven machine learning algorithms. Up to now, several metric learning approaches for designing a writer-independent signature verifier, have been developed within a Euclidean framework by means of having their operations closed with respect to real vector spaces. In this work, we propose, for the first time in the ASV literature, the use of a meta-learning framework in the space of the Symmetric Positive Definite (SPD) manifold as a means to learn a pairwise similarity metric for writer-independent ASV. To begin, pairs of handwritten signatures are converted into a multidimensional distance vector with elements corresponding SPD distances between spatial segments of corresponding covariance pairs. We propose a novel meta-learning approach which explores the structure of the input gradients of the SPD manifold by means of a recurrent model, constrained by the geometry of the SPD manifold. The experimental protocols utilize two popular signature datasets of Western and Asian origin in two blind-intra and blind-inter (or cross-lingual) transfer learning approach. It also provide evidence of the discriminating nature of the proposed framework at least when summarized against other State-of-the-Art models, typically realized under a framework of Euclidean, or vector space, nature",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexios Giazitzis",
      "Elias N. Zois"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Marouf_Mini_but_Mighty_Finetuning_ViTs_With_Mini_Adapters_WACV_2024_paper.html": {
    "title": "Mini but Mighty: Finetuning ViTs With Mini Adapters",
    "volume": "main",
    "abstract": "Vision Transformers (ViTs) have become one of the dominant architectures in computer vision, and pre-trained ViT models are commonly adapted to new tasks via fine-tuning. Recent works proposed several parameter-efficient transfer learning methods, such as adapters, to avoid the prohibitive training and storage cost of fine-tuning. In this work, we observe that adapters perform poorly when the dimension of adapters is small, and we propose MiMi, a training framework that addresses this issue. We start with large adapters which can reach high performance, and iteratively reduce the size of every adapter. We introduce a scoring function that compares neuron importance across layers and consequently allows automatic estimation of the hidden dimension of every adapter. Our method outperforms existing methods in finding the best trade-off between accuracy and trained parameters across the three dataset benchmarks DomainNet, VTAB, and Multi-task, for a total of 29 datasets. We will release our code publicly upon acceptance",
    "checked": true,
    "id": "5ccbc4e835a0fdcb10412f81e84386d4f8774aa0",
    "semantic_title": "mini but mighty: finetuning vits with mini adapters",
    "citation_count": 0,
    "authors": [
      "Imad Eddine Marouf",
      "Enzo Tartaglione",
      "Stéphane Lathuilière"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Fang_Dynamic_Multimodal_Information_Bottleneck_for_Multimodality_Classification_WACV_2024_paper.html": {
    "title": "Dynamic Multimodal Information Bottleneck for Multimodality Classification",
    "volume": "main",
    "abstract": "Effectively leveraging multimodal data such as various images, laboratory tests and clinical information is gaining traction in a variety of AI-based medical diagnosis and prognosis tasks. Most existing multi-modal techniques only focus on enhancing their performance by leveraging the differences or shared features from various modalities and fusing feature across different modalities. These approaches are generally not optimal for clinical settings, which pose the additional challenges of limited training data, as well as being rife with redundant data or noisy modality channels, leading to subpar performance. To address this gap, we study the robustness of existing methods to data redundancy and noise and propose a generalized dynamic multimodal information bottleneck framework for attaining a robust fused feature representation. Specifically, our information bottleneck module serves to filter out the task-irrelevant information and noises in the fused feature, and we further introduce a sufficiency loss to prevent dropping of task-relevant information, thus explicitly preserving the sufficiency of prediction information in the distilled feature. We validate our model on an in-house and a public COVID-19 dataset for mortality prediction as well as two public biomedical datasets for diagnostic tasks. Extensive experiments show that our method surpasses the state-of-the-art and is significantly more robust, being the only method to remain performant when large-scale noisy channels exist. Our code is publicly available at https://github.com/Anonymous-PaperSubmission/DMIB",
    "checked": true,
    "id": "4f3b0059baa3d0c0f26439c21e461e23e28f9719",
    "semantic_title": "dynamic multimodal information bottleneck for multimodality classification",
    "citation_count": 0,
    "authors": [
      "Yingying Fang",
      "Shuang Wu",
      "Sheng Zhang",
      "Chaoyan Huang",
      "Tieyong Zeng",
      "Xiaodan Xing",
      "Simon Walsh",
      "Guang Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Srinath_Learning_Generalizable_Perceptual_Representations_for_Data-Efficient_No-Reference_Image_Quality_Assessment_WACV_2024_paper.html": {
    "title": "Learning Generalizable Perceptual Representations for Data-Efficient No-Reference Image Quality Assessment",
    "volume": "main",
    "abstract": "No-reference (NR) image quality assessment (IQA) is an important tool in enhancing the user experience in diverse visual applications. A major drawback of state-of-the-art NR-IQA techniques is their reliance on a large number of human annotations to train models for a target IQA application. To mitigate this requirement, there is a need for unsupervised learning of generalizable quality representations that capture diverse distortions. We enable the learning of low-level quality features agnostic to distortion types by introducing a novel quality-aware contrastive loss. Further, we leverage the generalizability of vision-language models by fine-tuning one such model to extract high-level image quality information through relevant text prompts. The two sets of features are combined to effectively predict quality by training a simple regressor with very few samples on a target dataset. Additionally, we design zero-shot quality predictions from both pathways in a completely blind setting. Our experiments on diverse datasets encompassing various distortions show the generalizability of the features and their superior performance in the data-efficient and zero-shot settings",
    "checked": true,
    "id": "9fac4c5ae2652b9b9614d5564e1d9137811f6ba5",
    "semantic_title": "learning generalizable perceptual representations for data-efficient no-reference image quality assessment",
    "citation_count": 0,
    "authors": [
      "Suhas Srinath",
      "Shankhanil Mitra",
      "Shika Rao",
      "Rajiv Soundararajan"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Achary_Real_Time_GAZED_Online_Shot_Selection_and_Editing_of_Virtual_WACV_2024_paper.html": {
    "title": "Real Time GAZED: Online Shot Selection and Editing of Virtual Cameras From Wide-Angle Monocular Video Recordings",
    "volume": "main",
    "abstract": "Eliminating time-consuming post-production processes and delivering high-quality videos in today's fast-paced digital landscape are the key advantages of real-time approaches. To address these needs, we present Real Time GAZED: a real-time adaptation of the GAZED framework integrated with CineFilter, a novel real-time camera trajectory stabilization approach. It enables users to create professionally edited videos in real-time. Comparative evaluations against baseline methods, including the non-real-time GAZED, demonstrate that Real Time GAZED achieves similar editing results, ensuring high-quality video output. Furthermore, a user study confirms the aesthetic quality of the video edits produced by the Real Time GAZED approach. With these advancements in real-time camera trajectory optimization and video editing presented, the demand for immediate and dynamic content creation in industries such as live broadcasting, sports coverage, news reporting, and social media content creation can be met more efficiently",
    "checked": true,
    "id": "455b21a02fa5336376dd850eec19d17e3d034d98",
    "semantic_title": "real time gazed: online shot selection and editing of virtual cameras from wide-angle monocular video recordings",
    "citation_count": 0,
    "authors": [
      "Sudheer Achary",
      "Rohit Girmaji",
      "Adhiraj Anil Deshmukh",
      "Vineet Gandhi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Jung_ConfTrack_Kalman_Filter-Based_Multi-Person_Tracking_by_Utilizing_Confidence_Score_of_WACV_2024_paper.html": {
    "title": "ConfTrack: Kalman Filter-Based Multi-Person Tracking by Utilizing Confidence Score of Detection Box",
    "volume": "main",
    "abstract": "Kalman filter-based tracking-by-detection (KFTBD) trackers are effective methods for solving multi-person tracking tasks. However, in crowd circumstances, noisy detection results (bounding boxes with low-confidence scores) can cause ID switch and tracking failure of trackers since these trackers utilize the detector's output directly. In this paper, to solve the problem, we suggest a novel tracker called ConfTrack based on a KFTBD tracker. Compared with conventional KFTBD trackers, ConfTrack consists of novel algorithms, including low-confidence object penalization and cascading algorithms for effectively dealing with noisy detector outputs. ConfTrack is tested on diverse domains of datasets such as the MOT17, MOT20, DanceTrack, and HiEve datasets. ConfTrack has proved its robustness in crowd circumstances by achieving the highest score at HOTA and IDF1 metrics in the MOT20 dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeonchul Jung",
      "Seokjun Kang",
      "Takgen Kim",
      "HyeongKi Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Arora_Hybrid_Sample_Synthesis-Based_Debiasing_of_Classifier_in_Limited_Data_Setting_WACV_2024_paper.html": {
    "title": "Hybrid Sample Synthesis-Based Debiasing of Classifier in Limited Data Setting",
    "volume": "main",
    "abstract": "Deep learning models are known to suffer from the problem of bias, and researchers have been exploring methods to address this issue. However, most of these methods require prior knowledge of the bias and are not always practical. In this paper, we focus on a more practical setting with no prior information about the bias. Generally, in this setting, there are a large number of bias-aligned samples that cause the model to produce biased predictions and a few bias-conflicting samples that do not conform to the bias. If the training data is limited, the influence of the bias-aligned samples may become even stronger on the model predictions, and we experimentally demonstrate that existing debiasing techniques suffer severely in such cases. In this paper, we examine the effects of unknown bias in small dataset regimes and present a novel approach to mitigate this issue. The proposed approach directly addresses the issue of the extremely low occurrence of bias-conflicting samples in limited data settings through the synthesis of hybrid samples that can be used to reduce the effect of bias. We perform extensive experiments on several benchmark datasets and experimentally demonstrate the effectiveness of our proposed approach in addressing any unknown bias in the presence of limited data. Specifically, our approach outperforms the vanilla, LfF, LDD, and DebiAN debiasing methods by absolute margins of 10.39%, 9.08%, 8.07%, and 9.67% when only 10% of the Corrupted CIFAR-10 Type 1 dataset is available with a bias-conflicting sample ratio of 0.05",
    "checked": true,
    "id": "e538d4730a9d0b42eac3d1b65f162d767c089820",
    "semantic_title": "hybrid sample synthesis-based debiasing of classifier in limited data setting",
    "citation_count": 0,
    "authors": [
      "Piyush Arora",
      "Pratik Mazumder"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Islam_Visually_Guided_Audio_Source_Separation_With_Meta_Consistency_Learning_WACV_2024_paper.html": {
    "title": "Visually Guided Audio Source Separation With Meta Consistency Learning",
    "volume": "main",
    "abstract": "In this paper, we tackle the problem of visually guided audio source separation in the context of both known and unknown objects (e.g., musical instruments). Recent successful end-to-end deep learning approaches adopt a single network with fixed parameters to generalize across unseen test videos. However, it can be challenging to generalize in cases where the distribution shift between training and test videos is higher as they fail to utilize internal information of unknown test videos. Based on this observation, we introduce a meta-consistency driven test time adaptation scheme that enables the pretrained model to quickly adapt to known and unknown test music videos in order to bring substantial improvements. In particular, we design a self-supervised audio-visual consistency objective as an auxiliary task that learns the synchronization between audio and its corresponding visual embedding. Concretely, we apply a meta-consistency training scheme to further optimize the pretrained model for effective and faster test time adaptation. We obtain substantial performance gains with only a smaller number of gradient updates and without any additional parameters for the task of audio source separation. Extensive experimental results across datasets demonstrate the effectiveness of our proposed method",
    "checked": false,
    "id": "9357f63458e5fc9feaca17aa7bd07e2acd05de24",
    "semantic_title": "visual sound source separation with partial supervision learning",
    "citation_count": 0,
    "authors": [
      "Md Amirul Islam",
      "Seyed Shahabeddin Nabavi",
      "Irina Kezele",
      "Yang Wang",
      "Yuanhao Yu",
      "Jin Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Deane_RGBT-Dog_A_Parametric_Model_and_Pose_Prior_for_Canine_Body_WACV_2024_paper.html": {
    "title": "RGBT-Dog: A Parametric Model and Pose Prior for Canine Body Analysis Data Creation",
    "volume": "main",
    "abstract": "While there exists a great deal of labeled in-the-wild human data, the same is not true for animals. Manually creating new labels for the full range of animal species would take years of effort from the community. We are also now seeing the emerging potential for computer vision methods in areas like animal conservation, which is an additional motivation for this direction of research. Key to our approach is the ability to easily generate as many labeled training images as we desire across a range of different modalities. To achieve this, we present a new large scale canine motion capture dataset and parametric canine body and texture model. These are used to produce the first large scale, multi-domain, multi-task dataset for canine body analysis comprising of detailed synthetic labels on both real images and fully synthetic images in a range of realistic poses. We also introduce the first pose prior for animals in the form of a variational pose prior for canines which is used to fit the parametric model to images of canines. We demonstrate the effectiveness of our labels for training computer vision models on tasks such as parts-based segmentation and pose estimation and show such models can generalise to other animal species without additional training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jake Deane",
      "Sinéad Kearney",
      "Kwang In Kim",
      "Darren Cosker"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Harb_Diffusion-Based_Generation_of_Histopathological_Whole_Slide_Images_at_a_Gigapixel_WACV_2024_paper.html": {
    "title": "Diffusion-Based Generation of Histopathological Whole Slide Images at a Gigapixel Scale",
    "volume": "main",
    "abstract": "We present a novel diffusion-based approach to generate synthetic histopathological Whole Slide Images (WSIs) at an unprecedented gigapixel scale. Synthetic WSIs have many potential applications: They can augment training datasets to enhance the performance of many computational pathology applications. They allow the creation of synthesized copies of datasets that can be shared without violating privacy regulations. Or they can facilitate learning representations of WSIs without requiring data annotations. Despite this variety of applications, no existing deep-learning-based method generates WSIs at their typically high resolutions. Mainly due to the high computational complexity. Therefore, we propose a novel coarse-to-fine sampling scheme to tackle image generation of high-resolution WSIs. In this scheme, we increase the resolution of an initial low-resolution image to a high-resolution WSI. Particularly, a diffusion model sequentially adds fine details to images and increases their resolution. In our experiments, we train our method with WSIs from the TCGA- BRCA dataset. Additionally to quantitative evaluations, we also performed a user study with pathologists. The study results suggest that our generated WSIs resemble the structure of real WSIs",
    "checked": true,
    "id": "7e14d4503e1bf4e65d194a6d03b72ab39b853783",
    "semantic_title": "diffusion-based generation of histopathological whole slide images at a gigapixel scale",
    "citation_count": 0,
    "authors": [
      "Robert Harb",
      "Thomas Pock",
      "Heimo Müller"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Li_Bridging_the_Gap_Between_Multi-Focus_and_Multi-Modal_A_Focused_Integration_WACV_2024_paper.html": {
    "title": "Bridging the Gap Between Multi-Focus and Multi-Modal: A Focused Integration Framework for Multi-Modal Image Fusion",
    "volume": "main",
    "abstract": "Multi-modal image fusion (MMIF) integrates valuable information from different modality images into a fused one. However, the fusion of multiple visible images with different focal regions and infrared images is a unprecedented challenge in real MMIF applications. This is because of the limited depth of the focus of visible optical lenses, which impedes the simultaneous capture of the focal information within the same scene. To address this issue, in this paper, we propose a MMIF framework for joint focused integration and modalities information extraction. Specifically, a semi-sparsity-based smoothing filter is introduced to decompose the images into structure and texture components. Subsequently, a novel multi-scale operator is proposed to fuse the texture components, capable of detecting significant information by considering the pixel focus attributes and relevant data from various modal images. Additionally, to achieve an effective capture of scene luminance and reasonable contrast maintenance, we consider the distribution of energy information in the structural components in terms of multi-directional frequency variance and information entropy. Extensive experiments on existing MMIF datasets, as well as the object detection and depth estimation tasks, consistently demonstrate that the proposed algorithm can surpass the state-of-the-art methods in visual perception and quantitative evaluation.The code is available at https://github.com/ixilai/MFIF-MMIF",
    "checked": true,
    "id": "e8de9daed9b3afcf48944880c6dcaf44e27da487",
    "semantic_title": "bridging the gap between multi-focus and multi-modal: a focused integration framework for multi-modal image fusion",
    "citation_count": 0,
    "authors": [
      "Xilai Li",
      "Xiaosong Li",
      "Tao Ye",
      "Xiaoqi Cheng",
      "Wuyang Liu",
      "Haishu Tan"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Raine_Image_Labels_Are_All_You_Need_for_Coarse_Seagrass_Segmentation_WACV_2024_paper.html": {
    "title": "Image Labels Are All You Need for Coarse Seagrass Segmentation",
    "volume": "main",
    "abstract": "Seagrass meadows serve as critical carbon sinks, but estimating the amount of carbon they store requires knowledge of the seagrass species present. Underwater and surface vehicles equipped with machine learning algorithms can help to accurately estimate the composition and extent of seagrass meadows at scale. However, previous approaches for seagrass detection and classification have required supervision from patch-level labels. In this paper, we reframe seagrass classification as a weakly supervised coarse segmentation problem where image-level labels are used during training (25 times fewer labels compared to patch-level labeling) and patch-level outputs are obtained at inference time. To this end, we introduce SeaFeats, an architecture that uses unsupervised contrastive pre-training and feature similarity, and SeaCLIP, a model that showcases the effectiveness of large language models as a supervisory signal in domain-specific applications. We demonstrate that an ensemble of SeaFeats and SeaCLIP leads to highly robust performance. Our method outperforms previous approaches that require patch-level labels on the multi-species 'DeepSeagrass' dataset by 6.8% (absolute) for the class-weighted F1 score, and by 12.1% (absolute) for the seagrass presence/absence F1 score on the 'Global Wetlands' dataset. We also present two case studies for real-world deployment: outlier detection on the Global Wetlands dataset, and application of our method on imagery collected by the FloatyBoat autonomous surface vehicle",
    "checked": true,
    "id": "f6048d733de1dad2f3bac9988630f4b17fcf89ec",
    "semantic_title": "image labels are all you need for coarse seagrass segmentation",
    "citation_count": 0,
    "authors": [
      "Scarlett Raine",
      "Ross Marchant",
      "Brano Kusy",
      "Frederic Maire",
      "Tobias Fischer"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Henrich_Registered_and_Segmented_Deformable_Object_Reconstruction_From_a_Single_View_WACV_2024_paper.html": {
    "title": "Registered and Segmented Deformable Object Reconstruction From a Single View Point Cloud",
    "volume": "main",
    "abstract": "In deformable object manipulation, we often want to interact with specific segments of an object that are only defined in non-deformed models of the object. We thus require a system that can recognize and locate these segments in sensor data of deformed real world objects. This is normally done using deformable object registration, which is problem specific and complex to tune. Recent methods utilize neural occupancy functions to improve deformable object registration by registering to an object reconstruction. Going one step further, we propose a system that in addition to reconstruction learns segmentation of the reconstructed object. As the resulting output already contains the information about the segments, we can skip the registration process. Tested on a variety of deformable objects in simulation and the real world, we demonstrate that our method learns to robustly find these segments. We also introduce a simple sampling algorithm to generate better training data for occupancy learning",
    "checked": true,
    "id": "0e2793604e64ed2ca06e5974dd30a0261d2589a7",
    "semantic_title": "registered and segmented deformable object reconstruction from a single view point cloud",
    "citation_count": 0,
    "authors": [
      "Pit Henrich",
      "Balázs Gyenes",
      "Paul Maria Scheikl",
      "Gerhard Neumann",
      "Franziska Mathis-Ullrich"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Lazarou_Adaptive_Manifold_for_Imbalanced_Transductive_Few-Shot_Learning_WACV_2024_paper.html": {
    "title": "Adaptive Manifold for Imbalanced Transductive Few-Shot Learning",
    "volume": "main",
    "abstract": "Transductive few-shot learning algorithms have showed substantially superior performance over their inductive counterparts by leveraging the unlabeled queries at inference. However, the vast majority of transductive methods are evaluated on perfectly class-balanced benchmarks. It has been shown that they undergo remarkable drop in performance under a more realistic, imbalanced setting. To this end, we propose a novel algorithm to address imbalanced transductive few-shot learning, named Adaptive Manifold. Our algorithm exploits the underlying manifold of the labeled examples and unlabeled queries by using manifold similarity to predict the class probability distribution of every query. It is parameterized by one centroid per class and a set of manifold parameters that determine the manifold. All parameters are optimized by minimizing a loss function that can be tuned towards class-balanced or imbalanced distributions. The manifold similarity shows substantial improvement over Euclidean distance, especially in the 1-shot setting. Our algorithm outperforms all other state of the art methods in three benchmark datasets, namely miniImageNet, tieredImageNet and CUB, and two different backbones, namely ResNet-18 and WideResNet-28-10. In certain cases, our algorithm outperforms the previous state of the art by as much as 4.2%. The publicly available source code can be found in https://github.com/MichalisLazarou/AM",
    "checked": true,
    "id": "82dabfc26c1e677ed7d347520059ecd954deca07",
    "semantic_title": "adaptive manifold for imbalanced transductive few-shot learning",
    "citation_count": 1,
    "authors": [
      "Michalis Lazarou",
      "Yannis Avrithis",
      "Tania Stathaki"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Lin_Restoring_Degraded_Old_Films_With_Recursive_Recurrent_Transformer_Networks_WACV_2024_paper.html": {
    "title": "Restoring Degraded Old Films With Recursive Recurrent Transformer Networks",
    "volume": "main",
    "abstract": "There exists a large number of old films that have not only artistic value but also historical significance. However, due to the degradation of analogue medium over time, old films often suffer from various deteriorations that make it difficult to restore them with existing approaches. In this work, we proposed a novel framework called Recursive Recurrent Transformer Network (RRTN) which is specifically designed for restoring degraded old films. Our approach introduces several key advancements, including a more accurate film noise mask estimation method, the utilization of second-order grid propagation and flow-guided deformable alignment, and the incorporation of a recursive structure to further improve the removal of challenging film noise. Through qualitative and quantitative evaluations, our approach demonstrates superior performance compared to existing approaches, effectively improving the restoration for difficult film noises that cannot be perfectly handled by existing approaches. The code and model are available at https://github.com/mountln/RRTN-old-film-restoration",
    "checked": false,
    "id": "ea1db7d13d6b512c118e50691ec96c0a360de435",
    "semantic_title": "bringing old films back to life",
    "citation_count": 24,
    "authors": [
      "Shan Lin",
      "Edgar Simo-Serra"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Chodosh_Re-Evaluating_LiDAR_Scene_Flow_WACV_2024_paper.html": {
    "title": "Re-Evaluating LiDAR Scene Flow",
    "volume": "main",
    "abstract": "Popular benchmarks for self-supervised LiDAR scene flow (stereoKITTI, and FlyingThings3D) have unrealistic rates of dynamic motion, unrealistic correspondences, and unrealistic sampling patterns. As a result, progress on these benchmarks is misleading and may cause researchers to focus on the wrong problems. We evaluate a suite of top methods on a suite of real-world datasets (Argoverse 2.0, Waymo, and NuScenes) and report several conclusions. First, we find that performance on stereoKITTI is negatively correlated with performance on real-world data. Second, we find that one of this task's key components -- removing the dominant ego-motion -- is better solved by classic ICP than any tested method. Finally, we show that despite the emphasis placed on learning, most performance gains are caused by pre- and post-processing steps: piecewise- rigid refinement and ground removal. We demonstrate this through a baseline method that combines these processing steps with a learning-free test-time flow optimization. This baseline outperforms every evaluated method",
    "checked": false,
    "id": "64ef073bea8b40b7e1b189fe9591ae415bec8cce",
    "semantic_title": "re-evaluating lidar scene flow for autonomous driving",
    "citation_count": 5,
    "authors": [
      "Nathaniel Chodosh",
      "Deva Ramanan",
      "Simon Lucey"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ji_Unsupervised_3D_Pose_Estimation_With_Non-Rigid_Structure-From-Motion_Modeling_WACV_2024_paper.html": {
    "title": "Unsupervised 3D Pose Estimation With Non-Rigid Structure-From-Motion Modeling",
    "volume": "main",
    "abstract": "Most existing 3D human pose estimation work rely heavily on the powerful memory capability of networks to obtain suitable 2D-3D mappings from the training data. Few works have studied the modeling of human posture deformation in motion. In this paper, we propose a new modeling method for human pose deformations and design an accompanying diffusion-based motion prior. Inspired by the field of non-rigid structure-from-motion, we divide the task of reconstructing 3D human skeletons in motion into the estimation of a 3D reference skeleton, and a frame-by-frame skeleton deformation. A mixed spatial-temporal NRSfMformer is used to simultaneously estimate the 3D reference skeleton and the skeleton deformation of each frame from 2D observations sequence, and then sum them up to obtain the pose of each frame. Subsequently, a loss term based on the diffusion model is used to ensure that the pipeline learns the correct prior motion knowledge. Finally, we have evaluated our proposed method on mainstream datasets and obtained superior results outperforming the state-of-the-art",
    "checked": true,
    "id": "94286d66cd39f712678c56829dd367a0e5779ba5",
    "semantic_title": "unsupervised 3d pose estimation with non-rigid structure-from-motion modeling",
    "citation_count": 1,
    "authors": [
      "Haorui Ji",
      "Hui Deng",
      "Yuchao Dai",
      "Hongdong Li"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Yuan_FAKD_Feature_Augmented_Knowledge_Distillation_for_Semantic_Segmentation_WACV_2024_paper.html": {
    "title": "FAKD: Feature Augmented Knowledge Distillation for Semantic Segmentation",
    "volume": "main",
    "abstract": "In this work, we explore data augmentations for knowledge distillation on semantic segmentation. Due the capacity gap, small-sized student networks struggle to discover the discriminative feature space learned by a powerful teacher. Image-level augmentations allow the student to better imitate the teacher by providing extra outputs. However, existing distillation frameworks only augment a limited number of samples, which restricts the learning of a student. Inspired by the recent progress on semantic directions on feature space, this work proposes a feature-level augmented knowledge distillation (FAKD) which infinitely augments features along a semantic direction for optimal knowledge transfer. Furthermore, we introduce novel surrogate loss functions to distill the teacher's knowledge from an infinite number of samples. The surrogate loss is an upper bound of the expected distillation loss over infinite augmented samples. Extensive experiments on four semantic segmentation benchmarks demonstrate that the proposed method boosts the performance of current knowledge distillation methods without any significant overhead. The code will be released at FAKD",
    "checked": true,
    "id": "b52ccc26005919532ec3b4844146141d98fe30f1",
    "semantic_title": "fakd: feature augmented knowledge distillation for semantic segmentation",
    "citation_count": 2,
    "authors": [
      "Jianlong Yuan",
      "Minh Hieu Phan",
      "Liyang Liu",
      "Yifan Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ruan_TriCoLo_Trimodal_Contrastive_Loss_for_Text_To_Shape_Retrieval_WACV_2024_paper.html": {
    "title": "TriCoLo: Trimodal Contrastive Loss for Text To Shape Retrieval",
    "volume": "main",
    "abstract": "Text-to-shape retrieval is an increasingly relevant problem with the growth of 3D shape data. Recent work on contrastive losses for learning joint embeddings over multimodal data has been successful at tasks such as retrieval and classification. Thus far, work on joint representation learning for 3D shapes and text has focused on improving embeddings through modeling of complex attention between representations, or multi-task learning. We propose a trimodal learning scheme over text, multi-view images and 3D shape voxels, and show that with large batch contrastive learning we achieve good performance on text-to-shape retrieval without complex attention mechanisms or losses. Our experiments serve as a foundation for follow-up work on building trimodal embeddings for text-image-shape",
    "checked": true,
    "id": "b76cf6943a74196da3e4935ac5e6a18ed2fab082",
    "semantic_title": "tricolo: trimodal contrastive loss for text to shape retrieval",
    "citation_count": 11,
    "authors": [
      "Yue Ruan",
      "Han-Hung Lee",
      "Yiming Zhang",
      "Ke Zhang",
      "Angel X. Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Hur_Expanding_Expressiveness_of_Diffusion_Models_With_Limited_Data_via_Self-Distillation_WACV_2024_paper.html": {
    "title": "Expanding Expressiveness of Diffusion Models With Limited Data via Self-Distillation Based Fine-Tuning",
    "volume": "main",
    "abstract": "Training diffusion models on limited datasets poses challenges in terms of limited generation capacity and expressiveness, leading to unsatisfactory results in various downstream tasks utilizing pretrained diffusion models, such as domain translation and text-guided image manipulation. In this paper, we propose Self-Distillation for Fine-Tuning diffusion models (SDFT), a methodology to address these challenges by leveraging diverse features from diffusion models pretrained on large source datasets. SDFT distills more general features (shape, colors, etc.) and less domain-specific features (texture, fine details, etc) from the source model, allowing successful knowledge transfer without disturbing the training process on target datasets. The proposed method is not constrained by the specific architecture of the model and thus can be generally adopted to existing frameworks. Experimental results demonstrate that SDFT enhances the expressiveness of the diffusion model with limited datasets, resulting in improved generation capabilities across various downstream tasks",
    "checked": true,
    "id": "7a450dc65726c36012b1db96fc8c4d871158e045",
    "semantic_title": "expanding expressiveness of diffusion models with limited data via self-distillation based fine-tuning",
    "citation_count": 0,
    "authors": [
      "Jiwan Hur",
      "Jaehyun Choi",
      "Gyojin Han",
      "Dong-Jae Lee",
      "Junmo Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Mukhopadhyay_Diff2Lip_Audio_Conditioned_Diffusion_Models_for_Lip-Synchronization_WACV_2024_paper.html": {
    "title": "Diff2Lip: Audio Conditioned Diffusion Models for Lip-Synchronization",
    "volume": "main",
    "abstract": "The task of lip synchronization (lip-sync) seeks to match the lips of human faces with different audio. It has various applications in the film industry as well as for creating virtual avatars and for video conferencing. This is a challenging problem as one needs to simultaneously introduce detailed, realistic lip movements while preserving the identity, pose, emotions, and image quality. Many of the previous methods trying to solve this problem suffer from image quality degradation due to a lack of complete contextual information. In this paper, we present Diff2Lip, an audio-conditioned diffusion-based model which is able to do lip synchronization in-the-wild while preserving these qualities. We train our model on Voxceleb2, a video dataset containing in-the-wild talking face videos. Extensive studies show that our method outperforms popular methods like Wav2Lip and PC-AVS in Frechet inception distance (FID) metric and Mean Opinion Scores (MOS) of the users. We show results on both reconstruction (same audio-video inputs) as well as cross (different audio-video inputs) settings on Voxceleb2 and LRWdatasets. Video results are available at https://soumik-kanad.github.io/diff2lip",
    "checked": true,
    "id": "ea1cfd5f5ed1c0f3b6450c4a7da56d84618fab12",
    "semantic_title": "diff2lip: audio conditioned diffusion models for lip-synchronization",
    "citation_count": 2,
    "authors": [
      "Soumik Mukhopadhyay",
      "Saksham Suri",
      "Ravi Teja Gadde",
      "Abhinav Shrivastava"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kim_A_Atrous_Spatial_Temporal_Action_Recognition_for_Real_Time_Applications_WACV_2024_paper.html": {
    "title": "A*: Atrous Spatial Temporal Action Recognition for Real Time Applications",
    "volume": "main",
    "abstract": "Deep learning has become a popular tool across various fields and is increasingly being integrated into real-world applications such as autonomous driving cars and surveillance cameras. One area of active research is recognizing human actions, including identifying unsafe or abnormal behaviors. Temporal information is crucial for action recognition tasks. Global context, as well as the target person, are also important for judging human behaviors. However, larger networks that can capture all of these features face difficulties operating in real-time. To address these issues, we propose A*: Atrous Spatial Temporal Action Recognition for Real Time Applications. A* includes four modules aimed at improving action detection networks. First, we introduce a Low-Level Feature Aggregation module. Second, we propose the Atrous Spatio-Temporal Pyramid Pooling module. Third, we suggest to fuse all extracted image and video features in an Image-Video Feature Fusion module. Finally, we integrate the Proxy Anchor Loss for action features into the loss function. We evaluate A* on three common action detection benchmarks, and achieve state-of-the-art performance on JHMDB and UCF101-24, while staying competitive on AVA. Furthermore, we demonstrate that A* can achieve real-time inference speeds of 33 FPS, making it suitable for real-world applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Myeongjun Kim",
      "Federica Spinola",
      "Philipp Benz",
      "Tae-hoon Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Yi_Augment_the_Pairs_Semantics-Preserving_Image-Caption_Pair_Augmentation_for_Grounding-Based_Vision_WACV_2024_paper.html": {
    "title": "Augment the Pairs: Semantics-Preserving Image-Caption Pair Augmentation for Grounding-Based Vision and Language Models",
    "volume": "main",
    "abstract": "Grounding-based vision and language models have been successfully applied to low-level vision tasks, aiming to precisely locate objects referred in captions. The effectiveness of grounding representation learning heavily relies on the scale of the training dataset. Despite being a useful data enrichment strategy, data augmentation has received minimal attention in existing vision and language tasks as augmentation for image-caption pairs is non-trivial. In this study, we propose a robust phrase grounding model trained with text-conditioned and text-unconditioned data augmentations. Specifically, we apply text-conditioned color jittering and horizontal flipping to ensure semantic consistency between images and captions. To guarantee image-caption correspondence in the training samples, we modify the captions according to pre-defined keywords when applying horizontal flipping. Additionally, inspired by recent masked signal reconstruction, we propose to use pixel-level masking as a novel form of data augmentation. While we demonstrate our data augmentation method with MDETR framework, the proposed approach is applicable to common grounding-based vision and language tasks with other frameworks. Finally, we show that larger capacity image encoder such as CLIP can further improve the results. Through extensive experiments on three commonly applied datasets: Flickr30k, referring expressions, and GQA, our method demonstrates advanced performance over the state-of-the-arts with various metrics. Code can be found in https://github.com/amzn/augment-the-pairs-wacv2024",
    "checked": true,
    "id": "a33a971830e8e3a7e96602474395a89fca96276c",
    "semantic_title": "augment the pairs: semantics-preserving image-caption pair augmentation for grounding-based vision and language models",
    "citation_count": 0,
    "authors": [
      "Jingru Yi",
      "Burak Uzkent",
      "Oana Ignat",
      "Zili Li",
      "Amanmeet Garg",
      "Xiang Yu",
      "Linda Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Valvano_Controllable_Image_Synthesis_of_Industrial_Data_Using_Stable_Diffusion_WACV_2024_paper.html": {
    "title": "Controllable Image Synthesis of Industrial Data Using Stable Diffusion",
    "volume": "main",
    "abstract": "Training supervised deep neural networks that perform defect detection and segmentation requires large-scale fully-annotated datasets, which can be hard or even impossible to obtain in industrial environments. Generative AI offers opportunities to enlarge small industrial datasets artificially, thus enabling the usage of state-of-the-art supervised approaches in the industry. Unfortunately, also good generative models need a lot of data to train, while industrial datasets are often tiny. Here, we propose a new approach for reusing general-purpose pre-trained generative models on industrial data, ultimately allowing the generation of self-labelled defective images. First, we let the model learn the new concept, entailing the novel data distribution. Then, we force it to learn to condition the generative process, producing industrial images that satisfy well-defined topological characteristics and show defects with a given geometry and location. To highlight the advantage of our approach, we use the synthetic dataset to optimise a crack segmentor for a real industrial use case. When the available data is small, we observe considerable performance increase under several metrics, showing the method's potential in production environments",
    "checked": true,
    "id": "0dbd6ab5adc97ca4e31928fe845fde6274942841",
    "semantic_title": "controllable image synthesis of industrial data using stable diffusion",
    "citation_count": 0,
    "authors": [
      "Gabriele Valvano",
      "Antonino Agostino",
      "Giovanni De Magistris",
      "Antonino Graziano",
      "Giacomo Veneri"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Dong_Understanding_Dark_Scenes_by_Contrasting_Multi-Modal_Observations_WACV_2024_paper.html": {
    "title": "Understanding Dark Scenes by Contrasting Multi-Modal Observations",
    "volume": "main",
    "abstract": "Understanding dark scenes based on multi-modal image data is challenging, as both the visible and auxiliary modalities provide limited semantic information for the task. Previous methods focus on fusing the two modalities but neglect the correlations among semantic classes when minimizing losses to align pixels with labels, resulting in inaccurate class predictions. To address these issues, we introduce a supervised multi-modal contrastive learning approach to increase the semantic discriminability of the learned multi-modal feature spaces by jointly performing cross-modal and intra-modal contrast under the supervision of the class correlations. The cross-modal contrast encourages same-class embeddings from across the two modalities to be closer and pushes different-class ones apart. The intra-modal contrast forces same-class or different-class embeddings within each modality to be together or apart. We validate our approach on a variety of tasks that cover diverse light conditions and image modalities. Experiments show that our approach can effectively enhance dark scene understanding based on multi-modal images with limited semantics by shaping semantic-discriminative feature spaces. Comparisons with previous methods demonstrate our state-of-the-art performance. Code and pretrained models are available at https://github.com/palmdong/SMMCL",
    "checked": true,
    "id": "d3ffa8981f61f9402b584b5f18379b7326e83bb7",
    "semantic_title": "understanding dark scenes by contrasting multi-modal observations",
    "citation_count": 0,
    "authors": [
      "Xiaoyu Dong",
      "Naoto Yokoya"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Deng_Expanding_Hyperspherical_Space_for_Few-Shot_Class-Incremental_Learning_WACV_2024_paper.html": {
    "title": "Expanding Hyperspherical Space for Few-Shot Class-Incremental Learning",
    "volume": "main",
    "abstract": "In today's ever-changing world, the ability of machine learning models to continually learn new data without forgetting previous knowledge is of utmost importance. However, in the scenario of few-shot class-incremental learning (FSCIL), where models have limited access to new instances, this task becomes even more challenging. Current methods use prototypes as a replacement for classifiers, where the cosine similarity of instances to these prototypes is used for prediction. However, we have identified that the embedding space created by using the relu activation function is incomplete and crowded for future classes. To address this issue, we propose the Expanding Hyperspherical Space (EHS) method for FSCIL. In EHS, we utilize an odd-symmetric activation function to ensure the completeness and symmetry of embedding space. Additionally, we specify a region for base classes and reserve space for unseen future classes, which increases the distance between class distributions. Pseudo instances are also used to enable the model to anticipate possible upcoming samples. During inference, we provide rectification to the confidence to prevent bias towards base classes. We conducted experiments on benchmark datasets such as CIFAR100 and miniImageNet, which demonstrate that our proposed method achieves state-of-the-art performance",
    "checked": true,
    "id": "a015fdfef9f4ffd3b11f813d85ad86dc0df70157",
    "semantic_title": "expanding hyperspherical space for few-shot class-incremental learning",
    "citation_count": 0,
    "authors": [
      "Yao Deng",
      "Xiang Xiang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Luo_Differentially_Private_Video_Activity_Recognition_WACV_2024_paper.html": {
    "title": "Differentially Private Video Activity Recognition",
    "volume": "main",
    "abstract": "In recent years, differential privacy has seen significant advancements in image classification; however, its application to video activity recognition remains under-explored. This paper addresses the challenges of applying differential privacy to video activity recognition, which primarily stem from: (1) a discrepancy between the desired privacy level for entire videos and the nature of input data processed by contemporary video architectures, which are typically short, segmented clips; and (2) the complexity and sheer size of video datasets relative to those in image classification, which render traditional differential privacy methods inadequate. To tackle these issues, we propose Multi-Clip DP-SGD, a novel framework for enforcing video-level differential privacy through clip-based classification models. This method samples multiple clips from each video, averages their gradients, and applies gradient clipping in DP-SGD without incurring additional privacy loss. Moreover, we incorporate a parameter-efficient transfer learning strategy to make the model scalable for large-scale video datasets. Through extensive evaluations on the UCF-101 and HMDB-51 datasets, our approach exhibits impressive performance, achieving 81% accuracy with a privacy budget of epsilon=5 on UCF-101, marking a 76% improvement compared to a direct application of DP-SGD. Furthermore, we demonstrate that our transfer learning strategy is versatile and can enhance differentially private image classification across an array of datasets including CheXpert, ImageNet, CIFAR-10, and CIFAR-100",
    "checked": true,
    "id": "fde02283a6038d3208e433a24c228524dcd7b302",
    "semantic_title": "differentially private video activity recognition",
    "citation_count": 0,
    "authors": [
      "Zelun Luo",
      "Yuliang Zou",
      "Yijin Yang",
      "Zane Durante",
      "De-An Huang",
      "Zhiding Yu",
      "Chaowei Xiao",
      "Li Fei-Fei",
      "Animashree Anandkumar"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Gebauer_Towards_a_Dynamic_Vision_Sensor-Based_Insect_Camera_Trap_WACV_2024_paper.html": {
    "title": "Towards a Dynamic Vision Sensor-Based Insect Camera Trap",
    "volume": "main",
    "abstract": "This paper introduces a visual real-time insect monitoring approach capable of detecting and tracking tiny and fast-moving objects in cluttered wildlife conditions using an RGB-DVS stereo-camera system. By building on the intrinsic benefits of event vision data acquisition, we demonstrate that insect presence can be detected at an extremely high temporal rate (on average more than 40 times real-time) while surpassing the spatial and spectral sensitivity of conventional colour-based sensing. Our DVS-based detection and tracking algorithm extracts insect locations over time, and we evaluated our system based on 81104 manually annotated stereo-frames with 34453 insect appearances featuring highly varying scenes and imaging conditions (including clutter, wind-induced motion, etc.). Comparing our algorithm to two state-of-the-art deep learning algorithms reveals superior results in both detection performance and computational speed. Using the DVS as a trigger for the temporally synchronised RGB camera, we are able to correctly identify 73% of images with and without insects which can be increased to 76% with parameters optimised for different scenes. Overall, our study suggests that DVS-based sensing can be used for visual insect monitoring by enabling reliable real-time insect detection in wildlife conditions while significantly reducing the necessity for data storage, manual labour and energy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eike Gebauer",
      "Sebastian Thiele",
      "Pierre Ouvrard",
      "Adrien Sicard",
      "Benjamin Risse"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Chang_FLORA_Fine-Grained_Low-Rank_Architecture_Search_for_Vision_Transformer_WACV_2024_paper.html": {
    "title": "FLORA: Fine-Grained Low-Rank Architecture Search for Vision Transformer",
    "volume": "main",
    "abstract": "Vision Transformers (ViT) have recently demonstrated success across a myriad of computer vision tasks. However, their elevated computational demands pose significant challenges for real-world deployment. While low-rank approximation stands out as a renowned method to reduce computational loads, efficiently automating the target rank selection in ViT remains a challenge. Drawing from the notable similarity and alignment between the processes of rank selection and One-Shot NAS, we introduce FLORA, an end-to-end automatic framework based on NAS. To overcome the design challenge of supernet posed by vast search space, FLORA employs a low-rank aware candidate filtering strategy. This method adeptly identifies and eliminates underperforming candidates, effectively alleviating potential undertraining and interference among subnetworks. To further enhance the quality of low-rank supernets, we design a low-rank specific training paradigm. First, we propose weight inheritance to construct supernet and enable gradient sharing among low-rank modules. Secondly, we adopt low-rank aware sampling to strategically allocate training resources, taking into account inherited information from pre-trained models. Empirical results underscore FLORA's efficacy. With our method, a more fine-grained rank configuration can be generated automatically and yield up to 33% extra FLOPs reduction compared to a simple uniform configuration. More specific, FLORA-DeiT-B/FLORA-Swin-B can save up to 55%/42% FLOPs almost without performance degradtion. Importantly, FLORA boasts both versatility and orthogonality, offering an extra 21%-26% FLOPs reduction when integrated with leading compression techniques or compact hybrid structures. Our code is publicly available at https://github.com/shadowpa0327/FLORA",
    "checked": true,
    "id": "b751c7cec74c5351fc0484e8c3552c388bcbb27a",
    "semantic_title": "flora: fine-grained low-rank architecture search for vision transformer",
    "citation_count": 0,
    "authors": [
      "Chi-Chih Chang",
      "Yuan-Yao Sung",
      "Shixing Yu",
      "Ning-Chi Huang",
      "Diana Marculescu",
      "Kai-Chiang Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Yang_Latent-Guided_Exemplar-Based_Image_Re-Colorization_WACV_2024_paper.html": {
    "title": "Latent-Guided Exemplar-Based Image Re-Colorization",
    "volume": "main",
    "abstract": "Exemplar-based re-colorization transfers colors from a reference to a colored or grayscale source image, accounting for the semantic correspondences between the two. Existing grayscale colorization methods usually predict only the chromatic aberration while maintaining the source's luminance. Consequently, the result's color may diverge from the reference due to such luminance difference. On the other hand, global photorealistic stylization without segmentation cannot handle scenarios where different parts of the scene need different colors. To overcome this issue, we propose a novel and effective method for re-colorization: 1) We first exploit the spatial-adaptive latent space of SpaceEdit in the context of the re-colorization task and achieve re-colorization via latent maps prediction through a proposed network. 2) We then delve into SpaceEdit's self-reconstruct latent codes and maps to better characterize the global style and local color property, based on which we construct a novel loss to supervise re-colorization. Qualitative and quantitative results show that our method outperforms previous works by generating superior outputs with more consistent colors and global styles based on references",
    "checked": false,
    "id": "4a20c5f5b3221724e33706c5aa860bb1cd1c4a5f",
    "semantic_title": "spcolor: semantic prior guided exemplar-based image colorization",
    "citation_count": 0,
    "authors": [
      "Wenjie Yang",
      "Ning Xu",
      "Yifei Fan"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Fang_Data_Augmentation_for_Object_Detection_via_Controllable_Diffusion_Models_WACV_2024_paper.html": {
    "title": "Data Augmentation for Object Detection via Controllable Diffusion Models",
    "volume": "main",
    "abstract": "Data augmentation is vital for object detection tasks that require expensive bounding box annotations. Recent successes in diffusion models have inspired the use of diffusion-based synthetic images for data augmentation. However, existing works have primarily focused on image classification, and their applicability to boost object detection's performance remains unclear. To address this gap, we propose a data augmentation pipeline based on controllable diffusion models and CLIP. Our approach involves generating appropriate visual priors to control the generation of synthetic data and implementing post-filtering techniques using category-calibrated CLIP scores. The evaluation of our approach is conducted under few-shot settings in MSCOCO, full PASCAL VOC dataset, and selected downstream datasets. We observe the performance increase using our augmentation pipeline. Specifically, the mAP improvement is +18.0%/+15.6%/+15.9% for COCO 5/10/30-shot, +2.9% on full PASCAL VOC dataset, and +12.4% on average for selected downstream datasets",
    "checked": false,
    "id": "594601770237ed7563e902f7e787860e994f2742",
    "semantic_title": "scaling robot learning with semantic data augmentation through diffusion models",
    "citation_count": 0,
    "authors": [
      "Haoyang Fang",
      "Boran Han",
      "Shuai Zhang",
      "Su Zhou",
      "Cuixiong Hu",
      "Wen-Ming Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Almalki_Self-Supervised_Learning_With_Masked_Autoencoders_for_Teeth_Segmentation_From_Intra-Oral_WACV_2024_paper.html": {
    "title": "Self-Supervised Learning With Masked Autoencoders for Teeth Segmentation From Intra-Oral 3D Scans",
    "volume": "main",
    "abstract": "In modern dentistry, teeth localization, segmentation, and labeling from intra-oral 3D scans are crucial for improving dental diagnostics, treatment planning, and population-based studies on oral health. However, creating automated algorithms for teeth analysis is a challenging task due to the limited availability of accessible data for training, particularly from the point of view of deep learning. This study extends the self-supervised learning framework of the mesh masked autoencoder (MeshMAE) transformer. While the MeshMAE loss measures the quality of reconstructed masked mesh triangles, the loss of the proposed DentalMAE evaluates the predicted deep embeddings of masked mesh triangles. This yields a better generalization ability on a very limited number of 3D dental scans, as documented by our results on teeth segmentation of intra-oral scans. Our results show that masking-based unsupervised learning methods may, for the first time, provide convincing transfer learning improvements on 3D intra-oral scans, increasing the overall accuracy over both MeshMAE and prior self-supervised pre-training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amani Almalki",
      "Longin Jan Latecki"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Mun_Small_Objects_Matters_in_Weakly-Supervised_Semantic_Segmentation_WACV_2024_paper.html": {
    "title": "Small Objects Matters in Weakly-Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "Weakly-supervised semantic segmentation (WSSS) performs pixel-wise classification given only image-level labels for training. Despite the difficulty of this task, the research community has achieved promising results over the last five years. Still, current WSSS literature misses the detailed sense of how well the methods perform on different sizes of objects. Thus we propose a novel evaluation metric to provide a comprehensive assessment across different object sizes and collect a size-balanced evaluation set to complement PASCAL VOC. With these two gadgets, we reveal that the existing WSSS methods struggle in capturing small objects. Furthermore, we propose a size-balanced cross-entropy loss coupled with a proper training strategy. It generally improves existing WSSS methods as validated upon ten baselines on three different datasets",
    "checked": true,
    "id": "38b17598325104194dda5fb20608d0b4277036c8",
    "semantic_title": "small objects matters in weakly-supervised semantic segmentation",
    "citation_count": 0,
    "authors": [
      "Cheolhyun Mun",
      "Sanghuk Lee",
      "Youngjung Uh",
      "Junsuk Choe",
      "Hyeran Byun"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Rashwan_MaskConver_Revisiting_Pure_Convolution_Model_for_Panoptic_Segmentation_WACV_2024_paper.html": {
    "title": "MaskConver: Revisiting Pure Convolution Model for Panoptic Segmentation",
    "volume": "main",
    "abstract": "In recent years, transformer-based models have dominated panoptic segmentation, thanks to their strong modeling capabilities and their unified representation for both semantic and instance classes as global binary masks. In this paper, we revisit pure convolution model and propose a novel panoptic architecture named MaskConver. MaskConver proposes to fully unify things and stuff representation by predicting their centers. To that extent, it creates a lightweight class embedding module that can break the ties when multiple centers co-exist in the same location. Furthermore, our study shows that the decoder design is critical in ensuring that the model has sufficient context for accurate detection and segmentation. We introduce a powerful ConvNeXt-UNet decoder that closes the performance gap between convolution- and transformer-based models. With ResNet50 backbone, our MaskConver achieves 53.6% PQ on the COCO panoptic val set, out-performing the modern convolution-based model, Panoptic FCN, by 9.3% as well as transformer-based models such as Mask2Former (+1.7% PQ) and kMaX-DeepLab (+0.6% PQ). Additionally, MaskConver with a MobileNet backbone reaches 37.2% PQ, improving over Panoptic-DeepLab by +6.4% under the same FLOPs/latency constraints. A further optimized version of MaskConver achieves 29.7% PQ, while running in real-time on mobile devices. The code and model weights will be publicly available",
    "checked": true,
    "id": "220e5a2034e80535e170e16b1ea75387cecba5d6",
    "semantic_title": "maskconver: revisiting pure convolution model for panoptic segmentation",
    "citation_count": 0,
    "authors": [
      "Abdullah Rashwan",
      "Jiageng Zhang",
      "Ali Taalimi",
      "Fan Yang",
      "Xingyi Zhou",
      "Chaochao Yan",
      "Liang-Chieh Chen",
      "Yeqing Li"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Borer_From_Chaos_to_Calibration_A_Geometric_Mutual_Information_Approach_To_WACV_2024_paper.html": {
    "title": "From Chaos to Calibration: A Geometric Mutual Information Approach To Target-Free Camera LiDAR Extrinsic Calibration",
    "volume": "main",
    "abstract": "Sensor fusion is vital for the safe and robust operation of autonomous vehicles. Accurate extrinsic sensor to sensor calibration is necessary to accurately fuse multiple sensor's data in a common spatial reference frame. In this paper, we propose a target free extrinsic calibration algorithm that requires no ground truth training data, artificially constrained motion trajectories, hand engineered features or offline optimization and that is accurate, precise and extremely robust to initialization error. Most current research on online camera-LiDAR extrinsic calibration requires ground truth training data which is impossible to capture at scale. We revisit analytical mutual information based methods first proposed in 2012 and demonstrate that geometric features provide a robust information metric for camera-LiDAR extrinsic calibration. We demonstrate our proposed improvement using the KITTI and KITTI-360 fisheye data set",
    "checked": true,
    "id": "d1307905b43f6fe5d9b0f5b0ce401ca580ba5a42",
    "semantic_title": "from chaos to calibration: a geometric mutual information approach to target-free camera lidar extrinsic calibration",
    "citation_count": 0,
    "authors": [
      "Jack Borer",
      "Jeremy Tschirner",
      "Florian Ölsner",
      "Stefan Milz"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Peng_PHG-Net_Persistent_Homology_Guided_Medical_Image_Classification_WACV_2024_paper.html": {
    "title": "PHG-Net: Persistent Homology Guided Medical Image Classification",
    "volume": "main",
    "abstract": "Modern deep neural networks have achieved great successes in medical image analysis. However, the features captured by convolutional neural networks (CNNs) or Transformers tend to be optimized for pixel intensities and neglect key anatomical structures such as connected components and loops. In this paper, we propose a persistent homology guided approach (PHG-Net) that explores topological features of objects for medical image classification. For an input image, we first compute its cubical persistence diagram and extract topological features into a vector representation using a small neural network (called the PH module). The extracted topological features are then incorporated into the feature map generated by CNN or Transformer for feature fusion. The PH module is lightweight and capable of integrating topological features into any CNN or Transformer architectures in an end-to-end fashion. We evaluate our PHG-Net on three public datasets and demonstrate its considerable improvements on the target classification tasks over state-of-the-art methods",
    "checked": true,
    "id": "247b826e77ae4612118a750e546c1e1dde4ca0aa",
    "semantic_title": "phg-net: persistent homology guided medical image classification",
    "citation_count": 0,
    "authors": [
      "Yaopeng Peng",
      "Hongxiao Wang",
      "Milan Sonka",
      "Danny Z. Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Chin_Masking_Improves_Contrastive_Self-Supervised_Learning_for_ConvNets_and_Saliency_Tells_WACV_2024_paper.html": {
    "title": "Masking Improves Contrastive Self-Supervised Learning for ConvNets, and Saliency Tells You Where",
    "volume": "main",
    "abstract": "While image data starts to enjoy the simple-but-effective self-supervised learning scheme built upon masking and self-reconstruction objective thanks to the introduction of tokenization procedure and vision transformer backbone, convolutional neural networks as another important and widely-adopted architecture for image data, though having contrastive-learning techniques to drive the self-supervised learning, still face the difficulty of leveraging such straightforward and general masking operation to benefit their learning process significantly. In this work, we aim to alleviate the burden of including masking operation into the contrastive-learning framework for convolutional neural networks as an extra augmentation method. In addition to the additive but unwanted edges (between masked and unmasked regions) as well as other adverse effects caused by the masking operations for ConvNets, which have been discussed by prior works, we particularly identify the potential problem where for one view in a contrastive sample-pair the randomly-sampled masking regions could be overly concentrated on important/salient objects thus resulting in misleading contrastiveness to the other view. To this end, we propose to explicitly take the saliency constraint into consideration in which the masked regions are more evenly distributed among the foreground and background for realizing the masking-based augmentation. Moreover, we introduce hard negative samples by masking larger regions of salient patches in an input image. Extensive experiments conducted on various datasets, contrastive learning mechanisms, and downstream tasks well verify the efficacy as well as the superior performance of our proposed method with respect to several state-of-the-art baselines",
    "checked": true,
    "id": "2b182248a3b2af496caba031f6209c20378b649a",
    "semantic_title": "masking improves contrastive self-supervised learning for convnets, and saliency tells you where",
    "citation_count": 0,
    "authors": [
      "Zhi-Yi Chin",
      "Chieh-Ming Jiang",
      "Ching-Chun Huang",
      "Pin-Yu Chen",
      "Wei-Chen Chiu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zavrtanik_Cheating_Depth_Enhancing_3D_Surface_Anomaly_Detection_via_Depth_Simulation_WACV_2024_paper.html": {
    "title": "Cheating Depth: Enhancing 3D Surface Anomaly Detection via Depth Simulation",
    "volume": "main",
    "abstract": "RGB-based surface anomaly detection methods have advanced significantly. However, certain surface anomalies remain practically invisible in RGB alone, necessitating the incorporation of 3D information. Existing approaches that employ point-cloud backbones suffer from suboptimal representations and reduced applicability due to slow processing. Re-training RGB backbones, designed for faster dense input processing, on industrial depth datasets is hindered by the limited availability of sufficiently large datasets. We make several contributions to address these challenges. (i) We propose a novel Depth-Aware Discrete Autoencoder (DADA) architecture, that enables learning a general discrete latent space that jointly models RGB and 3D data for 3D surface anomaly detection. (ii) We tackle the lack of diverse industrial depth datasets by introducing a simulation process for learning informative depth features in the depth encoder. (iii) We propose a new surface anomaly detection method 3DSR, which outperforms all existing state-of-the-art on the challenging MVTec3D anomaly detection benchmark, both in terms of accuracy and processing speed. The experimental results validate the effectiveness and efficiency of our approach, highlighting the potential of utilizing depth information for improved surface anomaly detection",
    "checked": true,
    "id": "6ab46a5e7a93549d4705353d0eface089c9b2610",
    "semantic_title": "cheating depth: enhancing 3d surface anomaly detection via depth simulation",
    "citation_count": 0,
    "authors": [
      "Vitjan Zavrtanik",
      "Matej Kristan",
      "Danijel Skočaj"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Hirsch_CLID_Controlled-Length_Image_Descriptions_With_Limited_Data_WACV_2024_paper.html": {
    "title": "CLID: Controlled-Length Image Descriptions With Limited Data",
    "volume": "main",
    "abstract": "Controllable image captioning models generate human-like image descriptions, enabling some kind of control over the generated captions. This paper focuses on controlling the caption length, i.e. a short and concise description or a long and detailed one. Since existing image captioning datasets contain mostly short captions, generating long captions is challenging. To address the shortage of long training examples, we propose to enrich the dataset with varying-length self-generated captions. These, however, might be of varying quality and are thus unsuitable for conventional training. We introduce a novel training strategy that selects the data points to be used at different times during the training. Our method dramatically improves the length-control abilities, while exhibiting SoTA performance in terms of caption quality. Our approach is general and is shown to be applicable also to paragraph generation",
    "checked": true,
    "id": "5d0ca592554b5d6d201c51af8338f725f00dbf1e",
    "semantic_title": "clid: controlled-length image descriptions with limited data",
    "citation_count": 0,
    "authors": [
      "Elad Hirsch",
      "Ayellet Tal"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Li_Steering_Prototypes_With_Prompt-Tuning_for_Rehearsal-Free_Continual_Learning_WACV_2024_paper.html": {
    "title": "Steering Prototypes With Prompt-Tuning for Rehearsal-Free Continual Learning",
    "volume": "main",
    "abstract": "In the context of continual learning, prototypes--as representative class embeddings--offer advantages in memory conservation and the mitigation of catastrophic forgetting. However, challenges related to semantic drift and prototype interference persist. In this study, we introduce the Contrastive Prototypical Prompt (CPP) approach. Through task-specific prompt-tuning, underpinned by a contrastive learning objective, we effectively address both aforementioned challenges. Our evaluations on four challenging class-incremental benchmarks reveal that CPP achieves a significant 4% to 6% improvement over state-of-the-art methods. Importantly, CPP operates without a rehearsal buffer and narrows the performance divergence between continual and offline joint learning, suggesting an innovative scheme for Transformer-based continual learning systems",
    "checked": true,
    "id": "215e620a0f1f621ccc5ab1e507c185cb7a5f173b",
    "semantic_title": "steering prototypes with prompt-tuning for rehearsal-free continual learning",
    "citation_count": 6,
    "authors": [
      "Zhuowei Li",
      "Long Zhao",
      "Zizhao Zhang",
      "Han Zhang",
      "Di Liu",
      "Ting Liu",
      "Dimitris N. Metaxas"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Lyou_Modality-Aware_Representation_Learning_for_Zero-Shot_Sketch-Based_Image_Retrieval_WACV_2024_paper.html": {
    "title": "Modality-Aware Representation Learning for Zero-Shot Sketch-Based Image Retrieval",
    "volume": "main",
    "abstract": "Zero-shot learning offers an efficient solution for a machine learning model to treat unseen categories, avoiding exhaustive data collection. Zero-shot Sketch-based Image Retrieval (ZS-SBIR) simulates real-world scenarios where it is hard and costly to collect paired sketch-photo samples. We propose a novel framework that indirectly aligns sketches and photos by contrasting them through texts, removing the necessity of access to sketch-photo pairs. With an explicit modality encoding learned from data, our approach disentangles modality-agnostic semantics from modality-specific information, bridging the modality gap and enabling effective cross-modal content retrieval within a joint latent space. From comprehensive experiments, we verify the efficacy of the proposed model on ZS-SBIR, and it can be also applied to generalized and fine-grained settings",
    "checked": true,
    "id": "65d3fb26d3fddf3a1a6fe88462271398646bd5bb",
    "semantic_title": "modality-aware representation learning for zero-shot sketch-based image retrieval",
    "citation_count": 0,
    "authors": [
      "Eunyi Lyou",
      "Doyeon Lee",
      "Jooeun Kim",
      "Joonseok Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Yellin_Concurrent_Band_Selection_and_Traversability_Estimation_From_Long-Wave_Hyperspectral_Imagery_WACV_2024_paper.html": {
    "title": "Concurrent Band Selection and Traversability Estimation From Long-Wave Hyperspectral Imagery in Off-Road Settings",
    "volume": "main",
    "abstract": "Autonomous navigation has become increasingly popular in recent years; However, most existing methods focus on on-road navigation and utilize active sensors, such as LiDAR. This paper instead focuses on autonomous off-road navigation using traversability estimation from passive sensors, specifically long-wave (LW) hyperspectral imagery (HSI). We present a method for selecting a subset of hyperspectral bands that are most useful for traversability estimation by designing a band selection module that designs a minimal sensor that measures sparsely-sampled spectral bands while jointly training a semantic segmentation network for traversability estimation. The effectiveness of our method is demonstrated using our dataset of LW HSI from diverse off-road scenes including forest, desert, snow, ponds, and open fields. Our dataset includes imagery collected both during the daytime and nighttime during various weather conditions, including challenging scenes with a wide range of obstacles. Using our method, we learn a small subset (2%) of all the HSI bands that can achieve competitive or better traversability estimation accuracy to that achieved when utilizing all hyperspectral bands. Using only 5 bands, our method is able to achieve a mean class accuracy that is only 1.3% less than that achieved using full 256-band HSI and only 0.1% less than that achieved using 250-band HSI, demonstrating the success of our method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Florence Yellin",
      "Scott McCloskey",
      "Cole Hill",
      "Eric Smith",
      "Brian Clipp"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kim_Token_Fusion_Bridging_the_Gap_Between_Token_Pruning_and_Token_WACV_2024_paper.html": {
    "title": "Token Fusion: Bridging the Gap Between Token Pruning and Token Merging",
    "volume": "main",
    "abstract": "Vision Transformers (ViTs) have emerged as powerful backbones in computer vision, outperforming many traditional CNNs. However, their computational overhead, largely attributed to the self-attention mechanism, makes deployment on resource-constrained edge devices challenging. Multiple solutions rely on token pruning or token merging. In this paper, we introduce \"Token Fusion\" (ToFu), a method that amalgamates the benefits of both token pruning and token merging. Token pruning proves advantageous when the model exhibits sensitivity to input interpolations, while token merging is effective when the model manifests close to linear responses to inputs. We combine this to propose a new scheme called Token Fusion. Moreover, we tackle the limitations of average merging, which doesn't preserve the intrinsic feature norm, resulting in distributional shifts. To mitigate this, we introduce MLERP merging, a variant of the SLERP technique, tailored to merge multiple tokens while maintaining the norm distribution. ToFu is versatile, applicable to ViTs with or without additional training. Our empirical evaluations indicate that ToFu establishes new benchmarks in both classification and image generation tasks concerning computational efficiency and model accuracy",
    "checked": true,
    "id": "df09bfc9cf53fb64c0894782073435b438bfc8db",
    "semantic_title": "token fusion: bridging the gap between token pruning and token merging",
    "citation_count": 0,
    "authors": [
      "Minchul Kim",
      "Shangqian Gao",
      "Yen-Chang Hsu",
      "Yilin Shen",
      "Hongxia Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Liu_Global_Occlusion-Aware_Transformer_for_Robust_Stereo_Matching_WACV_2024_paper.html": {
    "title": "Global Occlusion-Aware Transformer for Robust Stereo Matching",
    "volume": "main",
    "abstract": "Despite the remarkable progress facilitated by learning-based stereo-matching algorithms, the performance in the ill-conditioned regions, such as the occluded regions, remains a bottleneck. Due to the limited receptive field, existing CNN-based methods struggle to handle these ill-conditioned regions effectively. To address this issue, this paper introduces a novel attention-based stereo-matching network called Global Occlusion-Aware Transformer (GOAT) to exploit long-range dependency and occlusion-awareness global context for disparity estimation. In the GOAT architecture, a parallel disparity and occlusion estimation module PDO is proposed to estimate the initial disparity map and the occlusion mask using a parallel attention mechanism. To further enhance the disparity estimates in the occluded regions, an occlusion-aware global aggregation module (OGA) is proposed. This module aims to refine the disparity in the occluded regions by leveraging restricted global correlation within the focus scope of the occluded areas. Extensive experiments were conducted on several public benchmark datasets including SceneFlow, KITTI 2015, and Middlebury. The results show that the proposed GOAT demonstrates outstanding performance among all benchmarks, particularly in the occluded regions",
    "checked": true,
    "id": "dcc41338fa83ae84e3c7fc5e5766f51a09764ebb",
    "semantic_title": "global occlusion-aware transformer for robust stereo matching",
    "citation_count": 0,
    "authors": [
      "Zihua Liu",
      "Yizhou Li",
      "Masatoshi Okutomi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Koch_SGRec3D_Self-Supervised_3D_Scene_Graph_Learning_via_Object-Level_Scene_Reconstruction_WACV_2024_paper.html": {
    "title": "SGRec3D: Self-Supervised 3D Scene Graph Learning via Object-Level Scene Reconstruction",
    "volume": "main",
    "abstract": "In the field of 3D scene understanding, 3D scene graphs have emerged as a new scene representation that combines geometric and semantic information about objects and their relationships. However, learning semantic 3D scene graphs in a fully supervised manner is inherently difficult as it requires not only object-level annotations but also relationship labels. While pre-training approaches have helped to boost the performance of many methods in various fields, pre-training for 3D scene graph prediction has received little attention. Furthermore, we find in this paper that classical contrastive point cloud-based pre-training approaches are ineffective for 3D scene graph learning. To this end, we present SGRec3D, a novel self-supervised pre-training method for 3D scene graph prediction. We propose to reconstruct the 3D input scene from a graph bottleneck as a pretext task. Pre-training SGRec3D does not require object relationship labels, making it possible to exploit large-scale 3D scene understanding datasets, which were off-limits for 3D scene graph learning before. Our experiments demonstrate that in contrast to recent point cloud-based pre-training approaches, our proposed pre-training improves the 3D scene graph prediction considerably, which results in SOTA performance, outperforming other 3D scene graph models by +10% on object prediction and +4% on relationship prediction. Additionally, we show that only using a small subset of 10% labeled data during fine-tuning is sufficient to outperform the same model without pre-training",
    "checked": true,
    "id": "11bbf05814b1b8164525c461324386b297c0decd",
    "semantic_title": "sgrec3d: self-supervised 3d scene graph learning via object-level scene reconstruction",
    "citation_count": 1,
    "authors": [
      "Sebastian Koch",
      "Pedro Hermosilla",
      "Narunas Vaskevicius",
      "Mirco Colosi",
      "Timo Ropinski"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ding_Estimating_Fog_Parameters_From_an_Image_Sequence_Using_Non-Linear_Optimisation_WACV_2024_paper.html": {
    "title": "Estimating Fog Parameters From an Image Sequence Using Non-Linear Optimisation",
    "volume": "main",
    "abstract": "Given a sequence of images taken in foggy weather, we seek to estimate the atmospheric light and the scattering coefficient. These are key parameters to characterise the nature of the fog, to reconstruct a clear image (defogging), and to infer scene depth. Existing methods adopt a sequential estimation strategy which is prone to error propagation. In sharp contrast, we take a more systematic approach and jointly estimate these parameters by solving a unified non-linear optimisation problem. Experimental results show that the proposed method is superior to existing ones in terms of both estimation accuracy and precision. Our method further demonstrates how image defogging and depth estimation can be linked to a visual localisation system, contributing to more comprehensive and robust perception in fog",
    "checked": false,
    "id": "ee24209bab58b3ad9f2a804aaa360ff7e06208ad",
    "semantic_title": "covid-19 time series forecasting of daily cases, deaths caused and recovered cases using machine learning",
    "citation_count": 0,
    "authors": [
      "Yining Ding",
      "Andrew M. Wallace",
      "Sen Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Phan_Fast_and_Interpretable_Face_Identification_for_Out-of-Distribution_Data_Using_Vision_WACV_2024_paper.html": {
    "title": "Fast and Interpretable Face Identification for Out-of-Distribution Data Using Vision Transformers",
    "volume": "main",
    "abstract": "Most face identification approaches employ a Siamese neural network to compare two images at the image embedding level. Yet, this technique can be subject to occlusion (e.g., faces with masks or sunglasses) and out-of-distribution data. DeepFace-EMD (Phan et al. 2022) reaches state-of-the-art accuracy on out-of-distribution data by first comparing two images at the image level, and then at the patch level. Yet, its later patch-wise re-ranking stage admits a large O(n^3 log n) time complexity (for n patches in an image) due to the optimal transport optimization. In this paper, we propose a novel, 2-image Vision Transformers (ViTs) that compares two images at the patch level using cross-attention. After training on 2M pairs of images on CASIA Webface (Yi et al. 2014), our model performs at a comparable accuracy as DeepFace-EMD on out-of-distribution data, yet at an inference speed more than twice as fast as DeepFace-EMD (Phan et al. 2022). In addition, via a human study, our model shows promising explainability through the visualization of cross-attention. We believe our work can inspire more explorations in using ViTs for face identification",
    "checked": true,
    "id": "480a8ebd46562022a1f94cacb8d56ed043dfdbac",
    "semantic_title": "fast and interpretable face identification for out-of-distribution data using vision transformers",
    "citation_count": 0,
    "authors": [
      "Hai Phan",
      "Cindy X. Le",
      "Vu Le",
      "Yihui He",
      "Anh “Totti” Nguyen"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Buettner_Investigating_the_Role_of_Attribute_Context_in_Vision-Language_Models_for_WACV_2024_paper.html": {
    "title": "Investigating the Role of Attribute Context in Vision-Language Models for Object Recognition and Detection",
    "volume": "main",
    "abstract": "Vision-language alignment learned from image-caption pairs has been shown to benefit tasks like object recognition and detection. Methods are mostly evaluated in terms of how well object class names are learned, but captions also contain rich attribute context that should be considered when learning object alignment. It is unclear how methods use this context in learning, as well as whether models succeed when tasks require attribute and object understanding. To address this gap, we conduct extensive analysis of the role of attributes in vision-language models. We specifically measure model sensitivity to the presence and meaning of attribute context, gauging influence on object embeddings through unsupervised phrase grounding and classification via description methods. We further evaluate the utility of attribute context in training for open-vocabulary object detection, fine-grained text-region retrieval, and attribution tasks. Our results show that attribute context can be wasted when learning alignment for detection, attribute meaning is not adequately considered in embeddings, and describing classes by only their attributes is ineffective. A viable strategy that we find to increase benefits from attributes is contrastive training with adjective-based negative captions",
    "checked": true,
    "id": "a9f6649a6885a66251ea3cccfb90f35dc35317df",
    "semantic_title": "investigating the role of attribute context in vision-language models for object recognition and detection",
    "citation_count": 0,
    "authors": [
      "Kyle Buettner",
      "Adriana Kovashka"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Cohen_Membership_Inference_Attack_Using_Self_Influence_Functions_WACV_2024_paper.html": {
    "title": "Membership Inference Attack Using Self Influence Functions",
    "volume": "main",
    "abstract": "Member inference (MI) attacks aim to determine if a specific data sample was used to train a machine learning model. Thus, MI is a major privacy threat to models trained on private sensitive data, such as medical records. In MI attacks one may consider the black-box settings, where the model's parameters and activations are hidden from the adversary, or the white-box case where they are available to the attacker. In this work, we focus on the latter and present a novel MI attack for it that employs influence functions, or more specifically the samples' self-influence scores, to perform MI prediction. The proposed method is evaluated on CIFAR-10, CIFAR-100, and Tiny ImageNet datasets using various architectures such as AlexNet, ResNet, and DenseNet. Our new attack method achieves new state-of-the-art (SOTA) results for MI even with limited adversarial knowledge, and is effective against MI defense methods such as data augmentation and differential privacy. Our code is available at https: //github.com/giladcohen/sif_mi_attack",
    "checked": true,
    "id": "570adab9c4ba166d021f96a2a0952dc9294ba936",
    "semantic_title": "membership inference attack using self influence functions",
    "citation_count": 4,
    "authors": [
      "Gilad Cohen",
      "Raja Giryes"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Eloul_Mixing_Gradients_in_Neural_Networks_as_a_Strategy_To_Enhance_WACV_2024_paper.html": {
    "title": "Mixing Gradients in Neural Networks as a Strategy To Enhance Privacy in Federated Learning",
    "volume": "main",
    "abstract": "Federated learning reduces the risk of information leakage, but remains vulnerable to attack. We show that well-mixed gradients provide numerical resistance to gradient inversion in neural networks. For example, we can enhance mixing gradients in a batch by choosing an appropriate loss function and drawing identical labels, and we support this with an approximate solution of batch inversion for linear layers. These simple architecture choices show no degradation to classification performance as opposed to noise perturbation defense. To accurately assess data recovery, we propose to use a variation distance metric for information leakage in images, derived from total variation. In contrast to Mean Squared Error or Structural Similarity Index metrics, it provides a continuous metric for information recovery. Finally, our empirical results of information recovery from various inversion attacks and training performance supports our defense strategies. These simple architecture choices found to be also useful for practical size of convolutional neural networks but depends on their size. We hope this work will trigger further defense studies using gradient mixing, towards achieving a trustful federation policy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaltiel Eloul",
      "Fran Silavong",
      "Sanket Kamthe",
      "Antonios Georgiadis",
      "Sean J. Moran"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Leon-Alcazar_Learning_to_Read_Analog_Gauges_from_Synthetic_Data_WACV_2024_paper.html": {
    "title": "Learning to Read Analog Gauges from Synthetic Data",
    "volume": "main",
    "abstract": "Manually reading and logging gauge data is time-inefficient, and the effort increases according to the number of gauges available. We present a pipeline that automates the reading of analog gauges. We propose a two-stage CNN pipeline that identifies the key structural components of an analog gauge and outputs an angular reading. To facilitate the training of our approach, a synthetic dataset is generated thus obtaining a set of realistic analog gauges with their corresponding annotation. To validate our proposal, an additional real-world dataset was collected with 4.813 manually curated images. When compared against state-of-the-art methodologies, our method shows a significant improvement of 4.55 in the average error, which is a 52% relative improvement. The resources for this project will be made available at: https://github.com/fuankarion/automatic-gauge-reading",
    "checked": true,
    "id": "3969481c57c107b69a0a088d0eb3fcf7ebde5b5b",
    "semantic_title": "learning to read analog gauges from synthetic data",
    "citation_count": 0,
    "authors": [
      "Juan Leon-Alcazar",
      "Yazeed Alnumay",
      "Cheng Zheng",
      "Hassane Trigui",
      "Sahejad Patel",
      "Bernard Ghanem"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Djilali_Learning_Saliency_From_Fixations_WACV_2024_paper.html": {
    "title": "Learning Saliency From Fixations",
    "volume": "main",
    "abstract": "We present a novel approach for saliency prediction in images, leveraging parallel decoding in transformers to learn saliency solely from fixation maps. Models typically rely on continuous saliency maps, to overcome the difficulty of optimizing for the discrete fixation map. We attempt to replicate the experimental setup that generates saliency datasets. Our approach treats saliency prediction as a direct set prediction problem, via a global loss that enforces unique fixations prediction through bipartite matching and a transformer encoder-decoder architecture. By utilizing a fixed set of learned fixation queries, the cross-attention reasons over the image features to directly output the fixation points, distinguishing it from other modern saliency predictors. Our approach, named Saliency TRansformer (SalTR) achieves remarkable results on the Salicon benchmark",
    "checked": true,
    "id": "acc0377d8ccbc35c8a3c2ca63cd42909f954561c",
    "semantic_title": "learning saliency from fixations",
    "citation_count": 0,
    "authors": [
      "Yasser Abdelaziz Dahou Djilali",
      "Kevin McGuinness",
      "Noel O’Connor"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Dadashzadeh_PECoP_Parameter_Efficient_Continual_Pretraining_for_Action_Quality_Assessment_WACV_2024_paper.html": {
    "title": "PECoP: Parameter Efficient Continual Pretraining for Action Quality Assessment",
    "volume": "main",
    "abstract": "The limited availability of labelled data in Action Quality Assessment (AQA), has forced previous works to fine-tune their models pretrained on large-scale domain-general datasets. This common approach results in weak generalisation, particularly when there is a significant domain shift. We propose a novel, parameter efficient, continual pretraining framework, PECoP, to reduce such domain shift via an additional pretraining stage. In PECoP, we introduce 3D-Adapters, inserted into the pretrained model, to learn spatiotemporal, in-domain information via self-supervised learning where only the adapter modules' parameters are updated. We demonstrate PECoP's ability to enhance the performance of recent state-of-the-art methods (MUSDL, CoRe, and TSA) applied to AQA, leading to considerable improvements on benchmark datasets, JIGSAWS (| 6.0%), MTL-AQA (| 0.99%), and FineDiving (| 2.54%). We also present a new Parkinson's Disease dataset, PD4T, of real patients performing four various actions, where we surpass (| 3.56%) the state-of-the-art in comparison. Our code, pretrained models, and the PD4T dataset are available at https://github.com/Plrbear/PECoP",
    "checked": true,
    "id": "24c3375fb3e18faf2356e0f2234f18c029ec1188",
    "semantic_title": "pecop: parameter efficient continual pretraining for action quality assessment",
    "citation_count": 1,
    "authors": [
      "Amirhossein Dadashzadeh",
      "Shuchao Duan",
      "Alan Whone",
      "Majid Mirmehdi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Suwala_Face_Identity-Aware_Disentanglement_in_StyleGAN_WACV_2024_paper.html": {
    "title": "Face Identity-Aware Disentanglement in StyleGAN",
    "volume": "main",
    "abstract": "Conditional GANs are frequently used for manipulating the attributes of face images, such as expression, hairstyle, pose, or age. Even though the state-of-the-art models successfully modify the requested attributes, they simultaneously modify other important characteristics of the image, such as a person's identity. In this paper, we focus on solving this problem by introducing PluGeN4Faces, a plugin to StyleGAN, which explicitly disentangles face attributes from a person's identity. Our key idea is to perform training on images retrieved from movie frames, where a given person appears in various poses and with different attributes. By applying a type of contrastive loss, we encourage the model to group images of the same person in similar regions of latent space. Our experiments demonstrate that the modifications of face attributes performed by PluGeN4Faces are significantly less invasive on the remaining characteristics of the image than in the existing state-of-the-art models",
    "checked": true,
    "id": "7b837ffaeb90bf9a7d4002e7900725d5961301a4",
    "semantic_title": "face identity-aware disentanglement in stylegan",
    "citation_count": 0,
    "authors": [
      "Adrian Suwała",
      "Bartosz Wójcik",
      "Magdalena Proszewska",
      "Jacek Tabor",
      "Przemysław Spurek",
      "Marek Śmieja"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wu_A_Robust_Diffusion_Modeling_Framework_for_Radar_Camera_3D_Object_WACV_2024_paper.html": {
    "title": "A Robust Diffusion Modeling Framework for Radar Camera 3D Object Detection",
    "volume": "main",
    "abstract": "Radar-camera 3D object detection aims at interacting radar signals with camera images for identifying objects of interest and localizing their corresponding 3D bounding boxes. To overcome the severe sparsity and ambiguity of radar signals, we propose a robust framework based on probabilistic denoising diffusion modeling. We design our framework to be easily implementable on different multi-view 3D detectors without the requirement of using LiDAR point clouds during either the training or inference. In specific, we first design our framework with a denoised radar-camera encoder via developing a lightweight denoising diffusion model with semantic embedding. Secondly, we develop the query denoising training into 3D space via introducing the reconstruction training at depth measurement for the transformer detection decoder. Our framework achieves new state-of-the-art performance on the nuScenes 3D detection benchmark but with few computational cost increases compared to the baseline detectors",
    "checked": false,
    "id": "d53b293645849c7131882bde37a8f674a881d5fe",
    "semantic_title": "3diffusiondet: diffusion model for 3d object detection with robust lidar-camera fusion",
    "citation_count": 0,
    "authors": [
      "Zizhang Wu",
      "Yunzhe Wu",
      "Xiaoquan Wang",
      "Yuanzhu Gan",
      "Jian Pu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Franchi_InfraParis_A_Multi-Modal_and_Multi-Task_Autonomous_Driving_Dataset_WACV_2024_paper.html": {
    "title": "InfraParis: A Multi-Modal and Multi-Task Autonomous Driving Dataset",
    "volume": "main",
    "abstract": "Current deep neural networks (DNNs) for autonomous driving computer vision are typically trained on specific datasets that only involve a single type of data and urban scenes. Consequently, these models struggle to handle new objects, noise, nighttime conditions, and diverse scenarios, which is essential for safety-critical applications. Despite ongoing efforts to enhance the resilience of computer vision DNNs, progress has been sluggish, partly due to the absence of benchmarks featuring multiple modalities. We introduce a novel and versatile dataset named InfraParis that supports multiple tasks across three modalities: RGB, depth, and infrared. We assess various state-of-the-art baseline techniques, encompassing models for the tasks of semantic segmentation, object detection, and depth estimation",
    "checked": true,
    "id": "da995e38275c3526a2f1eee31a4e805ca58e5014",
    "semantic_title": "infraparis: a multi-modal and multi-task autonomous driving dataset",
    "citation_count": 0,
    "authors": [
      "Gianni Franchi",
      "Marwane Hariat",
      "Xuanlong Yu",
      "Nacim Belkhir",
      "Antoine Manzanera",
      "David Filliat"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ye_LAVSS_Location-Guided_Audio-Visual_Spatial_Audio_Separation_WACV_2024_paper.html": {
    "title": "LAVSS: Location-Guided Audio-Visual Spatial Audio Separation",
    "volume": "main",
    "abstract": "Existing machine learning research has achieved promising results in monaural audio-visual separation (MAVS). However, most MAVS methods purely consider what the sound source is, not where it is located. This can be a problem in VR/AR scenarios, where listeners need to be able to distinguish between similar audio sources located in different directions. To address this limitation, we have generalized MAVS to spatial audio separation and proposed LAVSS: a location-guided audio-visual spatial audio separator. LAVSS is inspired by the correlation between spatial audio and visual location. We introduce the phase difference carried by binaural audio as spatial cues, and we utilize positional representations of sounding objects as additional modality guidance. We also leverage multi-level cross-modal attention to perform visual-positional collaboration with audio features. In addition, we adopt a pre-trained monaural separator to transfer knowledge from rich mono sounds to boost spatial audio separation. This exploits the correlation between monaural and binaural channels. Experiments on the FAIR-Play dataset demonstrate the superiority of the proposed LAVSS over existing benchmarks of audio-visual separation. Our project page: https://yyx666660.github.io/LAVSS/",
    "checked": true,
    "id": "206ae06ec92d323dee4d6c94d185838fd9b24038",
    "semantic_title": "lavss: location-guided audio-visual spatial audio separation",
    "citation_count": 0,
    "authors": [
      "Yuxin Ye",
      "Wenming Yang",
      "Yapeng Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Lee_PIDiffu_Pixel-Aligned_Diffusion_Model_for_High-Fidelity_Clothed_Human_Reconstruction_WACV_2024_paper.html": {
    "title": "PIDiffu: Pixel-Aligned Diffusion Model for High-Fidelity Clothed Human Reconstruction",
    "volume": "main",
    "abstract": "This paper presents the Pixel-aligned Diffusion Model (PIDiffu), a new framework for reconstructing high-fidelity clothed 3D human models from a single image. While existing PIFu variants have made significant advances using more complicated 2D and 3D feature extractions, these methods still suffer from floating artifacts and body part duplication due to their reliance on point-wise occupancy field estimations. PIDiffu employs a diffusion-based strategy for line-wise estimation along the ray direction, conditioned by pixel-aligned features with a guided attention. This approach improves the local details and structural accuracy of the reconstructed body shape and is robust to unfamiliar and complex image features. Moreover, PIDiffu can be easily integrated with existing PIFu-based methods to leverage their advantages. The paper demonstrates that PIDiffu outperforms state-of-the-art methods that do not rely on parametric 3D body models. Especially, our method is superior in handling 'in-the-wild' images, such as those with complex patterned clothes unseen in the training data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jungeun Lee",
      "Sanghun Kim",
      "Hansol Lee",
      "Tserendorj Adiya",
      "Hwasup Lim"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Tang_Kaizen_Practical_Self-Supervised_Continual_Learning_With_Continual_Fine-Tuning_WACV_2024_paper.html": {
    "title": "Kaizen: Practical Self-Supervised Continual Learning With Continual Fine-Tuning",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) has shown remarkable performance in computer vision tasks when trained offline. However, in a Continual Learning (CL) scenario where new data is introduced progressively, models still suffer from catastrophic forgetting. Retraining a model from scratch to adapt to newly generated data is time-consuming and inefficient. Previous approaches suggested re-purposing self-supervised objectives with knowledge distillation to mitigate forgetting across tasks, assuming that labels from all tasks are available during fine-tuning. In this paper, we generalize self-supervised continual learning in a practical setting where available labels can be leveraged in any step of the SSL process. With an increasing number of continual tasks, this offers more flexibility in the pre-training and fine-tuning phases. With Kaizen, we introduce a training architecture that is able to mitigate catastrophic forgetting for both the feature extractor and classifier with a carefully designed loss function. By using a set of comprehensive evaluation metrics reflecting different aspects of continual learning, we demonstrated that Kaizen significantly outperforms previous SSL models in competitive vision benchmarks, with up to 16.5% accuracy improvement on split CIFAR-100. Kaizen is able to balance the trade-off between knowledge retention and learning from new data with an end-to-end model, paving the way for practical deployment of continual learning systems",
    "checked": false,
    "id": "90c10e6a61addc009eca0ee400563c3af380c479",
    "semantic_title": "practical self-supervised continual learning with continual fine-tuning",
    "citation_count": 3,
    "authors": [
      "Chi Ian Tang",
      "Lorena Qendro",
      "Dimitris Spathis",
      "Fahim Kawsar",
      "Cecilia Mascolo",
      "Akhil Mathur"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Lu_SBCFormer_Lightweight_Network_Capable_of_Full-Size_ImageNet_Classification_at_1_WACV_2024_paper.html": {
    "title": "SBCFormer: Lightweight Network Capable of Full-Size ImageNet Classification at 1 FPS on Single Board Computers",
    "volume": "main",
    "abstract": "Computer vision has become increasingly prevalent in solving real-world problems across diverse domains, including smart agriculture, fishery, and livestock management. These applications may not require processing many image frames per second, leading practitioners to use single board computers (SBCs). Although many lightweight networks have been developed for \"mobile/edge\" devices, they primarily target smartphones with more powerful processors and not SBCs with the low-end CPUs. This paper introduces a CNN-ViT hybrid network called SBCFormer, which achieves high accuracy and fast computation on such low-end CPUs. The hardware constraints of these CPUs make the Transformer's attention mechanism preferable to convolution. However, using attention on low-end CPUs presents a challenge: high-resolution internal feature maps demand excessive computational resources, but reducing their resolution results in the loss of local image details. SBCFormer introduces an architectural design to address this issue. As a result, SBCFormer achieves the highest trade-off between accuracy and speed on a Raspberry Pi 4 Model B with an ARM-Cortex A72 CPU. For the first time, it achieves an ImageNet-1K top-1 accuracy of around 80% at a speed of 1.0 frame/sec on the SBC. Code is available at https://github.com/xyongLu/SBCFormer",
    "checked": true,
    "id": "14195d1203bd052428d969a35c41c4ae332ed2cc",
    "semantic_title": "sbcformer: lightweight network capable of full-size imagenet classification at 1 fps on single board computers",
    "citation_count": 0,
    "authors": [
      "Xiangyong Lu",
      "Masanori Suganuma",
      "Takayuki Okatani"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Meronen_Fixing_Overconfidence_in_Dynamic_Neural_Networks_WACV_2024_paper.html": {
    "title": "Fixing Overconfidence in Dynamic Neural Networks",
    "volume": "main",
    "abstract": "Dynamic neural networks are a recent technique that promises a remedy for the increasing size of modern deep learning models by dynamically adapting their computational cost to the difficulty of the inputs. In this way, the model can adjust to a limited computational budget. However, the poor quality of uncertainty estimates in deep learning models makes it difficult to distinguish between hard and easy samples. To address this challenge, we present a computationally efficient approach for post-hoc uncertainty quantification in dynamic neural networks. We show that adequately quantifying and accounting for both aleatoric and epistemic uncertainty through a probabilistic treatment of the last layers improves the predictive performance and aids decision-making when determining the computational budget. In the experiments, we show improvements on CIFAR-100, ImageNet, and Caltech-256 in terms of accuracy, capturing uncertainty, and calibration error",
    "checked": true,
    "id": "571926924b032adbc34dfa0c097bb1930e193a58",
    "semantic_title": "fixing overconfidence in dynamic neural networks",
    "citation_count": 4,
    "authors": [
      "Lassi Meronen",
      "Martin Trapp",
      "Andrea Pilzer",
      "Le Yang",
      "Arno Solin"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ramachandra_Multispectral_Imaging_for_Differential_Face_Morphing_Attack_Detection_A_Preliminary_WACV_2024_paper.html": {
    "title": "Multispectral Imaging for Differential Face Morphing Attack Detection: A Preliminary Study",
    "volume": "main",
    "abstract": "Face morphing attack detection is emerging as an increasingly challenging problem owing to advancements in high-quality and realistic morphing attack generation. Reliable detection of morphing attacks is essential because these attacks are targeted for border control applications. This paper presents a multispectral framework for differential morphing-attack detection (D-MAD). The D-MAD methods are based on using two facial images that are captured from the ePassport (also called the reference image) and the trusted device (for example, Automatic Border Control (ABC) gates) to detect whether the face image presented in ePassport is morphed. The proposed multispectral D-MAD framework introduce a multispectral image captured as a trusted capture to acquire seven different spectral bands to detect morphing attacks. Extensive experiments were conducted on the newly created Multispectral Morphed Datasets (MSMD) with 143 unique data subjects that were captured using both visible and multispectral cameras in multiple sessions. The results indicate the superior performance of the proposed multispectral framework compared to visible images",
    "checked": true,
    "id": "acd3bf0525175c77c32e6019785a7b254b7855c3",
    "semantic_title": "multispectral imaging for differential face morphing attack detection: a preliminary study",
    "citation_count": 0,
    "authors": [
      "Raghavendra Ramachandra",
      "Sushma Venkatesh",
      "Naser Damer",
      "Narayan Vetrekar",
      "R. S. Gad"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Singh_Learning_Robust_Deep_Visual_Representations_From_EEG_Brain_Recordings_WACV_2024_paper.html": {
    "title": "Learning Robust Deep Visual Representations From EEG Brain Recordings",
    "volume": "main",
    "abstract": "Decoding the human brain has been a hallmark of neuroscientists and Artificial Intelligence researchers alike. Reconstruction of visual images from brain Electroencephalography (EEG) signals has garnered a lot of interest due to its applications in brain-computer interfacing. This study proposes a two-stage method where the first step is to obtain EEG-derived features for robust learning of deep representations and subsequently utilize the learned representation for image generation and classification. We demonstrate the generalizability of our feature extraction pipeline across three different datasets using deep-learning architectures with supervised and contrastive learning methods. We have performed the zero-shot EEG classification task to support the generalizability claim further. We observed that a subject invariant linearly separable visual representation was learned using EEG data alone in an unimodal setting that gives better k-means accuracy as compared to a joint representation learning between EEG and images. Finally, we propose a novel framework to transform unseen images into the EEG space and reconstruct them with approximation, showcasing the potential for image reconstruction from EEG signals. Our proposed image synthesis method from EEG shows 62.9% and 36.13% inception score improvement on the EEGCVPR40 and the Thoughtviz datasets, which is better than state-of-the-art performance in GAN",
    "checked": true,
    "id": "e85eca077400548323cb6b17aac6f84a133c2e50",
    "semantic_title": "learning robust deep visual representations from eeg brain recordings",
    "citation_count": 0,
    "authors": [
      "Prajwal Singh",
      "Dwip Dalal",
      "Gautam Vashishtha",
      "Krishna Miyapuram",
      "Shanmuganathan Raman"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Cao_Spiking_Denoising_Diffusion_Probabilistic_Models_WACV_2024_paper.html": {
    "title": "Spiking Denoising Diffusion Probabilistic Models",
    "volume": "main",
    "abstract": "Spiking neural networks (SNNs) have ultra-low energy consumption and high biological plausibility due to their binary and bio-driven nature compared with artificial neural networks (ANNs). While previous research has primarily focused on enhancing the performance of SNNs in classification tasks, the generative potential of SNNs remains relatively unexplored. In our paper, we put forward Spiking Denoising Diffusion Probabilistic Models (SDDPM), a new class of SNN-based generative models that achieve high sample quality. To fully exploit the energy efficiency of SNNs, we propose a purely Spiking U-Net architecture, which achieves comparable performance to its ANN counterpart using only 4 time steps, resulting in significantly reduced energy consumption. Extensive experimental results reveal that our approach achieves state-of-the-art on the generative tasks and substantially outperforms other SNN-based generative models, achieving up to 12x and 6x improvement on the CIFAR-10 and the CelebA datasets, respectively. Moreover, we propose a threshold-guided strategy that can further improve the performances by 2.69% in a training-free manner. The SDDPM symbolizes a significant advancement in the field of SNN generation, injecting new perspectives and potential avenues of exploration. Our code is available at https://github.com/AndyCao1125/SDDPM",
    "checked": true,
    "id": "97560baa2e2b83d81c23f89f9cd1a4aa3bf2cd8a",
    "semantic_title": "spiking denoising diffusion probabilistic models",
    "citation_count": 4,
    "authors": [
      "Jiahang Cao",
      "Ziqing Wang",
      "Hanzhong Guo",
      "Hao Cheng",
      "Qiang Zhang",
      "Renjing Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Petit_An_Analysis_of_Initial_Training_Strategies_for_Exemplar-Free_Class-Incremental_Learning_WACV_2024_paper.html": {
    "title": "An Analysis of Initial Training Strategies for Exemplar-Free Class-Incremental Learning",
    "volume": "main",
    "abstract": "Class-Incremental Learning (CIL) aims to build classification models from data streams. At each step of the CIL process, new classes must be integrated into the model. Due to catastrophic forgetting, CIL is particularly challenging when examples from past classes cannot be stored, the case on which we focus here. To date, most approaches are based exclusively on the target dataset of the CIL process. However, the use of models pre-trained in a self-supervised way on large amounts of data has recently gained momentum. The initial model of the CIL process may only use the first batch of the target dataset, or also use pre-trained weights obtained on an auxiliary dataset. The choice between these two initial learning strategies can significantly influence the performance of the incremental learning model, but has not yet been studied in depth. Performance is also influenced by the choice of the CIL algorithm, the neural architecture, the nature of the target task, the distribution of classes in the stream and the number of examples available for learning. We conduct a comprehensive experimental study to assess the roles of these factors. We present a statistical analysis framework that quantifies the relative contribution of each factor to incremental performance. Our main finding is that the initial training strategy is the dominant factor influencing the average incremental accuracy, but that the choice of CIL algorithm is more important in preventing forgetting. Based on this analysis, we propose practical recommendations for choosing the right initial training strategy for a given incremental learning use case. These recommendations are intended to facilitate the practical deployment of incremental learning",
    "checked": true,
    "id": "89dbadac7e39460e9d5cc7cd9c996e8f31be0899",
    "semantic_title": "an analysis of initial training strategies for exemplar-free class-incremental learning",
    "citation_count": 0,
    "authors": [
      "Grégoire Petit",
      "Michaël Soumm",
      "Eva Feillet",
      "Adrian Popescu",
      "Bertrand Delezoide",
      "David Picard",
      "Céline Hudelot"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Malnick_Taming_Normalizing_Flows_WACV_2024_paper.html": {
    "title": "Taming Normalizing Flows",
    "volume": "main",
    "abstract": "We propose an algorithm for taming Normalizing Flow models - changing the probability that the model will produce a specific image or image category. We focus on Normalizing Flows because they can calculate the exact generation probability likelihood for a given image. We demonstrate taming using models that generate human faces, a subdomain with many interesting privacy and bias considerations. Our method can be used in the context of privacy, e.g., removing a specific person from the output of a model, and also in the context of debiasing by forcing a model to output specific image categories according to a given distribution. Taming is achieved with a fast fine-tuning process without retraining from scratch, achieving the goal in a matter of minutes. We evaluate our method qualitatively and quantitatively, showing that the generation quality remains intact, while the desired changes are applied",
    "checked": true,
    "id": "0c949c68cc15a0bad9ae1da59ca3ba0f4c0f06ea",
    "semantic_title": "taming normalizing flows",
    "citation_count": 0,
    "authors": [
      "Shimon Malnick",
      "Shai Avidan",
      "Ohad Fried"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Hwang_Booster-SHOT_Boosting_Stacked_Homography_Transformations_for_Multiview_Pedestrian_Detection_With_WACV_2024_paper.html": {
    "title": "Booster-SHOT: Boosting Stacked Homography Transformations for Multiview Pedestrian Detection With Attention",
    "volume": "main",
    "abstract": "Improving multi-view aggregation is integral for multi-view pedestrian detection, which aims to obtain a bird's-eye-view pedestrian occupancy map from images captured through a set of calibrated cameras. Inspired by the success of attention modules for deep neural networks, we first propose a Homography Attention Module (HAM) which is shown to boost the performance of existing end-to-end multiview detection approaches by utilizing a novel channel gate and spatial gate. Additionally, we propose Booster-SHOT, an end-to-end convolutional approach to multiview pedestrian detection incorporating our proposed HAM as well as elements from previous approaches such as view-coherent augmentation or stacked homography transformations. Booster-SHOT achieves 92.9% and 94.2% for MODA on Wildtrack and MultiviewX respectively, outperforming the state-of-the-art by 1.4% on Wildtrack and 0.5% on MultiviewX, achieving state-of-the-art performance overall for standard evaluation metrics used in multi-view pedestrian detection",
    "checked": true,
    "id": "f17d85934ef1251d869c8f6085e20beeb99120dd",
    "semantic_title": "booster-shot: boosting stacked homography transformations for multiview pedestrian detection with attention",
    "citation_count": 1,
    "authors": [
      "Jinwoo Hwang",
      "Philipp Benz",
      "Pete Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Corley_ZRG_A_Dataset_for_Multimodal_3D_Residential_Rooftop_Understanding_WACV_2024_paper.html": {
    "title": "ZRG: A Dataset for Multimodal 3D Residential Rooftop Understanding",
    "volume": "main",
    "abstract": "A crucial part of any home is the roof over our heads to protect us from the elements. In this paper we present the Zeitview Rooftop Geometry (ZRG) dataset for residential rooftop understanding. ZRG is a large-scale residential rooftop inspection dataset of over 20k properties from across the U.S. and includes high resolution aerial orthomosaics, digital surface models (DSM), colored point clouds, and 3D roof wireframe annotations. We provide an in-depth analysis and perform several experimental baselines including roof outline extraction, monocular height estimation, and planar roof structure extraction, to illustrate a few of the numerous applications unlocked by this dataset",
    "checked": true,
    "id": "c57596f9f474610db978cba8f5ed3ea789d0e530",
    "semantic_title": "zrg: a dataset for multimodal 3d residential rooftop understanding",
    "citation_count": 0,
    "authors": [
      "Isaac Corley",
      "Jonathan Lwowski",
      "Peyman Najafirad"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Azad_Beyond_Self-Attention_Deformable_Large_Kernel_Attention_for_Medical_Image_Segmentation_WACV_2024_paper.html": {
    "title": "Beyond Self-Attention: Deformable Large Kernel Attention for Medical Image Segmentation",
    "volume": "main",
    "abstract": "Medical image segmentation has seen significant improvements with transformer models, which excel in grasping far-reaching contexts and global contextual information. However, the increasing computational demands of these models, proportional to the squared token count, limit their depth and resolution capabilities. Most current methods process D volumetric image data slice-by-slice (called pseudo 3D), missing crucial inter-slice information and thus reducing the model's overall performance. To address these challenges, we introduce the concept of Deformable Large Kernel Attention (D-LKA Attention), a streamlined attention mechanism employing large convolution kernels to fully appreciate volumetric context. This mechanism operates within a receptive field akin to self-attention while sidestepping the computational overhead. Additionally, our proposed attention mechanism benefits from deformable convolutions to flexibly warp the sampling grid, enabling the model to adapt appropriately to diverse data patterns. We designed both 2D and 3D adaptations of the D-LKA Attention, with the latter excelling in cross-depth data understanding. Together, these components shape our novel hierarchical Vision Transformer architecture, the D-LKA Net. Evaluations of our model against leading methods on popular medical segmentation datasets (Synapse, NIH Pancreas, and Skin lesion) demonstrate its superior performance",
    "checked": true,
    "id": "bacdb732960a52dd3b47ad0f73d2343edf808da0",
    "semantic_title": "beyond self-attention: deformable large kernel attention for medical image segmentation",
    "citation_count": 2,
    "authors": [
      "Reza Azad",
      "Leon Niggemeier",
      "Michael Hüttemann",
      "Amirhossein Kazerouni",
      "Ehsan Khodapanah Aghdam",
      "Yury Velichko",
      "Ulas Bagci",
      "Dorit Merhof"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Seifi_OOD_Aware_Supervised_Contrastive_Learning_WACV_2024_paper.html": {
    "title": "OOD Aware Supervised Contrastive Learning",
    "volume": "main",
    "abstract": "Out-of-Distribution (OOD) detection is a crucial problem for the safe deployment of machine learning models identifying samples that fall outside of the training distribution, i.e. in-distribution data (ID). Most OOD works focus on the classification models trained with Cross Entropy (CE) and attempt to fix its inherent issues. In this work we leverage powerful representation learned with Supervised Contrastive (SupCon) training and propose a holistic approach to learn a classifier robust to OOD data. We extend SupCon loss with two additional contrast terms. The first term pushes auxiliary OOD representations away from ID representations without imposing any constraints on similarities among auxiliary data. The second term pushes OOD features far from the existing class prototypes, while pushing ID representations closer to their corresponding class prototype. When auxiliary OOD data is not available, we propose feature mixing techniques to efficiently generate pseudo-OOD features. Our solution is simple and efficient and acts as a natural extension of the closed-set supervised contrastive representation learning. We compare against different OOD detection methods on the common benchmarks and show state-of-the-art results",
    "checked": true,
    "id": "c8aa8a98d29ff4a9449a80e2ea72f547ee6d5e61",
    "semantic_title": "ood aware supervised contrastive learning",
    "citation_count": 0,
    "authors": [
      "Soroush Seifi",
      "Daniel Olmeda Reino",
      "Nikolay Chumerin",
      "Rahaf Aljundi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Lee_Meta-Learned_Kernel_for_Blind_Super-Resolution_Kernel_Estimation_WACV_2024_paper.html": {
    "title": "Meta-Learned Kernel for Blind Super-Resolution Kernel Estimation",
    "volume": "main",
    "abstract": "Recent image degradation estimation methods have enabled single-image super-resolution (SR) approaches to better upsample real-world images. Among these methods, explicit kernel estimation approaches have demonstrated unprecedented performance at handling unknown degradations. Nonetheless, a number of limitations constrain their efficacy when used by downstream SR models. Specifically, this family of methods yields i) excessive inference time due to long per-image adaptation times and ii)inferior image fidelity due to kernel mismatch. In this work, we introduce a learning-to-learn approach that meta-learns from the information contained in a distribution of images, thereby enabling significantly faster adaptation to new images with substantially improved performance in both kernel estimation and image fidelity. Specifically, we meta-train a kernel-generating GAN, named MetaKernelGAN, on a range of tasks, such that when a new image is presented, the generator starts from an informed kernel estimate and the discriminator starts with a strong capability to distinguish between patch distributions. Compared with state-of-the-art methods, our experiments show that MetaKernelGAN better estimates the magnitude and covariance of the kernel, leading to state-of-the-art blind SR results within a similar computational regime when combined with a non-blind SR model. Through supervised learning of an unsupervised learner, our method maintains the generalizability of the unsupervised learner, improves the optimization stability of kernel estimation, and hence image adaptation, and leads to a faster inference with a speedup between 14.24 to 102.1x over existing methods",
    "checked": true,
    "id": "e87323d3a7d9f2138c1fd5950b12176b4ebadaa0",
    "semantic_title": "meta-learned kernel for blind super-resolution kernel estimation",
    "citation_count": 1,
    "authors": [
      "Royson Lee",
      "Rui Li",
      "Stylianos Venieris",
      "Timothy Hospedales",
      "Ferenc Huszár",
      "Nicholas D. Lane"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Almansoori_DDAM-PS_Diligent_Domain_Adaptive_Mixer_for_Person_Search_WACV_2024_paper.html": {
    "title": "DDAM-PS: Diligent Domain Adaptive Mixer for Person Search",
    "volume": "main",
    "abstract": "Person search (PS) is a challenging computer vision problem where the objective is to achieve joint optimization for pedestrian detection and re-Person search (PS) is a challenging computer vision problem where the objective is to achieve joint optimization for pedestrian detection and re-identification (ReID). Although previous advancements have shown promising performance in the field under fully and weakly supervised learning fashion, there exists a major gap in investigating the domain adaptation ability of PS models. In this paper, we propose a diligent domain adaptive mixer (DDAM) for person search (DDAP-PS) framework that aims to bridge a gap to improve knowledge transfer from the labeled source domain to the unlabeled target domain. Specifically, we introduce a novel DDAM module that generates moderate mixed-domain representations by combining source and target domain representations. The proposed DDAM module encourages domain mixing to minimize the distance between the two extreme domains, thereby enhancing the ReID task. To achieve this, we introduce two bridge losses and a disparity loss. The objective of the two bridge losses is to guide the moderate mixed-domain representations to maintain an appropriate distance from both the source and target domain representations. The disparity loss aims to prevent the moderate mixed-domain representations from being biased towards either the source or target domains, thereby avoiding overfitting. Furthermore, we address the conflict between the two subtasks, localization and ReID, during domain adaptation. To handle this cross-task conflict, we forcefully decouple the norm-aware embedding, which aids in better learning of the moderate mixed-domain representation. We conduct experiments to validate the effectiveness of our proposed method. Our approach demonstrates favorable performance on the challenging PRW and CUHK-SYSU datasets. Our code is publicly available at https://github.com/mustansarfiaz/DDAM",
    "checked": true,
    "id": "1817ecaee757d1e8370bdb76b01522f4585afbb9",
    "semantic_title": "ddam-ps: diligent domain adaptive mixer for person search",
    "citation_count": 0,
    "authors": [
      "Mohammed Khaleed Almansoori",
      "Mustansar Fiaz",
      "Hisham Cholakkal"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Bleidt_ArtQuest_Countering_Hidden_Language_Biases_in_ArtVQA_WACV_2024_paper.html": {
    "title": "ArtQuest: Countering Hidden Language Biases in ArtVQA",
    "volume": "main",
    "abstract": "The task of Visual Question Answering (VQA) has been studied extensively on general-domain real-world images. Transferring insights from general domain VQA to the art domain (ArtVQA) is non-trivial, as the latter requires models to identify abstract concepts, details of brushstrokes and styles of paintings in the visual data as well as possess background knowledge about art. This is exacerbated by the lack of high-quality datasets. In this work, we shed light on hidden linguistic biases in the AQUA dataset, which is the only publicly available benchmark dataset for ArtVQA. As a result, the majority of questions can be answered without consulting the visual information, making the \"V\" in ArtVQA rather insignificant. In order to counter this problem, we create a simple, yet practical dataset, ArtQuest, using structured information from the SemArt collection. Our dataset and the pipeline to reproduce our results are publicly available at https://github.com/bletib/artquest",
    "checked": true,
    "id": "c99a5307ecf2310e208342b926978d5490bfbd5b",
    "semantic_title": "artquest: countering hidden language biases in artvqa",
    "citation_count": 0,
    "authors": [
      "Tibor Bleidt",
      "Sedigheh Eslami",
      "Gerard de Melo"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Gorlo_ISAR_A_Benchmark_for_Single-_and_Few-Shot_Object_Instance_Segmentation_WACV_2024_paper.html": {
    "title": "ISAR: A Benchmark for Single- and Few-Shot Object Instance Segmentation and Re-Identification",
    "volume": "main",
    "abstract": "Most object-level mapping systems in use today make use of an upstream learned object instance segmentation model. If we want to teach them about a new object or segmentation class, we need to build a large dataset and retrain the system. To build spatial AI systems that can quickly be taught about new objects, we need to effectively solve the problem of single-shot object detection, instance segmentation and re-identification. So far there is neither a method fulfilling all of these requirements in unison nor a benchmark that could be used to test such a method. Addressing this, we propose ISAR, a benchmark and baseline method for single- and few-shot object Instance Segmentation And Re-identification, in an effort to accelerate the development of algorithms that can robustly detect, segment, and re-identify objects from a single or a few sparse training examples. We provide a semi-synthetic dataset of video sequences with ground-truth semantic annotations, a standardized evaluation pipeline, and a baseline method. Our benchmark aligns with the emerging research trend of unifying Multi-Object Tracking, Video Object Segmentation, and Re-identification",
    "checked": true,
    "id": "53c7fb960fd5e08105c50b45252de6ba9d90f42f",
    "semantic_title": "isar: a benchmark for single- and few-shot object instance segmentation and re-identification",
    "citation_count": 0,
    "authors": [
      "Nicolas Gorlo",
      "Kenneth Blomqvist",
      "Francesco Milano",
      "Roland Siegwart"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kudale_Textron_Weakly_Supervised_Multilingual_Text_Detection_Through_Data_Programming_WACV_2024_paper.html": {
    "title": "Textron: Weakly Supervised Multilingual Text Detection Through Data Programming",
    "volume": "main",
    "abstract": "Several recent deep learning (DL) based techniques perform considerably well on image-based multilingual text detection. However, their performance relies heavily on the availability and quality of training data. There are numerous types of page-level document images consisting of information in several modalities, languages, fonts, and layouts. This makes text detection a challenging problem in the field of computer vision (CV), especially for low-resource or handwritten languages. Furthermore, there is a scarcity of word-level labeled data for text detection, especially for multilingual settings and Indian scripts that incorporate both printed and handwritten text. Conventionally, Indian script text detection requires training a DL model on plenty of labeled data, but to the best of our knowledge, no relevant datasets are available. Manual annotation of such data requires a lot of time, effort, and expertise. In order to solve this problem, we propose TEXTRON, a Data Programming-based approach, where users can plug various text detection methods into a weak supervision-based learning framework. One can view this approach to multilingual text detection as an ensemble of different CV-based techniques and DL approaches. TEXTRON can leverage the predictions of DL models pre-trained on a significant amount of language data in conjunction with CV-based methods to improve text detection in other languages. We demonstrate that TEXTRON can improve the detection performance for documents written in Indian languages, despite the absence of corresponding labeled data. Further, through extensive experimentation, we show improvement brought about by our approach over the current State-of-the-art (SOTA) models, especially for handwritten Devanagari text. Code and dataset has been made available at https://github.com/IITB-LEAP-OCR/TEXTRON",
    "checked": false,
    "id": "2f5f09e9bb53df94c6e4e834bd5a891a84dee6d8",
    "semantic_title": "t extron : weakly supervised multilingual text detection through data programming",
    "citation_count": 0,
    "authors": [
      "Dhruv Kudale",
      "Badri Vishal Kasuba",
      "Venkatapathy Subramanian",
      "Parag Chaudhuri",
      "Ganesh Ramakrishnan"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Lee_Sharp-NeRF_Grid-Based_Fast_Deblurring_Neural_Radiance_Fields_Using_Sharpness_Prior_WACV_2024_paper.html": {
    "title": "Sharp-NeRF: Grid-Based Fast Deblurring Neural Radiance Fields Using Sharpness Prior",
    "volume": "main",
    "abstract": "Neural Radiance Fields (NeRF) has shown its remarkable performance in neural rendering-based novel view synthesis. However, NeRF suffers from severe visual quality degradation when the input images have been captured under imperfect conditions, such as poor illumination, defocus blurring and lens aberrations. Especially, defocus blur is quite common in the images when they are normally captured using cameras. Although few recent studies have proposed to render sharp images of considerably high-quality, yet they still face many key challenges. In particular, those methods have employed a Multi-Layer Perceptron (MLP) based NeRF which requires tremendous computational time. To overcome these shortcomings, this paper proposes a novel technique Sharp-NeRF---a grid-based NeRF that renders clean and sharp images from the input blurry images within a half an hour training. To do so, we used several grid-based kernels to accurately model the sharpness/blurriness of the scene. The sharpness level of the pixels is computed to learn the spatially varying blur kernels. We have conducted experiments on the benchmarks consisting of blurry images and have evaluated full-reference and non-reference metrics. The qualitative and quantitative results have revealed that our approach renders the sharp novel views with vivid colors and fine details, and it has considerably faster training time than the previous works. Our code is available at https://github.com/benhenryL/SharpNeRF",
    "checked": true,
    "id": "41759568cbc845a19938a2bcfe9b783e020115d2",
    "semantic_title": "sharp-nerf: grid-based fast deblurring neural radiance fields using sharpness prior",
    "citation_count": 0,
    "authors": [
      "Byeonghyeon Lee",
      "Howoong Lee",
      "Usman Ali",
      "Eunbyung Park"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhou_4K-Resolution_Photo_Exposure_Correction_at_125_FPS_With_8K_Parameters_WACV_2024_paper.html": {
    "title": "4K-Resolution Photo Exposure Correction at 125 FPS With ~8K Parameters",
    "volume": "main",
    "abstract": "The illumination of improperly exposed photographs has been widely corrected using deep convolutional neural networks or Transformers. Despite with promising performance, these methods usually suffer from large parameter amounts and heavy computational FLOPs on high-resolution photographs. In this paper, we propose extremely light-weight (with only 8K parameters) Multi-Scale Linear Transformation (MSLT) networks under the multi-layer perception architecture, which can process 4K-resolution sRGB images at 125 Frame-Per-Second (FPS) by a Titan RTX GPU. Specifically, the proposed MSLT networks first decompose an input image into high and low frequency layers by Laplacian pyramid techniques, and then sequentially correct different layers by pixel-adaptive linear transformation, which is implemented by efficient bilateral grid learning or 1x1 convolutions. Experiments on two benchmark datasets demonstrate the efficiency of our MSLTs against the state-of-the-arts on photo exposure correction. Extensive ablation studies validate the effectiveness of our contributions. The code is available at https://github.com/Zhou-Yijie/MSLTNet",
    "checked": true,
    "id": "83d6b4424f2e285c1996f070f9fbf5701a309439",
    "semantic_title": "4k-resolution photo exposure correction at 125 fps with ~8k parameters",
    "citation_count": 1,
    "authors": [
      "Yijie Zhou",
      "Chao Li",
      "Jin Liang",
      "Tianyi Xu",
      "Xin Liu",
      "Jun Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Medina_Context-Based_Interpretable_Spatio-Temporal_Graph_Convolutional_Network_for_Human_Motion_Forecasting_WACV_2024_paper.html": {
    "title": "Context-Based Interpretable Spatio-Temporal Graph Convolutional Network for Human Motion Forecasting",
    "volume": "main",
    "abstract": "Human motion prediction is still an open problem extremely important for autonomous driving and safety applications. Due to the complex spatiotemporal relation of motion sequences, this remains a challenging problem not only for movement prediction but also to perform a preliminary interpretation of the joint connections. In this work, we present a Context-based Interpretable Spatio-Temporal Graph Convolutional Network (CIST-GCN), as an efficient 3D human pose forecasting model based on GCNs that encompasses specific layers, aiding model interpretability and providing information that might be useful when analyzing motion distribution and body behavior. Our architecture extracts meaningful information from pose sequences, aggregates displacements and accelerations into the input model, and finally predicts the output displacements. Extensive experiments on Human 3.6M, AMASS, 3DPW, and ExPI datasets demonstrate that CIST-GCN outperforms previous methods in human motion prediction and robustness. Since the idea of enhancing interpretability for motion prediction has its merits, we showcase experiments towards it and provide preliminary evaluations of such insights here",
    "checked": false,
    "id": "34ac3139aa551a617e11ec998ed7485b2148b27a",
    "semantic_title": "histgnn: hierarchical spatio-temporal graph neural networks for weather forecasting",
    "citation_count": 4,
    "authors": [
      "Edgar Medina",
      "Leyong Loh",
      "Namrata Gurung",
      "Kyung Hun Oh",
      "Niels Heller"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zheng_TPSeNCE_Towards_Artifact-Free_Realistic_Rain_Generation_for_Deraining_and_Object_WACV_2024_paper.html": {
    "title": "TPSeNCE: Towards Artifact-Free Realistic Rain Generation for Deraining and Object Detection in Rain",
    "volume": "main",
    "abstract": "Rain generation algorithms have the potential to improve the generalization of deraining methods and scene understanding in rainy conditions. However, in practice, they produce artifacts and distortions and struggle to control the amount of rain generated due to a lack of proper constraints. In this paper, we propose an unpaired image-to-image translation framework for generating realistic rainy images. We first introduce a Triangular Probability Similarity (TPS) constraint to guide the generated images toward clear and rainy images in the discriminator manifold, thereby minimizing artifacts and distortions during rain generation. Unlike conventional contrastive learning approaches, which indiscriminately push negative samples away from the anchors, we propose a Semantic Noise Contrastive Estimation (SeNCE) strategy and reassess the pushing force of negative samples based on the semantic similarity between the clear and the rainy images and the feature similarity between the anchor and the negative samples. Experiments demonstrate realistic rain generation with minimal artifacts and distortions, which benefits image deraining and object detection in rain. Furthermore, the method can be used to generate realistic snowy and night images, underscoring its potential for broader applicability. Code is available at https://github.com/ShenZheng2000/TPSeNCE",
    "checked": true,
    "id": "88efa6b96ce8d130b67f8834ca3dad2c498b4e93",
    "semantic_title": "tpsence: towards artifact-free realistic rain generation for deraining and object detection in rain",
    "citation_count": 0,
    "authors": [
      "Shen Zheng",
      "Changjie Lu",
      "Srinivasa G. Narasimhan"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Yang_Robust_Category-Level_3D_Pose_Estimation_From_Diffusion-Enhanced_Synthetic_Data_WACV_2024_paper.html": {
    "title": "Robust Category-Level 3D Pose Estimation From Diffusion-Enhanced Synthetic Data",
    "volume": "main",
    "abstract": "Obtaining accurate 3D object poses is vital for numerous computer vision applications, such as 3D reconstruction and scene understanding. However, annotating real-world objects is time-consuming and challenging. While synthetically generated training data is a viable alternative, the domain shift between real and synthetic data is a significant challenge. In this work, we aim to narrow the performance gap between models trained on synthetic data and fully supervised models trained on a large amount of real data. We achieve this by approaching the problem from two perspectives: 1) We introduce P3D-Diffusion, a new synthetic dataset with accurate 3D annotations generated with a graphics-guided diffusion model. 2) We propose Cross-domain 3D Consistency, CC3D, for unsupervised domain adaptation of neural mesh models. In particular, we exploit the spatial relationships between features on the mesh surface and a contrastive learning scheme to guide the domain adaptation process. Combined, these two approaches enable our models to perform competitively with state-of-the-art models using only 10% of the respective real training images, while outperforming the SOTA model by a wide margin using only 50% of the real training data. By encouraging the diversity of synthetic data and generating the images with an OOD-aware manner, our model further demonstrates robust generalization to out-of-distribution scenarios despite being trained with minimal real data",
    "checked": false,
    "id": "55aa226650e6eeed51e181195b7b7a9b87102bc5",
    "semantic_title": "robust category-level 3d pose estimation from synthetic data",
    "citation_count": 0,
    "authors": [
      "Jiahao Yang",
      "Wufei Ma",
      "Angtian Wang",
      "Xiaoding Yuan",
      "Alan Yuille",
      "Adam Kortylewski"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Rad_Vision_Transformer_for_Multispectral_Satellite_Imagery_Advancing_Landcover_Classification_WACV_2024_paper.html": {
    "title": "Vision Transformer for Multispectral Satellite Imagery: Advancing Landcover Classification",
    "volume": "main",
    "abstract": "Climate change is a global issue with significant impacts on ecosystems and human populations. Accurately classifying land cover from multi-spectral satellite imagery plays a crucial role in understanding the Earth's changing landscape and its implications for environmental processes. However, traditional methods struggle with challenges like limited data availability and capturing complex spatial-spectral relationships. Vision Transformers have emerged as a promising alternative to convolutional neural networks (CNN architectures), harnessing the power of self-attention mechanisms to capture global and long-range dependencies. However, their application to multi-spectral images is still limited. In this paper, we propose a novel Vision Transformer designed for multi-spectral satellite image datasets of limited size to perform reliable land cover identification with forty-four classes. We conduct extensive experiments on a curated dataset, simulating scenarios with limited data availability, and compare our approach to alternative architectures. The results demonstrate the potential of our Vision Transformer-based method in achieving accurate land cover classification, contributing to improving climate change modeling and environmental understanding",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryan Rad"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Lau_ENTED_Enhanced_Neural_Texture_Extraction_and_Distribution_for_Reference-Based_Blind_WACV_2024_paper.html": {
    "title": "ENTED: Enhanced Neural Texture Extraction and Distribution for Reference-Based Blind Face Restoration",
    "volume": "main",
    "abstract": "We present ENTED, a new framework for blind face restoration that aims to restore high-quality and realistic portrait images. Our method involves repairing a single degraded input image using a high-quality reference image. We utilize a texture extraction and distribution framework to transfer high-quality texture features between the degraded input and reference image. However, the StyleGAN-like architecture in our framework requires high-quality latent codes to generate realistic images. The latent code extracted from the degraded input image often contains corrupted features, making it difficult to align the semantic information from the input with the high-quality textures from the reference. To overcome this challenge, we employ two special techniques. The first technique, inspired by vector quantization, replaces corrupted semantic features with high-quality code words. The second technique generates style codes that carry photorealistic texture information from a more informative latent space developed using the high-quality features in the reference image's manifold. Extensive experiments conducted on synthetic and real-world datasets demonstrate that our method produces results with more realistic contextual details and outperforms state-of-the-art methods. A thorough ablation study confirms the effectiveness of each proposed module",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuen-Fui Lau",
      "Tianjia Zhang",
      "Zhefan Rao",
      "Qifeng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Chen_A_Sequential_Learning-Based_Approach_for_Monocular_Human_Performance_Capture_WACV_2024_paper.html": {
    "title": "A Sequential Learning-Based Approach for Monocular Human Performance Capture",
    "volume": "main",
    "abstract": "Human performance capture from RGB videos in unconstrained environments has become very popular for applications that require generating virtual avatars or digital actors. SOTA methods use neural network (NN) techniques to estimate the shape directly from photos, yielding a simplified model of the human body. While effective, NN techniques frequently fail under challenging poses and do not preserve temporal consistency. On the other hand, optimization-based methods like shape-from-silhouette can produce more precise reconstruction; however, they typically require a good initialization and are computationally more intensive than NN. To address issues of previous methods, this work proposes a learning-based approach for optimizing fine-grained shape representation (e.g., clothes, wrinkles) from a monocular RGB video. Our main idea is to sequentially recover different shape details (e.g., average shape, clothing, wrinkles) using separate neural networks. At each level, our network takes the sparse/noisy gradients of body mesh vertices w.r.t the shape, and predicts dense gradients to update the body shape. Despite being trained on synthetic data, these networks have surprisingly good generalization to real images. Experimental validation shows that our approach outperforms NN approaches in recovering shape details while also being an order of magnitude faster than optimization-based methods and robust across varied poses and novel views",
    "checked": false,
    "id": "0061f3b3f3b1f135d9e94922ab9a9fde84a871c6",
    "semantic_title": "an efficient approach for sequential human performance capture from monocular video",
    "citation_count": 0,
    "authors": [
      "Jianchun Chen",
      "Jayakorn Vongkulbhisal",
      "Fernando De la Torre Frade"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wang_VCISR_Blind_Single_Image_Super-Resolution_With_Video_Compression_Synthetic_Data_WACV_2024_paper.html": {
    "title": "VCISR: Blind Single Image Super-Resolution With Video Compression Synthetic Data",
    "volume": "main",
    "abstract": "In the blind single image super-resolution (SISR) task, existing works have been successful in restoring image-level unknown degradations. However, when a single video frame becomes the input, these works usually fail to address degradations caused by video compression, such as mosquito noise, ringing, blockiness, and staircase noise. In this work, we for the first time, present a video compression-based degradation model to synthesize low-resolution image data in the blind SISR task. Our proposed image synthesizing method is widely applicable to existing image datasets, so that a single degraded image can contain distortions caused by the lossy video compression algorithms. This overcomes the leak of feature diversity in video data and thus retains the training efficiency. By introducing video coding artifacts to SISR degradation models, neural networks can super-resolve images with the ability to restore video compression degradations, and achieve better results on restoring generic distortions caused by image compression as well. Our proposed approach achieves superior performance in SOTA no-reference Image Quality Assessment, and shows better visual quality on various datasets. In addition, we evaluate the SISR neural network trained with our degradation model on video super-resolution (VSR) datasets. Compared to architectures specifically designed for the VSR purpose, our method exhibits similar or better performance, evidencing that the presented strategy on infusing video-based degradation is generalizable to address more complicated compression artifacts even without temporal cues. The code is available at https://github.com/Kiteretsu77/VCISR-official",
    "checked": true,
    "id": "7c4ac9ab31d2d9b259bffcfd6385da81efffa6e9",
    "semantic_title": "vcisr: blind single image super-resolution with video compression synthetic data",
    "citation_count": 0,
    "authors": [
      "Boyang Wang",
      "Bowen Liu",
      "Shiyu Liu",
      "Fengyu Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Pan_Synthesizing_Coherent_Story_With_Auto-Regressive_Latent_Diffusion_Models_WACV_2024_paper.html": {
    "title": "Synthesizing Coherent Story With Auto-Regressive Latent Diffusion Models",
    "volume": "main",
    "abstract": "Conditioned diffusion models have demonstrated state-of-the-art text-to-image synthesis capacity. Recently, most works focus on synthesizing independent images; While for real-world applications, it is common and necessary to generate a series of coherent images for story-stelling. In this work, we mainly focus on story visualization and continuation tasks and propose AR-LDM, a latent diffusion model auto-regressively conditioned on history captions and generated images. Moreover, AR-LDM can generalize to new characters through adaptation. To our best knowledge, this is the first work successfully leveraging diffusion models for coherent visual story synthesizing. It also extends the text-conditioned method to multimodal conditioning. Quantitative results show that AR-LDM achieves SoTA FID scores on PororoSV, FlintstonesSV, and the adopted challenging dataset VIST containing natural images. Large-scale human evaluations show that AR-LDM has superior performance in terms of quality, relevance, and consistency",
    "checked": true,
    "id": "33ef78737ba57ecc1ff98c22369a8e17ed90eb98",
    "semantic_title": "synthesizing coherent story with auto-regressive latent diffusion models",
    "citation_count": 19,
    "authors": [
      "Xichen Pan",
      "Pengda Qin",
      "Yuhong Li",
      "Hui Xue",
      "Wenhu Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Text-to-Image_Editing_by_Image_Information_Removal_WACV_2024_paper.html": {
    "title": "Text-to-Image Editing by Image Information Removal",
    "volume": "main",
    "abstract": "Diffusion models have demonstrated impressive performance in text-guided image generation. Current methods that leverage the knowledge of these models for image editing either fine-tune them using the input image (e.g., Imagic) or incorporate structure information as additional constraints (e.g., ControlNet). However, fine-tuning large-scale diffusion models on a single image can lead to severe overfitting issues and lengthy inference time. Information leakage from pretrained models also make it challenging to preserve image content not related to the text input. Additionally, methods that incorporate structural guidance (e.g., edge maps, semantic maps, keypoints) find retaining attributes like colors and textures difficult. Using the input image as a control could mitigate these issues, but since these models are trained via reconstruction, a model can simply hide information about the original image when encoding it to perfectly reconstruct the image without learning the editing task. To address these challenges, we propose a text-to-image editing model with an Image Information Removal module (IIR) that selectively erases color-related and texture-related information from the original image, allowing us to better preserve the text-irrelevant content and avoid issues arising from information hiding. Our experiments on CUB, Outdoor Scenes, and COCO reports our approach achieves the best editability-fidelity trade-off results. In addition, a user study on COCO shows that our edited images are preferred 35% more often than prior work",
    "checked": true,
    "id": "4c8504f06a0063bd32e7e78efd168549e1f6a7c6",
    "semantic_title": "text-to-image editing by image information removal",
    "citation_count": 4,
    "authors": [
      "Zhongping Zhang",
      "Jian Zheng",
      "Zhiyuan Fang",
      "Bryan A. Plummer"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wang_Self-Annotated_3D_Geometric_Learning_for_Smeared_Points_Removal_WACV_2024_paper.html": {
    "title": "Self-Annotated 3D Geometric Learning for Smeared Points Removal",
    "volume": "main",
    "abstract": "There has been significant progress in improving the accuracy and quality of consumer-level dense depth sensors. Nevertheless, there remains a common depth pixel artifact which we call smeared points. These are points not on any 3D surface and typically occur as interpolations between foreground and background objects. As they cause fictitious surfaces, these points have the potential to harm applications dependent on the depth maps. Statistical outlier removal methods fare poorly in removing these points as they tend also to remove actual surface points. Trained network-based point removal faces difficulty in obtaining sufficient annotated data. To address this, we propose a fully self-annotated method to train a smeared point removal classifier. Our approach relies on gathering 3D geometric evidence from multiple perspectives to automatically detect and annotate smeared points and valid points. To validate the effectiveness of our method, we present a new benchmark dataset: the Real Azure-Kinect dataset. Experimental results and ablation studies show that our method outperforms traditional filters and other self-annotated methods. Our work is publicly available at https://github.com/wangmiaowei/wacv2024_smearedremover.git",
    "checked": true,
    "id": "fc5865f2802df8f705a9f65d950219586938bf23",
    "semantic_title": "self-annotated 3d geometric learning for smeared points removal",
    "citation_count": 0,
    "authors": [
      "Miaowei Wang",
      "Daniel Morris"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Gurbuz_Deep_Metric_Learning_With_Chance_Constraints_WACV_2024_paper.html": {
    "title": "Deep Metric Learning With Chance Constraints",
    "volume": "main",
    "abstract": "Deep metric learning (DML) aims to minimize empirical expected loss of the pairwise intra-/inter- class proximity violations in the embedding space. We relate DML to feasibility problem of finite chance constraints. We show that minimizer of proxy-based DML satisfies certain chance constraints, and that the worst case generalization performance of the proxy-based methods can be characterized by the radius of the smallest ball around a class proxy to cover the entire domain of the corresponding class samples, suggesting multiple proxies per class helps performance. To provide a scalable algorithm as well as exploiting more proxies, we consider the chance constraints implied by the minimizers of proxy-based DML instances and reformulate DML as finding a feasible point in intersection of such constraints, resulting in a problem to be approximately solved by iterative projections. Simply put, we repeatedly train a regularized proxy-based loss and re-initialize the proxies with the embeddings of the deliberately selected new samples. We applied our method with 4 well-accepted DML losses and show the effectiveness with extensive evaluations on 4 popular DML benchmarks. Code is available at: https://github.com/yetigurbuz/ccp-dml",
    "checked": true,
    "id": "2258a78041e0eefd525765f94a93d4877cbf6ed9",
    "semantic_title": "deep metric learning with chance constraints",
    "citation_count": 2,
    "authors": [
      "Yeti Z. Gürbüz",
      "Oğul Can",
      "Aydin Alatan"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Parslov_CrashCar101_Procedural_Generation_for_Damage_Assessment_WACV_2024_paper.html": {
    "title": "CrashCar101: Procedural Generation for Damage Assessment",
    "volume": "main",
    "abstract": "In this paper, we are interested in addressing the problem of damage assessment for vehicles, such as cars. This task requires not only detecting the location and the extent of the damage but also identifying the damaged part. To train a computer vision system for the semantic part and damage segmentation in images, we need to manually annotate images with costly pixel annotations for both part categories and damage types. To overcome this need, we propose to use synthetic data to train these models. Synthetic data can provide samples with high variability, pixel-accurate annotations, and arbitrarily large training sets without any human intervention. We propose a procedural generation pipeline that damages 3D car models and we obtain synthetic 2D images of damaged cars paired with pixel-accurate annotations for part and damage categories. To validate our idea, we execute our pipeline and render our CrashCar101 dataset. We run experiments on three real datasets for the tasks of part and damage segmentation. For part segmentation, we show that the segmentation models trained on a combination of real data and our synthetic data outperform all models trained only on real data. For damage segmentation, we show the sim2real transfer ability of CrashCar101",
    "checked": true,
    "id": "3c3dd10cfada0b762cfcd3f277fab6babdddaf61",
    "semantic_title": "crashcar101: procedural generation for damage assessment",
    "citation_count": 0,
    "authors": [
      "Jens Parslov",
      "Erik Riise",
      "Dim P. Papadopoulos"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Reddy_Towards_Domain-Aware_Knowledge_Distillation_for_Continual_Model_Generalization_WACV_2024_paper.html": {
    "title": "Towards Domain-Aware Knowledge Distillation for Continual Model Generalization",
    "volume": "main",
    "abstract": "Generalization on unseen domains is critical for Deep Neural Networks (DNNs) to perform well in real-world applications such as autonomous navigation. However, catastrophic forgetting limit the ability of domain generalization and unsupervised domain adaption approaches to adapt to constantly changing target domains. To overcome these challenges, We propose DoSe framework, a Domain-aware Self-Distillation method based on batch normalization prototypes to facilitate continual model generalization across varying target domains. Specifically, we enforce the consistency of batch normalization statistics between two batches of images sampled from the same target domain distribution between the student and teacher models. To alleviate catastrophic forgetting, we introduce a novel exemplar-based replay buffer to identify difficult samples for the model to retain the knowledge. Specifically, we demonstrate that identifying difficult samples and updating the model periodically using them can help in preserving knowledge learned from previously seen domains. We conduct extensive experiments on two real-world datasets ACDC, C-Driving, and one synthetic dataset SHIFT to verify the efficiency of the proposed DoSe framework. On ACDC, our method outperforms existing SOTA in Domain Generalization, Unsupervised Domain Adaptation, and Daytime settings by 26%, 14%, and 70% respectively",
    "checked": false,
    "id": "d99ec977d75a3a7c1d73185ef438a265edca4dde",
    "semantic_title": "lifelong pretraining: continually adapting language models to emerging corpora",
    "citation_count": 0,
    "authors": [
      "Nikhil Reddy",
      "Mahsa Baktashmotlagh",
      "Chetan Arora"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Yang_SCoRD_Subject-Conditional_Relation_Detection_With_Text-Augmented_Data_WACV_2024_paper.html": {
    "title": "SCoRD: Subject-Conditional Relation Detection With Text-Augmented Data",
    "volume": "main",
    "abstract": "We propose Subject-Conditional Relation Detection SCoRD, where conditioned on an input subject, the goal is to predict all its relations to other objects in a scene along with their locations. Based on the Open Images dataset, we propose a challenging OIv6-SCoRD benchmark such that the training and testing splits have a distribution shift in terms of the occurrence statistics of <subject, relation, object> triplets. To solve this problem, we propose an auto-regressive model that given a subject, it predicts its relations, objects, and object locations by casting this output as a sequence of tokens. First, we show that previous scene-graph prediction methods fail to produce as exhaustive an enumeration of relation-object pairs when conditioned on a subject on this benchmark. Particularly, we obtain a recall@3 of 83.8% for our relation-object predictions compared to the 49.75% obtained by a recent scene graph detector. Then, we show improved generalization on both relation-object and object-box predictions by leveraging during training relation-object pairs obtained automatically from textual captions and for which no object-box annotations are available. Particularly, for <subject, relation, object> triplets for which no object locations are available during training, we are able to obtain a recall@3 of 33.80% for relation-object pairs and 26.75% for their box locations",
    "checked": true,
    "id": "73f58a04fb694fd31e88224feda96ae8de5afa00",
    "semantic_title": "scord: subject-conditional relation detection with text-augmented data",
    "citation_count": 0,
    "authors": [
      "Ziyan Yang",
      "Kushal Kafle",
      "Zhe Lin",
      "Scott Cohen",
      "Zhihong Ding",
      "Vicente Ordonez"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhao_THInImg_Cross-Modal_Steganography_for_Presenting_Talking_Heads_in_Images_WACV_2024_paper.html": {
    "title": "THInImg: Cross-Modal Steganography for Presenting Talking Heads in Images",
    "volume": "main",
    "abstract": "Cross-modal Steganography is the practice of concealing secret signals in publicly available cover signals (distinct from the modality of the secret signals) unobtrusively. While previous approaches primarily concentrated on concealing a relatively small amount of information, we propose THInImg, which manages to hide lengthy audio data (and subsequently decode talking head video) inside an identity image by leveraging the properties of human face, which can be effectively utilized for covert communication, transmission and copyright protection. THInImg consists of two parts: the encoder and decoder. Inside the encoder-decoder pipeline, we introduce a novel architecture that substantially increase the capacity of hiding audio in images. Moreover, our framework can be extended to iteratively hide multiple audio clips into an identity image, offering multiple levels of control over permissions. We conduct extensive experiments to prove the effectiveness of our method, demonstrating that THInImg can present up to 80 seconds of high quality talking-head video (including audio) in an identity image with 160x160 resolution",
    "checked": true,
    "id": "e45cca9eca8c52c397eb88d2afb0696002425647",
    "semantic_title": "thinimg: cross-modal steganography for presenting talking heads in images",
    "citation_count": 0,
    "authors": [
      "Lin Zhao",
      "Hongxuan Li",
      "Xuefei Ning",
      "Xinru Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ahmad_Causal_Analysis_for_Robust_Interpretability_of_Neural_Networks_WACV_2024_paper.html": {
    "title": "Causal Analysis for Robust Interpretability of Neural Networks",
    "volume": "main",
    "abstract": "Interpreting the inner function of neural networks is crucial for the trustworthy development and deployment of these black-box models. Prior interpretability methods focus on correlation-based measures to attribute model decisions to individual examples. However, these measures are susceptible to noise and spurious correlations encoded in the model during the training phase (e.g., biased inputs, model overfitting, or misspecification). Moreover, this process has proven to result in noisy and unstable attributions that prevent any transparent understanding of the model's behavior. In this paper, we develop a robust interventional-based method grounded by causal analysis to capture cause-effect mechanisms in pre-trained neural networks and their relation to the prediction. Our novel approach relies on path interventions to infer the causal mechanisms within hidden layers and isolate relevant and necessary information (to model prediction), avoiding noisy ones. The result is task-specific causal explanatory graphs that can audit model behavior and express the actual causes underlying its performance. We apply our method to vision models trained on classification tasks. On image classification tasks, we provide extensive quantitative experiments to show that our approach can capture more stable and faithful explanations than standard attribution-based methods. Furthermore, the underlying causal graphs express the neural interactions in the model, making it a valuable tool in other applications (e.g., model repair)",
    "checked": true,
    "id": "14b37d576fc7f9b241ddfabc7208f6b7c353ea2c",
    "semantic_title": "causal analysis for robust interpretability of neural networks",
    "citation_count": 0,
    "authors": [
      "Ola Ahmad",
      "Nicolas Béreux",
      "Loïc Baret",
      "Vahid Hashemi",
      "Freddy Lecue"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ashraf_TransFed_A_Way_To_Epitomize_Focal_Modulation_Using_Transformer-Based_Federated_WACV_2024_paper.html": {
    "title": "TransFed: A Way To Epitomize Focal Modulation Using Transformer-Based Federated Learning",
    "volume": "main",
    "abstract": "Federated learning has emerged as a promising paradigm for collaborative machine learning, enabling multiple clients to train a model while preserving data privacy jointly. Tailored federated learning takes this concept further by accommodating client heterogeneity and facilitating the learning of personalized models. While the utilization of transformers within federated learning has attracted significant interest, there remains a need to investigate the effects of federated learning algorithms on the latest focal modulation-based transformers. In this paper, we investigate this relationship and uncover the detrimental effects of federated averaging (FedAvg) algorithms on Focal Modulation, particularly in scenarios with heterogeneous data. To address this challenge, we propose TransFed, a novel transformer-based federated learning framework that not only aggregates model parameters but also learns tailored Focal Modulation for each client. Instead of employing a conventional customization mechanism that maintains client-specific focal modulation layers locally, we introduce a learn-to-tailor approach that fosters client collaboration, enhancing scalability and adaptation in TransFed. Our method incorporates a hyper network on the server, responsible for learning personalized projection matrices for the focal modulation layers. This enables the generation of client-specific keys, values, and queries. Furthermore, we provide an analysis of adaptation bounds for TransFed using the learn-to-customize mechanism. Through intensive experiments on datasets related to pneumonia classification, we demonstrate that TransFed, in combination with the learn-to-tailor approach, achieves superior performance in scenarios with non-IID data distributions, surpassing existing methods. Overall, TransFed paves the way for leveraging focal Modulation in federated learning, advancing the capabilities of focal modulated transformer models in decentralized environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tajamul Ashraf",
      "Fuzayil Bin Afzal Mir",
      "Iqra Altaf Gillani"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Hsiao_Natural_Light_Can_Also_Be_Dangerous_Traffic_Sign_Misinterpretation_Under_WACV_2024_paper.html": {
    "title": "Natural Light Can Also Be Dangerous: Traffic Sign Misinterpretation Under Adversarial Natural Light Attacks",
    "volume": "main",
    "abstract": "Common illumination sources like sunlight or artificial light may introduce hidden vulnerabilities to AI systems. Our paper delves into these potential threats, offering a novel approach to simulate varying light conditions, including sunlight, headlights, and flashlight illuminations. Moreover, unlike typical physical adversarial attacks requiring conspicuous alterations, our method utilizes a model-agnostic black-box attack integrated with the Zeroth Order Optimization (ZOO) algorithm to identify deceptive patterns in a physically-applicable space. Consequently, attackers can recreate these simulated conditions, deceiving machine learning models with seemingly natural light. Empirical results demonstrate the efficacy of our method, misleading models trained on the GTSRB and LISA datasets under natural-like physical environments with an attack success rate exceeding 70% across all digital datasets, and remaining effective against all evaluated real-world traffic signs. Importantly, after adversarial training using samples generated from our approach, models showcase enhanced robustness, underscoring the dual value of our work in both identifying and mitigating potential threats",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Teng-Fang Hsiao",
      "Bo-Lun Huang",
      "Zi-Xiang Ni",
      "Yan-Ting Lin",
      "Hong-Han Shuai",
      "Yung-Hui Li",
      "Wen-Huang Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Shyam_PAIR_Perception_Aided_Image_Restoration_for_Natural_Driving_Conditions_WACV_2024_paper.html": {
    "title": "PAIR: Perception Aided Image Restoration for Natural Driving Conditions",
    "volume": "main",
    "abstract": "We present a two-stage mechanism for generic image restoration in natural driving conditions, where multiple non-linear degradations simultaneously impact perception for humans and driving assistance systems. Our approach overcomes the limitations of utilizing a single neural network that incurs excessive computational overhead and yields sub-optimal recovery. The proposed first stage comprises computationally inexpensive image processing operations applied at a patch level using a lightweight convolutional neural network (CNN) that determines their intensity of operation. This patch size is guided by the receptive field of the CNN, allowing for dynamic restoration of non-linear and non-homogeneous degradation profiles. The second stage leverages a lightweight end-to-end neural network functioning as an inpainting network. It identifies inadequately restored regions and leverages global semantic and structural information to fill the affected areas. This approach enhances the restoration process by considering the entire image and addresses the remainder of localized deficiencies. In addition, we integrate dense perception tasks such as semantic and depth estimation during the optimization cycle to ensure restored images that are perceptually pleasing and conducive for downstream perception tasks. Since datasets covering diverse degradation scenarios for high- and low-level perception tasks are lacking, we utilize a synthetic data augmentation technique to generate non-homogeneous non-linear degradation profiles. Experiments on images captured in adverse weather conditions demonstrate the efficacy of our approach, yielding higher perceptual quality in restored images and improved performance in downstream perception tasks under adverse driving conditions. Importantly, our method offers computational efficiency compared to end-to-end image restoration algorithms, making it suitable for real-time applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pranjay Shyam",
      "HyunJin Yoo"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kohler_RecycleNet_Latent_Feature_Recycling_Leads_to_Iterative_Decision_Refinement_WACV_2024_paper.html": {
    "title": "RecycleNet: Latent Feature Recycling Leads to Iterative Decision Refinement",
    "volume": "main",
    "abstract": "Despite the remarkable success of deep learning systems over the last decade, a key difference still remains between neural network and human decision-making: As humans, we can not only form a decision on the spot, but also ponder, revisiting an initial guess from different angles, distilling relevant information, arriving at a better decision. Here, we propose RecycleNet, a latent feature recycling method, instilling the pondering capability for neural networks to refine initial decisions over a number of recycling steps, where outputs are fed back into earlier network layers in an iterative fashion. This approach makes minimal assumptions about the neural network architecture and thus can be implemented in a wide variety of contexts. Using medical image segmentation as the evaluation environment, we show that latent feature recycling enables the network to iteratively refine initial predictions even beyond the iterations seen during training, converging towards an improved decision. We evaluate this across a variety of segmentation benchmarks and show consistent improvements even compared with top-performing segmentation methods. This allows trading increased computation time for improved performance, which can be beneficial, especially for safety-critical applications",
    "checked": true,
    "id": "62ee49179762926fd2e263ac17ad8bb35b034017",
    "semantic_title": "recyclenet: latent feature recycling leads to iterative decision refinement",
    "citation_count": 0,
    "authors": [
      "Gregor Köhler",
      "Tassilo Wald",
      "Constantin Ulrich",
      "David Zimmerer",
      "Paul F. Jäger",
      "Jörg K.H. Franke",
      "Simon Kohl",
      "Fabian Isensee",
      "Klaus H. Maier-Hein"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Khan_CamoFocus_Enhancing_Camouflage_Object_Detection_With_Split-Feature_Focal_Modulation_and_WACV_2024_paper.html": {
    "title": "CamoFocus: Enhancing Camouflage Object Detection With Split-Feature Focal Modulation and Context Refinement",
    "volume": "main",
    "abstract": "Camouflage Object Detection (COD) involves the challenge of isolating a target object from a visually similar background, presenting a formidable challenge for learning algorithms. Drawing inspiration from state-of-the-art (SOTA) Focal Modulation Networks, our objective is to proficiently modulate the foreground and background components, thereby capturing the distinct features of each. We introduce a Feature Split and Modulation (FSM) module to attain this goal. This module efficiently separates the object from the background by utilizing foreground and background modulators guided by a supervisory mask. For enhanced feature refinement, we propose a Context Refinement Module (CRM), which considers features acquired from FSM across various spatial scales, leading to comprehensive enrichment and highly accurate prediction maps. Through extensive experimentation, we showcase the superiority of CamoFocus over recent SOTA COD methods. Our evaluations encompass diverse benchmark datasets, including CAMO, COD10K, CHAMELEON, and NC4K. The findings underscore the potential and significance of the proposed CamoFocus model and establish its efficacy in addressing the critical challenges of camouflage object detection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abbas Khan",
      "Mustaqeem Khan",
      "Wail Gueaieb",
      "Abdulmotaleb El Saddik",
      "Giulia De Masi",
      "Fakhri Karray"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Noguchi_Scene_Text_Image_Super-Resolution_Based_on_Text-Conditional_Diffusion_Models_WACV_2024_paper.html": {
    "title": "Scene Text Image Super-Resolution Based on Text-Conditional Diffusion Models",
    "volume": "main",
    "abstract": "Scene Text Image Super-resolution (STISR) has recently achieved great success as a preprocessing method for scene text recognition. STISR aims to transform blurred and noisy low-resolution (LR) text images in real-world settings into clear high-resolution (HR) text images suitable for scene text recognition. In this study, we leverage text-conditional diffusion models (DMs), known for their impressive text-to-image synthesis capabilities, for STISR tasks. Our experimental results revealed that text-conditional DMs notably surpass existing STISR methods. Especially when texts from LR text images are given as input, the text-conditional DMs are able to produce superior quality super-resolution text images. Utilizing this capability, we propose a novel framework for synthesizing LR-HR paired text image datasets. This framework consists of three specialized text-conditional DMs, each dedicated to text image synthesis, super-resolution, and image degradation. These three modules are vital for synthesizing distinct LR and HR paired images, which are more suitable for training STISR methods. Our experiments confirmed that these synthesized image pairs significantly enhance the performance of STISR methods in the TextZoom evaluation",
    "checked": true,
    "id": "7a225d2782d839e28c894759cf052e4ff0498ec6",
    "semantic_title": "scene text image super-resolution based on text-conditional diffusion models",
    "citation_count": 1,
    "authors": [
      "Chihiro Noguchi",
      "Shun Fukuda",
      "Masao Yamanaka"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Pal_Domain_Adaptive_3D_Shape_Retrieval_From_Monocular_Images_WACV_2024_paper.html": {
    "title": "Domain Adaptive 3D Shape Retrieval From Monocular Images",
    "volume": "main",
    "abstract": "In this work, we address the novel and challenging problem of domain adaptive 3D shape retrieval from single 2D images (DA-IBSR). While the existing image-based 3D shape retrieval (IBSR) problem focuses on modality alignment for retrieving a matchable 3D shape from a shape repository given a 2D image query, it does not consider any distribution shift between the training and testing image-shape pairs, making the performance of off-the-shelves IBSR methods subpar. In contrast, the proposed DA-IBSR addresses the non-trivial problem of modality shift as well distribution shift across training and test sets. To address these issues, we propose an end-to-end trainable model called DAIS-NET. Our objective is to align the images and shapes separately from both domains while simultaneously learn a shared embedding space for the 2D and 3D modalities. The former problem is addressed by separately employing maximum mean discrepancy loss across the 2D images and 3D shapes of the two domains. To address the modality alignment, we incorporate the notion of negative sample mining and employ triplet loss to bridge the gap between positive 2D-3D pairs (of same class) and increase the separation between negative 2D-3D pairs (of different class). Additionally, we employ an entropy minimization strategy to align the unlabeled target domain data in the semantic space. To evaluate our proposed approach, we define the experimental setting of DA-IBSR on the following benchmarks: SHREC'14 <-> Pix3D and ShapeNet <-> SHREC'14. Considering the novelty of the problem statement, we have demonstrated that the issue of domain gap is prevalent by comparing our method with the existing literature. Additionally, through extensive evaluations, we demonstrate the capability of DAIS-NET to successfully mitigate this domain gap in image based 3D shape retrieval",
    "checked": false,
    "id": "df939395f6710c05a5202330edc9ea8811ab3a77",
    "semantic_title": "learning to in-paint: domain adaptive shape completion for 3d organ segmentation",
    "citation_count": 0,
    "authors": [
      "Harsh Pal",
      "Ritwik Khandelwal",
      "Shivam Pande",
      "Biplab Banerjee",
      "Srikrishna Karanam"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wang_Learning_Quality_Labels_for_Robust_Image_Classification_WACV_2024_paper.html": {
    "title": "Learning Quality Labels for Robust Image Classification",
    "volume": "main",
    "abstract": "Current deep learning paradigms largely benefit from the tremendous amount of annotated data. However, the quality of the annotations often varies among labelers. Multi-observer studies have been conducted to examine the annotation variances (by labeling the same data multiple times) and their effects on critical applications like medical image analysis. In this paper, we demonstrate how multiple sets of annotations (either hand-labeled or algorithm-generated) can be utilized together and mutually benefit the learning of classification tasks. The concept of learning-to-vote is introduced to sample quality label sets for each data entry on-the-fly during the training. Specifically, a meta-training-based label-sampling module is designed to achieve refined labels (weighted sum of attended ones) that benefit the model learning the most through additional back-propagations. We apply the learning-to-vote scheme on the classification task of a synthetic noisy CIFAR-10 to prove the concept and then demonstrate superior results (3-5% increase on average in multiple disease classification AUCs) on the chest x-ray images from a hospital-scale dataset (MIMIC-CXR) and hand-labeled dataset (OpenI) in comparison to regular training paradigms",
    "checked": false,
    "id": "c395f26e1eb89835e31ac9c18bff686497f947f6",
    "semantic_title": "rectified meta-learning from noisy labels for robust image-based plant disease classification",
    "citation_count": 8,
    "authors": [
      "Xiaosong Wang",
      "Ziyue Xu",
      "Dong Yang",
      "Leo Tam",
      "Holger Roth",
      "Daguang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Chang_LibreFace_An_Open-Source_Toolkit_for_Deep_Facial_Expression_Analysis_WACV_2024_paper.html": {
    "title": "LibreFace: An Open-Source Toolkit for Deep Facial Expression Analysis",
    "volume": "main",
    "abstract": "Facial expression analysis is an important tool for human-computer interaction. In this paper, we introduce LibreFace, an open-source toolkit for facial expression analysis. This open-source toolbox offers real-time and offline analysis of facial behavior through deep learning models, including facial action unit (AU) detection, AU intensity estimation, and facial expression recognition. To accomplish this, we employ several techniques, including the utilization of a large-scale pre-trained network, feature-wise knowledge distillation, and task-specific fine-tuning. These approaches are designed to effectively and accurately analyze facial expressions by leveraging visual information, thereby facilitating the implementation of real-time interactive applications. In terms of Action Unit (AU) intensity estimation, we achieve a Pearson Correlation Coefficient (PCC) of 0.63 on DISFA, which is 7% higher than the performance of OpenFace 2.0 while maintaining highly-efficient inference that runs two times faster than OpenFace 2.0. Despite being compact, our model also demonstrates competitive performance to state-of-the-art facial expression analysis methods on AffecNet, FFHQ, and RAF-DB",
    "checked": true,
    "id": "03c15be430885ca4a9d8d7b0f27762859db9133f",
    "semantic_title": "libreface: an open-source toolkit for deep facial expression analysis",
    "citation_count": 2,
    "authors": [
      "Di Chang",
      "Yufeng Yin",
      "Zongjian Li",
      "Minh Tran",
      "Mohammad Soleymani"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Chen_SCUNet_Swin-UNet_and_CNN_Bottleneck_Hybrid_Architecture_With_Multi-Fusion_Dense_WACV_2024_paper.html": {
    "title": "SCUNet++: Swin-UNet and CNN Bottleneck Hybrid Architecture With Multi-Fusion Dense Skip Connection for Pulmonary Embolism CT Image Segmentation",
    "volume": "main",
    "abstract": "Pulmonary embolism (PE) is a prevalent lung disease that can lead to right ventricular hypertrophy and failure in severe cases, ranking second in severity only to myocardial infarction and sudden death. Pulmonary artery CT angiography (CTPA) is a widely used diagnostic method for PE. However, PE detection presents challenges in clinical practice due to limitations in imaging technology. CTPA can produce noises similar to PE, making confirmation of its presence time-consuming and prone to overdiagnosis. Nevertheless, the traditional segmentation method of PE can not fully consider the hierarchical structure of features, local and global spatial features of PE CT images. In this paper, we propose an automatic PE segmentation method called SCUNet++ (Swin Conv UNet++). This method incorporates multiple fusion dense skip connections between the encoder and decoder, utilizing the Swin Transformer as the encoder. And fuses features of different scales in the decoder subnetwork to compensate for spatial information loss caused by the inevitable downsampling in Swin-UNet or other state-of-the-art methods, effectively solving the above problem. We provide a theoretical analysis of this method in detail and validate it on publicly available PE CT image datasets FUMPE and CAD-PE. The experimental results indicate that our proposed method achieved a Dice similarity coefficient (DSC) of 83.47% and a Hausdorff distance 95th percentile (HD95) of 3.83 on the FUMPE dataset, as well as a DSC of 83.42% and an HD95 of 5.10 on the CAD-PE dataset. These findings demonstrate that our method exhibits strong performance in PE segmentation tasks, potentially enhancing the accuracy of automatic segmentation of PE and providing a powerful diagnostic tool for clinical physicians. Our source code and new FUMPE dataset are available at https://github.com/JustlfC03/SCUNet-plusplus",
    "checked": true,
    "id": "f55f06c301cc72e3a127a35141dac9259c74423f",
    "semantic_title": "scunet++: swin-unet and cnn bottleneck hybrid architecture with multi-fusion dense skip connection for pulmonary embolism ct image segmentation",
    "citation_count": 0,
    "authors": [
      "Yifei Chen",
      "Binfeng Zou",
      "Zhaoxin Guo",
      "Yiyu Huang",
      "Yifan Huang",
      "Feiwei Qin",
      "Qinhai Li",
      "Changmiao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Vieira_e_Silva_Attention_Modules_Improve_Image-Level_Anomaly_Detection_for_Industrial_Inspection_A_WACV_2024_paper.html": {
    "title": "Attention Modules Improve Image-Level Anomaly Detection for Industrial Inspection: A DifferNet Case Study",
    "volume": "main",
    "abstract": "Within (semi-)automated visual industrial inspection, learning-based approaches for assessing visual defects, including deep neural networks, enable the processing of otherwise small defect patterns in pixel size on high-resolution imagery. The emergence of these often rarely occurring defect patterns explains the general need for labeled data corpora. To alleviate this issue and advance the current state of the art in unsupervised visual inspection, this work proposes a DifferNet-based solution enhanced with attention modules: AttentDifferNet. It improves image-level detection and classification capabilities on three visual anomaly detection datasets for industrial inspection: InsPLAD-fault, MVTec AD, and Semiconductor Wafer. In comparison to the state of the art, AttentDifferNet achieves improved results, which are, in turn, highlighted throughout our quali-quantitative study. Our quantitative evaluation shows an average improvement - compared to DifferNet - of 1.77 +- 0.25 percentage points in overall AUROC considering all three datasets, reaching SOTA results in InsPLAD-fault, an industrial inspection in-the-wild dataset. As our variants to AttentDifferNet show great prospects in the context of currently investigated approaches, a baseline is formulated, emphasizing the importance of attention for industrial anomaly detection both in the wild and in controlled environments",
    "checked": true,
    "id": "9641f2372af777820b17bf30058c08332e23dab5",
    "semantic_title": "attention modules improve image-level anomaly detection for industrial inspection: a differnet case study",
    "citation_count": 0,
    "authors": [
      "André Luiz Vieira e Silva",
      "Francisco Simões",
      "Danny Kowerko",
      "Tobias Schlosser",
      "Felipe Battisti",
      "Veronica Teichrieb"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Matsumoto_Indoor_Visual_Localization_Using_Point_and_Line_Correspondences_in_Dense_WACV_2024_paper.html": {
    "title": "Indoor Visual Localization Using Point and Line Correspondences in Dense Colored Point Cloud",
    "volume": "main",
    "abstract": "We propose a novel pipeline called Loc-PL that uses both points and lines for indoor visual localization in dense colored point cloud. Loc-PL utilizes the spatially complementary relationship between points and lines to address challenging indoor issues. There are two successive camera pose estimation modules. The first improves robustness against repetitive patterns by considering the geometric consistency of points and lines. The second utilizes points and lines to refine poses by Perspective-m-Point-n-Line (PmPnL) and circumvents unstable localization due to locally concentrated matches caused by less-textured environments. The modules use different schemes to obtain line correspondences; the first finds line matches using RANSAC, which is effective for image pairs with large viewpoint gaps, and the second utilizes rendered images from dense point cloud to get them by feature line matching. In addition, we develop a simple but effective module for evaluating the correctness of camera poses using matched point distances across two images. The experimental results on a large dataset, InLoc, show that Loc-PL achieves the state-of-the-art in four out of six scores",
    "checked": true,
    "id": "1051d5cedc4de1b2195974e5431a5fd590c097e6",
    "semantic_title": "indoor visual localization using point and line correspondences in dense colored point cloud",
    "citation_count": 0,
    "authors": [
      "Yuya Matsumoto",
      "Gaku Nakano",
      "Kazumine Ogura"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Teigen_RGB-D_Mapping_and_Tracking_in_a_Plenoxel_Radiance_Field_WACV_2024_paper.html": {
    "title": "RGB-D Mapping and Tracking in a Plenoxel Radiance Field",
    "volume": "main",
    "abstract": "The widespread adoption of Neural Radiance Fields (NeRFs) have ensured significant advances in the domain of novel view synthesis in recent years. These models capture a volumetric radiance field of a scene, creating highly convincing, dense, photorealistic models through the use of simple, differentiable rendering equations. Despite their popularity, these algorithms suffer from severe ambiguities in visual data inherent to the RGB sensor, which means that although images generated with view synthesis can visually appear very believable, the underlying 3D model will often be wrong. This considerably limits the usefulness of these models in practical applications like Robotics and Extended Reality (XR), where an accurate dense 3D reconstruction otherwise would be of significant value. In this paper, we present the vital differences between view synthesis models and 3D reconstruction models. We also comment on why a depth sensor is essential for modeling accurate geometry in general outward-facing scenes using the current paradigm of novel view synthesis methods. Focusing on the structure-from-motion task, we practically demonstrate this need by extending the Plenoxel radiance field model: Presenting an analytical differential approach for dense mapping and tracking with radiance fields based on RGB-D data without a neural network. Our method achieves state-of-the-art results in both mapping and tracking tasks, while also being faster than competing neural network-based approaches. The code is available at: https://github.com/ysus33/RGB-D_Plenoxel_Mapping_Tracking.git",
    "checked": true,
    "id": "ddbab15d4a50a7f73ff5cb17ab65f6bf4c2a522d",
    "semantic_title": "rgb-d mapping and tracking in a plenoxel radiance field",
    "citation_count": 2,
    "authors": [
      "Andreas L. Teigen",
      "Yeonsoo Park",
      "Annette Stahl",
      "Rudolf Mester"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ganesh_An_Empirical_Investigation_Into_Benchmarking_Model_Multiplicity_for_Trustworthy_Machine_WACV_2024_paper.html": {
    "title": "An Empirical Investigation Into Benchmarking Model Multiplicity for Trustworthy Machine Learning: A Case Study on Image Classification",
    "volume": "main",
    "abstract": "Deep learning models have proven to be highly successful. Yet, their over-parameterization gives rise to model multiplicity, a phenomenon in which multiple models achieve similar performance but exhibit distinct underlying behaviours. This multiplicity presents a significant challenge and necessitates additional specifications in model selection to prevent unexpected failures during deployment. While prior studies have examined these concerns, they focus on individual metrics in isolation, making it difficult to obtain a comprehensive view of multiplicity in trustworthy machine learning. Our work stands out by offering a one-stop empirical benchmark of multiplicity across various dimensions of model design and its impact on a diverse set of trustworthy metrics. In this work, we establish a consistent language for studying model multiplicity by translating several trustworthy metrics into accuracy under appropriate interventions. We also develop a framework, which we call multiplicity sheets, to benchmark multiplicity in various scenarios. We demonstrate the advantages of our setup through a case study in image classification and provide actionable insights into the impact and trends of different hyperparameters on model multiplicity. Finally, we show that multiplicity persists in deep learning models even after enforcing additional specifications during model selection, highlighting the severity of over-parameterization. The concerns of under-specification thus remain, and we seek to promote a more comprehensive discussion of multiplicity in trustworthy machine learning",
    "checked": true,
    "id": "3855a77d2097a4fcb67de9b731ab74f20a6f0067",
    "semantic_title": "an empirical investigation into benchmarking model multiplicity for trustworthy machine learning: a case study on image classification",
    "citation_count": 0,
    "authors": [
      "Prakhar Ganesh"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Carmichael_Pixel-Grounded_Prototypical_Part_Networks_WACV_2024_paper.html": {
    "title": "Pixel-Grounded Prototypical Part Networks",
    "volume": "main",
    "abstract": "Prototypical part neural networks (ProtoPartNNs), namely ProtoPNet and its derivatives, are an intrinsically interpretable approach to machine learning. Their prototype learning scheme enables intuitive explanations of the form, this (prototype) looks like that (testing image patch). But, does this actually look like that? In this work, we delve into why object part localization and associated heat maps in past work are misleading. Rather than localizing to object parts, existing ProtoPartNNs localize to the entire image, contrary to generated explanatory visualizations. We argue that detraction from these underlying issues is due to the alluring nature of visualizations and an over-reliance on intuition. To alleviate these issues, we devise new receptive field-based architectural constraints for meaningful localization and a principled pixel space mapping for ProtoPartNNs. To improve interpretability, we propose additional architectural improvements, including a simplified classification head. We also make additional corrections to ProtoPNet and its derivatives, such as the use of a validation set, rather than a test set, to evaluate generalization during training. Our approach, PixPNet (Pixel-grounded Prototypical part Network), is the only ProtoPartNN that truly learns and localizes to prototypical object parts. We demonstrate that PixPNet achieves quantifiably improved interpretability without sacrificing accuracy",
    "checked": true,
    "id": "5ced8639beb776774d2d2489000e000bb186f52f",
    "semantic_title": "pixel-grounded prototypical part networks",
    "citation_count": 0,
    "authors": [
      "Zachariah Carmichael",
      "Suhas Lohit",
      "Anoop Cherian",
      "Michael J. Jones",
      "Walter J. Scheirer"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Liu_LatentDR_Improving_Model_Generalization_Through_Sample-Aware_Latent_Degradation_and_Restoration_WACV_2024_paper.html": {
    "title": "LatentDR: Improving Model Generalization Through Sample-Aware Latent Degradation and Restoration",
    "volume": "main",
    "abstract": "Despite significant advances in deep learning, models often struggle to generalize well to new, unseen domains, especially when training data is limited. To address this challenge, we propose a novel approach for distribution-aware latent augmentation that leverages the relationships across samples to guide the augmentation procedure. Our approach first degrades the samples stochastically in the latent space, mapping them to augmented labels, and then restores the samples from their corrupted versions during training. This process confuses the classifier in the degradation step and restores the overall class distribution of the original samples, promoting diverse intra-class/cross-domain variability. We extensively evaluate our approach on a diverse set of datasets and tasks, including domain generalization benchmarks and medical imaging datasets with strong domain shift, where we show our approach achieves significant improvements over existing methods for latent space augmentation. We further show that our method can be flexibly adapted to long-tail recognition tasks, demonstrating its versatility in building more generalizable models. Code is at https://github.com/nerdslab/LatentDR",
    "checked": true,
    "id": "cb4cdfd8cd3a5923688f84a426ea5715650e3d0d",
    "semantic_title": "latentdr: improving model generalization through sample-aware latent degradation and restoration",
    "citation_count": 0,
    "authors": [
      "Ran Liu",
      "Sahil Khose",
      "Jingyun Xiao",
      "Lakshmi Sathidevi",
      "Keerthan Ramnath",
      "Zsolt Kira",
      "Eva L. Dyer"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Rahman_G-CASCADE_Efficient_Cascaded_Graph_Convolutional_Decoding_for_2D_Medical_Image_WACV_2024_paper.html": {
    "title": "G-CASCADE: Efficient Cascaded Graph Convolutional Decoding for 2D Medical Image Segmentation",
    "volume": "main",
    "abstract": "In this paper, we are the first to propose a new graph convolution-based decoder namely, Cascaded Graph Convolutional Attention Decoder (G-CASCADE), for 2D medical image segmentation. G-CASCADE progressively refines multi-stage feature maps generated by hierarchical transformer encoders with an efficient graph convolution block. The encoder utilizes the self-attention mechanism to capture long-range dependencies, while the decoder refines the feature maps preserving long-range information due to the global receptive fields of the graph convolution block. Rigorous evaluations of our decoder with multiple transformer encoders on five medical image segmentation tasks (i.e., Abdomen organs, Cardiac organs, Polyp lesions, Skin lesions, and Retinal vessels) show that our model outperforms other state-of-the-art (SOTA) methods. We also demonstrate that our decoder achieves better DICE scores than the SOTA CASCADE decoder with 80.8% fewer parameters and 82.3% fewer FLOPs. Our decoder can easily be used with other hierarchical encoders for general-purpose semantic and medical image segmentation tasks. The implementation can be found at: https://github.com/SLDGroup/G-CASCADE",
    "checked": true,
    "id": "a19c69ce93dba274b589b674fa3c8c243934e11d",
    "semantic_title": "g-cascade: efficient cascaded graph convolutional decoding for 2d medical image segmentation",
    "citation_count": 0,
    "authors": [
      "Md Mostafijur Rahman",
      "Radu Marculescu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Naumann_TAMPAR_Visual_Tampering_Detection_for_Parcel_Logistics_in_Postal_Supply_WACV_2024_paper.html": {
    "title": "TAMPAR: Visual Tampering Detection for Parcel Logistics in Postal Supply Chains",
    "volume": "main",
    "abstract": "Due to the steadily rising amount of valuable goods in supply chains, tampering detection for parcels is becoming increasingly important. In this work, we focus on the use-case last-mile delivery, where only a single RGB image is taken and compared against a reference from an existing database to detect potential appearance changes that indicate tampering. We propose a tampering detection pipeline that utilizes keypoint detection to identify the eight corner points of a parcel. This permits applying a perspective transformation to create normalized fronto-parallel views for each visible parcel side surface. These viewpoint-invariant parcel side surface representations facilitate the identification of signs of tampering on parcels within the supply chain, since they reduce the problem to parcel side surface matching with pair-wise appearance change detection. Experiments with multiple classical and deep learning-based change detection approaches are performed on our newly collected TAMpering detection dataset for PARcels, called TAMPAR. We evaluate keypoint and change detection separately, as well as in a unified system for tampering detection. Our evaluation shows promising results for keypoint (Keypoint AP 75.76) and tampering detection (81% accuracy, F1-Score 0.83) on real images. Furthermore, a sensitivity analysis for tampering types, lens distortion and viewing angles is presented. Code and dataset are available at https://a-nau.github.io/tampar",
    "checked": true,
    "id": "0e3ecb08877fd5b0de74ff88af22b0b2a82882ed",
    "semantic_title": "tampar: visual tampering detection for parcel logistics in postal supply chains",
    "citation_count": 0,
    "authors": [
      "Alexander Naumann",
      "Felix Hertlein",
      "Laura Dörr",
      "Kai Furmans"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhang_PGVT_Pose-Guided_Video_Transformer_for_Fine-Grained_Action_Recognition_WACV_2024_paper.html": {
    "title": "PGVT: Pose-Guided Video Transformer for Fine-Grained Action Recognition",
    "volume": "main",
    "abstract": "Based on recent advancements in transformer-based video models and multi-modal joint learning, we propose a novel model, named Pose-Guided Video Transformer (PGVT), to incorporate sparse high-level body joints locations and dense low-level visual pixels for effective learning and accurate recognition of human actions. PGVT leverages the pre-trained image models by freezing their parameters and introducing trainable adapters to effectively integrate two input modalities, i.e., human poses and video frames, to learn a pose-focused spatiotemporal representation of human actions. We design two novel core modules, i.e., Pose Temporal Attention and Pose-Video Spatial Attention, to facilitate interaction between body joint locations and uniform video tokens, enriching each modality with contextualized information from the other. We evaluate PGVT model on four action recognition datasets: Diving48, Gym99, and Gym288 for fine-grained action recognition, and Kinetics400 for coarse-grained action recognition. Our model achieves new SOTA performance on the three fine-grained human action recognition datasets and comparable performance on Kinetics400 with a small number of tunable parameters compared with SOTA methods. The PGVT model exploits effective multi-modality learning by explicitly modeling human body joints and leveraging their contextualized interactions with video clips",
    "checked": false,
    "id": "58cf50e770e975d5dd81ba77cac36fd57308fe1d",
    "semantic_title": "spatiotemporal self-attention mechanism driven by 3d pose to guide rgb cues for daily living human activity recognition",
    "citation_count": 0,
    "authors": [
      "Haosong Zhang",
      "Mei Chee Leong",
      "Liyuan Li",
      "Weisi Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Black_Multi-View_Classification_Using_Hybrid_Fusion_and_Mutual_Distillation_WACV_2024_paper.html": {
    "title": "Multi-View Classification Using Hybrid Fusion and Mutual Distillation",
    "volume": "main",
    "abstract": "Multi-view classification problems are common in medical image analysis, forensics, and other domains where problem queries involve multi-image input. Existing multi-view classification methods are often tailored to a specific task. In this paper, we repurpose off-the-shelf Hybrid CNN-Transformer networks for multi-view classification with either structured or unstructured views. Our approach incorporates a novel fusion scheme, mutual distillation, and introduces minimal additional parameters. We demonstrate the effectiveness and generalization capability of our approach, MV-HFMD, on multiple multi-view classification tasks and show that it outperforms other multi-view approaches, even task-specific methods. Code is available at https://github.com/vidarlab/multi-view-hybrid",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Black",
      "Richard Souvenir"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Lee_Real-Time_User-Guided_Adaptive_Colorization_With_Vision_Transformer_WACV_2024_paper.html": {
    "title": "Real-Time User-Guided Adaptive Colorization With Vision Transformer",
    "volume": "main",
    "abstract": "Recently, the vision transformer (ViT) has achieved remarkable performance in computer vision tasks and has been actively utilized in colorization. Vision transformer uses multi-head self attention to effectively propagate user hints to distant relevant areas in the image. However, despite the success of vision transformers in colorizing the image, heavy underlying ViT architecture and the large computational cost hinder active real-time user interaction for colorization applications. Several research removed redundant image patches to reduce the computational cost of ViT in image classification tasks. However, the existing efficient ViT methods cause severe performance degradation in colorization task since it completely removes the redundant patches. Thus, we propose a novel efficient ViT architecture for real-time interactive colorization, AdaColViT determines which redundant image patches and layers to reduce in the ViT. Unlike existing methods, our novel pruning method alleviates performance drop and flexibly allocates computational resources of input samples, effectively achieving actual acceleration. In addition, we demonstrate through extensive experiments on ImageNet-ctest10k, Oxford 102flowers, and CUB-200 datasets that our method outperforms the baseline methods",
    "checked": false,
    "id": "bf967f341cfa4c39a91d709551c65bdcbdff70a0",
    "semantic_title": "a-colvit: real-time interactive colorization by adaptive vision transformer",
    "citation_count": 0,
    "authors": [
      "Gwanghan Lee",
      "Saebyeol Shin",
      "Taeyoung Na",
      "Simon S. Woo"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Limanta_CAMOT_Camera_Angle-Aware_Multi-Object_Tracking_WACV_2024_paper.html": {
    "title": "CAMOT: Camera Angle-Aware Multi-Object Tracking",
    "volume": "main",
    "abstract": "This paper proposes CAMOT, a simple camera angle estimator for multi-object tracking to tackle two problems: 1) occlusion and 2) inaccurate distance estimation in the depth direction. Under the assumption that multiple objects are located on a flat plane in each video frame, CAMOT estimates the camera angle using object detection. In addition, it gives the depth of each object, enabling pseudo-3D MOT. We evaluated its performance by adding it to various 2D MOT methods on the MOT17 and MOT20 datasets and confirmed its effectiveness. Applying CAMOT to ByteTrack, we obtained 63.8% HOTA, 80.6% MOTA, and 78.5% IDF1 in MOT17, which are state-of-the-art results. Its computational cost is significantly lower than the existing deep-learning-based depth estimators for tracking",
    "checked": false,
    "id": "96782464d04c96248b359a481d17041bf499cafb",
    "semantic_title": "quality matters: embracing quality clues for robust 3d multi-object tracking",
    "citation_count": 9,
    "authors": [
      "Felix Limanta",
      "Kuniaki Uto",
      "Koichi Shinoda"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Shiota_Egocentric_Action_Recognition_by_Capturing_Hand-Object_Contact_and_Object_State_WACV_2024_paper.html": {
    "title": "Egocentric Action Recognition by Capturing Hand-Object Contact and Object State",
    "volume": "main",
    "abstract": "Improving the performance of egocentric action recognition (EAR) requires accurately capturing interactions between actors and objects. In this paper, we propose two learning methods that enable recognition models to capture hand object contact and object state change. We introduce Hand-Object Contact Learning (HOCL), which enables the model to focus on hand-object contact during actions, and Object State Learning (OSL), which enables the model to focus on object state changes caused by hand actions. Evaluation using a CNN-based model and a transformer-based model on the EGTEA, MECCANO, and EPIC-KITCHENS 100 datasets demonstrated the effectiveness of applying HOCL and OSL. Their application improved overall accuracy by up to 2.24% on EGTEA, 3.97% on MECCANO, and 1.49% on EPIC-KITCHENS 100. In addition, HOCL and OSL improved the performance on data with small training samples and one from unfamiliar scenes. Qualitative analysis revealed that their application enabled the models to precisely capture the interaction between actor and object",
    "checked": false,
    "id": "e047ebbce41ef209223ff311e07222c0698289cb",
    "semantic_title": "transformer-based unified recognition of two hands manipulating objects",
    "citation_count": 2,
    "authors": [
      "Tsukasa Shiota",
      "Motohiro Takagi",
      "Kaori Kumagai",
      "Hitoshi Seshimo",
      "Yushi Aono"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Schoonbeek_IndustReal_A_Dataset_for_Procedure_Step_Recognition_Handling_Execution_Errors_WACV_2024_paper.html": {
    "title": "IndustReal: A Dataset for Procedure Step Recognition Handling Execution Errors in Egocentric Videos in an Industrial-Like Setting",
    "volume": "main",
    "abstract": "Although action recognition for procedural tasks has received notable attention, it has a fundamental flaw in that no measure of success for actions is provided. This limits the applicability of such systems especially within the industrial domain, since the outcome of procedural actions is often significantly more important than the mere execution. To address this limitation, we define the novel task of procedure step recognition (PSR), focusing on recognizing the correct completion and order of procedural steps. Alongside the new task, we also present the multi-modal IndustReal dataset. Unlike currently available datasets, IndustReal contains procedural errors (such as omissions) as well as execution errors. A significant part of these errors are exclusively present in the validation and test sets, making IndustReal suitable to evaluate robustness of algorithms to new, unseen mistakes. Additionally, to encourage reproducibility and allow for scalable approaches trained on synthetic data, the 3D models of all parts are publicly available. Annotations and benchmark performance are provided for action recognition and assembly state detection, as well as the new PSR task. IndustReal, along with the code and model weights, is available at https://github.com/TimSchoonbeek/IndustReal",
    "checked": true,
    "id": "8b3f77912809e8f6393f2fdb6b6c2bc8765cb3b4",
    "semantic_title": "industreal: a dataset for procedure step recognition handling execution errors in egocentric videos in an industrial-like setting",
    "citation_count": 1,
    "authors": [
      "Tim J. Schoonbeek",
      "Tim Houben",
      "Hans Onvlee",
      "Peter H.N. de With",
      "Fons van der Sommen"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Suresh_FastCLIPstyler_Optimisation-Free_Text-Based_Image_Style_Transfer_Using_Style_Representations_WACV_2024_paper.html": {
    "title": "FastCLIPstyler: Optimisation-Free Text-Based Image Style Transfer Using Style Representations",
    "volume": "main",
    "abstract": "In recent years, language-driven artistic style transfer has emerged as a new type of style transfer technique, eliminating the need for a reference style image by using natural language descriptions of the style. The first model to achieve this, called CLIPstyler, has demonstrated impressive stylisation results. However, its lengthy optimisation procedure at runtime for each query limits its suitability for many practical applications. In this work, we present FastCLIPstyler, a generalised text-based image style transfer model capable of stylising images in a single forward pass for arbitrary text inputs. Furthermore, we introduce EdgeCLIPstyler, a lightweight model designed for compatibility with resource-constrained devices. Through quantitative and qualitative comparisons with state-of-the-art approaches, we demonstrate that our models achieve superior stylisation quality based on measurable metrics while offering significantly improved runtime efficiency, particularly on edge devices",
    "checked": true,
    "id": "b46c7d3fbf6c3b044067877ff4e857ff292e016e",
    "semantic_title": "fastclipstyler: optimisation-free text-based image style transfer using style representations",
    "citation_count": 1,
    "authors": [
      "Ananda Padhmanabhan Suresh",
      "Sanjana Jain",
      "Pavit Noinongyao",
      "Ankush Ganguly",
      "Ukrit Watchareeruetai",
      "Aubin Samacoits"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Shin_Video-kMaX_A_Simple_Unified_Approach_for_Online_and_Near-Online_Video_WACV_2024_paper.html": {
    "title": "Video-kMaX: A Simple Unified Approach for Online and Near-Online Video Panoptic Segmentation",
    "volume": "main",
    "abstract": "Video Panoptic Segmentation (VPS) aims to achieve comprehensive pixel-level scene understanding by segmenting all pixels and associating objects in a video. Current solutions can be categorized into online and near-online approaches. Evolving over the time, each category has its own specialized designs, making it nontrivial to adapt models between different categories. To alleviate the discrepancy, in this work, we propose a unified approach for online and near-online VPS. The meta architecture of the proposed Video-kMaX consists of two components: within-clip segmenter (for clip-level segmentation) and cross-clip associater (for association beyond clips). We propose clip-kMaX (clip k-means mask transformer) and LA-MB (locationaware memory buffer) to instantiate the segmenter and associater, respectively. Our general formulation includes the online scenario as a special case by adopting clip length of one. Without bells and whistles, Video-kMaX sets a new state-of-the-art on KITTI-STEP and VIPSeg for video panoptic segmentation Code will be made publicly available. Code and models are available at this link: https://github.com/dlsrbgg33/video_kmax",
    "checked": true,
    "id": "23726f9a2cbd56b96320709be1fce5daa57cf5ee",
    "semantic_title": "video-kmax: a simple unified approach for online and near-online video panoptic segmentation",
    "citation_count": 5,
    "authors": [
      "Inkyu Shin",
      "Dahun Kim",
      "Qihang Yu",
      "Jun Xie",
      "Hong-Seok Kim",
      "Bradley Green",
      "In So Kweon",
      "Kuk-Jin Yoon",
      "Liang-Chieh Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Aketi_Cross-Feature_Contrastive_Loss_for_Decentralized_Deep_Learning_on_Heterogeneous_Data_WACV_2024_paper.html": {
    "title": "Cross-Feature Contrastive Loss for Decentralized Deep Learning on Heterogeneous Data",
    "volume": "main",
    "abstract": "The current state-of-the-art decentralized learning algorithms mostly assume the data distribution to be Independent and Identically Distributed (IID). However, in practical scenarios, the distributed datasets can have significantly heterogeneous data distributions across the agents. In this work, we present a novel approach for decentralized learning on heterogeneous data, where data-free knowledge distillation through contrastive loss on cross-features is utilized to improve performance. Cross-features for a pair of neighboring agents are the features (i.e., last hidden layer activations) obtained from the data of an agent with respect to the model parameters of the other agent. We demonstrate the effectiveness of the proposed technique through an exhaustive set of experiments on various Computer Vision datasets (CIFAR-10, CIFAR-100, Fashion MNIST, ImageNette, and ImageNet), model architectures, and network topologies. Our experiments show that the proposed method achieves superior performance (0.2-4% improvement in test accuracy) compared to other existing techniques for decentralized learning on heterogeneous data",
    "checked": true,
    "id": "408984d122f86b7d1741a3a09483841a5fc066d0",
    "semantic_title": "cross-feature contrastive loss for decentralized deep learning on heterogeneous data",
    "citation_count": 0,
    "authors": [
      "Sai Aparna Aketi",
      "Kaushik Roy"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Raychaudhuri_MOPA_Modular_Object_Navigation_With_PointGoal_Agents_WACV_2024_paper.html": {
    "title": "MOPA: Modular Object Navigation With PointGoal Agents",
    "volume": "main",
    "abstract": "We propose a simple but effective modular approach MOPA (Modular ObjectNav with PointGoal agents) to systematically investigate the inherent modularity of the object navigation task in Embodied AI. MOPA consists of four modules: (a) an object detection module trained to identify objects from RGB images, (b) a map building module to build a semantic map of the observed objects, (c) an exploration module enabling the agent to explore the environment, and (d) a navigation module to move to identified target objects. We show that we can effectively reuse a pretrained PointGoal agent as the navigation model instead of learning to navigate from scratch, thus saving time and compute. We also compare various exploration strategies for MOPA and find that a simple uniform strategy significantly outperforms more advanced exploration methods",
    "checked": true,
    "id": "a92488e32c9f5a85d58ecd06169db9e095f7bccc",
    "semantic_title": "mopa: modular object navigation with pointgoal agents",
    "citation_count": 0,
    "authors": [
      "Sonia Raychaudhuri",
      "Tommaso Campari",
      "Unnat Jain",
      "Manolis Savva",
      "Angel X. Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Grieggs_The_Paleographers_Eye_ex_machina_Using_Computer_Vision_To_Assist_WACV_2024_paper.html": {
    "title": "The Paleographer's Eye ex machina: Using Computer Vision To Assist Humanists in Scribal Hand Identification",
    "volume": "main",
    "abstract": "The steady digitization of medieval manuscripts is rapidly changing the field of paleography, challenging existing assumptions about handwriting and book production. This development has identified historically important centers for the production of scribal texts, and even individual scribes themselves. For example, scholars of late medieval English literature have identified the copyists of a number of literary manuscripts, and the important role of London government clerks in shaping literary culture. However, traditional paleography has no agreed-upon methodology or fixed criteria for the attribution of handwriting to a particular community, period, or scribe. The approach taken by paleographers is inherently qualitative and subject to personal bias. Even those wielding the mighty \"paleographer's eye\" cannot claim objectivity. Computer vision offers solutions with spectacular performance on writer identification and retrieval benchmarks, but these have not been widely adopted by the paleography community because they tend not to hold up in practice. In this work, we attempt to bridge the divide with a software package designed not to automate paleography, but to augment the paleographer's eye. We introduce automated handwriting identification tools for which the results can be quickly visually understood and assessed, and used as one feature among many by expert paleographers when attributing previously unknown scribal hands. We also demonstrate a use case for our software by analyzing several items believed to be written by Thomas Hoccleve, a highly productive clerk of the Privy Seal who also happens to be an important fifteenth-century English poet",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Grieggs",
      "C. E. M. Henderson",
      "Sebastian Sobecki",
      "Alexandra Gillespie",
      "Walter Scheirer"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zunair_Learning_To_Recognize_Occluded_and_Small_Objects_With_Partial_Inputs_WACV_2024_paper.html": {
    "title": "Learning To Recognize Occluded and Small Objects With Partial Inputs",
    "volume": "main",
    "abstract": "Recognizing multiple objects in an image is challenging due to occlusions, and becomes even more so when the objects are small. While promising, existing multi-label image recognition models do not explicitly learn context-based representations, and hence struggle to correctly recognize small and occluded objects. Intuitively, recognizing occluded objects requires knowledge of partial input, and hence context. Motivated by this intuition, we propose Masked Supervised Learning (MSL), a single-stage, model-agnostic learning paradigm for multi-label image recognition. The key idea is to learn context-based representations using a masked branch and to model label co-occurrence using label consistency. Experimental results demonstrate the simplicity, applicability and more importantly the competitive performance of MSL against previous state-of-the-art methods on standard multi-label image recognition benchmarks. In addition, we show that MSL is robust to random masking and demonstrate its effectiveness in recognizing non-masked objects. Code and pretrained models are available on GitHub",
    "checked": true,
    "id": "ed4ebd7622352b0d847bb2a7baee32b9b1abe61f",
    "semantic_title": "learning to recognize occluded and small objects with partial inputs",
    "citation_count": 0,
    "authors": [
      "Hasib Zunair",
      "A. Ben Hamza"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhao_BALF_Simple_and_Efficient_Blur_Aware_Local_Feature_Detector_WACV_2024_paper.html": {
    "title": "BALF: Simple and Efficient Blur Aware Local Feature Detector",
    "volume": "main",
    "abstract": "Local feature detection is a key ingredient of many image processing and computer vision applications, such as visual odometry and localization. Most existing algorithms focus on feature detection from a sharp image. They would thus have degraded performance once the image is blurred, which could happen easily under low-lighting conditions. To address this issue, we propose a simple yet both efficient and effective keypoint detection method that is able to accurately localize the salient keypoints in a blurred image. Our method takes advantages of a novel multi-layer perceptron (MLP) based architecture that significantly improve the detection repeatability for a blurred image. The network is also light-weight and able to run in real-time, which enables its deployment for time-constrained applications. Extensive experimental results demonstrate that our detector is able to improve the detection repeatability with blurred images, while keeping comparable performance as existing state-of-the-art detectors for sharp images. The code and trained weights are publicly available at github.com/ericzzj1989/BALF",
    "checked": true,
    "id": "80b9a6bb05e1d193d7446ae216d8933bc53a14e7",
    "semantic_title": "balf: simple and efficient blur aware local feature detector",
    "citation_count": 1,
    "authors": [
      "Zhenjun Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wang_RS2G_Data-Driven_Scene-Graph_Extraction_and_Embedding_for_Robust_Autonomous_Perception_WACV_2024_paper.html": {
    "title": "RS2G: Data-Driven Scene-Graph Extraction and Embedding for Robust Autonomous Perception and Scenario Understanding",
    "volume": "main",
    "abstract": "Effectively capturing intricate interactions among road users is of critical importance to achieving safe navigation for autonomous vehicles. While graph learning (GL) has emerged as a promising approach to tackle this challenge, existing GL models rely on predefined domain-specific graph extraction rules that often fail in real-world drastically changing scenarios. Additionally, these graph extraction rules severely impede the capability of existing GL methods to generalize knowledge across domains. To address this issue, we propose RoadScene2Graph (RS2G), an innovative autonomous scenario understanding framework with a novel data-driven graph extraction and modeling approach that dynamically captures the diverse relations among road users. Our evaluations demonstrate that on average RS2G outperforms the state-of-the-art (SOTA) rule-based graph extraction method by 4.47% and the SOTA deep learning model by 22.19% in subjective risk assessment. More importantly, RS2G delivers notably better performance in transferring knowledge gained from simulation environments to unseen real-world scenarios",
    "checked": true,
    "id": "00e91136488df73f75e06ac8b1a4137d4bab4b63",
    "semantic_title": "rs2g: data-driven scene-graph extraction and embedding for robust autonomous perception and scenario understanding",
    "citation_count": 0,
    "authors": [
      "Junyao Wang",
      "Arnav Vaibhav Malawade",
      "Junhong Zhou",
      "Shih-Yuan Yu",
      "Mohammad Abdullah Al Faruque"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhao_Leveraging_the_Power_of_Data_Augmentation_for_Transformer-Based_Tracking_WACV_2024_paper.html": {
    "title": "Leveraging the Power of Data Augmentation for Transformer-Based Tracking",
    "volume": "main",
    "abstract": "Due to long-distance correlation and powerful pretrained models, transformer-based methods have initiated a breakthrough in visual object tracking performance. Previous works focus on designing effective architectures suited for tracking, but ignore that data augmentation is equally crucial for training a well-performing model. In this paper, we first explore the impact of general data augmentations on transformer-based trackers via systematic experiments, and reveal the limited effectiveness of these common strategies. Motivated by experimental observations, we then propose two data augmentation methods customized for tracking. First, we optimize existing random cropping via a dynamic search radius mechanism and simulation for boundary samples. Second, we propose a token-level feature mixing augmentation strategy, which enables the model against challenges like background interference. Extensive experiments on two transformer-based trackers and six benchmarks demonstrate the effectiveness and data efficiency of our methods, especially under challenging settings, like one-shot tracking and small image resolutions. Code is available at https://github.com/zj5559/DATr",
    "checked": true,
    "id": "ef4c63ccfc5a41ed7802d6e931c0ed69146a85c6",
    "semantic_title": "leveraging the power of data augmentation for transformer-based tracking",
    "citation_count": 0,
    "authors": [
      "Jie Zhao",
      "Johan Edstedt",
      "Michael Felsberg",
      "Dong Wang",
      "Huchuan Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Shen_Med-DANet_V2_A_Flexible_Dynamic_Architecture_for_Efficient_Medical_Volumetric_WACV_2024_paper.html": {
    "title": "Med-DANet V2: A Flexible Dynamic Architecture for Efficient Medical Volumetric Segmentation",
    "volume": "main",
    "abstract": "Recent works have shown that the computational efficiency of 3D medical image (e.g. CT and MRI) segmentation can be impressively improved by dynamic inference based on slice-wise complexity. As a pioneering work, a dynamic architecture network for medical volumetric segmentation (i.e. Med-DANet) has achieved a favorable accuracy and efficiency trade-off by dynamically selecting a suitable 2D candidate model from the pre-defined model bank for different slices. However, the issues of incomplete data analysis, high training costs, and the two-stage pipeline in Med-DANet require further improvement. To this end, this paper further explores a unified formulation of the dynamic inference framework from the perspective of both the data itself and the model structure. For each slice of the input volume, our proposed method dynamically selects an important foreground region for segmentation based on the policy generated by our Decision Network and Crop Position Network. Besides, we propose to insert a stage-wise quantization selector to the employed segmentation model (e.g. U-Net) for dynamic architecture adapting. Extensive experiments on BraTS 2019 and 2020 show that our method achieves comparable or better performance than previous state-of-the-art methods with much less model complexity. Compared with previous methods Med-DANet and TransBTS with dynamic and static architecture respectively, our framework improves the model efficiency by up to nearly 4.1 and 17.3 times with comparable segmentation results on BraTS 2019. Code will be available at https://github.com/Rubics-Xuan/Med-DANet",
    "checked": true,
    "id": "ee5cc1af164325389b2ad8812d365c03e171f0a6",
    "semantic_title": "med-danet v2: a flexible dynamic architecture for efficient medical volumetric segmentation",
    "citation_count": 0,
    "authors": [
      "Haoran Shen",
      "Yifu Zhang",
      "Wenxuan Wang",
      "Chen Chen",
      "Jing Liu",
      "Shanshan Song",
      "Jiangyun Li"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Bamba_Partial_Binarization_of_Neural_Networks_for_Budget-Aware_Efficient_Learning_WACV_2024_paper.html": {
    "title": "Partial Binarization of Neural Networks for Budget-Aware Efficient Learning",
    "volume": "main",
    "abstract": "Binarization is a powerful compression technique for neural networks, significantly reducing FLOPs, but often results in a significant drop in model performance. To address this issue, partial binarization techniques have been developed, but a systematic approach to mixing binary and full-precision parameters in a single network is still lacking. In this paper, we propose a controlled approach to partial binarization, creating a budgeted binary neural network (B2NN) with our MixBin strategy. This method optimizes the mixing of binary and full-precision components, allowing for explicit selection of the fraction of the network to remain binary. Our experiments show that B2NNs created using MixBin outperform those from random or iterative searches and state-of-the-art layer selection methods by up to 3% on the ImageNet-1K dataset. We also show that B2NNs outperform the structured pruning baseline by approximately 23% at the extreme FLOP budget of 15%, and perform well in object tracking, with up to a 12.4% relative improvement over other baselines. Additionally, we demonstrate that B2NNs developed by MixBin can be transferred across datasets, with some cases showing improved performance over directly applying MixBin on the downstream data",
    "checked": true,
    "id": "dc9e6ece6f0e0cb60d8a0d1fc1c543903457fb90",
    "semantic_title": "partial binarization of neural networks for budget-aware efficient learning",
    "citation_count": 0,
    "authors": [
      "Udbhav Bamba",
      "Neeraj Anand",
      "Saksham Aggarwal",
      "Dilip K. Prasad",
      "Deepak K. Gupta"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Improving_the_Fairness_of_the_Min-Max_Game_in_GANs_Training_WACV_2024_paper.html": {
    "title": "Improving the Fairness of the Min-Max Game in GANs Training",
    "volume": "main",
    "abstract": "Generative adversarial networks (GANs) have achieved great success and become more and more popular in recent years. However, understanding of the min-max game in GANs training is still limited. In this paper, we first utilize information game theory to analyze the min-max game in GANs and introduce a new viewpoint on the GANs training that the min-max game in existing GANs is unfair during training, leading to sub-optimal convergence. To tackle this, we propose a novel GAN called Information Gap GAN (IGGAN), which consists of one generator (G) and two discriminators (D1 and D2). Specifically, we apply different data augmentation methods to D1 and D2, respectively. The information gap between different data augmentation methods can change the information received by each player in the min-max game and lead to all three players G, D1 and D2 in IGGAN obtaining incomplete information, which improves the fairness of the min-max game, yielding better convergence. We conduct extensive experiments for large-scale and limited data settings on several common datasets with two backbones, i.e., BigGAN and StyleGAN2. The results demonstrate that IGGAN can achieve a higher Inception Score (IS) and a lower Frechet Inception Distance (FID) compared with other GANs. Codes are available at https://github.com/zzhang05/IGGAN",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoyu Zhang",
      "Yang Hua",
      "Hui Wang",
      "Seán McLoone"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Yu_When_3D_Bounding-Box_Meets_SAM_Point_Cloud_Instance_Segmentation_With_WACV_2024_paper.html": {
    "title": "When 3D Bounding-Box Meets SAM: Point Cloud Instance Segmentation With Weak-and-Noisy Supervision",
    "volume": "main",
    "abstract": "Learning from bounding-boxes annotations has shown great potential in weakly-supervised 3D point cloud in- stance segmentation. However, we observed that existing methods would suffer severe performance degradation with perturbed bounding box annotations. To tackle this is- sue, we propose a complementary image prompt-induced weakly-supervised point cloud instance segmentation (CIP- WPIS) method. CIP-WPIS leverages pretrained knowledge embedded in the 2D foundation model SAM and 3D geo- metric prior to achieve accurate point-wise instance labels from the bounding box annotations. Specifically, CIP-WPIS first selects image views in which 3D candidate points of an instance are fully visible. Then, we generate complemen- tary background and foreground prompts from projections to obtain SAM 2D instance mask predictions. According to these, we assign the confidence values to points indicating the likelihood of points belonging to the instance. Furthermore, we utilize 3D geometric homogeneity provided by superpoints to decide the final instance label assignments. In this fashion, we achieve high-quality 3D point-wise in- stance labels. Extensive experiments on both Scannet-v2 and S3DIS benchmarks proves that our method not only achieves state-of-the-art performance for bounding-boxes supervised point cloud instance segmentation, but also exhibits robustness against noisy 3D bounding-box annotations",
    "checked": true,
    "id": "bed858343355c17210072714fb07217b8f8f5701",
    "semantic_title": "when 3d bounding-box meets sam: point cloud instance segmentation with weak-and-noisy supervision",
    "citation_count": 1,
    "authors": [
      "Qingtao Yu",
      "Heming Du",
      "Chen Liu",
      "Xin Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Gondal_Domain_Aligned_CLIP_for_Few-Shot_Classification_WACV_2024_paper.html": {
    "title": "Domain Aligned CLIP for Few-Shot Classification",
    "volume": "main",
    "abstract": "Large vision-language representation learning models like CLIP have demonstrated impressive performance for zero-shot transfer to downstream tasks while largely benefiting from inter-modal (image-text) alignment via contrastive objectives. This downstream performance can further be enhanced by full-scale fine-tuning which is often compute intensive, requires large labelled data, and can reduce out-of-distribution (OOD) robustness. Furthermore, sole reliance on inter-modal alignment might overlook the rich information embedded within each individual modality. In this work, we introduce a sample-efficient domain adaptation strategy for CLIP, termed Domain Aligned CLIP (DAC), which improves both intra-modal (image-image) and inter-modal alignment on target distributions without fine-tuning the main model. For intra-modal alignment, we introduce a lightweight adapter that is specifically trained with an intra-modal contrastive objective. To improve inter-modal alignment, we introduce a simple framework to modulate the precomputed class text embeddings. The proposed few-shot fine-tuning framework is computationally efficient, robust to distribution shifts, and does not alter CLIP's parameters. We study the effectiveness of DAC by benchmarking on 11 widely used image classification tasks with consistent improvements in 16-shot classification upon strong baselines by about 2.3% and demonstrate competitive performance on 4 OOD robustness benchmarks",
    "checked": true,
    "id": "cf89e6527d680e191db489719b1e19db78a10704",
    "semantic_title": "domain aligned clip for few-shot classification",
    "citation_count": 0,
    "authors": [
      "Muhammad Waleed Gondal",
      "Jochen Gast",
      "Inigo Alonso Ruiz",
      "Richard Droste",
      "Tommaso Macri",
      "Suren Kumar",
      "Luitpold Staudigl"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Van_Landeghem_Beyond_Document_Page_Classification_Design_Datasets_and_Challenges_WACV_2024_paper.html": {
    "title": "Beyond Document Page Classification: Design, Datasets, and Challenges",
    "volume": "main",
    "abstract": "This paper highlights the need to bring document classification benchmarking closer to real-world applications, both in the nature of data tested (X: multi-channel, multi-paged, multi-industry; Y: class distributions and label set variety) and in classification tasks considered (f: multi-page document, page stream, and document bundle classification, ...). We identify the lack of public multi-page document classification datasets, formalize different classification tasks arising in application scenarios, and motivate the value of targeting efficient multi-page document representations. An experimental study on proposed multi-page document classification datasets demonstrates that current benchmarks have become irrelevant and need to be updated to evaluate complete documents, as they naturally occur in practice. This reality check also calls for more mature evaluation methodologies, covering calibration evaluation, inference complexity (time-memory), and a range of realistic distribution shifts (e.g., born-digital vs. scanning noise, shifting page order). Our study ends on a hopeful note by recommending concrete avenues for future improvements",
    "checked": true,
    "id": "0b93a89906c79bff6424054274de13585b4e738b",
    "semantic_title": "beyond document page classification: design, datasets, and challenges",
    "citation_count": 0,
    "authors": [
      "Jordy Van Landeghem",
      "Sanket Biswas",
      "Matthew Blaschko",
      "Marie-Francine Moens"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Dubinski_Towards_More_Realistic_Membership_Inference_Attacks_on_Large_Diffusion_Models_WACV_2024_paper.html": {
    "title": "Towards More Realistic Membership Inference Attacks on Large Diffusion Models",
    "volume": "main",
    "abstract": "Generative diffusion models, including Stable Diffusion and Midjourney, can generate visually appealing, diverse, and high-resolution images for various applications. These models are trained on billions of internet-sourced images, raising significant concerns about the potential unauthorized use of copyright-protected images. In this paper, we examine whether it is possible to determine if a specific image was used in the training set, a problem known as a membership inference attack. Our focus is on Stable Diffusion, and we address the challenge of designing a fair evaluation framework to answer this membership question. We propose a new dataset to establish a fair evaluation setup and apply it to Stable Diffusion, also applicable to other generative models. With the proposed dataset, we execute membership attacks (both known and newly introduced). Our research reveals that previously proposed evaluation setups do not provide a full understanding of the effectiveness of membership inference attacks. We conclude that the membership inference attack remains a significant challenge for large diffusion models (often deployed as black-box systems), indicating that related privacy and copyright issues will persist in the foreseeable future",
    "checked": true,
    "id": "04a880d1f01e773e3f739d27d388f6100874933a",
    "semantic_title": "towards more realistic membership inference attacks on large diffusion models",
    "citation_count": 4,
    "authors": [
      "Jan Dubiński",
      "Antoni Kowalczuk",
      "Stanisław Pawlak",
      "Przemyslaw Rokita",
      "Tomasz Trzciński",
      "Paweł Morawiecki"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Cho_Slice_and_Conquer_A_Planar-to-3D_Framework_for_Efficient_Interactive_Segmentation_WACV_2024_paper.html": {
    "title": "Slice and Conquer: A Planar-to-3D Framework for Efficient Interactive Segmentation of Volumetric Images",
    "volume": "main",
    "abstract": "Interactive segmentation methods have been investigated to address the potential need for additional refinement in automatic segmentation via human-in-the-loop techniques. For accurate segmentation of 3D images, we propose Slice-and-Conquer, a novel planar-to-3D pipeline formulating volumetric mask construction into two stages: 1) 2D interactive segmentation and 2) guided 3D segmentation. Specifically, the first stage enables users to focus on a single 2D slice and provides the corresponding 2D prediction results as strong shape priors. Taking the planar guidance, an accurate 3D mask can be constructed with minimal interactions. To support a flexible iterative refinement, our system recommends a next slice to annotate at the end of the second stage. Since volumetric segmentation can be completed by consecutively annotating a few recommended 2D slices, our method significantly reduces the cognitive burden of exploring volumetric space for users. Through extensive experiments on various datasets of 3D biomedical images, we demonstrate the effectiveness of the proposed pipeline",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wonwoo Cho",
      "Dongmin Choi",
      "Hyesu Lim",
      "Jinho Choi",
      "Saemee Choi",
      "Hyun-seok Min",
      "Sungbin Lim",
      "Jaegul Choo"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Li_Mitigate_Domain_Shift_by_Primary-Auxiliary_Objectives_Association_for_Generalizing_Person_WACV_2024_paper.html": {
    "title": "Mitigate Domain Shift by Primary-Auxiliary Objectives Association for Generalizing Person ReID",
    "volume": "main",
    "abstract": "While deep learning has significantly improved ReID model accuracy under the independent and identical distribution (IID) assumption, it has also become clear that such models degrade notably when applied to an unseen novel domain due to unpredictable/unknown domain shift. Contemporary domain generalization (DG) ReID models struggle in learning domain-invariant representation through solely training on an instance classification objective. We consider that a deep learning model is heavily influenced therefore biased towards domain-specific characteristics, e.g., background clutter, scale and viewpoint variations, limiting the generalizability of the learned model, and hypothesize that the pedestrians are domain invariant owning they share the same structural characteristics. To enable ReID model to be less domain-specific from these pure pedestrians and domain-specific factors, we introduce a method that guides model learning of the primary ReID instance classification objective by a concurrent auxiliary learning objective on weakly labeled pedestrian saliency detection. To solve the problem of conflicting optimization criteria in the model parameter space between the two learning objectives, we introduce a Primary-Auxiliary Objectives Association (PAOA) mechanism to calibrate the loss gradients of the auxiliary task towards the primary learning task gradients. Benefited from the harmonious multitask learning design, our model can be extended with the recent test-time diagram to form the PAOA+, which performs on-the-fly optimization against the auxiliary objective in order to maximize the model's generative capacity in the test target domain. Experiments demonstrate the superiority of the proposed PAOA model",
    "checked": true,
    "id": "b6c877467edfcbf203985ec92ec40fac8312c165",
    "semantic_title": "mitigate domain shift by primary-auxiliary objectives association for generalizing person reid",
    "citation_count": 0,
    "authors": [
      "Qilei Li",
      "Shaogang Gong"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Marsal_MonoProb_Self-Supervised_Monocular_Depth_Estimation_With_Interpretable_Uncertainty_WACV_2024_paper.html": {
    "title": "MonoProb: Self-Supervised Monocular Depth Estimation With Interpretable Uncertainty",
    "volume": "main",
    "abstract": "Self-supervised monocular depth estimation methods aim to be used in critical applications such as autonomous vehicles for environment analysis. To circumvent the potential imperfections of these approaches, a quantification of the prediction confidence is crucial to guide decision-making systems that rely on depth estimation. In this paper, we propose MonoProb, a new unsupervised monocular depth estimation method that returns an interpretable uncertainty, which means that the uncertainty reflects the expected error of the network in its depth predictions. We rethink the stereo or the structure-from-motion paradigms used to train unsupervised monocular depth models as a probabilistic problem. Within a single forward pass inference, this model provides a depth prediction and a measure of its confidence, without increasing the inference time. We then improve the performance on depth and uncertainty with a novel self-distillation loss for which a student is supervised by a pseudo ground truth that is a probability distribution on depth output by a teacher. To quantify the performance of our models we design new metrics that, unlike traditional ones, measure the absolute performance of uncertainty predictions. Our experiments highlight enhancements achieved by our method on standard depth and uncertainty metrics as well as on our tailored metrics. https://github.com/CEA-LIST/MonoProb",
    "checked": true,
    "id": "7b9cec6d1860ac0bbf9c52cec046c8b34a07a1ec",
    "semantic_title": "monoprob: self-supervised monocular depth estimation with interpretable uncertainty",
    "citation_count": 0,
    "authors": [
      "Rémi Marsal",
      "Florian Chabot",
      "Angélique Loesch",
      "William Grolleau",
      "Hichem Sahbi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Pham_LP-OVOD_Open-Vocabulary_Object_Detection_by_Linear_Probing_WACV_2024_paper.html": {
    "title": "LP-OVOD: Open-Vocabulary Object Detection by Linear Probing",
    "volume": "main",
    "abstract": "This paper addresses the challenging problem of open-vocabulary object detection (OVOD) where an object detector must identify both seen and unseen classes in test images without labeled examples of the unseen classes in training. A typical approach for OVOD is to use joint text-image embeddings of CLIP to assign box proposals to their closest text label. However, this method has a critical issue: many low-quality boxes, such as over- and under-covered-object boxes, have the same similarity score as high-quality boxes since CLIP is not trained on exact object location information. To address this issue, we propose a novel method, LP-OVOD, that discards low-quality boxes by training a sigmoid linear classifier on pseudo labels retrieved from the top relevant region proposals to the novel text. Notably, LP-OVOD seamlessly integrates the knowledge distillation technique from ViLD, resulting in a new state-of-the-art OVOD approach. Experimental results on COCO affirm the superior performance of our approach over prior work, achieving 40.5 in AP_novel using ResNet50 as the backbone and without external datasets or knowing novel classes in training. Our code will be available at https://github.com/VinAIResearch/LP-OVOD",
    "checked": true,
    "id": "8220cdb2037b084f89b8122bb8d6096a045ecd5b",
    "semantic_title": "lp-ovod: open-vocabulary object detection by linear probing",
    "citation_count": 1,
    "authors": [
      "Chau Pham",
      "Truong Vu",
      "Khoi Nguyen"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Beck_Beyond_Active_Learning_Leveraging_the_Full_Potential_of_Human_Interaction_WACV_2024_paper.html": {
    "title": "Beyond Active Learning: Leveraging the Full Potential of Human Interaction via Auto-Labeling, Human Correction, and Human Verification",
    "volume": "main",
    "abstract": "Active Learning (AL) is a human-in-the-loop framework to interactively and adaptively label data instances, thereby enabling significant gains in model performance compared to random sampling. AL approaches function by selecting the hardest instances to label, often relying on notions of diversity and uncertainty. However, we believe that these current paradigms of AL do not leverage the full potential of human interaction granted by automated label suggestions. Indeed, we show that for many classification tasks and datasets, most people verifying if an automatically suggested label is correct take 3x to 4x less time than they do changing an incorrect suggestion to the correct label (or labeling from scratch without any suggestion). Utilizing this result, we propose CLARIFIER (aCtive LeARnIng From tIEred haRdness), an Interactive Learning framework that admits more effective use of human interaction by leveraging the reduced cost of verification. By targeting the hard (uncertain) instances with existing AL methods, the intermediate instances with a novel label suggestion scheme using submodular mutual information functions on a per-class basis, and the easy (confident) instances with highest-confidence auto-labeling, CLARIFIER can improve over the performance of existing AL approaches on multiple datasets -- particularly on those that have a large number of classes -- by almost 1.5x to 2x in terms of relative labeling cost",
    "checked": true,
    "id": "91543b7bb67153e23b92cf9bb80d2c02a87c38d6",
    "semantic_title": "beyond active learning: leveraging the full potential of human interaction via auto-labeling, human correction, and human verification",
    "citation_count": 0,
    "authors": [
      "Nathan Beck",
      "Krishnateja Killamsetty",
      "Suraj Kothawade",
      "Rishabh Iyer"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Agnolucci_ARNIQA_Learning_Distortion_Manifold_for_Image_Quality_Assessment_WACV_2024_paper.html": {
    "title": "ARNIQA: Learning Distortion Manifold for Image Quality Assessment",
    "volume": "main",
    "abstract": "No-Reference Image Quality Assessment (NR-IQA) aims to develop methods to measure image quality in alignment with human perception without the need for a high-quality reference image. In this work, we propose a self-supervised approach named ARNIQA (leArning distoRtion maNifold for Image Quality Assessment) for modeling the image distortion manifold to obtain quality representations in an intrinsic manner. First, we introduce an image degradation model that randomly composes ordered sequences of consecutively applied distortions. In this way, we can synthetically degrade images with a large variety of degradation patterns. Second, we propose to train our model by maximizing the similarity between the representations of patches of different images distorted equally, despite varying content. Thus, images degraded in the same manner correspond to neighboring positions within the distortion manifold. Finally, we map the image representations to the quality scores with a simple linear regressor, thus without fine-tuning the encoder weights. The experiments show that our approach achieves state-of-the-art performance on several datasets. In addition, ARNIQA demonstrates improved data efficiency, generalization capabilities, and robustness compared to competing methods. The code and the model are publicly available at https://github.com/miccunifi/ARNIQA",
    "checked": true,
    "id": "5bbe3e2fee81c664ace91eeb4745096da34d7317",
    "semantic_title": "arniqa: learning distortion manifold for image quality assessment",
    "citation_count": 0,
    "authors": [
      "Lorenzo Agnolucci",
      "Leonardo Galteri",
      "Marco Bertini",
      "Alberto Del Bimbo"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ma_CVTHead_One-Shot_Controllable_Head_Avatar_With_Vertex-Feature_Transformer_WACV_2024_paper.html": {
    "title": "CVTHead: One-Shot Controllable Head Avatar With Vertex-Feature Transformer",
    "volume": "main",
    "abstract": "Reconstructing personalized animatable head avatars has significant implications in the fields of AR/VR. Existing methods for achieving explicit face control of 3D Morphable Models (3DMM) typically rely on multi-view images or videos of a single subject, making the reconstruction process complex. Additionally, the traditional rendering pipeline is time-consuming, limiting real-time animation possibilities. In this paper, we introduce CVTHead, a novel approach that generates controllable neural head avatars from a single reference image using point-based neural rendering. CVTHead considers the sparse vertices of mesh as the point set and employs the proposed Vertex-feature Transformer to learn local feature descriptors for each vertex. This enables the modeling of long-range dependencies among all the vertices. Experimental results on the VoxCeleb dataset demonstrate that CVTHead achieves comparable performance to state-of-the-art graphics-based methods. Moreover, it enables efficient rendering of novel human heads with various expressions, head poses, and camera views. These attributes can be explicitly controlled using the coefficients of 3DMMs, facilitating versatile and realistic animation in real-time scenarios",
    "checked": true,
    "id": "bd18b76861bfb50835ca53d58f9a555902e0e92f",
    "semantic_title": "cvthead: one-shot controllable head avatar with vertex-feature transformer",
    "citation_count": 0,
    "authors": [
      "Haoyu Ma",
      "Tong Zhang",
      "Shanlin Sun",
      "Xiangyi Yan",
      "Kun Han",
      "Xiaohui Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Yenamandra_FIRe_Fast_Inverse_Rendering_Using_Directional_and_Signed_Distance_Functions_WACV_2024_paper.html": {
    "title": "FIRe: Fast Inverse Rendering Using Directional and Signed Distance Functions",
    "volume": "main",
    "abstract": "Neural 3D implicit representations learn priors that are useful for diverse applications, such as single- or multiple-view 3D reconstruction. A major downside of existing approaches while rendering an image is that they require evaluating the network multiple times per camera ray so that the high computational time forms a bottleneck for downstream applications. We address this problem by introducing a novel neural scene representation that we call the directional distance function (DDF). To this end, we learn a signed distance function (SDF) along with our DDF model to represent a class of shapes. Specifically, our DDF is defined on the unit sphere and predicts the distance to the surface along any given direction. Therefore, our DDF allows rendering images with just a single network evaluation per camera ray. Based on our DDF, we present a novel fast algorithm (FIRe) to reconstruct 3D shapes given a posed depth map. We evaluate our proposed method on 3D reconstruction from single-view depth images, where we empirically show that our algorithm reconstructs 3D shapes more accurately and it is more than 15 times faster (per iteration) than competing methods",
    "checked": true,
    "id": "d752ea9713f769164fe7e671604edcf5ed8a1d16",
    "semantic_title": "fire: fast inverse rendering using directional and signed distance functions",
    "citation_count": 3,
    "authors": [
      "Tarun Yenamandra",
      "Ayush Tewari",
      "Nan Yang",
      "Florian Bernard",
      "Christian Theobalt",
      "Daniel Cremers"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Lin_Ego2HandsPose_A_Dataset_for_Egocentric_Two-Hand_3D_Global_Pose_Estimation_WACV_2024_paper.html": {
    "title": "Ego2HandsPose: A Dataset for Egocentric Two-Hand 3D Global Pose Estimation",
    "volume": "main",
    "abstract": "Color-based two-hand 3D pose estimation in the global coordinate system is essential in many applications. However, there are very few datasets dedicated to this task and no existing dataset supports estimation in a non-laboratory environment. This is largely attributed to the sophisticated data collection process required for 3D hand pose annotations, which also leads to difficulty in obtaining instances with the level of visual diversity needed for estimation in the wild. Progressing towards this goal, a large-scale dataset Ego2Hands was recently proposed to address the task of two-hand segmentation and detection in the wild. The proposed composition-based data generation technique can create two-hand instances with quality, quantity and diversity that generalize well to unseen domains. In this work, we present Ego2HandsPose, an extension of Ego2Hands that contains 3D hand pose annotation and is the first dataset that enables color-based two-hand 3D tracking in unseen domains. To this end, we develop a set of parametric fitting algorithms to enable 1) 3D hand pose annotation using a single image, 2) automatic conversion from 2D to 3D hand poses and 3) accurate two-hand tracking with temporal consistency. We provide incremental quantitative analysis on the multi-stage pipeline and show that training on our dataset achieves state-of-the-art results that significantly outperforms other datasets for the task of egocentric two-hand global 3D pose estimation",
    "checked": true,
    "id": "087bc64c4efdefa6613a7832cd6877a4cf319b53",
    "semantic_title": "ego2handspose: a dataset for egocentric two-hand 3d global pose estimation",
    "citation_count": 2,
    "authors": [
      "Fanqing Lin",
      "Tony Martinez"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Yang_Improving_Vision-and-Language_Reasoning_via_Spatial_Relations_Modeling_WACV_2024_paper.html": {
    "title": "Improving Vision-and-Language Reasoning via Spatial Relations Modeling",
    "volume": "main",
    "abstract": "Visual commonsense reasoning (VCR) is a challenging multi-modal task, which requires high-level cognition and commonsense reasoning ability about the real world. In recent years, large-scale pre-training approaches have been developed and promoted the state-of-the-art performance of VCR. However, the existing approaches almost employ the BERT-like objectives to learn multi-modal representations. These objectives motivated from the text-domain are insufficient for the excavation on the complex scenario of visual modality. Most importantly, the spatial distribution of the visual objects is basically neglected. To address the above issue, we propose to construct the spatial relation graph based on the given visual scenario. Further, we design two pre- training tasks named object position regression (OPR) and spatial relation classification (SRC) to learn to reconstruct the spatial relation graph respectively. Quantitative analysis suggests that the proposed method can guide the representations to maintain more spatial context and facilitate the attention on the essential visual regions for reasoning. We achieve the state-of-the-art results on VCR and two other vision-and-language reasoning tasks VQA, and NLVR2",
    "checked": true,
    "id": "6c9740c3d439aa833acce1aadf1924ba70bb12f4",
    "semantic_title": "improving vision-and-language reasoning via spatial relations modeling",
    "citation_count": 1,
    "authors": [
      "Cheng Yang",
      "Rui Xu",
      "Ye Guo",
      "Peixiang Huang",
      "Yiru Chen",
      "Wenkui Ding",
      "Zhongyuan Wang",
      "Hong Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Greenwell_WATCH_Wide-Area_Terrestrial_Change_Hypercube_WACV_2024_paper.html": {
    "title": "WATCH: Wide-Area Terrestrial Change Hypercube",
    "volume": "main",
    "abstract": "Monitoring Earth activity using data collected from multiple satellite imaging platforms in a unified way is a significant challenge, especially with large variability in image resolution, spectral bands, and revisit rates. Further, the availability of sensor data varies across time as new platforms are launched. In this work, we introduce an adaptable framework and network architecture capable of predicting on subsets of the available platforms, bands, or temporal ranges it was trained on. Our system, called WATCH, is highly general and can be applied to a variety of geospatial tasks. In this work, we analyze the performance of WATCH using the recent IARPA SMART public dataset and metrics. We focus primarily on the problem of broad area search for heavy construction sites. Experiments validate the robustness of WATCH during inference to limited sensor availability, as well the the ability to alter inference-time spatial or temporal sampling. WATCH is open source and available for use on this or other remote sensing problems. Code and model weights are available at: https://gitlab.kitware.com/computer-vision/geowatch",
    "checked": false,
    "id": "c3b96f637086fab1dd9e8c3e571935ce0b4c33e0",
    "semantic_title": "a review of the impacts of major terrestrial invasive alien plants in ireland",
    "citation_count": 6,
    "authors": [
      "Connor Greenwell",
      "Jon Crall",
      "Matthew Purri",
      "Kristin Dana",
      "Nathan Jacobs",
      "Armin Hadzic",
      "Scott Workman",
      "Matt Leotta"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Liu_Detecting_Content_Segments_From_Online_Sports_Streaming_Events_Challenges_and_WACV_2024_paper.html": {
    "title": "Detecting Content Segments From Online Sports Streaming Events: Challenges and Solutions",
    "volume": "main",
    "abstract": "Developing a client-side segmentation algorithm for online sports streaming holds significant importance. For instance, in order to assess the video quality from an end-user perspective such as artifact detection, it is important to initially segment the content within the streaming playback. The challenge lies in localizing the content due to the intricate scene changes between content and non-content sections in popular sports like football, tennis, baseball, and more. Client-side content detection can be implemented in two ways: intrusively, involving the interception of network traffic and parsing service provider data and logs, or non-intrusively, which entails capturing streamed videos from content providers and subjecting them to analysis using computer vision technologies. In this paper, we introduce a non-intrusive framework that leverages a combination of traditional machine learning algorithms and deep neural networks (DNN) to distinguish content sections from non-content sections across various online sports streaming services. Our algorithm has demonstrated a remarkable level of accuracy and effectiveness in sports broadcasting events, effectively overcoming the complexities introduced by intricate non-content insertion methods during the games",
    "checked": false,
    "id": "38a8627c4680da29e56ca1cb32e1e88dc7d0fac8",
    "semantic_title": "detecting newsworthy events in a journalistic platform based",
    "citation_count": 7,
    "authors": [
      "Zongyi Liu",
      "Yarong Feng",
      "Shunyan Luo",
      "Yuan Ling",
      "Shujing Dong",
      "Shuyi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Shukla_Vikriti-ID_A_Novel_Approach_for_Real_Looking_Fingerprint_Data-Set_Generation_WACV_2024_paper.html": {
    "title": "Vikriti-ID: A Novel Approach for Real Looking Fingerprint Data-Set Generation",
    "volume": "main",
    "abstract": "Fingerprint recognition research faces significant challenges due to the limited availability of extensive and publicly available fingerprint databases. Existing databases lack a sufficient number of identities and fingerprint impressions, which hinders progress in areas such as Fingerprintbased access control. To address this challenge, we present Vikriti-ID, a synthetic fingerprint generator capable of generating unique fingerprints with multiple impressions. Using Vikriti-ID, we generated a large database containing 500000 unique fingerprints, each with 10 associated impressions. We then demonstrate the effectiveness of the database generated by Vikriti-ID by evaluating it for imposter-genuine score distribution and EER score. Apart from this we also trained a deep network to check the usability of data. We trained a deep network on both Vikriti-ID generated data as well as public data. This generated data achieved an Equal Error Rate(EER) of 0.16%, AUC of 0.89%. This improvement is possible due to the limitations of existing publicly available data-set, which struggle in numbers or multiple impressions",
    "checked": true,
    "id": "66429ac38b33dcb7cf89d6755ef1602c2743997a",
    "semantic_title": "vikriti-id: a novel approach for real looking fingerprint data-set generation",
    "citation_count": 0,
    "authors": [
      "Rishabh Shukla",
      "Aditya Sinha",
      "Vansh Singh",
      "Harkeerat Kaur"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Berman_PETIT-GAN_Physically_Enhanced_Thermal_Image-Translating_Generative_Adversarial_Network_WACV_2024_paper.html": {
    "title": "PETIT-GAN: Physically Enhanced Thermal Image-Translating Generative Adversarial Network",
    "volume": "main",
    "abstract": "Thermal multispectral imagery is imperative for a plethora of environmental applications. Unfortunately, there are no publicly-available datasets of thermal multispectral images with a high spatial resolution that would enable the development of algorithms and systems in this field. However, image-to-image (I2I) translation could be used to artificially synthesize such data by transforming largely-available datasets of other visual modalities. In most cases, pairs of content-wise-aligned input-target images are not available, making it harder to train and converge to a satisfying solution. Nevertheless, some data domains, and particularly the thermal domain, have unique properties that tie the input to the output that could help mitigate those weaknesses. We propose PETIT-GAN, a physically enhanced thermal image-translating generative adversarial network to transform between different thermal modalities - a step toward synthesizing a complete thermal multispectral dataset. Our novel approach embeds physically modeled prior information in an UI2I translation to produce outputs with greater fidelity to the target modality. We further show that our solution outperforms the current state-of-the-art architectures at thermal UI2I translation by approximately 50% with respect to the standard perceptual metrics, and enjoys a more robust training procedure. The code and data used for the development and analysis of our method are publicly available and can be accessed through our project's website: https://bermanz.github.io/PETIT",
    "checked": false,
    "id": "841b19479c32746592af663443a02ddb79435e76",
    "semantic_title": "a conditional gan architecture for colorization of thermal infrared images",
    "citation_count": 0,
    "authors": [
      "Omri Berman",
      "Navot Oz",
      "David Mendlovic",
      "Nir Sochen",
      "Yafit Cohen",
      "Iftach Klapp"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Radhakrishnan_Design_Choices_for_Enhancing_Noisy_Student_Self-Training_WACV_2024_paper.html": {
    "title": "Design Choices for Enhancing Noisy Student Self-Training",
    "volume": "main",
    "abstract": "Semi-supervised learning approaches train on small sets of labeled data in addition to large sets of unlabeled data. Self-training is a semi-supervised teacher-student approach that often suffers from \"confirmation bias\" that occurs when the student model repeatedly overfits to incorrect pseudo-labels given by the teacher model for the unlabeled data. This bias impedes improvements in pseudo-label accuracy across self-training iterations, leading to unwanted saturation in model performance after just a few iterations. In this work, we study multiple design choices to improve the Noisy Student self-training pipeline and reduce confirmation bias. We showed that our proposed Weighted SplitBatch Sampler and Dataset-Adaptive Techniques for Model Calibration and Entropy-Based Pseudo-Label Selection provided performance gains over existing design choices across multiple datasets. Finally, we also study the extendability of our enhanced approach to Open Set unlabeled data (containing classes not seen in labeled data). The source code can be licensed for use via email",
    "checked": false,
    "id": "fe86f15492427d8953d6b461e5899aa866d56efc",
    "semantic_title": "enhancing self-training methods",
    "citation_count": 0,
    "authors": [
      "Aswathnarayan Radhakrishnan",
      "Jim Davis",
      "Zachary Rabin",
      "Benjamin Lewis",
      "Matthew Scherreik",
      "Roman Ilin"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Shugaev_ArcGeo_Localizing_Limited_Field-of-View_Images_Using_Cross-View_Matching_WACV_2024_paper.html": {
    "title": "ArcGeo: Localizing Limited Field-of-View Images Using Cross-View Matching",
    "volume": "main",
    "abstract": "Cross-view matching techniques for image geolocalization attempt to match features in ground level imagery against a collection of satellite images to determine the position of given query image. We present a novel cross-view image matching approach called ArcGeo which introduces a batch-all angular margin loss and several train-time strategies including large-scale pretraining and FoV-based data augmentation. This allows our model to perform well even in challenging cases with limited field-of-view (FoV). Further, we evaluate multiple model architectures, data augmentation approaches and optimization strategies to train a deep cross-view matching network, specifically optimized for limited FoV cases. In low FoV experiments (FoV = 90deg) our method improves top-1 image recall rate on the CVUSA dataset from 30.12% to 43.08%. We also demonstrate improved performance over the state-of-the-art techniques for panoramic cross-view retrieval, improving top-1 recall from 95.43% to 96.06% on the CVUSA dataset and from 64.52% to 79.88% on the CVACT test dataset. Lastly, we evaluate the role of large-scale pretraining for improved robustness. With appropriate pretraining on external data, our model improves top-1 recall dramatically to 66.83% for FoV = 90deg test case on CVUSA, an increase of over twice what is reported by existing approaches",
    "checked": false,
    "id": "549ca7b0442cfb23fc346c7758fde43b926ab9b8",
    "semantic_title": "wide-area geolocalization with a limited field of view camera",
    "citation_count": 1,
    "authors": [
      "Maxim Shugaev",
      "Ilya Semenov",
      "Kyle Ashley",
      "Michael Klaczynski",
      "Naresh Cuntoor",
      "Mun Wai Lee",
      "Nathan Jacobs"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Yue_Understanding_Hyperbolic_Metric_Learning_Through_Hard_Negative_Sampling_WACV_2024_paper.html": {
    "title": "Understanding Hyperbolic Metric Learning Through Hard Negative Sampling",
    "volume": "main",
    "abstract": "In recent years, there has been a growing trend of incorporating hyperbolic geometry methods into computer vision. While these methods have achieved state-of-the-art performance on various metric learning tasks using hyperbolic distance measurements, the underlying theoretical analysis supporting this superior performance remains under-exploited. In this study, we investigate the effects of integrating hyperbolic space into metric learning, particularly when training with contrastive loss. We identify a need for a comprehensive comparison between Euclidean and hyperbolic spaces regarding the temperature effect in the contrastive loss within the existing literature. To address this gap, we conduct an extensive investigation to benchmark the results of Vision Transformers (ViTs) using a hybrid objective function that combines loss from Euclidean and hyperbolic spaces. Additionally, we provide a theoretical analysis of the observed performance improvement. We also reveal that hyperbolic metric learning is highly related to hard negative sampling, providing insights for future work. This work will provide valuable data points and experience in understanding hyperbolic image embeddings. To shed more light on problem-solving and encourage further investigation into our approach, our code is available online",
    "checked": false,
    "id": "db38c49100135a1775d3fb07812b23cfa82991ce",
    "semantic_title": "duality of roles: understanding the work performance of teaching mothers on modular distance learning in the first districtof city division of santa rosa, laguna",
    "citation_count": 0,
    "authors": [
      "Yun Yue",
      "Fangzhou Lin",
      "Guanyi Mou",
      "Ziming Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Chhikara_FIRE_Food_Image_to_REcipe_Generation_WACV_2024_paper.html": {
    "title": "FIRE: Food Image to REcipe Generation",
    "volume": "main",
    "abstract": "Food computing has emerged as a prominent multidisciplinary field of research in recent years. An ambitious goal of food computing is to develop end-to-end intelligent systems capable of autonomously producing recipe information for a food image. Current image-to-recipe methods are retrieval-based and their success depends heavily on the dataset size and diversity, as well as the quality of learned embeddings. Meanwhile, the emergence of powerful attention-based vision and language models presents a promising avenue for accurate and generalizable recipe generation, which has yet to be extensively explored. This paper proposes FIRE, a novel multimodal methodology tailored to recipe generation in the food computing domain, which generates the food title, ingredients, and cooking instructions based on input food images. FIRE leverages the BLIP model to generate titles, utilizes a Vision Transformer with a decoder for ingredient extraction, and employs the T5 model to generate recipes incorporating titles and ingredients as inputs. We showcase two practical applications that can benefit from integrating FIRE with large language model prompting: recipe customization to fit recipes to user preferences and recipe-to-code transformation to enable automated cooking processes. Our experimental findings validate the efficacy of our proposed approach, underscoring its potential for future advancements and widespread adoption in food computing",
    "checked": true,
    "id": "9ecf08567b3d944d72633ad6e86a3e0e84f7d4fc",
    "semantic_title": "fire: food image to recipe generation",
    "citation_count": 1,
    "authors": [
      "Prateek Chhikara",
      "Dhiraj Chaurasia",
      "Yifan Jiang",
      "Omkar Masur",
      "Filip Ilievski"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Shen_DiffCLIP_Leveraging_Stable_Diffusion_for_Language_Grounded_3D_Classification_WACV_2024_paper.html": {
    "title": "DiffCLIP: Leveraging Stable Diffusion for Language Grounded 3D Classification",
    "volume": "main",
    "abstract": "Large pre-trained models have revolutionized the field of computer vision by facilitating multi-modal learning. Notably, the CLIP model has exhibited remarkable proficiency in tasks such as image classification, object detection, and semantic segmentation. Nevertheless, its efficacy in processing 3D point clouds is restricted by the domain gap between the depth maps derived from 3D projection and the training images of CLIP. This paper introduces DiffCLIP, a novel pre-training framework that seamlessly integrates stable diffusion with ControlNet. The primary objective of DiffCLIP is to bridge the domain gap inherent in the visual branch. Furthermore, to address few-shot tasks in the textual branch, we incorporate a style-prompt generation module. Extensive experiments on the ModelNet10, ModelNet40, and ScanObjectNN datasets show that DiffCLIP has strong abilities for 3D understanding. By using stable diffusion and style-prompt generation, DiffCLIP achieves an accuracy of 43.2% for zero-shot classification on OBJ_BG of ScanObjectNN, which is state-of-the-art performance, and an accuracy of 82.4% for zero-shot classification on ModelNet10, which is also state-of-the-art performance",
    "checked": true,
    "id": "837c9273dd5d2f3f80fd3376cd3d69f5fac024d3",
    "semantic_title": "diffclip: leveraging stable diffusion for language grounded 3d classification",
    "citation_count": 2,
    "authors": [
      "Sitian Shen",
      "Zilin Zhu",
      "Linqian Fan",
      "Harry Zhang",
      "Xinxiao Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/De_Nardin_A_One-Shot_Learning_Approach_To_Document_Layout_Segmentation_of_Ancient_WACV_2024_paper.html": {
    "title": "A One-Shot Learning Approach To Document Layout Segmentation of Ancient Arabic Manuscripts",
    "volume": "main",
    "abstract": "Document layout segmentation is a challenging task due to the variability and complexity of document layouts. Ancient manuscripts in particular are often damaged by age, have very irregular layouts, and are characterized by progressive editing from different authors over a large time window. All these factors make the semantic segmentation process of specific areas, such as main text and side text, very difficult. However, the study of these manuscripts turns out to be fundamental for historians and humanists, so much so that in recent years the demand for machine learning approaches aimed at simplifying the extraction of information from these documents has consistently increased, leading document layout analysis to become an increasingly important research area. In order for machine learning techniques to be applied effectively to this task, however, a large amount of correctly and precisely labeled images is required for their training. This is obviously a limitation for this field of research as ground truth must be precisely and manually crafted by expert humanists, making it a very time-consuming process. In this paper, with the aim of overcoming this limitation, we present an efficient document layout segmentation framework, which while being trained on only one labeled page per manuscript still achieves state-of-the-art performance compared to other popular approaches trained on all the available data when tested on a challenging dataset of ancient Arabic manuscripts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Axel De Nardin",
      "Silvia Zottin",
      "Claudio Piciarelli",
      "Emanuela Colombi",
      "Gian Luca Foresti"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Gilg_Do_We_Still_Need_Non-Maximum_Suppression_Accurate_Confidence_Estimates_and_WACV_2024_paper.html": {
    "title": "Do We Still Need Non-Maximum Suppression? Accurate Confidence Estimates and Implicit Duplication Modeling With IoU-Aware Calibration",
    "volume": "main",
    "abstract": "Object detectors are at the heart of many semi- and fully autonomous decision systems and are poised to become even more indispensable. They are, however, still lacking in accessibility and can sometimes produce unreliable predictions. Especially concerning in this regard are the (essentially hand-crafted) non-maximum suppression algorithms that lead to an obfuscated prediction process and biased confidence estimates. We show that we can eliminate classic NMS-style post-processing by using IoU-aware calibration. IoU-aware calibration is a conditional Beta calibration; this makes it parallelizable with no hyper-parameters. Instead of arbitrary cutoffs or discounts, it implicitly accounts for the likelihood of each detection being a duplicate and adjusts the confidence score accordingly, resulting in empirically based precision estimates for each detection. Our extensive experiments on diverse detection architectures show that the proposed IoU-aware calibration can successfully model duplicate detections and improve calibration. Compared to the standard sequential NMS and calibration approach, our joint modeling can deliver performance gains over the best NMS-based alternative while producing consistently better-calibrated confidence predictions with less complexity",
    "checked": true,
    "id": "8913e4970e5d4152e7a7acf76e6220a8118b0071",
    "semantic_title": "do we still need non-maximum suppression? accurate confidence estimates and implicit duplication modeling with iou-aware calibration",
    "citation_count": 0,
    "authors": [
      "Johannes Gilg",
      "Torben Teepe",
      "Fabian Herzog",
      "Philipp Wolters",
      "Gerhard Rigoll"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Saad_On_the_Importance_of_Large_Objects_in_CNN_Based_Object_WACV_2024_paper.html": {
    "title": "On the Importance of Large Objects in CNN Based Object Detection Algorithms",
    "volume": "main",
    "abstract": "Object detection models, a prominent class of machine learning algorithms, aim to identify and precisely locate objects in images or videos. However, the task of accurately localizing objects within images yields uneven performances sometimes caused by the objects sizes and the quality of the images and labels. In this paper, we highlight the importance of large objects in learning features that are critical for all sizes. Given these findings, we propose to address this by introducing a weighting term into the loss during training. This term is a function of the object area size. We show that giving more weight to large objects leads to improvement in detection scores across all sizes and so an overall improvement in Object Detectors performances (+2% mAP on small objects, +2% on medium and +4% on large on COCO val 2017 with InternImage-T). Additional experiments and ablation studies with different models and on different dataset further confirm the robustness of our findings",
    "checked": true,
    "id": "9c5be43fd122087d410a5419c718e24ec5202761",
    "semantic_title": "on the importance of large objects in cnn based object detection algorithms",
    "citation_count": 0,
    "authors": [
      "Ahmed Ben Saad",
      "Gabriele Facciolo",
      "Axel Davy"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Goto_Learning_Intra-Class_Multimodal_Distributions_With_Orthonormal_Matrices_WACV_2024_paper.html": {
    "title": "Learning Intra-Class Multimodal Distributions With Orthonormal Matrices",
    "volume": "main",
    "abstract": "In this paper, we address the challenges of representing feature distributions which have multimodality within a class in deep neural networks. Existing online clustering methods employ sub-centroids to capture intra-class variations. However, conducting online clustering faces some limitations, i.e., online clustering assigns only a single subcentroid to a feature vector extracted from a backbone and ignores the relationship between the other sub-centroids and the feature vector, and updating sub-centroids in an online clustering manner incurs significant storage costs. To address these limitations, we propose a novel method utilizing orthonormal matrices instead of sub-centroids for relaxing discrete assignments into continuous assignments. We update the orthonormal matrices using a gradient-based method, which eliminates the need for online clustering or additional storage. Experimental results on the CIFAR and ImageNet datasets exhibit that the proposed method outperforms current online clustering techniques in classification accuracy, sub-category discovery, and transferability, providing an efficient solution to the challenges posed by complex recognition targets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jumpei Goto",
      "Yohei Nakata",
      "Kiyofumi Abe",
      "Yasunori Ishii",
      "Takayoshi Yamashita"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Christensen_Assessing_Neural_Network_Robustness_via_Adversarial_Pivotal_Tuning_WACV_2024_paper.html": {
    "title": "Assessing Neural Network Robustness via Adversarial Pivotal Tuning",
    "volume": "main",
    "abstract": "The robustness of image classifiers is essential to their deployment in the real world. The ability to assess this resilience to manipulations or deviations from the training data is thus crucial. These modifications have traditionally consisted of minimal changes that still manage to fool classifiers, and modern approaches are increasingly robust to them. Semantic manipulations that modify elements of an image in meaningful ways have thus gained traction for this purpose. However, they have primarily been limited to style, color, or attribute changes. While expressive, these manipulations do not make use of the full capabilities of a pretrained generative model. In this work, we aim to bridge this gap. We show how a pretrained image generator can be used to semantically manipulate images in a detailed, diverse, and photorealistic way while still preserving the class of the original image. Inspired by recent GAN-based image inversion methods, we propose a method called Adversarial Pivotal Tuning (APT). Given an image, APT first finds a pivot latent space input that reconstructs the image using a pretrained generator. It then adjusts the generator's weights to create small yet semantic manipulations in order to fool a pretrained classifier. APT preserves the full expressive editing capabilities of the generative model. We demonstrate that APT is capable of a wide range of class-preserving semantic image manipulations that fool a variety of pretrained classifiers. Finally, we show that classifiers that are robust to other benchmarks are not robust to APT manipulations and suggest a method to improve them",
    "checked": true,
    "id": "736a85e083f881f81307cafcbc5fe19295588135",
    "semantic_title": "assessing neural network robustness via adversarial pivotal tuning",
    "citation_count": 0,
    "authors": [
      "Peter Ebert Christensen",
      "Vésteinn Snæbjarnarson",
      "Andrea Dittadi",
      "Serge Belongie",
      "Sagie Benaim"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Shukla_Opinion_Unaware_Image_Quality_Assessment_via_Adversarial_Convolutional_Variational_Autoencoder_WACV_2024_paper.html": {
    "title": "Opinion Unaware Image Quality Assessment via Adversarial Convolutional Variational Autoencoder",
    "volume": "main",
    "abstract": "Image quality assessment is a challenging computer vision task due to the lack of corresponding reference (pristine) images. This no-reference bottleneck has been tackled with the utilisation of subjective mean opinion scores (MOS) termed as supervised blind image quality assessment (BIQA) methods. However, inaccessible opinion score scenarios limits their applicability. To relieve these limitations, we propose to employ reconstruction based learning trained only on pristine images. This permits an implicit distribution learning of pristine images and the deviation from this learned feature distribution is subsequently utilised for unsupervised image quality assessment. Specifically, an adversarial convolutional variational auto-encoder framework is employed with KL divergence, perceptual and discriminator loss. With state-of-the-art results on four benchmark datasets, we demonstrate the effectiveness of our proposed framework. An ablation study has also been conducted to highlight the contribution of each module i.e. loss and quality metric for an efficient unsupervised BIQA",
    "checked": false,
    "id": "b8dc424708b0d6fb384c8b90b3f7151006da7380",
    "semantic_title": "bridge the gap between full-reference and no-reference: a totally full-reference induced blind image quality assessment via deep neural networks",
    "citation_count": 0,
    "authors": [
      "Ankit Shukla",
      "Avinash Upadhyay",
      "Swati Bhugra",
      "Manoj Sharma"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Matsune_A_Geometry_Loss_Combination_for_3D_Human_Pose_Estimation_WACV_2024_paper.html": {
    "title": "A Geometry Loss Combination for 3D Human Pose Estimation",
    "volume": "main",
    "abstract": "Root-relative loss has formed the basis of 3D human pose estimation for many years. However, this point-to-point loss treats every keypoint separately and ignores internal connection information of the human body. This leads to illegal pose prediction, which humans cannot form in the real world. It also suffers from differences in estimation difficulty between keypoints. The farther the keypoint is from the torso, the less accurate it is. To address the above problems, this paper proposes geometry loss combination to utilize the geometric relationship between each keypoint fully. This loss combination consists of three loss functions: root-relative pose, bone length, and body part orientation. The previous two have already been used in prior works. Beyond them, we further develop a loss function called body part orientation loss for local body parts. Intuitively, the human body can be divided into three parts: the head, torso, and limbs. Based on this, we select the corresponding keypoints and create virtual planes for each body part. Experiments with different datasets and models demonstrate that our proposed method improves the prediction accuracy. We also achieve MPJPE of 65.0 on the 3DPW test set, which outperforms state-of-the-art methods",
    "checked": false,
    "id": "40ffeae1b141191e34dccc8dc28167f410148f8c",
    "semantic_title": "self-supervised method for 3d human pose estimation with consistent shape and viewpoint factorization",
    "citation_count": 3,
    "authors": [
      "Ai Matsune",
      "Shichen Hu",
      "Guangquan Li",
      "Sihan Wen",
      "Xiantan Zhu",
      "Zhiming Tan"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Fukushi_Few-Shot_Generative_Model_for_Skeleton-Based_Human_Action_Synthesis_Using_Cross-Domain_WACV_2024_paper.html": {
    "title": "Few-Shot Generative Model for Skeleton-Based Human Action Synthesis Using Cross-Domain Adversarial Learning",
    "volume": "main",
    "abstract": "We propose few-shot generative models of skeleton-based human actions on limited samples of the target domain. We exploit large public datasets as a source of motion variations by introducing novel cross-domain and entropy regularization losses that effectively transfer the diversity of the motions contained in the source to the target domain. First, target samples are divided into patches, which are a set of short motion clips. For each patch, we search for a reference motion from the source dataset that is similar to the patch. Next, in adversarial training, our cross-domain regularization encourages the generated sequences to resemble the reference motion at the patch level. Entropy regularization prevents mode collapse by forcing the generator to follow the distribution of the source dataset. Experiments are performed on public datasets where we utilize three action classes from NTU RGB+D 120 as the target and all data of 60 action classes in NTU RGB+D as the source. Ten samples for each target action class, 30 in total, are selected as target data. The results demonstrate that data augmented with the proposed method improve recognition accuracy by 28 % using a ST-GCN classifier",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kenichiro Fukushi",
      "Yoshitaka Nozaki",
      "Kosuke Nishihara",
      "Kentaro Nakahara"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Liang_Linking_Convolutional_Kernel_Size_to_Generalization_Bias_in_Face_Analysis_WACV_2024_paper.html": {
    "title": "Linking Convolutional Kernel Size to Generalization Bias in Face Analysis CNNs",
    "volume": "main",
    "abstract": "Training dataset biases are by far the most scrutinized factors when explaining algorithmic biases of neural networks. In contrast, hyperparameters related to the neural network architecture have largely been ignored even though different network parameterizations are known to induce different implicit biases over learned features. For example, convolutional kernel size is known to affect the frequency content of features learned in CNNs. In this work, we present a causal framework for linking an architectural hyperparameter to out-of-distribution algorithmic bias. Our framework is experimental, in that we train several versions of a network with an intervention to a specific hyperparameter, and measure the resulting causal effect of this choice on performance bias when a particular out-of-distribution image perturbation is applied. In our experiments, we focused on measuring the causal relationship between convolutional kernel size and face analysis classification bias across different subpopulations (race/gender), with respect to high-frequency image details. We show that modifying kernel size, even in one layer of a CNN, changes the frequency content of learned features significantly across data subgroups leading to biased generalization performance even in the presence of a balanced dataset",
    "checked": true,
    "id": "6f0fb8ec1cbd6e0d97acbc29e735d14727abd38c",
    "semantic_title": "linking convolutional kernel size to generalization bias in face analysis cnns",
    "citation_count": 0,
    "authors": [
      "Hao Liang",
      "Josue Ortega Caro",
      "Vikram Maheshri",
      "Ankit B. Patel",
      "Guha Balakrishnan"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Yuan_Cross-Attention_Between_Satellite_and_Ground_Views_for_Enhanced_Fine-Grained_Robot_WACV_2024_paper.html": {
    "title": "Cross-Attention Between Satellite and Ground Views for Enhanced Fine-Grained Robot Geo-Localization",
    "volume": "main",
    "abstract": "Cross-view image geo-localization aims to determine the locations of outdoor robots by mapping current street-view images with GPS-tagged satellite image patches. Recent works have attained a remarkable level of accuracy in identifying which satellite patches the robot is in, where the location of the central pixel within the matched satellite patch is used as the robot coarse location estimation. This work focuses on robot fine-grained localization within a known satellite patch. Existing fine-grain localization work utilizes correlation operation to obtain similarity between satellite image local descriptors and street-view global descriptors. The correlation operation based on liner matching simplifies the interaction process between two views, leading to a large distance error and affecting model generalization. To address this issue, we devise a cross-view feature fusion network with self-attention and cross-attention layers to replace correlation operation. Additionally, we combine classification and regression prediction to further decrease location distance error. Experiments show that our novel network architecture outperforms the state-of-the-art, exhibiting better generalization capabilities in unseen areas. Specifically, our method reduces the median localization distance error by 43% and 50% respectively in the same area and unseen areas on the VIGOR benchmark",
    "checked": false,
    "id": "f32a96ccbb655a1315d37ea54f5ce7a182283e60",
    "semantic_title": "cross-attention between satellite and ground views for enhanced fine-grained robot geo-localization supplementary material",
    "citation_count": 0,
    "authors": [
      "Dong Yuan",
      "Frederic Maire",
      "Feras Dayoub"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Song_StyleGAN-Fusion_Diffusion_Guided_Domain_Adaptation_of_Image_Generators_WACV_2024_paper.html": {
    "title": "StyleGAN-Fusion: Diffusion Guided Domain Adaptation of Image Generators",
    "volume": "main",
    "abstract": "Can a text-to-image diffusion model be used as a training objective for adapting a GAN generator to another domain? In this paper, we show that the classifier-free guidance can be leveraged as a critic and enable generators to distill knowledge from large-scale text-to-image diffusion models. Generators can be efficiently shifted into new domains indicated by text prompts without access to ground truth samples from target domains. We demonstrate the effectiveness and controllability of our method through extensive experiments. Although not trained to minimize CLIP loss, our model achieves equally high CLIP scores and significantly lower FID than prior work on short prompts and outperforms the baseline qualitatively and quantitatively on long and complicated prompts. To our best knowledge, the proposed method is the first attempt at incorporating large-scale pre-trained diffusion models and distillation sampling for text-driven image generator domain adaptation and gives a quality previously beyond possible. Moreover, we extend our work to 3D-aware style-based generators and DreamBooth guidance",
    "checked": false,
    "id": "ce5bcccd5ab303e73e166b0cbf511d5f5b45acbb",
    "semantic_title": "diffusion guided domain adaptation of image generators",
    "citation_count": 18,
    "authors": [
      "Kunpeng Song",
      "Ligong Han",
      "Bingchen Liu",
      "Dimitris Metaxas",
      "Ahmed Elgammal"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wang_TSP-Transformer_Task-Specific_Prompts_Boosted_Transformer_for_Holistic_Scene_Understanding_WACV_2024_paper.html": {
    "title": "TSP-Transformer: Task-Specific Prompts Boosted Transformer for Holistic Scene Understanding",
    "volume": "main",
    "abstract": "Holistic scene understanding includes semantic segmentation, surface normal estimation, object boundary detection, depth estimation, etc. The key aspect of this problem is to learn representation effectively, as each subtask builds upon not only correlated but also distinct attributes. Inspired by visual-prompt tuning, we propose a Task-Specific Prompts Transformer, dubbed TSP-Transformer, for holistic scene understanding. It features a vanilla transformer in the early stage and tasks-specific prompts transformer encoder in the lateral stage, where tasks-specific prompts are augmented. By doing so, the transformer layer learns the generic information from the shared parts and is endowed with task-specific capacity. First, the tasks-specific prompts serve as induced priors for each task effectively. Moreover, the task-specific prompts can be seen as switches to favor task-specific representation learning for different tasks. Extensive experiments on NYUD-v2 and PASCAL-Context show that our method achieves state-of-the-art performance, validating the effectiveness of our method for holistic scene understanding",
    "checked": true,
    "id": "95c654303277fe5345332d4d2a268b890a246b2d",
    "semantic_title": "tsp-transformer: task-specific prompts boosted transformer for holistic scene understanding",
    "citation_count": 0,
    "authors": [
      "Shuo Wang",
      "Jing Li",
      "Zibo Zhao",
      "Dongze Lian",
      "Binbin Huang",
      "Xiaomei Wang",
      "Zhengxin Li",
      "Shenghua Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Amosy_Late_to_the_Party_On-Demand_Unlabeled_Personalized_Federated_Learning_WACV_2024_paper.html": {
    "title": "Late to the Party? On-Demand Unlabeled Personalized Federated Learning",
    "volume": "main",
    "abstract": "In Federated Learning (FL), multiple clients collaborate to learn a shared model through a central server while keeping data decentralized. Personalized Federated Learning (PFL) further extends FL by learning a personalized model per client. In both FL and PFL, all clients participate in the training process and their labeled data are used for training. However, in reality, novel clients may wish to join a prediction service after it has been deployed, obtaining predictions for their own unlabeled data. Here, we introduce a new learning setup, On-Demand Unlabeled PFL (OD-PFL), where a system trained on a set of clients, needs to be later applied to novel unlabeled clients at inference time. We propose a novel approach to this problem, ODPFL-HN, which learns to produce a new model for the late-to-the-party client. Specifically, we train an encoder network that learns a representation for a client given its unlabeled data. That client representation is fed to a hypernetwork that generates a personalized model for that client. Evaluated on five benchmark datasets, we find that ODPFL-HN generalizes better than the current FL and PFL methods, especially when the novel client has a large shift from training clients. We also analyzed the generalization error for novel clients, and showed analytically and experimentally how novel clients can apply differential privacy to protect their data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ohad Amosy",
      "Gal Eyal",
      "Gal Chechik"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Batzner_EfficientAD_Accurate_Visual_Anomaly_Detection_at_Millisecond-Level_Latencies_WACV_2024_paper.html": {
    "title": "EfficientAD: Accurate Visual Anomaly Detection at Millisecond-Level Latencies",
    "volume": "main",
    "abstract": "Detecting anomalies in images is an important task, especially in real-time computer vision applications. In this work, we focus on computational efficiency and propose a lightweight feature extractor that processes an image in less than a millisecond on a modern GPU. We then use a student-teacher approach to detect anomalous features. We train a student network to predict the extracted features of normal, i.e., anomaly-free training images. The detection of anomalies at test time is enabled by the student failing to predict their features. We propose a training loss that hinders the student from imitating the teacher feature extractor beyond the normal images. It allows us to drastically reduce the computational cost of the student-teacher model, while improving the detection of anomalous features. We furthermore address the detection of challenging logical anomalies that involve invalid combinations of normal local features, for example, a wrong ordering of objects. We detect these anomalies by efficiently incorporating an autoencoder that analyzes images globally. We evaluate our method, called EfficientAD, on 32 datasets from three industrial anomaly detection dataset collections. EfficientAD sets new standards for both the detection and the localization of anomalies. At a latency of two milliseconds and a throughput of six hundred images per second, it enables a fast handling of anomalies. Together with its low error rate, this makes it an economical solution for real-world applications and a fruitful basis for future research",
    "checked": true,
    "id": "a5a1486a7874c85fb62c700594db390976a47632",
    "semantic_title": "efficientad: accurate visual anomaly detection at millisecond-level latencies",
    "citation_count": 11,
    "authors": [
      "Kilian Batzner",
      "Lars Heckler",
      "Rebecca König"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Naylor_Implicit_Neural_Representation_for_Change_Detection_WACV_2024_paper.html": {
    "title": "Implicit Neural Representation for Change Detection",
    "volume": "main",
    "abstract": "Identifying changes in a pair of 3D aerial LiDAR point clouds, obtained during two distinct time periods over the same geographic region presents a significant challenge due to the disparities in spatial coverage and the presence of noise in the acquisition system. The most commonly used approaches to detecting changes in point clouds are based on supervised methods which necessitate extensive labelled data often unavailable in real-world applications. To address these issues, we propose an unsupervised approach that comprises two components: Implicit Neural Representation (INR) for continuous shape reconstruction and a Gaussian Mixture Model for categorising changes. INR offers a grid-agnostic representation for encoding bi-temporal point clouds, with unmatched spatial support that can be regularised to enhance high-frequency details and reduce noise. The reconstructions at each timestamp are compared at arbitrary spatial scales, leading to a significant increase in detection capabilities. We apply our method to a benchmark dataset comprising simulated LiDAR point clouds for urban sprawling. This dataset encompasses diverse challenging scenarios, varying in resolutions, input modalities and noise levels. This enables a comprehensive multi-scenario evaluation, comparing our method with the current state-of-the-art approach. We outperform the previous methods by a margin of 10% in the intersection over union metric. In addition, we put our techniques to practical use by applying them in a real-world scenario to identify instances of illicit excavation of archaeological sites and validate our results by comparing them with findings from field experts",
    "checked": true,
    "id": "f8f7b0d964db2c380550f79ab21e604e0c4cf6cc",
    "semantic_title": "implicit neural representation for change detection",
    "citation_count": 0,
    "authors": [
      "Peter Naylor",
      "Diego Di Carlo",
      "Arianna Traviglia",
      "Makoto Yamada",
      "Marco Fiorucci"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wang_Maximum_Knowledge_Orthogonality_Reconstruction_With_Gradients_in_Federated_Learning_WACV_2024_paper.html": {
    "title": "Maximum Knowledge Orthogonality Reconstruction With Gradients in Federated Learning",
    "volume": "main",
    "abstract": "Federated learning (FL) aims at keeping client data local to preserve privacy. Instead of gathering the data itself, the server only collects aggregated gradient updates from clients. Following the popularity of FL, there has been considerable amount of work, revealing the vulnerability of FL approaches by reconstructing the input data from gradient updates. Yet, most existing works assume an FL setting with unrealistically small batch size, and have poor image quality when the batch size is large. Other works modify the neural network architectures or parameters to the point of being suspicious, and thus, can be detected by clients. Moreover, most of them can only reconstruct one sample input from a large batch. To address these limitations, we propose a novel and completely analytical approach, referred to as the maximum knowledge orthogonality reconstruction (MKOR), to reconstruct clients' input data. Our proposed method reconstructs a mathematically proven high quality image from large batches. MKOR only requires the server to send secretly modified parameters to clients and can efficiently and inconspicuously reconstruct the input images from clients' gradient updates. We evaluate MKOR's performance on the MNIST, CIFAR-100, and ImageNet dataset and compare it with the state-of-the-art works. The results show that MKOR outperforms the existing approaches, and draws attention to a pressing need for further research on the privacy protection of FL so that comprehensive defense approaches can be developed",
    "checked": true,
    "id": "7379fb66224cbc6ab11fef88a0429750dcb5007b",
    "semantic_title": "maximum knowledge orthogonality reconstruction with gradients in federated learning",
    "citation_count": 0,
    "authors": [
      "Feng Wang",
      "Senem Velipasalar",
      "M. Cenk Gursoy"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ragusa_ENIGMA-51_Towards_a_Fine-Grained_Understanding_of_Human_Behavior_in_Industrial_WACV_2024_paper.html": {
    "title": "ENIGMA-51: Towards a Fine-Grained Understanding of Human Behavior in Industrial Scenarios",
    "volume": "main",
    "abstract": "ENIGMA-51 is a new egocentric dataset acquired in an industrial scenario by 19 subjects who followed instructions to complete the repair of electrical boards using industrial tools (e.g., electric screwdriver) and equipments (e.g., oscilloscope). The 51 egocentric video sequences are densely annotated with a rich set of labels that enable the systematic study of human behavior in the industrial domain. We provide benchmarks on four tasks related to human behavior: 1) untrimmed temporal detection of human-object interactions, 2) egocentric human-object interaction detection, 3) short-term object interaction anticipation and 4) natural language understanding of intents and entities. Baseline results show that the ENIGMA-51 dataset poses a challenging benchmark to study human behavior in industrial scenarios. We publicly release the dataset at https://iplab.dmi.unict.it/ENIGMA-51",
    "checked": false,
    "id": "a7401ae3df7969870eef139755553c7cedb7e088",
    "semantic_title": "enigma-51: towards a fine-grained understanding of human-object interactions in industrial scenarios",
    "citation_count": 1,
    "authors": [
      "Francesco Ragusa",
      "Rosario Leonardi",
      "Michele Mazzamuto",
      "Claudia Bonanno",
      "Rosario Scavo",
      "Antonino Furnari",
      "Giovanni Maria Farinella"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Lee_HELA-VFA_A_Hellinger_Distance-Attention-Based_Feature_Aggregation_Network_for_Few-Shot_Classification_WACV_2024_paper.html": {
    "title": "HELA-VFA: A Hellinger Distance-Attention-Based Feature Aggregation Network for Few-Shot Classification",
    "volume": "main",
    "abstract": "Enabling effective learning using only a few presented examples is a crucial but difficult computer vision objective. Few-shot learning have been proposed to address the challenges, and more recently variational inference-based approaches are incorporated to enhance few-shot classification performances. However, the current dominant strategy utilized the Kullback-Leibler (KL) divergences to find the log marginal likelihood of the target class distribution, while neglecting the possibility of other probabilistic comparative measures, as well as the possibility of incorporating attention in the feature extraction stages, which can increase the effectiveness of the few-shot model. To this end, we proposed the HELlinger-Attention Variational Feature Aggregation network (HELA-VFA), which utilized the Hellinger distance along with attention in the encoder to fulfill the aforementioned gaps. We show that our approach enables the derivation of an alternate form of the lower bound commonly presented in prior works, thus making the variational optimization feasible and be trained on the same footing in a given setting. Extensive experiments performed on four benchmarked few-shot classification datasets demonstrated the feasibility and superiority of our approach relative to the State-Of-The-Arts (SOTAs) approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gao Yu Lee",
      "Tanmoy Dam",
      "Daniel Puiu Poenar",
      "Vu N. Duong",
      "Md Meftahul Ferdaus"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Abdelreheem_ScanEnts3D_Exploiting_Phrase-to-3D-Object_Correspondences_for_Improved_Visio-Linguistic_Models_in_3D_WACV_2024_paper.html": {
    "title": "ScanEnts3D: Exploiting Phrase-to-3D-Object Correspondences for Improved Visio-Linguistic Models in 3D Scenes",
    "volume": "main",
    "abstract": "The two popular datasets ScanRefer [20] and ReferIt3D [5] connect natural language to real-world 3D scenes. In this paper, we curate a complementary dataset extending both the aforementioned ones. We associate all objects mentioned in a referential sentence with their underlying instances inside a 3D scene. In contrast, previous work did this only for a single object per sentence. Our Scan Entities in 3D (ScanEnts3D) dataset provides explicit cor- respondences between 369k objects across 84k referential sentences, covering 705 real-world scenes. We propose novel architecture modifications and losses that enable learning from this new type of data and improve the performance for both neural listening and language generation. For neu- ral listening, we improve the SoTA in both the Nr3D and ScanRefer benchmarks by 4.3% and 5.0%, respectively. For language generation, we improve the SoTA by 13.2 CIDEr points on the Nr3D benchmark. For both of these tasks, the new type of data is only used to improve training, but no additional annotations are required at inference time. Our introduced dataset is available on the project's webpage at https://scanents3d.github.io/",
    "checked": true,
    "id": "eccf53e26fd92d09eede89c5345743442bb139f1",
    "semantic_title": "scanents3d: exploiting phrase-to-3d-object correspondences for improved visio-linguistic models in 3d scenes",
    "citation_count": 11,
    "authors": [
      "Ahmed Abdelreheem",
      "Kyle Olszewski",
      "Hsin-Ying Lee",
      "Peter Wonka",
      "Panos Achlioptas"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Subramanya_A_Closer_Look_at_Robustness_of_Vision_Transformers_to_Backdoor_WACV_2024_paper.html": {
    "title": "A Closer Look at Robustness of Vision Transformers to Backdoor Attacks",
    "volume": "main",
    "abstract": "Transformer architectures are based on self-attention mechanism that processes images as a sequence of patches. As their design is quite different compared to CNNs, it is important to take a closer look at their vulnerability to backdoor attacks and how different transformer architectures affect robustness. Backdoor attacks happen when an attacker poisons a small part of the training images with a specific trigger or backdoor which will be activated later. The model performance is good on clean test images, but the attacker can manipulate the decision of the model by showing the trigger on an image at test time. In this paper, we compare state-of-the-art architectures through the lens of backdoor attacks, specifically how attention mechanisms affect robustness. We observe that the well known vision transformer architecture (ViT) is the least robust architecture and ResMLP, which belongs to a class called Feed Forward Networks (FFN), is most robust to backdoor attacks among state-of-the-art architectures. We also find an intriguing difference between transformers and CNNs -- interpretation algorithms effectively highlight the trigger on test images for transformers but not for CNNs. Based on this observation, we find that a test-time image blocking defense reduces the attack success rate by a large margin for transformers. We also show that such blocking mechanisms can be incorporated during the training process to improve robustness even further. We believe our experimental findings will encourage the community to understand the building block components in developing novel architectures robust to backdoor attacks. Code is available here:https://github.com/UCDvision/backdoor_transformer.git",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshayvarun Subramanya",
      "Soroush Abbasi Koohpayegani",
      "Aniruddha Saha",
      "Ajinkya Tejankar",
      "Hamed Pirsiavash"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Reich_Differentiable_JPEG_The_Devil_Is_in_the_Details_WACV_2024_paper.html": {
    "title": "Differentiable JPEG: The Devil Is in the Details",
    "volume": "main",
    "abstract": "JPEG remains one of the most widespread lossy image coding methods. However, the non-differentiable nature of JPEG restricts the application in deep learning pipelines. Several differentiable approximations of JPEG have recently been proposed to address this issue. This paper conducts a comprehensive review of existing diff. JPEG approaches and identifies critical details that have been missed by previous methods. To this end, we propose a novel diff. JPEG approach, overcoming previous limitations. Our approach is differentiable w.r.t. the input image, the JPEG quality, the quantization tables, and the color conversion parameters. We evaluate the forward and backward performance of our diff. JPEG approach against existing methods. Additionally, extensive ablations are performed to evaluate crucial design choices. Our proposed diff. JPEG resembles the (non-diff.) reference implementation best, significantly surpassing the recent-best diff. approach by 3.47dB (PSNR) on average. For strong compression rates, we can even improve PSNR by 9.51dB. Strong adversarial attack results are yielded by our diff. JPEG, demonstrating the effective gradient approximation. Our code is available at https://github.com/necla-ml/Diff-JPEG",
    "checked": true,
    "id": "ecd8f1ebfb427e61469d969e76aee7a43fda0889",
    "semantic_title": "differentiable jpeg: the devil is in the details",
    "citation_count": 0,
    "authors": [
      "Christoph Reich",
      "Biplob Debnath",
      "Deep Patel",
      "Srimat Chakradhar"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wysoczanska_CLIP-DIY_CLIP_Dense_Inference_Yields_Open-Vocabulary_Semantic_Segmentation_For-Free_WACV_2024_paper.html": {
    "title": "CLIP-DIY: CLIP Dense Inference Yields Open-Vocabulary Semantic Segmentation For-Free",
    "volume": "main",
    "abstract": "The emergence of CLIP has opened the way for open-world image perception. The zero-shot classification capabilities of the model are impressive but are harder to use for dense tasks such as image segmentation. Several methods have proposed different modifications and learning schemes to produce dense output. Instead, we propose in this work an open-vocabulary semantic segmentation method, dubbed CLIP-DIY, which does not require any additional training or annotations, but instead leverages existing unsupervised object localization approaches. In particular, CLIP-DIY is a multi-scale approach that directly exploits CLIP classification abilities on patches of different sizes and aggregates the decision in a single map. We further guide the segmentation using foreground/background scores obtained using unsupervised object localization methods. With our method, we obtain state-of-the-art zero-shot semantic segmentation results on PASCAL VOC and perform on par with the best methods on COCO",
    "checked": true,
    "id": "22d19c592ff3cc41f48a92ad9dbf743caba5b6c3",
    "semantic_title": "clip-diy: clip dense inference yields open-vocabulary semantic segmentation for-free",
    "citation_count": 2,
    "authors": [
      "Monika Wysoczańska",
      "Michaël Ramamonjisoa",
      "Tomasz Trzciński",
      "Oriane Siméoni"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Choi_Dual_Domain_Diffusion_Guidance_for_3D_CBCT_Metal_Artifact_Reduction_WACV_2024_paper.html": {
    "title": "Dual Domain Diffusion Guidance for 3D CBCT Metal Artifact Reduction",
    "volume": "main",
    "abstract": "Previous methods to solve the problem of metal artifact reduction (MAR) have mostly focused on 2D MAR, making it challenging to apply to problems with 3-dimensional CT such as CBCT. In this paper, we propose a novel approach for 3D MAR which utilizes two diffusion models to model the metal-free CBCT prior and metal artifact prior. Through dual-domain guidance in the image and projection domains, the 3D connectivity is enhanced in the restored images. Moreover, we propose a memory-efficient technique for an efficient sampling of 3-dimensional data, which reduces the memory usage by orders of magnitude. Experiments show that our method achieves the state-of-the-art performance not only with synthetic data but also with real-world clinical and out-of-distribution data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongjin Choi",
      "Doeyoung Kwon",
      "Seung Jun Baek"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/McGriff_Joint_3D_Shape_and_Motion_Estimation_From_Rolling_Shutter_Light-Field_WACV_2024_paper.html": {
    "title": "Joint 3D Shape and Motion Estimation From Rolling Shutter Light-Field Images",
    "volume": "main",
    "abstract": "In this paper, we propose an approach to address the problem of 3D reconstruction of scenes from a single image captured by a light-field camera equipped with a rolling shutter sensor. Our method leverages the 3D information cues present in the light-field and the motion information provided by the rolling shutter effect. We present a generic model for the imaging process of this sensor and a two-stage algorithm that minimizes the re-projection error while considering the position and motion of the camera in a motion-shape bundle adjustment estimation strategy. Thereby, we provide an instantaneous 3D shape-and-pose-and-velocity sensing paradigm. To the best of our knowledge, this is the first study to leverage this type of sensor for this purpose. We also present a new benchmark dataset composed of different light-fields showing rolling shutter effects, which can be used as a common base to improve the evaluation and tracking the progress in the field. We demonstrate the effectiveness and advantages of our approach through several experiments conducted for different scenes and types of motions. The source code and dataset are publicly available at: https://github.com/ICB-Vision-AI/RSLF",
    "checked": true,
    "id": "45ee3c11bf57048c4121fb5bf860bd1da05a902e",
    "semantic_title": "joint 3d shape and motion estimation from rolling shutter light-field images",
    "citation_count": 0,
    "authors": [
      "Hermès McGriff",
      "Renato Martins",
      "Nicolas Andreff",
      "Cédric Demonceaux"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Purohit_ConeQuest_A_Benchmark_for_Cone_Segmentation_on_Mars_WACV_2024_paper.html": {
    "title": "ConeQuest: A Benchmark for Cone Segmentation on Mars",
    "volume": "main",
    "abstract": "Over the years, space scientists have collected terabytes of Mars data from satellites and rovers. One important set of features identified in Mars orbital images is pitted cones, which are interpreted to be mud volcanoes believed to form in regions that were once saturated in water (i.e., a lake or ocean). Identifying pitted cones globally on Mars would be of great importance, but expert geologists are unable to sort through the massive orbital image archives to identify all examples. However, this task is well suited for computer vision. Although several computer vision datasets exist for various Mars-related tasks, there is currently no open-source dataset available for cone detection/segmentation. Furthermore, previous studies trained models using data from a single region, which limits their applicability for global detection and mapping. Motivated by this, we introduce ConeQuest, the first expert-annotated public dataset to identify cones on Mars. ConeQuest consists of >13k samples from 3 different regions of Mars. We propose two benchmark tasks using ConeQuest: (i) Spatial Generalization and (ii) Cone-size Generalization. We finetune and evaluate widely-used segmentation models on both benchmark tasks. Results indicate that cone segmentation is a challenging open problem not solved by existing segmentation models, which achieve an average IoU of 52.52% and 42.55% on in-distribution data for tasks (i) and (ii), respectively. We believe this new benchmark dataset will facilitate the development of more accurate and robust models for cone segmentation. Data and code are available at https://github.com/kerner-lab/ConeQuest",
    "checked": true,
    "id": "61d0307974cfee21bde6da31d0d524f1a4b5c17e",
    "semantic_title": "conequest: a benchmark for cone segmentation on mars",
    "citation_count": 0,
    "authors": [
      "Mirali Purohit",
      "Jacob Adler",
      "Hannah Kerner"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Doshi_A_Multimodal_Benchmark_and_Improved_Architecture_for_Zero_Shot_Learning_WACV_2024_paper.html": {
    "title": "A Multimodal Benchmark and Improved Architecture for Zero Shot Learning",
    "volume": "main",
    "abstract": "In this work, we demonstrate that due to the inadequacies in the existing evaluation protocols and datasets, there is a need to revisit and comprehensively examine the multimodal Zero-Shot Learning (MZSL) problem formulation. Specifically, we address two major challenges faced by current MZSL approaches; (1) Established baselines are frequently incomparable and occasionally even flawed since existing evaluation datasets often have some overlap with the training dataset, thus violating the zero-shot paradigm; (2) Most existing methods are biased towards seen classes, which significantly reduces the performance when evaluated on both seen and unseen classes. To address these challenges, we first introduce a new multimodal dataset for zero-shot evaluation called MZSL-50 with 4462 videos from 50 widely diversified classes and no overlap with the training data. Further, we propose a novel multimodal zeroshot transformer (MZST) architecture that leverages attention bottlenecks for multimodal fusion. Our model directly predicts the semantic representation and is superior at reducing the bias towards seen classes. We conduct extensive ablation studies, and achieve state-of-the-art results on three benchmark datasets and our novel MZSL-50 dataset. Specifically, we improve the conventional MZSL performance by a margin of 2.1%, 9.81% and 8.68% on VGGSound, UCF-101 and ActivityNet, respectively. Finally, we expect the introduction of the MZSL-50 dataset will promote the future in-depth research on multimodal zero-shot learning in the community",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keval Doshi",
      "Amanmeet Garg",
      "Burak Uzkent",
      "Xiaolong Wang",
      "Mohamed Omar"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Lopes_PlantPlotGAN_A_Physics-Informed_Generative_Adversarial_Network_for_Plant_Disease_Prediction_WACV_2024_paper.html": {
    "title": "PlantPlotGAN: A Physics-Informed Generative Adversarial Network for Plant Disease Prediction",
    "volume": "main",
    "abstract": "Monitoring plantations is crucial for crop management and producing healthy harvests. Unmanned Aerial Vehicles (UAVs) have been used to collect multispectral images that aid in this monitoring. However, given the number of hectares to be monitored and the limitations of flight, plant disease signals become visually clear only in the later stages of plant growth and only if the disease has spread throughout a significant portion of the plantation. This limited amount of relevant data hampers the prediction models, as the algorithms struggle to generalize patterns with unbalanced or unrealistic augmented datasets effectively. To address this issue, we propose PlantPlotGAN, a physics-informed generative model capable of reproducing synthetic multispectral plot images with realistic vegetation indices. These indices served as a proxy for early disease detection and were used to evaluate if our model could help increase the accuracy of prediction models. The results demonstrate that the synthetic imagery generated from PlantPlotGAN outperforms state-of-the-art methods regarding the Frechet inception distance. Moreover, prediction models achieve higher accuracy metrics when trained with synthetic and original imagery for earlier plant disease detection compared to the training processes based solely on real imagery",
    "checked": true,
    "id": "8b9a87a166b3b03e21c66c52906f9686e8382991",
    "semantic_title": "plantplotgan: a physics-informed generative adversarial network for plant disease prediction",
    "citation_count": 0,
    "authors": [
      "Felipe A. Lopes",
      "Vasit Sagan",
      "Flavio Esposito"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Lin_Common_Diffusion_Noise_Schedules_and_Sample_Steps_Are_Flawed_WACV_2024_paper.html": {
    "title": "Common Diffusion Noise Schedules and Sample Steps Are Flawed",
    "volume": "main",
    "abstract": "We discover that common diffusion noise schedules do not enforce the last timestep to have zero signal-to-noise ratio (SNR), and some implementations of diffusion samplers do not start from the last timestep. Such designs are flawed and do not reflect the fact that the model is given pure Gaussian noise at inference, creating a discrepancy between training and inference. We show that the flawed design causes real problems in existing implementations. In Stable Diffusion, it severely limits the model to only generate images with medium brightness and prevents it from generating very bright and dark samples. We propose a few simple fixes: (1) rescale the noise schedule to enforce zero terminal SNR; (2) train the model with v prediction; (3) change the sampler to always start from the last timestep; (4) rescale classifier-free guidance to prevent over-exposure. These simple changes ensure the diffusion process is congruent between training and inference and allow the model to generate samples more faithful to the original data distribution",
    "checked": true,
    "id": "5003fdf35af631d4cb17fd3c1ce2469f665064f1",
    "semantic_title": "common diffusion noise schedules and sample steps are flawed",
    "citation_count": 40,
    "authors": [
      "Shanchuan Lin",
      "Bingchen Liu",
      "Jiashi Li",
      "Xiao Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Roy_Efficient_Expansion_and_Gradient_Based_Task_Inference_for_Replay_Free_WACV_2024_paper.html": {
    "title": "Efficient Expansion and Gradient Based Task Inference for Replay Free Incremental Learning",
    "volume": "main",
    "abstract": "This paper proposes a simple but highly efficient expansion-based model for continual learning. The recent feature transformation, masking and factorization-based methods are efficient, but they grow the model only over the global or shared parameter. Therefore, these approaches do not fully utilize the previously learned information because the same task-specific parameter forgets the earlier knowledge. Thus, these approaches show limited transfer learning ability. Moreover, most of these models have constant parameter growth for all tasks, irrespective of the task complexity. Our work proposes a simple filter and channel expansion-based method that grows the model over the previous task parameters and not just over the global parameter. Therefore, it fully utilizes all the previously learned information without forgetting, which results in better knowledge transfer. The growth rate in our proposed model is a function of task complexity; therefore for a simple task, the model has a smaller parameter growth, while for complex tasks, the model requires more parameters to adapt to the current task. Recent expansion-based models show promising results for task incremental learning (TIL). However, for class incremental learning (CIL), prediction of task id is a crucial challenge; hence, their results degrade rapidly as the number of tasks increase. In this work, we propose a robust task prediction method that leverages entropy weighted data augmentations and the model's gradient using pseudo labels. We evaluate our model on various datasets and architectures in the TIL, CIL and generative continual learning settings. The proposed approach shows state-of-the-art results in all these settings. Our extensive ablation studies show the efficacy of the proposed components",
    "checked": true,
    "id": "372f38a266df2235fd39f2be03408060fd7e361e",
    "semantic_title": "efficient expansion and gradient based task inference for replay free incremental learning",
    "citation_count": 0,
    "authors": [
      "Soumya Roy",
      "Vinay Verma",
      "Deepak Gupta"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Yang_PolyMaX_General_Dense_Prediction_With_Mask_Transformer_WACV_2024_paper.html": {
    "title": "PolyMaX: General Dense Prediction With Mask Transformer",
    "volume": "main",
    "abstract": "Dense prediction tasks, such as semantic segmentation, depth estimation, and surface normal prediction, can be easily formulated as per-pixel classification (discrete outputs) or regression (continuous outputs). This per-pixel prediction paradigm has remained popular due to the prevalence of fully convolutional networks. However, on the recent frontier of segmentation task, the community has been witnessing a shift of paradigm from per-pixel prediction to cluster-prediction with the emergence of transformer architectures, particularly the mask transformers, which directly predicts a label for a mask instead of a pixel. Despite this shift, methods based on the per-pixel prediction paradigm still dominate the benchmarks on the other dense prediction tasks that require continuous outputs, such as depth estimation and surface normal prediction. Motivated by the success of DORN and AdaBins in depth estimation, achieved by discretizing the continuous output space, we propose to generalize the cluster-prediction based method to general dense prediction tasks. This allows us to unify dense prediction tasks with the mask transformer framework. Remarkably, the resulting model PolyMaX demonstrates state-of-the-art performance on three benchmarks of NYUD-v2 dataset. We hope our simple yet effective design can inspire more research on exploiting mask transformers for more dense prediction tasks. Code and model will be made available",
    "checked": true,
    "id": "21a001954cc3b620a7db2a0f29c85d99732bc608",
    "semantic_title": "polymax: general dense prediction with mask transformer",
    "citation_count": 1,
    "authors": [
      "Xuan Yang",
      "Liangzhe Yuan",
      "Kimberly Wilber",
      "Astuti Sharma",
      "Xiuye Gu",
      "Siyuan Qiao",
      "Stephanie Debats",
      "Huisheng Wang",
      "Hartwig Adam",
      "Mikhail Sirotenko",
      "Liang-Chieh Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Weiherer_Approximating_Intersections_and_Differences_Between_Linear_Statistical_Shape_Models_Using_WACV_2024_paper.html": {
    "title": "Approximating Intersections and Differences Between Linear Statistical Shape Models Using Markov Chain Monte Carlo",
    "volume": "main",
    "abstract": "To date, the comparison of Statistical Shape Models (SSMs) is often solely performance-based, carried out by means of simplistic metrics such as compactness, generalization, or specificity. Any similarities or differences between the actual shape spaces can neither be visualized nor quantified. In this paper, we present a new method to qualitatively compare two linear SSMs in dense correspondence by computing approximate intersection spaces and set-theoretic differences between the (hyper-ellipsoidal) allowable shape domains spanned by the models. To this end, we approximate the distribution of shapes lying in the intersection space using Markov chain Monte Carlo and subsequently apply Principal Component Analysis (PCA) to the posterior samples, eventually yielding a new SSM of the intersection space. We estimate differences between linear SSMs in a similar manner; here, however, the resulting spaces are no longer convex and we do not apply PCA but instead use the posterior samples for visualization. We showcase the proposed algorithm qualitatively by computing and analyzing intersection spaces and differences between publicly available face models, focusing on gender-specific male and female as well as identity and expression models. Our quantitative evaluation based on SSMs built from synthetic and real-world data sets provides detailed evidence that the introduced method is able to recover ground-truth intersection spaces and differences accurately",
    "checked": false,
    "id": "eaeb7a5b7fe2feb729c8350dcafa549641d67d7b",
    "semantic_title": "approximating intersections and differences between statistical shape models",
    "citation_count": 0,
    "authors": [
      "Maximilian Weiherer",
      "Finn Klein",
      "Bernhard Egger"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Shi_Few-Shot_Shape_Recognition_by_Learning_Deep_Shape-Aware_Features_WACV_2024_paper.html": {
    "title": "Few-Shot Shape Recognition by Learning Deep Shape-Aware Features",
    "volume": "main",
    "abstract": "Traditional shape descriptors have been gradually replaced by convolutional neural networks due to their superior performance in feature extraction and classification. The state-of-the-art methods recognize object shapes via image reconstruction or pixel classification. However, these methods are biased toward texture information and overlook the essential shape descriptions, thus, they fail to generalize to unseen shapes. We are the first to propose a few-shot shape descriptor (FSSD) to recognize object shapes given only one or a few samples. We employ an embedding module for FSSD to extract transformation-invariant shape features. Secondly, we develop a dual attention mechanism to decompose and reconstruct the shape features via learnable shape primitives. In this way, any shape can be formed through a finite set basis, and the learned representation model is highly interpretable and extendable to unseen shapes. Thirdly, we propose a decoding module to include the supervision of shape masks and edges and align the original and reconstructed shape features, enforcing the learned features to be more shape-aware. Lastly, all the proposed modules are assembled into a few-shot shape recognition scheme. Experiments on five datasets show that our FSSD significantly improves the shape classification compared to the state-of-the-art under the few-shot setting",
    "checked": true,
    "id": "ded2e5fb4c208ed4c4bce181a94fa05fb8d4b63a",
    "semantic_title": "few-shot shape recognition by learning deep shape-aware features",
    "citation_count": 0,
    "authors": [
      "Wenlong Shi",
      "Changsheng Lu",
      "Ming Shao",
      "Yinjie Zhang",
      "Siyu Xia",
      "Piotr Koniusz"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kolbeinsson_Multi-Class_Segmentation_From_Aerial_Views_Using_Recursive_Noise_Diffusion_WACV_2024_paper.html": {
    "title": "Multi-Class Segmentation From Aerial Views Using Recursive Noise Diffusion",
    "volume": "main",
    "abstract": "Semantic segmentation from aerial views is a crucial task for autonomous drones, as they rely on precise and accurate segmentation to navigate safely and efficiently. However, aerial images present unique challenges such as diverse viewpoints, extreme scale variations, and high scene complexity. In this paper, we propose an end-to-end multi-class semantic segmentation diffusion model that addresses these challenges. We introduce recursive denoising to allow information to propagate through the denoising process, as well as a hierarchical multi-scale approach that complements the diffusion process. Our method achieves promising results on the UAVid dataset and state-of-the-art performance on the Vaihingen Building segmentation benchmark. Being the first iteration of this method, it shows great promise for future improvements. Our code and models are available at: https://github.com/benediktkol/recursive-noise-diffusion",
    "checked": true,
    "id": "97bc252a539401eac3b9f07f848e7ba8c45ee20f",
    "semantic_title": "multi-class segmentation from aerial views using recursive noise diffusion",
    "citation_count": 6,
    "authors": [
      "Benedikt Kolbeinsson",
      "Krystian Mikolajczyk"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Aung_Enhancing_Multi-View_Pedestrian_Detection_Through_Generalized_3D_Feature_Pulling_WACV_2024_paper.html": {
    "title": "Enhancing Multi-View Pedestrian Detection Through Generalized 3D Feature Pulling",
    "volume": "main",
    "abstract": "The main challenge in multi-view pedestrian detection is integrating view-specific features into a unified space for comprehensive end-to-end perception. Prior multi-view detection methods have focused on projecting perspective-view features onto the ground plane, creating a \"bird's eye view\" (BEV) representation of the scene. This paper proposes a simple but effective architecture that utilizes a non-parametric 3D feature-pulling strategy. This strategy directly extracts the corresponding 2D features for each valid voxel within the 3D feature volume, addressing the feature loss that may arise in previous methods. The proposed framework introduces three novel modules, each crafted to bolster the generalization capabilities of multi-view detection systems. Through extensive experiments, the efficacy of the proposed model is demonstrated. The results show a new state-of-the-art accuracy, both in conventional scenarios and particularly in the context of scene generalization benchmarks",
    "checked": false,
    "id": "1c41f4d55b05db05e8100e7907f9305201ef0ad8",
    "semantic_title": "anyview: generalizable indoor 3d object detection with variable frames",
    "citation_count": 1,
    "authors": [
      "Sithu Aung",
      "Haesol Park",
      "Hyungjoo Jung",
      "Junghyun Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Fujii_Automated_Sperm_Assessment_Framework_and_Neural_Network_Specialized_for_Sperm_WACV_2024_paper.html": {
    "title": "Automated Sperm Assessment Framework and Neural Network Specialized for Sperm Video Recognition",
    "volume": "main",
    "abstract": "Infertility is a global health problem, and an increasing number of couples are seeking medical assistance to achieve reproduction, at least half of which are caused by men. The success rate of assisted reproductive technologies depends on sperm assessment, in which experts determine whether sperm can be used for reproduction based on morphology and motility of sperm. Previous sperm assessment studies with deep learning have used datasets comprising images that include only sperm heads, which cannot consider motility and other morphologies of sperm. Furthermore, the labels of the dataset are one-hot, which provides insufficient support for experts, because assessment results are inconsistent between experts, and they have no absolute answer. Therefore, we constructed the video dataset for sperm assessment whose videos include sperm head as well as neck and tail, and its labels were annotated with soft-label. Furthermore, we proposed the sperm assessment framework and the neural network, RoSTFine, for sperm video recognition. Experimental results showed that RoSTFine could improve the sperm assessment performances compared to existing video recognition models and focus strongly on important sperm parts (i.e., head and neck)",
    "checked": true,
    "id": "4880f54e49df36fd7d045b61f0bddcff53a70000",
    "semantic_title": "automated sperm assessment framework and neural network specialized for sperm video recognition",
    "citation_count": 0,
    "authors": [
      "Takuro Fujii",
      "Hayato Nakagawa",
      "Teppei Takeshima",
      "Yasushi Yumura",
      "Tomoki Hamagami"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Shoeb_Have_We_Ever_Encountered_This_Before_Retrieving_Out-of-Distribution_Road_Obstacles_WACV_2024_paper.html": {
    "title": "Have We Ever Encountered This Before? Retrieving Out-of-Distribution Road Obstacles From Driving Scenes",
    "volume": "main",
    "abstract": "In the life cycle of highly automated systems operating in an open and dynamic environment, the ability to adjust to emerging challenges is crucial. For systems integrating data-driven AI-based components, rapid responses to deployment issues require fast access to related data for testing and reconfiguration. In the context of automated driving, this especially applies to road obstacles that were not included in the training data, commonly referred to as out-of-distribution (OoD) road obstacles. Given the availability of large uncurated recordings of driving scenes, a pragmatic approach is to query a database to retrieve similar scenarios featuring the same safety concerns due to OoD road obstacles. In this work, we extend beyond identifying OoD road obstacles in video streams and offer a comprehensive approach to extract sequences of OoD road obstacles using text queries, thereby proposing a way of curating a collection of OoD data for subsequent analysis. Our proposed method leverages the recent advances in OoD segmentation and multi-modal foundation models to identify and efficiently extract safety-relevant scenes from unlabeled videos. We present a first approach for the novel task of text-based OoD object retrieval, which addresses the question \"Have we ever encountered this before?\"",
    "checked": true,
    "id": "3ff2461852c8b78c6abb4348ca1ffa6c3bcb3ba8",
    "semantic_title": "have we ever encountered this before? retrieving out-of-distribution road obstacles from driving scenes",
    "citation_count": 0,
    "authors": [
      "Youssef Shoeb",
      "Robin Chan",
      "Gesina Schwalbe",
      "Azarm Nowzad",
      "Fatma Güney",
      "Hanno Gottschalk"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhao_Polarimetric_PatchMatch_Multi-View_Stereo_WACV_2024_paper.html": {
    "title": "Polarimetric PatchMatch Multi-View Stereo",
    "volume": "main",
    "abstract": "PatchMatch Multi-View Stereo (PatchMatch MVS) is one of the popular MVS approaches, owing to its balanced accuracy and efficiency. In this paper, we propose Polarimetric PatchMatch multi-view Stereo (PolarPMS), which is the first method exploiting polarization cues to PatchMatch MVS. The key of PatchMatch MVS is to generate depth and normal hypotheses, which form local 3D planes and slanted stereo matching windows, and efficiently search for the best hypothesis based on the consistency among multi-view images. In addition to standard photometric consistency, our PolarPMS evaluates polarimetric consistency to assess the validness of a depth and normal hypothesis, motivated by the physical property that the polarimetric information is related to the object's surface normal. Experimental results demonstrate that our PolarPMS can improve the accuracy and the completeness of reconstructed 3D models, especially for texture-less surfaces, compared with state-of-the-art PatchMatch MVS methods",
    "checked": true,
    "id": "016bbb63cd8bd7324e2addaebe31eef66a74f7cf",
    "semantic_title": "polarimetric patchmatch multi-view stereo",
    "citation_count": 0,
    "authors": [
      "Jinyu Zhao",
      "Jumpei Oishi",
      "Yusuke Monno",
      "Masatoshi Okutomi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Jonnarth_High-Fidelity_Pseudo-Labels_for_Boosting_Weakly-Supervised_Segmentation_WACV_2024_paper.html": {
    "title": "High-Fidelity Pseudo-Labels for Boosting Weakly-Supervised Segmentation",
    "volume": "main",
    "abstract": "Image-level weakly-supervised semantic segmentation (WSSS) reduces the usually vast data annotation cost by surrogate segmentation masks during training. The typical approach involves training an image classification network using global average pooling (GAP) on convolutional feature maps. This enables the estimation of object locations based on class activation maps (CAMs), which identify the importance of image regions. The CAMs are then used to generate pseudo-labels, in the form of segmentation masks, to supervise a segmentation model in the absence of pixel-level ground truth. Our work is based on two techniques for improving CAMs; importance sampling, which is a substitute for GAP, and the feature similarity loss, which utilizes a heuristic that object contours almost always align with color edges in images. However, both are based on the multinomial posterior with softmax, and implicitly assume that classes are mutually exclusive, which turns out suboptimal in our experiments. Thus, we reformulate both techniques based on binomial posteriors of multiple independent binary problems. This has two benefits; their performance is improved and they become more general, resulting in an add-on method that can boost virtually any WSSS method. This is demonstrated on a wide variety of baselines on the PASCAL VOC dataset, improving the region similarity and contour quality of all implemented state-of-the-art methods. Experiments on the MS COCO dataset further show that our proposed add-on is well-suited for large-scale settings. Our code implementation is available at https://github.com/arvijj/hfpl",
    "checked": true,
    "id": "8d9ea15660238e54662e3e74de766619f01ec850",
    "semantic_title": "high-fidelity pseudo-labels for boosting weakly-supervised segmentation",
    "citation_count": 0,
    "authors": [
      "Arvi Jonnarth",
      "Yushan Zhang",
      "Michael Felsberg"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Yoon_Optical_Flow_Domain_Adaptation_via_Target_Style_Transfer_WACV_2024_paper.html": {
    "title": "Optical Flow Domain Adaptation via Target Style Transfer",
    "volume": "main",
    "abstract": "Optical flows play an integral role for a variety of motion-related tasks such as action recognition, object segmentation, and tracking in videos. While state-of-the-art optical flow methods heavily rely on learning, the learned optical flow methods significantly degrade when applied to different domains, and the training datasets are very limited due to the extreme cost of flow-level annotation. To tackle the issue, we introduce a domain adaptation technique for optical flow estimation. Our method extracts diverse style statistics of the target domain and use them in training to generate synthetic features from the source features, which contain the contents of the source but the style of the target. We also impose motion consistency between the synthetic target and the source and deploy adversarial learning at the flow prediction to encourage domain-invariant features. Experimental results show that the proposed method achieves substantial and consistent improvements in different domain adaptation scenarios on VKITTI 2, Sintel, and KITTI 2015 benchmarks",
    "checked": false,
    "id": "f6232c063b9598b505401fc35cd4c6c0b0776d87",
    "semantic_title": "source-free domain adaptation via target prediction distribution searching",
    "citation_count": 1,
    "authors": [
      "Jeongbeen Yoon",
      "Sanghyun Kim",
      "Suha Kwak",
      "Minsu Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Li_Controlling_Character_Motions_Without_Observable_Driving_Source_WACV_2024_paper.html": {
    "title": "Controlling Character Motions Without Observable Driving Source",
    "volume": "main",
    "abstract": "How to generate diverse, life-like, and unlimited long head/body sequences without any driving source? We argue that this under-investigated research problem is non-trivial at all, and has unique technical challenges behind it. Without semantic constraints from the driving sources, using the standard autoregressive model to generate infinitely long sequences would easily result in 1) out-of-distribution (OOD) issue due to the accumulated error, 2) insufficient diversity to produce natural and life-like motion sequences and 3) undesired periodic patterns along the time. To tackle the above challenges, we propose a systematic framework that marries the benefits of VQ-VAE and a novel token-level control policy trained with reinforcement learning using carefully designed reward functions. A high-level prior model can be easily injected on top to generate unlimited long and diverse sequences. Although we focus on no driving sources now, our framework can be generalized for controlled synthesis with explicit driving sources. Through comprehensive evaluations, we conclude that our proposed framework can address all the above-mentioned challenges and outperform other strong baselines very significantly",
    "checked": true,
    "id": "97622dbadf8bcb03ae2dc735be995f11490b97ef",
    "semantic_title": "controlling character motions without observable driving source",
    "citation_count": 0,
    "authors": [
      "Weiyuan Li",
      "Bin Dai",
      "Ziyi Zhou",
      "Qi Yao",
      "Baoyuan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Vellenga_Evaluation_of_Video_Masked_Autoencoders_Performance_and_Uncertainty_Estimations_for_WACV_2024_paper.html": {
    "title": "Evaluation of Video Masked Autoencoders' Performance and Uncertainty Estimations for Driver Action and Intention Recognition",
    "volume": "main",
    "abstract": "Traffic fatalities remain among the leading death causes worldwide. To reduce this figure, car safety is listed as one of the most important factors. To actively support human drivers, it is essential for advanced driving assistance systems to be able to recognize the driver's actions and intentions. Prior studies have demonstrated various approaches to recognize driving actions and intentions based on in-cabin and external video footage. Given the performance of self-supervised video pre-trained (SSVP) Video Masked Autoencoders (VMAEs) on multiple action recognition datasets, we evaluate the performance of SSVP VMAEs on the Honda Research Institute Driving Dataset for driver action recognition (DAR) and on the Brain4Cars dataset for driver intention recognition (DIR). Besides the performance, the application of an artificial intelligence system in a safety-critical environment must be capable to express when it is uncertain about the produced results. Therefore, we also analyze uncertainty estimations produced by a Bayes-by-Backprop last-layer (BBB-LL) and Monte-Carlo (MC) dropout variants of an VMAE. Our experiments show that an VMAE achieves a higher overall performance for both offline DAR and end-to-end DIR compared to the state-of-the-art. The analysis of the BBB-LL and MC dropout models show higher uncertainty estimates for incorrectly classified test instances compared to correctly predicted test instances",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Koen Vellenga",
      "H. Joe Steinhauer",
      "Göran Falkman",
      "Tomas Björklund"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Elata_Nested_Diffusion_Processes_for_Anytime_Image_Generation_WACV_2024_paper.html": {
    "title": "Nested Diffusion Processes for Anytime Image Generation",
    "volume": "main",
    "abstract": "Diffusion models are the current state-of-the-art in image generation, synthesizing high-quality images by breaking down the generation process into many fine-grained denoising steps. Despite their good performance, diffusion models are computationally expensive, requiring many neural function evaluations (NFEs). In this work, we propose an anytime diffusion-based method that can generate viable images when stopped at arbitrary times before completion. Using existing pretrained diffusion models, we show that the generation scheme can be recomposed as two nested diffusion processes, enabling fast iterative refinement of a generated image. In experiments on ImageNet and Stable Diffusion-based text-to-image generation, we show, both qualitatively and quantitatively, that our method's intermediate generation quality greatly exceeds that of the original diffusion model, while the final generation result remains comparable. We illustrate the applicability of Nested Diffusion in several settings, including for solving inverse problems, and for rapid text-based content creation by allowing user intervention throughout the sampling process",
    "checked": true,
    "id": "cfc24ad313d67aedf5a3c90b057ef43e155d4466",
    "semantic_title": "nested diffusion processes for anytime image generation",
    "citation_count": 0,
    "authors": [
      "Noam Elata",
      "Bahjat Kawar",
      "Tomer Michaeli",
      "Michael Elad"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Venkataraman_Can_You_Even_Tell_Left_From_Right_Presenting_a_New_WACV_2024_paper.html": {
    "title": "Can You Even Tell Left From Right? Presenting a New Challenge for VQA",
    "volume": "main",
    "abstract": "Visual Question Answering (VQA) needs a means of evaluating the strengths and weaknesses of models. One aspect of such an evaluation is the measurement of compositional generalisation. This relates to the ability of a model to answer well on scenes whose compositions are different from those of scenes in the training dataset. In this work, we present several quantitative measures of compositional separation and find that popular datasets for VQA are not good compositional evaluators. To solve this, we present Uncommon Objects in Unseen Configurations (UOUC), a synthetic dataset for VQA. UOUC is at once fairly complex while also being compositionally well-separated. The object-class of UOUC consists of 380 clasess taken from 528 characters from the Dungeons and Dragons game. The training dataset of UOUC consists of 200,000 scenes; whereas the test set consists of 30,000 scenes. In order to study compositional generalisation, simple reasoning and memorisation, each scene of UOUC is annotated with up to 10 novel questions. These deal with spatial relationships, hypothetical changes to scenes, counting, comparison, memorisation and memory-based reasoning. In total, UOUC presents over 2 million questions. Our evaluation of recent high-performing models for VQA shows that they exhibit poor compositional generalisation, and comparatively lower ability towards simple reasoning. These results suggest that UOUC could lead to advances in research by being a strong benchmark for VQA, especially in the study of compositional generalisation",
    "checked": true,
    "id": "0d0c64122bdafe0541c9831b9ea1ec6a3876ed68",
    "semantic_title": "can you even tell left from right? presenting a new challenge for vqa",
    "citation_count": 0,
    "authors": [
      "Sai Raam Venkataraman",
      "Rishi Sridhar Rao",
      "S. Balasubramanian",
      "R. Raghunatha Sarma",
      "Chandra Sekhar Vorugunti"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Unal_2D_Feature_Distillation_for_Weakly-_and_Semi-Supervised_3D_Semantic_Segmentation_WACV_2024_paper.html": {
    "title": "2D Feature Distillation for Weakly- and Semi-Supervised 3D Semantic Segmentation",
    "volume": "main",
    "abstract": "As 3D perception problems grow in popularity and the need for large-scale labeled datasets for LiDAR semantic segmentation increase, new methods arise that aim to reduce the necessity for dense annotations by employing weakly-supervised training. However these methods continue to show weak boundary estimation and high false negative rates for small objects and distant sparse regions. We argue that such weaknesses can be compensated by using RGB images which provide a denser representation of the scene. We propose an image-guidance network (IGNet) which builds upon the idea of distilling high level feature information from a domain adapted synthetically trained 2D semantic segmentation network. We further utilize a one-way contrastive learning scheme alongside a novel mixing strategy called FOVMix, to combat the horizontal field-of-view mismatch between the two sensors and enhance the effects of image guidance. IGNet achieves state-of-the-art results for weakly-supervised LiDAR semantic segmentation on ScribbleKITTI, boasting up to 98% relative performance to fully supervised training with only 8% labeled points, while introducing no additional annotation burden or computational/memory cost during inference. Furthermore, we show that our contributions also prove effective for semi-supervised training, where IGNet claims state-of-the-art results on both ScribbleKITTI and SemanticKITTI",
    "checked": true,
    "id": "8d4ac11879cb372eefb8d4b53a1d26ac5e9e63ea",
    "semantic_title": "2d feature distillation for weakly- and semi-supervised 3d semantic segmentation",
    "citation_count": 0,
    "authors": [
      "Ozan Unal",
      "Dengxin Dai",
      "Lukas Hoyer",
      "Yigit Baran Can",
      "Luc Van Gool"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Krispel_MAELi_Masked_Autoencoder_for_Large-Scale_LiDAR_Point_Clouds_WACV_2024_paper.html": {
    "title": "MAELi: Masked Autoencoder for Large-Scale LiDAR Point Clouds",
    "volume": "main",
    "abstract": "The sensing process of large-scale LiDAR point clouds inevitably causes large blind spots, i.e. regions not visible to the sensor. We demonstrate how these inherent sampling properties can be effectively utilized for self-supervised representation learning by designing a highly effective pre-training framework that considerably reduces the need for tedious 3D annotations to train state-of-the-art object detectors. Our Masked AutoEncoder for LiDAR point clouds (MAELi) intuitively leverages the sparsity of LiDAR point clouds in both the encoder and decoder during reconstruction. This results in more expressive and useful initialization, which can be directly applied to downstream perception tasks, such as 3D object detection or semantic segmentation for autonomous driving. In a novel reconstruction approach, MAELi distinguishes between empty and occluded space and employs a new masking strategy that targets the LiDAR's inherent spherical projection. Thereby, without any ground truth whatsoever and trained on single frames only, MAELi obtains an understanding of the underlying 3D scene geometry and semantics. To demonstrate the potential of MAELi, we pre-train backbones in an end-to-end manner and show the effectiveness of our unsupervised pre-trained weights on the tasks of 3D object detection and semantic segmentation",
    "checked": true,
    "id": "d102e7d6515ae066c72789950b6a5ea3c52cc51b",
    "semantic_title": "maeli: masked autoencoder for large-scale lidar point clouds",
    "citation_count": 5,
    "authors": [
      "Georg Krispel",
      "David Schinagl",
      "Christian Fruhwirth-Reisinger",
      "Horst Possegger",
      "Horst Bischof"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Lai_Empowering_Unsupervised_Domain_Adaptation_With_Large-Scale_Pre-Trained_Vision-Language_Models_WACV_2024_paper.html": {
    "title": "Empowering Unsupervised Domain Adaptation With Large-Scale Pre-Trained Vision-Language Models",
    "volume": "main",
    "abstract": "Unsupervised Domain Adaptation (UDA) aims to leverage the labeled source domain to solve the tasks on the unlabeled target domain. Traditional UDA methods face the challenge of the tradeoff between domain alignment and semantic class discriminability, especially when a large domain gap exists between the source and target domain. The efforts of applying large-scale pre-training to bridge the domain gaps remain limited. In this work, we propose that Vision-Language Models (VLMs) can empower UDA tasks due to their training pattern with language alignment and their large-scale pre-trained datasets. For example, CLIP and GLIP have shown promising zero-shot generalization in classification and detection tasks. However, directly fine-tuning these VLMs into downstream tasks may be computationally expensive and not scalable if we have multiple domains that need to be adapted. Therefore, in this work, we first study an efficient adaption of VLMs to preserve the original knowledge while maximizing its flexibility for learning new knowledge. Then, we design a domain-aware pseudo-labeling scheme tailored to VLMs for domain disentanglement. We show the superiority of the proposed methods in four UDA-classification and two UDA-detection benchmarks, with a significant improvement (+9.9%) on DomainNet",
    "checked": false,
    "id": "84604d1a41c72299ef292f5560f096a4a1cb2a43",
    "semantic_title": "unsupervised prototype adapter for vision-language models",
    "citation_count": 2,
    "authors": [
      "Zhengfeng Lai",
      "Haoping Bai",
      "Haotian Zhang",
      "Xianzhi Du",
      "Jiulong Shan",
      "Yinfei Yang",
      "Chen-Nee Chuah",
      "Meng Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wang_FreMIM_Fourier_Transform_Meets_Masked_Image_Modeling_for_Medical_Image_WACV_2024_paper.html": {
    "title": "FreMIM: Fourier Transform Meets Masked Image Modeling for Medical Image Segmentation",
    "volume": "main",
    "abstract": "The research community has witnessed the powerful potential of self-supervised Masked Image Modeling (MIM), which enables the models capable of learning visual representation from unlabeled data. In this paper, to incorporate both the crucial global structural information and local details for dense prediction tasks, we alter the perspective to the frequency domain and present a new MIM-based framework named FreMIM for self-supervised pre-training to better accomplish medical image segmentation tasks. Based on the observations that the detailed structural information mainly lies in the high-frequency components and the high-level semantics are abundant in the low-frequency counterparts, we further incorporate multi-stage supervision to guide the representation learning during the pre-training phase. Extensive experiments on three benchmark datasets show the superior advantage of our FreMIM over previous state-of-the-art MIM methods. Compared with various baselines trained from scratch, our FreMIM could consistently bring considerable improvements to model performance. The code will be made publicly available at https://github.com/jingw193/FreMIM",
    "checked": true,
    "id": "bf1b3027435d53cb368735022ed461cd6ac22474",
    "semantic_title": "fremim: fourier transform meets masked image modeling for medical image segmentation",
    "citation_count": 0,
    "authors": [
      "Wenxuan Wang",
      "Jing Wang",
      "Chen Chen",
      "Jianbo Jiao",
      "Yuanxiu Cai",
      "Shanshan Song",
      "Jiangyun Li"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Hisadome_Rotation-Constrained_Cross-View_Feature_Fusion_for_Multi-View_Appearance-Based_Gaze_Estimation_WACV_2024_paper.html": {
    "title": "Rotation-Constrained Cross-View Feature Fusion for Multi-View Appearance-Based Gaze Estimation",
    "volume": "main",
    "abstract": "Appearance-based gaze estimation has been actively studied in recent years. However, its generalization performance for unseen head poses is still a significant limitation for existing methods. This work proposes a generalizable multi-view gaze estimation task and a cross-view feature fusion method to address this issue. In addition to paired images, our method takes the relative rotation matrix between two cameras as additional input. The proposed network learns to extract rotatable feature representation by using relative rotation as a constraint and adaptively fuses the rotatable features via stacked fusion modules. This simple yet efficient approach significantly improves generalization performance under unseen head poses without significantly increasing computational cost. The model can be trained with random combinations of cameras without fixing the positioning and can generalize to unseen camera pairs during inference. Through experiments using multiple datasets, we demonstrate the advantage of the proposed method over baseline methods, including state-of-the-art domain generalization approaches",
    "checked": true,
    "id": "bcdb05829314f46d467f661bffacccdaf75844bb",
    "semantic_title": "rotation-constrained cross-view feature fusion for multi-view appearance-based gaze estimation",
    "citation_count": 0,
    "authors": [
      "Yoichiro Hisadome",
      "Tianyi Wu",
      "Jiawei Qin",
      "Yusuke Sugano"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ranem_Continual_Atlas-Based_Segmentation_of_Prostate_MRI_WACV_2024_paper.html": {
    "title": "Continual Atlas-Based Segmentation of Prostate MRI",
    "volume": "main",
    "abstract": "Continual learning (CL) methods designed for natural image classification often fail to reach basic quality standards for medical image segmentation. Atlas-based segmentation, a well-established approach in medical imaging, incorporates domain knowledge on the region of interest, leading to semantically coherent predictions. This is especially promising for CL, as it allows us to leverage structural information and strike an optimal balance between model rigidity and plasticity over time. When combined with privacy-preserving prototypes, this process offers the advantages of rehearsal-based CL without compromising patient privacy. We propose Atlas Replay, an atlas-based segmentation approach that uses prototypes to generate high-quality segmentation masks through image registration that maintain consistency even as the training distribution changes. We explore how our proposed method performs compared to state-of-the-art CL methods in terms of knowledge transferability across seven publicly available prostate segmentation datasets. Prostate segmentation plays a vital role in diagnosing prostate cancer, however, it poses challenges due to substantial anatomical variations, benign structural differences in older age groups, and fluctuating acquisition parameters. Our results show that Atlas Replay is both robust and generalizes well to yet-unseen domains while being able to maintain knowledge, unlike end-to-end segmentation methods. Our code base is available under https://github.com/MECLabTUDA/Atlas-Replay",
    "checked": true,
    "id": "11af25c50024908fd2cdf963c1cb7993327177b6",
    "semantic_title": "continual atlas-based segmentation of prostate mri",
    "citation_count": 0,
    "authors": [
      "Amin Ranem",
      "Camila González",
      "Daniel Pinto dos Santos",
      "Andreas M. Bucher",
      "Ahmed E. Othman",
      "Anirban Mukhopadhyay"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Pepe_CGAPoseNetGCAN_A_Geometric_Clifford_Algebra_Network_for_Geometry-Aware_Camera_Pose_WACV_2024_paper.html": {
    "title": "CGAPoseNet+GCAN: A Geometric Clifford Algebra Network for Geometry-Aware Camera Pose Regression",
    "volume": "main",
    "abstract": "We introduce CGAPoseNet+GCAN, which enhances CGAPoseNet, an architecture for camera pose regression, with a Geometric Clifford Algebra Network (GCAN). With the addition of the GCAN we obtain a geometry-aware pipeline for camera pose regression from RGB images only. CGAPoseNet employs Clifford Geometric Algebra to unify quaternions and translation vectors into a single mathematical object, the motor, which can be used to uniquely describe camera poses. CGAPoseNet solves the issue of balancing rotation and translation components in the loss function, and can obtain comparable results to other approaches without the need of expensive tuning of the loss function or additional information about the scene, such as 3D point clouds, which might not always be available. CGAPoseNet, however, like several approaches in the literature, only learns to predict motor coefficients, and it is unaware of the mathematical space in which predictions sit in and of their geometrical meaning. By leveraging recent advances in Geometric Deep Learning, we modify CGAPoseNet with a GCAN: proposals of possible motor coefficients associated with a camera frame are obtained from the InceptionV3 backbone, and the GCAN downsamples them to a single motor through a sequence of layers that work in G_ 4,0 . The network is hence geometry-aware, has multivector-valued inputs, weights and biases and preserves the grade of the objects that it receives in input. CGAPoseNet+GCAN has almost 4 million fewer trainable parameters, it reduces the average rotation error by 41% and the average translation error by 8.8% compared to CGAPoseNet. Similarly, it reduces rotation and translation errors by 32.6% and 19.9%, respectively, compared to the best performing PoseNet strategy. CGAPoseNet+GCAN reaches the state-of-the-art results on 13 commonly employed datasets. To the best of our knowledge, it is the first experiment in GCANs applied to the problem of camera pose regression",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alberto Pepe",
      "Joan Lasenby",
      "Sven Buchholz"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Contextual_Affinity_Distillation_for_Image_Anomaly_Detection_WACV_2024_paper.html": {
    "title": "Contextual Affinity Distillation for Image Anomaly Detection",
    "volume": "main",
    "abstract": "Previous studies on unsupervised industrial anomaly detection mainly focus on 'structural' types of anomalies such as cracks and color contamination by matching or learning local feature representations. While achieving significantly high detection performance on this kind of anomaly, they are faced with 'logical' types of anomalies that violate the long-range dependencies such as a normal object placed in the wrong position. Noting the reverse distillation approaches that are under the encoder-decoder paradigm could learn from the high abstract level knowledge, we propose to use two students (local and global) to better mimic the teacher's local and global behavior in reverse distillation. The local student, which is used in previous studies mainly focuses on accurate local feature learning while the global student pays attention to learning global correlations. To further encourage the global student's learning to capture long-range dependencies, we design the global context condensing block (GCCB) and propose a contextual affinity loss for the student training and anomaly scoring. Experimental results show that the proposed method sets a new state-of-the-art performance on the MVTec LOCO AD dataset without using complex training techniques",
    "checked": true,
    "id": "fa5aaa7c45e4cd727226a75f5b1b8e5d33460a87",
    "semantic_title": "contextual affinity distillation for image anomaly detection",
    "citation_count": 0,
    "authors": [
      "Jie Zhang",
      "Masanori Suganuma",
      "Takayuki Okatani"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Tang_Semantic-Aware_Video_Representation_for_Few-Shot_Action_Recognition_WACV_2024_paper.html": {
    "title": "Semantic-Aware Video Representation for Few-Shot Action Recognition",
    "volume": "main",
    "abstract": "Recent work on action recognition leverages 3D features and textual information to achieve state-of-the-art performance. However, most of the current few-shot action recognition methods still rely on 2D frame-level representations, often require additional components to model temporal relations, and employ complex distance functions to achieve accurate alignment of these representations. In addition, existing methods struggle to effectively integrate textual semantics, some resorting to concatenation or addition of textual and visual features, and some using text merely as an additional supervision without truly achieving feature fusion and information transfer from different modalities. In this work, we propose a simple yet effective Semantic-Aware Few-Shot Action Recognition (SAFSAR) model to address these issues. We show that directly leveraging a 3D feature extractor combined with an effective feature-fusion scheme, and a simple cosine similarity for classification can yield better performance without the need of extra components for temporal modeling or complex distance functions. We introduce an innovative scheme to encode the textual semantics into the video representation which adaptively fuses features from text and video, and encourages the visual encoder to extract more semantically consistent features. In this scheme, SAFSAR achieves alignment and fusion in a compact way. Experiments on five challenging few-shot action recognition benchmarks under various settings demonstrate that the proposed SAFSAR model significantly improves the state-of-the-art performance",
    "checked": true,
    "id": "a50bd54f082da48582fa0dabbfea89b23c676113",
    "semantic_title": "semantic-aware video representation for few-shot action recognition",
    "citation_count": 0,
    "authors": [
      "Yutao Tang",
      "Benjamín Béjar",
      "René Vidal"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ilhan_Adaptive_Deep_Neural_Network_Inference_Optimization_With_EENet_WACV_2024_paper.html": {
    "title": "Adaptive Deep Neural Network Inference Optimization With EENet",
    "volume": "main",
    "abstract": "Well-trained deep neural networks (DNNs) treat all test samples equally during prediction. Adaptive DNN inference with early exiting leverages the observation that some test examples can be easier to predict than others. This paper presents EENet, a novel early-exiting scheduling framework for multi-exit DNN models. Instead of having every sample go through all DNN layers during prediction, EENet learns an early exit scheduler, which can intelligently terminate the inference earlier for certain predictions, which the model has high confidence of early exit. As opposed to previous early-exiting solutions with heuristics-based methods, our EENet framework optimizes an early-exiting policy to maximize model accuracy while satisfying the given per-sample average inference budget. Extensive experiments are conducted on four computer vision datasets (CIFAR-10, CIFAR-100, ImageNet, Cityscapes) and two NLP datasets (SST-2, AgNews). The results demonstrate that the adaptive inference by EENet can outperform the representative existing early exit techniques. We also perform a detailed visualization analysis of the comparison results to interpret the benefits of EENet",
    "checked": true,
    "id": "6c48f0c5e3daf5ac9f02e304984722a47b7d73b7",
    "semantic_title": "adaptive deep neural network inference optimization with eenet",
    "citation_count": 3,
    "authors": [
      "Fatih Ilhan",
      "Ka-Ho Chow",
      "Sihao Hu",
      "Tiansheng Huang",
      "Selim Tekin",
      "Wenqi Wei",
      "Yanzhao Wu",
      "Myungjin Lee",
      "Ramana Kompella",
      "Hugo Latapie",
      "Gaowen Liu",
      "Ling Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wu_MIVC_Multiple_Instance_Visual_Component_for_Visual-Language_Models_WACV_2024_paper.html": {
    "title": "MIVC: Multiple Instance Visual Component for Visual-Language Models",
    "volume": "main",
    "abstract": "Vision-language models have been widely explored across a wide range of tasks and achieve satisfactory performance. However, it's under-explored how to consolidate entity understanding through a varying number of images and to align it with the pre-trained language models for generative tasks. In this paper, we propose MIVC, a general multiple instance visual component to bridge the gap between various image inputs with off-the-shelf vision-language models by aggregating visual representations in a permutation-invariant fashion through a neural network. We show that MIVC could be plugged into the visual-language models to improve the model performance consistently on visual question answering, classification and captioning tasks on a public available e-commerce dataset with multiple images per product. Furthermore, we show that the component provides insight into the contribution of each image to the downstream tasks",
    "checked": true,
    "id": "074ab652cf539d695aabd9d5fe07c69386deb8da",
    "semantic_title": "mivc: multiple instance visual component for visual-language models",
    "citation_count": 0,
    "authors": [
      "Wenyi Wu",
      "Qi Li",
      "Wenliang Zhong",
      "Junzhou Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Hegde_Attentive_Prototypes_for_Source-Free_Unsupervised_Domain_Adaptive_3D_Object_Detection_WACV_2024_paper.html": {
    "title": "Attentive Prototypes for Source-Free Unsupervised Domain Adaptive 3D Object Detection",
    "volume": "main",
    "abstract": "3D object detection networks tend to be biased towards the data they are trained on. Evaluation on datasets captured in different locations, conditions or sensors than that of the training (source) data results in a drop in model performance due to the gap in distribution with the test (or target) data. Current methods for domain adaptation either assume access to source data during training, which may not be available due to privacy or memory concerns, or require a sequence of lidar frames as an input. We propose a single-frame approach for source-free, unsupervised domain adaptation of lidar-based 3D object detectors that uses class prototypes to mitigate the effect pseudo-label noise. Addressing the limitations of traditional feature aggregation methods for prototype computation in the presence of noisy labels, we utilize a transformer module to identify outlier ROI's that correspond to incorrect, over-confident annotations, and compute an attentive class prototype. Under an iterative training strategy, the losses associated with noisy pseudo labels are down-weighed and thus refined in the process of self-training. To validate the effectiveness of our proposed approach, we examine the domain shift associated with networks trained on large, label-rich datasets (such as the Waymo Open Dataset and nuScenes) and evaluate on smaller, label-poor datasets (such as KITTI) and vice-versa. We demonstrate our approach on two recent object detectors and achieve results that out-perform the other domain adaptation works",
    "checked": false,
    "id": "1e81575cdb59e7ddca500e405da51f06a9dee1dc",
    "semantic_title": "gpa-3d: geometry-aware prototype alignment for unsupervised domain adaptive 3d object detection from point clouds",
    "citation_count": 0,
    "authors": [
      "Deepti Hegde",
      "Vishal M. Patel"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Panev_Exploring_the_Impact_of_Rendering_Method_and_Motion_Quality_on_WACV_2024_paper.html": {
    "title": "Exploring the Impact of Rendering Method and Motion Quality on Model Performance When Using Multi-View Synthetic Data for Action Recognition",
    "volume": "main",
    "abstract": "This paper explores the use of synthetic data in a human action recognition (HAR) task to avoid the challenges of obtaining and labeling real-world datasets. We introduce a new dataset suite comprising five datasets, eleven common human activities, three synchronized camera views (aerial and ground) in three outdoor environments, and three visual domains (real and two synthetic). For the synthetic data, two rendering methods (standard computer graphics and neural rendering) and two sources of human motions (motion capture and video-based motion reconstruction) were employed. We evaluated each dataset type by training popular activity recognition models and comparing the performance on the real test data. Our results show that synthetic data achieve slightly lower accuracy (4-8%) than real data. On the other hand, a model pre-trained on synthetic data and fine-tuned on limited real data surpasses the performance of either domain alone. Standard computer graphics (CG)-rendered data delivers better performance than the data generated from the neural-based rendering method. The results suggest that the quality of the human motions in the training data also affects the test results: motion capture delivers higher test accuracy. Additionally, a model trained on CG aerial view synthetic data exhibits greater robustness against camera viewpoint changes than one trained on real data. See the project page: http://humansensinglab.github.io/REMAG/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stanislav Panev",
      "Emily Kim",
      "Sai Abhishek Si Namburu",
      "Desislava Nikolova",
      "Celso de Melo",
      "Fernando De la Torre",
      "Jessica Hodgins"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Krumpl_ATS_Adaptive_Temperature_Scaling_for_Enhancing_Out-of-Distribution_Detection_Methods_WACV_2024_paper.html": {
    "title": "ATS: Adaptive Temperature Scaling for Enhancing Out-of-Distribution Detection Methods",
    "volume": "main",
    "abstract": "Out-of-distribution (OOD) detection is essential to ensure the reliability and robustness of machine learning models in real-world applications. Post-hoc OOD detection methods have gained significant attention due to the fact that they offer the advantage of not requiring additional re-training, which could degrade model performance and increase training time. However, most existing post-hoc methods rely only on the encoder output (features), logits, or the softmax probability, meaning they have no access to information that might be lost in the feature extraction process. In this work, we address this limitation by introducing Adaptive Temperature Scaling (ATS), a novel approach that dynamically calculates a temperature value based on activations of the intermediate layers. Fusing this sample-specific adjustment with class-dependent logits, our ATS captures additional statistical information before they are lost in the feature extraction process, leading to a more robust and powerful OOD detection method. We conduct extensive experiments to demonstrate the efficacy of our approach. Notably, our method can be seamlessly combined with SOTA post-hoc OOD detection methods that rely on the logits, thereby enhancing their performance and improving their robustness",
    "checked": false,
    "id": "12f0abac80173061b22062d72d7535435ddab014",
    "semantic_title": "ats-yolov7: a real-time multi-scale object detection method for uav aerial images based on improved yolov7",
    "citation_count": 0,
    "authors": [
      "Gerhard Krumpl",
      "Henning Avenhaus",
      "Horst Possegger",
      "Horst Bischof"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kim_Exploring_Adversarial_Robustness_of_Vision_Transformers_in_the_Spectral_Perspective_WACV_2024_paper.html": {
    "title": "Exploring Adversarial Robustness of Vision Transformers in the Spectral Perspective",
    "volume": "main",
    "abstract": "The Vision Transformer has emerged as a powerful tool for image classification tasks, surpassing the performance of convolutional neural networks (CNNs). Recently, many researchers have attempted to understand the robustness of Transformers against adversarial attacks. However, previous researches have focused solely on perturbations in the spatial domain. This paper proposes an additional perspective that explores the adversarial robustness of Transformers against frequency-selective perturbations in the spectral domain. To facilitate comparison between these two domains, an attack framework is formulated as a flexible tool for implementing attacks on images in both the spatial and spectral domains. The experiments reveal that Transformers rely more on phase and low frequency information, which can render them more vulnerable to frequency-selective attacks than CNNs. This work offers new insights into the properties and adversarial robustness of Transformers",
    "checked": true,
    "id": "c8b85908dc1584cc598d7c82c604d940a9f2aa2d",
    "semantic_title": "exploring adversarial robustness of vision transformers in the spectral perspective",
    "citation_count": 2,
    "authors": [
      "Gihyun Kim",
      "Juyeop Kim",
      "Jong-Seok Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Mehraban_MotionAGFormer_Enhancing_3D_Human_Pose_Estimation_With_a_Transformer-GCNFormer_Network_WACV_2024_paper.html": {
    "title": "MotionAGFormer: Enhancing 3D Human Pose Estimation With a Transformer-GCNFormer Network",
    "volume": "main",
    "abstract": "Recent transformer-based approaches have demonstrated excellent performance in 3D human pose estimation. However, they have a holistic view and by encoding global relationships between all the joints, they do not capture the local dependencies precisely. In this paper, we present a novel Attention-GCNFormer (AGFormer) block that divides the number of channels by using two parallel transformer and GCNFormer streams. Our proposed GCNFormer module exploits the local relationship between adjacent joints, outputting a new representation that is complementary to the transformer output. By fusing these two representation in an adaptive way, AGFormer exhibits the ability to better learn the underlying 3D structure. By stacking multiple AGFormer blocks, we propose MotionAGFormer in four different variants, which can be chosen based on the speed-accuracy trade-off. We evaluate our model on two popular benchmark datasets: Human3.6M and MPI-INF-3DHP. MotionAGFormer-B achieves state-of-the-art results, with P1 errors of 38.4 mm and 16.2 mm, respectively. Remarkably, it uses a quarter of the parameters and is three times more computationally efficient than the previous leading model on Human3.6M dataset. Code and models are available at https://github.com/TaatiTeam/MotionAGFormer",
    "checked": true,
    "id": "df44b2c9e011399100e89eccc2ec80e3e89e4fd1",
    "semantic_title": "motionagformer: enhancing 3d human pose estimation with a transformer-gcnformer network",
    "citation_count": 0,
    "authors": [
      "Soroush Mehraban",
      "Vida Adeli",
      "Babak Taati"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wan_Density-Based_Flow_Mask_Integration_via_Deformable_Convolution_for_Video_People_WACV_2024_paper.html": {
    "title": "Density-Based Flow Mask Integration via Deformable Convolution for Video People Flux Estimation",
    "volume": "main",
    "abstract": "Crowd counting is currently applied in many areas, such as transportation hubs and streets. However, most of the research still focuses on counting the number of people in a single image, and there is little research on solving the problem of calculating the number of non-repeated people in a video segment. Currently, multiple object tracking is mainly relied upon for video counting, but this method is not suitable for situations where the crowd density is too high. Therefore, we propose a Flow Mask Integration Deformable Convolution network (FMDC) combined with Intra-Frame Head Contrastive Learning (IFHC) to predict the situation of people entering and exiting the screen in a density-based manner. We verify that our proposed method is highly effective in densely populated situations and diverse scenes, and the experimental results show that our proposed method surpasses existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chang-Lin Wan",
      "Feng-Kai Huang",
      "Hong-Han Shuai"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Bele_Learning_Class_and_Domain_Augmentations_for_Single-Source_Open-Domain_Generalization_WACV_2024_paper.html": {
    "title": "Learning Class and Domain Augmentations for Single-Source Open-Domain Generalization",
    "volume": "main",
    "abstract": "Single-source open-domain generalization (SS-ODG) addresses the challenge of labeled source domains with supervision during training and unlabeled novel target domains during testing. The target domain includes both known classes from the source domain and samples from previously unseen classes. Existing techniques for SS-ODG primarily focus on calibrating source-domain classifiers to identify open samples in the target domain. However, these methods struggle with visually fine-grained open-closed data, often misclassifying open samples as closed-set classes. Moreover, relying solely on a single source domain restricts the model's ability to generalize. To overcome these limitations, we propose a novel framework called SODG-NET that simultaneously synthesizes novel domains and generates pseudo-open samples using a learning-based objective, in contrast to the ad-hoc mixing strategies commonly found in the literature. Our approach enhances generalization by diversifying the styles of known class samples using a novel metric criterion and generates diverse pseudo-open samples to train a unified and confident multiclass classifier capable of handling both open and closed-set data. Extensive experimental evaluations conducted on multiple benchmarks consistently demonstrate the superior performance of SODG-NET compared to the literature",
    "checked": true,
    "id": "f2837f29323f7642dbdf8f8c547e03eb583e258c",
    "semantic_title": "learning class and domain augmentations for single-source open-domain generalization",
    "citation_count": 0,
    "authors": [
      "Prathmesh Bele",
      "Valay Bundele",
      "Avigyan Bhattacharya",
      "Ankit Jha",
      "Gemma Roig",
      "Biplab Banerjee"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Feng_RankDVQA_Deep_VQA_Based_on_Ranking-Inspired_Hybrid_Training_WACV_2024_paper.html": {
    "title": "RankDVQA: Deep VQA Based on Ranking-Inspired Hybrid Training",
    "volume": "main",
    "abstract": "In recent years, deep learning techniques have shown significant potential for improving video quality assessment (VQA), achieving higher correlation with subjective opinions compared to conventional approaches. However, the development of deep VQA methods has been constrained by the limited availability of large-scale training databases and ineffective training methodologies. As a result, it is difficult for deep VQA approaches to achieve consistently superior performance and model generalization. In this context, this paper proposes new VQA methods based on a two-stage training methodology which motivates us to develop a large-scale VQA training database without employing human subjects to provide ground truth labels. This method was used to train a new transformer-based network architecture, exploiting quality ranking of different distorted sequences rather than minimizing the difference from the ground-truth quality labels. The resulting deep VQA methods (for both full reference and no reference scenarios), FR- and NR-RankDVQA, exhibit consistently higher correlation with perceptual quality compared to the state-of-the-art conventional and deep VQA methods, with average SROCC values of 0.8972 (FR) and 0.7791 (NR) over eight test sets without performing cross-validation. The source code of the proposed quality metrics and the large training database are available at https://chenfeng-bristol.github.io/RankDVQA",
    "checked": true,
    "id": "3c67a0721e27433584bdc3cd261f18db006498b6",
    "semantic_title": "rankdvqa: deep vqa based on ranking-inspired hybrid training",
    "citation_count": 4,
    "authors": [
      "Chen Feng",
      "Duolikun Danier",
      "Fan Zhang",
      "David Bull"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Reynolds_Salient_Object_Detection_for_Images_Taken_by_People_With_Vision_WACV_2024_paper.html": {
    "title": "Salient Object Detection for Images Taken by People With Vision Impairments",
    "volume": "main",
    "abstract": "Salient object detection is the task of producing a binary mask for an image that deciphers which pixels belong to the foreground object versus background. We introduce a new salient object detection dataset using images taken by people who are visually impaired who were seeking to better understand their surroundings, which we call VizWiz-SalientObject. Compared to seven existing datasets, VizWiz-SalientObject is the largest (i.e., 32,000 human-annotated images) and contains unique characteristics including a higher prevalence of text in the salient objects (i.e., in 68% of images) and salient objects that occupy a larger ratio of the images (i.e., on average, 50% coverage). We benchmarked ten modern models on our dataset. While most methods fall below human performance, struggling most for images with salient objects that are large, have less complex boundaries, and lack text as well as for lower quality images, one method one method is very close. To facilitate future extensions of this work, we publicly share the dataset at https://vizwiz.org/tasks-and-datasets/salient-object-detection",
    "checked": true,
    "id": "d17a2f6dfea0e83e33492ba32b816cbfb636209f",
    "semantic_title": "salient object detection for images taken by people with vision impairments",
    "citation_count": 2,
    "authors": [
      "Jarek Reynolds",
      "Chandra Kanth Nagesh",
      "Danna Gurari"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wu_HD-Fusion_Detailed_Text-to-3D_Generation_Leveraging_Multiple_Noise_Estimation_WACV_2024_paper.html": {
    "title": "HD-Fusion: Detailed Text-to-3D Generation Leveraging Multiple Noise Estimation",
    "volume": "main",
    "abstract": "In this paper, we study Text-to-3D content generation leveraging 2D diffusion priors to enhance the quality and detail of the generated 3D models. Recent progresses in text-to-3D have shown that employing high-resolution (e.g., 512 x 512) renderings can lead to the production of high-quality 3D models using latent diffusion priors. To enable rendering at even higher resolutions, which has the poten tial to further augment the quality and detail of the models, we propose a novel approach that combines multiple noise estimation processes with a pretrained diffusion prior. Distinct from the Bar-Tal et al.s' study which binds multiple denoised results [1] to generate images from texts, our approach integrates the computation of scoring distillation losses such as SDS loss and VSD loss which are essential techniques for the 3D content generation with 2D diffusion priors. We experimentally evaluated the proposed approach on XXX. The results show that the proposed approach can generate high-quality details more than the baselines",
    "checked": true,
    "id": "d8aaed01dffc621488aecbb0ef01b50f86e44bc1",
    "semantic_title": "hd-fusion: detailed text-to-3d generation leveraging multiple noise estimation",
    "citation_count": 9,
    "authors": [
      "Jinbo Wu",
      "Xiaobo Gao",
      "Xing Liu",
      "Zhengyang Shen",
      "Chen Zhao",
      "Haocheng Feng",
      "Jingtuo Liu",
      "Errui Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Sreenivas_pSTarC_Pseudo_Source_Guided_Target_Clustering_for_Fully_Test-Time_Adaptation_WACV_2024_paper.html": {
    "title": "pSTarC: Pseudo Source Guided Target Clustering for Fully Test-Time Adaptation",
    "volume": "main",
    "abstract": "Test Time Adaptation (TTA) is a pivotal concept in machine learning, enabling models to perform well in real-world scenarios, where test data distribution differs from training. In this work, we propose a novel approach called pseudo Source guided Target Clustering (pSTarC) addressing the relatively unexplored area of TTA under real-world domain shifts. This method draws inspiration from target clustering techniques and exploits the source classifier for generating pseudo source samples. The test samples are strategically aligned with these pseudo source samples, facilitating their clustering and thereby enhancing TTA performance. pSTarC operates solely within the fully test-time adaptation protocol, removing the need for actual source data. Experimental validation on a variety of domain shift datasets, namely VisDA, Office-Home, DomainNet-126, CIFAR-100C verifies pSTarC's effectiveness. This method exhibits significant improvements in prediction accuracy along with efficient computational requirements. Furthermore, we also demonstrate the universality of the pSTarC framework by showing its effectiveness for the continuous TTA framework",
    "checked": true,
    "id": "dad6bbb2a26492dd9c0687f4011b3bb3eaf02a3e",
    "semantic_title": "pstarc: pseudo source guided target clustering for fully test-time adaptation",
    "citation_count": 0,
    "authors": [
      "Manogna Sreenivas",
      "Goirik Chakrabarty",
      "Soma Biswas"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Nguyen_FocusTune_Tuning_Visual_Localization_Through_Focus-Guided_Sampling_WACV_2024_paper.html": {
    "title": "FocusTune: Tuning Visual Localization Through Focus-Guided Sampling",
    "volume": "main",
    "abstract": "We propose FocusTune, a focus-guided sampling technique to improve the performance of visual localization algorithms. FocusTune directs a scene coordinate regression model towards regions critical for 3D point triangulation by exploiting key geometric constraints. Specifically, rather than uniformly sampling points across the image for training the scene coordinate regression model, we instead re-project 3D scene coordinates onto the 2D image plane and sample within a local neighborhood of the re-projected points. While our proposed sampling strategy is generally applicable, we showcase FocusTune by integrating it with the recently introduced Accelerated Coordinate Encoding (ACE) model. Our results demonstrate that FocusTune both improves or matches state-of-the-art performance whilst keeping ACE's appealing low storage and compute requirements, for example reducing translation error from 25 to 19 and 17 to 15 cm for single and ensemble models, respectively, on the Cambridge Landmarks dataset. This combination of high performance and low compute and storage requirements is particularly promising for applications in areas like mobile robotics and augmented reality. We made our code available at https://github.com/sontung/focus-tune",
    "checked": true,
    "id": "4972362bb22922c1ffabbaa173185b9092cdeb65",
    "semantic_title": "focustune: tuning visual localization through focus-guided sampling",
    "citation_count": 0,
    "authors": [
      "Son Tung Nguyen",
      "Alejandro Fontan",
      "Michael Milford",
      "Tobias Fischer"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Khoshsirat_Improving_Normalization_With_the_James-Stein_Estimator_WACV_2024_paper.html": {
    "title": "Improving Normalization With the James-Stein Estimator",
    "volume": "main",
    "abstract": "Stein's paradox holds considerable sway in high-dimensional statistics, highlighting that the sample mean, traditionally considered the de facto estimator, might not be the most efficacious in higher dimensions. To address this, the James-Stein estimator proposes an enhancement by steering the sample means toward a more centralized mean vector. In this paper, first, we establish that normalization layers in deep learning use inadmissible estimators for mean and variance. Next, we introduce a novel method to employ the James-Stein estimator to improve the estimation of mean and variance within normalization layers. We evaluate our method on different computer vision tasks: image classification, semantic segmentation, and 3D object classification. Through these evaluations, it is evident that our improved normalization layers consistently yield superior accuracy across all tasks without extra computational burden. Moreover, recognizing that a plethora of shrinkage estimators surpass the traditional estimator in performance, we study two other prominent shrinkage estimators: Ridge and LASSO. Additionally, we provide visual representations to intuitively demonstrate the impact of shrinkage on the estimated layer statistics. Finally, we study the effect of regularization and batch size on our modified batch normalization. The studies show that our method is less sensitive to batch size and regularization, improving accuracy under various setups",
    "checked": true,
    "id": "389fbe9558d6bbfcf05dbe9f6ec592ff07e04c66",
    "semantic_title": "improving normalization with the james-stein estimator",
    "citation_count": 0,
    "authors": [
      "Seyedalireza Khoshsirat",
      "Chandra Kambhamettu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Chen_Depth_From_Asymmetric_Frame-Event_Stereo_A_Divide-and-Conquer_Approach_WACV_2024_paper.html": {
    "title": "Depth From Asymmetric Frame-Event Stereo: A Divide-and-Conquer Approach",
    "volume": "main",
    "abstract": "Event cameras asynchronously measure brightness changes in a scene without motion blur or saturation, while frame cameras capture images with dense intensity and fine details at a fixed rate. The exclusive advantages of the two modalities make depth estimation from Stereo Asymmetric Frame-Event (SAFE) systems appealing. However, due to the inevitable information absence of one modality in certain challenging regions, existing stereo matching methods lose efficacy for asymmetric inputs from SAFE systems. In this paper, we propose a divide-and-conquer approach that decomposes depth estimation from SAFE systems into three sub-tasks, i.e., frame-event stereo matching, frame-based Structure-from-Motion (SfM), and event-based SfM. In this way, the above challenging regions are addressed by monocular SfM, which estimates robust depth with two views belonging to the same functioning modality. Moreover, we propose a dual sampling strategy to construct cost volumes with identical spatial locations and depth hypotheses for different sub-tasks, which enables sub-task fusion at the cost volume level. To tackle the occlusion issue raised by the sampling strategy, we further introduce a temporal fusion scheme to utilize long-term sequential inputs with multi-view information. Experimental results validate the superior performance of our method over existing solutions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xihao Chen",
      "Wenming Weng",
      "Yueyi Zhang",
      "Zhiwei Xiong"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Hossain_Framework-Agnostic_Semantically-Aware_Global_Reasoning_for_Segmentation_WACV_2024_paper.html": {
    "title": "Framework-Agnostic Semantically-Aware Global Reasoning for Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mir Rayat Imtiaz Hossain",
      "Leonid Sigal",
      "James J. Little"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Xu_Self-Supervised_Relation_Alignment_for_Scene_Graph_Generation_WACV_2024_paper.html": {
    "title": "Self-Supervised Relation Alignment for Scene Graph Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bicheng Xu",
      "Renjie Liao",
      "Leonid Sigal"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Liu_Tackling_Data_Bias_in_MUSIC-AVQA_Crafting_a_Balanced_Dataset_for_WACV_2024_paper.html": {
    "title": "Tackling Data Bias in MUSIC-AVQA: Crafting a Balanced Dataset for Unbiased Question-Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiulong Liu",
      "Zhikang Dong",
      "Peng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wu_RPCANet_Deep_Unfolding_RPCA_Based_Infrared_Small_Target_Detection_WACV_2024_paper.html": {
    "title": "RPCANet: Deep Unfolding RPCA Based Infrared Small Target Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengyi Wu",
      "Tianfang Zhang",
      "Lei Li",
      "Yian Huang",
      "Zhenming Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Lee_GLAD_Global-Local_View_Alignment_and_Background_Debiasing_for_Unsupervised_Video_WACV_2024_paper.html": {
    "title": "GLAD: Global-Local View Alignment and Background Debiasing for Unsupervised Video Domain Adaptation With Large Domain Gap",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyogun Lee",
      "Kyungho Bae",
      "Seong Jong Ha",
      "Yumin Ko",
      "Gyeong-Moon Park",
      "Jinwoo Choi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Teterwak_Learning_To_Compose_SuperWeights_for_Neural_Parameter_Allocation_Search_WACV_2024_paper.html": {
    "title": "Learning To Compose SuperWeights for Neural Parameter Allocation Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Piotr Teterwak",
      "Soren Nelson",
      "Nikoli Dryden",
      "Dina Bashkirova",
      "Kate Saenko",
      "Bryan A. Plummer"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wen_Second-Order_Graph_ODEs_for_Multi-Agent_Trajectory_Forecasting_WACV_2024_paper.html": {
    "title": "Second-Order Graph ODEs for Multi-Agent Trajectory Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Song Wen",
      "Hao Wang",
      "Di Liu",
      "Qilong Zhangli",
      "Dimitris Metaxas"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Boyne_FOUND_Foot_Optimization_With_Uncertain_Normals_for_Surface_Deformation_Using_WACV_2024_paper.html": {
    "title": "FOUND: Foot Optimization With Uncertain Normals for Surface Deformation Using Synthetic Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oliver Boyne",
      "Gwangbin Bae",
      "James Charles",
      "Roberto Cipolla"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Fox_Unsupervised_Event-Based_Video_Reconstruction_WACV_2024_paper.html": {
    "title": "Unsupervised Event-Based Video Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gereon Fox",
      "Xingang Pan",
      "Ayush Tewari",
      "Mohamed Elgharib",
      "Christian Theobalt"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Park_Can_CLIP_Help_Sound_Source_Localization_WACV_2024_paper.html": {
    "title": "Can CLIP Help Sound Source Localization?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sooyoung Park",
      "Arda Senocak",
      "Joon Son Chung"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Lin_FastSR-NeRF_Improving_NeRF_Efficiency_on_Consumer_Devices_With_a_Simple_WACV_2024_paper.html": {
    "title": "FastSR-NeRF: Improving NeRF Efficiency on Consumer Devices With a Simple Super-Resolution Pipeline",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chien-Yu Lin",
      "Qichen Fu",
      "Thomas Merth",
      "Karren Yang",
      "Anurag Ranjan"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Raghavan_Online_Class-Incremental_Learning_for_Real-World_Food_Image_Classification_WACV_2024_paper.html": {
    "title": "Online Class-Incremental Learning for Real-World Food Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siddeshwar Raghavan",
      "Jiangpeng He",
      "Fengqing Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Bansal_United_We_Stand_Divided_We_Fall_UnityGraph_for_Unsupervised_Procedure_WACV_2024_paper.html": {
    "title": "United We Stand, Divided We Fall: UnityGraph for Unsupervised Procedure Learning From Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siddhant Bansal",
      "Chetan Arora",
      "C. V. Jawahar"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Feng_3D_Face_Style_Transfer_With_a_Hybrid_Solution_of_NeRF_WACV_2024_paper.html": {
    "title": "3D Face Style Transfer With a Hybrid Solution of NeRF and Mesh Rasterization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianwei Feng",
      "Prateek Singhal"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Jeon_USDN_A_Unified_Sample-Wise_Dynamic_Network_With_Mixed-Precision_and_Early-Exit_WACV_2024_paper.html": {
    "title": "USDN: A Unified Sample-Wise Dynamic Network With Mixed-Precision and Early-Exit",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ji-Ye Jeon",
      "Xuan Truong Nguyen",
      "Soojung Ryu",
      "Hyuk-Jae Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Hoang_Learn_To_Unlearn_for_Deep_Neural_Networks_Minimizing_Unlearning_Interference_WACV_2024_paper.html": {
    "title": "Learn To Unlearn for Deep Neural Networks: Minimizing Unlearning Interference With Gradient Projection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tuan Hoang",
      "Santu Rana",
      "Sunil Gupta",
      "Svetha Venkatesh"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kim_Human_Motion_Aware_Text-to-Video_Generation_With_Explicit_Camera_Control_WACV_2024_paper.html": {
    "title": "Human Motion Aware Text-to-Video Generation With Explicit Camera Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taehoon Kim",
      "ChanHee Kang",
      "JaeHyuk Park",
      "Daun Jeong",
      "ChangHee Yang",
      "Suk-Ju Kang",
      "Kyeongbo Kong"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Mayer_Beyond_SOT_Tracking_Multiple_Generic_Objects_at_Once_WACV_2024_paper.html": {
    "title": "Beyond SOT: Tracking Multiple Generic Objects at Once",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christoph Mayer",
      "Martin Danelljan",
      "Ming-Hsuan Yang",
      "Vittorio Ferrari",
      "Luc Van Gool",
      "Alina Kuznetsova"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Katsumata_Revisiting_Latent_Space_of_GAN_Inversion_for_Robust_Real_Image_WACV_2024_paper.html": {
    "title": "Revisiting Latent Space of GAN Inversion for Robust Real Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Katsumata",
      "Duc Minh Vo",
      "Bei Liu",
      "Hideki Nakayama"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kenia_Robust_TRISO-Fueled_Pebble_Identification_by_Digit_Recognition_WACV_2024_paper.html": {
    "title": "Robust TRISO-Fueled Pebble Identification by Digit Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roshan Kenia",
      "Jihane Mendil",
      "Ahmed Jasim",
      "Muthanna Al-Dahhan",
      "Zhaozheng Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Duan_Evidential_Uncertainty_Quantification_A_Variance-Based_Perspective_WACV_2024_paper.html": {
    "title": "Evidential Uncertainty Quantification: A Variance-Based Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruxiao Duan",
      "Brian Caffo",
      "Harrison X. Bai",
      "Haris I. Sair",
      "Craig Jones"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Neshatavar_ICF-SRSR_Invertible_Scale-Conditional_Function_for_Self-Supervised_Real-World_Single_Image_Super-Resolution_WACV_2024_paper.html": {
    "title": "ICF-SRSR: Invertible Scale-Conditional Function for Self-Supervised Real-World Single Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reyhaneh Neshatavar",
      "Mohsen Yavartanoo",
      "Sanghyun Son",
      "Kyoung Mu Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ding_PATROL_Privacy-Oriented_Pruning_for_Collaborative_Inference_Against_Model_Inversion_Attacks_WACV_2024_paper.html": {
    "title": "PATROL: Privacy-Oriented Pruning for Collaborative Inference Against Model Inversion Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiwei Ding",
      "Lan Zhang",
      "Miao Pan",
      "Xiaoyong Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Sarukkai_Collage_Diffusion_WACV_2024_paper.html": {
    "title": "Collage Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vishnu Sarukkai",
      "Linden Li",
      "Arden Ma",
      "Christopher Ré",
      "Kayvon Fatahalian"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wijayasingha_Camera-Independent_Single_Image_Depth_Estimation_From_Defocus_Blur_WACV_2024_paper.html": {
    "title": "Camera-Independent Single Image Depth Estimation From Defocus Blur",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lahiru Wijayasingha",
      "Homa Alemzadeh",
      "John A. Stankovic"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Liu_Wakening_Past_Concepts_Without_Past_Data_Class-Incremental_Learning_From_Online_WACV_2024_paper.html": {
    "title": "Wakening Past Concepts Without Past Data: Class-Incremental Learning From Online Placebos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaoyao Liu",
      "Yingying Li",
      "Bernt Schiele",
      "Qianru Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wahed_Fine-Grained_Alignment_for_Cross-Modal_Recipe_Retrieval_WACV_2024_paper.html": {
    "title": "Fine-Grained Alignment for Cross-Modal Recipe Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muntasir Wahed",
      "Xiaona Zhou",
      "Tianjiao Yu",
      "Ismini Lourentzou"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Bernal_NOMAD_A_Natural_Occluded_Multi-Scale_Aerial_Dataset_for_Emergency_Response_WACV_2024_paper.html": {
    "title": "NOMAD: A Natural, Occluded, Multi-Scale Aerial Dataset, for Emergency Response Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arturo Miguel Russell Bernal",
      "Walter Scheirer",
      "Jane Cleland-Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Lee_UNSPAT_Uncertainty-Guided_SpatioTemporal_Transformer_for_3D_Human_Pose_and_Shape_WACV_2024_paper.html": {
    "title": "UNSPAT: Uncertainty-Guided SpatioTemporal Transformer for 3D Human Pose and Shape Estimation on Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minsoo Lee",
      "Hyunmin Lee",
      "Bumsoo Kim",
      "Seunghwan Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhu_Consistent_Multimodal_Generation_via_a_Unified_GAN_Framework_WACV_2024_paper.html": {
    "title": "Consistent Multimodal Generation via a Unified GAN Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Zhu",
      "Yijun Li",
      "Weijie Lyu",
      "Krishna Kumar Singh",
      "Zhixin Shu",
      "Sören Pirk",
      "Derek Hoiem"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kwon_Self-Supervised_Learning_of_Semantic_Correspondence_Using_Web_Videos_WACV_2024_paper.html": {
    "title": "Self-Supervised Learning of Semantic Correspondence Using Web Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donghyeon Kwon",
      "Minsu Cho",
      "Suha Kwak"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Grimal_TIAM_-_A_Metric_for_Evaluating_Alignment_in_Text-to-Image_Generation_WACV_2024_paper.html": {
    "title": "TIAM - A Metric for Evaluating Alignment in Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul Grimal",
      "Hervé Le Borgne",
      "Olivier Ferret",
      "Julien Tourille"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Xue_HDMNet_A_Hierarchical_Matching_Network_With_Double_Attention_for_Large-Scale_WACV_2024_paper.html": {
    "title": "HDMNet: A Hierarchical Matching Network With Double Attention for Large-Scale Outdoor LiDAR Point Cloud Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiyi Xue",
      "Fan Lu",
      "Guang Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Lee_UGPNet_Universal_Generative_Prior_for_Image_Restoration_WACV_2024_paper.html": {
    "title": "UGPNet: Universal Generative Prior for Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hwayoon Lee",
      "Kyoungkook Kang",
      "Hyeongmin Lee",
      "Seung-Hwan Baek",
      "Sunghyun Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Sun_Defense_Against_Adversarial_Cloud_Attack_on_Remote_Sensing_Salient_Object_WACV_2024_paper.html": {
    "title": "Defense Against Adversarial Cloud Attack on Remote Sensing Salient Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huiming Sun",
      "Lan Fu",
      "Jinlong Li",
      "Qing Guo",
      "Zibo Meng",
      "Tianyun Zhang",
      "Yuewei Lin",
      "Hongkai Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Nguyen_Diffusion_in_the_Dark_A_Diffusion_Model_for_Low-Light_Text_WACV_2024_paper.html": {
    "title": "Diffusion in the Dark: A Diffusion Model for Low-Light Text Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cindy M. Nguyen",
      "Eric R. Chan",
      "Alexander W. Bergman",
      "Gordon Wetzstein"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Drenkow_RobustCLEVR_A_Benchmark_and_Framework_for_Evaluating_Robustness_in_Object-Centric_WACV_2024_paper.html": {
    "title": "RobustCLEVR: A Benchmark and Framework for Evaluating Robustness in Object-Centric Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nathan Drenkow",
      "Mathias Unberath"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Yan_AFTer-SAM_Adapting_SAM_With_Axial_Fusion_Transformer_for_Medical_Imaging_WACV_2024_paper.html": {
    "title": "AFTer-SAM: Adapting SAM With Axial Fusion Transformer for Medical Imaging Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyi Yan",
      "Shanlin Sun",
      "Kun Han",
      "Thanh-Tung Le",
      "Haoyu Ma",
      "Chenyu You",
      "Xiaohui Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Gomez-Villa_Plasticity-Optimized_Complementary_Networks_for_Unsupervised_Continual_Learning_WACV_2024_paper.html": {
    "title": "Plasticity-Optimized Complementary Networks for Unsupervised Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Gomez-Villa",
      "Bartlomiej Twardowski",
      "Kai Wang",
      "Joost van de Weijer"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Weijler_FATE_Feature-Agnostic_Transformer-Based_Encoder_for_Learning_Generalized_Embedding_Spaces_in_WACV_2024_paper.html": {
    "title": "FATE: Feature-Agnostic Transformer-Based Encoder for Learning Generalized Embedding Spaces in Flow Cytometry Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lisa Weijler",
      "Florian Kowarsch",
      "Michael Reiter",
      "Pedro Hermosilla",
      "Margarita Maurer-Granofszky",
      "Michael Dworzak"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Katsumata_Label_Augmentation_As_Inter-Class_Data_Augmentation_for_Conditional_Image_Synthesis_WACV_2024_paper.html": {
    "title": "Label Augmentation As Inter-Class Data Augmentation for Conditional Image Synthesis With Imbalanced Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Katsumata",
      "Duc Minh Vo",
      "Hideki Nakayama"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Xie_Sign_Language_Production_With_Latent_Motion_Transformer_WACV_2024_paper.html": {
    "title": "Sign Language Production With Latent Motion Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pan Xie",
      "Taiying Peng",
      "Yao Du",
      "Qipeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Stypulkowski_Diffused_Heads_Diffusion_Models_Beat_GANs_on_Talking-Face_Generation_WACV_2024_paper.html": {
    "title": "Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michał Stypułkowski",
      "Konstantinos Vougioukas",
      "Sen He",
      "Maciej Zięba",
      "Stavros Petridis",
      "Maja Pantic"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Liu_U3DS3_Unsupervised_3D_Semantic_Scene_Segmentation_WACV_2024_paper.html": {
    "title": "U3DS3: Unsupervised 3D Semantic Scene Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxu Liu",
      "Zhengdi Yu",
      "Toby P. Breckon",
      "Hubert P. H. Shum"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Xu_GIPCOL_Graph-Injected_Soft_Prompting_for_Compositional_Zero-Shot_Learning_WACV_2024_paper.html": {
    "title": "GIPCOL: Graph-Injected Soft Prompting for Compositional Zero-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangyue Xu",
      "Joyce Chai",
      "Parisa Kordjamshidi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Garcia-Bordils_STEP_-_Towards_Structured_Scene-Text_Spotting_WACV_2024_paper.html": {
    "title": "STEP - Towards Structured Scene-Text Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sergi Garcia-Bordils",
      "Dimosthenis Karatzas",
      "Marçal Rusiñol"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Roy_ClipSitu_Effectively_Leveraging_CLIP_for_Conditional_Predictions_in_Situation_Recognition_WACV_2024_paper.html": {
    "title": "ClipSitu: Effectively Leveraging CLIP for Conditional Predictions in Situation Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Debaditya Roy",
      "Dhruv Verma",
      "Basura Fernando"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wang_Multimodality-Guided_Image_Style_Transfer_Using_Cross-Modal_GAN_Inversion_WACV_2024_paper.html": {
    "title": "Multimodality-Guided Image Style Transfer Using Cross-Modal GAN Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanyu Wang",
      "Pengxiang Wu",
      "Kevin Dela Rosa",
      "Chen Wang",
      "Abhinav Shrivastava"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Verma_Meta-Learned_Attribute_Self-Interaction_Network_for_Continual_and_Generalized_Zero-Shot_Learning_WACV_2024_paper.html": {
    "title": "Meta-Learned Attribute Self-Interaction Network for Continual and Generalized Zero-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vinay Verma",
      "Nikhil Mehta",
      "Kevin J. Liang",
      "Aakansha Mishra",
      "Lawrence Carin"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Nicolas_MoP-CLIP_A_Mixture_of_Prompt-Tuned_CLIP_Models_for_Domain_Incremental_WACV_2024_paper.html": {
    "title": "MoP-CLIP: A Mixture of Prompt-Tuned CLIP Models for Domain Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julien Nicolas",
      "Florent Chiaroni",
      "Imtiaz Ziko",
      "Ola Ahmad",
      "Christian Desrosiers",
      "Jose Dolz"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Gloudemans_So_You_Think_You_Can_Track_WACV_2024_paper.html": {
    "title": "So You Think You Can Track?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Derek Gloudemans",
      "Gergely Zachár",
      "Yanbing Wang",
      "Junyi Ji",
      "Matt Nice",
      "Matt Bunting",
      "William W. Barbour",
      "Jonathan Sprinkle",
      "Benedetto Piccoli",
      "Maria Laura Delle Monache",
      "Alexandre Bayen",
      "Benjamin Seibold",
      "Daniel B. Work"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Srivastava_OmniVec_Learning_Robust_Representations_With_Cross_Modal_Sharing_WACV_2024_paper.html": {
    "title": "OmniVec: Learning Robust Representations With Cross Modal Sharing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siddharth Srivastava",
      "Gaurav Sharma"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Song_MSCC_Multi-Scale_Transformers_for_Camera_Calibration_WACV_2024_paper.html": {
    "title": "MSCC: Multi-Scale Transformers for Camera Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Song",
      "Hao Kang",
      "Atsunori Moteki",
      "Genta Suzuki",
      "Yoshie Kobayashi",
      "Zhiming Tan"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Hou_Multi-Modal_Gaze_Following_in_Conversational_Scenarios_WACV_2024_paper.html": {
    "title": "Multi-Modal Gaze Following in Conversational Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuqi Hou",
      "Zhongqun Zhang",
      "Nora Horanyi",
      "Jaewon Moon",
      "Yihua Cheng",
      "Hyung Jin Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Nguyen_Contrastive_Viewpoint-Aware_Shape_Learning_for_Long-Term_Person_Re-Identification_WACV_2024_paper.html": {
    "title": "Contrastive Viewpoint-Aware Shape Learning for Long-Term Person Re-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vuong D. Nguyen",
      "Khadija Khaldi",
      "Dung Nguyen",
      "Pranav Mantini",
      "Shishir Shah"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Huang_Scale-Adaptive_Feature_Aggregation_for_Efficient_Space-Time_Video_Super-Resolution_WACV_2024_paper.html": {
    "title": "Scale-Adaptive Feature Aggregation for Efficient Space-Time Video Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhewei Huang",
      "Ailin Huang",
      "Xiaotao Hu",
      "Chen Hu",
      "Jun Xu",
      "Shuchang Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhu_SSP_Semi-Signed_Prioritized_Neural_Fitting_for_Surface_Reconstruction_From_Unoriented_WACV_2024_paper.html": {
    "title": "SSP: Semi-Signed Prioritized Neural Fitting for Surface Reconstruction From Unoriented Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runsong Zhu",
      "Di Kang",
      "Ka-Hei Hui",
      "Yue Qian",
      "Shi Qiu",
      "Zhen Dong",
      "Linchao Bao",
      "Pheng-Ann Heng",
      "Chi-Wing Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Fedynyak_DeVos_Flow-Guided_Deformable_Transformer_for_Video_Object_Segmentation_WACV_2024_paper.html": {
    "title": "DeVos: Flow-Guided Deformable Transformer for Video Object Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Volodymyr Fedynyak",
      "Yaroslav Romanus",
      "Bohdan Hlovatskyi",
      "Bohdan Sydor",
      "Oles Dobosevych",
      "Igor Babin",
      "Roman Riazantsev"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Verma_GraphFill_Deep_Image_Inpainting_Using_Graphs_WACV_2024_paper.html": {
    "title": "GraphFill: Deep Image Inpainting Using Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shashikant Verma",
      "Aman Sharma",
      "Roopa Sheshadri",
      "Shanmuganathan Raman"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kuang_AU-Aware_Dynamic_3D_Face_Reconstruction_From_Videos_With_Transformer_WACV_2024_paper.html": {
    "title": "AU-Aware Dynamic 3D Face Reconstruction From Videos With Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenyi Kuang",
      "Jeffrey O. Kephart",
      "Qiang Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Gandikota_Unified_Concept_Editing_in_Diffusion_Models_WACV_2024_paper.html": {
    "title": "Unified Concept Editing in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rohit Gandikota",
      "Hadas Orgad",
      "Yonatan Belinkov",
      "Joanna Materzyńska",
      "David Bau"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Bui_MEGANet_Multi-Scale_Edge-Guided_Attention_Network_for_Weak_Boundary_Polyp_Segmentation_WACV_2024_paper.html": {
    "title": "MEGANet: Multi-Scale Edge-Guided Attention Network for Weak Boundary Polyp Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nhat-Tan Bui",
      "Dinh-Hieu Hoang",
      "Quang-Thuc Nguyen",
      "Minh-Triet Tran",
      "Ngan Le"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wang_GazeGNN_A_Gaze-Guided_Graph_Neural_Network_for_Chest_X-Ray_Classification_WACV_2024_paper.html": {
    "title": "GazeGNN: A Gaze-Guided Graph Neural Network for Chest X-Ray Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Wang",
      "Hongyi Pan",
      "Armstrong Aboah",
      "Zheyuan Zhang",
      "Elif Keles",
      "Drew Torigian",
      "Baris Turkbey",
      "Elizabeth Krupinski",
      "Jayaram Udupa",
      "Ulas Bagci"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Silva_LipAT_Beyond_Style_Transfer_for_Controllable_Neural_Simulation_of_Lipstick_WACV_2024_paper.html": {
    "title": "LipAT: Beyond Style Transfer for Controllable Neural Simulation of Lipstick Using Cosmetic Attributes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amila Silva",
      "Olga Moskvyak",
      "Alexander Long",
      "Ravi Garg",
      "Stephen Gould",
      "Gil Avraham",
      "Anton van den Hengel"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Cermak_WildlifeDatasets_An_Open-Source_Toolkit_for_Animal_Re-Identification_WACV_2024_paper.html": {
    "title": "WildlifeDatasets: An Open-Source Toolkit for Animal Re-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vojtěch Čermák",
      "Lukas Picek",
      "Lukáš Adam",
      "Kostas Papafitsoros"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Li_OTAS_Unsupervised_Boundary_Detection_for_Object-Centric_Temporal_Action_Segmentation_WACV_2024_paper.html": {
    "title": "OTAS: Unsupervised Boundary Detection for Object-Centric Temporal Action Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuerong Li",
      "Zhengrong Xue",
      "Huazhe Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Luthra_Deblur-NSFF_Neural_Scene_Flow_Fields_for_Blurry_Dynamic_Scenes_WACV_2024_paper.html": {
    "title": "Deblur-NSFF: Neural Scene Flow Fields for Blurry Dynamic Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Achleshwar Luthra",
      "Shiva Souhith Gantha",
      "Xiyun Song",
      "Heather Yu",
      "Zongfang Lin",
      "Liang Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Belal_Multi-Source_Domain_Adaptation_for_Object_Detection_With_Prototype-Based_Mean_Teacher_WACV_2024_paper.html": {
    "title": "Multi-Source Domain Adaptation for Object Detection With Prototype-Based Mean Teacher",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Atif Belal",
      "Akhil Meethal",
      "Francisco Perdigon Romero",
      "Marco Pedersoli",
      "Eric Granger"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Yellapragada_PathLDM_Text_Conditioned_Latent_Diffusion_Model_for_Histopathology_WACV_2024_paper.html": {
    "title": "PathLDM: Text Conditioned Latent Diffusion Model for Histopathology",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Srikar Yellapragada",
      "Alexandros Graikos",
      "Prateek Prasanna",
      "Tahsin Kurc",
      "Joel Saltz",
      "Dimitris Samaras"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Hwang_EASUM_Enhancing_Affective_State_Understanding_Through_Joint_Sentiment_and_Emotion_WACV_2024_paper.html": {
    "title": "EASUM: Enhancing Affective State Understanding Through Joint Sentiment and Emotion Modeling for Multimodal Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yewon Hwang",
      "Jong-Hwan Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Huber_Efficient_Explainable_Face_Verification_Based_on_Similarity_Score_Argument_Backpropagation_WACV_2024_paper.html": {
    "title": "Efficient Explainable Face Verification Based on Similarity Score Argument Backpropagation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco Huber",
      "Anh Thi Luu",
      "Philipp Terhörst",
      "Naser Damer"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Sachdeva_Rank2Tell_A_Multimodal_Driving_Dataset_for_Joint_Importance_Ranking_and_WACV_2024_paper.html": {
    "title": "Rank2Tell: A Multimodal Driving Dataset for Joint Importance Ranking and Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enna Sachdeva",
      "Nakul Agarwal",
      "Suhas Chundi",
      "Sean Roelofs",
      "Jiachen Li",
      "Mykel Kochenderfer",
      "Chiho Choi",
      "Behzad Dariush"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Hayon_ArcAid_Analysis_of_Archaeological_Artifacts_Using_Drawings_WACV_2024_paper.html": {
    "title": "ArcAid: Analysis of Archaeological Artifacts Using Drawings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Offry Hayon",
      "Stefan Münger",
      "Ilan Shimshoni",
      "Ayellet Tal"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Dawkins_FishTrack23_An_Ensemble_Underwater_Dataset_for_Multi-Object_Tracking_WACV_2024_paper.html": {
    "title": "FishTrack23: An Ensemble Underwater Dataset for Multi-Object Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Dawkins",
      "Jack Prior",
      "Bryon Lewis",
      "Robin Faillettaz",
      "Thompson Banez",
      "Mary Salvi",
      "Audrey Rollo",
      "Julien Simon",
      "Matthew Campbell",
      "Matthew Lucero",
      "Aashish Chaudhary",
      "Benjamin Richards",
      "Anthony Hoogs"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Gupta_Reducing_the_Side-Effects_of_Oscillations_in_Training_of_Quantized_YOLO_WACV_2024_paper.html": {
    "title": "Reducing the Side-Effects of Oscillations in Training of Quantized YOLO Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kartik Gupta",
      "Akshay Asthana"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Grady_PressureVision_Estimating_Fingertip_Pressure_From_Diverse_RGB_Images_WACV_2024_paper.html": {
    "title": "PressureVision++: Estimating Fingertip Pressure From Diverse RGB Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrick Grady",
      "Jeremy A. Collins",
      "Chengcheng Tang",
      "Christopher D. Twigg",
      "Kunal Aneja",
      "James Hays",
      "Charles C. Kemp"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Tailanian_Diffusion_Models_Meet_Image_Counter-Forensics_WACV_2024_paper.html": {
    "title": "Diffusion Models Meet Image Counter-Forensics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matías Tailanián",
      "Marina Gardella",
      "Alvaro Pardo",
      "Pablo Musé"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Bose_STYLIP_Multi-Scale_Style-Conditioned_Prompt_Learning_for_CLIP-Based_Domain_Generalization_WACV_2024_paper.html": {
    "title": "STYLIP: Multi-Scale Style-Conditioned Prompt Learning for CLIP-Based Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shirsha Bose",
      "Ankit Jha",
      "Enrico Fini",
      "Mainak Singha",
      "Elisa Ricci",
      "Biplab Banerjee"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Metta_Increasing_Biases_Can_Be_More_Efficient_Than_Increasing_Weights_WACV_2024_paper.html": {
    "title": "Increasing Biases Can Be More Efficient Than Increasing Weights",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Carlo Metta",
      "Marco Fantozzi",
      "Andrea Papini",
      "Gianluca Amato",
      "Matteo Bergamaschi",
      "Silvia Giulia Galfrè",
      "Alessandro Marchetti",
      "Michelangelo Vegliò",
      "Maurizio Parton",
      "Francesco Morandin"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Dalbah_TransRadar_Adaptive-Directional_Transformer_for_Real-Time_Multi-View_Radar_Semantic_Segmentation_WACV_2024_paper.html": {
    "title": "TransRadar: Adaptive-Directional Transformer for Real-Time Multi-View Radar Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yahia Dalbah",
      "Jean Lahoud",
      "Hisham Cholakkal"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Sequential_Transformer_for_End-to-End_Video_Text_Detection_WACV_2024_paper.html": {
    "title": "Sequential Transformer for End-to-End Video Text Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun-Bo Zhang",
      "Meng-Biao Zhao",
      "Fei Yin",
      "Cheng-Lin Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kara_The_Background_Also_Matters_Background-Aware_Motion-Guided_Objects_Discovery_WACV_2024_paper.html": {
    "title": "The Background Also Matters: Background-Aware Motion-Guided Objects Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sandra Kara",
      "Hejer Ammar",
      "Florian Chabot",
      "Quoc-Cuong Pham"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Li_Neural_Style_Protection_Counteracting_Unauthorized_Neural_Style_Transfer_WACV_2024_paper.html": {
    "title": "Neural Style Protection: Counteracting Unauthorized Neural Style Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaxin Li",
      "Jie Ren",
      "Han Xu",
      "Hui Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ogawa_FRoG-MOT_Fast_and_Robust_Generic_Multiple-Object_Tracking_by_IoU_and_WACV_2024_paper.html": {
    "title": "FRoG-MOT: Fast and Robust Generic Multiple-Object Tracking by IoU and Motion-State Associations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takuya Ogawa",
      "Takashi Shibata",
      "Toshinori Hosoi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Alexandropoulos_OVeNet_Offset_Vector_Network_for_Semantic_Segmentation_WACV_2024_paper.html": {
    "title": "OVeNet: Offset Vector Network for Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stamatis Alexandropoulos",
      "Christos Sakaridis",
      "Petros Maragos"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Logothetis_A_Neural_Height-Map_Approach_for_the_Binocular_Photometric_Stereo_Problem_WACV_2024_paper.html": {
    "title": "A Neural Height-Map Approach for the Binocular Photometric Stereo Problem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fotios Logothetis",
      "Ignas Budvytis",
      "Roberto Cipolla"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Feinglass_Towards_Addressing_the_Misalignment_of_Object_Proposal_Evaluation_for_Vision-Language_WACV_2024_paper.html": {
    "title": "Towards Addressing the Misalignment of Object Proposal Evaluation for Vision-Language Tasks via Semantic Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joshua Feinglass",
      "Yezhou Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Lin_Spiking_Neural_Networks_for_Active_Time-Resolved_SPAD_Imaging_WACV_2024_paper.html": {
    "title": "Spiking Neural Networks for Active Time-Resolved SPAD Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Lin",
      "Edoardo Charbon"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Domain_Generalization_With_Correlated_Style_Uncertainty_WACV_2024_paper.html": {
    "title": "Domain Generalization With Correlated Style Uncertainty",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheyuan Zhang",
      "Bin Wang",
      "Debesh Jha",
      "Ugur Demir",
      "Ulas Bagci"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Thakur_Leveraging_Next-Active_Objects_for_Context-Aware_Anticipation_in_Egocentric_Videos_WACV_2024_paper.html": {
    "title": "Leveraging Next-Active Objects for Context-Aware Anticipation in Egocentric Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanket Thakur",
      "Cigdem Beyan",
      "Pietro Morerio",
      "Vittorio Murino",
      "Alessio Del Bue"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Fan_CryoRL_Reinforcement_Learning_Enables_Efficient_Cryo-EM_Data_Collection_WACV_2024_paper.html": {
    "title": "CryoRL: Reinforcement Learning Enables Efficient Cryo-EM Data Collection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quanfu Fan",
      "Yilai Li",
      "Yuguang Yao",
      "John Cohn",
      "Sijia Liu",
      "Ziping Xu",
      "Seychelle Vos",
      "Michael Cianfrocco"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Mehra_On_the_Fly_Neural_Style_Smoothing_for_Risk-Averse_Domain_Generalization_WACV_2024_paper.html": {
    "title": "On the Fly Neural Style Smoothing for Risk-Averse Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshay Mehra",
      "Yunbei Zhang",
      "Bhavya Kailkhura",
      "Jihun Hamm"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ntavelis_StyleGenes_Discrete_and_Efficient_Latent_Distributions_for_GANs_WACV_2024_paper.html": {
    "title": "StyleGenes: Discrete and Efficient Latent Distributions for GANs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Evangelos Ntavelis",
      "Mohamad Shahbazi",
      "Iason Kastanis",
      "Martin Danelljan",
      "Luc Van Gool"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Sanyal_Aligning_Non-Causal_Factors_for_Transformer-Based_Source-Free_Domain_Adaptation_WACV_2024_paper.html": {
    "title": "Aligning Non-Causal Factors for Transformer-Based Source-Free Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sunandini Sanyal",
      "Ashish Ramayee Asokan",
      "Suvaansh Bhambri",
      "Pradyumna YM",
      "Akshay Kulkarni",
      "Jogendra Nath Kundu",
      "R. Venkatesh Babu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Shi_Benchmarking_Out-of-Distribution_Detection_in_Visual_Question_Answering_WACV_2024_paper.html": {
    "title": "Benchmarking Out-of-Distribution Detection in Visual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangxi Shi",
      "Stefan Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Shvets_Joint_Depth_Prediction_and_Semantic_Segmentation_With_Multi-View_SAM_WACV_2024_paper.html": {
    "title": "Joint Depth Prediction and Semantic Segmentation With Multi-View SAM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mykhailo Shvets",
      "Dongxu Zhao",
      "Marc Niethammer",
      "Roni Sengupta",
      "Alexander C. Berg"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Rawal_GC-VTON_Predicting_Globally_Consistent_and_Occlusion_Aware_Local_Flows_With_WACV_2024_paper.html": {
    "title": "GC-VTON: Predicting Globally Consistent and Occlusion Aware Local Flows With Neighborhood Integrity Preservation for Virtual Try-On",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hamza Rawal",
      "Muhammad Junaid Ahmad",
      "Farooq Zaman"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Li_Enforcing_Sparsity_on_Latent_Space_for_Robust_and_Explainable_Representations_WACV_2024_paper.html": {
    "title": "Enforcing Sparsity on Latent Space for Robust and Explainable Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanao Li",
      "Tian Han"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhao_Unsupervised_Domain_Adaptation_for_Semantic_Segmentation_With_Pseudo_Label_Self-Refinement_WACV_2024_paper.html": {
    "title": "Unsupervised Domain Adaptation for Semantic Segmentation With Pseudo Label Self-Refinement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingchen Zhao",
      "Niluthpol Chowdhury Mithun",
      "Abhinav Rajvanshi",
      "Han-Pang Chiu",
      "Supun Samarasekera"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Medeiros_HalluciDet_Hallucinating_RGB_Modality_for_Person_Detection_Through_Privileged_Information_WACV_2024_paper.html": {
    "title": "HalluciDet: Hallucinating RGB Modality for Person Detection Through Privileged Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heitor Rapela Medeiros",
      "Fidel A. Guerrero Peña",
      "Masih Aminbeidokhti",
      "Thomas Dubail",
      "Eric Granger",
      "Marco Pedersoli"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ju_Improving_Fairness_in_Deepfake_Detection_WACV_2024_paper.html": {
    "title": "Improving Fairness in Deepfake Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Ju",
      "Shu Hu",
      "Shan Jia",
      "George H. Chen",
      "Siwei Lyu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Yu_Evolve_Enhancing_Unsupervised_Continual_Learning_With_Multiple_Experts_WACV_2024_paper.html": {
    "title": "Evolve: Enhancing Unsupervised Continual Learning With Multiple Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaofan Yu",
      "Tajana Rosing",
      "Yunhui Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Sun_NeRFEditor_Differentiable_Style_Decomposition_for_3D_Scene_Editing_WACV_2024_paper.html": {
    "title": "NeRFEditor: Differentiable Style Decomposition for 3D Scene Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunyi Sun",
      "Yanbin Liu",
      "Junlin Han",
      "Stephen Gould"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Xu_Personalized_Face_Inpainting_With_Diffusion_Models_by_Parallel_Visual_Attention_WACV_2024_paper.html": {
    "title": "Personalized Face Inpainting With Diffusion Models by Parallel Visual Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianjin Xu",
      "Saman Motamed",
      "Praneetha Vaddamanu",
      "Chen Henry Wu",
      "Christian Haene",
      "Jean-Charles Bazin",
      "Fernando De la Torre"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Karthikeyan_AvatarOne_Monocular_3D_Human_Animation_WACV_2024_paper.html": {
    "title": "AvatarOne: Monocular 3D Human Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akash Karthikeyan",
      "Robert Ren",
      "Yash Kant",
      "Igor Gilitschenski"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Hukkelas_Synthesizing_Anyone_Anywhere_in_Any_Pose_WACV_2024_paper.html": {
    "title": "Synthesizing Anyone, Anywhere, in Any Pose",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Håkon Hukkelås",
      "Frank Lindseth"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Deng_Ray_Deformation_Networks_for_Novel_View_Synthesis_of_Refractive_Objects_WACV_2024_paper.html": {
    "title": "Ray Deformation Networks for Novel View Synthesis of Refractive Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijian Deng",
      "Dylan Campbell",
      "Chunyi Sun",
      "Shubham Kanitkar",
      "Matthew Shaffer",
      "Stephen Gould"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Hempel_NITEC_Versatile_Hand-Annotated_Eye_Contact_Dataset_for_Ego-Vision_Interaction_WACV_2024_paper.html": {
    "title": "NITEC: Versatile Hand-Annotated Eye Contact Dataset for Ego-Vision Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thorsten Hempel",
      "Magnus Jung",
      "Ahmed A. Abdelrahman",
      "Ayoub Al-Hamadi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Inkawhich_Tunable_Hybrid_Proposal_Networks_for_the_Open_World_WACV_2024_paper.html": {
    "title": "Tunable Hybrid Proposal Networks for the Open World",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Inkawhich",
      "Nathan Inkawhich",
      "Hai Li",
      "Yiran Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Cha_3D_Reconstruction_of_Interacting_Multi-Person_in_Clothing_From_a_Single_WACV_2024_paper.html": {
    "title": "3D Reconstruction of Interacting Multi-Person in Clothing From a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junuk Cha",
      "Hansol Lee",
      "Jaewon Kim",
      "Nhat Nguyen Bao Truong",
      "Jaeshin Yoon",
      "Seungryul Baek"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kim_LensNeRF_Rethinking_Volume_Rendering_Based_on_Thin-Lens_Camera_Model_WACV_2024_paper.html": {
    "title": "LensNeRF: Rethinking Volume Rendering Based on Thin-Lens Camera Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Min-Jung Kim",
      "Gyojung Gu",
      "Jaegul Choo"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Jamwal_Composite_Diffusion_whole__Sparts_WACV_2024_paper.html": {
    "title": "Composite Diffusion: whole >= Sparts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vikram Jamwal",
      "Ramaneswaran S."
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Chong_P2D_Plug_and_Play_Discriminator_for_Accelerating_GAN_Frameworks_WACV_2024_paper.html": {
    "title": "P2D: Plug and Play Discriminator for Accelerating GAN Frameworks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Min Jin Chong",
      "Krishna Kumar Singh",
      "Yijun Li",
      "Jingwan Lu",
      "David Forsyth"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Xian_PMI_Sampler_Patch_Similarity_Guided_Frame_Selection_for_Aerial_Action_WACV_2024_paper.html": {
    "title": "PMI Sampler: Patch Similarity Guided Frame Selection for Aerial Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiqi Xian",
      "Xijun Wang",
      "Divya Kothandaraman",
      "Dinesh Manocha"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Seto_REALM_Robust_Entropy_Adaptive_Loss_Minimization_for_Improved_Single-Sample_Test-Time_WACV_2024_paper.html": {
    "title": "REALM: Robust Entropy Adaptive Loss Minimization for Improved Single-Sample Test-Time Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Skyler Seto",
      "Barry-John Theobald",
      "Federico Danieli",
      "Navdeep Jaitly",
      "Dan Busbridge"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Xiao_TSA2_Temporal_Segment_Adaptation_and_Aggregation_for_Video_Harmonization_WACV_2024_paper.html": {
    "title": "TSA2: Temporal Segment Adaptation and Aggregation for Video Harmonization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyu Xiao",
      "Yurui Zhu",
      "Xueyang Fu",
      "Zhiwei Xiong"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhang_PMVC_Promoting_Multi-View_Consistency_for_3D_Scene_Reconstruction_WACV_2024_paper.html": {
    "title": "PMVC: Promoting Multi-View Consistency for 3D Scene Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chushan Zhang",
      "Jinguang Tong",
      "Tao Jun Lin",
      "Chuong Nguyen",
      "Hongdong Li"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Yang_MGM-AE_Self-Supervised_Learning_on_3D_Shape_Using_Mesh_Graph_Masked_WACV_2024_paper.html": {
    "title": "MGM-AE: Self-Supervised Learning on 3D Shape Using Mesh Graph Masked Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhangsihao Yang",
      "Kaize Ding",
      "Huan Liu",
      "Yalin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Cho_Interactive_Network_Perturbation_Between_Teacher_and_Students_for_Semi-Supervised_Semantic_WACV_2024_paper.html": {
    "title": "Interactive Network Perturbation Between Teacher and Students for Semi-Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyuna Cho",
      "Injun Choi",
      "Suha Kwak",
      "Won Hwa Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Yashwanth_Minimizing_Layerwise_Activation_Norm_Improves_Generalization_in_Federated_Learning_WACV_2024_paper.html": {
    "title": "Minimizing Layerwise Activation Norm Improves Generalization in Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "M. Yashwanth",
      "Gaurav Kumar Nayak",
      "Harsh Rangwani",
      "Arya Singh",
      "R. Venkatesh Babu",
      "Anirban Chakraborty"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Hu_ReCLIP_Refine_Contrastive_Language_Image_Pre-Training_With_Source_Free_Domain_WACV_2024_paper.html": {
    "title": "ReCLIP: Refine Contrastive Language Image Pre-Training With Source Free Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuefeng Hu",
      "Ke Zhang",
      "Lu Xia",
      "Albert Chen",
      "Jiajia Luo",
      "Yuyin Sun",
      "Ken Wang",
      "Nan Qiao",
      "Xiao Zeng",
      "Min Sun",
      "Cheng-Hao Kuo",
      "Ram Nevatia"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Tran_PointCT_Point_Central_Transformer_Network_for_Weakly-Supervised_Point_Cloud_Semantic_WACV_2024_paper.html": {
    "title": "PointCT: Point Central Transformer Network for Weakly-Supervised Point Cloud Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anh-Thuan Tran",
      "Hoanh-Su Le",
      "Suk-Hwan Lee",
      "Ki-Ryong Kwon"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Khorramshahi_Lightweight_Delivery_Detection_on_Doorbell_Cameras_WACV_2024_paper.html": {
    "title": "Lightweight Delivery Detection on Doorbell Cameras",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pirazh Khorramshahi",
      "Zhe Wu",
      "Tianchen Wang",
      "Luke DeLuccia",
      "Hongcheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Jaziri_Designing_a_Hybrid_Neural_System_To_Learn_Real-World_Crack_Segmentation_WACV_2024_paper.html": {
    "title": "Designing a Hybrid Neural System To Learn Real-World Crack Segmentation From Fractal-Based Simulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Achref Jaziri",
      "Martin Mundt",
      "Andres Fernandez",
      "Visvanathan Ramesh"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Karaderi_Deep_Visual-Genetic_Biometrics_for_Taxonomic_Classification_of_Rare_Species_WACV_2024_paper.html": {
    "title": "Deep Visual-Genetic Biometrics for Taxonomic Classification of Rare Species",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tayfun Karaderi",
      "Tilo Burghardt",
      "Raphaël Morard",
      "Daniela N. Schmidt"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Handformer2T_A_Lightweight_Regression-Based_Model_for_Interacting_Hands_Pose_Estimation_WACV_2024_paper.html": {
    "title": "Handformer2T: A Lightweight Regression-Based Model for Interacting Hands Pose Estimation From a Single RGB Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengfei Zhang",
      "Deying Kong"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Yan_Universal_Semi-Supervised_Model_Adaptation_via_Collaborative_Consistency_Training_WACV_2024_paper.html": {
    "title": "Universal Semi-Supervised Model Adaptation via Collaborative Consistency Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zizheng Yan",
      "Yushuang Wu",
      "Yipeng Qin",
      "Xiaoguang Han",
      "Shuguang Cui",
      "Guanbin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Marsden_Universal_Test-Time_Adaptation_Through_Weight_Ensembling_Diversity_Weighting_and_Prior_WACV_2024_paper.html": {
    "title": "Universal Test-Time Adaptation Through Weight Ensembling, Diversity Weighting, and Prior Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robert A. Marsden",
      "Mario Döbler",
      "Bin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Siddiqui_Uncertainty_Estimation_in_Instance_Segmentation_With_Star-Convex_Shapes_WACV_2024_paper.html": {
    "title": "Uncertainty Estimation in Instance Segmentation With Star-Convex Shapes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qasim M. K. Siddiqui",
      "Sebastian Starke",
      "Peter Steinbach"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kobayashi_Spatio-Temporal_Filter_Analysis_Improves_3D-CNN_for_Action_Classification_WACV_2024_paper.html": {
    "title": "Spatio-Temporal Filter Analysis Improves 3D-CNN for Action Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takumi Kobayashi",
      "Jiaxing Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Chopin_Bipartite_Graph_Diffusion_Model_for_Human_Interaction_Generation_WACV_2024_paper.html": {
    "title": "Bipartite Graph Diffusion Model for Human Interaction Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baptiste Chopin",
      "Hao Tang",
      "Mohamed Daoudi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Mei_Latent_Feature-Guided_Diffusion_Models_for_Shadow_Removal_WACV_2024_paper.html": {
    "title": "Latent Feature-Guided Diffusion Models for Shadow Removal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kangfu Mei",
      "Luis Figueroa",
      "Zhe Lin",
      "Zhihong Ding",
      "Scott Cohen",
      "Vishal M. Patel"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Weilharter_HAMMER_Learning_Entropy_Maps_To_Create_Accurate_3D_Models_in_WACV_2024_paper.html": {
    "title": "HAMMER: Learning Entropy Maps To Create Accurate 3D Models in Multi-View Stereo",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rafael Weilharter",
      "Friedrich Fraundorfer"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Park_Localization_and_Manipulation_of_Immoral_Visual_Cues_for_Safe_Text-to-Image_WACV_2024_paper.html": {
    "title": "Localization and Manipulation of Immoral Visual Cues for Safe Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seongbeom Park",
      "Suhong Moon",
      "Seunghyun Park",
      "Jinkyu Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Liu_Dynamic_Token-Pass_Transformers_for_Semantic_Segmentation_WACV_2024_paper.html": {
    "title": "Dynamic Token-Pass Transformers for Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuang Liu",
      "Qiang Zhou",
      "Jing Wang",
      "Zhibin Wang",
      "Fan Wang",
      "Jun Wang",
      "Wei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Rahman_MIST_Medical_Image_Segmentation_Transformer_With_Convolutional_Attention_Mixing_CAM_WACV_2024_paper.html": {
    "title": "MIST: Medical Image Segmentation Transformer With Convolutional Attention Mixing (CAM) Decoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Motiur Rahman",
      "Shiva Shokouhmand",
      "Smriti Bhatt",
      "Miad Faezipour"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Yamani_Active_Learning_for_Single-Stage_Object_Detection_in_UAV_Images_WACV_2024_paper.html": {
    "title": "Active Learning for Single-Stage Object Detection in UAV Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Asma Yamani",
      "Albandari Alyami",
      "Hamzah Luqman",
      "Bernard Ghanem",
      "Silvio Giancola"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Jeevan_WaveMixSR_Resource-Efficient_Neural_Network_for_Image_Super-Resolution_WACV_2024_paper.html": {
    "title": "WaveMixSR: Resource-Efficient Neural Network for Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pranav Jeevan",
      "Akella Srinidhi",
      "Pasunuri Prathiba",
      "Amit Sethi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Li_Disentangled_Pre-Training_for_Image_Matting_WACV_2024_paper.html": {
    "title": "Disentangled Pre-Training for Image Matting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanda Li",
      "Zilong Huang",
      "Gang Yu",
      "Ling Chen",
      "Yunchao Wei",
      "Jianbo Jiao"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Li_PromptAD_Zero-Shot_Anomaly_Detection_Using_Text_Prompts_WACV_2024_paper.html": {
    "title": "PromptAD: Zero-Shot Anomaly Detection Using Text Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiting Li",
      "Adam Goodge",
      "Fayao Liu",
      "Chuan-Sheng Foo"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Hirsch_Random_Walks_for_Temporal_Action_Segmentation_With_Timestamp_Supervision_WACV_2024_paper.html": {
    "title": "Random Walks for Temporal Action Segmentation With Timestamp Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roy Hirsch",
      "Regev Cohen",
      "Tomer Golany",
      "Daniel Freedman",
      "Ehud Rivlin"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wu_Masked_Collaborative_Contrast_for_Weakly_Supervised_Semantic_Segmentation_WACV_2024_paper.html": {
    "title": "Masked Collaborative Contrast for Weakly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangwen Wu",
      "Jingxuan He",
      "Yufei Yin",
      "Yanbin Hao",
      "Gang Huang",
      "Lechao Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kanebako_Critical_Gap_Between_Generalization_Error_and_Empirical_Error_in_Active_WACV_2024_paper.html": {
    "title": "Critical Gap Between Generalization Error and Empirical Error in Active Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yusuke Kanebako"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Lee_Semi-Supervised_Scene_Change_Detection_by_Distillation_From_Feature-Metric_Alignment_WACV_2024_paper.html": {
    "title": "Semi-Supervised Scene Change Detection by Distillation From Feature-Metric Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seonhoon Lee",
      "Jong-Hwan Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Park_Point-DynRF_Point-Based_Dynamic_Radiance_Fields_From_a_Monocular_Video_WACV_2024_paper.html": {
    "title": "Point-DynRF: Point-Based Dynamic Radiance Fields From a Monocular Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Byeongjun Park",
      "Changick Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Lee_Re-VoxelDet_Rethinking_Neck_and_Head_Architectures_for_High-Performance_Voxel-Based_3D_WACV_2024_paper.html": {
    "title": "Re-VoxelDet: Rethinking Neck and Head Architectures for High-Performance Voxel-Based 3D Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jae-Keun Lee",
      "Jin-Hee Lee",
      "Joohyun Lee",
      "Soon Kwon",
      "Heechul Jung"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Paruchuri_Motion_Matters_Neural_Motion_Transfer_for_Better_Camera_Physiological_Measurement_WACV_2024_paper.html": {
    "title": "Motion Matters: Neural Motion Transfer for Better Camera Physiological Measurement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshay Paruchuri",
      "Xin Liu",
      "Yulu Pan",
      "Shwetak Patel",
      "Daniel McDuff",
      "Soumyadip Sengupta"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Shimoda_Towards_Diverse_and_Consistent_Typography_Generation_WACV_2024_paper.html": {
    "title": "Towards Diverse and Consistent Typography Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wataru Shimoda",
      "Daichi Haraguchi",
      "Seiichi Uchida",
      "Kota Yamaguchi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Darestani_IR-FRestormer_Iterative_Refinement_With_Fourier-Based_Restormer_for_Accelerated_MRI_Reconstruction_WACV_2024_paper.html": {
    "title": "IR-FRestormer: Iterative Refinement With Fourier-Based Restormer for Accelerated MRI Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Zalbagi Darestani",
      "Vishwesh Nath",
      "Wenqi Li",
      "Yufan He",
      "Holger R. Roth",
      "Ziyue Xu",
      "Daguang Xu",
      "Reinhard Heckel",
      "Can Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Shu_Deep_Plug-and-Play_Nighttime_Non-Blind_Deblurring_With_Saturated_Pixel_Handling_Schemes_WACV_2024_paper.html": {
    "title": "Deep Plug-and-Play Nighttime Non-Blind Deblurring With Saturated Pixel Handling Schemes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hung-Yu Shu",
      "Yi-Hsien Lin",
      "Yi-Chang Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Manandhar_One_Style_Is_All_You_Need_To_Generate_a_Video_WACV_2024_paper.html": {
    "title": "One Style Is All You Need To Generate a Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sandeep Manandhar",
      "Auguste Genovesio"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Mori_Wino_Vidi_Vici_Conquering_Numerical_Instability_of_8-Bit_Winograd_Convolution_WACV_2024_paper.html": {
    "title": "Wino Vidi Vici: Conquering Numerical Instability of 8-Bit Winograd Convolution for Accurate Inference Acceleration on Edge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pierpaolo Mori",
      "Lukas Frickenstein",
      "Shambhavi Balamuthu Sampath",
      "Moritz Thoma",
      "Nael Fasfous",
      "Manoj Rohit Vemparala",
      "Alexander Frickenstein",
      "Christian Unger",
      "Walter Stechele",
      "Daniel Mueller-Gritschneder",
      "Claudio Passerone"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ehrlich_Leveraging_Bitstream_Metadata_for_Fast_Accurate_Generalized_Compressed_Video_Quality_WACV_2024_paper.html": {
    "title": "Leveraging Bitstream Metadata for Fast, Accurate, Generalized Compressed Video Quality Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Max Ehrlich",
      "Jon Barker",
      "Namitha Padmanabhan",
      "Larry Davis",
      "Andrew Tao",
      "Bryan Catanzaro",
      "Abhinav Shrivastava"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wodlinger_ECSIC_Epipolar_Cross_Attention_for_Stereo_Image_Compression_WACV_2024_paper.html": {
    "title": "ECSIC: Epipolar Cross Attention for Stereo Image Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthias Wödlinger",
      "Jan Kotera",
      "Manuel Keglevic",
      "Jan Xu",
      "Robert Sablatnig"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Georgiou_FacadeNet_Conditional_Facade_Synthesis_via_Selective_Editing_WACV_2024_paper.html": {
    "title": "FacadeNet: Conditional Facade Synthesis via Selective Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiangos Georgiou",
      "Marios Loizou",
      "Tom Kelly",
      "Melinos Averkiou"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ren_VEATIC_Video-Based_Emotion_and_Affect_Tracking_in_Context_Dataset_WACV_2024_paper.html": {
    "title": "VEATIC: Video-Based Emotion and Affect Tracking in Context Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihang Ren",
      "Jefferson Ortega",
      "Yifan Wang",
      "Zhimin Chen",
      "Yunhui Guo",
      "Stella X. Yu",
      "David Whitney"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Yang_SimpliMix_A_Simplified_Manifold_Mixup_for_Few-Shot_Point_Cloud_Classification_WACV_2024_paper.html": {
    "title": "SimpliMix: A Simplified Manifold Mixup for Few-Shot Point Cloud Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minmin Yang",
      "Weiheng Chai",
      "Jiyang Wang",
      "Senem Velipasalar"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Han_ProxEdit_Improving_Tuning-Free_Real_Image_Editing_With_Proximal_Guidance_WACV_2024_paper.html": {
    "title": "ProxEdit: Improving Tuning-Free Real Image Editing With Proximal Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ligong Han",
      "Song Wen",
      "Qi Chen",
      "Zhixing Zhang",
      "Kunpeng Song",
      "Mengwei Ren",
      "Ruijiang Gao",
      "Anastasis Stathopoulos",
      "Xiaoxiao He",
      "Yuxiao Chen",
      "Di Liu",
      "Qilong Zhangli",
      "Jindong Jiang",
      "Zhaoyang Xia",
      "Akash Srivastava",
      "Dimitris Metaxas"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Nayman_Diverse_Imagenet_Models_Transfer_Better_WACV_2024_paper.html": {
    "title": "Diverse Imagenet Models Transfer Better",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niv Nayman",
      "Avram Golbert",
      "Asaf Noy",
      "Lihi Zelnik-Manor"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Huang_SOAP_Cross-Sensor_Domain_Adaptation_for_3D_Object_Detection_Using_Stationary_WACV_2024_paper.html": {
    "title": "SOAP: Cross-Sensor Domain Adaptation for 3D Object Detection Using Stationary Object Aggregation Pseudo-Labelling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengjie Huang",
      "Vahdat Abdelzad",
      "Sean Sedwards",
      "Krzysztof Czarnecki"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Park_Layer-Wise_Auto-Weighting_for_Non-Stationary_Test-Time_Adaptation_WACV_2024_paper.html": {
    "title": "Layer-Wise Auto-Weighting for Non-Stationary Test-Time Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyoung Park",
      "Jin Kim",
      "Hyeongjun Kwon",
      "Ilhoon Yoon",
      "Kwanghoon Sohn"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/DInca_Improving_Fairness_Using_Vision-Language_Driven_Image_Augmentation_WACV_2024_paper.html": {
    "title": "Improving Fairness Using Vision-Language Driven Image Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moreno D'Incà",
      "Christos Tzelepis",
      "Ioannis Patras",
      "Nicu Sebe"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Brahimi_SupeRVol_Super-Resolution_Shape_and_Reflectance_Estimation_in_Inverse_Volume_Rendering_WACV_2024_paper.html": {
    "title": "SupeRVol: Super-Resolution Shape and Reflectance Estimation in Inverse Volume Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammed Brahimi",
      "Bjoern Haefner",
      "Tarun Yenamandra",
      "Bastian Goldluecke",
      "Daniel Cremers"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Mathur_Object_Aware_Contrastive_Prior_for_Interactive_Image_Segmentation_WACV_2024_paper.html": {
    "title": "Object Aware Contrastive Prior for Interactive Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Praful Mathur",
      "Shashi Kumar Parwani",
      "Mrinmoy Sen",
      "Roopa Sheshadri",
      "Aman Sharma"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Gupta_Torque_Based_Structured_Pruning_for_Deep_Neural_Network_WACV_2024_paper.html": {
    "title": "Torque Based Structured Pruning for Deep Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arshita Gupta",
      "Tien Bau",
      "Joonsoo Kim",
      "Zhe Zhu",
      "Sumit Jha",
      "Hrishikesh Garud"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Instruct_Me_More_Random_Prompting_for_Visual_In-Context_Learning_WACV_2024_paper.html": {
    "title": "Instruct Me More! Random Prompting for Visual In-Context Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Zhang",
      "Bowen Wang",
      "Liangzhi Li",
      "Yuta Nakashima",
      "Hajime Nagahara"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Madan_CL-MAE_Curriculum-Learned_Masked_Autoencoders_WACV_2024_paper.html": {
    "title": "CL-MAE: Curriculum-Learned Masked Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Neelu Madan",
      "Nicolae-Cătălin Ristea",
      "Kamal Nasrollahi",
      "Thomas B. Moeslund",
      "Radu Tudor Ionescu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ishay_Think_Before_You_Simulate_Symbolic_Reasoning_To_Orchestrate_Neural_Computation_WACV_2024_paper.html": {
    "title": "Think Before You Simulate: Symbolic Reasoning To Orchestrate Neural Computation for Counterfactual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adam Ishay",
      "Zhun Yang",
      "Joohyung Lee",
      "Ilgu Kang",
      "Dongjae Lim"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Gupta_Robust_Object_Detection_in_Challenging_Weather_Conditions_WACV_2024_paper.html": {
    "title": "Robust Object Detection in Challenging Weather Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Himanshu Gupta",
      "Oleksandr Kotlyar",
      "Henrik Andreasson",
      "Achim J. Lilienthal"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Okuyama_DiffBody_Diffusion-Based_Pose_and_Shape_Editing_of_Human_Images_WACV_2024_paper.html": {
    "title": "DiffBody: Diffusion-Based Pose and Shape Editing of Human Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuta Okuyama",
      "Yuki Endo",
      "Yoshihiro Kanamori"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/He_Sound3DVDet_3D_Sound_Source_Detection_Using_Multiview_Microphone_Array_and_WACV_2024_paper.html": {
    "title": "Sound3DVDet: 3D Sound Source Detection Using Multiview Microphone Array and RGB Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang He",
      "Sangyun Shin",
      "Anoop Cherian",
      "Niki Trigoni",
      "Andrew Markham"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Liu_Annotation-Free_Audio-Visual_Segmentation_WACV_2024_paper.html": {
    "title": "Annotation-Free Audio-Visual Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinxiang Liu",
      "Yu Wang",
      "Chen Ju",
      "Chaofan Ma",
      "Ya Zhang",
      "Weidi Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Juyal_SC-MIL_Supervised_Contrastive_Multiple_Instance_Learning_for_Imbalanced_Classification_in_WACV_2024_paper.html": {
    "title": "SC-MIL: Supervised Contrastive Multiple Instance Learning for Imbalanced Classification in Pathology",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dinkar Juyal",
      "Siddhant Shingi",
      "Syed Ashar Javed",
      "Harshith Padigela",
      "Chintan Shah",
      "Anand Sampat",
      "Archit Khosla",
      "John Abel",
      "Amaro Taylor-Weiner"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kang_MetaSeg_MetaFormer-Based_Global_Contexts-Aware_Network_for_Efficient_Semantic_Segmentation_WACV_2024_paper.html": {
    "title": "MetaSeg: MetaFormer-Based Global Contexts-Aware Network for Efficient Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Beoungwoo Kang",
      "Seunghun Moon",
      "Yubin Cho",
      "Hyunwoo Yu",
      "Suk-Ju Kang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Guermal_JOADAA_Joint_Online_Action_Detection_and_Action_Anticipation_WACV_2024_paper.html": {
    "title": "JOADAA: Joint Online Action Detection and Action Anticipation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammed Guermal",
      "Abid Ali",
      "Rui Dai",
      "François Brémond"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Yu_Denoising_and_Selecting_Pseudo-Heatmaps_for_Semi-Supervised_Human_Pose_Estimation_WACV_2024_paper.html": {
    "title": "Denoising and Selecting Pseudo-Heatmaps for Semi-Supervised Human Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoran Yu",
      "Manchen Wang",
      "Yanbei Chen",
      "Paolo Favaro",
      "Davide Modolo"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Hung_CSAM_A_2.5D_Cross-Slice_Attention_Module_for_Anisotropic_Volumetric_Medical_WACV_2024_paper.html": {
    "title": "CSAM: A 2.5D Cross-Slice Attention Module for Anisotropic Volumetric Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Ling Yu Hung",
      "Haoxin Zheng",
      "Kai Zhao",
      "Xiaoxi Du",
      "Kaifeng Pang",
      "Qi Miao",
      "Steven S. Raman",
      "Demetri Terzopoulos",
      "Kyunghyun Sung"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ren_Segment_Anything_From_Space_WACV_2024_paper.html": {
    "title": "Segment Anything, From Space?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simiao Ren",
      "Francesco Luzi",
      "Saad Lahrichi",
      "Kaleb Kassaw",
      "Leslie M. Collins",
      "Kyle Bradbury",
      "Jordan M. Malof"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Bui_UOW-Vessel_A_Benchmark_Dataset_of_High-Resolution_Optical_Satellite_Images_for_WACV_2024_paper.html": {
    "title": "UOW-Vessel: A Benchmark Dataset of High-Resolution Optical Satellite Images for Vessel Detection and Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ly Bui",
      "Son Lam Phung",
      "Yang Di",
      "Thanh Le",
      "Tran Thanh Phong Nguyen",
      "Sandy Burden",
      "Abdesselam Bouzerdoum"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Guttikonda_Single_Frame_Semantic_Segmentation_Using_Multi-Modal_Spherical_Images_WACV_2024_paper.html": {
    "title": "Single Frame Semantic Segmentation Using Multi-Modal Spherical Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suresh Guttikonda",
      "Jason Rambach"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Singh_SynthProv_Interpretable_Framework_for_Profiling_Identity_Leakage_WACV_2024_paper.html": {
    "title": "SynthProv: Interpretable Framework for Profiling Identity Leakage",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaisidh Singh",
      "Harshil Bhatia",
      "Mayank Vatsa",
      "Richa Singh",
      "Aparna Bharati"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Tanjim_Discovering_and_Mitigating_Biases_in_CLIP-Based_Image_Editing_WACV_2024_paper.html": {
    "title": "Discovering and Mitigating Biases in CLIP-Based Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Mehrab Tanjim",
      "Krishna Kumar Singh",
      "Kushal Kafle",
      "Ritwik Sinha",
      "Garrison W. Cottrell"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Li_Repetitive_Action_Counting_With_Motion_Feature_Learning_WACV_2024_paper.html": {
    "title": "Repetitive Action Counting With Motion Feature Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinjie Li",
      "Huijuan Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Sinitsa_Deep_Image_Fingerprint_Towards_Low_Budget_Synthetic_Image_Detection_and_WACV_2024_paper.html": {
    "title": "Deep Image Fingerprint: Towards Low Budget Synthetic Image Detection and Model Lineage Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sergey Sinitsa",
      "Ohad Fried"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Biswas_HALSIE_Hybrid_Approach_to_Learning_Segmentation_by_Simultaneously_Exploiting_Image_WACV_2024_paper.html": {
    "title": "HALSIE: Hybrid Approach to Learning Segmentation by Simultaneously Exploiting Image and Event Modalities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shristi Das Biswas",
      "Adarsh Kosta",
      "Chamika Liyanagedera",
      "Marco Apolinario",
      "Kaushik Roy"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Lu_Hierarchical_Diffusion_Autoencoders_and_Disentangled_Image_Manipulation_WACV_2024_paper.html": {
    "title": "Hierarchical Diffusion Autoencoders and Disentangled Image Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyu Lu",
      "Chengyue Wu",
      "Xinyuan Chen",
      "Yaohui Wang",
      "Lei Bai",
      "Yu Qiao",
      "Xihui Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Shamsi_Improved_Topological_Preservation_in_3D_Axon_Segmentation_and_Centerline_Detection_WACV_2024_paper.html": {
    "title": "Improved Topological Preservation in 3D Axon Segmentation and Centerline Detection Using Geometric Assessment-Driven Topological Smoothing (GATS)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nina I. Shamsi",
      "Alec S. Xu",
      "Lars A. Gjesteby",
      "Laura J. Brattain"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Destro_CycleCL_Self-Supervised_Learning_for_Periodic_Videos_WACV_2024_paper.html": {
    "title": "CycleCL: Self-Supervised Learning for Periodic Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matteo Destro",
      "Michael Gygli"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/ElHabebe_DR10K_Transfer_Learning_Using_Weak_Labels_for_Grading_Diabetic_Retinopathy_WACV_2024_paper.html": {
    "title": "DR10K: Transfer Learning Using Weak Labels for Grading Diabetic Retinopathy on DR10K Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed ElHabebe",
      "Shereen ElKordi",
      "Ahmed Gamal ElDin",
      "Noha Adly",
      "Marwan Torki",
      "Ahmed Elmassry",
      "Islam SH Ahmed"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Open-NeRF_Towards_Open_Vocabulary_NeRF_Decomposition_WACV_2024_paper.html": {
    "title": "Open-NeRF: Towards Open Vocabulary NeRF Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Zhang",
      "Fang Li",
      "Narendra Ahuja"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Everaert_Exploiting_the_Signal-Leak_Bias_in_Diffusion_Models_WACV_2024_paper.html": {
    "title": "Exploiting the Signal-Leak Bias in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Martin Nicolas Everaert",
      "Athanasios Fitsios",
      "Marco Bocchio",
      "Sami Arpa",
      "Sabine Süsstrunk",
      "Radhakrishna Achanta"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Bar-Shalom_Weakly-Supervised_Representation_Learning_for_Video_Alignment_and_Analysis_WACV_2024_paper.html": {
    "title": "Weakly-Supervised Representation Learning for Video Alignment and Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guy Bar-Shalom",
      "George Leifman",
      "Michael Elad"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Cha_NCIS_Neural_Contextual_Iterative_Smoothing_for_Purifying_Adversarial_Perturbations_WACV_2024_paper.html": {
    "title": "NCIS: Neural Contextual Iterative Smoothing for Purifying Adversarial Perturbations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungmin Cha",
      "Naeun Ko",
      "Heewoong Choi",
      "Youngjoon Yoo",
      "Taesup Moon"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhang_D3GU_Multi-Target_Active_Domain_Adaptation_via_Enhancing_Domain_Alignment_WACV_2024_paper.html": {
    "title": "D3GU: Multi-Target Active Domain Adaptation via Enhancing Domain Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Zhang",
      "Linghan Xu",
      "Saman Motamed",
      "Shayok Chakraborty",
      "Fernando De la Torre"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Barral_Fixed_Pattern_Noise_Removal_for_Multi-View_Single-Sensor_Infrared_Camera_WACV_2024_paper.html": {
    "title": "Fixed Pattern Noise Removal for Multi-View Single-Sensor Infrared Camera",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arnaud Barral",
      "Pablo Arias",
      "Axel Davy"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wang_Painterly_Image_Harmonization_via_Adversarial_Residual_Learning_WACV_2024_paper.html": {
    "title": "Painterly Image Harmonization via Adversarial Residual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xudong Wang",
      "Li Niu",
      "Junyan Cao",
      "Yan Hong",
      "Liqing Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Verma_CoD_Coherent_Detection_of_Entities_From_Images_With_Multiple_Modalities_WACV_2024_paper.html": {
    "title": "CoD: Coherent Detection of Entities From Images With Multiple Modalities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vinay Verma",
      "Dween Sanny",
      "Abhishek Singh",
      "Deepak Gupta"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Hart_Improving_Graph_Networks_Through_Selection-Based_Convolution_WACV_2024_paper.html": {
    "title": "Improving Graph Networks Through Selection-Based Convolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Hart",
      "Bryan Morse"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Xie_Beyond_Fusion_Modality_Hallucination-Based_Multispectral_Fusion_for_Pedestrian_Detection_WACV_2024_paper.html": {
    "title": "Beyond Fusion: Modality Hallucination-Based Multispectral Fusion for Pedestrian Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Xie",
      "Ta-Ying Cheng",
      "Jia-Xing Zhong",
      "Kaichen Zhou",
      "Andrew Markham",
      "Niki Trigoni"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Conde_BSRAW_Improving_Blind_RAW_Image_Super-Resolution_WACV_2024_paper.html": {
    "title": "BSRAW: Improving Blind RAW Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marcos V. Conde",
      "Florin Vasluianu",
      "Radu Timofte"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Sani_SICKLE_A_Multi-Sensor_Satellite_Imagery_Dataset_Annotated_With_Multiple_Key_WACV_2024_paper.html": {
    "title": "SICKLE: A Multi-Sensor Satellite Imagery Dataset Annotated With Multiple Key Cropping Parameters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Depanshu Sani",
      "Sandeep Mahato",
      "Sourabh Saini",
      "Harsh Kumar Agarwal",
      "Charu Chandra Devshali",
      "Saket Anand",
      "Gaurav Arora",
      "Thiagarajan Jayaraman"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Corneanu_LatentPaint_Image_Inpainting_in_Latent_Space_With_Diffusion_Models_WACV_2024_paper.html": {
    "title": "LatentPaint: Image Inpainting in Latent Space With Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ciprian Corneanu",
      "Raghudeep Gadde",
      "Aleix M. Martinez"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kim_Efficient_Semantic_Matching_With_Hypercolumn_Correlation_WACV_2024_paper.html": {
    "title": "Efficient Semantic Matching With Hypercolumn Correlation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungwook Kim",
      "Juhong Min",
      "Minsu Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ahuja_OptFlow_Fast_Optimization-Based_Scene_Flow_Estimation_Without_Supervision_WACV_2024_paper.html": {
    "title": "OptFlow: Fast Optimization-Based Scene Flow Estimation Without Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rahul Ahuja",
      "Chris Baker",
      "Wilko Schwarting"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Nikhal_HashReID_Dynamic_Network_With_Binary_Codes_for_Efficient_Person_Re-Identification_WACV_2024_paper.html": {
    "title": "HashReID: Dynamic Network With Binary Codes for Efficient Person Re-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kshitij Nikhal",
      "Yujunrong Ma",
      "Shuvra S. Bhattacharyya",
      "Benjamin S. Riggan"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Shi_Conditional_Velocity_Score_Estimation_for_Image_Restoration_WACV_2024_paper.html": {
    "title": "Conditional Velocity Score Estimation for Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqiang Shi",
      "Rujie Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Jain_Stochastic_Binary_Network_for_Universal_Domain_Adaptation_WACV_2024_paper.html": {
    "title": "Stochastic Binary Network for Universal Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saurabh Kumar Jain",
      "Sukhendu Das"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Yin_FG-Net_Facial_Action_Unit_Detection_With_Generalizable_Pyramidal_Features_WACV_2024_paper.html": {
    "title": "FG-Net: Facial Action Unit Detection With Generalizable Pyramidal Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufeng Yin",
      "Di Chang",
      "Guoxian Song",
      "Shen Sang",
      "Tiancheng Zhi",
      "Jing Liu",
      "Linjie Luo",
      "Mohammad Soleymani"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Gu_Complex_Organ_Mask_Guided_Radiology_Report_Generation_WACV_2024_paper.html": {
    "title": "Complex Organ Mask Guided Radiology Report Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiancheng Gu",
      "Dongnan Liu",
      "Zhiyuan Li",
      "Weidong Cai"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wittmann_Link_Prediction_for_Flow-Driven_Spatial_Networks_WACV_2024_paper.html": {
    "title": "Link Prediction for Flow-Driven Spatial Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bastian Wittmann",
      "Johannes C. Paetzold",
      "Chinmay Prabhakar",
      "Daniel Rueckert",
      "Bjoern Menze"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Shi_Training-Free_Object_Counting_With_Prompts_WACV_2024_paper.html": {
    "title": "Training-Free Object Counting With Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zenglin Shi",
      "Ying Sun",
      "Mengmi Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Nagar_SEMA_Semantic_Attention_for_Capturing_Long-Range_Dependencies_in_Egocentric_Lifelogs_WACV_2024_paper.html": {
    "title": "SEMA: Semantic Attention for Capturing Long-Range Dependencies in Egocentric Lifelogs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pravin Nagar",
      "K.N. Ajay Shastry",
      "Jayesh Chaudhari",
      "Chetan Arora"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Jiang_Neural_Image_Compression_Using_Masked_Sparse_Visual_Representation_WACV_2024_paper.html": {
    "title": "Neural Image Compression Using Masked Sparse Visual Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Jiang",
      "Wei Wang",
      "Yue Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Chen_Letting_3D_Guide_the_Way_3D_Guided_2D_Few-Shot_Image_WACV_2024_paper.html": {
    "title": "Letting 3D Guide the Way: 3D Guided 2D Few-Shot Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiajing Chen",
      "Minmin Yang",
      "Senem Velipasalar"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Iwaguchi_Specular_Object_Reconstruction_Behind_Frosted_Glass_by_Differentiable_Rendering_WACV_2024_paper.html": {
    "title": "Specular Object Reconstruction Behind Frosted Glass by Differentiable Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takafumi Iwaguchi",
      "Hiroyuki Kubo",
      "Hiroshi Kawasaki"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Nguyen_Debiasing_Calibrating_and_Improving_Semi-Supervised_Learning_Performance_via_Simple_Ensemble_WACV_2024_paper.html": {
    "title": "Debiasing, Calibrating, and Improving Semi-Supervised Learning Performance via Simple Ensemble Projector",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khanh-Binh Nguyen"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kansal_Privacy-Enhancing_Person_Re-Identification_Framework_-_A_Dual-Stage_Approach_WACV_2024_paper.html": {
    "title": "Privacy-Enhancing Person Re-Identification Framework - A Dual-Stage Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kajal Kansal",
      "Yongkang Wong",
      "Mohan Kankanhalli"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Scheurer_Detection_Defenses_An_Empty_Promise_Against_Adversarial_Patch_Attacks_on_WACV_2024_paper.html": {
    "title": "Detection Defenses: An Empty Promise Against Adversarial Patch Attacks on Optical Flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Erik Scheurer",
      "Jenny Schmalfuss",
      "Alexander Lis",
      "Andrés Bruhn"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Xie_SAM_Fewshot_Finetuning_for_Anatomical_Segmentation_in_Medical_Images_WACV_2024_paper.html": {
    "title": "SAM Fewshot Finetuning for Anatomical Segmentation in Medical Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiyi Xie",
      "Nathalie Willems",
      "Shubham Patil",
      "Yang Li",
      "Mayank Kumar"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Sastry_BirdSAT_Cross-View_Contrastive_Masked_Autoencoders_for_Bird_Species_Classification_and_WACV_2024_paper.html": {
    "title": "BirdSAT: Cross-View Contrastive Masked Autoencoders for Bird Species Classification and Mapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Srikumar Sastry",
      "Subash Khanal",
      "Aayush Dhakal",
      "Di Huang",
      "Nathan Jacobs"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhuo_FELGA_Unsupervised_Fragment_Embedding_for_Fine-Grained_Cross-Modal_Association_WACV_2024_paper.html": {
    "title": "FELGA: Unsupervised Fragment Embedding for Fine-Grained Cross-Modal Association",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaoxin Zhuo",
      "Baoxin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Tantaru_Weakly-Supervised_Deepfake_Localization_in_Diffusion-Generated_Images_WACV_2024_paper.html": {
    "title": "Weakly-Supervised Deepfake Localization in Diffusion-Generated Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dragoș-Constantin Țânțaru",
      "Elisabeta Oneață",
      "Dan Oneață"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Woo_Sketch-Based_Video_Object_Localization_WACV_2024_paper.html": {
    "title": "Sketch-Based Video Object Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sangmin Woo",
      "So-Yeong Jeon",
      "Jinyoung Park",
      "Minji Son",
      "Sumin Lee",
      "Changick Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Moreira_Hyperbolic_vs_Euclidean_Embeddings_in_Few-Shot_Learning_Two_Sides_of_WACV_2024_paper.html": {
    "title": "Hyperbolic vs Euclidean Embeddings in Few-Shot Learning: Two Sides of the Same Coin",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriel Moreira",
      "Manuel Marques",
      "João Paulo Costeira",
      "Alexander Hauptmann"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Raswa_Attention-Guided_Prototype_Mixing_Diversifying_Minority_Context_on_Imbalanced_Whole_Slide_WACV_2024_paper.html": {
    "title": "Attention-Guided Prototype Mixing: Diversifying Minority Context on Imbalanced Whole Slide Images Classification Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Farchan Hakim Raswa",
      "Chun-Shien Lu",
      "Jia-Ching Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Perez_StyleAvatar_Stylizing_Animatable_Head_Avatars_WACV_2024_paper.html": {
    "title": "StyleAvatar: Stylizing Animatable Head Avatars",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juan C. Pérez",
      "Thu Nguyen-Phuoc",
      "Chen Cao",
      "Artsiom Sanakoyeu",
      "Tomas Simon",
      "Pablo Arbeláez",
      "Bernard Ghanem",
      "Ali Thabet",
      "Albert Pumarola"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhang_On_the_Quantification_of_Image_Reconstruction_Uncertainty_Without_Training_Data_WACV_2024_paper.html": {
    "title": "On the Quantification of Image Reconstruction Uncertainty Without Training Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxin Zhang",
      "Sirui Bi",
      "Victor Fung"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wang_Sparse_Convolutional_Networks_for_Surface_Reconstruction_From_Noisy_Point_Clouds_WACV_2024_paper.html": {
    "title": "Sparse Convolutional Networks for Surface Reconstruction From Noisy Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Wang",
      "Jing Wu",
      "Ze Ji",
      "Yu-Kun Lai"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Leng_Self-Sampling_Meta_SAM_Enhancing_Few-Shot_Medical_Image_Segmentation_With_Meta-Learning_WACV_2024_paper.html": {
    "title": "Self-Sampling Meta SAM: Enhancing Few-Shot Medical Image Segmentation With Meta-Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianang Leng",
      "Yiming Zhang",
      "Kun Han",
      "Xiaohui Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Makwana_LIVENet_A_Novel_Network_for_Real-World_Low-Light_Image_Denoising_and_WACV_2024_paper.html": {
    "title": "LIVENet: A Novel Network for Real-World Low-Light Image Denoising and Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dhruv Makwana",
      "Gayatri Deshmukh",
      "Onkar Susladkar",
      "Sparsh Mittal",
      "Sai Chandra Teja R."
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Stergiou_Holistic_Representation_Learning_for_Multitask_Trajectory_Anomaly_Detection_WACV_2024_paper.html": {
    "title": "Holistic Representation Learning for Multitask Trajectory Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexandros Stergiou",
      "Brent De Weerdt",
      "Nikos Deligiannis"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Hasan_Text-Guided_Face_Recognition_Using_Multi-Granularity_Cross-Modal_Contrastive_Learning_WACV_2024_paper.html": {
    "title": "Text-Guided Face Recognition Using Multi-Granularity Cross-Modal Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Mahedi Hasan",
      "Shoaib Meraj Sami",
      "Nasser Nasrabadi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Smink_Computer_Vision_on_the_Edge_Individual_Cattle_Identification_in_Real-Time_WACV_2024_paper.html": {
    "title": "Computer Vision on the Edge: Individual Cattle Identification in Real-Time With ReadMyCow System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moniek Smink",
      "Haotian Liu",
      "Dörte Döpfer",
      "Yong Jae Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Gorade_SynergyNet_Bridging_the_Gap_Between_Discrete_and_Continuous_Representations_for_WACV_2024_paper.html": {
    "title": "SynergyNet: Bridging the Gap Between Discrete and Continuous Representations for Precise Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vandan Gorade",
      "Sparsh Mittal",
      "Debesh Jha",
      "Ulas Bagci"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Sun_Rethinking_Visibility_in_Human_Pose_Estimation_Occluded_Pose_Reasoning_via_WACV_2024_paper.html": {
    "title": "Rethinking Visibility in Human Pose Estimation: Occluded Pose Reasoning via Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengzhan Sun",
      "Kerui Gu",
      "Yunsong Wang",
      "Linlin Yang",
      "Angela Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Jahedi_CCMR_High_Resolution_Optical_Flow_Estimation_via_Coarse-To-Fine_Context-Guided_Motion_WACV_2024_paper.html": {
    "title": "CCMR: High Resolution Optical Flow Estimation via Coarse-To-Fine Context-Guided Motion Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Azin Jahedi",
      "Maximilian Luz",
      "Marc Rivinius",
      "Andrés Bruhn"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Hyun_ReConPatch_Contrastive_Patch_Representation_Learning_for_Industrial_Anomaly_Detection_WACV_2024_paper.html": {
    "title": "ReConPatch: Contrastive Patch Representation Learning for Industrial Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeeho Hyun",
      "Sangyun Kim",
      "Giyoung Jeon",
      "Seung Hwan Kim",
      "Kyunghoon Bae",
      "Byung Jun Kang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Huix_Are_Natural_Domain_Foundation_Models_Useful_for_Medical_Image_Classification_WACV_2024_paper.html": {
    "title": "Are Natural Domain Foundation Models Useful for Medical Image Classification?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joana Palés Huix",
      "Adithya Raju Ganeshan",
      "Johan Fredin Haslum",
      "Magnus Söderberg",
      "Christos Matsoukas",
      "Kevin Smith"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Shastry_Favoring_One_Among_Equals_-_Not_a_Good_Idea_Many-to-One_WACV_2024_paper.html": {
    "title": "Favoring One Among Equals - Not a Good Idea: Many-to-One Matching for Robust Transformer Based Pedestrian Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "K.N. Ajay Shastry",
      "K. Ravi Sri Teja",
      "Aditya Nigam",
      "Chetan Arora"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ma_MACP_Efficient_Model_Adaptation_for_Cooperative_Perception_WACV_2024_paper.html": {
    "title": "MACP: Efficient Model Adaptation for Cooperative Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunsheng Ma",
      "Juanwu Lu",
      "Can Cui",
      "Sicheng Zhao",
      "Xu Cao",
      "Wenqian Ye",
      "Ziran Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Szatkowski_Adapt_Your_Teacher_Improving_Knowledge_Distillation_for_Exemplar-Free_Continual_Learning_WACV_2024_paper.html": {
    "title": "Adapt Your Teacher: Improving Knowledge Distillation for Exemplar-Free Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Filip Szatkowski",
      "Mateusz Pyla",
      "Marcin Przewięźlikowski",
      "Sebastian Cygert",
      "Bartłomiej Twardowski",
      "Tomasz Trzciński"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ren_Content-Aware_Image_Color_Editing_With_Auxiliary_Color_Restoration_Tasks_WACV_2024_paper.html": {
    "title": "Content-Aware Image Color Editing With Auxiliary Color Restoration Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixuan Ren",
      "Jing Shi",
      "Zhifei Zhang",
      "Yifei Fan",
      "Zhe Lin",
      "Bo He",
      "Abhinav Shrivastava"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Gao_Self-Supervised_Representation_Learning_With_Cross-Context_Learning_Between_Global_and_Hypercolumn_WACV_2024_paper.html": {
    "title": "Self-Supervised Representation Learning With Cross-Context Learning Between Global and Hypercolumn Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Gao",
      "Chen Feng",
      "Ioannis Patras"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Weber_Constrained_Probabilistic_Mask_Learning_for_Task-Specific_Undersampled_MRI_Reconstruction_WACV_2024_paper.html": {
    "title": "Constrained Probabilistic Mask Learning for Task-Specific Undersampled MRI Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobias Weber",
      "Michael Ingrisch",
      "Bernd Bischl",
      "David Rügamer"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Li_CPSeg_Finer-Grained_Image_Semantic_Segmentation_via_Chain-of-Thought_Language_Prompting_WACV_2024_paper.html": {
    "title": "CPSeg: Finer-Grained Image Semantic Segmentation via Chain-of-Thought Language Prompting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Li"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wang_Hyb-NeRF_A_Multiresolution_Hybrid_Encoding_for_Neural_Radiance_Fields_WACV_2024_paper.html": {
    "title": "Hyb-NeRF: A Multiresolution Hybrid Encoding for Neural Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Wang",
      "Yi Gong",
      "Yuan Zeng"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Nguyen_SequenceMatch_Revisiting_the_Design_of_Weak-Strong_Augmentations_for_Semi-Supervised_Learning_WACV_2024_paper.html": {
    "title": "SequenceMatch: Revisiting the Design of Weak-Strong Augmentations for Semi-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khanh-Binh Nguyen"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Nguyen_Robust_Learning_via_Conditional_Prevalence_Adjustment_WACV_2024_paper.html": {
    "title": "Robust Learning via Conditional Prevalence Adjustment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minh Nguyen",
      "Alan Q. Wang",
      "Heejong Kim",
      "Mert R. Sabuncu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Suri_GRIT_GAN_Residuals_for_Paired_Image-to-Image_Translation_WACV_2024_paper.html": {
    "title": "GRIT: GAN Residuals for Paired Image-to-Image Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saksham Suri",
      "Moustafa Meshry",
      "Larry S. Davis",
      "Abhinav Shrivastava"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Hu_Embodied_Human_Activity_Recognition_WACV_2024_paper.html": {
    "title": "Embodied Human Activity Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sha Hu",
      "Yu Gong",
      "Greg Mori"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kazerouni_INCODE_Implicit_Neural_Conditioning_With_Prior_Knowledge_Embeddings_WACV_2024_paper.html": {
    "title": "INCODE: Implicit Neural Conditioning With Prior Knowledge Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amirhossein Kazerouni",
      "Reza Azad",
      "Alireza Hosseini",
      "Dorit Merhof",
      "Ulas Bagci"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Niloy_Effective_Restoration_of_Source_Knowledge_in_Continual_Test_Time_Adaptation_WACV_2024_paper.html": {
    "title": "Effective Restoration of Source Knowledge in Continual Test Time Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fahim Faisal Niloy",
      "Sk Miraj Ahmed",
      "Dripta S. Raychaudhuri",
      "Samet Oymak",
      "Amit K. Roy-Chowdhury"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Fulari_Unsupervised_Model-Based_Learning_for_Simultaneous_Video_Deflickering_and_Deblotching_WACV_2024_paper.html": {
    "title": "Unsupervised Model-Based Learning for Simultaneous Video Deflickering and Deblotching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anuj Fulari",
      "Satish Mulleti",
      "Ajit Rajwade"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Westfechtel_Gradual_Source_Domain_Expansion_for_Unsupervised_Domain_Adaptation_WACV_2024_paper.html": {
    "title": "Gradual Source Domain Expansion for Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Westfechtel",
      "Hao-Wei Yeh",
      "Dexuan Zhang",
      "Tatsuya Harada"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Sun_Towards_Better_Structured_Pruning_Saliency_by_Reorganizing_Convolution_WACV_2024_paper.html": {
    "title": "Towards Better Structured Pruning Saliency by Reorganizing Convolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinglong Sun",
      "Humphrey Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kim_Controllable_Text-to-Image_Synthesis_for_Multi-Modality_MR_Images_WACV_2024_paper.html": {
    "title": "Controllable Text-to-Image Synthesis for Multi-Modality MR Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyuri Kim",
      "Yoonho Na",
      "Sung-Joon Ye",
      "Jimin Lee",
      "Sung Soo Ahn",
      "Ji Eun Park",
      "Hwiyoung Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhu_CATS_Combined_Activation_and_Temporal_Suppression_for_Efficient_Network_Inference_WACV_2024_paper.html": {
    "title": "CATS: Combined Activation and Temporal Suppression for Efficient Network Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeqi Zhu",
      "Arash Pourtaherian",
      "Luc Waeijen",
      "Ibrahim Batuhan Akkaya",
      "Egor Bondarev",
      "Orlando Moreira"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ishikawa_Learnable_Cube-Based_Video_Encryption_for_Privacy-Preserving_Action_Recognition_WACV_2024_paper.html": {
    "title": "Learnable Cube-Based Video Encryption for Privacy-Preserving Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchi Ishikawa",
      "Masayoshi Kondo",
      "Hirokatsu Kataoka"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Hariat_Learning_To_Generate_Training_Datasets_for_Robust_Semantic_Segmentation_WACV_2024_paper.html": {
    "title": "Learning To Generate Training Datasets for Robust Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marwane Hariat",
      "Olivier Laurent",
      "Rémi Kazmierczak",
      "Shihao Zhang",
      "Andrei Bursuc",
      "Angela Yao",
      "Gianni Franchi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Mehl_Stereo_Conversion_With_Disparity-Aware_Warping_Compositing_and_Inpainting_WACV_2024_paper.html": {
    "title": "Stereo Conversion With Disparity-Aware Warping, Compositing and Inpainting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lukas Mehl",
      "Andrés Bruhn",
      "Markus Gross",
      "Christopher Schroers"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Xu_GTP-ViT_Efficient_Vision_Transformers_via_Graph-Based_Token_Propagation_WACV_2024_paper.html": {
    "title": "GTP-ViT: Efficient Vision Transformers via Graph-Based Token Propagation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuwei Xu",
      "Sen Wang",
      "Yudong Chen",
      "Yanping Zheng",
      "Zhewei Wei",
      "Jiajun Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kapitanov_HaGRID_--_HAnd_Gesture_Recognition_Image_Dataset_WACV_2024_paper.html": {
    "title": "HaGRID -- HAnd Gesture Recognition Image Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Kapitanov",
      "Karina Kvanchiani",
      "Alexander Nagaev",
      "Roman Kraynov",
      "Andrei Makhliarchuk"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Glatt_Beyond_RGB_A_Real_World_Dataset_for_Multispectral_Imaging_in_WACV_2024_paper.html": {
    "title": "Beyond RGB: A Real World Dataset for Multispectral Imaging in Mobile Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ortal Glatt",
      "Yotam Ater",
      "Woo-Shik Kim",
      "Shira Werman",
      "Oded Berby",
      "Yael Zini",
      "Shay Zelinger",
      "Sangyoon Lee",
      "Heejin Choi",
      "Evgeny Soloveichik"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhong_Lightweight_Portrait_Matting_via_Regional_Attention_and_Refinement_WACV_2024_paper.html": {
    "title": "Lightweight Portrait Matting via Regional Attention and Refinement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yatao Zhong",
      "Ilya Zharkov"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Shao_Analyzing_the_Domain_Shift_Immunity_of_Deep_Homography_Estimation_WACV_2024_paper.html": {
    "title": "Analyzing the Domain Shift Immunity of Deep Homography Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingzhen Shao",
      "Tolga Tasdizen",
      "Sarang Joshi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Sivasubramanian_Gradient_Coreset_for_Federated_Learning_WACV_2024_paper.html": {
    "title": "Gradient Coreset for Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Durga Sivasubramanian",
      "Lokesh Nagalapatti",
      "Rishabh Iyer",
      "Ganesh Ramakrishnan"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Huang_Semantic_Fusion_Augmentation_and_Semantic_Boundary_Detection_A_Novel_Approach_WACV_2024_paper.html": {
    "title": "Semantic Fusion Augmentation and Semantic Boundary Detection: A Novel Approach to Multi-Target Video Moment Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Huang",
      "Yi-Lun Wu",
      "Hong-Han Shuai",
      "Ching-Chun Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Burges_CHAI_Craters_in_Historical_Aerial_Images_WACV_2024_paper.html": {
    "title": "CHAI: Craters in Historical Aerial Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marvin Burges",
      "Sebastian Zambanini",
      "Philipp Pirker"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Aakerberg_PDA-RWSR_Pixel-Wise_Degradation_Adaptive_Real-World_Super-Resolution_WACV_2024_paper.html": {
    "title": "PDA-RWSR: Pixel-Wise Degradation Adaptive Real-World Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas Aakerberg",
      "Majed El Helou",
      "Kamal Nasrollahi",
      "Thomas Moeslund"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Luo_Zero-Shot_Video_Moment_Retrieval_From_Frozen_Vision-Language_Models_WACV_2024_paper.html": {
    "title": "Zero-Shot Video Moment Retrieval From Frozen Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dezhao Luo",
      "Jiabo Huang",
      "Shaogang Gong",
      "Hailin Jin",
      "Yang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Krull_Image_Denoising_and_the_Generative_Accumulation_of_Photons_WACV_2024_paper.html": {
    "title": "Image Denoising and the Generative Accumulation of Photons",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Krull",
      "Hector Basevi",
      "Benjamin Salmon",
      "Andre Zeug",
      "Franziska Müller",
      "Samuel Tonks",
      "Leela Muppala",
      "Aleš Leonardis"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Shah_Ordinal_Classification_With_Distance_Regularization_for_Robust_Brain_Age_Prediction_WACV_2024_paper.html": {
    "title": "Ordinal Classification With Distance Regularization for Robust Brain Age Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jay Shah",
      "Md Mahfuzur Rahman Siddiquee",
      "Yi Su",
      "Teresa Wu",
      "Baoxin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wen_The_Growing_Strawberries_Dataset_Tracking_Multiple_Objects_With_Biological_Development_WACV_2024_paper.html": {
    "title": "The Growing Strawberries Dataset: Tracking Multiple Objects With Biological Development Over an Extended Period",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhan Wen",
      "Camiel R. Verschoor",
      "Chengming Feng",
      "Irina-Mona Epure",
      "Thomas Abeel",
      "Mathijs de Weerdt"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Fang_Face_Presentation_Attack_Detection_by_Excavating_Causal_Clues_and_Adapting_WACV_2024_paper.html": {
    "title": "Face Presentation Attack Detection by Excavating Causal Clues and Adapting Embedding Statistics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meiling Fang",
      "Naser Damer"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Xiong_Glance_To_Count_Learning_To_Rank_With_Anchors_for_Weakly-Supervised_WACV_2024_paper.html": {
    "title": "Glance To Count: Learning To Rank With Anchors for Weakly-Supervised Crowd Counting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Xiong",
      "Liangyu Chai",
      "Wenxi Liu",
      "Yongtuo Liu",
      "Sucheng Ren",
      "Shengfeng He"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Lan_Gradient-Guided_Knowledge_Distillation_for_Object_Detectors_WACV_2024_paper.html": {
    "title": "Gradient-Guided Knowledge Distillation for Object Detectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qizhen Lan",
      "Qing Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Tarsi_SciOL_and_MuLMS-Img_Introducing_a_Large-Scale_Multimodal_Scientific_Dataset_and_WACV_2024_paper.html": {
    "title": "SciOL and MuLMS-Img: Introducing a Large-Scale Multimodal Scientific Dataset and Models for Image-Text Tasks in the Scientific Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Tarsi",
      "Heike Adel",
      "Jan Hendrik Metzen",
      "Dan Zhang",
      "Matteo Finco",
      "Annemarie Friedrich"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Tahmasebzadeh_Few-Shot_Event_Classification_in_Images_Using_Knowledge_Graphs_for_Prompting_WACV_2024_paper.html": {
    "title": "Few-Shot Event Classification in Images Using Knowledge Graphs for Prompting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Golsa Tahmasebzadeh",
      "Matthias Springstein",
      "Ralph Ewerth",
      "Eric Müller-Budack"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Petryk_Simple_Token-Level_Confidence_Improves_Caption_Correctness_WACV_2024_paper.html": {
    "title": "Simple Token-Level Confidence Improves Caption Correctness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suzanne Petryk",
      "Spencer Whitehead",
      "Joseph E. Gonzalez",
      "Trevor Darrell",
      "Anna Rohrbach",
      "Marcus Rohrbach"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ofori-Oduro_Defending_Object_Detection_Models_Against_Image_Distortions_WACV_2024_paper.html": {
    "title": "Defending Object Detection Models Against Image Distortions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mark Ofori-Oduro",
      "Maria Amer"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Khanfir_Graph_Neural_Networks_for_End-to-End_Information_Extraction_From_Handwritten_Documents_WACV_2024_paper.html": {
    "title": "Graph Neural Networks for End-to-End Information Extraction From Handwritten Documents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yessine Khanfir",
      "Marwa Dhiaf",
      "Emna Ghodhbani",
      "Ahmed Cheikh Rouhou",
      "Yousri Kessentini"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Odo_Automated_Monitoring_of_Ear_Biting_in_Pigs_by_Tracking_Individuals_WACV_2024_paper.html": {
    "title": "Automated Monitoring of Ear Biting in Pigs by Tracking Individuals and Events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anicetus Odo",
      "Niall McLaughlin",
      "Ilias Kyriazakis"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Sun_RSMPNet_Relationship_Guided_Semantic_Map_Prediction_WACV_2024_paper.html": {
    "title": "RSMPNet: Relationship Guided Semantic Map Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingwen Sun",
      "Jing Wu",
      "Ze Ji",
      "Yu-Kun Lai"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Dittakavi_CARE_Counterfactual-Based_Algorithmic_Recourse_for_Explainable_Pose_Correction_WACV_2024_paper.html": {
    "title": "CARE: Counterfactual-Based Algorithmic Recourse for Explainable Pose Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bhat Dittakavi",
      "Bharathi Callepalli",
      "Aleti Vardhan",
      "Sai Vikas Desai",
      "Vineeth N. Balasubramanian"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Hekimoglu_Monocular_3D_Object_Detection_With_LiDAR_Guided_Semi_Supervised_Active_WACV_2024_paper.html": {
    "title": "Monocular 3D Object Detection With LiDAR Guided Semi Supervised Active Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aral Hekimoglu",
      "Michael Schmidt",
      "Alvaro Marcos-Ramiro"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Johanson_S3AD_Semi-Supervised_Small_Apple_Detection_in_Orchard_Environments_WACV_2024_paper.html": {
    "title": "S3AD: Semi-Supervised Small Apple Detection in Orchard Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robert Johanson",
      "Christian Wilms",
      "Ole Johannsen",
      "Simone Frintrop"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Li_Task-Oriented_Human-Object_Interactions_Generation_With_Implicit_Neural_Representations_WACV_2024_paper.html": {
    "title": "Task-Oriented Human-Object Interactions Generation With Implicit Neural Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quanzhou Li",
      "Jingbo Wang",
      "Chen Change Loy",
      "Bo Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Yang_Convolutional_Masked_Image_Modeling_for_Dense_Prediction_Tasks_on_Pathology_WACV_2024_paper.html": {
    "title": "Convolutional Masked Image Modeling for Dense Prediction Tasks on Pathology Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Yang",
      "Liyuan Pan",
      "Liu Liu",
      "Eric A. Stone"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Li_Controlling_Virtual_Try-On_Pipeline_Through_Rendering_Policies_WACV_2024_paper.html": {
    "title": "Controlling Virtual Try-On Pipeline Through Rendering Policies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kedan Li",
      "Jeffrey Zhang",
      "Shao-Yu Chang",
      "David Forsyth"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wan_Interpretable_Object_Recognition_by_Semantic_Prototype_Analysis_WACV_2024_paper.html": {
    "title": "Interpretable Object Recognition by Semantic Prototype Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiyang Wan",
      "Ruiping Wang",
      "Xilin Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Sharma_Assist_Is_Just_As_Important_as_the_Goal_Image_Resurfacing_WACV_2024_paper.html": {
    "title": "Assist Is Just As Important as the Goal: Image Resurfacing To Aid Model's Robust Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhijith Sharma",
      "Phil Munz",
      "Apurva Narayan"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Murugesan_Prompting_Classes_Exploring_the_Power_of_Prompt_Class_Learning_in_WACV_2024_paper.html": {
    "title": "Prompting Classes: Exploring the Power of Prompt Class Learning in Weakly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Balamurali Murugesan",
      "Rukhshanda Hussain",
      "Rajarshi Bhattacharya",
      "Ismail Ben Ayed",
      "Jose Dolz"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wen_From_Denoising_Training_To_Test-Time_Adaptation_Enhancing_Domain_Generalization_for_WACV_2024_paper.html": {
    "title": "From Denoising Training To Test-Time Adaptation: Enhancing Domain Generalization for Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruxue Wen",
      "Hangjie Yuan",
      "Dong Ni",
      "Wenbo Xiao",
      "Yaoyao Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Kim_Implicit_Neural_Image_Stitching_With_Enhanced_and_Blended_Feature_Reconstruction_WACV_2024_paper.html": {
    "title": "Implicit Neural Image Stitching With Enhanced and Blended Feature Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minsu Kim",
      "Jaewon Lee",
      "Byeonghun Lee",
      "Sunghoon Im",
      "Kyong Hwan Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Teng_360BEV_Panoramic_Semantic_Mapping_for_Indoor_Birds-Eye_View_WACV_2024_paper.html": {
    "title": "360BEV: Panoramic Semantic Mapping for Indoor Bird's-Eye View",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhifeng Teng",
      "Jiaming Zhang",
      "Kailun Yang",
      "Kunyu Peng",
      "Hao Shi",
      "Simon Reiß",
      "Ke Cao",
      "Rainer Stiefelhagen"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Rahman_Semi-Supervised_Semantic_Depth_Estimation_Using_Symbiotic_Transformer_and_NearFarMix_Augmentation_WACV_2024_paper.html": {
    "title": "Semi-Supervised Semantic Depth Estimation Using Symbiotic Transformer and NearFarMix Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Awsafur Rahman",
      "Shaikh Anowarul Fattah"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Tripathi_Query-Guided_Attention_in_Vision_Transformers_for_Localizing_Objects_Using_a_WACV_2024_paper.html": {
    "title": "Query-Guided Attention in Vision Transformers for Localizing Objects Using a Single Sketch",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditay Tripathi",
      "Anand Mishra",
      "Anirban Chakraborty"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Pham_I-AI_A_Controllable__Interpretable_AI_System_for_Decoding_Radiologists_WACV_2024_paper.html": {
    "title": "I-AI: A Controllable & Interpretable AI System for Decoding Radiologists' Intense Focus for Accurate CXR Diagnoses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Trong Thang Pham",
      "Jacob Brecheisen",
      "Anh Nguyen",
      "Hien Nguyen",
      "Ngan Le"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Suin_Diffuse_and_Restore_A_Region-Adaptive_Diffusion_Model_for_Identity-Preserving_Blind_WACV_2024_paper.html": {
    "title": "Diffuse and Restore: A Region-Adaptive Diffusion Model for Identity-Preserving Blind Face Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maitreya Suin",
      "Nithin Gopalakrishnan Nair",
      "Chun Pong Lau",
      "Vishal M. Patel",
      "Rama Chellappa"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Roy_Interaction_Region_Visual_Transformer_for_Egocentric_Action_Anticipation_WACV_2024_paper.html": {
    "title": "Interaction Region Visual Transformer for Egocentric Action Anticipation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Debaditya Roy",
      "Ramanathan Rajendiran",
      "Basura Fernando"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Preserving_Image_Properties_Through_Initializations_in_Diffusion_Models_WACV_2024_paper.html": {
    "title": "Preserving Image Properties Through Initializations in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeffrey Zhang",
      "Shao-Yu Chang",
      "Kedan Li",
      "David Forsyth"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Das_Limited_Data_Unlimited_Potential_A_Study_on_ViTs_Augmented_by_WACV_2024_paper.html": {
    "title": "Limited Data, Unlimited Potential: A Study on ViTs Augmented by Masked Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Srijan Das",
      "Tanmay Jain",
      "Dominick Reilly",
      "Pranav Balaji",
      "Soumyajit Karmakar",
      "Shyam Marjit",
      "Xiang Li",
      "Abhijit Das",
      "Michael S. Ryoo"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ahmed_Unsupervised_Co-Generation_of_Foreground-Background_Segmentation_From_Text-to-Image_Synthesis_WACV_2024_paper.html": {
    "title": "Unsupervised Co-Generation of Foreground-Background Segmentation From Text-to-Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yeruru Asrar Ahmed",
      "Anurag Mittal"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ardelean_High-Fidelity_Zero-Shot_Texture_Anomaly_Localization_Using_Feature_Correspondence_Analysis_WACV_2024_paper.html": {
    "title": "High-Fidelity Zero-Shot Texture Anomaly Localization Using Feature Correspondence Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrei-Timotei Ardelean",
      "Tim Weyrich"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Long_Hierarchical_Text_Spotter_for_Joint_Text_Spotting_and_Layout_Analysis_WACV_2024_paper.html": {
    "title": "Hierarchical Text Spotter for Joint Text Spotting and Layout Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shangbang Long",
      "Siyang Qin",
      "Yasuhisa Fujii",
      "Alessandro Bissacco",
      "Michalis Raptis"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Law_Label-Free_Synthetic_Pretraining_of_Object_Detectors_WACV_2024_paper.html": {
    "title": "Label-Free Synthetic Pretraining of Object Detectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hei Law",
      "Jia Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Haalck_Tracking_Tiny_Insects_in_Cluttered_Natural_Environments_Using_Refinable_Recurrent_WACV_2024_paper.html": {
    "title": "Tracking Tiny Insects in Cluttered Natural Environments Using Refinable Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lars Haalck",
      "Sebastian Thiele",
      "Benjamin Risse"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Deevi_RGB-X_Object_Detection_via_Scene-Specific_Fusion_Modules_WACV_2024_paper.html": {
    "title": "RGB-X Object Detection via Scene-Specific Fusion Modules",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sri Aditya Deevi",
      "Connor Lee",
      "Lu Gan",
      "Sushruth Nagesh",
      "Gaurav Pandey",
      "Soon-Jo Chung"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ni_3D-Aware_Talking-Head_Video_Motion_Transfer_WACV_2024_paper.html": {
    "title": "3D-Aware Talking-Head Video Motion Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haomiao Ni",
      "Jiachen Liu",
      "Yuan Xue",
      "Sharon X. Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Shyam_Lightweight_Thermal_Super-Resolution_and_Object_Detection_for_Robust_Perception_in_WACV_2024_paper.html": {
    "title": "Lightweight Thermal Super-Resolution and Object Detection for Robust Perception in Adverse Weather Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pranjay Shyam",
      "HyunJin Yoo"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Qin_Revolutionize_the_Oceanic_Drone_RGB_Imagery_With_Pioneering_Sun_Glint_WACV_2024_paper.html": {
    "title": "Revolutionize the Oceanic Drone RGB Imagery With Pioneering Sun Glint Detection and Removal Techniques",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiangying Qin",
      "Ming Li",
      "Jie Zhao",
      "Jiageng Zhong",
      "Hanqi Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Pang_Revisiting_Pixel-Level_Contrastive_Pre-Training_on_Scene_Images_WACV_2024_paper.html": {
    "title": "Revisiting Pixel-Level Contrastive Pre-Training on Scene Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zongshang Pang",
      "Yuta Nakashima",
      "Mayu Otani",
      "Hajime Nagahara"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Wang_Neural_Textured_Deformable_Meshes_for_Robust_Analysis-by-Synthesis_WACV_2024_paper.html": {
    "title": "Neural Textured Deformable Meshes for Robust Analysis-by-Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Angtian Wang",
      "Wufei Ma",
      "Alan Yuille",
      "Adam Kortylewski"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Cosma_PsyMo_A_Dataset_for_Estimating_Self-Reported_Psychological_Traits_From_Gait_WACV_2024_paper.html": {
    "title": "PsyMo: A Dataset for Estimating Self-Reported Psychological Traits From Gait",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adrian Cosma",
      "Emilian Radoi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Jiang_Back_to_Optimization_Diffusion-Based_Zero-Shot_3D_Human_Pose_Estimation_WACV_2024_paper.html": {
    "title": "Back to Optimization: Diffusion-Based Zero-Shot 3D Human Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongyu Jiang",
      "Zhuoran Zhou",
      "Lei Li",
      "Wenhao Chai",
      "Cheng-Yen Yang",
      "Jenq-Neng Hwang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ben-Shabat_IKEA_Ego_3D_Dataset_Understanding_Furniture_Assembly_Actions_From_Ego-View_WACV_2024_paper.html": {
    "title": "IKEA Ego 3D Dataset: Understanding Furniture Assembly Actions From Ego-View 3D Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yizhak Ben-Shabat",
      "Jonathan Paul",
      "Eviatar Segev",
      "Oren Shrout",
      "Stephen Gould"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Hong_Concept-Centric_Transformers_Enhancing_Model_Interpretability_Through_Object-Centric_Concept_Learning_Within_WACV_2024_paper.html": {
    "title": "Concept-Centric Transformers: Enhancing Model Interpretability Through Object-Centric Concept Learning Within a Shared Global Workspace",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinyung Hong",
      "Keun Hee Park",
      "Theodore P. Pavlic"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Gothe_Whats_in_the_Flow_Exploiting_Temporal_Motion_Cues_for_Unsupervised_WACV_2024_paper.html": {
    "title": "What's in the Flow? Exploiting Temporal Motion Cues for Unsupervised Generic Event Boundary Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sourabh Vasant Gothe",
      "Vibhav Agarwal",
      "Sourav Ghosh",
      "Jayesh Rajkumar Vachhani",
      "Pranay Kashyap",
      "Barath Raj Kandur Raja"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Song_SyntheWorld_A_Large-Scale_Synthetic_Dataset_for_Land_Cover_Mapping_and_WACV_2024_paper.html": {
    "title": "SyntheWorld: A Large-Scale Synthetic Dataset for Land Cover Mapping and Building Change Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Song",
      "Hongruixuan Chen",
      "Naoto Yokoya"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Galappaththige_Generalizing_to_Unseen_Domains_in_Diabetic_Retinopathy_Classification_WACV_2024_paper.html": {
    "title": "Generalizing to Unseen Domains in Diabetic Retinopathy Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chamuditha Jayanga Galappaththige",
      "Gayal Kuruppu",
      "Muhammad Haris Khan"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Ehret_A_Generic_and_Flexible_Regularization_Framework_for_NeRFs_WACV_2024_paper.html": {
    "title": "A Generic and Flexible Regularization Framework for NeRFs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thibaud Ehret",
      "Roger Marí",
      "Gabriele Facciolo"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Paheding_MarsLS-Net_Martian_Landslides_Segmentation_Network_and_Benchmark_Dataset_WACV_2024_paper.html": {
    "title": "MarsLS-Net: Martian Landslides Segmentation Network and Benchmark Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sidike Paheding",
      "Abel A. Reyes",
      "A. Rajaneesh",
      "K.S. Sajinkumar",
      "Thomas Oommen"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Gupta_You_Can_Run_but_Not_Hide_Improving_Gait_Recognition_With_WACV_2024_paper.html": {
    "title": "You Can Run but Not Hide: Improving Gait Recognition With Intrinsic Occlusion Type Awareness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ayush Gupta",
      "Rama Chellappa"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Gava_SphereCraft_A_Dataset_for_Spherical_Keypoint_Detection_Matching_and_Camera_WACV_2024_paper.html": {
    "title": "SphereCraft: A Dataset for Spherical Keypoint Detection, Matching and Camera Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christiano Gava",
      "Yunmin Cho",
      "Federico Raue",
      "Sebastian Palacio",
      "Alain Pagani",
      "Andreas Dengel"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Weng_Best_of_Both_Worlds_Learning_Arbitrary-Scale_Blind_Super-Resolution_via_Dual_WACV_2024_paper.html": {
    "title": "Best of Both Worlds: Learning Arbitrary-Scale Blind Super-Resolution via Dual Degradation Representations and Cycle-Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shao-Yu Weng",
      "Hsuan Yuan",
      "Yu-Syuan Xu",
      "Ching-Chun Huang",
      "Wei-Chen Chiu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Li_VMFormer_End-to-End_Video_Matting_With_Transformer_WACV_2024_paper.html": {
    "title": "VMFormer: End-to-End Video Matting With Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiachen Li",
      "Vidit Goel",
      "Marianna Ohanyan",
      "Shant Navasardyan",
      "Yunchao Wei",
      "Humphrey Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Bohdal_Feed-Forward_Latent_Domain_Adaptation_WACV_2024_paper.html": {
    "title": "Feed-Forward Latent Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ondrej Bohdal",
      "Da Li",
      "Shell Xu Hu",
      "Timothy Hospedales"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Nie_Triplet_Attention_Transformer_for_Spatiotemporal_Predictive_Learning_WACV_2024_paper.html": {
    "title": "Triplet Attention Transformer for Spatiotemporal Predictive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuesong Nie",
      "Xi Chen",
      "Haoyuan Jin",
      "Zhihang Zhu",
      "Yunfeng Yan",
      "Donglian Qi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Tsai_Arbitrary-Resolution_and_Arbitrary-Scale_Face_Super-Resolution_With_Implicit_Representation_Networks_WACV_2024_paper.html": {
    "title": "Arbitrary-Resolution and Arbitrary-Scale Face Super-Resolution With Implicit Representation Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Ting Tsai",
      "Yu Wei Chen",
      "Hong-Han Shuai",
      "Ching-Chun Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Shoshan_FPGAN-Control_A_Controllable_Fingerprint_Generator_for_Training_With_Synthetic_Data_WACV_2024_paper.html": {
    "title": "FPGAN-Control: A Controllable Fingerprint Generator for Training With Synthetic Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alon Shoshan",
      "Nadav Bhonker",
      "Emanuel Ben Baruch",
      "Ori Nizan",
      "Igor Kviatkovsky",
      "Joshua Engelsma",
      "Manoj Aggarwal",
      "Gérard Medioni"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Chawla_Continual_Learning_of_Unsupervised_Monocular_Depth_From_Videos_WACV_2024_paper.html": {
    "title": "Continual Learning of Unsupervised Monocular Depth From Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hemang Chawla",
      "Arnav Varma",
      "Elahe Arani",
      "Bahram Zonooz"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Shentu_CXR-IRGen_An_Integrated_Vision_and_Language_Model_for_the_Generation_WACV_2024_paper.html": {
    "title": "CXR-IRGen: An Integrated Vision and Language Model for the Generation of Clinically Accurate Chest X-Ray Image-Report Pairs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Shentu",
      "Noura Al Moubayed"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Song_Overcoming_Catastrophic_Forgetting_for_Multi-Label_Class-Incremental_Learning_WACV_2024_paper.html": {
    "title": "Overcoming Catastrophic Forgetting for Multi-Label Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Song",
      "Kuang Shu",
      "Songlin Dong",
      "Jie Cheng",
      "Xing Wei",
      "Yihong Gong"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024/html/Nagendra_PatchRefineNet_Improving_Binary_Segmentation_by_Incorporating_Signals_From_Optimal_Patch-Wise_WACV_2024_paper.html": {
    "title": "PatchRefineNet: Improving Binary Segmentation by Incorporating Signals From Optimal Patch-Wise Binarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Savinay Nagendra",
      "Daniel Kifer"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/CV4Smalls/html/Kholiavchenko_KABR_In-Situ_Dataset_for_Kenyan_Animal_Behavior_Recognition_From_Drone_WACVW_2024_paper.html": {
    "title": "KABR: In-Situ Dataset for Kenyan Animal Behavior Recognition From Drone Videos",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maksim Kholiavchenko",
      "Jenna Kline",
      "Michelle Ramirez",
      "Sam Stevens",
      "Alec Sheets",
      "Reshma Babu",
      "Namrata Banerji",
      "Elizabeth Campolongo",
      "Matthew Thompson",
      "Nina Van Tiel",
      "Jackson Miliko",
      "Eduardo Bessa",
      "Isla Duporge",
      "Tanya Berger-Wolf",
      "Daniel Rubenstein",
      "Charles Stewart"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/CV4Smalls/html/Zhou_Efficient_Domain_Adaptation_via_Generative_Prior_for_3D_Infant_Pose_WACVW_2024_paper.html": {
    "title": "Efficient Domain Adaptation via Generative Prior for 3D Infant Pose Estimation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoran Zhou",
      "Zhongyu Jiang",
      "Wenhao Chai",
      "Cheng-Yen Yang",
      "Lei Li",
      "Jenq-Neng Hwang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/CV4Smalls/html/Nepovinnykh_NORPPA_NOvel_Ringed_Seal_Re-Identification_by_Pelage_Pattern_Aggregation_WACVW_2024_paper.html": {
    "title": "NORPPA: NOvel Ringed Seal Re-Identification by Pelage Pattern Aggregation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ekaterina Nepovinnykh",
      "Tuomas Eerola",
      "Heikki Kälviäinen",
      "Ilia Chelak"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/CV4Smalls/html/Straka_The_Hitchhikers_Guide_to_Endangered_Species_Pose_Estimation_WACVW_2024_paper.html": {
    "title": "The Hitchhiker's Guide to Endangered Species Pose Estimation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jakub Straka",
      "Marek Hruz",
      "Lukas Picek"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/CV4Smalls/html/Shooter_DigiDogs_Single-View_3D_Pose_Estimation_of_Dogs_Using_Synthetic_Training_WACVW_2024_paper.html": {
    "title": "DigiDogs: Single-View 3D Pose Estimation of Dogs Using Synthetic Training Data",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moira Shooter",
      "Charles Malleson",
      "Adrian Hilton"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/CV4Smalls/html/Hatamimajoumerd_Challenges_in_Video-Based_Infant_Action_Recognition_A_Critical_Examination_of_WACVW_2024_paper.html": {
    "title": "Challenges in Video-Based Infant Action Recognition: A Critical Examination of the State of the Art",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elaheh Hatamimajoumerd",
      "Pooria Daneshvar Kakhaki",
      "Xiaofei Huang",
      "Lingfei Luan",
      "Somaieh Amraee",
      "Sarah Ostadabbas"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/CV4Smalls/html/Cotton_Dynamic_Gaussian_Splatting_From_Markerless_Motion_Capture_Reconstruct_Infants_Movements_WACVW_2024_paper.html": {
    "title": "Dynamic Gaussian Splatting From Markerless Motion Capture Reconstruct Infants Movements",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "R. James Cotton",
      "Colleen Peyton"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/CV4Smalls/html/Amraee_Multiple_Toddler_Tracking_in_Indoor_Videos_WACVW_2024_paper.html": {
    "title": "Multiple Toddler Tracking in Indoor Videos",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Somaieh Amraee",
      "Bishoy Galoaa",
      "Matthew Goodwin",
      "Elaheh Hatamimajoumerd",
      "Sarah Ostadabbas"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/CV4Smalls/html/Waldmann_Neural_Texture_Puppeteer_A_Framework_for_Neural_Geometry_and_Texture_WACVW_2024_paper.html": {
    "title": "Neural Texture Puppeteer: A Framework for Neural Geometry and Texture Rendering of Articulated Shapes, Enabling Re-Identification at Interactive Speed",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Urs Waldmann",
      "Ole Johannsen",
      "Bastian Goldluecke"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/CV4Smalls/html/Peng_Learning_Part_Segmentation_From_Synthetic_Animals_WACVW_2024_paper.html": {
    "title": "Learning Part Segmentation From Synthetic Animals",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Peng",
      "Ju He",
      "Prakhar Kaushik",
      "Zihao Xiao",
      "Jiteng Mu",
      "Alan Yuille"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/RWS/html/Madsen_Person_Fall_Detection_Using_Weakly_Supervised_Methods_WACVW_2024_paper.html": {
    "title": "Person Fall Detection Using Weakly Supervised Methods",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kjartan Madsen",
      "Zenjie Li",
      "Francois Lauze",
      "Kamal Nasrollahi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/RWS/html/Bui_C2T-Net_Channel-Aware_Cross-Fused_Transformer-Style_Networks_for_Pedestrian_Attribute_Recognition_WACVW_2024_paper.html": {
    "title": "C2T-Net: Channel-Aware Cross-Fused Transformer-Style Networks for Pedestrian Attribute Recognition",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Doanh C. Bui",
      "Thinh V. Le",
      "Ba Hung Ngo"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/RWS/html/Friederich_Security_Fence_Inspection_at_Airports_Using_Object_Detection_WACVW_2024_paper.html": {
    "title": "Security Fence Inspection at Airports Using Object Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nils Friederich",
      "Andreas Specker"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/RWS/html/Gkrispanis_Filter-Pruning_of_Lightweight_Face_Detectors_Using_a_Geometric_Median_Criterion_WACVW_2024_paper.html": {
    "title": "Filter-Pruning of Lightweight Face Detectors Using a Geometric Median Criterion",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Konstantinos Gkrispanis",
      "Nikolaos Gkalelis",
      "Vasileios Mezaris"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/RWS/html/Nguyen_Temporal_3D_Shape_Modeling_for_Video-Based_Cloth-Changing_Person_Re-Identification_WACVW_2024_paper.html": {
    "title": "Temporal 3D Shape Modeling for Video-Based Cloth-Changing Person Re-Identification",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vuong D. Nguyen",
      "Pranav Mantini",
      "Shishir K. Shah"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/RWS/html/AlMarri_A_Multi-Head_Approach_With_Shuffled_Segments_for_Weakly-Supervised_Video_Anomaly_WACVW_2024_paper.html": {
    "title": "A Multi-Head Approach With Shuffled Segments for Weakly-Supervised Video Anomaly Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Salem AlMarri",
      "Muhammad Zaigham Zaheer",
      "Karthik Nandakumar"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/RWS/html/OBrien_Accenture-MM1_A_Multimodal_Person_Recognition_Dataset_WACVW_2024_paper.html": {
    "title": "Accenture-MM1: A Multimodal Person Recognition Dataset",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyle O'Brien",
      "Michelle Rybak",
      "Jiong Huang",
      "Adam Stevens",
      "Madeline Fredriksz",
      "Michael Chaberski",
      "Danielle Russell",
      "Lindsey Castin",
      "Michelle Jou",
      "Nishant Gurrapadi",
      "Marc Bosch"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/RWS/html/Tu_FedFSLAR_A_Federated_Learning_Framework_for_Few-Shot_Action_Recognition_WACVW_2024_paper.html": {
    "title": "FedFSLAR: A Federated Learning Framework for Few-Shot Action Recognition",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nguyen Anh Tu",
      "Assanali Abu",
      "Nartay Aikyn",
      "Nursultan Makhanov",
      "Min-Ho Lee",
      "Khiem Le-Huy",
      "Kok-Seng Wong"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/RWS/html/Cormier_Enhancing_Skeleton-Based_Action_Recognition_in_Real-World_Scenarios_Through_Realistic_Data_WACVW_2024_paper.html": {
    "title": "Enhancing Skeleton-Based Action Recognition in Real-World Scenarios Through Realistic Data Augmentation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mickael Cormier",
      "Yannik Schmid",
      "Jürgen Beyerer"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/RWS/html/Nunez_Identifying_Loitering_Behavior_With_Trajectory_Analysis_WACVW_2024_paper.html": {
    "title": "Identifying Loitering Behavior With Trajectory Analysis",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Johnny Núñez",
      "Zenjie Li",
      "Sergio Escalera",
      "Kamal Nasrollahi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/RWS/html/Tan_Overlooked_Video_Classification_in_Weakly_Supervised_Video_Anomaly_Detection_WACVW_2024_paper.html": {
    "title": "Overlooked Video Classification in Weakly Supervised Video Anomaly Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijun Tan",
      "Qi Yao",
      "Jingfeng Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/RWS/html/Oliu_Swin_on_Axes_Extending_Swin_Transformers_to_Quadtree_Image_Representations_WACVW_2024_paper.html": {
    "title": "Swin on Axes: Extending Swin Transformers to Quadtree Image Representations",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marc Oliu",
      "Kamal Nasrollahi",
      "Sergio Escalera",
      "Thomas B. Moeslund"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/RWS/html/Blanch_LiDAR-Assisted_3D_Human_Detection_for_Video_Surveillance_WACVW_2024_paper.html": {
    "title": "LiDAR-Assisted 3D Human Detection for Video Surveillance",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miquel Romero Blanch",
      "Zenjie Li",
      "Sergio Escalera",
      "Kamal Nasrollahi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/RWS/html/Wolf_Knowledge-Distillation-Based_Label_Smoothing_for_Fine-Grained_Open-Set_Vehicle_Recognition_WACVW_2024_paper.html": {
    "title": "Knowledge-Distillation-Based Label Smoothing for Fine-Grained Open-Set Vehicle Recognition",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefan Wolf",
      "Dennis Loran",
      "Jürgen Beyerer"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/RWS/html/Arkushin_GEFF_Improving_Any_Clothes-Changing_Person_ReID_Model_Using_Gallery_Enrichment_WACVW_2024_paper.html": {
    "title": "GEFF: Improving Any Clothes-Changing Person ReID Model Using Gallery Enrichment With Face Features",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Arkushin",
      "Bar Cohen",
      "Shmuel Peleg",
      "Ohad Fried"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/RWS/html/Teepe_EarlyBird_Early-Fusion_for_Multi-View_Tracking_in_the_Birds_Eye_View_WACVW_2024_paper.html": {
    "title": "EarlyBird: Early-Fusion for Multi-View Tracking in the Bird's Eye View",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Torben Teepe",
      "Philipp Wolters",
      "Johannes Gilg",
      "Fabian Herzog",
      "Gerhard Rigoll"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/RWS/html/Rahman_Spatio-Temporal_Activity_Detection_via_Joint_Optimization_of_Spatial_and_Temporal_WACVW_2024_paper.html": {
    "title": "Spatio-Temporal Activity Detection via Joint Optimization of Spatial and Temporal Localization",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Atiqur Rahman",
      "Robert Laganière"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/RWS/html/Ha_HOD_New_Harmful_Object_Detection_Benchmarks_for_Robust_Surveillance_WACVW_2024_paper.html": {
    "title": "HOD: New Harmful Object Detection Benchmarks for Robust Surveillance",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eungyeom Ha",
      "Heemook Kim",
      "Dongbin Na"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/RWS/html/Shyam_Enhancing_Self-Supervised_Monocular_Depth_Estimation_via_Piece-Wise_Pose_Estimation_and_WACVW_2024_paper.html": {
    "title": "Enhancing Self-Supervised Monocular Depth Estimation via Piece-Wise Pose Estimation and Geometric Constraints",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pranjay Shyam",
      "Alexandre Okon",
      "HyunJin Yoo"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/RWS/html/Munir_Investigation_of_UAV_Detection_in_Images_With_Complex_Backgrounds_and_WACVW_2024_paper.html": {
    "title": "Investigation of UAV Detection in Images With Complex Backgrounds and Rainy Artifacts",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adnan Munir",
      "Abdul Jabbar Siddiqui",
      "Saeed Anwar"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/RWS/html/Hwang_Aerial_View_3D_Human_Pose_Estimation_Using_Double_Vector_Quantized-Variational_WACVW_2024_paper.html": {
    "title": "Aerial View 3D Human Pose Estimation Using Double Vector Quantized-Variational AutoEncoders",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juheon Hwang",
      "Jiwoo Kang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/RWS/html/Khaldi_Unsupervised_Person_Re-Identification_in_Aerial_Imagery_WACVW_2024_paper.html": {
    "title": "Unsupervised Person Re-Identification in Aerial Imagery",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khadija Khaldi",
      "Vuong D. Nguyen",
      "Pranav Mantini",
      "Shishir Shah"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/RWS/html/Cormier_UPAR_Challenge_2024_Pedestrian_Attribute_Recognition_and_Attribute-Based_Person_Retrieval_WACVW_2024_paper.html": {
    "title": "UPAR Challenge 2024: Pedestrian Attribute Recognition and Attribute-Based Person Retrieval - Dataset, Design, and Results",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mickael Cormier",
      "Andreas Specker",
      "Julio C. S. Jacques Junior",
      "Lennart Moritz",
      "Jürgen Metzler",
      "Thomas B. Moeslund",
      "Kamal Nasrollahi",
      "Sergio Escalera",
      "Jürgen Beyerer"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/RWS/html/Fawakherji_TextAug_Test_Time_Text_Augmentation_for_Multimodal_Person_Re-Identification_WACVW_2024_paper.html": {
    "title": "TextAug: Test Time Text Augmentation for Multimodal Person Re-Identification",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mulham Fawakherji",
      "Eduard Vazquez",
      "Pasquale Giampa",
      "Binod Bhattarai"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/RWS/html/Latortue_Evaluating_Supervision_Levels_Trade-Offs_for_Infrared-Based_People_Counting_WACVW_2024_paper.html": {
    "title": "Evaluating Supervision Levels Trade-Offs for Infrared-Based People Counting",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Latortue",
      "Moetez Kdayem",
      "Fidel A. Guerrero Peña",
      "Eric Granger",
      "Marco Pedersoli"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/RWS/html/Lerch_Unsupervised_3D_Skeleton-Based_Action_Recognition_Using_Cross-Attention_With_Conditioned_Generation_WACVW_2024_paper.html": {
    "title": "Unsupervised 3D Skeleton-Based Action Recognition Using Cross-Attention With Conditioned Generation Capabilities",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David J. Lerch",
      "Zeyun Zhong",
      "Manuel Martin",
      "Michael Voit",
      "Jürgen Beyerer"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/RWS/html/Huang_Iterative_Scale-Up_ExpansionIoU_and_Deep_Features_Association_for_Multi-Object_Tracking_WACVW_2024_paper.html": {
    "title": "Iterative Scale-Up ExpansionIoU and Deep Features Association for Multi-Object Tracking in Sports",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hsiang-Wei Huang",
      "Cheng-Yen Yang",
      "Jiacheng Sun",
      "Pyong-Kun Kim",
      "Kwang-Ju Kim",
      "Kyoungoh Lee",
      "Chung-I Huang",
      "Jenq-Neng Hwang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/VAQ/html/Park_RealPixVSR_Pixel-Level_Visual_Representation_Informed_Super-Resolution_of_Real-World_Videos_WACVW_2024_paper.html": {
    "title": "RealPixVSR: Pixel-Level Visual Representation Informed Super-Resolution of Real-World Videos",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tony Nokap Park",
      "Yunho Jeon",
      "Taeyoung Na"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/VAQ/html/Saini_HIDRO-VQA_High_Dynamic_Range_Oracle_for_Video_Quality_Assessment_WACVW_2024_paper.html": {
    "title": "HIDRO-VQA: High Dynamic Range Oracle for Video Quality Assessment",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shreshth Saini",
      "Avinab Saha",
      "Alan C. Bovik"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/VAQ/html/Daultani_Consolidating_Separate_Degradations_Model_via_Weights_Fusion_and_Distillation_WACVW_2024_paper.html": {
    "title": "Consolidating Separate Degradations Model via Weights Fusion and Distillation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dinesh Daultani",
      "Hugo Larochelle"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/VAQ/html/Poudel_DeepLIR_Attention-Based_Approach_for_Mask-Based_Lensless_Image_Reconstruction_WACVW_2024_paper.html": {
    "title": "DeepLIR: Attention-Based Approach for Mask-Based Lensless Image Reconstruction",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arpan Poudel",
      "Ukash Nakarmi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/VAQ/html/Feng_Noise-Free_Audio_Signal_Processing_in_Noisy_Environment_A_Hardware_and_WACVW_2024_paper.html": {
    "title": "Noise-Free Audio Signal Processing in Noisy Environment: A Hardware and Algorithm Solution",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yarong Feng",
      "Zongyi Liu",
      "Shunyan Luo",
      "Yuan Ling",
      "Shujing Dong",
      "Shuyi Wang",
      "Bruce Ferry"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/VAQ/html/Zhao_A_Lightweight_Generalizable_Evaluation_and_Enhancement_Framework_for_Generative_Models_WACVW_2024_paper.html": {
    "title": "A Lightweight Generalizable Evaluation and Enhancement Framework for Generative Models and Generated Samples",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ganning Zhao",
      "Vasileios Magoulianitis",
      "Suya You",
      "C.-C. Jay Kuo"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/VAQ/html/Bhatta_Impact_of_Blur_and_Resolution_on_Demographic_Disparities_in_1-to-Many_WACVW_2024_paper.html": {
    "title": "Impact of Blur and Resolution on Demographic Disparities in 1-to-Many Facial Identification",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aman Bhatta",
      "Gabriella Pangelinan",
      "Michael C. King",
      "Kevin W. Bowyer"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/VAQ/html/Sharma_Generating_Point_Cloud_Augmentations_via_Class-Conditioned_Diffusion_Model_WACVW_2024_paper.html": {
    "title": "Generating Point Cloud Augmentations via Class-Conditioned Diffusion Model",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gulshan Sharma",
      "Chetan Gupta",
      "Aastha Agarwal",
      "Lalit Sharma",
      "Abhinav Dhall"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/VAQ/html/Gupta_Perceptual_Synchronization_Scoring_of_Dubbed_Content_Using_Phoneme-Viseme_Agreement_WACVW_2024_paper.html": {
    "title": "Perceptual Synchronization Scoring of Dubbed Content Using Phoneme-Viseme Agreement",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Honey Gupta"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/VAQ/html/Lee_AutoCaCoNet_Automatic_Cartoon_Colorization_Network_Using_Self-Attention_GAN_Segmentation_and_WACVW_2024_paper.html": {
    "title": "AutoCaCoNet: Automatic Cartoon Colorization Network Using Self-Attention GAN, Segmentation, and Color Correction",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungpeel Lee",
      "Eunil Park"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/VAQ/html/Freeman_Enhancing_Surveillance_Camera_FOV_Quality_via_Semantic_Line_Detection_and_WACVW_2024_paper.html": {
    "title": "Enhancing Surveillance Camera FOV Quality via Semantic Line Detection and Classification With Deep Hough Transform",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew Freeman",
      "Wenjing Shi",
      "Bin Hwang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/VAQ/html/Wang_A_Diffusion-Based_Method_for_Multi-Turn_Compositional_Image_Generation_WACVW_2024_paper.html": {
    "title": "A Diffusion-Based Method for Multi-Turn Compositional Image Generation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/VAQ/html/Yuan_Inflation_With_Diffusion_Efficient_Temporal_Adaptation_for_Text-to-Video_Super-Resolution_WACVW_2024_paper.html": {
    "title": "Inflation With Diffusion: Efficient Temporal Adaptation for Text-to-Video Super-Resolution",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Yuan",
      "Jinoo Baek",
      "Keyang Xu",
      "Omer Tov",
      "Hongliang Fei"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/VAQ/html/Ma_Super_Efficient_Neural_Network_for_Compression_Artifacts_Reduction_and_Super_WACVW_2024_paper.html": {
    "title": "Super Efficient Neural Network for Compression Artifacts Reduction and Super Resolution",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wen Ma",
      "Qiuwen Lou",
      "Arman Kazemi",
      "Julian Faraone",
      "Tariq Afzal"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/Pretrain/html/Denize_COMEDIAN_Self-Supervised_Learning_and_Knowledge_Distillation_for_Action_Spotting_Using_WACVW_2024_paper.html": {
    "title": "COMEDIAN: Self-Supervised Learning and Knowledge Distillation for Action Spotting Using Transformers",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julien Denize",
      "Mykola Liashuha",
      "Jaonary Rabarisoa",
      "Astrid Orcesi",
      "Romain Hérault"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/Pretrain/html/Shrestha_Self-Supervised_Pre-Training_for_Semantic_Segmentation_in_an_Indoor_Scene_WACVW_2024_paper.html": {
    "title": "Self-Supervised Pre-Training for Semantic Segmentation in an Indoor Scene",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sulabh Shrestha",
      "Yimeng Li",
      "Jana Košecká"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/Pretrain/html/Fang_E-ViLM_Efficient_Video-Language_Model_via_Masked_Video_Modeling_With_Semantic_WACVW_2024_paper.html": {
    "title": "E-ViLM: Efficient Video-Language Model via Masked Video Modeling With Semantic Vector-Quantized Tokenizer",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Fang",
      "Skyler Zheng",
      "Vasu Sharma",
      "Robinson Piramuthu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/Pretrain/html/Katageri_Metric_Learning_for_3D_Point_Clouds_Using_Optimal_Transport_WACVW_2024_paper.html": {
    "title": "Metric Learning for 3D Point Clouds Using Optimal Transport",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siddharth Katageri",
      "Srinjay Sarkar",
      "Charu Sharma"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/Pretrain/html/Seth_Does_the_Fairness_of_Your_Pre-Training_Hold_Up_Examining_the_WACVW_2024_paper.html": {
    "title": "Does the Fairness of Your Pre-Training Hold Up? Examining the Influence of Pre-Training Techniques on Skin Tone Bias in Skin Lesion Classification",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pratinav Seth",
      "Abhilash K. Pai"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/Pretrain/html/Nanduri_Semi-Supervised_Cross-Spectral_Face_Recognition_With_Small_Datasets_WACVW_2024_paper.html": {
    "title": "Semi-Supervised Cross-Spectral Face Recognition With Small Datasets",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anirudh Nanduri",
      "Rama Chellappa"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/Pretrain/html/Li_Labeling_Indoor_Scenes_With_Fusion_of_Out-of-the-Box_Perception_Models_WACVW_2024_paper.html": {
    "title": "Labeling Indoor Scenes With Fusion of Out-of-the-Box Perception Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yimeng Li",
      "Navid Rajabi",
      "Sulabh Shrestha",
      "Reza Alimoor",
      "Jana Košecká"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/Pretrain/html/Zielinski_RDIR_Capturing_Temporally-Invariant_Representations_of_Multiple_Objects_in_Videos_WACVW_2024_paper.html": {
    "title": "RDIR: Capturing Temporally-Invariant Representations of Multiple Objects in Videos",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Piotr Zieliński",
      "Tomasz Kajdanowicz"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/Pretrain/html/Mei_SLVP_Self-Supervised_Language-Video_Pre-Training_for_Referring_Video_Object_Segmentation_WACVW_2024_paper.html": {
    "title": "SLVP: Self-Supervised Language-Video Pre-Training for Referring Video Object Segmentation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Mei",
      "AJ Piergiovanni",
      "Jenq-Neng Hwang",
      "Wei Li"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/Pretrain/html/Zhang_How_Does_Contrastive_Learning_Organize_Images_WACVW_2024_paper.html": {
    "title": "How Does Contrastive Learning Organize Images?",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunzhe Zhang",
      "Yao Lu",
      "Qi Xuan"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/Pretrain/html/Yamagiwa_Zero-Shot_Edge_Detection_With_SCESAME_Spectral_Clustering-Based_Ensemble_for_Segment_WACVW_2024_paper.html": {
    "title": "Zero-Shot Edge Detection With SCESAME: Spectral Clustering-Based Ensemble for Segment Anything Model Estimation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hiroaki Yamagiwa",
      "Yusuke Takase",
      "Hiroyuki Kambe",
      "Ryosuke Nakamoto"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/Pretrain/html/Rizzoli_Source-Free_Domain_Adaptation_for_RGB-D_Semantic_Segmentation_With_Vision_Transformers_WACVW_2024_paper.html": {
    "title": "Source-Free Domain Adaptation for RGB-D Semantic Segmentation With Vision Transformers",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giulia Rizzoli",
      "Donald Shenaj",
      "Pietro Zanuttigh"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/Pretrain/html/Panta_Cross-Modal_Contrastive_Learning_With_Asymmetric_Co-Attention_Network_for_Video_Moment_WACVW_2024_paper.html": {
    "title": "Cross-Modal Contrastive Learning With Asymmetric Co-Attention Network for Video Moment Retrieval",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Love Panta",
      "Prashant Shrestha",
      "Brabeem Sapkota",
      "Amrita Bhattarai",
      "Suresh Manandhar",
      "Anand Kumar Sah"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/Pretrain/html/Lekkala_Evaluating_Pretrained_Models_for_Deployable_Lifelong_Learning_WACVW_2024_paper.html": {
    "title": "Evaluating Pretrained Models for Deployable Lifelong Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kiran Lekkala",
      "Eshan Bhargava",
      "Laurent Itti"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/Pretrain/html/Rangel_A_Unified_Framework_for_Cropland_Field_Boundary_Detection_and_Segmentation_WACVW_2024_paper.html": {
    "title": "A Unified Framework for Cropland Field Boundary Detection and Segmentation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rodrigo Fill Rangel",
      "Vítor Nascimento Lourenço",
      "Lucas Volochen Oldoni",
      "Ana Flavia Carrara Bonamigo",
      "Wallas Santos",
      "Bruno Silva Oliveira",
      "Mateus Neves Barreto"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/SCIoT/html/Peng_MLP_Kernel-Based_To_Predict_the_Optimal_Conditions_of_Transglutaminase_on_WACVW_2024_paper.html": {
    "title": "MLP Kernel-Based To Predict the Optimal Conditions of Transglutaminase on Protein Polymerization",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zengyan Peng",
      "Miao-Hsin Hsu",
      "Dong-Meau Chang",
      "Chun-Chi Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/SCIoT/html/Liu_The_Optimizated_CIELAB_Colour_Model_for_All-Analog_Photoelectronic_High_Speed_WACVW_2024_paper.html": {
    "title": "The Optimizated CIELAB Colour Model for All-Analog Photoelectronic High Speed Vision-Task Chip (ACCEL) by Creative Computing Approach",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinwei Liu",
      "Yuchen Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/SCIoT/html/Yang_Security_and_Privacy_Concerns_in_Information_Usability_WACVW_2024_paper.html": {
    "title": "Security and Privacy Concerns in Information Usability",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liang-Chih Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/SCIoT/html/Tang_Consumer_Evaluation_Using_Machine_Learning_for_the_Predictive_Analysis_of_WACVW_2024_paper.html": {
    "title": "Consumer Evaluation Using Machine Learning for the Predictive Analysis of Consumer Purchase Indicators",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "BaoFu Tang",
      "Dong-Meau Chang",
      "Junjie Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/SCIoT/html/Quelennec_Towards_On-Device_Learning_on_the_Edge_Ways_To_Select_Neurons_WACVW_2024_paper.html": {
    "title": "Towards On-Device Learning on the Edge: Ways To Select Neurons To Update Under a Budget Constraint",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aël Quélennec",
      "Enzo Tartaglione",
      "Pavlo Mozharovskyi",
      "Van-Tam Nguyen"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/SCIoT/html/Zhang_Image_Detection_of_Rare_Orthopedic_Diseases_Based_on_Explainable_AI_WACVW_2024_paper.html": {
    "title": "Image Detection of Rare Orthopedic Diseases Based on Explainable AI",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi-Xiang Zhang",
      "Shun-Ping Wang",
      "Yu-Wei Chan",
      "Chih-Hung Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/SCIoT/html/Liang_Semi-Supervised_SPO_Tree_Classifier_Based_on_the_DPC_Framework_WACVW_2024_paper.html": {
    "title": "Semi-Supervised SPO Tree Classifier Based on the DPC Framework",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhou Liang",
      "Liqiong Lu",
      "Junjie Yang",
      "Weiming Hong",
      "Dong-Meau Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/SCIoT/html/Hsu_Designing_a_Secure_and_Scalable_Service_Model_Using_Blockchain_and_WACVW_2024_paper.html": {
    "title": "Designing a Secure and Scalable Service Model Using Blockchain and MQTT for IoT Devices",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tse-Chuan Hsu",
      "Han-Sheng Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/SCIoT/html/Yang_Colour_Creation_Muse_CCM_Focusing_on_Primary_Colours_for_an_WACVW_2024_paper.html": {
    "title": "Colour Creation Muse (CCM): Focusing on Primary Colours for an Imagination Based Creativity Generation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongji Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/SCIoT/html/Tsai_An_Effective_Deep_Neural_Network_in_Edge_Computing_Enabled_Internet_WACVW_2024_paper.html": {
    "title": "An Effective Deep Neural Network in Edge Computing Enabled Internet of Things for Plant Diseases Monitoring",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao-Hong Tsai",
      "Tse-Chuan Hsu"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/DACL/html/Flotzinger_Dacl-Challenge_Semantic_Segmentation_During_Visual_Bridge_Inspections_WACVW_2024_paper.html": {
    "title": "Dacl-Challenge: Semantic Segmentation During Visual Bridge Inspections",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Johannes Flotzinger",
      "Philipp J. Rösch",
      "Christian Benz",
      "Muneer Ahmad",
      "Murat Cankaya",
      "Helmut Mayer",
      "Volker Rodehorst",
      "Norbert Oswald",
      "Thomas Braml"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/PRAW/html/Kikuchi_Self-Supervised_Human-Object_Interaction_of_Complex_Scenes_With_Context-Aware_Mixing_Towards_WACVW_2024_paper.html": {
    "title": "Self-Supervised Human-Object Interaction of Complex Scenes With Context-Aware Mixing: Towards In-Store Consumer Behavior Analysis",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takashi Kikuchi",
      "Shun Takeuchi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/PRAW/html/Bhattacharya_PMTL_A_Progressive_Multi-Level_Training_Framework_for_Retail_Taxonomy_Classification_WACVW_2024_paper.html": {
    "title": "PMTL: A Progressive Multi-Level Training Framework for Retail Taxonomy Classification",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaurab Bhattacharya",
      "Gaurav Sharma",
      "Kallol Chatterjee",
      "Chakrapani",
      "Bagya Lakshmi V",
      "Jayavardhana Gubbi",
      "Arpan Pal",
      "Ramachandran Rajagopalan"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/CDL/html/Luckett_The_SARFish_Dataset_and_Challenge_WACVW_2024_paper.html": {
    "title": "The SARFish Dataset and Challenge",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Connor Luckett",
      "Benjamin McCarthy",
      "Tri-Tan Cao",
      "Antonio Robles-Kelly"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/3D4Science/html/de_Chanlatte_Proceedings_of_the_Workshop_on_3D_Geometry_Generation_for_Scientific_WACVW_2024_paper.html": {
    "title": "Proceedings of the Workshop on 3D Geometry Generation for Scientific Computing",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marissa Ramirez de Chanlatte",
      "Phil Colella",
      "Trevor Darrell",
      "Alexandra Katherine Carlson",
      "Peter H. N. de With",
      "Huayu Deng",
      "Shanyan Guan",
      "James Hays",
      "Tim Houben",
      "Thomas Huisman",
      "Nikita Jaipuria",
      "Hans Johansen",
      "Shuja Khalid",
      "Akshay Krishnan",
      "Chuming Li",
      "Maxim Pisarenco",
      "Amit Raj",
      "Frank Rudzicz",
      "Tim J. Schoonbeek",
      "Sandhya Sridhar",
      "Nathan Tseng",
      "Fons van der Sommen",
      "Chen Wang",
      "Yunbo Wang",
      "Tong Wu",
      "Xiaokang Yang",
      "Jiawei Yao",
      "Derek Young",
      "Xianling Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/CV4EO/html/Straka_Modernized_Training_of_U-Net_for_Aerial_Semantic_Segmentation_WACVW_2024_paper.html": {
    "title": "Modernized Training of U-Net for Aerial Semantic Segmentation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jakub Straka",
      "Ivan Gruber"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/CV4EO/html/Khomiakov_GAST_Geometry-Aware_Structure_Transformer_WACVW_2024_paper.html": {
    "title": "GAST: Geometry-Aware Structure Transformer",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maxim Khomiakov",
      "Michael Riis Andersen",
      "Jes Frellsen"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/CV4EO/html/Zhu_TinyWT_A_Large-Scale_Wind_Turbine_Dataset_of_Satellite_Images_for_WACVW_2024_paper.html": {
    "title": "TinyWT: A Large-Scale Wind Turbine Dataset of Satellite Images for Tiny Object Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingye Zhu",
      "Zhicheng Yang",
      "Hang Zhou",
      "Chen Du",
      "Andy Wong",
      "Yibing Wei",
      "Zhuo Deng",
      "Mei Han",
      "Jui-Hsin Lai"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/CV4EO/html/Zhang_CNet_A_Novel_Seabed_Coral_Reef_Image_Segmentation_Approach_Based_WACVW_2024_paper.html": {
    "title": "CNet: A Novel Seabed Coral Reef Image Segmentation Approach Based on Deep Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanqi Zhang",
      "Ming Li",
      "Jiageng Zhong",
      "Jiangying Qin"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/MaCVi/html/Yang_Sea_You_Later_Metadata-Guided_Long-Term_Re-Identification_for_UAV-Based_Multi-Object_Tracking_WACVW_2024_paper.html": {
    "title": "Sea You Later: Metadata-Guided Long-Term Re-Identification for UAV-Based Multi-Object Tracking",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng-Yen Yang",
      "Hsiang-Wei Huang",
      "Zhongyu Jiang",
      "Heng-Cheng Kuo",
      "Jie Mei",
      "Chung-I Huang",
      "Jenq-Neng Hwang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/MaCVi/html/Chowdhury_Active_Learning_Strategy_Using_Contrastive_Learning_and_K-Means_for_Aquatic_WACVW_2024_paper.html": {
    "title": "Active Learning Strategy Using Contrastive Learning and K-Means for Aquatic Invasive Species Recognition",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaif Chowdhury",
      "Greg Hamerly",
      "Monica McGarrity"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/MaCVi/html/Savathrakis_An_Automated_Method_for_the_Creation_of_Oriented_Bounding_Boxes_WACVW_2024_paper.html": {
    "title": "An Automated Method for the Creation of Oriented Bounding Boxes in Remote Sensing Ship Detection Datasets",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giorgos Savathrakis",
      "Antonis Argyros"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/MaCVi/html/Tran_SafeSea_Synthetic_Data_Generation_for_Adverse__Low_Probability_Maritime_WACVW_2024_paper.html": {
    "title": "SafeSea: Synthetic Data Generation for Adverse & Low Probability Maritime Conditions",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Martin Tran",
      "Jordan Shipard",
      "Hermawan Mulyono",
      "Arnold Wiliem",
      "Clinton Fookes"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/MaCVi/html/Kiefer_2nd_Workshop_on_Maritime_Computer_Vision_MaCVi_2024_Challenge_Results_WACVW_2024_paper.html": {
    "title": "2nd Workshop on Maritime Computer Vision (MaCVi) 2024: Challenge Results",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Kiefer",
      "Lojze Žust",
      "Matej Kristan",
      "Janez Perš",
      "Matija Teršek",
      "Arnold Wiliem",
      "Martin Messmer",
      "Cheng-Yen Yang",
      "Hsiang-Wei Huang",
      "Zhongyu Jiang",
      "Heng-Cheng Kuo",
      "Jie Mei",
      "Jenq-Neng Hwang",
      "Daniel Stadler",
      "Lars Sommer",
      "Kaer Huang",
      "Aiguo Zheng",
      "Weitu Chong",
      "Kanokphan Lertniphonphan",
      "Jun Xie",
      "Feng Chen",
      "Jian Li",
      "Zhepeng Wang",
      "Luca Zedda",
      "Andrea Loddo",
      "Cecilia Di Ruberto",
      "Tuan-Anh Vu",
      "Hai Nguyen-Truong",
      "Tan-Sang Ha",
      "Quan-Dung Pham",
      "Sai-Kit Yeung",
      "Yuan Feng",
      "Nguyen Thanh Thien",
      "Lixin Tian",
      "Andreas Michel",
      "Wolfgang Gross",
      "Martin Weinmann",
      "Borja Carrillo-Perez",
      "Alexander Klein",
      "Antje Alex",
      "Edgardo Solano-Carrillo",
      "Yannik Steiniger",
      "Angel Bueno Rodriguez",
      "Sheng-Yao Kuan",
      "Yuan-Hao Ho",
      "Felix Sattler",
      "Matej Fabijanić",
      "Magdalena Šimunec",
      "Nadir Kapetanović"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/MaCVi/html/Gulsoylu_Image_and_AIS_Data_Fusion_Technique_for_Maritime_Computer_Vision_WACVW_2024_paper.html": {
    "title": "Image and AIS Data Fusion Technique for Maritime Computer Vision Applications",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emre Gülsoylu",
      "Paul Koch",
      "Mert Yildiz",
      "Manfred Constapel",
      "André Peter Kelm"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/MaCVi/html/Trinh_SeaDSC_A_Video-Based_Unsupervised_Method_for_Dynamic_Scene_Change_Detection_WACVW_2024_paper.html": {
    "title": "SeaDSC: A Video-Based Unsupervised Method for Dynamic Scene Change Detection in Unmanned Surface Vehicles",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linh Trinh",
      "Ali Anwar",
      "Siegfried Mercelis"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/FRCSyn/html/Melzi_FRCSyn_Challenge_at_WACV_2024_Face_Recognition_Challenge_in_the_WACVW_2024_paper.html": {
    "title": "FRCSyn Challenge at WACV 2024: Face Recognition Challenge in the Era of Synthetic Data",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pietro Melzi",
      "Ruben Tolosana",
      "Ruben Vera-Rodriguez",
      "Minchul Kim",
      "Christian Rathgeb",
      "Xiaoming Liu",
      "Ivan DeAndres-Tame",
      "Aythami Morales",
      "Julian Fierrez",
      "Javier Ortega-Garcia",
      "Weisong Zhao",
      "Xiangyu Zhu",
      "Zheyu Yan",
      "Xiao-Yu Zhang",
      "Jinlin Wu",
      "Zhen Lei",
      "Suvidha Tripathi",
      "Mahak Kothari",
      "Md Haider Zama",
      "Debayan Deb",
      "Bernardo Biesseck",
      "Pedro Vidal",
      "Roger Granada",
      "Guilherme Fickel",
      "Gustavo Führ",
      "David Menotti",
      "Alexander Unnervik",
      "Anjith George",
      "Christophe Ecabert",
      "Hatef Otroshi Shahreza",
      "Parsa Rahimi",
      "Sébastien Marcel",
      "Ioannis Sarridis",
      "Christos Koutlis",
      "Georgia Baltsou",
      "Symeon Papadopoulos",
      "Christos Diou",
      "Nicolò Di Domenico",
      "Guido Borghi",
      "Lorenzo Pellegrini",
      "Enrique Mas-Candela",
      "Ángela Sánchez-Pérez",
      "Andrea Atzori",
      "Fadi Boutros",
      "Naser Damer",
      "Gianni Fenu",
      "Mirko Marras"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD/html/Yang_Human-Centric_Autonomous_Systems_With_LLMs_for_User_Command_Reasoning_WACVW_2024_paper.html": {
    "title": "Human-Centric Autonomous Systems With LLMs for User Command Reasoning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Yang",
      "Qingwen Zhang",
      "Ci Li",
      "Daniel Simões Marta",
      "Nazre Batool",
      "John Folkesson"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD/html/Shubodh_LIP-Loc_LiDAR_Image_Pretraining_for_Cross-Modal_Localization_WACVW_2024_paper.html": {
    "title": "LIP-Loc: LiDAR Image Pretraining for Cross-Modal Localization",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sai Shubodh",
      "Mohammad Omama",
      "Husain Zaidi",
      "Udit Singh Parihar",
      "Madhava Krishna"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD/html/Cui_A_Survey_on_Multimodal_Large_Language_Models_for_Autonomous_Driving_WACVW_2024_paper.html": {
    "title": "A Survey on Multimodal Large Language Models for Autonomous Driving",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Can Cui",
      "Yunsheng Ma",
      "Xu Cao",
      "Wenqian Ye",
      "Yang Zhou",
      "Kaizhao Liang",
      "Jintai Chen",
      "Juanwu Lu",
      "Zichong Yang",
      "Kuei-Da Liao",
      "Tianren Gao",
      "Erlong Li",
      "Kun Tang",
      "Zhipeng Cao",
      "Tong Zhou",
      "Ao Liu",
      "Xinrui Yan",
      "Shuqi Mei",
      "Jianguo Cao",
      "Ziran Wang",
      "Chao Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD/html/Park_VLAAD_Vision_and_Language_Assistant_for_Autonomous_Driving_WACVW_2024_paper.html": {
    "title": "VLAAD: Vision and Language Assistant for Autonomous Driving",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "SungYeon Park",
      "MinJae Lee",
      "JiHyuk Kang",
      "Hahyeon Choi",
      "Yoonah Park",
      "Juhwan Cho",
      "Adam Lee",
      "DongKyu Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD/html/Cui_Drive_As_You_Speak_Enabling_Human-Like_Interaction_With_Large_Language_WACVW_2024_paper.html": {
    "title": "Drive As You Speak: Enabling Human-Like Interaction With Large Language Models in Autonomous Vehicles",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Can Cui",
      "Yunsheng Ma",
      "Xu Cao",
      "Wenqian Ye",
      "Ziran Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD/html/Inoue_NuScenes-MQA_Integrated_Evaluation_of_Captions_and_QA_for_Autonomous_Driving_WACVW_2024_paper.html": {
    "title": "NuScenes-MQA: Integrated Evaluation of Captions and QA for Autonomous Driving Datasets Using Markup Annotations",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuichi Inoue",
      "Yuki Yada",
      "Kotaro Tanahashi",
      "Yu Yamaguchi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD/html/Zampokas_Latency_Driven_Spatially_Sparse_Optimization_for_Multi-Branch_CNNs_for_Semantic_WACVW_2024_paper.html": {
    "title": "Latency Driven Spatially Sparse Optimization for Multi-Branch CNNs for Semantic Segmentation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Georgios Zampokas",
      "Christos-Savvas Bouganis",
      "Dimitrios Tzovaras"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD/html/Zhong_A_Safer_Vision-Based_Autonomous_Planning_System_for_Quadrotor_UAVs_With_WACVW_2024_paper.html": {
    "title": "A Safer Vision-Based Autonomous Planning System for Quadrotor UAVs With Dynamic Obstacle Trajectory Prediction and Its Application With LLMs",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiageng Zhong",
      "Ming Li",
      "Yinliang Chen",
      "Zihang Wei",
      "Fan Yang",
      "Haoran Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/ASTAD/html/Nizan_K-NNN_Nearest_Neighbors_of_Neighbors_for_Anomaly_Detection_WACVW_2024_paper.html": {
    "title": "K-NNN: Nearest Neighbors of Neighbors for Anomaly Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ori Nizan",
      "Ayellet Tal"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/ASTAD/html/Noghre_An_Exploratory_Study_on_Human-Centric_Video_Anomaly_Detection_Through_Variational_WACVW_2024_paper.html": {
    "title": "An Exploratory Study on Human-Centric Video Anomaly Detection Through Variational Autoencoders and Trajectory Prediction",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ghazal Alinezhad Noghre",
      "Armin Danesh Pazho",
      "Hamed Tabkhi"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/MAP-A/html/Jauhari_Iris_Presentation_Attack_Assessing_the_Impact_of_Combining_Vanadium_Dioxide_WACVW_2024_paper.html": {
    "title": "Iris Presentation Attack: Assessing the Impact of Combining Vanadium Dioxide Films With Artificial Eyes",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Darshika Jauhari",
      "Renu Sharma",
      "Cunjian Chen",
      "Nelson Sepulveda",
      "Arun Ross"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/MAP-A/html/Medvedev_Fused_Classification_for_Differential_Face_Morphing_Detection_WACVW_2024_paper.html": {
    "title": "Fused Classification for Differential Face Morphing Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Iurii Medvedev",
      "Joana Alves Pimenta",
      "Nuno Gonçalves"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/MAP-A/html/Ivanovska_On_the_Vulnerability_of_Deepfake_Detectors_to_Attacks_Generated_by_WACVW_2024_paper.html": {
    "title": "On the Vulnerability of Deepfake Detectors to Attacks Generated by Denoising Diffusion Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marija Ivanovska",
      "Vitomir Struc"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/MAP-A/html/Li_Does_Capture_Background_Influence_the_Accuracy_of_the_Deep_Learning_WACVW_2024_paper.html": {
    "title": "Does Capture Background Influence the Accuracy of the Deep Learning Based Fingerphoto Presentation Attack Detection Techniques?",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hailin Li",
      "Raghavendra Ramachandra"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/MAP-A/html/Ciamarra_Deepfake_Detection_by_Exploiting_Surface_Anomalies_The_SurFake_Approach_WACVW_2024_paper.html": {
    "title": "Deepfake Detection by Exploiting Surface Anomalies: The SurFake Approach",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea Ciamarra",
      "Roberto Caldelli",
      "Federico Becattini",
      "Lorenzo Seidenari",
      "Alberto Del Bimbo"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/MAP-A/html/Sharma_Investigating_Weight-Perturbed_Deep_Neural_Networks_With_Application_in_Iris_Presentation_WACVW_2024_paper.html": {
    "title": "Investigating Weight-Perturbed Deep Neural Networks With Application in Iris Presentation Attack Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renu Sharma",
      "Redwan Sony",
      "Arun Ross"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/MAP-A/html/Bhuiyan_Forensic_Iris_Image_Synthesis_WACVW_2024_paper.html": {
    "title": "Forensic Iris Image Synthesis",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rasel Ahmed Bhuiyan",
      "Adam Czajka"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/MAP-A/html/Banerjee_Alpha-Wolves_and_Alpha-Mammals_Exploring_Dictionary_Attacks_on_Iris_Recognition_Systems_WACVW_2024_paper.html": {
    "title": "Alpha-Wolves and Alpha-Mammals: Exploring Dictionary Attacks on Iris Recognition Systems",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sudipta Banerjee",
      "Anubhav Jain",
      "Zehua Jiang",
      "Nasir Memon",
      "Julian Togelius",
      "Arun Ross"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/MAP-A/html/Seraj_Semi-Supervised_Deep_Domain_Adaptation_for_Deepfake_Detection_WACVW_2024_paper.html": {
    "title": "Semi-Supervised Deep Domain Adaptation for Deepfake Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Shamim Seraj",
      "Ankita Singh",
      "Shayok Chakraborty"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/WVLL/html/Nasim_Fog-Resilient_Bangla_Car_Plate_Recognition_Using_Dark_Channel_Prior_and_WACVW_2024_paper.html": {
    "title": "Fog-Resilient Bangla Car Plate Recognition Using Dark Channel Prior and YOLO",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hamim Ibne Nasim",
      "Fateha Jannat Printia",
      "Mahamudul Hasan",
      "Rubaba Rashid",
      "Iffat Jahan Chowdhury",
      "Joyanta Jyoti Mondal",
      "Md. Farhadul Islam",
      "Jannatun Noor"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/WVLL/html/Rabby_Enhancement_of_Bengali_OCR_by_Specialized_Models_and_Advanced_Techniques_WACVW_2024_paper.html": {
    "title": "Enhancement of Bengali OCR by Specialized Models and Advanced Techniques for Diverse Document Types",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "AKM Shahariar Azad Rabby",
      "Hasmot Ali",
      "Md. Majedul Islam",
      "Sheikh Abujar",
      "Fuad Rahman"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/WVLL/html/Fayyazsanavi_Fingerspelling_PoseNet_Enhancing_Fingerspelling_Translation_With_Pose-Based_Transformer_Models_WACVW_2024_paper.html": {
    "title": "Fingerspelling PoseNet: Enhancing Fingerspelling Translation With Pose-Based Transformer Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pooya Fayyazsanavi",
      "Negar Nejatishahidin",
      "Jana Košecká"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/DVPBA/html/Rezgui_Enhancing_Soft_Biometric_Face_Template_Privacy_With_Mutual_Information-Based_Image_WACVW_2024_paper.html": {
    "title": "Enhancing Soft Biometric Face Template Privacy With Mutual Information-Based Image Attacks",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zohra Rezgui",
      "Nicola Strisciuglio",
      "Raymond Veldhuis"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/DVPBA/html/Kotwal_Mitigating_Demographic_Bias_in_Face_Recognition_via_Regularized_Score_Calibration_WACVW_2024_paper.html": {
    "title": "Mitigating Demographic Bias in Face Recognition via Regularized Score Calibration",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ketan Kotwal",
      "Sébastien Marcel"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/DVPBA/html/Wu_Facial_Hair_Area_in_Face_Recognition_Across_Demographics_Small_Size_WACVW_2024_paper.html": {
    "title": "Facial Hair Area in Face Recognition Across Demographics: Small Size, Big Effect",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiyu Wu",
      "Sicong Tian",
      "Aman Bhatta",
      "Kağan Öztürk",
      "Karl Ricanek",
      "Kevin W. Bowyer"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/DVPBA/html/Merino_Who_Wore_It_Best_And_Who_Paid_Less_Effects_of_WACVW_2024_paper.html": {
    "title": "Who Wore It Best? And Who Paid Less? Effects of Privacy-Preserving Techniques Across Demographics",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xavier Merino",
      "Michael King"
    ]
  },
  "https://openaccess.thecvf.com/content/WACV2024W/DVPBA/html/Pangelinan_The_CHROMA-FIT_Dataset_Characterizing_Human_Ranges_of_Melanin_for_Increased_WACVW_2024_paper.html": {
    "title": "The CHROMA-FIT Dataset: Characterizing Human Ranges of Melanin for Increased Tone-Awareness",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriella Pangelinan",
      "Xavier Merino",
      "Samuel Langborgh",
      "Kushal Vangara",
      "Joyce Annan",
      "Audison Beaubrun",
      "Troy Weekes",
      "Michael C. King"
    ]
  }
}