{
  "https://aclanthology.org/2021.acl-long.1": {
    "title": "Investigating label suggestions for opinion mining in German Covid-19 social media",
    "volume": "long",
    "abstract": "This work investigates the use of interactively updated label suggestions to improve upon the efficiency of gathering annotations on the task of opinion mining in German Covid-19 social media data. We develop guidelines to conduct a controlled annotation study with social science students and find that suggestions from a model trained on a small, expert-annotated dataset already lead to a substantial improvement – in terms of inter-annotator agreement (+.14 Fleiss' κ) and annotation quality – compared to students that do not receive any label suggestions. We further find that label suggestions from interactively trained models do not lead to an improvement over suggestions from a static model. Nonetheless, our analysis of suggestion bias shows that annotators remain capable of reflecting upon the suggested label in general. Finally, we confirm the quality of the annotated data in transfer learning experiments between different annotator groups. To facilitate further research in opinion mining on social media data, we release our collected data consisting of 200 expert and 2,785 student annotations",
    "checked": true,
    "id": "5dda8a1bd1a9312186a2c03f90891c7dd9884378",
    "semantic_title": "investigating label suggestions for opinion mining in german covid-19 social media",
    "citation_count": 14,
    "authors": [
      "Tilman Beck",
      "Ji-Ung Lee",
      "Christina Viehmann",
      "Marcus Maurer",
      "Oliver Quiring",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2021.acl-long.2": {
    "title": "How Did This Get Funded?! Automatically Identifying Quirky Scientific Achievements",
    "volume": "long",
    "abstract": "Humor is an important social phenomenon, serving complex social and psychological functions. However, despite being studied for millennia humor is computationally not well understood, often considered an AI-complete problem. In this work, we introduce a novel setting in humor mining: automatically detecting funny and unusual scientific papers. We are inspired by the Ig Nobel prize, a satirical prize awarded annually to celebrate funny scientific achievements (example past winner: \"Are cows more likely to lie down the longer they stand?\"). This challenging task has unique characteristics that make it particularly suitable for automatic learning. We construct a dataset containing thousands of funny papers and use it to learn classifiers, combining findings from psychology and linguistics with recent advances in NLP. We use our models to identify potentially funny papers in a large dataset of over 630,000 articles. The results demonstrate the potential of our methods, and more broadly the utility of integrating state-of-the-art NLP methods with insights from more traditional disciplines",
    "checked": true,
    "id": "4831154d45fcb145bfa5202a6d921d411a822e28",
    "semantic_title": "how did this get funded?! automatically identifying quirky scientific achievements",
    "citation_count": 2,
    "authors": [
      "Chen Shani",
      "Nadav Borenstein",
      "Dafna Shahaf"
    ]
  },
  "https://aclanthology.org/2021.acl-long.3": {
    "title": "Engage the Public: Poll Question Generation for Social Media Posts",
    "volume": "long",
    "abstract": "This paper presents a novel task to generate poll questions for social media posts. It offers an easy way to hear the voice from the public and learn from their feelings to important social topics. While most related work tackles formal languages (e.g., exam papers), we generate poll questions for short and colloquial social media messages exhibiting severe data sparsity. To deal with that, we propose to encode user comments and discover latent topics therein as contexts. They are then incorporated into a sequence-to-sequence (S2S) architecture for question generation and its extension with dual decoders to additionally yield poll choices (answers). For experiments, we collect a large-scale Chinese dataset from Sina Weibo containing over 20K polls. The results show that our model outperforms the popular S2S models without exploiting topics from comments and the dual decoder design can further benefit the prediction of both questions and answers. Human evaluations further exhibit our superiority in yielding high-quality polls helpful to draw user engagements",
    "checked": true,
    "id": "d90b938431e39d290c794d5dea363c262cd71fdb",
    "semantic_title": "engage the public: poll question generation for social media posts",
    "citation_count": 6,
    "authors": [
      "Zexin Lu",
      "Keyang Ding",
      "Yuji Zhang",
      "Jing Li",
      "Baolin Peng",
      "Lemao Liu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.4": {
    "title": "HateCheck: Functional Tests for Hate Speech Detection Models",
    "volume": "long",
    "abstract": "Detecting online hate is a difficult task that even state-of-the-art models struggle with. Typically, hate speech detection models are evaluated by measuring their performance on held-out test data using metrics such as accuracy and F1 score. However, this approach makes it difficult to identify specific model weak points. It also risks overestimating generalisable model performance due to increasingly well-evidenced systematic gaps and biases in hate speech datasets. To enable more targeted diagnostic insights, we introduce HateCheck, a suite of functional tests for hate speech detection models. We specify 29 model functionalities motivated by a review of previous research and a series of interviews with civil society stakeholders. We craft test cases for each functionality and validate their quality through a structured annotation process. To illustrate HateCheck's utility, we test near-state-of-the-art transformer models as well as two popular commercial models, revealing critical model weaknesses",
    "checked": true,
    "id": "2b4929707e20e37a3a6351036ac48c20b5466a6d",
    "semantic_title": "hatecheck: functional tests for hate speech detection models",
    "citation_count": 147,
    "authors": [
      "Paul Röttger",
      "Bertie Vidgen",
      "Dong Nguyen",
      "Zeerak Waseem",
      "Helen Margetts",
      "Janet Pierrehumbert"
    ]
  },
  "https://aclanthology.org/2021.acl-long.5": {
    "title": "Unified Dual-view Cognitive Model for Interpretable Claim Verification",
    "volume": "long",
    "abstract": "Recent studies constructing direct interactions between the claim and each single user response (a comment or a relevant article) to capture evidence have shown remarkable success in interpretable claim verification. Owing to different single responses convey different cognition of individual users (i.e., audiences), the captured evidence belongs to the perspective of individual cognition. However, individuals' cognition of social things is not always able to truly reflect the objective. There may be one-sided or biased semantics in their opinions on a claim. The captured evidence correspondingly contains some unobjective and biased evidence fragments, deteriorating task performance. In this paper, we propose a Dual-view model based on the views of Collective and Individual Cognition (CICD) for interpretable claim verification. From the view of the collective cognition, we not only capture the word-level semantics based on individual users, but also focus on sentence-level semantics (i.e., the overall responses) among all users and adjust the proportion between them to generate global evidence. From the view of individual cognition, we select the top-k articles with high degree of difference and interact with the claim to explore the local key evidence fragments. To weaken the bias of individual cognition-view evidence, we devise inconsistent loss to suppress the divergence between global and local evidence for strengthening the consistent shared evidence between the both. Experiments on three benchmark datasets confirm that CICD achieves state-of-the-art performance",
    "checked": true,
    "id": "37f16fd6ed8f6fd161eceb9384e7a4848b383775",
    "semantic_title": "unified dual-view cognitive model for interpretable claim verification",
    "citation_count": 12,
    "authors": [
      "Lianwei Wu",
      "Yuan Rao",
      "Yuqian Lan",
      "Ling Sun",
      "Zhaoyin Qi"
    ]
  },
  "https://aclanthology.org/2021.acl-long.6": {
    "title": "DeepRapper: Neural Rap Generation with Rhyme and Rhythm Modeling",
    "volume": "long",
    "abstract": "Rap generation, which aims to produce lyrics and corresponding singing beats, needs to model both rhymes and rhythms. Previous works for rap generation focused on rhyming lyrics, but ignored rhythmic beats, which are important for rap performance. In this paper, we develop DeepRapper, a Transformer-based rap generation system that can model both rhymes and rhythms. Since there is no available rap datasets with rhythmic beats, we develop a data mining pipeline to collect a large-scale rap dataset, which includes a large number of rap songs with aligned lyrics and rhythmic beats. Second, we design a Transformer-based autoregressive language model which carefully models rhymes and rhythms. Specifically, we generate lyrics in the reverse order with rhyme representation and constraint for rhyme enhancement, and insert a beat symbol into lyrics for rhythm/beat modeling. To our knowledge, DeepRapper is the first system to generate rap with both rhymes and rhythms. Both objective and subjective evaluations demonstrate that DeepRapper generates creative and high-quality raps with rhymes and rhythms",
    "checked": true,
    "id": "19a9937a1029ee5b1f6da8a945ed9a5c5a029c57",
    "semantic_title": "deeprapper: neural rap generation with rhyme and rhythm modeling",
    "citation_count": 22,
    "authors": [
      "Lanqing Xue",
      "Kaitao Song",
      "Duocai Wu",
      "Xu Tan",
      "Nevin L. Zhang",
      "Tao Qin",
      "Wei-Qiang Zhang",
      "Tie-Yan Liu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.7": {
    "title": "PENS: A Dataset and Generic Framework for Personalized News Headline Generation",
    "volume": "long",
    "abstract": "In this paper, we formulate the personalized news headline generation problem whose goal is to output a user-specific title based on both a user's reading interests and a candidate news body to be exposed to her. To build up a benchmark for this problem, we publicize a large-scale dataset named PENS (PErsonalized News headlineS). The training set is collected from user impressions logs of Microsoft News, and the test set is manually created by hundreds of native speakers to enable a fair testbed for evaluating models in an offline mode. We propose a generic framework as a preparatory solution to our problem. At its heart, user preference is learned by leveraging the user behavioral data, and three kinds of user preference injections are proposed to personalize a text generator and establish personalized headlines. We investigate our dataset by implementing several state-of-the-art user modeling methods in our framework to demonstrate a benchmark score for the proposed dataset. The dataset is available at https://msnews.github.io/pens.html",
    "checked": true,
    "id": "90bc51ebd5b676ff31d27a599f36745194d53693",
    "semantic_title": "pens: a dataset and generic framework for personalized news headline generation",
    "citation_count": 23,
    "authors": [
      "Xiang Ao",
      "Xiting Wang",
      "Ling Luo",
      "Ying Qiao",
      "Qing He",
      "Xing Xie"
    ]
  },
  "https://aclanthology.org/2021.acl-long.8": {
    "title": "Enhancing Content Preservation in Text Style Transfer Using Reverse Attention and Conditional Layer Normalization",
    "volume": "long",
    "abstract": "Text style transfer aims to alter the style (e.g., sentiment) of a sentence while preserving its content. A common approach is to map a given sentence to content representation that is free of style, and the content representation is fed to a decoder with a target style. Previous methods in filtering style completely remove tokens with style at the token level, which incurs the loss of content information. In this paper, we propose to enhance content preservation by implicitly removing the style information of each token with reverse attention, and thereby retain the content. Furthermore, we fuse content information when building the target style representation, making it dynamic with respect to the content. Our method creates not only style-independent content representation, but also content-dependent style representation in transferring style. Empirical results show that our method outperforms the state-of-the-art baselines by a large margin in terms of content preservation. In addition, it is also competitive in terms of style transfer accuracy and fluency",
    "checked": true,
    "id": "22934eb94be2df4e454cf7df3c9babc4aa6a752b",
    "semantic_title": "enhancing content preservation in text style transfer using reverse attention and conditional layer normalization",
    "citation_count": 32,
    "authors": [
      "Dongkyu Lee",
      "Zhiliang Tian",
      "Lanqing Xue",
      "Nevin L. Zhang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.9": {
    "title": "Mention Flags (MF): Constraining Transformer-based Text Generators",
    "volume": "long",
    "abstract": "This paper focuses on Seq2Seq (S2S) constrained text generation where the text generator is constrained to mention specific words which are inputs to the encoder in the generated outputs. Pre-trained S2S models or a Copy Mechanism are trained to copy the surface tokens from encoders to decoders, but they cannot guarantee constraint satisfaction. Constrained decoding algorithms always produce hypotheses satisfying all constraints. However, they are computationally expensive and can lower the generated text quality. In this paper, we propose Mention Flags (MF), which traces whether lexical constraints are satisfied in the generated outputs in an S2S decoder. The MF models can be trained to generate tokens in a hypothesis until all constraints are satisfied, guaranteeing high constraint satisfaction. Our experiments on the Common Sense Generation task (CommonGen) (Lin et al., 2020), End2end Restaurant Dialog task (E2ENLG) (Duˇsek et al., 2020) and Novel Object Captioning task (nocaps) (Agrawal et al., 2019) show that the MF models maintain higher constraint satisfaction and text quality than the baseline models and other constrained decoding algorithms, achieving state-of-the-art performance on all three tasks. These results are achieved with a much lower run-time than constrained decoding algorithms. We also show that the MF models work well in the low-resource setting",
    "checked": true,
    "id": "a4242e9077e9674d0e3896b1a4d651ff742cb5ca",
    "semantic_title": "mention flags (mf): constraining transformer-based text generators",
    "citation_count": 22,
    "authors": [
      "Yufei Wang",
      "Ian Wood",
      "Stephen Wan",
      "Mark Dras",
      "Mark Johnson"
    ]
  },
  "https://aclanthology.org/2021.acl-long.10": {
    "title": "Generalising Multilingual Concept-to-Text NLG with Language Agnostic Delexicalisation",
    "volume": "long",
    "abstract": "Concept-to-text Natural Language Generation is the task of expressing an input meaning representation in natural language. Previous approaches in this task have been able to generalise to rare or unseen instances by relying on a delexicalisation of the input. However, this often requires that the input appears verbatim in the output text. This poses challenges in multilingual settings, where the task expands to generate the output text in multiple languages given the same input. In this paper, we explore the application of multilingual models in concept-to-text and propose Language Agnostic Delexicalisation, a novel delexicalisation method that uses multilingual pretrained embeddings, and employs a character-level post-editing model to inflect words in their correct form during relexicalisation. Our experiments across five datasets and five languages show that multilingual models outperform monolingual models in concept-to-text and that our framework outperforms previous approaches, especially in low resource conditions",
    "checked": true,
    "id": "aed58f90d8a2e8e9dca35efee21e1b7e806600a8",
    "semantic_title": "generalising multilingual concept-to-text nlg with language agnostic delexicalisation",
    "citation_count": 3,
    "authors": [
      "Giulio Zhou",
      "Gerasimos Lampouras"
    ]
  },
  "https://aclanthology.org/2021.acl-long.11": {
    "title": "Conversations Are Not Flat: Modeling the Dynamic Information Flow across Dialogue Utterances",
    "volume": "long",
    "abstract": "Nowadays, open-domain dialogue models can generate acceptable responses according to the historical context based on the large-scale pre-trained language models. However, they generally concatenate the dialogue history directly as the model input to predict the response, which we named as the flat pattern and ignores the dynamic information flow across dialogue utterances. In this work, we propose the DialoFlow model, in which we introduce a dynamic flow mechanism to model the context flow, and design three training objectives to capture the information dynamics across dialogue utterances by addressing the semantic influence brought about by each utterance in large-scale pre-training. Experiments on the multi-reference Reddit Dataset and DailyDialog Dataset demonstrate that our DialoFlow significantly outperforms the DialoGPT on the dialogue generation task. Besides, we propose the Flow score, an effective automatic metric for evaluating interactive human-bot conversation quality based on the pre-trained DialoFlow, which presents high chatbot-level correlation (r=0.9) with human ratings among 11 chatbots. Code and pre-trained models will be public",
    "checked": true,
    "id": "1b05155aa1f7532609bf2cf5fea668a745399780",
    "semantic_title": "conversations are not flat: modeling the dynamic information flow across dialogue utterances",
    "citation_count": 43,
    "authors": [
      "Zekang Li",
      "Jinchao Zhang",
      "Zhengcong Fei",
      "Yang Feng",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2021.acl-long.12": {
    "title": "Dual Slot Selector via Local Reliability Verification for Dialogue State Tracking",
    "volume": "long",
    "abstract": "The goal of dialogue state tracking (DST) is to predict the current dialogue state given all previous dialogue contexts. Existing approaches generally predict the dialogue state at every turn from scratch. However, the overwhelming majority of the slots in each turn should simply inherit the slot values from the previous turn. Therefore, the mechanism of treating slots equally in each turn not only is inefficient but also may lead to additional errors because of the redundant slot value generation. To address this problem, we devise the two-stage DSS-DST which consists of the Dual Slot Selector based on the current turn dialogue, and the Slot Value Generator based on the dialogue history. The Dual Slot Selector determines each slot whether to update slot value or to inherit the slot value from the previous turn from two aspects: (1) if there is a strong relationship between it and the current turn dialogue utterances; (2) if a slot value with high reliability can be obtained for it through the current turn dialogue. The slots selected to be updated are permitted to enter the Slot Value Generator to update values by a hybrid method, while the other slots directly inherit the values from the previous turn. Empirical results show that our method achieves 56.93%, 60.73%, and 58.04% joint accuracy on MultiWOZ 2.0, MultiWOZ 2.1, and MultiWOZ 2.2 datasets respectively and achieves a new state-of-the-art performance with significant improvements",
    "checked": true,
    "id": "ba3f9cac51ed1612f4ff6f51906a3fa9854943a1",
    "semantic_title": "dual slot selector via local reliability verification for dialogue state tracking",
    "citation_count": 9,
    "authors": [
      "Jinyu Guo",
      "Kai Shuang",
      "Jijie Li",
      "Zihan Wang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.13": {
    "title": "Transferable Dialogue Systems and User Simulators",
    "volume": "long",
    "abstract": "One of the difficulties in training dialogue systems is the lack of training data. We explore the possibility of creating dialogue data through the interaction between a dialogue system and a user simulator. Our goal is to develop a modelling framework that can incorporate new dialogue scenarios through self-play between the two agents. In this framework, we first pre-train the two agents on a collection of source domain dialogues, which equips the agents to converse with each other via natural language. With further fine-tuning on a small amount of target domain data, the agents continue to interact with the aim of improving their behaviors using reinforcement learning with structured reward functions. In experiments on the MultiWOZ dataset, two practical transfer learning problems are investigated: 1) domain adaptation and 2) single-to-multiple domain transfer. We demonstrate that the proposed framework is highly effective in bootstrapping the performance of the two agents in transfer learning. We also show that our method leads to improvements in dialogue system performance on complete datasets",
    "checked": true,
    "id": "4d1a14352ffb526a1fa0e1cd90e2484e188cddc0",
    "semantic_title": "transferable dialogue systems and user simulators",
    "citation_count": 37,
    "authors": [
      "Bo-Hsiang Tseng",
      "Yinpei Dai",
      "Florian Kreyssig",
      "Bill Byrne"
    ]
  },
  "https://aclanthology.org/2021.acl-long.14": {
    "title": "BoB: BERT Over BERT for Training Persona-based Dialogue Models from Limited Personalized Data",
    "volume": "long",
    "abstract": "Maintaining a consistent persona is essential for dialogue agents. Although tremendous advancements have been brought, the limited-scale of annotated personalized dialogue datasets is still a barrier towards training robust and consistent persona-based dialogue models. This work shows how this challenge can be addressed by disentangling persona-based dialogue generation into two sub-tasks with a novel BERT-over-BERT (BoB) model. Specifically, the model consists of a BERT-based encoder and two BERT-based decoders, where one decoder is for response generation, and another is for consistency understanding. In particular, to learn the ability of consistency understanding from large-scale non-dialogue inference data, we train the second decoder in an unlikelihood manner. Under different limited data settings, both automatic and human evaluations demonstrate that the proposed model outperforms strong baselines in response quality and persona consistency",
    "checked": true,
    "id": "fd81c369245be3ea63614ca7994a861ddfa4705d",
    "semantic_title": "bob: bert over bert for training persona-based dialogue models from limited personalized data",
    "citation_count": 67,
    "authors": [
      "Haoyu Song",
      "Yan Wang",
      "Kaiyan Zhang",
      "Wei-Nan Zhang",
      "Ting Liu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.15": {
    "title": "GL-GIN: Fast and Accurate Non-Autoregressive Model for Joint Multiple Intent Detection and Slot Filling",
    "volume": "long",
    "abstract": "Multi-intent SLU can handle multiple intents in an utterance, which has attracted increasing attention. However, the state-of-the-art joint models heavily rely on autoregressive approaches, resulting in two issues: slow inference speed and information leakage. In this paper, we explore a non-autoregressive model for joint multiple intent detection and slot filling, achieving more fast and accurate. Specifically, we propose a Global-Locally Graph Interaction Network (GL-GIN) where a local slot-aware graph interaction layer is proposed to model slot dependency for alleviating uncoordinated slots problem while a global intent-slot graph interaction layer is introduced to model the interaction between multiple intents and all slots in the utterance. Experimental results on two public datasets show that our framework achieves state-of-the-art performance while being 11.5 times faster",
    "checked": true,
    "id": "58b2abacdc271158946a1a002bf58cceeaea4e26",
    "semantic_title": "gl-gin: fast and accurate non-autoregressive model for joint multiple intent detection and slot filling",
    "citation_count": 43,
    "authors": [
      "Libo Qin",
      "Fuxuan Wei",
      "Tianbao Xie",
      "Xiao Xu",
      "Wanxiang Che",
      "Ting Liu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.16": {
    "title": "Accelerating BERT Inference for Sequence Labeling via Early-Exit",
    "volume": "long",
    "abstract": "Both performance and efficiency are crucial factors for sequence labeling tasks in many real-world scenarios. Although the pre-trained models (PTMs) have significantly improved the performance of various sequence labeling tasks, their computational cost is expensive. To alleviate this problem, we extend the recent successful early-exit mechanism to accelerate the inference of PTMs for sequence labeling tasks. However, existing early-exit mechanisms are specifically designed for sequence-level tasks, rather than sequence labeling. In this paper, we first propose a simple extension of sentence-level early-exit for sequence labeling tasks. To further reduce the computational cost, we also propose a token-level early-exit mechanism that allows partial tokens to exit early at different layers. Considering the local dependency inherent in sequence labeling, we employed a window-based criterion to decide for a token whether or not to exit. The token-level early-exit brings the gap between training and inference, so we introduce an extra self-sampling fine-tuning stage to alleviate it. The extensive experiments on three popular sequence labeling tasks show that our approach can save up to 66%∼75% inference cost with minimal performance degradation. Compared with competitive compressed models such as DistilBERT, our approach can achieve better performance under the same speed-up ratios of 2×, 3×, and 4×",
    "checked": true,
    "id": "9c053552dfa6184f7dc56d620bcb1e8f22c729a3",
    "semantic_title": "accelerating bert inference for sequence labeling via early-exit",
    "citation_count": 17,
    "authors": [
      "Xiaonan Li",
      "Yunfan Shao",
      "Tianxiang Sun",
      "Hang Yan",
      "Xipeng Qiu",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.17": {
    "title": "Modularized Interaction Network for Named Entity Recognition",
    "volume": "long",
    "abstract": "Although the existing Named Entity Recognition (NER) models have achieved promising performance, they suffer from certain drawbacks. The sequence labeling-based NER models do not perform well in recognizing long entities as they focus only on word-level information, while the segment-based NER models which focus on processing segment instead of single word are unable to capture the word-level dependencies within the segment. Moreover, as boundary detection and type prediction may cooperate with each other for the NER task, it is also important for the two sub-tasks to mutually reinforce each other by sharing their information. In this paper, we propose a novel Modularized Interaction Network (MIN) model which utilizes both segment-level information and word-level dependencies, and incorporates an interaction mechanism to support information sharing between boundary detection and type prediction to enhance the performance for the NER task. We have conducted extensive experiments based on three NER benchmark datasets. The performance results have shown that the proposed MIN model has outperformed the current state-of-the-art models",
    "checked": true,
    "id": "fde2f03cb9d58b6cfb1bbc5966782cba30a1ba49",
    "semantic_title": "modularized interaction network for named entity recognition",
    "citation_count": 21,
    "authors": [
      "Fei Li",
      "Zheng Wang",
      "Siu Cheung Hui",
      "Lejian Liao",
      "Dandan Song",
      "Jing Xu",
      "Guoxiu He",
      "Meihuizi Jia"
    ]
  },
  "https://aclanthology.org/2021.acl-long.18": {
    "title": "Capturing Event Argument Interaction via A Bi-Directional Entity-Level Recurrent Decoder",
    "volume": "long",
    "abstract": "Capturing interactions among event arguments is an essential step towards robust event argument extraction (EAE). However, existing efforts in this direction suffer from two limitations: 1) The argument role type information of contextual entities is mainly utilized as training signals, ignoring the potential merits of directly adopting it as semantically rich input features; 2) The argument-level sequential semantics, which implies the overall distribution pattern of argument roles over an event mention, is not well characterized. To tackle the above two bottlenecks, we formalize EAE as a Seq2Seq-like learning problem for the first time, where a sentence with a specific event trigger is mapped to a sequence of event argument roles. A neural architecture with a novel Bi-directional Entity-level Recurrent Decoder (BERD) is proposed to generate argument roles by incorporating contextual entities' argument role predictions, like a word-by-word text generation process, thereby distinguishing implicit argument distribution patterns within an event more accurately",
    "checked": true,
    "id": "22e18a1d1b63a536267303bf581fe71869254b40",
    "semantic_title": "capturing event argument interaction via a bi-directional entity-level recurrent decoder",
    "citation_count": 15,
    "authors": [
      "Xi Xiangyu",
      "Wei Ye",
      "Shikun Zhang",
      "Quanxiu Wang",
      "Huixing Jiang",
      "Wei Wu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.19": {
    "title": "UniRE: A Unified Label Space for Entity Relation Extraction",
    "volume": "long",
    "abstract": "Many joint entity relation extraction models setup two separated label spaces for the two sub-tasks (i.e., entity detection and relation classification). We argue that this setting may hinder the information interaction between entities and relations. In this work, we propose to eliminate the different treatment on the two sub-tasks' label spaces. The input of our model is a table containing all word pairs from a sentence. Entities and relations are represented by squares and rectangles in the table. We apply a unified classifier to predict each cell's label, which unifies the learning of two sub-tasks. For testing, an effective (yet fast) approximate decoder is proposed for finding squares and rectangles from tables. Experiments on three benchmarks (ACE04, ACE05, SciERC) show that, using only half the number of parameters, our model achieves competitive accuracy with the best extractor, and is faster",
    "checked": true,
    "id": "360426953357dba87b99f3dedefae2337a671543",
    "semantic_title": "unire: a unified label space for entity relation extraction",
    "citation_count": 63,
    "authors": [
      "Yijun Wang",
      "Changzhi Sun",
      "Yuanbin Wu",
      "Hao Zhou",
      "Lei Li",
      "Junchi Yan"
    ]
  },
  "https://aclanthology.org/2021.acl-long.20": {
    "title": "Refining Sample Embeddings with Relation Prototypes to Enhance Continual Relation Extraction",
    "volume": "long",
    "abstract": "Continual learning has gained increasing attention in recent years, thanks to its biological interpretation and efficiency in many real-world applications. As a typical task of continual learning, continual relation extraction (CRE) aims to extract relations between entities from texts, where the samples of different relations are delivered into the model continuously. Some previous works have proved that storing typical samples of old relations in memory can help the model keep a stable understanding of old relations and avoid forgetting them. However, most methods heavily depend on the memory size in that they simply replay these memorized samples in subsequent tasks. To fully utilize memorized samples, in this paper, we employ relation prototype to extract useful information of each relation. Specifically, the prototype embedding for a specific relation is computed based on memorized samples of this relation, which is collected by K-means algorithm. The prototypes of all observed relations at current learning stage are used to re-initialize a memory network to refine subsequent sample embeddings, which ensures the model's stable understanding on all observed relations when learning a new task. Compared with previous CRE models, our model utilizes the memory information sufficiently and efficiently, resulting in enhanced CRE performance. Our experiments show that the proposed model outperforms the state-of-the-art CRE models and has great advantage in avoiding catastrophic forgetting. The code and datasets are released on https://github.com/fd2014cl/RP-CRE",
    "checked": true,
    "id": "0bac11c3e354e2909f17cd6f0472c009abab2cc7",
    "semantic_title": "refining sample embeddings with relation prototypes to enhance continual relation extraction",
    "citation_count": 26,
    "authors": [
      "Li Cui",
      "Deqing Yang",
      "Jiaxin Yu",
      "Chengwei Hu",
      "Jiayang Cheng",
      "Jingjie Yi",
      "Yanghua Xiao"
    ]
  },
  "https://aclanthology.org/2021.acl-long.21": {
    "title": "Contrastive Learning for Many-to-many Multilingual Neural Machine Translation",
    "volume": "long",
    "abstract": "Existing multilingual machine translation approaches mainly focus on English-centric directions, while the non-English directions still lag behind. In this work, we aim to build a many-to-many translation system with an emphasis on the quality of non-English language directions. Our intuition is based on the hypothesis that a universal cross-language representation leads to better multilingual translation performance. To this end, we propose mRASP2, a training method to obtain a single unified multilingual translation model. mRASP2 is empowered by two techniques: a) a contrastive learning scheme to close the gap among representations of different languages, and b) data augmentation on both multiple parallel and monolingual data to further align token representations. For English-centric directions, mRASP2 achieves competitive or even better performance than a strong pre-trained model mBART on tens of WMT benchmarks. For non-English directions, mRASP2 achieves an improvement of average 10+ BLEU compared with the multilingual baseline",
    "checked": true,
    "id": "56665fd91f7d05842380fe0f928461d66f10c5de",
    "semantic_title": "contrastive learning for many-to-many multilingual neural machine translation",
    "citation_count": 137,
    "authors": [
      "Xiao Pan",
      "Mingxuan Wang",
      "Liwei Wu",
      "Lei Li"
    ]
  },
  "https://aclanthology.org/2021.acl-long.22": {
    "title": "Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation",
    "volume": "long",
    "abstract": "Neural Machine Translation (NMT) currently exhibits biases such as producing translations that are too short and overgenerating frequent words, and shows poor robustness to copy noise in training data or domain shift. Recent work has tied these shortcomings to beam search – the de facto standard inference algorithm in NMT – and Eikema & Aziz (2020) propose to use Minimum Bayes Risk (MBR) decoding on unbiased samples instead. In this paper, we empirically investigate the properties of MBR decoding on a number of previously reported biases and failure cases of beam search. We find that MBR still exhibits a length and token frequency bias, owing to the MT metrics used as utility functions, but that MBR also increases robustness against copy noise in the training data and domain shift",
    "checked": true,
    "id": "af59aeeb46fb5d8412f550f6dd5c5dc99afc9c1a",
    "semantic_title": "understanding the properties of minimum bayes risk decoding in neural machine translation",
    "citation_count": 29,
    "authors": [
      "Mathias Müller",
      "Rico Sennrich"
    ]
  },
  "https://aclanthology.org/2021.acl-long.23": {
    "title": "Multi-Head Highly Parallelized LSTM Decoder for Neural Machine Translation",
    "volume": "long",
    "abstract": "One of the reasons Transformer translation models are popular is that self-attention networks for context modelling can be easily parallelized at sequence level. However, the computational complexity of a self-attention network is O(n2), increasing quadratically with sequence length. By contrast, the complexity of LSTM-based approaches is only O(n). In practice, however, LSTMs are much slower to train than self-attention networks as they cannot be parallelized at sequence level: to model context, the current LSTM state relies on the full LSTM computation of the preceding state. This has to be computed n times for a sequence of length n. The linear transformations involved in the LSTM gate and state computations are the major cost factors in this. To enable sequence-level parallelization of LSTMs, we approximate full LSTM context modelling by computing hidden states and gates with the current input and a simple bag-of-words representation of the preceding tokens context. This allows us to compute each input step efficiently in parallel, avoiding the formerly costly sequential linear transformations. We then connect the outputs of each parallel step with computationally cheap element-wise computations. We call this the Highly Parallelized LSTM. To further constrain the number of LSTM parameters, we compute several small HPLSTMs in parallel like multi-head attention in the Transformer. The experiments show that our MHPLSTM decoder achieves significant BLEU improvements, while being even slightly faster than the self-attention network in training, and much faster than the standard LSTM",
    "checked": true,
    "id": "312fd02d69bb5f95761bd627f51fc5157940dfa6",
    "semantic_title": "multi-head highly parallelized lstm decoder for neural machine translation",
    "citation_count": 7,
    "authors": [
      "Hongfei Xu",
      "Qiuhui Liu",
      "Josef van Genabith",
      "Deyi Xiong",
      "Meng Zhang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.24": {
    "title": "A Bidirectional Transformer Based Alignment Model for Unsupervised Word Alignment",
    "volume": "long",
    "abstract": "Word alignment and machine translation are two closely related tasks. Neural translation models, such as RNN-based and Transformer models, employ a target-to-source attention mechanism which can provide rough word alignments, but with a rather low accuracy. High-quality word alignment can help neural machine translation in many different ways, such as missing word detection, annotation transfer and lexicon injection. Existing methods for learning word alignment include statistical word aligners (e.g. GIZA++) and recently neural word alignment models. This paper presents a bidirectional Transformer based alignment (BTBA) model for unsupervised learning of the word alignment task. Our BTBA model predicts the current target word by attending the source context and both left-side and right-side target context to produce accurate target-to-source attention (alignment). We further fine-tune the target-to-source attention in the BTBA model to obtain better alignments using a full context based optimization method and self-supervised training. We test our method on three word alignment tasks and show that our method outperforms both previous neural word alignment approaches and the popular statistical word aligner GIZA++",
    "checked": true,
    "id": "21ece0c7828e9f73a1991d88b6094cfca77fe17b",
    "semantic_title": "a bidirectional transformer based alignment model for unsupervised word alignment",
    "citation_count": 8,
    "authors": [
      "Jingyi Zhang",
      "Josef van Genabith"
    ]
  },
  "https://aclanthology.org/2021.acl-long.25": {
    "title": "Learning Language Specific Sub-network for Multilingual Machine Translation",
    "volume": "long",
    "abstract": "Multilingual neural machine translation aims at learning a single translation model for multiple languages. These jointly trained models often suffer from performance degradationon rich-resource language pairs. We attribute this degeneration to parameter interference. In this paper, we propose LaSS to jointly train a single unified multilingual MT model. LaSS learns Language Specific Sub-network (LaSS) for each language pair to counter parameter interference. Comprehensive experiments on IWSLT and WMT datasets with various Transformer architectures show that LaSS obtains gains on 36 language pairs by up to 1.2 BLEU. Besides, LaSS shows its strong generalization performance at easy adaptation to new language pairs and zero-shot translation. LaSS boosts zero-shot translation with an average of 8.3 BLEU on 30 language pairs. Codes and trained models are available at https://github.com/NLP-Playground/LaSS",
    "checked": true,
    "id": "6ff5ea40e0d1be8c71e5b675db64fe730018db03",
    "semantic_title": "learning language specific sub-network for multilingual machine translation",
    "citation_count": 55,
    "authors": [
      "Zehui Lin",
      "Liwei Wu",
      "Mingxuan Wang",
      "Lei Li"
    ]
  },
  "https://aclanthology.org/2021.acl-long.26": {
    "title": "Exploring the Efficacy of Automatically Generated Counterfactuals for Sentiment Analysis",
    "volume": "long",
    "abstract": "While state-of-the-art NLP models have been achieving the excellent performance of a wide range of tasks in recent years, important questions are being raised about their robustness and their underlying sensitivity to systematic biases that may exist in their training and test data. Such issues come to be manifest in performance problems when faced with out-of-distribution data in the field. One recent solution has been to use counterfactually augmented datasets in order to reduce any reliance on spurious patterns that may exist in the original data. Producing high-quality augmented data can be costly and time-consuming as it usually needs to involve human feedback and crowdsourcing efforts. In this work, we propose an alternative by describing and evaluating an approach to automatically generating counterfactual data for the purpose of data augmentation and explanation. A comprehensive evaluation on several different datasets and using a variety of state-of-the-art benchmarks demonstrate how our approach can achieve significant improvements in model performance when compared to models training on the original data and even when compared to models trained with the benefit of human-generated augmented data",
    "checked": true,
    "id": "048af7345faefb6fb0bee2924275e222f21742e2",
    "semantic_title": "exploring the efficacy of automatically generated counterfactuals for sentiment analysis",
    "citation_count": 37,
    "authors": [
      "Linyi Yang",
      "Jiazheng Li",
      "Padraig Cunningham",
      "Yue Zhang",
      "Barry Smyth",
      "Ruihai Dong"
    ]
  },
  "https://aclanthology.org/2021.acl-long.27": {
    "title": "Bridge-Based Active Domain Adaptation for Aspect Term Extraction",
    "volume": "long",
    "abstract": "As a fine-grained task, the annotation cost of aspect term extraction is extremely high. Recent attempts alleviate this issue using domain adaptation that transfers common knowledge across domains. Since most aspect terms are domain-specific, they cannot be transferred directly. Existing methods solve this problem by associating aspect terms with pivot words (we call this passive domain adaptation because the transfer of aspect terms relies on the links to pivots). However, all these methods need either manually labeled pivot words or expensive computing resources to build associations. In this paper, we propose a novel active domain adaptation method. Our goal is to transfer aspect terms by actively supplementing transferable knowledge. To this end, we construct syntactic bridges by recognizing syntactic roles as pivots instead of as links to pivots. We also build semantic bridges by retrieving transferable semantic prototypes. Extensive experiments show that our method significantly outperforms previous approaches",
    "checked": true,
    "id": "9490de58a877a629f96f1a6744ce0cbb93c7b463",
    "semantic_title": "bridge-based active domain adaptation for aspect term extraction",
    "citation_count": 19,
    "authors": [
      "Zhuang Chen",
      "Tieyun Qian"
    ]
  },
  "https://aclanthology.org/2021.acl-long.28": {
    "title": "Multimodal Sentiment Detection Based on Multi-channel Graph Neural Networks",
    "volume": "long",
    "abstract": "With the popularity of smartphones, we have witnessed the rapid proliferation of multimodal posts on various social media platforms. We observe that the multimodal sentiment expression has specific global characteristics, such as the interdependencies of objects or scenes within the image. However, most previous studies only considered the representation of a single image-text post and failed to capture the global co-occurrence characteristics of the dataset. In this paper, we propose Multi-channel Graph Neural Networks with Sentiment-awareness (MGNNS) for image-text sentiment detection. Specifically, we first encode different modalities to capture hidden representations. Then, we introduce multi-channel graph neural networks to learn multimodal representations based on the global characteristics of the dataset. Finally, we implement multimodal in-depth fusion with the multi-head attention mechanism to predict the sentiment of image-text pairs. Extensive experiments conducted on three publicly available datasets demonstrate the effectiveness of our approach for multimodal sentiment detection",
    "checked": true,
    "id": "d2d7481a52bda466ecc2f197d82937444059f6cf",
    "semantic_title": "multimodal sentiment detection based on multi-channel graph neural networks",
    "citation_count": 38,
    "authors": [
      "Xiaocui Yang",
      "Shi Feng",
      "Yifei Zhang",
      "Daling Wang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.29": {
    "title": "Aspect-Category-Opinion-Sentiment Quadruple Extraction with Implicit Aspects and Opinions",
    "volume": "long",
    "abstract": "Product reviews contain a large number of implicit aspects and implicit opinions. However, most of the existing studies in aspect-based sentiment analysis ignored this problem. In this work, we introduce a new task, named Aspect-Category-Opinion-Sentiment (ACOS) Quadruple Extraction, with the goal to extract all aspect-category-opinion-sentiment quadruples in a review sentence and provide full support for aspect-based sentiment analysis with implicit aspects and opinions. We furthermore construct two new datasets, Restaurant-ACOS and Laptop-ACOS, for this new task, both of which contain the annotations of not only aspect-category-opinion-sentiment quadruples but also implicit aspects and opinions. The former is an extension of the SemEval Restaurant dataset; the latter is a newly collected and annotated Laptop dataset, twice the size of the SemEval Laptop dataset. We finally benchmark the task with four baseline systems. Experiments demonstrate the feasibility of the new task and its effectiveness in extracting and describing implicit aspects and implicit opinions. The two datasets and source code of four systems are publicly released at https://github.com/NUSTM/ACOS",
    "checked": true,
    "id": "8298cbdc43c1e93257be69a078ca2662da89db9c",
    "semantic_title": "aspect-category-opinion-sentiment quadruple extraction with implicit aspects and opinions",
    "citation_count": 67,
    "authors": [
      "Hongjie Cai",
      "Rui Xia",
      "Jianfei Yu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.30": {
    "title": "PASS: Perturb-and-Select Summarizer for Product Reviews",
    "volume": "long",
    "abstract": "The product reviews summarization task aims to automatically produce a short summary for a set of reviews of a given product. Such summaries are expected to aggregate a range of different opinions in a concise, coherent and informative manner. This challenging task gives rise to two shortcomings in existing work. First, summarizers tend to favor generic content that appears in reviews for many different products, resulting in template-like, less informative summaries. Second, as reviewers often disagree on the pros and cons of a given product, summarizers sometimes yield inconsistent, self-contradicting summaries. We propose the PASS system (Perturb-and-Select Summarizer) that employs a large pre-trained Transformer-based model (T5 in our case), which follows a few-shot fine-tuning scheme. A key component of the PASS system relies on applying systematic perturbations to the model's input during inference, which allows it to generate multiple different summaries per product. We develop a method for ranking these summaries according to desired criteria, coherence in our case, enabling our system to almost entirely avoid the problem of self-contradiction. We compare our system against strong baselines on publicly available datasets, and show that it produces summaries which are more informative, diverse and coherent",
    "checked": true,
    "id": "af667edb95bc4b78031c9b8a5fbdc8e2ad65a90b",
    "semantic_title": "pass: perturb-and-select summarizer for product reviews",
    "citation_count": 18,
    "authors": [
      "Nadav Oved",
      "Ran Levy"
    ]
  },
  "https://aclanthology.org/2021.acl-long.31": {
    "title": "Deep Differential Amplifier for Extractive Summarization",
    "volume": "long",
    "abstract": "For sentence-level extractive summarization, there is a disproportionate ratio of selected and unselected sentences, leading to flatting the summary features when maximizing the accuracy. The imbalanced classification of summarization is inherent, which can't be addressed by common algorithms easily. In this paper, we conceptualize the single-document extractive summarization as a rebalance problem and present a deep differential amplifier framework. Specifically, we first calculate and amplify the semantic difference between each sentence and all other sentences, and then apply the residual unit as the second item of the differential amplifier to deepen the architecture. Finally, to compensate for the imbalance, the corresponding objective loss of minority class is boosted by a weighted cross-entropy. In contrast to previous approaches, this model pays more attention to the pivotal information of one sentence, instead of all the informative context modeling by recurrent or Transformer architecture. We demonstrate experimentally on two benchmark datasets that our summarizer performs competitively against state-of-the-art methods. Our source code will be available on Github",
    "checked": true,
    "id": "f9ebd3fa308d5741dc9edc84b3df6f9a3b528844",
    "semantic_title": "deep differential amplifier for extractive summarization",
    "citation_count": 8,
    "authors": [
      "Ruipeng Jia",
      "Yanan Cao",
      "Fang Fang",
      "Yuchen Zhou",
      "Zheng Fang",
      "Yanbing Liu",
      "Shi Wang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.32": {
    "title": "Multi-TimeLine Summarization (MTLS): Improving Timeline Summarization by Generating Multiple Summaries",
    "volume": "long",
    "abstract": "In this paper, we address a novel task, Multiple TimeLine Summarization (MTLS), which extends the flexibility and versatility of Time-Line Summarization (TLS). Given any collection of time-stamped news articles, MTLS automatically discovers important yet different stories and generates a corresponding time-line for each story. To achieve this, we propose a novel unsupervised summarization framework based on two-stage affinity propagation. We also introduce a quantitative evaluation measure for MTLS based on previousTLS evaluation methods. Experimental results show that our MTLS framework demonstrates high effectiveness and MTLS task can give bet-ter results than TLS",
    "checked": true,
    "id": "acf02e62879e84e359bc0fdd4bc86f87beef81a5",
    "semantic_title": "multi-timeline summarization (mtls): improving timeline summarization by generating multiple summaries",
    "citation_count": 18,
    "authors": [
      "Yi Yu",
      "Adam Jatowt",
      "Antoine Doucet",
      "Kazunari Sugiyama",
      "Masatoshi Yoshikawa"
    ]
  },
  "https://aclanthology.org/2021.acl-long.33": {
    "title": "Self-Supervised Multimodal Opinion Summarization",
    "volume": "long",
    "abstract": "Recently, opinion summarization, which is the generation of a summary from multiple reviews, has been conducted in a self-supervised manner by considering a sampled review as a pseudo summary. However, non-text data such as image and metadata related to reviews have been considered less often. To use the abundant information contained in non-text data, we propose a self-supervised multimodal opinion summarization framework called MultimodalSum. Our framework obtains a representation of each modality using a separate encoder for each modality, and the text decoder generates a summary. To resolve the inherent heterogeneity of multimodal data, we propose a multimodal training pipeline. We first pretrain the text encoder–decoder based solely on text modality data. Subsequently, we pretrain the non-text modality encoders by considering the pretrained text decoder as a pivot for the homogeneous representation of multimodal data. Finally, to fuse multimodal representations, we train the entire framework in an end-to-end manner. We demonstrate the superiority of MultimodalSum by conducting experiments on Yelp and Amazon datasets",
    "checked": true,
    "id": "99bd323be33f950d13df3569ca223ccffb308df1",
    "semantic_title": "self-supervised multimodal opinion summarization",
    "citation_count": 22,
    "authors": [
      "Jinbae Im",
      "Moonki Kim",
      "Hoyeop Lee",
      "Hyunsouk Cho",
      "Sehee Chung"
    ]
  },
  "https://aclanthology.org/2021.acl-long.34": {
    "title": "A Training-free and Reference-free Summarization Evaluation Metric via Centrality-weighted Relevance and Self-referenced Redundancy",
    "volume": "long",
    "abstract": "In recent years, reference-based and supervised summarization evaluation metrics have been widely explored. However, collecting human-annotated references and ratings are costly and time-consuming. To avoid these limitations, we propose a training-free and reference-free summarization evaluation metric. Our metric consists of a centrality-weighted relevance score and a self-referenced redundancy score. The relevance score is computed between the pseudo reference built from the source document and the given summary, where the pseudo reference content is weighted by the sentence centrality to provide importance guidance. Besides an F1-based relevance score, we also design an F𝛽-based variant that pays more attention to the recall score. As for the redundancy score of the summary, we compute a self-masked similarity score with the summary itself to evaluate the redundant information in the summary. Finally, we combine the relevance and redundancy scores to produce the final evaluation score of the given summary. Extensive experiments show that our methods can significantly outperform existing methods on both multi-document and single-document summarization evaluation. The source code is released at https://github.com/Chen-Wang-CUHK/Training-Free-and-Ref-Free-Summ-Evaluation",
    "checked": true,
    "id": "f635f5478a1102465dfeaff0afdda857291fb613",
    "semantic_title": "a training-free and reference-free summarization evaluation metric via centrality-weighted relevance and self-referenced redundancy",
    "citation_count": 16,
    "authors": [
      "Wang Chen",
      "Piji Li",
      "Irwin King"
    ]
  },
  "https://aclanthology.org/2021.acl-long.35": {
    "title": "DESCGEN: A Distantly Supervised Datasetfor Generating Entity Descriptions",
    "volume": "long",
    "abstract": "Short textual descriptions of entities provide summaries of their key attributes and have been shown to be useful sources of background knowledge for tasks such as entity linking and question answering. However, generating entity descriptions, especially for new and long-tail entities, can be challenging since relevant information is often scattered across multiple sources with varied content and style. We introduce DESCGEN: given mentions spread over multiple documents, the goal is to generate an entity summary description. DESCGEN consists of 37K entity descriptions from Wikipedia and Fandom, each paired with nine evidence documents on average. The documents were collected using a combination of entity linking and hyperlinks into the entity pages, which together provide high-quality distant supervision. Compared to other multi-document summarization tasks, our task is entity-centric, more abstractive, and covers a wide range of domains. We also propose a two-stage extract-then-generate baseline and show that there exists a large gap (19.9% in ROUGE-L) between state-of-art models and human performance, suggesting that the data will support significant future work",
    "checked": true,
    "id": "304584c54c2e06fe26df675d65d0605dcfe426c7",
    "semantic_title": "descgen: a distantly supervised datasetfor generating entity descriptions",
    "citation_count": 2,
    "authors": [
      "Weijia Shi",
      "Mandar Joshi",
      "Luke Zettlemoyer"
    ]
  },
  "https://aclanthology.org/2021.acl-long.36": {
    "title": "Introducing Orthogonal Constraint in Structural Probes",
    "volume": "long",
    "abstract": "With the recent success of pre-trained models in NLP, a significant focus was put on interpreting their representations. One of the most prominent approaches is structural probing (Hewitt and Manning, 2019), where a linear projection of word embeddings is performed in order to approximate the topology of dependency structures. In this work, we introduce a new type of structural probing, where the linear projection is decomposed into 1. iso-morphic space rotation; 2. linear scaling that identifies and scales the most relevant dimensions. In addition to syntactic dependency, we evaluate our method on two novel tasks (lexical hypernymy and position in a sentence). We jointly train the probes for multiple tasks and experimentally show that lexical and syntactic information is separated in the representations. Moreover, the orthogonal constraint makes the Structural Probes less vulnerable to memorization",
    "checked": true,
    "id": "c6f84341be562043d107575b5beb1fc1575c9382",
    "semantic_title": "introducing orthogonal constraint in structural probes",
    "citation_count": 12,
    "authors": [
      "Tomasz Limisiewicz",
      "David Mareček"
    ]
  },
  "https://aclanthology.org/2021.acl-long.37": {
    "title": "Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger",
    "volume": "long",
    "abstract": "Backdoor attacks are a kind of insidious security threat against machine learning models. After being injected with a backdoor in training, the victim model will produce adversary-specified outputs on the inputs embedded with predesigned triggers but behave properly on normal inputs during inference. As a sort of emergent attack, backdoor attacks in natural language processing (NLP) are investigated insufficiently. As far as we know, almost all existing textual backdoor attack methods insert additional contents into normal samples as triggers, which causes the trigger-embedded samples to be detected and the backdoor attacks to be blocked without much effort. In this paper, we propose to use the syntactic structure as the trigger in textual backdoor attacks. We conduct extensive experiments to demonstrate that the syntactic trigger-based attack method can achieve comparable attack performance (almost 100% success rate) to the insertion-based methods but possesses much higher invisibility and stronger resistance to defenses. These results also reveal the significant insidiousness and harmfulness of textual backdoor attacks. All the code and data of this paper can be obtained at https://github.com/thunlp/HiddenKiller",
    "checked": true,
    "id": "ac6d17a1e4345b6699965fca636590edb91f10a8",
    "semantic_title": "hidden killer: invisible textual backdoor attacks with syntactic trigger",
    "citation_count": 92,
    "authors": [
      "Fanchao Qi",
      "Mukai Li",
      "Yangyi Chen",
      "Zhengyan Zhang",
      "Zhiyuan Liu",
      "Yasheng Wang",
      "Maosong Sun"
    ]
  },
  "https://aclanthology.org/2021.acl-long.38": {
    "title": "Examining the Inductive Bias of Neural Language Models with Artificial Languages",
    "volume": "long",
    "abstract": "Since language models are used to model a wide variety of languages, it is natural to ask whether the neural architectures used for the task have inductive biases towards modeling particular types of languages. Investigation of these biases has proved complicated due to the many variables that appear in the experimental setup. Languages vary in many typological dimensions, and it is difficult to single out one or two to investigate without the others acting as confounders. We propose a novel method for investigating the inductive biases of language models using artificial languages. These languages are constructed to allow us to create parallel corpora across languages that differ only in the typological feature being investigated, such as word order. We then use them to train and test language models. This constitutes a fully controlled causal framework, and demonstrates how grammar engineering can serve as a useful tool for analyzing neural models. Using this method, we find that commonly used neural architectures exhibit different inductive biases: LSTMs display little preference with respect to word ordering, while transformers display a clear preference for some orderings over others. Further, we find that neither the inductive bias of the LSTM nor that of the transformer appear to reflect any tendencies that we see in attested natural languages",
    "checked": true,
    "id": "83ed3184cc7b2dcee3c2b91529870bc304513468",
    "semantic_title": "examining the inductive bias of neural language models with artificial languages",
    "citation_count": 21,
    "authors": [
      "Jennifer C. White",
      "Ryan Cotterell"
    ]
  },
  "https://aclanthology.org/2021.acl-long.39": {
    "title": "Explaining Contextualization in Language Models using Visual Analytics",
    "volume": "long",
    "abstract": "Despite the success of contextualized language models on various NLP tasks, it is still unclear what these models really learn. In this paper, we contribute to the current efforts of explaining such models by exploring the continuum between function and content words with respect to contextualization in BERT, based on linguistically-informed insights. In particular, we utilize scoring and visual analytics techniques: we use an existing similarity-based score to measure contextualization and integrate it into a novel visual analytics technique, presenting the model's layers simultaneously and highlighting intra-layer properties and inter-layer differences. We show that contextualization is neither driven by polysemy nor by pure context variation. We also provide insights on why BERT fails to model words in the middle of the functionality continuum",
    "checked": true,
    "id": "442d18798e64c988954e2e7fdb11bcae98fcda2a",
    "semantic_title": "explaining contextualization in language models using visual analytics",
    "citation_count": 9,
    "authors": [
      "Rita Sevastjanova",
      "Aikaterini-Lida Kalouli",
      "Christin Beck",
      "Hanna Schäfer",
      "Mennatallah El-Assady"
    ]
  },
  "https://aclanthology.org/2021.acl-long.40": {
    "title": "Improving the Faithfulness of Attention-based Explanations with Task-specific Information for Text Classification",
    "volume": "long",
    "abstract": "Neural network architectures in natural language processing often use attention mechanisms to produce probability distributions over input token representations. Attention has empirically been demonstrated to improve performance in various tasks, while its weights have been extensively used as explanations for model predictions. Recent studies (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019) have showed that it cannot generally be considered as a faithful explanation (Jacovi and Goldberg, 2020) across encoders and tasks. In this paper, we seek to improve the faithfulness of attention-based explanations for text classification. We achieve this by proposing a new family of Task-Scaling (TaSc) mechanisms that learn task-specific non-contextualised information to scale the original attention weights. Evaluation tests for explanation faithfulness, show that the three proposed variants of TaSc improve attention-based explanations across two attention mechanisms, five encoders and five text classification datasets without sacrificing predictive performance. Finally, we demonstrate that TaSc consistently provides more faithful attention-based explanations compared to three widely-used interpretability techniques",
    "checked": true,
    "id": "32a6daa76efd00f657e65842771971decf104efc",
    "semantic_title": "improving the faithfulness of attention-based explanations with task-specific information for text classification",
    "citation_count": 23,
    "authors": [
      "George Chrysostomou",
      "Nikolaos Aletras"
    ]
  },
  "https://aclanthology.org/2021.acl-long.41": {
    "title": "Generating Landmark Navigation Instructions from Maps as a Graph-to-Text Problem",
    "volume": "long",
    "abstract": "Car-focused navigation services are based on turns and distances of named streets, whereas navigation instructions naturally used by humans are centered around physical objects called landmarks. We present a neural model that takes OpenStreetMap representations as input and learns to generate navigation instructions that contain visible and salient landmarks from human natural language instructions. Routes on the map are encoded in a location- and rotation-invariant graph representation that is decoded into natural language instructions. Our work is based on a novel dataset of 7,672 crowd-sourced instances that have been verified by human navigation in Street View. Our evaluation shows that the navigation instructions generated by our system have similar properties as human-generated instructions, and lead to successful human navigation in Street View",
    "checked": true,
    "id": "5c15688fb335e1e3303094d85751bd62018e2c36",
    "semantic_title": "generating landmark navigation instructions from maps as a graph-to-text problem",
    "citation_count": 13,
    "authors": [
      "Raphael Schumann",
      "Stefan Riezler"
    ]
  },
  "https://aclanthology.org/2021.acl-long.42": {
    "title": "E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning",
    "volume": "long",
    "abstract": "Vision-language pre-training (VLP) on large-scale image-text pairs has achieved huge success for the cross-modal downstream tasks. The most existing pre-training methods mainly adopt a two-step training procedure, which firstly employs a pre-trained object detector to extract region-based visual features, then concatenates the image representation and text embedding as the input of Transformer to train. However, these methods face problems of using task-specific visual representation of the specific object detector for generic cross-modal understanding, and the computation inefficiency of two-stage pipeline. In this paper, we propose the first end-to-end vision-language pre-trained model for both V+L understanding and generation, namely E2E-VLP, where we build a unified Transformer framework to jointly learn visual representation, and semantic alignments between image and text. We incorporate the tasks of object detection and image captioning into pre-training with a unified Transformer encoder-decoder architecture for enhancing visual learning. An extensive set of experiments have been conducted on well-established vision-language downstream tasks to demonstrate the effectiveness of this novel VLP paradigm",
    "checked": true,
    "id": "fea02a76f504f6dfefd2497220da913c5274f5ab",
    "semantic_title": "e2e-vlp: end-to-end vision-language pre-training enhanced by visual learning",
    "citation_count": 85,
    "authors": [
      "Haiyang Xu",
      "Ming Yan",
      "Chenliang Li",
      "Bin Bi",
      "Songfang Huang",
      "Wenming Xiao",
      "Fei Huang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.43": {
    "title": "Learning Relation Alignment for Calibrated Cross-modal Retrieval",
    "volume": "long",
    "abstract": "Despite the achievements of large-scale multimodal pre-training approaches, cross-modal retrieval, e.g., image-text retrieval, remains a challenging task. To bridge the semantic gap between the two modalities, previous studies mainly focus on word-region alignment at the object level, lacking the matching between the linguistic relation among the words and the visual relation among the regions. The neglect of such relation consistency impairs the contextualized representation of image-text pairs and hinders the model performance and the interpretability. In this paper, we first propose a novel metric, Intra-modal Self-attention Distance (ISD), to quantify the relation consistency by measuring the semantic distance between linguistic and visual relations. In response, we present Inter-modal Alignment on Intra-modal Self-attentions (IAIS), a regularized training method to optimize the ISD and calibrate intra-modal self-attentions from the two modalities mutually via inter-modal alignment. The IAIS regularizer boosts the performance of prevailing models on Flickr30k and MS COCO datasets by a considerable margin, which demonstrates the superiority of our approach",
    "checked": true,
    "id": "b78aa53020b70d7d30f31b15e3721c0a6891e703",
    "semantic_title": "learning relation alignment for calibrated cross-modal retrieval",
    "citation_count": 19,
    "authors": [
      "Shuhuai Ren",
      "Junyang Lin",
      "Guangxiang Zhao",
      "Rui Men",
      "An Yang",
      "Jingren Zhou",
      "Xu Sun",
      "Hongxia Yang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.44": {
    "title": "KM-BART: Knowledge Enhanced Multimodal BART for Visual Commonsense Generation",
    "volume": "long",
    "abstract": "We present Knowledge Enhanced Multimodal BART (KM-BART), which is a Transformer-based sequence-to-sequence model capable of reasoning about commonsense knowledge from multimodal inputs of images and texts. We adapt the generative BART architecture (Lewis et al., 2020) to a multimodal model with visual and textual inputs. We further develop novel pretraining tasks to improve the model performance on the Visual Commonsense Generation (VCG) task. In particular, our pretraining task of Knowledge-based Commonsense Generation (KCG) boosts model performance on the VCG task by leveraging commonsense knowledge from a large language model pretrained on external commonsense knowledge graphs. To the best of our knowledge, we are the first to propose a dedicated task for improving model performance on the VCG task. Experimental results show that our model reaches state-of-the-art performance on the VCG task (Park et al., 2020) by applying these novel pretraining tasks",
    "checked": true,
    "id": "fe6e9bc5040a69e310d88677a1045a2fef640f48",
    "semantic_title": "km-bart: knowledge enhanced multimodal bart for visual commonsense generation",
    "citation_count": 24,
    "authors": [
      "Yiran Xing",
      "Zai Shi",
      "Zhao Meng",
      "Gerhard Lakemeyer",
      "Yunpu Ma",
      "Roger Wattenhofer"
    ]
  },
  "https://aclanthology.org/2021.acl-long.45": {
    "title": "Cascaded Head-colliding Attention",
    "volume": "long",
    "abstract": "Transformers have advanced the field of natural language processing (NLP) on a variety of important tasks. At the cornerstone of the Transformer architecture is the multi-head attention (MHA) mechanism which models pairwise interactions between the elements of the sequence. Despite its massive success, the current framework ignores interactions among different heads, leading to the problem that many of the heads are redundant in practice, which greatly wastes the capacity of the model. To improve parameter efficiency, we re-formulate the MHA as a latent variable model from a probabilistic perspective. We present cascaded head-colliding attention (CODA) which explicitly models the interactions between attention heads through a hierarchical variational distribution. We conduct extensive experiments and demonstrate that CODA outperforms the transformer baseline, by 0.6 perplexity on Wikitext-103 in language modeling, and by 0.6 BLEU on WMT14 EN-DE in machine translation, due to its improvements on the parameter efficiency",
    "checked": true,
    "id": "8b3ab63b477767c461e5ad208d106ab4df8bb0ec",
    "semantic_title": "cascaded head-colliding attention",
    "citation_count": 2,
    "authors": [
      "Lin Zheng",
      "Zhiyong Wu",
      "Lingpeng Kong"
    ]
  },
  "https://aclanthology.org/2021.acl-long.46": {
    "title": "Structural Knowledge Distillation: Tractably Distilling Information for Structured Predictor",
    "volume": "long",
    "abstract": "Knowledge distillation is a critical technique to transfer knowledge between models, typically from a large model (the teacher) to a more fine-grained one (the student). The objective function of knowledge distillation is typically the cross-entropy between the teacher and the student's output distributions. However, for structured prediction problems, the output space is exponential in size; therefore, the cross-entropy objective becomes intractable to compute and optimize directly. In this paper, we derive a factorized form of the knowledge distillation objective for structured prediction, which is tractable for many typical choices of the teacher and student models. In particular, we show the tractability and empirical effectiveness of structural knowledge distillation between sequence labeling and dependency parsing models under four different scenarios: 1) the teacher and student share the same factorization form of the output structure scoring function; 2) the student factorization produces more fine-grained substructures than the teacher factorization; 3) the teacher factorization produces more fine-grained substructures than the student factorization; 4) the factorization forms from the teacher and the student are incompatible",
    "checked": true,
    "id": "a29af72749b3f080779a2f0ab4f03c4ca46d9236",
    "semantic_title": "structural knowledge distillation: tractably distilling information for structured predictor",
    "citation_count": 9,
    "authors": [
      "Xinyu Wang",
      "Yong Jiang",
      "Zhaohui Yan",
      "Zixia Jia",
      "Nguyen Bach",
      "Tao Wang",
      "Zhongqiang Huang",
      "Fei Huang",
      "Kewei Tu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.47": {
    "title": "Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks",
    "volume": "long",
    "abstract": "State-of-the-art parameter-efficient fine-tuning methods rely on introducing adapter modules between the layers of a pretrained language model. However, such modules are trained separately for each task and thus do not enable sharing information across tasks. In this paper, we show that we can learn adapter parameters for all layers and tasks by generating them using shared hypernetworks, which condition on task, adapter position, and layer id in a transformer model. This parameter-efficient multi-task learning framework allows us to achieve the best of both worlds by sharing knowledge across tasks via hypernetworks while enabling the model to adapt to each individual task through task-specific adapters. Experiments on the well-known GLUE benchmark show improved performance in multi-task learning while adding only 0.29% parameters per task. We additionally demonstrate substantial performance improvements in few-shot domain generalization across a variety of tasks. Our code is publicly available in https://github.com/rabeehk/hyperformer",
    "checked": true,
    "id": "bb3425318de7eed5641cda147d61c9a057b9d054",
    "semantic_title": "parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks",
    "citation_count": 152,
    "authors": [
      "Rabeeh Karimi Mahabadi",
      "Sebastian Ruder",
      "Mostafa Dehghani",
      "James Henderson"
    ]
  },
  "https://aclanthology.org/2021.acl-long.48": {
    "title": "COSY: COunterfactual SYntax for Cross-Lingual Understanding",
    "volume": "long",
    "abstract": "Pre-trained multilingual language models, e.g., multilingual-BERT, are widely used in cross-lingual tasks, yielding the state-of-the-art performance. However, such models suffer from a large performance gap between source and target languages, especially in the zero-shot setting, where the models are fine-tuned only on English but tested on other languages for the same task. We tackle this issue by incorporating language-agnostic information, specifically, universal syntax such as dependency relations and POS tags, into language models, based on the observation that universal syntax is transferable across different languages. Our approach, called COunterfactual SYntax (COSY), includes the design of SYntax-aware networks as well as a COunterfactual training method to implicitly force the networks to learn not only the semantics but also the syntax. To evaluate COSY, we conduct cross-lingual experiments on natural language inference and question answering using mBERT and XLM-R as network backbones. Our results show that COSY achieves the state-of-the-art performance for both tasks, without using auxiliary training data",
    "checked": true,
    "id": "734089edd71619a3a2b3cebfe63f97dd92b2671c",
    "semantic_title": "cosy: counterfactual syntax for cross-lingual understanding",
    "citation_count": 5,
    "authors": [
      "Sicheng Yu",
      "Hao Zhang",
      "Yulei Niu",
      "Qianru Sun",
      "Jing Jiang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.49": {
    "title": "OoMMix: Out-of-manifold Regularization in Contextual Embedding Space for Text Classification",
    "volume": "long",
    "abstract": "Recent studies on neural networks with pre-trained weights (i.e., BERT) have mainly focused on a low-dimensional subspace, where the embedding vectors computed from input words (or their contexts) are located. In this work, we propose a new approach, called OoMMix, to finding and regularizing the remainder of the space, referred to as out-of-manifold, which cannot be accessed through the words. Specifically, we synthesize the out-of-manifold embeddings based on two embeddings obtained from actually-observed words, to utilize them for fine-tuning the network. A discriminator is trained to detect whether an input embedding is located inside the manifold or not, and simultaneously, a generator is optimized to produce new embeddings that can be easily identified as out-of-manifold by the discriminator. These two modules successfully collaborate in a unified and end-to-end manner for regularizing the out-of-manifold. Our extensive evaluation on various text classification benchmarks demonstrates the effectiveness of our approach, as well as its good compatibility with existing data augmentation techniques which aim to enhance the manifold",
    "checked": true,
    "id": "ff5cdf3b07d45369c0f2702cf156f03f3e68c323",
    "semantic_title": "oommix: out-of-manifold regularization in contextual embedding space for text classification",
    "citation_count": 2,
    "authors": [
      "Seonghyeon Lee",
      "Dongha Lee",
      "Hwanjo Yu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.50": {
    "title": "Understanding and Countering Stereotypes: A Computational Approach to the Stereotype Content Model",
    "volume": "long",
    "abstract": "Stereotypical language expresses widely-held beliefs about different social categories. Many stereotypes are overtly negative, while others may appear positive on the surface, but still lead to negative consequences. In this work, we present a computational approach to interpreting stereotypes in text through the Stereotype Content Model (SCM), a comprehensive causal theory from social psychology. The SCM proposes that stereotypes can be understood along two primary dimensions: warmth and competence. We present a method for defining warmth and competence axes in semantic embedding space, and show that the four quadrants defined by this subspace accurately represent the warmth and competence concepts, according to annotated lexicons. We then apply our computational SCM model to textual stereotype data and show that it compares favourably with survey-based studies in the psychological literature. Furthermore, we explore various strategies to counter stereotypical beliefs with anti-stereotypes. It is known that countering stereotypes with anti-stereotypical examples is one of the most effective ways to reduce biased thinking, yet the problem of generating anti-stereotypes has not been previously studied. Thus, a better understanding of how to generate realistic and effective anti-stereotypes can contribute to addressing pressing societal concerns of stereotyping, prejudice, and discrimination",
    "checked": true,
    "id": "d8595df785e647cd80aa86a2863731841bc7de2e",
    "semantic_title": "understanding and countering stereotypes: a computational approach to the stereotype content model",
    "citation_count": 18,
    "authors": [
      "Kathleen C. Fraser",
      "Isar Nejadgholi",
      "Svetlana Kiritchenko"
    ]
  },
  "https://aclanthology.org/2021.acl-long.51": {
    "title": "Structurizing Misinformation Stories via Rationalizing Fact-Checks",
    "volume": "long",
    "abstract": "Misinformation has recently become a well-documented matter of public concern. Existing studies on this topic have hitherto adopted a coarse concept of misinformation, which incorporates a broad spectrum of story types ranging from political conspiracies to misinterpreted pranks. This paper aims to structurize these misinformation stories by leveraging fact-check articles. Our intuition is that key phrases in a fact-check article that identify the misinformation type(s) (e.g., doctored images, urban legends) also act as rationales that determine the verdict of the fact-check (e.g., false). We experiment on rationalized models with domain knowledge as weak supervision to extract these phrases as rationales, and then cluster semantically similar rationales to summarize prevalent misinformation types. Using archived fact-checks from Snopes.com, we identify ten types of misinformation stories. We discuss how these types have evolved over the last ten years and compare their prevalence between the 2016/2020 US presidential elections and the H1N1/COVID-19 pandemics",
    "checked": true,
    "id": "123990544303c2bc0098a2cf4d5a6f31a04c55e9",
    "semantic_title": "structurizing misinformation stories via rationalizing fact-checks",
    "citation_count": 5,
    "authors": [
      "Shan Jiang",
      "Christo Wilson"
    ]
  },
  "https://aclanthology.org/2021.acl-long.52": {
    "title": "Modeling Language Usage and Listener Engagement in Podcasts",
    "volume": "long",
    "abstract": "While there is an abundance of advice to podcast creators on how to speak in ways that engage their listeners, there has been little data-driven analysis of podcasts that relates linguistic style with engagement. In this paper, we investigate how various factors – vocabulary diversity, distinctiveness, emotion, and syntax, among others – correlate with engagement, based on analysis of the creators' written descriptions and transcripts of the audio. We build models with different textual representations, and show that the identified features are highly predictive of engagement. Our analysis tests popular wisdom about stylistic elements in high-engagement podcasts, corroborating some pieces of advice and adding new perspectives on others",
    "checked": true,
    "id": "d38102db4ee52ecf309bbd78eb9d426c1364ad03",
    "semantic_title": "modeling language usage and listener engagement in podcasts",
    "citation_count": 7,
    "authors": [
      "Sravana Reddy",
      "Mariya Lazarova",
      "Yongze Yu",
      "Rosie Jones"
    ]
  },
  "https://aclanthology.org/2021.acl-long.53": {
    "title": "Breaking Down the Invisible Wall of Informal Fallacies in Online Discussions",
    "volume": "long",
    "abstract": "People debate on a variety of topics on online platforms such as Reddit, or Facebook. Debates can be lengthy, with users exchanging a wealth of information and opinions. However, conversations do not always go smoothly, and users sometimes engage in unsound argumentation techniques to prove a claim. These techniques are called fallacies. Fallacies are persuasive arguments that provide insufficient or incorrect evidence to support the claim. In this paper, we study the most frequent fallacies on Reddit, and we present them using the pragma-dialectical theory of argumentation. We construct a new annotated dataset of fallacies, using user comments containing fallacy mentions as noisy labels, and cleaning the data via crowdsourcing. Finally, we study the task of classifying fallacies using neural models. We find that generally the models perform better in the presence of conversational context. We have released the data and the code at github.com/sahaisaumya/informal_fallacies",
    "checked": true,
    "id": "bcf086239b9d0ddc12fb88c1b57b56928f36f16b",
    "semantic_title": "breaking down the invisible wall of informal fallacies in online discussions",
    "citation_count": 6,
    "authors": [
      "Saumya Sahai",
      "Oana Balalau",
      "Roxana Horincar"
    ]
  },
  "https://aclanthology.org/2021.acl-long.54": {
    "title": "SocAoG: Incremental Graph Parsing for Social Relation Inference in Dialogues",
    "volume": "long",
    "abstract": "Inferring social relations from dialogues is vital for building emotionally intelligent robots to interpret human language better and act accordingly. We model the social network as an And-or Graph, named SocAoG, for the consistency of relations among a group and leveraging attributes as inference cues. Moreover, we formulate a sequential structure prediction task, and propose an 𝛼-𝛽-𝛾 strategy to incrementally parse SocAoG for the dynamic inference upon any incoming utterance: (i) an 𝛼 process predicting attributes and relations conditioned on the semantics of dialogues, (ii) a 𝛽 process updating the social relations based on related attributes, and (iii) a 𝛾 process updating individual's attributes based on interpersonal social relations. Empirical results on DialogRE and MovieGraph show that our model infers social relations more accurately than the state-of-the-art methods. Moreover, the ablation study shows the three processes complement each other, and the case study demonstrates the dynamic relational inference",
    "checked": true,
    "id": "acd43357c8ef49524ce5fd5b0e48a1f760f9dbef",
    "semantic_title": "socaog: incremental graph parsing for social relation inference in dialogues",
    "citation_count": 11,
    "authors": [
      "Liang Qiu",
      "Yuan Liang",
      "Yizhou Zhao",
      "Pan Lu",
      "Baolin Peng",
      "Zhou Yu",
      "Ying Nian Wu",
      "Song-Chun Zhu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.55": {
    "title": "TicketTalk: Toward human-level performance with end-to-end, transaction-based dialog systems",
    "volume": "long",
    "abstract": "We present a data-driven, end-to-end approach to transaction-based dialog systems that performs at near-human levels in terms of verbal response quality and factual grounding accuracy. We show that two essential components of the system produce these results: a sufficiently large and diverse, in-domain labeled dataset, and a neural network-based, pre-trained model that generates both verbal responses and API call predictions. In terms of data, we introduce TicketTalk, a movie ticketing dialog dataset with 23,789 annotated conversations. The conversations range from completely open-ended and unrestricted to more structured, both in terms of their knowledge base, discourse features, and number of turns. In qualitative human evaluations, model-generated responses trained on just 10,000 TicketTalk dialogs were rated to \"make sense\" 86.5% of the time, almost the same as human responses in the same contexts. Our simple, API-focused annotation schema results in a much easier labeling task making it faster and more cost effective. It is also the key component for being able to predict API calls accurately. We handle factual grounding by incorporating API calls in the training data, allowing our model to learn which actions to take and when. Trained on the same 10,000-dialog set, the model's API call predictions were rated to be correct 93.9% of the time in our evaluations, surpassing the ratings for the corresponding human labels. We show how API prediction and response generation scores improve as the dataset size incrementally increases from 5000 to 21,000 dialogs. Our analysis also clearly illustrates the benefits of pre-training. To facilitate future work on transaction-based dialog systems, we are publicly releasing the TicketTalk dataset at https://git.io/JL8an",
    "checked": true,
    "id": "c7d1487552f645afc2daa9b8439ab7fa1adc3d5e",
    "semantic_title": "tickettalk: toward human-level performance with end-to-end, transaction-based dialog systems",
    "citation_count": 19,
    "authors": [
      "Bill Byrne",
      "Karthik Krishnamoorthi",
      "Saravanan Ganesh",
      "Mihir Kale"
    ]
  },
  "https://aclanthology.org/2021.acl-long.56": {
    "title": "Improving Dialog Systems for Negotiation with Personality Modeling",
    "volume": "long",
    "abstract": "In this paper, we explore the ability to model and infer personality types of opponents, predict their responses, and use this information to adapt a dialog agent's high-level strategy in negotiation tasks. Inspired by the idea of incorporating a theory of mind (ToM) into machines, we introduce a probabilistic formulation to encapsulate the opponent's personality type during both learning and inference. We test our approach on the CraigslistBargain dataset (He et al. 2018) and show that our method using ToM inference achieves a 20% higher dialog agreement rate compared to baselines on a mixed population of opponents. We also demonstrate that our model displays diverse negotiation behavior with different types of opponents",
    "checked": true,
    "id": "7bf03c12f74a83852fc831c468aa754e0008a6d7",
    "semantic_title": "improving dialog systems for negotiation with personality modeling",
    "citation_count": 19,
    "authors": [
      "Runzhe Yang",
      "Jingxiao Chen",
      "Karthik Narasimhan"
    ]
  },
  "https://aclanthology.org/2021.acl-long.57": {
    "title": "Learning from Perturbations: Diverse and Informative Dialogue Generation with Inverse Adversarial Training",
    "volume": "long",
    "abstract": "In this paper, we propose Inverse Adversarial Training (IAT) algorithm for training neural dialogue systems to avoid generic responses and model dialogue history better. In contrast to standard adversarial training algorithms, IAT encourages the model to be sensitive to the perturbation in the dialogue history and therefore learning from perturbations. By giving higher rewards for responses whose output probability reduces more significantly when dialogue history is perturbed, the model is encouraged to generate more diverse and consistent responses. By penalizing the model when generating the same response given perturbed dialogue history, the model is forced to better capture dialogue history and generate more informative responses. Experimental results on two benchmark datasets show that our approach can better model dialogue history and generate more diverse and consistent responses. In addition, we point out a problem of the widely used maximum mutual information (MMI) based methods for improving the diversity of dialogue response generation models and demonstrate it empirically",
    "checked": true,
    "id": "a4ab2f3e6335d3f1408da16d5f9152d49a3afc18",
    "semantic_title": "learning from perturbations: diverse and informative dialogue generation with inverse adversarial training",
    "citation_count": 15,
    "authors": [
      "Wangchunshu Zhou",
      "Qifei Li",
      "Chenle Li"
    ]
  },
  "https://aclanthology.org/2021.acl-long.58": {
    "title": "Increasing Faithfulness in Knowledge-Grounded Dialogue with Controllable Features",
    "volume": "long",
    "abstract": "Knowledge-grounded dialogue systems are intended to convey information that is based on evidence provided in a given source text. We discuss the challenges of training a generative neural dialogue model for such systems that is controlled to stay faithful to the evidence. Existing datasets contain a mix of conversational responses that are faithful to selected evidence as well as more subjective or chit-chat style responses. We propose different evaluation measures to disentangle these different styles of responses by quantifying the informativeness and objectivity. At training time, additional inputs based on these evaluation measures are given to the dialogue model. At generation time, these additional inputs act as stylistic controls that encourage the model to generate responses that are faithful to the provided evidence. We also investigate the usage of additional controls at decoding time using resampling techniques. In addition to automatic metrics, we perform a human evaluation study where raters judge the output of these controlled generation models to be generally more objective and faithful to the evidence compared to baseline dialogue systems",
    "checked": true,
    "id": "ad219f774290a128a3c2150fa93ed9b39b5b2a65",
    "semantic_title": "increasing faithfulness in knowledge-grounded dialogue with controllable features",
    "citation_count": 70,
    "authors": [
      "Hannah Rashkin",
      "David Reitter",
      "Gaurav Singh Tomar",
      "Dipanjan Das"
    ]
  },
  "https://aclanthology.org/2021.acl-long.59": {
    "title": "CitationIE: Leveraging the Citation Graph for Scientific Information Extraction",
    "volume": "long",
    "abstract": "Automatically extracting key information from scientific documents has the potential to help scientists work more efficiently and accelerate the pace of scientific progress. Prior work has considered extracting document-level entity clusters and relations end-to-end from raw scientific text, which can improve literature search and help identify methods and materials for a given problem. Despite the importance of this task, most existing works on scientific information extraction (SciIE) consider extraction solely based on the content of an individual paper, without considering the paper's place in the broader literature. In contrast to prior work, we augment our text representations by leveraging a complementary source of document context: the citation graph of referential links between citing and cited papers. On a test set of English-language scientific documents, we show that simple ways of utilizing the structure and content of the citation graph can each lead to significant gains in different scientific information extraction tasks. When these tasks are combined, we observe a sizable improvement in end-to-end information extraction over the state-of-the-art, suggesting the potential for future work along this direction. We release software tools to facilitate citation-aware SciIE development",
    "checked": true,
    "id": "21c9c624bc328686cef4bb1f80a786a5027d8886",
    "semantic_title": "citationie: leveraging the citation graph for scientific information extraction",
    "citation_count": 16,
    "authors": [
      "Vijay Viswanathan",
      "Graham Neubig",
      "Pengfei Liu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.60": {
    "title": "From Discourse to Narrative: Knowledge Projection for Event Relation Extraction",
    "volume": "long",
    "abstract": "Current event-centric knowledge graphs highly rely on explicit connectives to mine relations between events. Unfortunately, due to the sparsity of connectives, these methods severely undermine the coverage of EventKGs. The lack of high-quality labelled corpora further exacerbates that problem. In this paper, we propose a knowledge projection paradigm for event relation extraction: projecting discourse knowledge to narratives by exploiting the commonalities between them. Specifically, we propose Multi-tier Knowledge Projection Network (MKPNet), which can leverage multi-tier discourse knowledge effectively for event relation extraction. In this way, the labelled data requirement is significantly reduced, and implicit event relations can be effectively extracted. Intrinsic experimental results show that MKPNet achieves the new state-of-the-art performance and extrinsic experimental results verify the value of the extracted event relations",
    "checked": true,
    "id": "6e33a455df9f03d54e9c35adbf1111cc32faa21e",
    "semantic_title": "from discourse to narrative: knowledge projection for event relation extraction",
    "citation_count": 11,
    "authors": [
      "Jialong Tang",
      "Hongyu Lin",
      "Meng Liao",
      "Yaojie Lu",
      "Xianpei Han",
      "Le Sun",
      "Weijian Xie",
      "Jin Xu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.61": {
    "title": "AdvPicker: Effectively Leveraging Unlabeled Data via Adversarial Discriminator for Cross-Lingual NER",
    "volume": "long",
    "abstract": "Neural methods have been shown to achieve high performance in Named Entity Recognition (NER), but rely on costly high-quality labeled data for training, which is not always available across languages. While previous works have shown that unlabeled data in a target language can be used to improve cross-lingual model performance, we propose a novel adversarial approach (AdvPicker) to better leverage such data and further improve results. We design an adversarial learning framework in which an encoder learns entity domain knowledge from labeled source-language data and better shared features are captured via adversarial training - where a discriminator selects less language-dependent target-language data via similarity to the source language. Experimental results on standard benchmark datasets well demonstrate that the proposed method benefits strongly from this data selection process and outperforms existing state-of-the-art methods; without requiring any additional external resources (e.g., gazetteers or via machine translation)",
    "checked": true,
    "id": "06691af1a1775bcbba48887b5deaa8f1495ce55e",
    "semantic_title": "advpicker: effectively leveraging unlabeled data via adversarial discriminator for cross-lingual ner",
    "citation_count": 24,
    "authors": [
      "Weile Chen",
      "Huiqiang Jiang",
      "Qianhui Wu",
      "Börje Karlsson",
      "Yi Guan"
    ]
  },
  "https://aclanthology.org/2021.acl-long.62": {
    "title": "Compare to The Knowledge: Graph Neural Fake News Detection with External Knowledge",
    "volume": "long",
    "abstract": "Nowadays, fake news detection, which aims to verify whether a news document is trusted or fake, has become urgent and important. Most existing methods rely heavily on linguistic and semantic features from the news content, and fail to effectively exploit external knowledge which could help determine whether the news document is trusted. In this paper, we propose a novel end-to-end graph neural model called CompareNet, which compares the news to the knowledge base (KB) through entities for fake news detection. Considering that fake news detection is correlated with topics, we also incorporate topics to enrich the news representation. Specifically, we first construct a directed heterogeneous document graph for each news incorporating topics and entities. Based on the graph, we develop a heterogeneous graph attention network for learning the topic-enriched news representation as well as the contextual entity representations that encode the semantics of the news content. The contextual entity representations are then compared to the corresponding KB-based entity representations through a carefully designed entity comparison network, to capture the consistency between the news content and KB. Finally, the topic-enriched news representation combining the entity comparison features is fed into a fake news classifier. Experimental results on two benchmark datasets demonstrate that CompareNet significantly outperforms state-of-the-art methods",
    "checked": true,
    "id": "f95a4568a714c34984aa32327fa66344ebe52861",
    "semantic_title": "compare to the knowledge: graph neural fake news detection with external knowledge",
    "citation_count": 79,
    "authors": [
      "Linmei Hu",
      "Tianchi Yang",
      "Luhao Zhang",
      "Wanjun Zhong",
      "Duyu Tang",
      "Chuan Shi",
      "Nan Duan",
      "Ming Zhou"
    ]
  },
  "https://aclanthology.org/2021.acl-long.63": {
    "title": "Discontinuous Named Entity Recognition as Maximal Clique Discovery",
    "volume": "long",
    "abstract": "Named entity recognition (NER) remains challenging when entity mentions can be discontinuous. Existing methods break the recognition process into several sequential steps. In training, they predict conditioned on the golden intermediate results, while at inference relying on the model output of the previous steps, which introduces exposure bias. To solve this problem, we first construct a segment graph for each sentence, in which each node denotes a segment (a continuous entity on its own, or a part of discontinuous entities), and an edge links two nodes that belong to the same entity. The nodes and edges can be generated respectively in one stage with a grid tagging scheme and learned jointly using a novel architecture named Mac. Then discontinuous NER can be reformulated as a non-parametric process of discovering maximal cliques in the graph and concatenating the spans in each clique. Experiments on three benchmarks show that our method outperforms the state-of-the-art (SOTA) results, with up to 3.5 percentage points improvement on F1, and achieves 5x speedup over the SOTA model",
    "checked": true,
    "id": "ca7149790938ae935864ceffb45ecfed6a5234cc",
    "semantic_title": "discontinuous named entity recognition as maximal clique discovery",
    "citation_count": 32,
    "authors": [
      "Yucheng Wang",
      "Bowen Yu",
      "Hongsong Zhu",
      "Tingwen Liu",
      "Nan Yu",
      "Limin Sun"
    ]
  },
  "https://aclanthology.org/2021.acl-long.64": {
    "title": "LNN-EL: A Neuro-Symbolic Approach to Short-text Entity Linking",
    "volume": "long",
    "abstract": "Entity linking (EL) is the task of disambiguating mentions appearing in text by linking them to entities in a knowledge graph, a crucial task for text understanding, question answering or conversational systems. In the special case of short-text EL, which poses additional challenges due to limited context, prior approaches have reached good performance by employing heuristics-based methods or purely neural approaches. Here, we take a different, neuro-symbolic approach that combines the advantages of using interpretable rules based on first-order logic with the performance of neural learning. Even though constrained to use rules, we show that we reach competitive or better performance with SoTA black-box neural approaches. Furthermore, our framework has the benefits of extensibility and transferability. We show that we can easily blend existing rule templates given by a human expert, with multiple types of features (priors, BERT encodings, box embeddings, etc), and even with scores resulting from previous EL methods, thus improving on such methods. As an example of improvement, on the LC-QuAD-1.0 dataset, we show more than 3% increase in F1 score relative to previous SoTA. Finally, we show that the inductive bias offered by using logic results in a set of learned rules that transfers from one dataset to another, sometimes without finetuning, while still having high accuracy",
    "checked": true,
    "id": "6bbfb871a4157b35007b6e15c50dbd2ee7fe1cc3",
    "semantic_title": "lnn-el: a neuro-symbolic approach to short-text entity linking",
    "citation_count": 13,
    "authors": [
      "Hang Jiang",
      "Sairam Gurajada",
      "Qiuhao Lu",
      "Sumit Neelam",
      "Lucian Popa",
      "Prithviraj Sen",
      "Yunyao Li",
      "Alexander Gray"
    ]
  },
  "https://aclanthology.org/2021.acl-long.65": {
    "title": "Do Context-Aware Translation Models Pay the Right Attention?",
    "volume": "long",
    "abstract": "Context-aware machine translation models are designed to leverage contextual information, but often fail to do so. As a result, they inaccurately disambiguate pronouns and polysemous words that require context for resolution. In this paper, we ask several questions: What contexts do human translators use to resolve ambiguous words? Are models paying large amounts of attention to the same context? What if we explicitly train them to do so? To answer these questions, we introduce SCAT (Supporting Context for Ambiguous Translations), a new English-French dataset comprising supporting context words for 14K translations that professional translators found useful for pronoun disambiguation. Using SCAT, we perform an in-depth analysis of the context used to disambiguate, examining positional and lexical characteristics of the supporting words. Furthermore, we measure the degree of alignment between the model's attention scores and the supporting context from SCAT, and apply a guided attention strategy to encourage agreement between the two",
    "checked": true,
    "id": "72d89aa7cd77c3f22a667f2b0707758eb8d52a7a",
    "semantic_title": "do context-aware translation models pay the right attention?",
    "citation_count": 19,
    "authors": [
      "Kayo Yin",
      "Patrick Fernandes",
      "Danish Pruthi",
      "Aditi Chaudhary",
      "André F. T. Martins",
      "Graham Neubig"
    ]
  },
  "https://aclanthology.org/2021.acl-long.66": {
    "title": "Adapting High-resource NMT Models to Translate Low-resource Related Languages without Parallel Data",
    "volume": "long",
    "abstract": "The scarcity of parallel data is a major obstacle for training high-quality machine translation systems for low-resource languages. Fortunately, some low-resource languages are linguistically related or similar to high-resource languages; these related languages may share many lexical or syntactic structures. In this work, we exploit this linguistic overlap to facilitate translating to and from a low-resource language with only monolingual data, in addition to any parallel data in the related high-resource language. Our method, NMT-Adapt, combines denoising autoencoding, back-translation and adversarial objectives to utilize monolingual data for low-resource adaptation. We experiment on 7 languages from three different language families and show that our technique significantly improves translation into low-resource language compared to other translation baselines",
    "checked": true,
    "id": "817e8a03162adbb268f08b206a5f04c5bfa97b26",
    "semantic_title": "adapting high-resource nmt models to translate low-resource related languages without parallel data",
    "citation_count": 20,
    "authors": [
      "Wei-Jen Ko",
      "Ahmed El-Kishky",
      "Adithya Renduchintala",
      "Vishrav Chaudhary",
      "Naman Goyal",
      "Francisco Guzmán",
      "Pascale Fung",
      "Philipp Koehn",
      "Mona Diab"
    ]
  },
  "https://aclanthology.org/2021.acl-long.67": {
    "title": "Bilingual Lexicon Induction via Unsupervised Bitext Construction and Word Alignment",
    "volume": "long",
    "abstract": "Bilingual lexicons map words in one language to their translations in another, and are typically induced by learning linear projections to align monolingual word embedding spaces. In this paper, we show it is possible to produce much higher quality lexicons with methods that combine (1) unsupervised bitext mining and (2) unsupervised word alignment. Directly applying a pipeline that uses recent algorithms for both subproblems significantly improves induced lexicon quality and further gains are possible by learning to filter the resulting lexical entries, with both unsupervised and semi-supervised schemes. Our final model outperforms the state of the art on the BUCC 2020 shared task by 14 F1 points averaged over 12 language pairs, while also providing a more interpretable approach that allows for rich reasoning of word meaning in context. Further analysis of our output and the standard reference lexicons suggests they are of comparable quality, and new benchmarks may be needed to measure further progress on this task",
    "checked": true,
    "id": "bdac6a763f9d5f438595133716c08d18e0071c93",
    "semantic_title": "bilingual lexicon induction via unsupervised bitext construction and word alignment",
    "citation_count": 24,
    "authors": [
      "Haoyue Shi",
      "Luke Zettlemoyer",
      "Sida I. Wang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.68": {
    "title": "Multilingual Speech Translation from Efficient Finetuning of Pretrained Models",
    "volume": "long",
    "abstract": "We present a simple yet effective approach to build multilingual speech-to-text (ST) translation through efficient transfer learning from a pretrained speech encoder and text decoder. Our key finding is that a minimalistic LNA (LayerNorm and Attention) finetuning can achieve zero-shot crosslingual and cross-modality transfer ability by only finetuning 10 50% of the pretrained parameters. This effectively leverages large pretrained models at low training cost such as wav2vec 2.0 for acoustic modeling, and mBART for multilingual text generation. This sets a new state-of-the-art for 36 translation directions (and surpassing cascaded ST for 26 of them) on the large-scale multilingual ST benchmark CoVoST 2 (+6.4 BLEU on average for En-X directions and +6.7 BLEU for X-En directions). Our approach demonstrates strong zero-shot performance in a many-to-many multilingual model (+5.6 BLEU on average across 28 non-English directions), making it an appealing approach for attaining high-quality speech translation with improved parameter and data efficiency",
    "checked": true,
    "id": "4d031f78553e933e9f05f72ae6dc1872e867d299",
    "semantic_title": "multilingual speech translation from efficient finetuning of pretrained models",
    "citation_count": 107,
    "authors": [
      "Xian Li",
      "Changhan Wang",
      "Yun Tang",
      "Chau Tran",
      "Yuqing Tang",
      "Juan Pino",
      "Alexei Baevski",
      "Alexis Conneau",
      "Michael Auli"
    ]
  },
  "https://aclanthology.org/2021.acl-long.69": {
    "title": "Learning Faithful Representations of Causal Graphs",
    "volume": "long",
    "abstract": "Learning contextual text embeddings that represent causal graphs has been useful in improving the performance of downstream tasks like causal treatment effect estimation. However, existing causal embeddings which are trained to predict direct causal links, fail to capture other indirect causal links of the graph, thus leading to spurious correlations in downstream tasks. In this paper, we define the faithfulness property of contextual embeddings to capture geometric distance-based properties of directed acyclic causal graphs. By incorporating these faithfulness properties, we learn text embeddings that are 31.3% more faithful to human validated causal graphs with about 800K and 200K causal links and achieve 21.1% better Precision-Recall AUC in a link prediction fine-tuning task. Further, in a crowdsourced causal question-answering task on Yahoo! Answers with questions of the form \"What causes X?\", our faithful embeddings achieved a precision of the first ranked answer (P@1) of 41.07%, outperforming the existing baseline by 10.2%",
    "checked": true,
    "id": "e26bfb383f2ff67e5f2be5b90657d33b6d562f2d",
    "semantic_title": "learning faithful representations of causal graphs",
    "citation_count": 5,
    "authors": [
      "Ananth Balashankar",
      "Lakshminarayanan Subramanian"
    ]
  },
  "https://aclanthology.org/2021.acl-long.70": {
    "title": "What Context Features Can Transformer Language Models Use?",
    "volume": "long",
    "abstract": "Transformer-based language models benefit from conditioning on contexts of hundreds to thousands of previous tokens. What aspects of these contexts contribute to accurate model prediction? We describe a series of experiments that measure usable information by selectively ablating lexical and structural information in transformer language models trained on English Wikipedia. In both mid- and long-range contexts, we find that several extremely destructive context manipulations—including shuffling word order within sentences and deleting all words other than nouns—remove less than 15% of the usable information. Our results suggest that long contexts, but not their detailed syntactic and propositional content, are important for the low perplexity of current transformer language models",
    "checked": true,
    "id": "e1bc348fd7da000da6585e82994ecfedcecb5a4c",
    "semantic_title": "what context features can transformer language models use?",
    "citation_count": 46,
    "authors": [
      "Joe O’Connor",
      "Jacob Andreas"
    ]
  },
  "https://aclanthology.org/2021.acl-long.71": {
    "title": "Integrated Directional Gradients: Feature Interaction Attribution for Neural NLP Models",
    "volume": "long",
    "abstract": "In this paper, we introduce Integrated Directional Gradients (IDG), a method for attributing importance scores to groups of features, indicating their relevance to the output of a neural network model for a given input. The success of Deep Neural Networks has been attributed to their ability to capture higher level feature interactions. Hence, in the last few years capturing the importance of these feature interactions has received increased prominence in ML interpretability literature. In this paper, we formally define the feature group attribution problem and outline a set of axioms that any intuitive feature group attribution method should satisfy. Earlier, cooperative game theory inspired axiomatic methods only borrowed axioms from solution concepts (such as Shapley value) for individual feature attributions and introduced their own extensions to model interactions. In contrast, our formulation is inspired by axioms satisfied by characteristic functions as well as solution concepts in cooperative game theory literature. We believe that characteristic functions are much better suited to model importance of groups compared to just solution concepts. We demonstrate that our proposed method, IDG, satisfies all the axioms. Using IDG we analyze two state-of-the-art text classifiers on three benchmark datasets for sentiment analysis. Our experiments show that IDG is able to effectively capture semantic interactions in linguistic models via negations and conjunctions",
    "checked": true,
    "id": "bcf35117a0cb08fba6b4d00f9aa4f499946b57d9",
    "semantic_title": "integrated directional gradients: feature interaction attribution for neural nlp models",
    "citation_count": 16,
    "authors": [
      "Sandipan Sikdar",
      "Parantapa Bhattacharya",
      "Kieran Heese"
    ]
  },
  "https://aclanthology.org/2021.acl-long.72": {
    "title": "DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations",
    "volume": "long",
    "abstract": "Sentence embeddings are an important component of many natural language processing (NLP) systems. Like word embeddings, sentence embeddings are typically learned on large text corpora and then transferred to various downstream tasks, such as clustering and retrieval. Unlike word embeddings, the highest performing solutions for learning sentence embeddings require labelled data, limiting their usefulness to languages and domains where labelled data is abundant. In this paper, we present DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations. Inspired by recent advances in deep metric learning (DML), we carefully design a self-supervised objective for learning universal sentence embeddings that does not require labelled training data. When used to extend the pretraining of transformer-based language models, our approach closes the performance gap between unsupervised and supervised pretraining for universal sentence encoders. Importantly, our experiments suggest that the quality of the learned embeddings scale with both the number of trainable parameters and the amount of unlabelled training data. Our code and pretrained models are publicly available and can be easily adapted to new domains or used to embed unseen text",
    "checked": true,
    "id": "32d281a1e7a0a2d4e2b3f34e0f71780c987e1374",
    "semantic_title": "declutr: deep contrastive learning for unsupervised textual representations",
    "citation_count": 344,
    "authors": [
      "John Giorgi",
      "Osvald Nitski",
      "Bo Wang",
      "Gary Bader"
    ]
  },
  "https://aclanthology.org/2021.acl-long.73": {
    "title": "XLPT-AMR: Cross-Lingual Pre-Training via Multi-Task Learning for Zero-Shot AMR Parsing and Text Generation",
    "volume": "long",
    "abstract": "Due to the scarcity of annotated data, Abstract Meaning Representation (AMR) research is relatively limited and challenging for languages other than English. Upon the availability of English AMR dataset and English-to- X parallel datasets, in this paper we propose a novel cross-lingual pre-training approach via multi-task learning (MTL) for both zeroshot AMR parsing and AMR-to-text generation. Specifically, we consider three types of relevant tasks, including AMR parsing, AMR-to-text generation, and machine translation. We hope that knowledge gained while learning for English AMR parsing and text generation can be transferred to the counterparts of other languages. With properly pretrained models, we explore four different finetuning methods, i.e., vanilla fine-tuning with a single task, one-for-all MTL fine-tuning, targeted MTL fine-tuning, and teacher-studentbased MTL fine-tuning. Experimental results on AMR parsing and text generation of multiple non-English languages demonstrate that our approach significantly outperforms a strong baseline of pre-training approach, and greatly advances the state of the art. In detail, on LDC2020T07 we have achieved 70.45%, 71.76%, and 70.80% in Smatch F1 for AMR parsing of German, Spanish, and Italian, respectively, while for AMR-to-text generation of the languages, we have obtained 25.69, 31.36, and 28.42 in BLEU respectively. We make our code available on github https://github.com/xdqkid/XLPT-AMR",
    "checked": true,
    "id": "dcb1c1f51cea321e3b250ccec488df4445bc7564",
    "semantic_title": "xlpt-amr: cross-lingual pre-training via multi-task learning for zero-shot amr parsing and text generation",
    "citation_count": 9,
    "authors": [
      "Dongqin Xu",
      "Junhui Li",
      "Muhua Zhu",
      "Min Zhang",
      "Guodong Zhou"
    ]
  },
  "https://aclanthology.org/2021.acl-long.74": {
    "title": "Span-based Semantic Parsing for Compositional Generalization",
    "volume": "long",
    "abstract": "Despite the success of sequence-to-sequence (seq2seq) models in semantic parsing, recent work has shown that they fail in compositional generalization, i.e., the ability to generalize to new structures built of components observed during training. In this work, we posit that a span-based parser should lead to better compositional generalization. we propose SpanBasedSP, a parser that predicts a span tree over an input utterance, explicitly encoding how partial programs compose over spans in the input. SpanBasedSP extends Pasupat et al. (2019) to be comparable to seq2seq models by (i) training from programs, without access to gold trees, treating trees as latent variables, (ii) parsing a class of non-projective trees through an extension to standard CKY. On GeoQuery, SCAN and CLOSURE datasets, SpanBasedSP performs similarly to strong seq2seq baselines on random splits, but dramatically improves performance compared to baselines on splits that require compositional generalization: from 61.0 → 88.9 average accuracy",
    "checked": true,
    "id": "307ec233777755b3d89b2096f4b54c83d9cd80ba",
    "semantic_title": "span-based semantic parsing for compositional generalization",
    "citation_count": 84,
    "authors": [
      "Jonathan Herzig",
      "Jonathan Berant"
    ]
  },
  "https://aclanthology.org/2021.acl-long.75": {
    "title": "Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?",
    "volume": "long",
    "abstract": "Sequence-to-sequence models excel at handling natural language variation, but have been shown to struggle with out-of-distribution compositional generalization. This has motivated new specialized architectures with stronger compositional biases, but most of these approaches have only been evaluated on synthetically-generated datasets, which are not representative of natural language variation. In this work we ask: can we develop a semantic parsing approach that handles both natural language variation and compositional generalization? To better assess this capability, we propose new train and test splits of non-synthetic datasets. We demonstrate that strong existing approaches do not perform well across a broad set of evaluations. We also propose NQG-T5, a hybrid model that combines a high-precision grammar-based approach with a pre-trained sequence-to-sequence model. It outperforms existing approaches across several compositional generalization challenges on non-synthetic data, while also being competitive with the state-of-the-art on standard evaluations. While still far from solving this problem, our study highlights the importance of diverse evaluations and the open challenge of handling both compositional generalization and natural language variation in semantic parsing",
    "checked": true,
    "id": "acf8a1040034820bf99379a3422815f4e0859ec9",
    "semantic_title": "compositional generalization and natural language variation: can a semantic parsing approach handle both?",
    "citation_count": 127,
    "authors": [
      "Peter Shaw",
      "Ming-Wei Chang",
      "Panupong Pasupat",
      "Kristina Toutanova"
    ]
  },
  "https://aclanthology.org/2021.acl-long.76": {
    "title": "A Targeted Assessment of Incremental Processing in Neural Language Models and Humans",
    "volume": "long",
    "abstract": "We present a targeted, scaled-up comparison of incremental processing in humans and neural language models by collecting by-word reaction time data for sixteen different syntactic test suites across a range of structural phenomena. Human reaction time data comes from a novel online experimental paradigm called the Interpolated Maze task. We compare human reaction times to by-word probabilities for four contemporary language models, with different architectures and trained on a range of data set sizes. We find that across many phenomena, both humans and language models show increased processing difficulty in ungrammatical sentence regions with human and model ‘accuracy' scores a la Marvin and Linzen (2018) about equal. However, although language model outputs match humans in direction, we show that models systematically under-predict the difference in magnitude of incremental processing difficulty between grammatical and ungrammatical sentences. Specifically, when models encounter syntactic violations they fail to accurately predict the longer reading times observed in the human data. These results call into question whether contemporary language models are approaching human-like performance for sensitivity to syntactic violations",
    "checked": true,
    "id": "13726d2c53df02b73a7e5b317952648f416d13b4",
    "semantic_title": "a targeted assessment of incremental processing in neural language models and humans",
    "citation_count": 15,
    "authors": [
      "Ethan Wilcox",
      "Pranali Vani",
      "Roger Levy"
    ]
  },
  "https://aclanthology.org/2021.acl-long.77": {
    "title": "The Possible, the Plausible, and the Desirable: Event-Based Modality Detection for Language Processing",
    "volume": "long",
    "abstract": "Modality is the linguistic ability to describe vents with added information such as how desirable, plausible, or feasible they are. Modality is important for many NLP downstream tasks such as the detection of hedging, uncertainty, speculation, and more. Previous studies that address modality detection in NLP often restrict modal expressions to a closed syntactic class, and the modal sense labels are vastly different across different studies, lacking an accepted standard. Furthermore, these senses are often analyzed independently of the events that they modify. This work builds on the theoretical foundations of the Georgetown Gradable Modal Expressions (GME) work by Rubinstein et al. (2013) to propose an event-based modality detection task where modal expressions can be words of any syntactic class and sense labels are drawn from a comprehensive taxonomy which harmonizes the modal concepts contributed by the different studies. We present experiments on the GME corpus aiming to detect and classify fine-grained modal concepts and associate them with their modified events. We show that detecting and classifying modal expressions is not only feasible, it also improves the detection of modal events in their own right",
    "checked": true,
    "id": "94a7921fe01fe3a9b1f1f94bfb20e539b5dc1a48",
    "semantic_title": "the possible, the plausible, and the desirable: event-based modality detection for language processing",
    "citation_count": 10,
    "authors": [
      "Valentina Pyatkin",
      "Shoval Sadde",
      "Aynat Rubinstein",
      "Paul Portner",
      "Reut Tsarfaty"
    ]
  },
  "https://aclanthology.org/2021.acl-long.78": {
    "title": "To POS Tag or Not to POS Tag: The Impact of POS Tags on Morphological Learning in Low-Resource Settings",
    "volume": "long",
    "abstract": "Part-of-Speech (POS) tags are routinely included as features in many NLP tasks. However, the importance and usefulness of POS tags needs to be examined as NLP expands to low-resource languages because linguists who provide many annotated resources do not place priority on early identification and tagging of POS. This paper describes an empirical study about the effect that POS tags have on two computational morphological tasks with the Transformer architecture. Each task is tested twice on identical data except for the presence/absence of POS tags, using published data in ten high- to low-resource languages or unpublished linguistic field data in five low-resource languages. We find that the presence or absence of POS tags does not have a significant bearing on performance. In joint segmentation and glossing, the largest average difference is an .09 improvement in F1-scores by removing POS tags. In reinflection, the greatest average difference is 1.2% in accuracy for published data and 5% for unpublished and noisy field data",
    "checked": true,
    "id": "44a63a138eb007a0ed94c00c7cbae9803f5c0a68",
    "semantic_title": "to pos tag or not to pos tag: the impact of pos tags on morphological learning in low-resource settings",
    "citation_count": 4,
    "authors": [
      "Sarah Moeller",
      "Ling Liu",
      "Mans Hulden"
    ]
  },
  "https://aclanthology.org/2021.acl-long.79": {
    "title": "Prosodic segmentation for parsing spoken dialogue",
    "volume": "long",
    "abstract": "Parsing spoken dialogue poses unique difficulties, including disfluencies and unmarked boundaries between sentence-like units. Previous work has shown that prosody can help with parsing disfluent speech (Tran et al. 2018), but has assumed that the input to the parser is already segmented into sentence-like units (SUs), which isn't true in existing speech applications. We investigate how prosody affects a parser that receives an entire dialogue turn as input (a turn-based model), instead of gold standard pre-segmented SUs (an SU-based model). In experiments on the English Switchboard corpus, we find that when using transcripts alone, the turn-based model has trouble segmenting SUs, leading to worse parse performance than the SU-based model. However, prosody can effectively replace gold standard SU boundaries: with prosody, the turn-based model performs as well as the SU-based model (91.38 vs. 91.06 F1 score, respectively), despite performing two tasks (SU segmentation and parsing) rather than one (parsing alone). Analysis shows that pitch and intensity features are the most important for this corpus, since they allow the model to correctly distinguish an SU boundary from a speech disfluency – a distinction that the model otherwise struggles to make",
    "checked": true,
    "id": "eb0bd07d316f2e1f2c19f1e416c8937a012e28ba",
    "semantic_title": "prosodic segmentation for parsing spoken dialogue",
    "citation_count": 1,
    "authors": [
      "Elizabeth Nielsen",
      "Mark Steedman",
      "Sharon Goldwater"
    ]
  },
  "https://aclanthology.org/2021.acl-long.80": {
    "title": "VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation",
    "volume": "long",
    "abstract": "We introduce VoxPopuli, a large-scale multilingual corpus providing 400K hours of unlabeled speech data in 23 languages. It is the largest open data to date for unsupervised representation learning as well as semi-supervised learning. VoxPopuli also contains 1.8K hours of transcribed speeches in 15 languages and their aligned oral interpretations into 15 target languages totaling 17.3K hours. We provide speech recognition (ASR) baselines and validate the versatility of VoxPopuli unlabeled data in semi-supervised ASR and speech-to-text translation under challenging out-of-domain settings. The corpus is available at https://github.com/facebookresearch/voxpopuli",
    "checked": true,
    "id": "6da74c5b1b3cffc236c4b2d75ac46b767f327e62",
    "semantic_title": "voxpopuli: a large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation",
    "citation_count": 249,
    "authors": [
      "Changhan Wang",
      "Morgane Riviere",
      "Ann Lee",
      "Anne Wu",
      "Chaitanya Talnikar",
      "Daniel Haziza",
      "Mary Williamson",
      "Juan Pino",
      "Emmanuel Dupoux"
    ]
  },
  "https://aclanthology.org/2021.acl-long.81": {
    "title": "Stereotyping Norwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets",
    "volume": "long",
    "abstract": "Auditing NLP systems for computational harms like surfacing stereotypes is an elusive goal. Several recent efforts have focused on benchmark datasets consisting of pairs of contrastive sentences, which are often accompanied by metrics that aggregate an NLP system's behavior on these pairs into measurements of harms. We examine four such benchmarks constructed for two NLP tasks: language modeling and coreference resolution. We apply a measurement modeling lens—originating from the social sciences—to inventory a range of pitfalls that threaten these benchmarks' validity as measurement models for stereotyping. We find that these benchmarks frequently lack clear articulations of what is being measured, and we highlight a range of ambiguities and unstated assumptions that affect how these benchmarks conceptualize and operationalize stereotyping",
    "checked": true,
    "id": "3e65f572322e192fe36ae52a8a7f025b0685dfc6",
    "semantic_title": "stereotyping norwegian salmon: an inventory of pitfalls in fairness benchmark datasets",
    "citation_count": 147,
    "authors": [
      "Su Lin Blodgett",
      "Gilsinia Lopez",
      "Alexandra Olteanu",
      "Robert Sim",
      "Hanna Wallach"
    ]
  },
  "https://aclanthology.org/2021.acl-long.82": {
    "title": "Robust Knowledge Graph Completion with Stacked Convolutions and a Student Re-Ranking Network",
    "volume": "long",
    "abstract": "Knowledge Graph (KG) completion research usually focuses on densely connected benchmark datasets that are not representative of real KGs. We curate two KG datasets that include biomedical and encyclopedic knowledge and use an existing commonsense KG dataset to explore KG completion in the more realistic setting where dense connectivity is not guaranteed. We develop a deep convolutional network that utilizes textual entity representations and demonstrate that our model outperforms recent KG completion methods in this challenging setting. We find that our model's performance improvements stem primarily from its robustness to sparsity. We then distill the knowledge from the convolutional network into a student network that re-ranks promising candidate entities. This re-ranking stage leads to further improvements in performance and demonstrates the effectiveness of entity re-ranking for KG completion",
    "checked": true,
    "id": "19db2eb6b4906f669f397a7bbb75a974bfcd934e",
    "semantic_title": "robust knowledge graph completion with stacked convolutions and a student re-ranking network",
    "citation_count": 16,
    "authors": [
      "Justin Lovelace",
      "Denis Newman-Griffis",
      "Shikhar Vashishth",
      "Jill Fain Lehman",
      "Carolyn Rosé"
    ]
  },
  "https://aclanthology.org/2021.acl-long.83": {
    "title": "A DQN-based Approach to Finding Precise Evidences for Fact Verification",
    "volume": "long",
    "abstract": "Computing precise evidences, namely minimal sets of sentences that support or refute a given claim, rather than larger evidences is crucial in fact verification (FV), since larger evidences may contain conflicting pieces some of which support the claim while the other refute, thereby misleading FV. Despite being important, precise evidences are rarely studied by existing methods for FV. It is challenging to find precise evidences due to a large search space with lots of local optimums. Inspired by the strong exploration ability of the deep Q-learning network (DQN), we propose a DQN-based approach to retrieval of precise evidences. In addition, to tackle the label bias on Q-values computed by DQN, we design a post-processing strategy which seeks best thresholds for determining the true labels of computed evidences. Experimental results confirm the effectiveness of DQN in computing precise evidences and demonstrate improvements in achieving accurate claim verification",
    "checked": true,
    "id": "289fed30b09af6c5ec1d35987103b79a356ab551",
    "semantic_title": "a dqn-based approach to finding precise evidences for fact verification",
    "citation_count": 11,
    "authors": [
      "Hai Wan",
      "Haicheng Chen",
      "Jianfeng Du",
      "Weilin Luo",
      "Rongzhen Ye"
    ]
  },
  "https://aclanthology.org/2021.acl-long.84": {
    "title": "The Art of Abstention: Selective Prediction and Error Regularization for Natural Language Processing",
    "volume": "long",
    "abstract": "In selective prediction, a classifier is allowed to abstain from making predictions on low-confidence examples. Though this setting is interesting and important, selective prediction has rarely been examined in natural language processing (NLP) tasks. To fill this void in the literature, we study in this paper selective prediction for NLP, comparing different models and confidence estimators. We further propose a simple error regularization trick that improves confidence estimation without substantially increasing the computation budget. We show that recent pre-trained transformer models simultaneously improve both model accuracy and confidence estimation effectiveness. We also find that our proposed regularization improves confidence estimation and can be applied to other relevant scenarios, such as using classifier cascades for accuracy–efficiency trade-offs. Source code for this paper can be found at https://github.com/castorini/transformers-selective",
    "checked": true,
    "id": "122ed3c2e45badc1292422d7e9a5f3a43c402128",
    "semantic_title": "",
    "citation_count": 25,
    "authors": [
      "Ji Xin",
      "Raphael Tang",
      "Yaoliang Yu",
      "Jimmy Lin"
    ]
  },
  "https://aclanthology.org/2021.acl-long.85": {
    "title": "Unsupervised Out-of-Domain Detection via Pre-trained Transformers",
    "volume": "long",
    "abstract": "Deployed real-world machine learning applications are often subject to uncontrolled and even potentially malicious inputs. Those out-of-domain inputs can lead to unpredictable outputs and sometimes catastrophic safety issues. Prior studies on out-of-domain detection require in-domain task labels and are limited to supervised classification scenarios. Our work tackles the problem of detecting out-of-domain samples with only unsupervised in-domain data. We utilize the latent representations of pre-trained transformers and propose a simple yet effective method to transform features across all layers to construct out-of-domain detectors efficiently. Two domain-specific fine-tuning approaches are further proposed to boost detection accuracy. Our empirical evaluations of related methods on two datasets validate that our method greatly improves out-of-domain detection ability in a more general scenario",
    "checked": true,
    "id": "35f4c130025dc76b2d8532077508842ca2f0694d",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Keyang Xu",
      "Tongzheng Ren",
      "Shikun Zhang",
      "Yihao Feng",
      "Caiming Xiong"
    ]
  },
  "https://aclanthology.org/2021.acl-long.86": {
    "title": "MATE-KD: Masked Adversarial TExt, a Companion to Knowledge Distillation",
    "volume": "long",
    "abstract": "The advent of large pre-trained language models has given rise to rapid progress in the field of Natural Language Processing (NLP). While the performance of these models on standard benchmarks has scaled with size, compression techniques such as knowledge distillation have been key in making them practical. We present MATE-KD, a novel text-based adversarial training algorithm which improves the performance of knowledge distillation. MATE-KD first trains a masked language model-based generator to perturb text by maximizing the divergence between teacher and student logits. Then using knowledge distillation a student is trained on both the original and the perturbed training samples. We evaluate our algorithm, using BERT-based models, on the GLUE benchmark and demonstrate that MATE-KD outperforms competitive adversarial learning and data augmentation baselines. On the GLUE test set our 6 layer RoBERTa based model outperforms BERT-large",
    "checked": true,
    "id": "0789d0f6f3459b7689486ebc80ce39373dd8cc17",
    "semantic_title": "",
    "citation_count": 26,
    "authors": [
      "Ahmad Rashid",
      "Vasileios Lioutas",
      "Mehdi Rezagholizadeh"
    ]
  },
  "https://aclanthology.org/2021.acl-long.87": {
    "title": "Selecting Informative Contexts Improves Language Model Fine-tuning",
    "volume": "long",
    "abstract": "Language model fine-tuning is essential for modern natural language processing, but is computationally expensive and time-consuming. Further, the effectiveness of fine-tuning is limited by the inclusion of training examples that negatively affect performance. Here we present a general fine-tuning method that we call information gain filtration for improving the overall training efficiency and final performance of language model fine-tuning. We define the information gain of an example as the improvement on a validation metric after training on that example. A secondary learner is then trained to approximate this quantity. During fine-tuning, this learner selects informative examples and skips uninformative ones. We show that our method has consistent improvement across datasets, fine-tuning tasks, and language model architectures. For example, we achieve a median perplexity of 54.0 on a books dataset compared to 57.3 for standard fine-tuning. We present statistical evidence that offers insight into the improvements of our method over standard fine-tuning. The generality of our method leads us to propose a new paradigm for language model fine-tuning — we encourage researchers to release pretrained secondary learners on common corpora to promote efficient and effective fine-tuning, thereby improving the performance and reducing the overall energy footprint of language model fine-tuning",
    "checked": true,
    "id": "6dcc865de3f030f07864d244689314871b7f2c4f",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Richard Antonello",
      "Nicole Beckage",
      "Javier Turek",
      "Alexander Huth"
    ]
  },
  "https://aclanthology.org/2021.acl-long.88": {
    "title": "Explainable Prediction of Text Complexity: The Missing Preliminaries for Text Simplification",
    "volume": "long",
    "abstract": "Text simplification reduces the language complexity of professional content for accessibility purposes. End-to-end neural network models have been widely adopted to directly generate the simplified version of input text, usually functioning as a blackbox. We show that text simplification can be decomposed into a compact pipeline of tasks to ensure the transparency and explainability of the process. The first two steps in this pipeline are often neglected: 1) to predict whether a given piece of text needs to be simplified, and 2) if yes, to identify complex parts of the text. The two tasks can be solved separately using either lexical or deep learning methods, or solved jointly. Simply applying explainable complexity prediction as a preliminary step, the out-of-sample text simplification performance of the state-of-the-art, black-box simplification models can be improved by a large margin",
    "checked": true,
    "id": "656b1bda6c514a1da6ab3e0d1bd9c61dde236a82",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Cristina Garbacea",
      "Mengtian Guo",
      "Samuel Carton",
      "Qiaozhu Mei"
    ]
  },
  "https://aclanthology.org/2021.acl-long.89": {
    "title": "Multi-Task Retrieval for Knowledge-Intensive Tasks",
    "volume": "long",
    "abstract": "Retrieving relevant contexts from a large corpus is a crucial step for tasks such as open-domain question answering and fact checking. Although neural retrieval outperforms traditional methods like tf-idf and BM25, its performance degrades considerably when applied to out-of-domain data. Driven by the question of whether a neural retrieval model can be _universal_ and perform robustly on a wide variety of problems, we propose a multi-task trained model. Our approach not only outperforms previous methods in the few-shot setting, but also rivals specialised neural retrievers, even when in-domain training data is abundant. With the help of our retriever, we improve existing models for downstream tasks and closely match or improve the state of the art on multiple benchmarks",
    "checked": true,
    "id": "89181e26874c4ea418937d7a6980d1476d4c0b0b",
    "semantic_title": "",
    "citation_count": 44,
    "authors": [
      "Jean Maillard",
      "Vladimir Karpukhin",
      "Fabio Petroni",
      "Wen-tau Yih",
      "Barlas Oguz",
      "Veselin Stoyanov",
      "Gargi Ghosh"
    ]
  },
  "https://aclanthology.org/2021.acl-long.90": {
    "title": "When Do You Need Billions of Words of Pretraining Data?",
    "volume": "long",
    "abstract": "NLP is currently dominated by language models like RoBERTa which are pretrained on billions of words. But what exact knowledge or skills do Transformer LMs learn from large-scale pretraining that they cannot learn from less data? To explore this question, we adopt five styles of evaluation: classifier probing, information-theoretic probing, unsupervised relative acceptability judgments, unsupervised language model knowledge probing, and fine-tuning on NLU tasks. We then draw learning curves that track the growth of these different measures of model ability with respect to pretraining data volume using the MiniBERTas, a group of RoBERTa models pretrained on 1M, 10M, 100M and 1B words. We find that these LMs require only about 10M to 100M words to learn to reliably encode most syntactic and semantic features we test. They need a much larger quantity of data in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks. The results suggest that, while the ability to encode linguistic features is almost certainly necessary for language understanding, it is likely that other, unidentified, forms of knowledge are the major drivers of recent improvements in language understanding among large pretrained models",
    "checked": true,
    "id": "31392ad8722d9c66181b621936e2013199e02edc",
    "semantic_title": "",
    "citation_count": 63,
    "authors": [
      "Yian Zhang",
      "Alex Warstadt",
      "Xiaocheng Li",
      "Samuel R. Bowman"
    ]
  },
  "https://aclanthology.org/2021.acl-long.91": {
    "title": "Analyzing the Source and Target Contributions to Predictions in Neural Machine Translation",
    "volume": "long",
    "abstract": "In Neural Machine Translation (and, more generally, conditional language modeling), the generation of a target token is influenced by two types of context: the source and the prefix of the target sequence. While many attempts to understand the internal workings of NMT models have been made, none of them explicitly evaluates relative source and target contributions to a generation decision. We argue that this relative contribution can be evaluated by adopting a variant of Layerwise Relevance Propagation (LRP). Its underlying ‘conservation principle’ makes relevance propagation unique: differently from other methods, it evaluates not an abstract quantity reflecting token importance, but the proportion of each token’s influence. We extend LRP to the Transformer and conduct an analysis of NMT models which explicitly evaluates the source and target relative contributions to the generation process. We analyze changes in these contributions when conditioning on different types of prefixes, when varying the training objective or the amount of training data, and during the training process. We find that models trained with more data tend to rely on source information more and to have more sharp token contributions; the training process is non-monotonic with several stages of different nature",
    "checked": true,
    "id": "cf592385909a1e3e9a428d8d6d8f427ab70b60a9",
    "semantic_title": "",
    "citation_count": 47,
    "authors": [
      "Elena Voita",
      "Rico Sennrich",
      "Ivan Titov"
    ]
  },
  "https://aclanthology.org/2021.acl-long.92": {
    "title": "Comparing Test Sets with Item Response Theory",
    "volume": "long",
    "abstract": "Recent years have seen numerous NLP datasets introduced to evaluate the performance of fine-tuned models on natural language understanding tasks. Recent results from large pretrained models, though, show that many of these datasets are largely saturated and unlikely to be able to detect further progress. What kind of datasets are still effective at discriminating among strong models, and what kind of datasets should we expect to be able to detect future improvements? To measure this uniformly across datasets, we draw on Item Response Theory and evaluate 29 datasets using predictions from 18 pretrained Transformer models on individual test examples. We find that Quoref, HellaSwag, and MC-TACO are best suited for distinguishing among state-of-the-art models, while SNLI, MNLI, and CommitmentBank seem to be saturated for current strong models. We also observe span selection task format, which is used for QA datasets like QAMR or SQuAD2.0, is effective in differentiating between strong and weak models",
    "checked": true,
    "id": "4ca39cf99747b8962fe37e7e025e284872df3425",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Clara Vania",
      "Phu Mon Htut",
      "William Huang",
      "Dhara Mungra",
      "Richard Yuanzhe Pang",
      "Jason Phang",
      "Haokun Liu",
      "Kyunghyun Cho",
      "Samuel R. Bowman"
    ]
  },
  "https://aclanthology.org/2021.acl-long.93": {
    "title": "Uncovering Constraint-Based Behavior in Neural Models via Targeted Fine-Tuning",
    "volume": "long",
    "abstract": "A growing body of literature has focused on detailing the linguistic knowledge embedded in large, pretrained language models. Existing work has shown that non-linguistic biases in models can drive model behavior away from linguistic generalizations. We hypothesized that competing linguistic processes within a language, rather than just non-linguistic model biases, could obscure underlying linguistic knowledge. We tested this claim by exploring a single phenomenon in four languages: English, Chinese, Spanish, and Italian. While human behavior has been found to be similar across languages, we find cross-linguistic variation in model behavior. We show that competing processes in a language act as constraints on model behavior and demonstrate that targeted fine-tuning can re-weight the learned constraints, uncovering otherwise dormant linguistic knowledge in models. Our results suggest that models need to learn both the linguistic constraints in a language and their relative ranking, with mismatches in either producing non-human-like behavior",
    "checked": true,
    "id": "c8ff35e58c7d8db1024cd898cdf422452a11aefd",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Forrest Davis",
      "Marten van Schijndel"
    ]
  },
  "https://aclanthology.org/2021.acl-long.94": {
    "title": "More Identifiable yet Equally Performant Transformers for Text Classification",
    "volume": "long",
    "abstract": "Interpretability is an important aspect of the trustworthiness of a model’s predictions. Transformer’s predictions are widely explained by the attention weights, i.e., a probability distribution generated at its self-attention unit (head). Current empirical studies provide shreds of evidence that attention weights are not explanations by proving that they are not unique. A recent study showed theoretical justifications to this observation by proving the non-identifiability of attention weights. For a given input to a head and its output, if the attention weights generated in it are unique, we call the weights identifiable. In this work, we provide deeper theoretical analysis and empirical observations on the identifiability of attention weights. Ignored in the previous works, we find the attention weights are more identifiable than we currently perceive by uncovering the hidden role of the key vector. However, the weights are still prone to be non-unique attentions that make them unfit for interpretation. To tackle this issue, we provide a variant of the encoder layer that decouples the relationship between key and value vector and provides identifiable weights up to the desired length of the input. We prove the applicability of such variations by providing empirical justifications on varied text classification tasks. The implementations are available at https://github.com/declare-lab/identifiable-transformers",
    "checked": true,
    "id": "e67369b9aaeebee93c48d244e31aab436b9fb066",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Rishabh Bhardwaj",
      "Navonil Majumder",
      "Soujanya Poria",
      "Eduard Hovy"
    ]
  },
  "https://aclanthology.org/2021.acl-long.95": {
    "title": "AugNLG: Few-shot Natural Language Generation using Self-trained Data Augmentation",
    "volume": "long",
    "abstract": "Natural Language Generation (NLG) is a key component in a task-oriented dialogue system, which converts the structured meaning representation (MR) to the natural language. For large-scale conversational systems, where it is common to have over hundreds of intents and thousands of slots, neither template-based approaches nor model-based approaches are scalable. Recently, neural NLGs started leveraging transfer learning and showed promising results in few-shot settings. This paper proposes AugNLG, a novel data augmentation approach that combines a self-trained neural retrieval model with a few-shot learned NLU model, to automatically create MR-to-Text data from open-domain texts. The proposed system mostly outperforms the state-of-the-art methods on the FewshotWOZ data in both BLEU and Slot Error Rate. We further confirm improved results on the FewshotSGD data and provide comprehensive analysis results on key components of our system. Our code and data are available at https://github.com/XinnuoXu/AugNLG",
    "checked": true,
    "id": "c32b32c85e96dd82e64fe941d8ff82db40e773d2",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Xinnuo Xu",
      "Guoyin Wang",
      "Young-Bum Kim",
      "Sungjin Lee"
    ]
  },
  "https://aclanthology.org/2021.acl-long.96": {
    "title": "Can vectors read minds better than experts? Comparing data augmentation strategies for the automated scoring of children's mindreading ability",
    "volume": "long",
    "abstract": "In this paper we implement and compare 7 different data augmentation strategies for the task of automatic scoring of children’s ability to understand others’ thoughts, feelings, and desires (or “mindreading”). We recruit in-domain experts to re-annotate augmented samples and determine to what extent each strategy preserves the original rating. We also carry out multiple experiments to measure how much each augmentation strategy improves the performance of automatic scoring systems. To determine the capabilities of automatic systems to generalize to unseen data, we create UK-MIND-20 - a new corpus of children’s performance on tests of mindreading, consisting of 10,320 question-answer pairs. We obtain a new state-of-the-art performance on the MIND-CA corpus, improving macro-F1-score by 6 points. Results indicate that both the number of training examples and the quality of the augmentation strategies affect the performance of the systems. The task-specific augmentations generally outperform task-agnostic augmentations. Automatic augmentations based on vectors (GloVe, FastText) perform the worst. We find that systems trained on MIND-CA generalize well to UK-MIND-20. We demonstrate that data augmentation strategies also improve the performance on unseen data",
    "checked": true,
    "id": "dc26a59057227d96823b24d75603acc830ce8031",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Venelin Kovatchev",
      "Phillip Smith",
      "Mark Lee",
      "Rory Devine"
    ]
  },
  "https://aclanthology.org/2021.acl-long.97": {
    "title": "A Dataset and Baselines for Multilingual Reply Suggestion",
    "volume": "long",
    "abstract": "Reply suggestion models help users process emails and chats faster. Previous work only studies English reply suggestion. Instead, we present MRS, a multilingual reply suggestion dataset with ten languages. MRS can be used to compare two families of models: 1) retrieval models that select the reply from a fixed set and 2) generation models that produce the reply from scratch. Therefore, MRS complements existing cross-lingual generalization benchmarks that focus on classification and sequence labeling tasks. We build a generation model and a retrieval model as baselines for MRS. The two models have different strengths in the monolingual setting, and they require different strategies to generalize across languages. MRS is publicly available at https://github.com/zhangmozhi/mrs",
    "checked": true,
    "id": "f96e1494d714f6bbf496bdd137300c7db5500753",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Mozhi Zhang",
      "Wei Wang",
      "Budhaditya Deb",
      "Guoqing Zheng",
      "Milad Shokouhi",
      "Ahmed Hassan Awadallah"
    ]
  },
  "https://aclanthology.org/2021.acl-long.98": {
    "title": "What Ingredients Make for an Effective Crowdsourcing Protocol for Difficult NLU Data Collection Tasks?",
    "volume": "long",
    "abstract": "Crowdsourcing is widely used to create data for common natural language understanding tasks. Despite the importance of these datasets for measuring and refining model understanding of language, there has been little focus on the crowdsourcing methods used for collecting the datasets. In this paper, we compare the efficacy of interventions that have been proposed in prior work as ways of improving data quality. We use multiple-choice question answering as a testbed and run a randomized trial by assigning crowdworkers to write questions under one of four different data collection protocols. We find that asking workers to write explanations for their examples is an ineffective stand-alone strategy for boosting NLU example difficulty. However, we find that training crowdworkers, and then using an iterative process of collecting data, sending feedback, and qualifying workers based on expert judgments is an effective means of collecting challenging data. But using crowdsourced, instead of expert judgments, to qualify workers and send feedback does not prove to be effective. We observe that the data from the iterative protocol with expert assessments is more challenging by several measures. Notably, the human–model gap on the unanimous agreement portion of this data is, on average, twice as large as the gap for the baseline protocol data",
    "checked": true,
    "id": "7d9a3b94f78827952b078c664b0da1c02e1c2ee3",
    "semantic_title": "",
    "citation_count": 26,
    "authors": [
      "Nikita Nangia",
      "Saku Sugawara",
      "Harsh Trivedi",
      "Alex Warstadt",
      "Clara Vania",
      "Samuel R. Bowman"
    ]
  },
  "https://aclanthology.org/2021.acl-long.99": {
    "title": "Align Voting Behavior with Public Statements for Legislator Representation Learning",
    "volume": "long",
    "abstract": "Ideology of legislators is typically estimated by ideal point models from historical records of votes. It represents legislators and legislation as points in a latent space and shows promising results for modeling voting behavior. However, it fails to capture more specific attitudes of legislators toward emerging issues and is unable to model newly-elected legislators without voting histories. In order to mitigate these two problems, we explore to incorporate both voting behavior and public statements on Twitter to jointly model legislators. In addition, we propose a novel task, namely hashtag usage prediction to model the ideology of legislators on Twitter. In practice, we construct a heterogeneous graph for the legislative context and use relational graph neural networks to learn the representation of legislators with the guidance of historical records of their voting and hashtag usage. Experiment results indicate that our model yields significant improvements for the task of roll call vote prediction. Further analysis further demonstrates that legislator representation we learned captures nuances in statements",
    "checked": true,
    "id": "d8daacb809e890742d3e4681cc3bbd5fdb4786a4",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Xinyi Mou",
      "Zhongyu Wei",
      "Lei Chen",
      "Shangyi Ning",
      "Yancheng He",
      "Changjian Jiang",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.100": {
    "title": "Measure and Evaluation of Semantic Divergence across Two Languages",
    "volume": "long",
    "abstract": "Languages are dynamic systems: word usage may change over time, reflecting various societal factors. However, all languages do not evolve identically: the impact of an event, the influence of a trend or thinking, can differ between communities. In this paper, we propose to track these divergences by comparing the evolution of a word and its translation across two languages. We investigate several methods of building time-varying and bilingual word embeddings, using contextualised and non-contextualised embeddings. We propose a set of scenarios to characterize semantic divergence across two languages, along with a setup to differentiate them in a bilingual corpus. We evaluate the different methods by generating a corpus of synthetic semantic change across two languages, English and French, before applying them to newspaper corpora to detect bilingual semantic divergence and provide qualitative insight for the task. We conclude that BERT embeddings coupled with a clustering step lead to the best performance on synthetic corpora; however, the performance of CBOW embeddings is very competitive and more adapted to an exploratory analysis on a large corpus",
    "checked": true,
    "id": "b0559ebf5f7989f80e8e2f61f6184557c5f7419a",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Syrielle Montariol",
      "Alexandre Allauzen"
    ]
  },
  "https://aclanthology.org/2021.acl-long.101": {
    "title": "Improving Zero-Shot Translation by Disentangling Positional Information",
    "volume": "long",
    "abstract": "Multilingual neural machine translation has shown the capability of directly translating between language pairs unseen in training, i.e. zero-shot translation. Despite being conceptually attractive, it often suffers from low output quality. The difficulty of generalizing to new translation directions suggests the model representations are highly specific to those language pairs seen in training. We demonstrate that a main factor causing the language-specific representations is the positional correspondence to input tokens. We show that this can be easily alleviated by removing residual connections in an encoder layer. With this modification, we gain up to 18.5 BLEU points on zero-shot translation while retaining quality on supervised directions. The improvements are particularly prominent between related languages, where our proposed model outperforms pivot-based translation. Moreover, our approach allows easy integration of new languages, which substantially expands translation coverage. By thorough inspections of the hidden layer outputs, we show that our approach indeed leads to more language-independent representations",
    "checked": true,
    "id": "50a77f5996a28662d26b85c0020be84edb07050f",
    "semantic_title": "",
    "citation_count": 26,
    "authors": [
      "Danni Liu",
      "Jan Niehues",
      "James Cross",
      "Francisco Guzmán",
      "Xian Li"
    ]
  },
  "https://aclanthology.org/2021.acl-long.102": {
    "title": "Common Sense Beyond English: Evaluating and Improving Multilingual Language Models for Commonsense Reasoning",
    "volume": "long",
    "abstract": "Commonsense reasoning research has so far been limited to English. We aim to evaluate and improve popular multilingual language models (ML-LMs) to help advance commonsense reasoning (CSR) beyond English. We collect the Mickey corpus, consisting of 561k sentences in 11 different languages, which can be used for analyzing and improving ML-LMs. We propose Mickey Probe, a language-general probing task for fairly evaluating the common sense of popular ML-LMs across different languages. In addition, we also create two new datasets, X-CSQA and X-CODAH, by translating their English versions to 14 other languages, so that we can evaluate popular ML-LMs for cross-lingual commonsense reasoning. To improve the performance beyond English, we propose a simple yet effective method — multilingual contrastive pretraining (MCP). It significantly enhances sentence representations, yielding a large performance gain on both benchmarks (e.g., +2.7% accuracy for X-CSQA over XLM-R_L)",
    "checked": true,
    "id": "c6fd846b9b8f9eb0a492d6d6242fffce987c4580",
    "semantic_title": "",
    "citation_count": 21,
    "authors": [
      "Bill Yuchen Lin",
      "Seyeon Lee",
      "Xiaoyang Qiao",
      "Xiang Ren"
    ]
  },
  "https://aclanthology.org/2021.acl-long.103": {
    "title": "Attention Calibration for Transformer in Neural Machine Translation",
    "volume": "long",
    "abstract": "Attention mechanisms have achieved substantial improvements in neural machine translation by dynamically selecting relevant inputs for different predictions. However, recent studies have questioned the attention mechanisms’ capability for discovering decisive inputs. In this paper, we propose to calibrate the attention weights by introducing a mask perturbation model that automatically evaluates each input’s contribution to the model outputs. We increase the attention weights assigned to the indispensable tokens, whose removal leads to a dramatic performance decrease. The extensive experiments on the Transformer-based translation have demonstrated the effectiveness of our model. We further find that the calibrated attention weights are more uniform at lower layers to collect multiple information while more concentrated on the specific inputs at higher layers. Detailed analyses also show a great need for calibration in the attention weights with high entropy where the model is unconfident about its decision",
    "checked": true,
    "id": "6088df145eacfc6c018cc356c3cb9e22583fcbed",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Yu Lu",
      "Jiali Zeng",
      "Jiajun Zhang",
      "Shuangzhi Wu",
      "Mu Li"
    ]
  },
  "https://aclanthology.org/2021.acl-long.104": {
    "title": "Diverse Pretrained Context Encodings Improve Document Translation",
    "volume": "long",
    "abstract": "We propose a new architecture for adapting a sentence-level sequence-to-sequence transformer by incorporating multiple pre-trained document context signals and assess the impact on translation performance of (1) different pretraining approaches for generating these signals, (2) the quantity of parallel data for which document context is available, and (3) conditioning on source, target, or source and target contexts. Experiments on the NIST Chinese-English, and IWSLT and WMT English-German tasks support four general conclusions: that using pre-trained context representations markedly improves sample efficiency, that adequate parallel data resources are crucial for learning to use document context, that jointly conditioning on multiple context representations outperforms any single representation, and that source context is more valuable for translation performance than target side context. Our best multi-context model consistently outperforms the best existing context-aware transformers",
    "checked": true,
    "id": "fcbe50d809d26d683ece76ef91f55949c2d40408",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Domenic Donato",
      "Lei Yu",
      "Chris Dyer"
    ]
  },
  "https://aclanthology.org/2021.acl-long.105": {
    "title": "Exploiting Language Relatedness for Low Web-Resource Language Model Adaptation: An Indic Languages Study",
    "volume": "long",
    "abstract": "Recent research in multilingual language models (LM) has demonstrated their ability to effectively handle multiple languages in a single model. This holds promise for low web-resource languages (LRL) as multilingual models can enable transfer of supervision from high resource languages to LRLs. However, incorporating a new language in an LM still remains a challenge, particularly for languages with limited corpora and in unseen scripts. In this paper we argue that relatedness among languages in a language family may be exploited to overcome some of the corpora limitations of LRLs, and propose RelateLM. We focus on Indian languages, and exploit relatedness along two dimensions: (1) script (since many Indic scripts originated from the Brahmic script), and (2) sentence structure. RelateLM uses transliteration to convert the unseen script of limited LRL text into the script of a Related Prominent Language (RPL) (Hindi in our case). While exploiting similar sentence structures, RelateLM utilizes readily available bilingual dictionaries to pseudo translate RPL text into LRL corpora. Experiments on multiple real-world benchmark datasets provide validation to our hypothesis that using a related language as pivot, along with transliteration and pseudo translation based data augmentation, can be an effective way to adapt LMs for LRLs, rather than direct training or pivoting through English",
    "checked": true,
    "id": "dec618c83386fc00b5c730ca688f522985b21b89",
    "semantic_title": "",
    "citation_count": 17,
    "authors": [
      "Yash Khemchandani",
      "Sarvesh Mehtani",
      "Vaidehi Patil",
      "Abhijeet Awasthi",
      "Partha Talukdar",
      "Sunita Sarawagi"
    ]
  },
  "https://aclanthology.org/2021.acl-long.106": {
    "title": "On Finding the K-best Non-projective Dependency Trees",
    "volume": "long",
    "abstract": "The connection between the maximum spanning tree in a directed graph and the best dependency tree of a sentence has been exploited by the NLP community. However, for many dependency parsing schemes, an important detail of this approach is that the spanning tree must have exactly one edge emanating from the root. While work has been done to efficiently solve this problem for finding the one-best dependency tree, no research has attempted to extend this solution to finding the K-best dependency trees. This is arguably a more important extension as a larger proportion of decoded trees will not be subject to the root constraint of dependency trees. Indeed, we show that the rate of root constraint violations increases by an average of 13 times when decoding with K=50 as opposed to K=1. In this paper, we provide a simplification of the K-best spanning tree algorithm of Camerini et al. (1980). Our simplification allows us to obtain a constant time speed-up over the original algorithm. Furthermore, we present a novel extension of the algorithm for decoding the K-best dependency trees of a graph which are subject to a root constraint",
    "checked": true,
    "id": "13503a496706d1abdcc1f79addca246789253356",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Ran Zmigrod",
      "Tim Vieira",
      "Ryan Cotterell"
    ]
  },
  "https://aclanthology.org/2021.acl-long.107": {
    "title": "Towards Argument Mining for Social Good: A Survey",
    "volume": "long",
    "abstract": "This survey builds an interdisciplinary picture of Argument Mining (AM), with a strong focus on its potential to address issues related to Social and Political Science. More specifically, we focus on AM challenges related to its applications to social media and in the multilingual domain, and then proceed to the widely debated notion of argument quality. We propose a novel definition of argument quality which is integrated with that of deliberative quality from the Social Science literature. Under our definition, the quality of a contribution needs to be assessed at multiple levels: the contribution itself, its preceding context, and the consequential effect on the development of the upcoming discourse. The latter has not received the deserved attention within the community. We finally define an application of AM for Social Good: (semi-)automatic moderation, a highly integrative application which (a) represents a challenging testbed for the integrated notion of quality we advocate, (b) allows the empirical quantification of argument/deliberative quality to benefit from the developments in other NLP fields (i.e. hate speech detection, fact checking, debiasing), and (c) has a clearly beneficial potential at the level of its societal thanks to its real-world application (even if extremely ambitious)",
    "checked": true,
    "id": "dcb0b23685c9c116d8d53fe47e5157753659d3bd",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Eva Maria Vecchi",
      "Neele Falk",
      "Iman Jundi",
      "Gabriella Lapesa"
    ]
  },
  "https://aclanthology.org/2021.acl-long.108": {
    "title": "Automated Generation of Storytelling Vocabulary from Photographs for use in AAC",
    "volume": "long",
    "abstract": "Research on the application of NLP in symbol-based Augmentative and Alternative Communication (AAC) tools for improving social interaction support is scarce. We contribute a novel method for generating context-related vocabulary from photographs of personally relevant events aimed at supporting people with language impairments in retelling their past experiences. Performance was calculated with information retrieval concepts on the relevance of vocabulary generated for communicating a corpus of 9730 narrative phrases about events depicted in 1946 photographs. In comparison to a baseline generation composed of frequent English words, our method generated vocabulary with a 4.6 gain in mean average precision, regardless of the level of contextual information in the input photographs, and 6.9 for photographs in which contextual information was extracted correctly. We conclude by discussing how our findings provide insights for system optimization and usage",
    "checked": true,
    "id": "93db638ccd1342ed20ffd0d28fcc4007b836d547",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Mauricio Fontana de Vargas",
      "Karyn Moffatt"
    ]
  },
  "https://aclanthology.org/2021.acl-long.109": {
    "title": "CLIP: A Dataset for Extracting Action Items for Physicians from Hospital Discharge Notes",
    "volume": "long",
    "abstract": "Continuity of care is crucial to ensuring positive health outcomes for patients discharged from an inpatient hospital setting, and improved information sharing can help. To share information, caregivers write discharge notes containing action items to share with patients and their future caregivers, but these action items are easily lost due to the lengthiness of the documents. In this work, we describe our creation of a dataset of clinical action items annotated over MIMIC-III, the largest publicly available dataset of real clinical notes. This dataset, which we call CLIP, is annotated by physicians and covers 718 documents representing 100K sentences. We describe the task of extracting the action items from these documents as multi-aspect extractive summarization, with each aspect representing a type of action to be taken. We evaluate several machine learning models on this task, and show that the best models exploit in-domain language model pre-training on 59K unannotated documents, and incorporate context from neighboring sentences. We also propose an approach to pre-training data selection that allows us to explore the trade-off between size and domain-specificity of pre-training datasets for this task",
    "checked": true,
    "id": "d41feee506b3a85718ac6cb7fd8667c91de03f31",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "James Mullenbach",
      "Yada Pruksachatkun",
      "Sean Adler",
      "Jennifer Seale",
      "Jordan Swartz",
      "Greg McKelvey",
      "Hui Dai",
      "Yi Yang",
      "David Sontag"
    ]
  },
  "https://aclanthology.org/2021.acl-long.110": {
    "title": "Assessing Emoji Use in Modern Text Processing Tools",
    "volume": "long",
    "abstract": "Emojis have become ubiquitous in digital communication, due to their visual appeal as well as their ability to vividly convey human emotion, among other factors. This also leads to an increased need for systems and tools to operate on text containing emojis. In this study, we assess this support by considering test sets of tweets with emojis, based on which we perform a series of experiments investigating the ability of prominent NLP and text processing tools to adequately process them. In particular, we consider tokenization, part-of-speech tagging, dependency parsing, as well as sentiment analysis. Our findings show that many systems still have notable shortcomings when operating on text containing emojis",
    "checked": true,
    "id": "662f3d965f1be7bf2d06811cfb96e296134d3731",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Abu Awal Md Shoeb",
      "Gerard de Melo"
    ]
  },
  "https://aclanthology.org/2021.acl-long.111": {
    "title": "Select, Extract and Generate: Neural Keyphrase Generation with Layer-wise Coverage Attention",
    "volume": "long",
    "abstract": "Natural language processing techniques have demonstrated promising results in keyphrase generation. However, one of the major challenges in neural keyphrase generation is processing long documents using deep neural networks. Generally, documents are truncated before given as inputs to neural networks. Consequently, the models may miss essential points conveyed in the target document. To overcome this limitation, we propose SEG-Net, a neural keyphrase generation model that is composed of two major components, (1) a selector that selects the salient sentences in a document and (2) an extractor-generator that jointly extracts and generates keyphrases from the selected sentences. SEG-Net uses Transformer, a self-attentive architecture, as the basic building block with a novel layer-wise coverage attention to summarize most of the points discussed in the document. The experimental results on seven keyphrase generation benchmarks from scientific and web documents demonstrate that SEG-Net outperforms the state-of-the-art neural generative methods by a large margin",
    "checked": true,
    "id": "6f1ef17c614fb85adef3bbfaa49bbb9b94e8a673",
    "semantic_title": "",
    "citation_count": 24,
    "authors": [
      "Wasi Ahmad",
      "Xiao Bai",
      "Soomin Lee",
      "Kai-Wei Chang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.112": {
    "title": "Factorising Meaning and Form for Intent-Preserving Paraphrasing",
    "volume": "long",
    "abstract": "We propose a method for generating paraphrases of English questions that retain the original intent but use a different surface form. Our model combines a careful choice of training objective with a principled information bottleneck, to induce a latent encoding space that disentangles meaning and form. We train an encoder-decoder model to reconstruct a question from a paraphrase with the same meaning and an exemplar with the same surface form, leading to separated encoding spaces. We use a Vector-Quantized Variational Autoencoder to represent the surface form as a set of discrete latent variables, allowing us to use a classifier to select a different surface form at test time. Crucially, our method does not require access to an external source of target exemplars. Extensive experiments and a human evaluation show that we are able to generate paraphrases with a better tradeoff between semantic preservation and syntactic novelty compared to previous methods",
    "checked": true,
    "id": "0910da49f55e91237d4f763a8483941db5fa84f2",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Tom Hosking",
      "Mirella Lapata"
    ]
  },
  "https://aclanthology.org/2021.acl-long.113": {
    "title": "AggGen: Ordering and Aggregating while Generating",
    "volume": "long",
    "abstract": "We present AggGen (pronounced ‘again’) a data-to-text model which re-introduces two explicit sentence planning stages into neural data-to-text systems: input ordering and input aggregation. In contrast to previous work using sentence planning, our model is still end-to-end: AggGen performs sentence planning at the same time as generating text by learning latent alignments (via semantic facts) between input representation and target text. Experiments on the WebNLG and E2E challenge data show that by using fact-based alignments our approach is more interpretable, expressive, robust to noise, and easier to control, while retaining the advantages of end-to-end systems in terms of fluency. Our code is available at https://github.com/XinnuoXu/AggGen",
    "checked": true,
    "id": "9748c19744ea164af31f354357d7ff1ca47903b9",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Xinnuo Xu",
      "Ondřej Dušek",
      "Verena Rieser",
      "Ioannis Konstas"
    ]
  },
  "https://aclanthology.org/2021.acl-long.114": {
    "title": "Reflective Decoding: Beyond Unidirectional Generation with Off-the-Shelf Language Models",
    "volume": "long",
    "abstract": "Publicly available, large pretrained Language Models (LMs) generate text with remarkable quality, but only sequentially from left to right. As a result, they are not immediately applicable to generation tasks that break the unidirectional assumption, such as paraphrasing or text-infilling, necessitating task-specific supervision. In this paper, we present Reflective Decoding, a novel unsupervised algorithm that allows for direct application of unidirectional LMs to non-sequential tasks. Our 2-step approach requires no supervision or even parallel corpora, only two off-the-shelf pretrained LMs in opposite directions: forward and backward. First, in the contextualization step, we use LMs to generate ensembles of past and future contexts which collectively capture the input (e.g. the source sentence for paraphrasing). Second, in the reflection step, we condition on these “context ensembles”, generating outputs that are compatible with them. Comprehensive empirical results demonstrate that Reflective Decoding outperforms strong unsupervised baselines on both paraphrasing and abductive text infilling, significantly narrowing the gap between unsupervised and supervised methods. Reflective Decoding surpasses multiple supervised baselines on various metrics including human evaluation",
    "checked": true,
    "id": "1dc513a688ca92809e504144c3d1e361d1df9927",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Peter West",
      "Ximing Lu",
      "Ari Holtzman",
      "Chandra Bhagavatula",
      "Jena D. Hwang",
      "Yejin Choi"
    ]
  },
  "https://aclanthology.org/2021.acl-long.115": {
    "title": "Towards Table-to-Text Generation with Numerical Reasoning",
    "volume": "long",
    "abstract": "Recent neural text generation models have shown significant improvement in generating descriptive text from structured data such as table formats. One of the remaining important challenges is generating more analytical descriptions that can be inferred from facts in a data source. The use of a template-based generator and a pointer-generator is among the potential alternatives for table-to-text generators. In this paper, we propose a framework consisting of a pre-trained model and a copy mechanism. The pre-trained models are fine-tuned to produce fluent text that is enriched with numerical reasoning. However, it still lacks fidelity to the table contents. The copy mechanism is incorporated in the fine-tuning step by using general placeholders to avoid producing hallucinated phrases that are not supported by a table while preserving high fluency. In summary, our contributions are (1) a new dataset for numerical table-to-text generation using pairs of a table and a paragraph of a table description with richer inference from scientific papers, and (2) a table-to-text generation framework enriched with numerical reasoning",
    "checked": true,
    "id": "0ce43def1c95802a464449f055e78c08d6d99dc2",
    "semantic_title": "",
    "citation_count": 32,
    "authors": [
      "Lya Hulliyyatus Suadaa",
      "Hidetaka Kamigaito",
      "Kotaro Funakoshi",
      "Manabu Okumura",
      "Hiroya Takamura"
    ]
  },
  "https://aclanthology.org/2021.acl-long.116": {
    "title": "BACO: A Background Knowledge- and Content-Based Framework for Citing Sentence Generation",
    "volume": "long",
    "abstract": "In this paper, we focus on the problem of citing sentence generation, which entails generating a short text to capture the salient information in a cited paper and the connection between the citing and cited paper. We present BACO, a BAckground knowledge- and COntent-based framework for citing sentence generation, which considers two types of information: (1) background knowledge by leveraging structural information from a citation network; and (2) content, which represents in-depth information about what to cite and why to cite. First, a citation network is encoded to provide background knowledge. Second, we apply salience estimation to identify what to cite by estimating the importance of sentences in the cited paper. During the decoding stage, both types of information are combined to facilitate the text generation, and then we conduct a joint training for the generator and citation function classification to make the model aware of why to cite. Our experimental results show that our framework outperforms comparative baselines",
    "checked": true,
    "id": "cc80f9033147b9f008cef40488db74be076103e5",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Yubin Ge",
      "Ly Dinh",
      "Xiaofeng Liu",
      "Jinsong Su",
      "Ziyao Lu",
      "Ante Wang",
      "Jana Diesner"
    ]
  },
  "https://aclanthology.org/2021.acl-long.117": {
    "title": "Language Model as an Annotator: Exploring DialoGPT for Dialogue Summarization",
    "volume": "long",
    "abstract": "Current dialogue summarization systems usually encode the text with a number of general semantic features (e.g., keywords and topics) to gain more powerful dialogue modeling capabilities. However, these features are obtained via open-domain toolkits that are dialog-agnostic or heavily relied on human annotations. In this paper, we show how DialoGPT, a pre-trained model for conversational response generation, can be developed as an unsupervised dialogue annotator, which takes advantage of dialogue background knowledge encoded in DialoGPT. We apply DialoGPT to label three types of features on two dialogue summarization datasets, SAMSum and AMI, and employ pre-trained and non pre-trained models as our summarizers. Experimental results show that our proposed method can obtain remarkable improvements on both datasets and achieves new state-of-the-art performance on the SAMSum dataset",
    "checked": true,
    "id": "f80b837a211f71c6647c5755d4569559f0c2c0f7",
    "semantic_title": "",
    "citation_count": 42,
    "authors": [
      "Xiachong Feng",
      "Xiaocheng Feng",
      "Libo Qin",
      "Bing Qin",
      "Ting Liu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.118": {
    "title": "Challenges in Information-Seeking QA: Unanswerable Questions and Paragraph Retrieval",
    "volume": "long",
    "abstract": "Recent pretrained language models “solved” many reading comprehension benchmarks, where questions are written with access to the evidence document. However, datasets containing information-seeking queries where evidence documents are provided after the queries are written independently remain challenging. We analyze why answering information-seeking queries is more challenging and where their prevalent unanswerabilities arise, on Natural Questions and TyDi QA. Our controlled experiments suggest two headrooms – paragraph selection and answerability prediction, i.e. whether the paired evidence document contains the answer to the query or not. When provided with a gold paragraph and knowing when to abstain from answering, existing models easily outperform a human annotator. However, predicting answerability itself remains challenging. We manually annotate 800 unanswerable examples across six languages on what makes them challenging to answer. With this new data, we conduct per-category answerability prediction, revealing issues in the current dataset collection as well as task formulation. Together, our study points to avenues for future research in information-seeking question answering, both for dataset creation and model development. Our code and annotated data is publicly available at https://github.com/AkariAsai/unanswerable_qa",
    "checked": true,
    "id": "372cce47fa16c538946972e6a7ac8420e64000b0",
    "semantic_title": "",
    "citation_count": 27,
    "authors": [
      "Akari Asai",
      "Eunsol Choi"
    ]
  },
  "https://aclanthology.org/2021.acl-long.119": {
    "title": "A Gradually Soft Multi-Task and Data-Augmented Approach to Medical Question Understanding",
    "volume": "long",
    "abstract": "Users of medical question answering systems often submit long and detailed questions, making it hard to achieve high recall in answer retrieval. To alleviate this problem, we propose a novel Multi-Task Learning (MTL) method with data augmentation for medical question understanding. We first establish an equivalence between the tasks of question summarization and Recognizing Question Entailment (RQE) using their definitions in the medical domain. Based on this equivalence, we propose a data augmentation algorithm to use just one dataset to optimize for both tasks, with a weighted MTL loss. We introduce gradually soft parameter-sharing: a constraint for decoder parameters to be close, that is gradually loosened as we move to the highest layer. We show through ablation studies that our proposed novelties improve performance. Our method outperforms existing MTL methods across 4 datasets of medical question pairs, in ROUGE scores, RQE accuracy and human evaluation. Finally, we show that our method fares better than single-task learning under 4 low-resource settings",
    "checked": true,
    "id": "562f4062b43830115568f9552b140721c958a72c",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Khalil Mrini",
      "Franck Dernoncourt",
      "Seunghyun Yoon",
      "Trung Bui",
      "Walter Chang",
      "Emilia Farcas",
      "Ndapa Nakashole"
    ]
  },
  "https://aclanthology.org/2021.acl-long.120": {
    "title": "Leveraging Type Descriptions for Zero-shot Named Entity Recognition and Classification",
    "volume": "long",
    "abstract": "A common issue in real-world applications of named entity recognition and classification (NERC) is the absence of annotated data for the target entity classes during training. Zero-shot learning approaches address this issue by learning models from classes with training data that can predict classes without it. This paper presents the first approach for zero-shot NERC, introducing novel architectures that leverage the fact that textual descriptions for many entity classes occur naturally. We address the zero-shot NERC specific challenge that the not-an-entity class is not well defined as different entity classes are considered in training and testing. For evaluation, we adapt two datasets, OntoNotes and MedMentions, emulating the difficulty of real-world zero-shot learning by testing models on the rarest entity classes. Our proposed approach outperforms baselines adapted from machine reading comprehension and zero-shot text classification. Furthermore, we assess the effect of different class descriptions for this task",
    "checked": true,
    "id": "19fde8f1c620d3d2a43ad126e9078f7460fa3ec4",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Rami Aly",
      "Andreas Vlachos",
      "Ryan McDonald"
    ]
  },
  "https://aclanthology.org/2021.acl-long.121": {
    "title": "MECT: Multi-Metadata Embedding based Cross-Transformer for Chinese Named Entity Recognition",
    "volume": "long",
    "abstract": "Recently, word enhancement has become very popular for Chinese Named Entity Recognition (NER), reducing segmentation errors and increasing the semantic and boundary information of Chinese words. However, these methods tend to ignore the information of the Chinese character structure after integrating the lexical information. Chinese characters have evolved from pictographs since ancient times, and their structure often reflects more information about the characters. This paper presents a novel Multi-metadata Embedding based Cross-Transformer (MECT) to improve the performance of Chinese NER by fusing the structural information of Chinese characters. Specifically, we use multi-metadata embedding in a two-stream Transformer to integrate Chinese character features with the radical-level embedding. With the structural characteristics of Chinese characters, MECT can better capture the semantic information of Chinese characters for NER. The experimental results obtained on several well-known benchmarking datasets demonstrate the merits and superiority of the proposed MECT method",
    "checked": true,
    "id": "a7c9905f06f3d13953a748af4d9102464030623e",
    "semantic_title": "",
    "citation_count": 36,
    "authors": [
      "Shuang Wu",
      "Xiaoning Song",
      "Zhenhua Feng"
    ]
  },
  "https://aclanthology.org/2021.acl-long.122": {
    "title": "Factuality Assessment as Modal Dependency Parsing",
    "volume": "long",
    "abstract": "As the sources of information that we consume everyday rapidly diversify, it is becoming increasingly important to develop NLP tools that help to evaluate the credibility of the information we receive. A critical step towards this goal is to determine the factuality of events in text. In this paper, we frame factuality assessment as a modal dependency parsing task that identifies the events and their sources, formally known as conceivers, and then determine the level of certainty that the sources are asserting with respect to the events. We crowdsource the first large-scale data set annotated with modal dependency structures that consists of 353 Covid-19 related news articles, 24,016 events, and 2,938 conceivers. We also develop the first modal dependency parser that jointly extracts events, conceivers and constructs the modal dependency structure of a text. We evaluate the joint model against a pipeline model and demonstrate the advantage of the joint model in conceiver extraction and modal dependency structure construction when events and conceivers are automatically extracted. We believe the dataset and the models will be a valuable resource for a whole host of NLP applications such as fact checking and rumor detection",
    "checked": true,
    "id": "ddf45536044b870a7e93966317474ef3b2ab2560",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Jiarui Yao",
      "Haoling Qiu",
      "Jin Zhao",
      "Bonan Min",
      "Nianwen Xue"
    ]
  },
  "https://aclanthology.org/2021.acl-long.123": {
    "title": "Directed Acyclic Graph Network for Conversational Emotion Recognition",
    "volume": "long",
    "abstract": "The modeling of conversational context plays a vital role in emotion recognition from conversation (ERC). In this paper, we put forward a novel idea of encoding the utterances with a directed acyclic graph (DAG) to better model the intrinsic structure within a conversation, and design a directed acyclic neural network, namely DAG-ERC, to implement this idea. In an attempt to combine the strengths of conventional graph-based neural models and recurrence-based neural models, DAG-ERC provides a more intuitive way to model the information flow between long-distance conversation background and nearby context. Extensive experiments are conducted on four ERC benchmarks with state-of-the-art models employed as baselines for comparison. The empirical results demonstrate the superiority of this new model and confirm the motivation of the directed acyclic graph architecture for ERC",
    "checked": true,
    "id": "b779d2f7b66323f29444de112468818cc528393b",
    "semantic_title": "",
    "citation_count": 69,
    "authors": [
      "Weizhou Shen",
      "Siyue Wu",
      "Yunyi Yang",
      "Xiaojun Quan"
    ]
  },
  "https://aclanthology.org/2021.acl-long.124": {
    "title": "Improving Formality Style Transfer with Context-Aware Rule Injection",
    "volume": "long",
    "abstract": "Models pre-trained on large-scale regular text corpora often do not work well for user-generated data where the language styles differ significantly from the mainstream text. Here we present Context-Aware Rule Injection (CARI), an innovative method for formality style transfer (FST) by injecting multiple rules into an end-to-end BERT-based encoder and decoder model. CARI is able to learn to select optimal rules based on context. The intrinsic evaluation showed that CARI achieved the new highest performance on the FST benchmark dataset. Our extrinsic evaluation showed that CARI can greatly improve the regular pre-trained models’ performance on several tweet sentiment analysis tasks. Our contributions are as follows: 1.We propose a new method, CARI, to integrate rules for pre-trained language models. CARI is context-aware and can trained end-to-end with the downstream NLP applications. 2.We have achieved new state-of-the-art results for FST on the benchmark GYAFC dataset. 3.We are the first to evaluate FST methods with extrinsic evaluation and specifically on sentiment classification tasks. We show that CARI outperformed existing rule-based FST approaches for sentiment classification",
    "checked": true,
    "id": "e7a0c610844d6b6f49118d46353a8b8ba924445e",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Zonghai Yao",
      "Hong Yu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.125": {
    "title": "Topic-Driven and Knowledge-Aware Transformer for Dialogue Emotion Detection",
    "volume": "long",
    "abstract": "Emotion detection in dialogues is challenging as it often requires the identification of thematic topics underlying a conversation, the relevant commonsense knowledge, and the intricate transition patterns between the affective states. In this paper, we propose a Topic-Driven Knowledge-Aware Transformer to handle the challenges above. We firstly design a topic-augmented language model (LM) with an additional layer specialized for topic detection. The topic-augmented LM is then combined with commonsense statements derived from a knowledge base based on the dialogue contextual information. Finally, a transformer-based encoder-decoder architecture fuses the topical and commonsense information, and performs the emotion label sequence prediction. The model has been experimented on four datasets in dialogue emotion detection, demonstrating its superiority empirically over the existing state-of-the-art approaches. Quantitative and qualitative results show that the model can discover topics which help in distinguishing emotion categories",
    "checked": true,
    "id": "cf2725b1b376ff842cb6ae890b6156fff2f8f4fe",
    "semantic_title": "",
    "citation_count": 40,
    "authors": [
      "Lixing Zhu",
      "Gabriele Pergola",
      "Lin Gui",
      "Deyu Zhou",
      "Yulan He"
    ]
  },
  "https://aclanthology.org/2021.acl-long.126": {
    "title": "Syntopical Graphs for Computational Argumentation Tasks",
    "volume": "long",
    "abstract": "Approaches to computational argumentation tasks such as stance detection and aspect detection have largely focused on the text of independent claims, losing out on potentially valuable context provided by the rest of the collection. We introduce a general approach to these tasks motivated by syntopical reading, a reading process that emphasizes comparing and contrasting viewpoints in order to improve topic understanding. To capture collection-level context, we introduce the syntopical graph, a data structure for linking claims within a collection. A syntopical graph is a typed multi-graph where nodes represent claims and edges represent different possible pairwise relationships, such as entailment, paraphrase, or support. Experiments applying syntopical graphs to the problems of detecting stance and aspects demonstrate state-of-the-art performance in each domain, significantly outperforming approaches that do not utilize collection-level information",
    "checked": true,
    "id": "6e0bde6ebe84eeb745c42c5901332cdf307bfaab",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Joe Barrow",
      "Rajiv Jain",
      "Nedim Lipka",
      "Franck Dernoncourt",
      "Vlad Morariu",
      "Varun Manjunatha",
      "Douglas Oard",
      "Philip Resnik",
      "Henning Wachsmuth"
    ]
  },
  "https://aclanthology.org/2021.acl-long.127": {
    "title": "Stance Detection in COVID-19 Tweets",
    "volume": "long",
    "abstract": "The prevalence of the COVID-19 pandemic in day-to-day life has yielded large amounts of stance detection data on social media sites, as users turn to social media to share their views regarding various issues related to the pandemic, e.g. stay at home mandates and wearing face masks when out in public. We set out to make use of this data by collecting the stance expressed by Twitter users, with respect to topics revolving around the pandemic. We annotate a new stance detection dataset, called COVID-19-Stance. Using this newly annotated dataset, we train several established stance detection models to ascertain a baseline performance for this specific task. To further improve the performance, we employ self-training and domain adaptation approaches to take advantage of large amounts of unlabeled data and existing stance detection datasets. The dataset, code, and other resources are available on GitHub",
    "checked": true,
    "id": "9d97b26877c51030ff7d89abae491803651320ed",
    "semantic_title": "",
    "citation_count": 57,
    "authors": [
      "Kyle Glandt",
      "Sarthak Khanal",
      "Yingjie Li",
      "Doina Caragea",
      "Cornelia Caragea"
    ]
  },
  "https://aclanthology.org/2021.acl-long.128": {
    "title": "Topic-Aware Evidence Reasoning and Stance-Aware Aggregation for Fact Verification",
    "volume": "long",
    "abstract": "Fact verification is a challenging task that requires simultaneously reasoning and aggregating over multiple retrieved pieces of evidence to evaluate the truthfulness of a claim. Existing approaches typically (i) explore the semantic interaction between the claim and evidence at different granularity levels but fail to capture their topical consistency during the reasoning process, which we believe is crucial for verification; (ii) aggregate multiple pieces of evidence equally without considering their implicit stances to the claim, thereby introducing spurious information. To alleviate the above issues, we propose a novel topic-aware evidence reasoning and stance-aware aggregation model for more accurate fact verification, with the following four key properties: 1) checking topical consistency between the claim and evidence; 2) maintaining topical coherence among multiple pieces of evidence; 3) ensuring semantic similarity between the global topic information and the semantic representation of evidence; 4) aggregating evidence based on their implicit stances to the claim. Extensive experiments conducted on the two benchmark datasets demonstrate the superiority of the proposed model over several state-of-the-art approaches for fact verification. The source code can be obtained from https://github.com/jasenchn/TARSA",
    "checked": true,
    "id": "b05352f81e8924c30246334b855b1afadf6b67d6",
    "semantic_title": "",
    "citation_count": 19,
    "authors": [
      "Jiasheng Si",
      "Deyu Zhou",
      "Tongzhe Li",
      "Xingyu Shi",
      "Yulan He"
    ]
  },
  "https://aclanthology.org/2021.acl-long.129": {
    "title": "Changes in European Solidarity Before and During COVID-19: Evidence from a Large Crowd- and Expert-Annotated Twitter Dataset",
    "volume": "long",
    "abstract": "We introduce the well-established social scientific concept of social solidarity and its contestation, anti-solidarity, as a new problem setting to supervised machine learning in NLP to assess how European solidarity discourses changed before and after the COVID-19 outbreak was declared a global pandemic. To this end, we annotate 2.3k English and German tweets for (anti-)solidarity expressions, utilizing multiple human annotators and two annotation approaches (experts vs. crowds). We use these annotations to train a BERT model with multiple data augmentation strategies. Our augmented BERT model that combines both expert and crowd annotations outperforms the baseline BERT classifier trained with expert annotations only by over 25 points, from 58% macro-F1 to almost 85%. We use this high-quality model to automatically label over 270k tweets between September 2019 and December 2020. We then assess the automatically labeled data for how statements related to European (anti-)solidarity discourses developed over time and in relation to one another, before and during the COVID-19 crisis. Our results show that solidarity became increasingly salient and contested during the crisis. While the number of solidarity tweets remained on a higher level and dominated the discourse in the scrutinized time frame, anti-solidarity tweets initially spiked, then decreased to (almost) pre-COVID-19 values before rising to a stable higher level until the end of 2020",
    "checked": true,
    "id": "2148872ff9ea585d4039672c3f0ca51769a44bea",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Alexandra Ils",
      "Dan Liu",
      "Daniela Grunow",
      "Steffen Eger"
    ]
  },
  "https://aclanthology.org/2021.acl-long.130": {
    "title": "Measuring Conversational Uptake: A Case Study on Student-Teacher Interactions",
    "volume": "long",
    "abstract": "In conversation, uptake happens when a speaker builds on the contribution of their interlocutor by, for example, acknowledging, repeating or reformulating what they have said. In education, teachers’ uptake of student contributions has been linked to higher student achievement. Yet measuring and improving teachers’ uptake at scale is challenging, as existing methods require expensive annotation by experts. We propose a framework for computationally measuring uptake, by (1) releasing a dataset of student-teacher exchanges extracted from US math classroom transcripts annotated for uptake by experts; (2) formalizing uptake as pointwise Jensen-Shannon Divergence (pJSD), estimated via next utterance classification; (3) conducting a linguistically-motivated comparison of different unsupervised measures and (4) correlating these measures with educational outcomes. We find that although repetition captures a significant part of uptake, pJSD outperforms repetition-based baselines, as it is capable of identifying a wider range of uptake phenomena like question answering and reformulation. We apply our uptake measure to three different educational datasets with outcome indicators. Unlike baseline measures, pJSD correlates significantly with instruction quality in all three, providing evidence for its generalizability and for its potential to serve as an automated professional development tool for teachers",
    "checked": true,
    "id": "30b0269ab643058c11cb878ee194972fb516ddac",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Dorottya Demszky",
      "Jing Liu",
      "Zid Mancenido",
      "Julie Cohen",
      "Heather Hill",
      "Dan Jurafsky",
      "Tatsunori Hashimoto"
    ]
  },
  "https://aclanthology.org/2021.acl-long.131": {
    "title": "A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies",
    "volume": "long",
    "abstract": "The analysis of data in which multiple languages are represented has gained popularity among computational linguists in recent years. So far, much of this research focuses mainly on the improvement of computational methods and largely ignores linguistic and social aspects of C-S discussed across a wide range of languages within the long-established literature in linguistics. To fill this gap, we offer a survey of code-switching (C-S) covering the literature in linguistics with a reflection on the key issues in language technologies. From the linguistic perspective, we provide an overview of structural and functional patterns of C-S focusing on the literature from European and Indian contexts as highly multilingual areas. From the language technologies perspective, we discuss how massive language models fail to represent diverse C-S types due to lack of appropriate training data, lack of robust evaluation benchmarks for C-S (across multilingual situations and types of C-S) and lack of end-to- end systems that cover sociolinguistic aspects of C-S as well. Our survey will be a step to- wards an outcome of mutual benefit for computational scientists and linguists with a shared interest in multilingualism and C-S",
    "checked": true,
    "id": "ee6d66efc86746d42ace14db30fcbaf9d3380e25",
    "semantic_title": "",
    "citation_count": 23,
    "authors": [
      "A. Seza Doğruöz",
      "Sunayana Sitaram",
      "Barbara E. Bullock",
      "Almeida Jacqueline Toribio"
    ]
  },
  "https://aclanthology.org/2021.acl-long.132": {
    "title": "Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection",
    "volume": "long",
    "abstract": "We present a human-and-model-in-the-loop process for dynamically generating datasets and training better performing and more robust hate detection models. We provide a new dataset of 40,000 entries, generated and labelled by trained annotators over four rounds of dynamic data creation. It includes 15,000 challenging perturbations and each hateful entry has fine-grained labels for the type and target of hate. Hateful entries make up 54% of the dataset, which is substantially higher than comparable datasets. We show that model performance is substantially improved using this approach. Models trained on later rounds of data collection perform better on test sets and are harder for annotators to trick. They also have better performance on HateCheck, a suite of functional tests for online hate detection. We provide the code, dataset and annotation guidelines for other researchers to use",
    "checked": true,
    "id": "6de4dc0bb66971071ced5201e37ed8f3ccd5e062",
    "semantic_title": "",
    "citation_count": 74,
    "authors": [
      "Bertie Vidgen",
      "Tristan Thrush",
      "Zeerak Waseem",
      "Douwe Kiela"
    ]
  },
  "https://aclanthology.org/2021.acl-long.133": {
    "title": "InfoSurgeon: Cross-Media Fine-grained Information Consistency Checking for Fake News Detection",
    "volume": "long",
    "abstract": "To defend against machine-generated fake news, an effective mechanism is urgently needed. We contribute a novel benchmark for fake news detection at the knowledge element level, as well as a solution for this task which incorporates cross-media consistency checking to detect the fine-grained knowledge elements making news articles misinformative. Due to training data scarcity, we also formulate a novel data synthesis method by manipulating knowledge elements within the knowledge graph to generate noisy training data with specific, hard to detect, known inconsistencies. Our detection approach outperforms the state-of-the-art (up to 16.8% accuracy gain), and more critically, yields fine-grained explanations",
    "checked": true,
    "id": "bbfed74eed1796b4534bcce6811b2c7c0b74024a",
    "semantic_title": "",
    "citation_count": 32,
    "authors": [
      "Yi Fung",
      "Christopher Thomas",
      "Revanth Gangi Reddy",
      "Sandeep Polisetty",
      "Heng Ji",
      "Shih-Fu Chang",
      "Kathleen McKeown",
      "Mohit Bansal",
      "Avi Sil"
    ]
  },
  "https://aclanthology.org/2021.acl-long.134": {
    "title": "I like fish, especially dolphins: Addressing Contradictions in Dialogue Modeling",
    "volume": "long",
    "abstract": "To quantify how well natural language understanding models can capture consistency in a general conversation, we introduce the DialoguE COntradiction DEtection task (DECODE) and a new conversational dataset containing both human-human and human-bot contradictory dialogues. We show that: (i) our newly collected dataset is notably more effective at providing supervision for the dialogue contradiction detection task than existing NLI data including those aimed to cover the dialogue domain; (ii) Transformer models that explicitly hinge on utterance structures for dialogue contradiction detection are more robust and generalize well on both analysis and out-of-distribution dialogues than standard (unstructured) Transformers. We also show that our best contradiction detection model correlates well with human judgments and further provide evidence for its usage in both automatically evaluating and improving the consistency of state-of-the-art generative chatbots",
    "checked": true,
    "id": "01a9102aa93b152f2d2978c568fb7061eb7152f1",
    "semantic_title": "",
    "citation_count": 52,
    "authors": [
      "Yixin Nie",
      "Mary Williamson",
      "Mohit Bansal",
      "Douwe Kiela",
      "Jason Weston"
    ]
  },
  "https://aclanthology.org/2021.acl-long.135": {
    "title": "A Sequence-to-Sequence Approach to Dialogue State Tracking",
    "volume": "long",
    "abstract": "This paper is concerned with dialogue state tracking (DST) in a task-oriented dialogue system. Building a DST module that is highly effective is still a challenging issue, although significant progresses have been made recently. This paper proposes a new approach to dialogue state tracking, referred to as Seq2Seq-DU, which formalizes DST as a sequence-to-sequence problem. Seq2Seq-DU employs two BERT-based encoders to respectively encode the utterances in the dialogue and the descriptions of schemas, an attender to calculate attentions between the utterance embeddings and the schema embeddings, and a decoder to generate pointers to represent the current state of dialogue. Seq2Seq-DU has the following advantages. It can jointly model intents, slots, and slot values; it can leverage the rich representations of utterances and schemas based on BERT; it can effectively deal with categorical and non-categorical slots, and unseen schemas. In addition, Seq2Seq-DU can also be used in the NLU (natural language understanding) module of a dialogue system. Experimental results on benchmark datasets in different settings (SGD, MultiWOZ2.2, MultiWOZ2.1, WOZ2.0, DSTC2, M2M, SNIPS, and ATIS) show that Seq2Seq-DU outperforms the existing methods",
    "checked": true,
    "id": "5db91e3454f1f48c8d139c8b0e1bccbda0a218ee",
    "semantic_title": "",
    "citation_count": 34,
    "authors": [
      "Yue Feng",
      "Yang Wang",
      "Hang Li"
    ]
  },
  "https://aclanthology.org/2021.acl-long.136": {
    "title": "Discovering Dialog Structure Graph for Coherent Dialog Generation",
    "volume": "long",
    "abstract": "Learning discrete dialog structure graph from human-human dialogs yields basic insights into the structure of conversation, and also provides background knowledge to facilitate dialog generation. However, this problem is less studied in open-domain dialogue. In this paper, we conduct unsupervised discovery of discrete dialog structure from chitchat corpora, and then leverage it to facilitate coherent dialog generation in downstream systems. To this end, we present an unsupervised model, Discrete Variational Auto-Encoder with Graph Neural Network (DVAE-GNN), to discover discrete hierarchical latent dialog states (at the level of both session and utterance) and their transitions from corpus as a dialog structure graph. Then we leverage it as background knowledge to facilitate dialog management in a RL based dialog system. Experimental results on two benchmark corpora confirm that DVAE-GNN can discover meaningful dialog structure graph, and the use of dialog structure as background knowledge can significantly improve multi-turn coherence",
    "checked": true,
    "id": "80fd76403c42e3cba6f80b7501458df239c80ff6",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Jun Xu",
      "Zeyang Lei",
      "Haifeng Wang",
      "Zheng-Yu Niu",
      "Hua Wu",
      "Wanxiang Che"
    ]
  },
  "https://aclanthology.org/2021.acl-long.137": {
    "title": "Dialogue Response Selection with Hierarchical Curriculum Learning",
    "volume": "long",
    "abstract": "We study the learning of a matching model for dialogue response selection. Motivated by the recent finding that models trained with random negative samples are not ideal in real-world scenarios, we propose a hierarchical curriculum learning framework that trains the matching model in an “easy-to-difficult” scheme. Our learning framework consists of two complementary curricula: (1) corpus-level curriculum (CC); and (2) instance-level curriculum (IC). In CC, the model gradually increases its ability in finding the matching clues between the dialogue context and a response candidate. As for IC, it progressively strengthens the model’s ability in identifying the mismatching information between the dialogue context and a response candidate. Empirical studies on three benchmark datasets with three state-of-the-art matching models demonstrate that the proposed learning framework significantly improves the model performance across various evaluation metrics",
    "checked": true,
    "id": "80f75ff5251f50fd0185f46cce22b4173063dee3",
    "semantic_title": "",
    "citation_count": 28,
    "authors": [
      "Yixuan Su",
      "Deng Cai",
      "Qingyu Zhou",
      "Zibo Lin",
      "Simon Baker",
      "Yunbo Cao",
      "Shuming Shi",
      "Nigel Collier",
      "Yan Wang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.138": {
    "title": "A Joint Model for Dropped Pronoun Recovery and Conversational Discourse Parsing in Chinese Conversational Speech",
    "volume": "long",
    "abstract": "In this paper, we present a neural model for joint dropped pronoun recovery (DPR) and conversational discourse parsing (CDP) in Chinese conversational speech. We show that DPR and CDP are closely related, and a joint model benefits both tasks. We refer to our model as DiscProReco, and it first encodes the tokens in each utterance in a conversation with a directed Graph Convolutional Network (GCN). The token states for an utterance are then aggregated to produce a single state for each utterance. The utterance states are then fed into a biaffine classifier to construct a conversational discourse graph. A second (multi-relational) GCN is then applied to the utterance states to produce a discourse relation-augmented representation for the utterances, which are then fused together with token states in each utterance as input to a dropped pronoun recovery layer. The joint model is trained and evaluated on a new Structure Parsing-enhanced Dropped Pronoun Recovery (SPDPR) data set that we annotated with both two types of information. Experimental results on the SPDPR dataset and other benchmarks show that DiscProReco significantly outperforms the state-of-the-art baselines of both tasks",
    "checked": true,
    "id": "fe4558c2301b21bed2b7073de04fc664823fd9cb",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Jingxuan Yang",
      "Kerui Xu",
      "Jun Xu",
      "Si Li",
      "Sheng Gao",
      "Jun Guo",
      "Nianwen Xue",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2021.acl-long.139": {
    "title": "A Systematic Investigation of KB-Text Embedding Alignment at Scale",
    "volume": "long",
    "abstract": "Knowledge bases (KBs) and text often contain complementary knowledge: KBs store structured knowledge that can support long range reasoning, while text stores more comprehensive and timely knowledge in an unstructured way. Separately embedding the individual knowledge sources into vector spaces has demonstrated tremendous successes in encoding the respective knowledge, but how to jointly embed and reason with both knowledge sources to fully leverage the complementary information is still largely an open problem. We conduct a large-scale, systematic investigation of aligning KB and text embeddings for joint reasoning. We set up a novel evaluation framework with two evaluation tasks, few-shot link prediction and analogical reasoning, and evaluate an array of KB-text embedding alignment methods. We also demonstrate how such alignment can infuse textual information into KB embeddings for more accurate link prediction on emerging entities and events, using COVID-19 as a case study",
    "checked": true,
    "id": "da1688c89dad9045c8159bf8abe4ecdbf0a7a63f",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Vardaan Pahuja",
      "Yu Gu",
      "Wenhu Chen",
      "Mehdi Bahrami",
      "Lei Liu",
      "Wei-Peng Chen",
      "Yu Su"
    ]
  },
  "https://aclanthology.org/2021.acl-long.140": {
    "title": "Named Entity Recognition with Small Strongly Labeled and Large Weakly Labeled Data",
    "volume": "long",
    "abstract": "Weak supervision has shown promising results in many natural language processing tasks, such as Named Entity Recognition (NER). Existing work mainly focuses on learning deep NER models only with weak supervision, i.e., without any human annotation, and shows that by merely using weakly labeled data, one can achieve good performance, though still underperforms fully supervised NER with manually/strongly labeled data. In this paper, we consider a more practical scenario, where we have both a small amount of strongly labeled data and a large amount of weakly labeled data. Unfortunately, we observe that weakly labeled data does not necessarily improve, or even deteriorate the model performance (due to the extensive noise in the weak labels) when we train deep NER models over a simple or weighted combination of the strongly labeled and weakly labeled data. To address this issue, we propose a new multi-stage computational framework – NEEDLE with three essential ingredients: (1) weak label completion, (2) noise-aware loss function, and (3) final fine-tuning over the strongly labeled data. Through experiments on E-commerce query NER and Biomedical NER, we demonstrate that NEEDLE can effectively suppress the noise of the weak labels and outperforms existing methods. In particular, we achieve new SOTA F1-scores on 3 Biomedical NER datasets: BC5CDR-chem 93.74, BC5CDR-disease 90.69, NCBI-disease 92.28",
    "checked": true,
    "id": "7990f22dc8d1ad2114fd5c9c36bd8ecdcf4f77dc",
    "semantic_title": "",
    "citation_count": 18,
    "authors": [
      "Haoming Jiang",
      "Danqing Zhang",
      "Tianyu Cao",
      "Bing Yin",
      "Tuo Zhao"
    ]
  },
  "https://aclanthology.org/2021.acl-long.141": {
    "title": "Ultra-Fine Entity Typing with Weak Supervision from a Masked Language Model",
    "volume": "long",
    "abstract": "Recently, there is an effort to extend fine-grained entity typing by using a richer and ultra-fine set of types, and labeling noun phrases including pronouns and nominal nouns instead of just named entity mentions. A key challenge for this ultra-fine entity typing task is that human annotated data are extremely scarce, and the annotation ability of existing distant or weak supervision approaches is very limited. To remedy this problem, in this paper, we propose to obtain training data for ultra-fine entity typing by using a BERT Masked Language Model (MLM). Given a mention in a sentence, our approach constructs an input for the BERT MLM so that it predicts context dependent hypernyms of the mention, which can be used as type labels. Experimental results demonstrate that, with the help of these automatically generated labels, the performance of an ultra-fine entity typing model can be improved substantially. We also show that our approach can be applied to improve traditional fine-grained entity typing after performing simple type mapping",
    "checked": true,
    "id": "70b49a024787d3ad374fb78dc87e3ba2b5e16566",
    "semantic_title": "",
    "citation_count": 29,
    "authors": [
      "Hongliang Dai",
      "Yangqiu Song",
      "Haixun Wang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.142": {
    "title": "Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning",
    "volume": "long",
    "abstract": "Recent advances in Named Entity Recognition (NER) show that document-level contexts can significantly improve model performance. In many application scenarios, however, such contexts are not available. In this paper, we propose to find external contexts of a sentence by retrieving and selecting a set of semantically relevant texts through a search engine, with the original sentence as the query. We find empirically that the contextual representations computed on the retrieval-based input view, constructed through the concatenation of a sentence and its external contexts, can achieve significantly improved performance compared to the original input view based only on the sentence. Furthermore, we can improve the model performance of both input views by Cooperative Learning, a training method that encourages the two input views to produce similar contextual representations or output label distributions. Experiments show that our approach can achieve new state-of-the-art performance on 8 NER data sets across 5 domains",
    "checked": true,
    "id": "e6fc6a23c057ce90c950adff480afceb07979386",
    "semantic_title": "",
    "citation_count": 65,
    "authors": [
      "Xinyu Wang",
      "Yong Jiang",
      "Nguyen Bach",
      "Tao Wang",
      "Zhongqiang Huang",
      "Fei Huang",
      "Kewei Tu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.143": {
    "title": "Implicit Representations of Meaning in Neural Language Models",
    "volume": "long",
    "abstract": "Does the effectiveness of neural language models derive entirely from accurate modeling of surface word co-occurrence statistics, or do these models represent and reason about the world they describe? In BART and T5 transformer language models, we identify contextual word representations that function as *models of entities and situations* as they evolve throughout a discourse. These neural representations have functional similarities to linguistic models of dynamic semantics: they support a linear readout of each entity’s current properties and relations, and can be manipulated with predictable effects on language generation. Our results indicate that prediction in pretrained neural language models is supported, at least in part, by dynamic representations of meaning and implicit simulation of entity state, and that this behavior can be learned with only text as training data",
    "checked": true,
    "id": "ac879df2cc36f3f824fa24149517622b6bc7bd09",
    "semantic_title": "",
    "citation_count": 52,
    "authors": [
      "Belinda Z. Li",
      "Maxwell Nye",
      "Jacob Andreas"
    ]
  },
  "https://aclanthology.org/2021.acl-long.144": {
    "title": "Causal Analysis of Syntactic Agreement Mechanisms in Neural Language Models",
    "volume": "long",
    "abstract": "Targeted syntactic evaluations have demonstrated the ability of language models to perform subject-verb agreement given difficult contexts. To elucidate the mechanisms by which the models accomplish this behavior, this study applies causal mediation analysis to pre-trained neural language models. We investigate the magnitude of models’ preferences for grammatical inflections, as well as whether neurons process subject-verb agreement similarly across sentences with different syntactic structures. We uncover similarities and differences across architectures and model sizes—notably, that larger models do not necessarily learn stronger preferences. We also observe two distinct mechanisms for producing subject-verb agreement depending on the syntactic structure of the input sentence. Finally, we find that language models rely on similar sets of neurons when given sentences with similar syntactic structure",
    "checked": true,
    "id": "488de57dde884402d88ccecfd02347dd7e4d01a1",
    "semantic_title": "",
    "citation_count": 39,
    "authors": [
      "Matthew Finlayson",
      "Aaron Mueller",
      "Sebastian Gehrmann",
      "Stuart Shieber",
      "Tal Linzen",
      "Yonatan Belinkov"
    ]
  },
  "https://aclanthology.org/2021.acl-long.145": {
    "title": "Bird's Eye: Probing for Linguistic Graph Structures with a Simple Information-Theoretic Approach",
    "volume": "long",
    "abstract": "NLP has a rich history of representing our prior understanding of language in the form of graphs. Recent work on analyzing contextualized text representations has focused on hand-designed probe models to understand how and to what extent do these representations encode a particular linguistic phenomenon. However, due to the inter-dependence of various phenomena and randomness of training probe models, detecting how these representations encode the rich information in these linguistic graphs remains a challenging problem. In this paper, we propose a new information-theoretic probe, Bird’s Eye, which is a fairly simple probe method for detecting if and how these representations encode the information in these linguistic graphs. Instead of using model performance, our probe takes an information-theoretic view of probing and estimates the mutual information between the linguistic graph embedded in a continuous space and the contextualized word representations. Furthermore, we also propose an approach to use our probe to investigate localized linguistic information in the linguistic graphs using perturbation analysis. We call this probing setup Worm’s Eye. Using these probes, we analyze the BERT models on its ability to encode a syntactic and a semantic graph structure, and find that these models encode to some degree both syntactic as well as semantic information; albeit syntactic information to a greater extent",
    "checked": true,
    "id": "999d5aaa7e565f62bbcb7167a5e000777d417e12",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Yifan Hou",
      "Mrinmaya Sachan"
    ]
  },
  "https://aclanthology.org/2021.acl-long.146": {
    "title": "Knowledgeable or Educated Guess? Revisiting Language Models as Knowledge Bases",
    "volume": "long",
    "abstract": "Previous literatures show that pre-trained masked language models (MLMs) such as BERT can achieve competitive factual knowledge extraction performance on some datasets, indicating that MLMs can potentially be a reliable knowledge source. In this paper, we conduct a rigorous study to explore the underlying predicting mechanisms of MLMs over different extraction paradigms. By investigating the behaviors of MLMs, we find that previous decent performance mainly owes to the biased prompts which overfit dataset artifacts. Furthermore, incorporating illustrative cases and external contexts improve knowledge prediction mainly due to entity type guidance and golden answer leakage. Our findings shed light on the underlying predicting mechanisms of MLMs, and strongly question the previous conclusion that current MLMs can potentially serve as reliable factual knowledge bases",
    "checked": true,
    "id": "e337ed6543c2e6e7e51c312c7d998798fc79fdde",
    "semantic_title": "",
    "citation_count": 55,
    "authors": [
      "Boxi Cao",
      "Hongyu Lin",
      "Xianpei Han",
      "Le Sun",
      "Lingyong Yan",
      "Meng Liao",
      "Tong Xue",
      "Jin Xu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.147": {
    "title": "Poisoning Knowledge Graph Embeddings via Relation Inference Patterns",
    "volume": "long",
    "abstract": "We study the problem of generating data poisoning attacks against Knowledge Graph Embedding (KGE) models for the task of link prediction in knowledge graphs. To poison KGE models, we propose to exploit their inductive abilities which are captured through the relationship patterns like symmetry, inversion and composition in the knowledge graph. Specifically, to degrade the model’s prediction confidence on target facts, we propose to improve the model’s prediction confidence on a set of decoy facts. Thus, we craft adversarial additions that can improve the model’s prediction confidence on decoy facts through different inference patterns. Our experiments demonstrate that the proposed poisoning attacks outperform state-of-art baselines on four KGE models for two publicly available datasets. We also find that the symmetry pattern based attacks generalize across all model-dataset combinations which indicates the sensitivity of KGE models to this pattern",
    "checked": true,
    "id": "36badeefc8a9743a834195e8082e70af23466bbf",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Peru Bhardwaj",
      "John Kelleher",
      "Luca Costabello",
      "Declan O’Sullivan"
    ]
  },
  "https://aclanthology.org/2021.acl-long.148": {
    "title": "Bad Seeds: Evaluating Lexical Methods for Bias Measurement",
    "volume": "long",
    "abstract": "A common factor in bias measurement methods is the use of hand-curated seed lexicons, but there remains little guidance for their selection. We gather seeds used in prior work, documenting their common sources and rationales, and in case studies of three English-language corpora, we enumerate the different types of social biases and linguistic features that, once encoded in the seeds, can affect subsequent bias measurements. Seeds developed in one context are often re-used in other contexts, but documentation and evaluation remain necessary precursors to relying on seeds for sensitive measurements",
    "checked": true,
    "id": "3ca7a604e0d351e512bfda045d2837caeb9831df",
    "semantic_title": "",
    "citation_count": 42,
    "authors": [
      "Maria Antoniak",
      "David Mimno"
    ]
  },
  "https://aclanthology.org/2021.acl-long.149": {
    "title": "A Survey of Race, Racism, and Anti-Racism in NLP",
    "volume": "long",
    "abstract": "Despite inextricable ties between race and language, little work has considered race in NLP research and development. In this work, we survey 79 papers from the ACL anthology that mention race. These papers reveal various types of race-related bias in all stages of NLP model development, highlighting the need for proactive consideration of how NLP systems can uphold racial hierarchies. However, persistent gaps in research on race and NLP remain: race has been siloed as a niche topic and remains ignored in many NLP tasks; most work operationalizes race as a fixed single-dimensional variable with a ground-truth label, which risks reinforcing differences produced by historical racism; and the voices of historically marginalized people are nearly absent in NLP literature. By identifying where and how NLP literature has and has not considered race, especially in comparison to related fields, our work calls for inclusion and racial justice in NLP research practices",
    "checked": true,
    "id": "353c88c231ce156d604e074af276422422fc73f7",
    "semantic_title": "",
    "citation_count": 56,
    "authors": [
      "Anjalie Field",
      "Su Lin Blodgett",
      "Zeerak Waseem",
      "Yulia Tsvetkov"
    ]
  },
  "https://aclanthology.org/2021.acl-long.150": {
    "title": "Intrinsic Bias Metrics Do Not Correlate with Application Bias",
    "volume": "long",
    "abstract": "Natural Language Processing (NLP) systems learn harmful societal biases that cause them to amplify inequality as they are deployed in more and more situations. To guide efforts at debiasing these systems, the NLP community relies on a variety of metrics that quantify bias in models. Some of these metrics are intrinsic, measuring bias in word embedding spaces, and some are extrinsic, measuring bias in downstream tasks that the word embeddings enable. Do these intrinsic and extrinsic metrics correlate with each other? We compare intrinsic and extrinsic metrics across hundreds of trained models covering different tasks and experimental conditions. Our results show no reliable correlation between these metrics that holds in all scenarios across tasks and languages. We urge researchers working on debiasing to focus on extrinsic measures of bias, and to make using these measures more feasible via creation of new challenge sets and annotated test data. To aid this effort, we release code, a new intrinsic metric, and an annotated test set focused on gender bias in hate speech",
    "checked": true,
    "id": "497d29459a894ac38a48ed58753976ccbf2aa433",
    "semantic_title": "",
    "citation_count": 76,
    "authors": [
      "Seraphina Goldfarb-Tarrant",
      "Rebecca Marchant",
      "Ricardo Muñoz Sánchez",
      "Mugdha Pandya",
      "Adam Lopez"
    ]
  },
  "https://aclanthology.org/2021.acl-long.151": {
    "title": "RedditBias: A Real-World Resource for Bias Evaluation and Debiasing of Conversational Language Models",
    "volume": "long",
    "abstract": "Text representation models are prone to exhibit a range of societal biases, reflecting the non-controlled and biased nature of the underlying pretraining data, which consequently leads to severe ethical issues and even bias amplification. Recent work has predominantly focused on measuring and mitigating bias in pretrained language models. Surprisingly, the landscape of bias measurements and mitigation resources and methods for conversational language models is still very scarce: it is limited to only a few types of bias, artificially constructed resources, and completely ignores the impact that debiasing methods may have on the final perfor mance in dialog tasks, e.g., conversational response generation. In this work, we present REDDITBIAS, the first conversational data set grounded in the actual human conversations from Reddit, allowing for bias measurement and mitigation across four important bias dimensions: gender,race,religion, and queerness. Further, we develop an evaluation framework which simultaneously 1)measures bias on the developed REDDITBIAS resource, and 2)evaluates model capability in dialog tasks after model debiasing. We use the evaluation framework to benchmark the widely used conversational DialoGPT model along with the adaptations of four debiasing methods. Our results indicate that DialoGPT is biased with respect to religious groups and that some debiasing techniques can remove this bias while preserving downstream task performance",
    "checked": true,
    "id": "2add974973ab45e46f1f8d3b932d24ba88cbb0b4",
    "semantic_title": "",
    "citation_count": 57,
    "authors": [
      "Soumya Barikeri",
      "Anne Lauscher",
      "Ivan Vulić",
      "Goran Glavaš"
    ]
  },
  "https://aclanthology.org/2021.acl-long.152": {
    "title": "Contributions of Transformer Attention Heads in Multi- and Cross-lingual Tasks",
    "volume": "long",
    "abstract": "This paper studies the relative importance of attention heads in Transformer-based models to aid their interpretability in cross-lingual and multi-lingual tasks. Prior research has found that only a few attention heads are important in each mono-lingual Natural Language Processing (NLP) task and pruning the remaining heads leads to comparable or improved performance of the model. However, the impact of pruning attention heads is not yet clear in cross-lingual and multi-lingual tasks. Through extensive experiments, we show that (1) pruning a number of attention heads in a multi-lingual Transformer-based model has, in general, positive effects on its performance in cross-lingual and multi-lingual tasks and (2) the attention heads to be pruned can be ranked using gradients and identified with a few trial experiments. Our experiments focus on sequence labeling tasks, with potential applicability on other cross-lingual and multi-lingual tasks. For comprehensiveness, we examine two pre-trained multi-lingual models, namely multi-lingual BERT (mBERT) and XLM-R, on three tasks across 9 languages each. We also discuss the validity of our findings and their extensibility to truly resource-scarce languages and other task settings",
    "checked": true,
    "id": "3e7972eb3468848c648941342da4535e5e5df0b4",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Weicheng Ma",
      "Kai Zhang",
      "Renze Lou",
      "Lili Wang",
      "Soroush Vosoughi"
    ]
  },
  "https://aclanthology.org/2021.acl-long.153": {
    "title": "Crafting Adversarial Examples for Neural Machine Translation",
    "volume": "long",
    "abstract": "Effective adversary generation for neural machine translation (NMT) is a crucial prerequisite for building robust machine translation systems. In this work, we investigate veritable evaluations of NMT adversarial attacks, and propose a novel method to craft NMT adversarial examples. We first show the current NMT adversarial attacks may be improperly estimated by the commonly used mono-directional translation, and we propose to leverage the round-trip translation technique to build valid metrics for evaluating NMT adversarial attacks. Our intuition is that an effective NMT adversarial example, which imposes minor shifting on the source and degrades the translation dramatically, would naturally lead to a semantic-destroyed round-trip translation result. We then propose a promising black-box attack method called Word Saliency speedup Local Search (WSLS) that could effectively attack the mainstream NMT architectures. Comprehensive experiments demonstrate that the proposed metrics could accurately evaluate the attack effectiveness, and the proposed WSLS could significantly break the state-of-art NMT models with small perturbation. Besides, WSLS exhibits strong transferability on attacking Baidu and Bing online translators",
    "checked": true,
    "id": "e0b753249b8bdd6768f9ece2186e669078071ff6",
    "semantic_title": "",
    "citation_count": 26,
    "authors": [
      "Xinze Zhang",
      "Junzhe Zhang",
      "Zhenhua Chen",
      "Kun He"
    ]
  },
  "https://aclanthology.org/2021.acl-long.154": {
    "title": "UXLA: A Robust Unsupervised Data Augmentation Framework for Zero-Resource Cross-Lingual NLP",
    "volume": "long",
    "abstract": "Transfer learning has yielded state-of-the-art (SoTA) results in many supervised NLP tasks. However, annotated data for every target task in every target language is rare, especially for low-resource languages. We propose UXLA, a novel unsupervised data augmentation framework for zero-resource transfer learning scenarios. In particular, UXLA aims to solve cross-lingual adaptation problems from a source language task distribution to an unknown target language task distribution, assuming no training label in the target language. At its core, UXLA performs simultaneous self-training with data augmentation and unsupervised sample selection. To show its effectiveness, we conduct extensive experiments on three diverse zero-resource cross-lingual transfer tasks. UXLA achieves SoTA results in all the tasks, outperforming the baselines by a good margin. With an in-depth framework dissection, we demonstrate the cumulative contributions of different components to its success",
    "checked": true,
    "id": "73930e122a7485414e7303cf2709e2b162a94339",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "M Saiful Bari",
      "Tasnim Mohiuddin",
      "Shafiq Joty"
    ]
  },
  "https://aclanthology.org/2021.acl-long.155": {
    "title": "Glancing Transformer for Non-Autoregressive Neural Machine Translation",
    "volume": "long",
    "abstract": "Recent work on non-autoregressive neural machine translation (NAT) aims at improving the efficiency by parallel decoding without sacrificing the quality. However, existing NAT methods are either inferior to Transformer or require multiple decoding passes, leading to reduced speedup. We propose the Glancing Language Model (GLM) for single-pass parallel generation models. With GLM, we develop Glancing Transformer (GLAT) for machine translation. With only single-pass parallel decoding, GLAT is able to generate high-quality translation with 8×-15× speedup. Note that GLAT does not modify the network architecture, which is a training method to learn word interdependency. Experiments on multiple WMT language directions show that GLAT outperforms all previous single pass non-autoregressive methods, and is nearly comparable to Transformer, reducing the gap to 0.25-0.9 BLEU points",
    "checked": true,
    "id": "33d05a1ff5f15bdc60fc43fa8523a4888af5f116",
    "semantic_title": "",
    "citation_count": 74,
    "authors": [
      "Lihua Qian",
      "Hao Zhou",
      "Yu Bao",
      "Mingxuan Wang",
      "Lin Qiu",
      "Weinan Zhang",
      "Yong Yu",
      "Lei Li"
    ]
  },
  "https://aclanthology.org/2021.acl-long.156": {
    "title": "Hierarchical Context-aware Network for Dense Video Event Captioning",
    "volume": "long",
    "abstract": "Dense video event captioning aims to generate a sequence of descriptive captions for each event in a long untrimmed video. Video-level context provides important information and facilities the model to generate consistent and less redundant captions between events. In this paper, we introduce a novel Hierarchical Context-aware Network for dense video event captioning (HCN) to capture context from various aspects. In detail, the model leverages local and global context with different mechanisms to jointly learn to generate coherent captions. The local context module performs full interaction between neighbor frames and the global context module selectively attends to previous or future events. According to our extensive experiment on both Youcook2 and Activitynet Captioning datasets, the video-level HCN model outperforms the event-level context-agnostic model by a large margin. The code is available at https://github.com/KirkGuo/HCN",
    "checked": true,
    "id": "4468fd0591f17e1cb9115804b6528213d4b5a4ee",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Lei Ji",
      "Xianglin Guo",
      "Haoyang Huang",
      "Xilin Chen"
    ]
  },
  "https://aclanthology.org/2021.acl-long.157": {
    "title": "Control Image Captioning Spatially and Temporally",
    "volume": "long",
    "abstract": "Generating image captions with user intention is an emerging need. The recently published Localized Narratives dataset takes mouse traces as another input to the image captioning task, which is an intuitive and efficient way for a user to control what to describe in the image. However, how to effectively employ traces to improve generation quality and controllability is still under exploration. This paper aims to solve this problem by proposing a novel model called LoopCAG, which connects Contrastive constraints and Attention Guidance in a Loop manner, engaged explicit spatial and temporal constraints to the generating process. Precisely, each generated sentence is temporally aligned to the corresponding trace sequence through a contrastive learning strategy. Besides, each generated text token is supervised to attend to the correct visual objects under heuristic spatial attention guidance. Comprehensive experimental results demonstrate that our LoopCAG model learns better correspondence among the three modalities (vision, language, and traces) and achieves SOTA performance on trace-controlled image captioning task. Moreover, the controllability and explainability of LoopCAG are validated by analyzing spatial and temporal sensitivity during the generation process",
    "checked": true,
    "id": "3f65c7277488ffb888f20b7b3246dfabc3b73de1",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Kun Yan",
      "Lei Ji",
      "Huaishao Luo",
      "Ming Zhou",
      "Nan Duan",
      "Shuai Ma"
    ]
  },
  "https://aclanthology.org/2021.acl-long.158": {
    "title": "Edited Media Understanding Frames: Reasoning About the Intent and Implications of Visual Misinformation",
    "volume": "long",
    "abstract": "Understanding manipulated media, from automatically generated ‘deepfakes’ to manually edited ones, raises novel research challenges. Because the vast majority of edited or manipulated images are benign, such as photoshopped images for visual enhancements, the key challenge is to understand the complex layers of underlying intents of media edits and their implications with respect to disinformation. In this paper, we study Edited Media Frames, a new formalism to understand visual media manipulation as structured annotations with respect to the intents, emotional reactions, attacks on individuals, and the overall implications of disinformation. We introduce a dataset for our task, EMU, with 56k question-answer pairs written in rich natural language. We evaluate a wide variety of vision-and-language models for our task, and introduce a new model PELICAN, which builds upon recent progress in pretrained multimodal representations. Our model obtains promising results on our dataset, with humans rating its answers as accurate 48.2% of the time. At the same time, there is still much work to be done – and we provide analysis that highlights areas for further progress",
    "checked": true,
    "id": "acfcb88fbd0ece7956cf5ad5eb0f8087311b5b3d",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Jeff Da",
      "Maxwell Forbes",
      "Rowan Zellers",
      "Anthony Zheng",
      "Jena D. Hwang",
      "Antoine Bosselut",
      "Yejin Choi"
    ]
  },
  "https://aclanthology.org/2021.acl-long.159": {
    "title": "PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World",
    "volume": "long",
    "abstract": "We propose PIGLeT: a model that learns physical commonsense knowledge through interaction, and then uses this knowledge to ground language. We factorize PIGLeT into a physical dynamics model, and a separate language model. Our dynamics model learns not just what objects are but also what they do: glass cups break when thrown, plastic ones don’t. We then use it as the interface to our language model, giving us a unified model of linguistic form and grounded meaning. PIGLeT can read a sentence, simulate neurally what might happen next, and then communicate that result through a literal symbolic representation, or natural language. Experimental results show that our model effectively learns world dynamics, along with how to communicate them. It is able to correctly forecast what happens next given an English sentence over 80% of the time, outperforming a 100x larger, text-to-text approach by over 10%. Likewise, its natural language summaries of physical interactions are also judged by humans as more accurate than LM alternatives. We present comprehensive analysis showing room for future work",
    "checked": true,
    "id": "32feca141fce06c6588b4014d27953a3fc25f19b",
    "semantic_title": "",
    "citation_count": 32,
    "authors": [
      "Rowan Zellers",
      "Ari Holtzman",
      "Matthew Peters",
      "Roozbeh Mottaghi",
      "Aniruddha Kembhavi",
      "Ali Farhadi",
      "Yejin Choi"
    ]
  },
  "https://aclanthology.org/2021.acl-long.160": {
    "title": "Modeling Fine-Grained Entity Types with Box Embeddings",
    "volume": "long",
    "abstract": "Neural entity typing models typically represent fine-grained entity types as vectors in a high-dimensional space, but such spaces are not well-suited to modeling these types’ complex interdependencies. We study the ability of box embeddings, which embed concepts as d-dimensional hyperrectangles, to capture hierarchies of types even when these relationships are not defined explicitly in the ontology. Our model represents both types and entity mentions as boxes. Each mention and its context are fed into a BERT-based model to embed that mention in our box space; essentially, this model leverages typological clues present in the surface text to hypothesize a type representation for the mention. Box containment can then be used to derive both the posterior probability of a mention exhibiting a given type and the conditional probability relations between types themselves. We compare our approach with a vector-based typing model and observe state-of-the-art performance on several entity typing benchmarks. In addition to competitive typing performance, our box-based model shows better performance in prediction consistency (predicting a supertype and a subtype together) and confidence (i.e., calibration), demonstrating that the box-based model captures the latent type hierarchies better than the vector-based model does",
    "checked": true,
    "id": "176e3cbe3141c8b874df663711dca9b7470b8243",
    "semantic_title": "",
    "citation_count": 40,
    "authors": [
      "Yasumasa Onoe",
      "Michael Boratko",
      "Andrew McCallum",
      "Greg Durrett"
    ]
  },
  "https://aclanthology.org/2021.acl-long.161": {
    "title": "ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin Information",
    "volume": "long",
    "abstract": "Recent pretraining models in Chinese neglect two important aspects specific to the Chinese language: glyph and pinyin, which carry significant syntax and semantic information for language understanding. In this work, we propose ChineseBERT, which incorporates both the glyph and pinyin information of Chinese characters into language model pretraining. The glyph embedding is obtained based on different fonts of a Chinese character, being able to capture character semantics from the visual features, and the pinyin embedding characterizes the pronunciation of Chinese characters, which handles the highly prevalent heteronym phenomenon in Chinese (the same character has different pronunciations with different meanings). Pretrained on large-scale unlabeled Chinese corpus, the proposed ChineseBERT model yields significant performance boost over baseline models with fewer training steps. The proposed model achieves new SOTA performances on a wide range of Chinese NLP tasks, including machine reading comprehension, natural language inference, text classification, sentence pair matching, and competitive performances in named entity recognition and word segmentation",
    "checked": true,
    "id": "654ea3c14dc3dd9dc2c1ff19bf4f9e71dda78d10",
    "semantic_title": "",
    "citation_count": 62,
    "authors": [
      "Zijun Sun",
      "Xiaoya Li",
      "Xiaofei Sun",
      "Yuxian Meng",
      "Xiang Ao",
      "Qing He",
      "Fei Wu",
      "Jiwei Li"
    ]
  },
  "https://aclanthology.org/2021.acl-long.162": {
    "title": "Weight Distillation: Transferring the Knowledge in Neural Network Parameters",
    "volume": "long",
    "abstract": "Knowledge distillation has been proven to be effective in model acceleration and compression. It transfers knowledge from a large neural network to a small one by using the large neural network predictions as targets of the small neural network. But this way ignores the knowledge inside the large neural networks, e.g., parameters. Our preliminary study as well as the recent success in pre-training suggests that transferring parameters are more effective in distilling knowledge. In this paper, we propose Weight Distillation to transfer the knowledge in parameters of a large neural network to a small neural network through a parameter generator. On the WMT16 En-Ro, NIST12 Zh-En, and WMT14 En-De machine translation tasks, our experiments show that weight distillation learns a small network that is 1.88 2.94x faster than the large network but with competitive BLEU performance. When fixing the size of small networks, weight distillation outperforms knowledge distillation by 0.51 1.82 BLEU points",
    "checked": true,
    "id": "c4184393982749b6ab743b5b234ea8aa206051d0",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Ye Lin",
      "Yanyang Li",
      "Ziyang Wang",
      "Bei Li",
      "Quan Du",
      "Tong Xiao",
      "Jingbo Zhu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.163": {
    "title": "Optimizing Deeper Transformers on Small Datasets",
    "volume": "long",
    "abstract": "It is a common belief that training deep transformers from scratch requires large datasets. Consequently, for small datasets, people usually use shallow and simple additional layers on top of pre-trained models during fine-tuning. This work shows that this does not always need to be the case: with proper initialization and optimization, the benefits of very deep transformers can carry over to challenging tasks with small datasets, including Text-to-SQL semantic parsing and logical reading comprehension. In particular, we successfully train 48 layers of transformers, comprising 24 fine-tuned layers from pre-trained RoBERTa and 24 relation-aware layers trained from scratch. With fewer training steps and no task-specific pre-training, we obtain the state of the art performance on the challenging cross-domain Text-to-SQL parsing benchmark Spider. We achieve this by deriving a novel Data dependent Transformer Fixed-update initialization scheme (DT-Fixup), inspired by the prior T-Fixup work. Further error analysis shows that increasing depth can help improve generalization on small datasets for hard cases that require reasoning and structural understanding",
    "checked": true,
    "id": "cd02e0a094953077217e2e62f3557b36a365acff",
    "semantic_title": "",
    "citation_count": 32,
    "authors": [
      "Peng Xu",
      "Dhruv Kumar",
      "Wei Yang",
      "Wenjie Zi",
      "Keyi Tang",
      "Chenyang Huang",
      "Jackie Chi Kit Cheung",
      "Simon J.D. Prince",
      "Yanshuai Cao"
    ]
  },
  "https://aclanthology.org/2021.acl-long.164": {
    "title": "BERTAC: Enhancing Transformer-based Language Models with Adversarially Pretrained Convolutional Neural Networks",
    "volume": "long",
    "abstract": "Transformer-based language models (TLMs), such as BERT, ALBERT and GPT-3, have shown strong performance in a wide range of NLP tasks and currently dominate the field of NLP. However, many researchers wonder whether these models can maintain their dominance forever. Of course, we do not have answers now, but, as an attempt to find better neural architectures and training schemes, we pretrain a simple CNN using a GAN-style learning scheme and Wikipedia data, and then integrate it with standard TLMs. We show that on the GLUE tasks, the combination of our pretrained CNN with ALBERT outperforms the original ALBERT and achieves a similar performance to that of SOTA. Furthermore, on open-domain QA (Quasar-T and SearchQA), the combination of the CNN with ALBERT or RoBERTa achieved stronger performance than SOTA and the original TLMs. We hope that this work provides a hint for developing a novel strong network architecture along with its training scheme. Our source code and models are available at https://github.com/nict-wisdom/bertac",
    "checked": true,
    "id": "0454472088f18f3109d062a5e2445779b44bc997",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jong-Hoon Oh",
      "Ryu Iida",
      "Julien Kloetzer",
      "Kentaro Torisawa"
    ]
  },
  "https://aclanthology.org/2021.acl-long.165": {
    "title": "COVID-Fact: Fact Extraction and Verification of Real-World Claims on COVID-19 Pandemic",
    "volume": "long",
    "abstract": "We introduce a FEVER-like dataset COVID-Fact of 4,086 claims concerning the COVID-19 pandemic. The dataset contains claims, evidence for the claims, and contradictory claims refuted by the evidence. Unlike previous approaches, we automatically detect true claims and their source articles and then generate counter-claims using automatic methods rather than employing human annotators. Along with our constructed resource, we formally present the task of identifying relevant evidence for the claims and verifying whether the evidence refutes or supports a given claim. In addition to scientific claims, our data contains simplified general claims from media sources, making it better suited for detecting general misinformation regarding COVID-19. Our experiments indicate that COVID-Fact will provide a challenging testbed for the development of new systems and our approach will reduce the costs of building domain-specific datasets for detecting misinformation",
    "checked": true,
    "id": "c530bef97ee809c01ce59d04a7011d445fb1e147",
    "semantic_title": "",
    "citation_count": 51,
    "authors": [
      "Arkadiy Saakyan",
      "Tuhin Chakrabarty",
      "Smaranda Muresan"
    ]
  },
  "https://aclanthology.org/2021.acl-long.166": {
    "title": "Explaining Relationships Between Scientific Documents",
    "volume": "long",
    "abstract": "We address the task of explaining relationships between two scientific documents using natural language text. This task requires modeling the complex content of long technical documents, deducing a relationship between these documents, and expressing the details of that relationship in text. In addition to the theoretical interest of this task, successful solutions can help improve researcher efficiency in search and review. In this paper we establish a dataset of 622K examples from 154K documents. We pretrain a large language model to serve as the foundation for autoregressive approaches to the task. We explore the impact of taking different views on the two documents, including the use of dense representations extracted with scientific IE systems. We provide extensive automatic and human evaluations which show the promise of such models, but make clear challenges for future work",
    "checked": true,
    "id": "70139335657559df6f0de540f9a0bd4f9f0d8bac",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Kelvin Luu",
      "Xinyi Wu",
      "Rik Koncel-Kedziorski",
      "Kyle Lo",
      "Isabel Cachola",
      "Noah A. Smith"
    ]
  },
  "https://aclanthology.org/2021.acl-long.167": {
    "title": "IrEne: Interpretable Energy Prediction for Transformers",
    "volume": "long",
    "abstract": "Existing software-based energy measurements of NLP models are not accurate because they do not consider the complex interactions between energy consumption and model execution. We present IrEne, an interpretable and extensible energy prediction system that accurately predicts the inference energy consumption of a wide range of Transformer-based NLP models. IrEne constructs a model tree graph that breaks down the NLP model into modules that are further broken down into low-level machine learning (ML) primitives. IrEne predicts the inference energy consumption of the ML primitives as a function of generalizable features and fine-grained runtime resource usage. IrEne then aggregates these low-level predictions recursively to predict the energy of each module and finally of the entire model. Experiments across multiple Transformer models show IrEne predicts inference energy consumption of transformer models with an error of under 7% compared to the ground truth. In contrast, existing energy models see an error of over 50%. We also show how IrEne can be used to conduct energy bottleneck analysis and to easily evaluate the energy impact of different architectural choices. We release the code and data at https://github.com/StonyBrookNLP/irene",
    "checked": true,
    "id": "d1e5ac96faf9165ea60bf593c0da2a1f55e390fd",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Qingqing Cao",
      "Yash Kumar Lal",
      "Harsh Trivedi",
      "Aruna Balasubramanian",
      "Niranjan Balasubramanian"
    ]
  },
  "https://aclanthology.org/2021.acl-long.168": {
    "title": "Mitigating Bias in Session-based Cyberbullying Detection: A Non-Compromising Approach",
    "volume": "long",
    "abstract": "The element of repetition in cyberbullying behavior has directed recent computational studies toward detecting cyberbullying based on a social media session. In contrast to a single text, a session may consist of an initial post and an associated sequence of comments. Yet, emerging efforts to enhance the performance of session-based cyberbullying detection have largely overlooked unintended social biases in existing cyberbullying datasets. For example, a session containing certain demographic-identity terms (e.g., “gay” or “black”) is more likely to be classified as an instance of cyberbullying. In this paper, we first show evidence of such bias in models trained on sessions collected from different social media platforms (e.g., Instagram). We then propose a context-aware and model-agnostic debiasing strategy that leverages a reinforcement learning technique, without requiring any extra resources or annotations apart from a pre-defined set of sensitive triggers commonly used for identifying cyberbullying instances. Empirical evaluations show that the proposed strategy can simultaneously alleviate the impacts of the unintended biases and improve the detection performance",
    "checked": true,
    "id": "2f93db693b63381316b16a4fd176310621833b06",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Lu Cheng",
      "Ahmadreza Mosallanezhad",
      "Yasin Silva",
      "Deborah Hall",
      "Huan Liu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.169": {
    "title": "PlotCoder: Hierarchical Decoding for Synthesizing Visualization Code in Programmatic Context",
    "volume": "long",
    "abstract": "Creating effective visualization is an important part of data analytics. While there are many libraries for creating visualization, writing such code remains difficult given the myriad of parameters that users need to provide. In this paper, we propose the new task of synthesizing visualization programs from a combination of natural language utterances and code context. To tackle the learning problem, we introduce PlotCoder, a new hierarchical encoder-decoder architecture that models both the code context and the input utterance. We use PlotCoder to first determine the template of the visualization code, followed by predicting the data to be plotted. We use Jupyter notebooks containing visualization programs crawled from GitHub to train PlotCoder. On a comprehensive set of test samples from those notebooks, we show that PlotCoder correctly predicts the plot type of about 70% samples, and synthesizes the correct programs for 35% samples, performing 3-4.5% better than the baselines",
    "checked": true,
    "id": "ed2a639dd4a6b3abdcc0040733a264cb1dbcd7d4",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Xinyun Chen",
      "Linyuan Gong",
      "Alvin Cheung",
      "Dawn Song"
    ]
  },
  "https://aclanthology.org/2021.acl-long.170": {
    "title": "Changing the World by Changing the Data",
    "volume": "long",
    "abstract": "NLP community is currently investing a lot more research and resources into development of deep learning models than training data. While we have made a lot of progress, it is now clear that our models learn all kinds of spurious patterns, social biases, and annotation artifacts. Algorithmic solutions have so far had limited success. An alternative that is being actively discussed is more careful design of datasets so as to deliver specific signals. This position paper maps out the arguments for and against data curation, and argues that fundamentally the point is moot: curation already is and will be happening, and it is changing the world. The question is only how much thought we want to invest into that process",
    "checked": true,
    "id": "a309ad4c4088843d230be1a85806960e633e1e46",
    "semantic_title": "",
    "citation_count": 44,
    "authors": [
      "Anna Rogers"
    ]
  },
  "https://aclanthology.org/2021.acl-long.171": {
    "title": "EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets",
    "volume": "long",
    "abstract": "Heavily overparameterized language models such as BERT, XLNet and T5 have achieved impressive success in many NLP tasks. However, their high model complexity requires enormous computation resources and extremely long training time for both pre-training and fine-tuning. Many works have studied model compression on large NLP models, but only focusing on reducing inference time while still requiring an expensive training process. Other works use extremely large batch sizes to shorten the pre-training time, at the expense of higher computational resource demands. In this paper, inspired by the Early-Bird Lottery Tickets recently studied for computer vision tasks, we propose EarlyBERT, a general computationally-efficient training algorithm applicable to both pre-training and fine-tuning of large-scale language models. By slimming the self-attention and fully-connected sub-layers inside a transformer, we are the first to identify structured winning tickets in the early stage of BERT training. We apply those tickets towards efficient BERT training, and conduct comprehensive pre-training and fine-tuning experiments on GLUE and SQuAD downstream tasks. Our results show that EarlyBERT achieves comparable performance to standard BERT, with 35 45% less training time. Code is available at https://github.com/VITA-Group/EarlyBERT",
    "checked": true,
    "id": "0c9d97d2ba489256d4f1760598dc2c7be6d90d96",
    "semantic_title": "",
    "citation_count": 56,
    "authors": [
      "Xiaohan Chen",
      "Yu Cheng",
      "Shuohang Wang",
      "Zhe Gan",
      "Zhangyang Wang",
      "Jingjing Liu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.172": {
    "title": "On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation",
    "volume": "long",
    "abstract": "Adapter-based tuning has recently arisen as an alternative to fine-tuning. It works by adding light-weight adapter modules to a pretrained language model (PrLM) and only updating the parameters of adapter modules when learning on a downstream task. As such, it adds only a few trainable parameters per new task, allowing a high degree of parameter sharing. Prior studies have shown that adapter-based tuning often achieves comparable results to fine-tuning. However, existing work only focuses on the parameter-efficient aspect of adapter-based tuning while lacking further investigation on its effectiveness. In this paper, we study the latter. We first show that adapter-based tuning better mitigates forgetting issues than fine-tuning since it yields representations with less deviation from those generated by the initial PrLM. We then empirically compare the two tuning methods on several downstream NLP tasks and settings. We demonstrate that 1) adapter-based tuning outperforms fine-tuning on low-resource and cross-lingual tasks; 2) it is more robust to overfitting and less sensitive to changes in learning rates",
    "checked": true,
    "id": "448af0627240e46df757e7b9c640ee30507c18e9",
    "semantic_title": "",
    "citation_count": 71,
    "authors": [
      "Ruidan He",
      "Linlin Liu",
      "Hai Ye",
      "Qingyu Tan",
      "Bosheng Ding",
      "Liying Cheng",
      "Jiawei Low",
      "Lidong Bing",
      "Luo Si"
    ]
  },
  "https://aclanthology.org/2021.acl-long.173": {
    "title": "Data Augmentation for Text Generation Without Any Augmented Data",
    "volume": "long",
    "abstract": "Data augmentation is an effective way to improve the performance of many neural text generation models. However, current data augmentation methods need to define or choose proper data mapping functions that map the original samples into the augmented samples. In this work, we derive an objective to formulate the problem of data augmentation on text generation tasks without any use of augmented data constructed by specific mapping functions. Our proposed objective can be efficiently optimized and applied to popular loss functions on text generation tasks with a convergence rate guarantee. Experiments on five datasets of two text generation tasks show that our approach can approximate or even surpass popular data augmentation methods",
    "checked": true,
    "id": "54f491e9b622a8025ed44f2ead6a13eef387a968",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Wei Bi",
      "Huayang Li",
      "Jiacheng Huang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.174": {
    "title": "Integrating Semantics and Neighborhood Information with Graph-Driven Generative Models for Document Retrieval",
    "volume": "long",
    "abstract": "With the need of fast retrieval speed and small memory footprint, document hashing has been playing a crucial role in large-scale information retrieval. To generate high-quality hashing code, both semantics and neighborhood information are crucial. However, most existing methods leverage only one of them or simply combine them via some intuitive criteria, lacking a theoretical principle to guide the integration process. In this paper, we encode the neighborhood information with a graph-induced Gaussian distribution, and propose to integrate the two types of information with a graph-driven generative model. To deal with the complicated correlations among documents, we further propose a tree-structured approximation method for learning. Under the approximation, we prove that the training objective can be decomposed into terms involving only singleton or pairwise documents, enabling the model to be trained as efficiently as uncorrelated ones. Extensive experimental results on three benchmark datasets show that our method achieves superior performance over state-of-the-art methods, demonstrating the effectiveness of the proposed model for simultaneously preserving semantic and neighborhood information",
    "checked": true,
    "id": "e3199e4e164aec3c9d204e7a882e3aa6aa962069",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Zijing Ou",
      "Qinliang Su",
      "Jianxing Yu",
      "Bang Liu",
      "Jingwen Wang",
      "Ruihui Zhao",
      "Changyou Chen",
      "Yefeng Zheng"
    ]
  },
  "https://aclanthology.org/2021.acl-long.175": {
    "title": "SMURF: SeMantic and linguistic UndeRstanding Fusion for Caption Evaluation via Typicality Analysis",
    "volume": "long",
    "abstract": "The open-ended nature of visual captioning makes it a challenging area for evaluation. The majority of proposed models rely on specialized training to improve human-correlation, resulting in limited adoption, generalizability, and explainabilty. We introduce “typicality”, a new formulation of evaluation rooted in information theory, which is uniquely suited for problems lacking a definite ground truth. Typicality serves as our framework to develop a novel semantic comparison, SPARCS, as well as referenceless fluency evaluation metrics. Over the course of our analysis, two separate dimensions of fluency naturally emerge: style, captured by metric SPURTS, and grammar, captured in the form of grammatical outlier penalties. Through extensive experiments and ablation studies on benchmark datasets, we show how these decomposed dimensions of semantics and fluency provide greater system-level insight into captioner differences. Our proposed metrics along with their combination, SMURF, achieve state-of-the-art correlation with human judgment when compared with other rule-based evaluation metrics",
    "checked": true,
    "id": "81c774bf206d518ffbefafc9acfe670ffe2d1377",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Joshua Feinglass",
      "Yezhou Yang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.176": {
    "title": "KaggleDBQA: Realistic Evaluation of Text-to-SQL Parsers",
    "volume": "long",
    "abstract": "The goal of database question answering is to enable natural language querying of real-life relational databases in diverse application domains. Recently, large-scale datasets such as Spider and WikiSQL facilitated novel modeling techniques for text-to-SQL parsing, improving zero-shot generalization to unseen databases. In this work, we examine the challenges that still prevent these techniques from practical deployment. First, we present KaggleDBQA, a new cross-domain evaluation dataset of real Web databases, with domain-specific data types, original formatting, and unrestricted questions. Second, we re-examine the choice of evaluation tasks for text-to-SQL parsers as applied in real-life settings. Finally, we augment our in-domain evaluation task with database documentation, a naturally occurring source of implicit domain knowledge. We show that KaggleDBQA presents a challenge to state-of-the-art zero-shot parsers but a more realistic evaluation setting and creative use of associated database documentation boosts their accuracy by over 13.2%, doubling their performance",
    "checked": true,
    "id": "9cd3d6eef7c574830be410598c3024191ee974d4",
    "semantic_title": "",
    "citation_count": 21,
    "authors": [
      "Chia-Hsuan Lee",
      "Oleksandr Polozov",
      "Matthew Richardson"
    ]
  },
  "https://aclanthology.org/2021.acl-long.177": {
    "title": "QASR: QCRI Aljazeera Speech Resource A Large Scale Annotated Arabic Speech Corpus",
    "volume": "long",
    "abstract": "We introduce the largest transcribed Arabic speech corpus, QASR, collected from the broadcast domain. This multi-dialect speech dataset contains 2,000 hours of speech sampled at 16kHz crawled from Aljazeera news channel. The dataset is released with lightly supervised transcriptions, aligned with the audio segments. Unlike previous datasets, QASR contains linguistically motivated segmentation, punctuation, speaker information among others. QASR is suitable for training and evaluating speech recognition systems, acoustics- and/or linguistics- based Arabic dialect identification, punctuation restoration, speaker identification, speaker linking, and potentially other NLP modules for spoken data. In addition to QASR transcription, we release a dataset of 130M words to aid in designing and training a better language model. We show that end-to-end automatic speech recognition trained on QASR reports a competitive word error rate compared to the previous MGB-2 corpus. We report baseline results for downstream natural language processing tasks such as named entity recognition using speech transcript. We also report the first baseline for Arabic punctuation restoration. We make the corpus available for the research community",
    "checked": true,
    "id": "6e2a57b46a96694e6dfd3c65796c877dd0094f3d",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Hamdy Mubarak",
      "Amir Hussein",
      "Shammur Absar Chowdhury",
      "Ahmed Ali"
    ]
  },
  "https://aclanthology.org/2021.acl-long.178": {
    "title": "An Empirical Study on Hyperparameter Optimization for Fine-Tuning Pre-trained Language Models",
    "volume": "long",
    "abstract": "The performance of fine-tuning pre-trained language models largely depends on the hyperparameter configuration. In this paper, we investigate the performance of modern hyperparameter optimization methods (HPO) on fine-tuning pre-trained language models. First, we study and report three HPO algorithms’ performances on fine-tuning two state-of-the-art language models on the GLUE dataset. We find that using the same time budget, HPO often fails to outperform grid search due to two reasons: insufficient time budget and overfitting. We propose two general strategies and an experimental procedure to systematically troubleshoot HPO’s failure cases. By applying the procedure, we observe that HPO can succeed with more appropriate settings in the search space and time budget; however, in certain cases overfitting remains. Finally, we make suggestions for future work. Our implementation can be found in https://github.com/microsoft/FLAML/tree/main/flaml/nlp/",
    "checked": true,
    "id": "3a9efbd662326ec9338d4eaa32543f77a0834974",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Xueqing Liu",
      "Chi Wang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.179": {
    "title": "Better than Average: Paired Evaluation of NLP systems",
    "volume": "long",
    "abstract": "Evaluation in NLP is usually done by comparing the scores of competing systems independently averaged over a common set of test instances. In this work, we question the use of averages for aggregating evaluation scores into a final number used to decide which system is best, since the average, as well as alternatives such as the median, ignores the pairing arising from the fact that systems are evaluated on the same test instances. We illustrate the importance of taking the instancelevel pairing of evaluation scores into account and demonstrate, both theoretically and empirically, the advantages of aggregation methods based on pairwise comparisons, such as the Bradley–Terry (BT) model, a mechanism based on the estimated probability that a given system scores better than another on the test set. By re-evaluating 296 real NLP evaluation setups across four tasks and 18 evaluation metrics, we show that the choice of aggregation mechanism matters and yields different conclusions as to which systems are state of the art in about 30% of the setups. To facilitate the adoption of pairwise evaluation, we release a practical tool for performing the full analysis of evaluation scores with the mean, median, BT, and two variants of BT (Elo and TrueSkill), alongside functionality for appropriate statistical testing",
    "checked": true,
    "id": "df37ba004c3bec70c9ed6f614944338d6ec6ec68",
    "semantic_title": "",
    "citation_count": 18,
    "authors": [
      "Maxime Peyrard",
      "Wei Zhao",
      "Steffen Eger",
      "Robert West"
    ]
  },
  "https://aclanthology.org/2021.acl-long.180": {
    "title": "Chase: A Large-Scale and Pragmatic Chinese Dataset for Cross-Database Context-Dependent Text-to-SQL",
    "volume": "long",
    "abstract": "The cross-database context-dependent Text-to-SQL (XDTS) problem has attracted considerable attention in recent years due to its wide range of potential applications. However, we identify two biases in existing datasets for XDTS: (1) a high proportion of context-independent questions and (2) a high proportion of easy SQL queries. These biases conceal the major challenges in XDTS to some extent. In this work, we present Chase, a large-scale and pragmatic Chinese dataset for XDTS. It consists of 5,459 coherent question sequences (17,940 questions with their SQL queries annotated) over 280 databases, in which only 35% of questions are context-independent, and 28% of SQL queries are easy. We experiment on Chase with three state-of-the-art XDTS approaches. The best approach only achieves an exact match accuracy of 40% over all questions and 16% over all question sequences, indicating that Chase highlights the challenging problems of XDTS. We believe that XDTS can provide fertile soil for addressing the problems",
    "checked": true,
    "id": "d4d26ccbf1e64e725b5bffc08ab28a72e271facb",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Jiaqi Guo",
      "Ziliang Si",
      "Yu Wang",
      "Qian Liu",
      "Ming Fan",
      "Jian-Guang Lou",
      "Zijiang Yang",
      "Ting Liu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.181": {
    "title": "CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding",
    "volume": "long",
    "abstract": "Despite pre-trained language models have proven useful for learning high-quality semantic representations, these models are still vulnerable to simple perturbations. Recent works aimed to improve the robustness of pre-trained models mainly focus on adversarial training from perturbed examples with similar semantics, neglecting the utilization of different or even opposite semantics. Different from the image processing field, the text is discrete and few word substitutions can cause significant semantic changes. To study the impact of semantics caused by small perturbations, we conduct a series of pilot experiments and surprisingly find that adversarial training is useless or even harmful for the model to detect these semantic changes. To address this problem, we propose Contrastive Learning with semantIc Negative Examples (CLINE), which constructs semantic negative examples unsupervised to improve the robustness under semantically adversarial attacking. By comparing with similar and opposite semantic examples, the model can effectively perceive the semantic changes caused by small perturbations. Empirical results show that our approach yields substantial improvements on a range of sentiment analysis, reasoning, and reading comprehension tasks. And CLINE also ensures the compactness within the same semantics and separability across different semantics in sentence-level",
    "checked": true,
    "id": "8b954e1654c6b759a957fd11e66c111e6105fb3f",
    "semantic_title": "",
    "citation_count": 61,
    "authors": [
      "Dong Wang",
      "Ning Ding",
      "Piji Li",
      "Haitao Zheng"
    ]
  },
  "https://aclanthology.org/2021.acl-long.182": {
    "title": "Tree-Structured Topic Modeling with Nonparametric Neural Variational Inference",
    "volume": "long",
    "abstract": "Topic modeling has been widely used for discovering the latent semantic structure of documents, but most existing methods learn topics with a flat structure. Although probabilistic models can generate topic hierarchies by introducing nonparametric priors like Chinese restaurant process, such methods have data scalability issues. In this study, we develop a tree-structured topic model by leveraging nonparametric neural variational inference. Particularly, the latent components of the stick-breaking process are first learned for each document, then the affiliations of latent components are modeled by the dependency matrices between network layers. Utilizing this network structure, we can efficiently extract a tree-structured topic hierarchy with reasonable structure, low redundancy, and adaptable widths. Experiments on real-world datasets validate the effectiveness of our method",
    "checked": true,
    "id": "dd0079122a90cf1c6325fc1347fe027d85abf905",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Ziye Chen",
      "Cheng Ding",
      "Zusheng Zhang",
      "Yanghui Rao",
      "Haoran Xie"
    ]
  },
  "https://aclanthology.org/2021.acl-long.183": {
    "title": "ExCAR: Event Graph Knowledge Enhanced Explainable Causal Reasoning",
    "volume": "long",
    "abstract": "Prior work infers the causation between events mainly based on the knowledge induced from the annotated causal event pairs. However, additional evidence information intermediate to the cause and effect remains unexploited. By incorporating such information, the logical law behind the causality can be unveiled, and the interpretability and stability of the causal reasoning system can be improved. To facilitate this, we present an Event graph knowledge enhanced explainable CAusal Reasoning framework (ExCAR). ExCAR first acquires additional evidence information from a large-scale causal event graph as logical rules for causal reasoning. To learn the conditional probabilistic of logical rules, we propose the Conditional Markov Neural Logic Network (CMNLN) that combines the representation learning and structure learning of logical rules in an end-to-end differentiable manner. Experimental results demonstrate that ExCAR outperforms previous state-of-the-art methods. Adversarial evaluation shows the improved stability of ExCAR over baseline systems. Human evaluation shows that ExCAR can achieve a promising explainable performance",
    "checked": true,
    "id": "cab8ddc42cefbedc7ef8579888c2475e9609a5d7",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Li Du",
      "Xiao Ding",
      "Kai Xiong",
      "Ting Liu",
      "Bing Qin"
    ]
  },
  "https://aclanthology.org/2021.acl-long.184": {
    "title": "Distributed Representations of Emotion Categories in Emotion Space",
    "volume": "long",
    "abstract": "Emotion category is usually divided into different ones by human beings, but it is indeed difficult to clearly distinguish and define the boundaries between different emotion categories. The existing studies working on emotion detection usually focus on how to improve the performance of model prediction, in which emotions are represented with one-hot vectors. However, emotion relations are ignored in one-hot representations. In this article, we first propose a general framework to learn the distributed representations for emotion categories in emotion space from a given emotion classification dataset. Furthermore, based on the soft labels predicted by the pre-trained neural network model, we derive a simple and effective algorithm. Experiments have validated that the proposed representations in emotion space can express emotion relations much better than word vectors in semantic space",
    "checked": true,
    "id": "fe3e8ba3eebaad59f2c7c7c27b5287b405638e34",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Xiangyu Wang",
      "Chengqing Zong"
    ]
  },
  "https://aclanthology.org/2021.acl-long.185": {
    "title": "Style is NOT a single variable: Case Studies for Cross-Stylistic Language Understanding",
    "volume": "long",
    "abstract": "Every natural text is written in some style. Style is formed by a complex combination of different stylistic factors, including formality markers, emotions, metaphors, etc. One cannot form a complete understanding of a text without considering these factors. The factors combine and co-vary in complex ways to form styles. Studying the nature of the covarying combinations sheds light on stylistic language in general, sometimes called cross-style language understanding. This paper provides the benchmark corpus (XSLUE) that combines existing datasets and collects a new one for sentence-level cross-style language understanding and evaluation. The benchmark contains text in 15 different styles under the proposed four theoretical groupings: figurative, personal, affective, and interpersonal groups. For valid evaluation, we collect an additional diagnostic set by annotating all 15 styles on the same text. Using XSLUE, we propose three interesting cross-style applications in classification, correlation, and generation. First, our proposed cross-style classifier trained with multiple styles together helps improve overall classification performance against individually-trained style classifiers. Second, our study shows that some styles are highly dependent on each other in human-written text. Finally, we find that combinations of some contradictive styles likely generate stylistically less appropriate text. We believe our benchmark and case studies help explore interesting future directions for cross-style research. The preprocessed datasets and code are publicly available",
    "checked": true,
    "id": "8c5b844ff2397925356306f23c8f570dd07a49fc",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Dongyeop Kang",
      "Eduard Hovy"
    ]
  },
  "https://aclanthology.org/2021.acl-long.186": {
    "title": "DynaSent: A Dynamic Benchmark for Sentiment Analysis",
    "volume": "long",
    "abstract": "We introduce DynaSent (‘Dynamic Sentiment’), a new English-language benchmark task for ternary (positive/negative/neutral) sentiment analysis. DynaSent combines naturally occurring sentences with sentences created using the open-source Dynabench Platform, which facilities human-and-model-in-the-loop dataset creation. DynaSent has a total of 121,634 sentences, each validated by five crowdworkers, and its development and test splits are designed to produce chance performance for even the best models we have been able to develop; when future models solve this task, we will use them to create DynaSent version 2, continuing the dynamic evolution of this benchmark. Here, we report on the dataset creation effort, focusing on the steps we took to increase quality and reduce artifacts. We also present evidence that DynaSent’s Neutral category is more coherent than the comparable category in other benchmarks, and we motivate training models from scratch for each round over successive fine-tuning",
    "checked": true,
    "id": "284dfcf7f25ca87b2db235c6cdc848b4143d3923",
    "semantic_title": "",
    "citation_count": 44,
    "authors": [
      "Christopher Potts",
      "Zhengxuan Wu",
      "Atticus Geiger",
      "Douwe Kiela"
    ]
  },
  "https://aclanthology.org/2021.acl-long.187": {
    "title": "A Hierarchical VAE for Calibrating Attributes while Generating Text using Normalizing Flow",
    "volume": "long",
    "abstract": "In this digital age, online users expect personalized content. To cater to diverse group of audiences across online platforms it is necessary to generate multiple variants of same content with differing degree of characteristics (sentiment, style, formality, etc.). Though text-style transfer is a well explored related area, it focuses on flipping the style attribute polarity instead of regulating a fine-grained attribute transfer. In this paper we propose a hierarchical architecture for finer control over the at- tribute, preserving content using attribute dis- entanglement. We demonstrate the effective- ness of the generative process for two different attributes with varied complexity, namely sentiment and formality. With extensive experiments and human evaluation on five real-world datasets, we show that the framework can generate natural looking sentences with finer degree of control of intensity of a given attribute",
    "checked": true,
    "id": "3b9ddd1445edd74f69fa0a5c1b3d92b1f56f2c31",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Bidisha Samanta",
      "Mohit Agrawal",
      "NIloy Ganguly"
    ]
  },
  "https://aclanthology.org/2021.acl-long.188": {
    "title": "A Unified Generative Framework for Aspect-based Sentiment Analysis",
    "volume": "long",
    "abstract": "Aspect-based Sentiment Analysis (ABSA) aims to identify the aspect terms, their corresponding sentiment polarities, and the opinion terms. There exist seven subtasks in ABSA. Most studies only focus on the subsets of these subtasks, which leads to various complicated ABSA models while hard to solve these subtasks in a unified framework. In this paper, we redefine every subtask target as a sequence mixed by pointer indexes and sentiment class indexes, which converts all ABSA subtasks into a unified generative formulation. Based on the unified formulation, we exploit the pre-training sequence-to-sequence model BART to solve all ABSA subtasks in an end-to-end framework. Extensive experiments on four ABSA datasets for seven subtasks demonstrate that our framework achieves substantial performance gain and provides a real unified end-to-end solution for the whole ABSA subtasks, which could benefit multiple tasks",
    "checked": true,
    "id": "968820dfae97f80b63674fd76cd7d82332498708",
    "semantic_title": "",
    "citation_count": 89,
    "authors": [
      "Hang Yan",
      "Junqi Dai",
      "Tuo Ji",
      "Xipeng Qiu",
      "Zheng Zhang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.189": {
    "title": "Discovering Dialogue Slots with Weak Supervision",
    "volume": "long",
    "abstract": "Task-oriented dialogue systems typically require manual annotation of dialogue slots in training data, which is costly to obtain. We propose a method that eliminates this requirement: We use weak supervision from existing linguistic annotation models to identify potential slot candidates, then automatically identify domain-relevant slots by using clustering algorithms. Furthermore, we use the resulting slot annotation to train a neural-network-based tagger that is able to perform slot tagging with no human intervention. This tagger is trained solely on the outputs of our method and thus does not rely on any labeled data. Our model demonstrates state-of-the-art performance in slot tagging without labeled training data on four different dialogue domains. Moreover, we find that slot annotations discovered by our model significantly improve the performance of an end-to-end dialogue response generation model, compared to using no slot annotation at all",
    "checked": true,
    "id": "417d36b9f37eaf7ced15c6f067f7c5b2501d7472",
    "semantic_title": "",
    "citation_count": 17,
    "authors": [
      "Vojtěch Hudeček",
      "Ondřej Dušek",
      "Zhou Yu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.190": {
    "title": "Enhancing the generalization for Intent Classification and Out-of-Domain Detection in SLU",
    "volume": "long",
    "abstract": "Intent classification is a major task in spoken language understanding (SLU). Since most models are built with pre-collected in-domain (IND) training utterances, their ability to detect unsupported out-of-domain (OOD) utterances has a critical effect in practical use. Recent works have shown that using extra data and labels can improve the OOD detection performance, yet it could be costly to collect such data. This paper proposes to train a model with only IND data while supporting both IND intent classification and OOD detection. Our method designs a novel domain-regularized module (DRM) to reduce the overconfident phenomenon of a vanilla classifier, achieving a better generalization in both cases. Besides, DRM can be used as a drop-in replacement for the last layer in any neural network-based intent classifier, providing a low-cost strategy for a significant improvement. The evaluation on four datasets shows that our method built on BERT and RoBERTa models achieves state-of-the-art performance against existing approaches and the strong baselines we created for the comparisons",
    "checked": true,
    "id": "0390383a49ec1feb7921d6f61f80b6988b3598f2",
    "semantic_title": "",
    "citation_count": 18,
    "authors": [
      "Yilin Shen",
      "Yen-Chang Hsu",
      "Avik Ray",
      "Hongxia Jin"
    ]
  },
  "https://aclanthology.org/2021.acl-long.191": {
    "title": "PROTAUGMENT: Unsupervised diverse short-texts paraphrasing for intent detection meta-learning",
    "volume": "long",
    "abstract": "Recent research considers few-shot intent detection as a meta-learning problem: the model is learning to learn from a consecutive set of small tasks named episodes. In this work, we propose ProtAugment, a meta-learning algorithm for short texts classification (the intent detection task). ProtAugment is a novel extension of Prototypical Networks, that limits overfitting on the bias introduced by the few-shots classification objective at each episode. It relies on diverse paraphrasing: a conditional language model is first fine-tuned for paraphrasing, and diversity is later introduced at the decoding stage at each meta-learning episode. The diverse paraphrasing is unsupervised as it is applied to unlabelled data, and then fueled to the Prototypical Network training objective as a consistency loss. ProtAugment is the state-of-the-art method for intent detection meta-learning, at no extra labeling efforts and without the need to fine-tune a conditional language model on a given application domain",
    "checked": true,
    "id": "67a21672f9e606b99795f1d799a847bad42ba102",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Thomas Dopierre",
      "Christophe Gravier",
      "Wilfried Logerais"
    ]
  },
  "https://aclanthology.org/2021.acl-long.192": {
    "title": "Robustness Testing of Language Understanding in Task-Oriented Dialog",
    "volume": "long",
    "abstract": "Most language understanding models in task-oriented dialog systems are trained on a small amount of annotated training data, and evaluated in a small set from the same distribution. However, these models can lead to system failure or undesirable output when being exposed to natural language perturbation or variation in practice. In this paper, we conduct comprehensive evaluation and analysis with respect to the robustness of natural language understanding models, and introduce three important aspects related to language understanding in real-world dialog systems, namely, language variety, speech characteristics, and noise perturbation. We propose a model-agnostic toolkit LAUG to approximate natural language perturbations for testing the robustness issues in task-oriented dialog. Four data augmentation approaches covering the three aspects are assembled in LAUG, which reveals critical robustness issues in state-of-the-art models. The augmented dataset through LAUG can be used to facilitate future research on the robustness testing of language understanding in task-oriented dialog",
    "checked": true,
    "id": "ecde9d77e3a7be50f02cf202a4789d772dc05299",
    "semantic_title": "",
    "citation_count": 26,
    "authors": [
      "Jiexi Liu",
      "Ryuichi Takanobu",
      "Jiaxin Wen",
      "Dazhen Wan",
      "Hongguang Li",
      "Weiran Nie",
      "Cheng Li",
      "Wei Peng",
      "Minlie Huang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.193": {
    "title": "Comprehensive Study: How the Context Information of Different Granularity Affects Dialogue State Tracking?",
    "volume": "long",
    "abstract": "Dialogue state tracking (DST) plays a key role in task-oriented dialogue systems to monitor the user’s goal. In general, there are two strategies to track a dialogue state: predicting it from scratch and updating it from previous state. The scratch-based strategy obtains each slot value by inquiring all the dialogue history, and the previous-based strategy relies on the current turn dialogue to update the previous dialogue state. However, it is hard for the scratch-based strategy to correctly track short-dependency dialogue state because of noise; meanwhile, the previous-based strategy is not very useful for long-dependency dialogue state tracking. Obviously, it plays different roles for the context information of different granularity to track different kinds of dialogue states. Thus, in this paper, we will study and discuss how the context information of different granularity affects dialogue state tracking. First, we explore how greatly different granularities affect dialogue state tracking. Then, we further discuss how to combine multiple granularities for dialogue state tracking. Finally, we apply the findings about context granularity to few-shot learning scenario. Besides, we have publicly released all codes",
    "checked": true,
    "id": "e6ed7af2957f8c98de48ef41cfec6a9de49a5fca",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Puhai Yang",
      "Heyan Huang",
      "Xian-Ling Mao"
    ]
  },
  "https://aclanthology.org/2021.acl-long.194": {
    "title": "OTTers: One-turn Topic Transitions for Open-Domain Dialogue",
    "volume": "long",
    "abstract": "Mixed initiative in open-domain dialogue requires a system to pro-actively introduce new topics. The one-turn topic transition task explores how a system connects two topics in a cooperative and coherent manner. The goal of the task is to generate a “bridging” utterance connecting the new topic to the topic of the previous conversation turn. We are especially interested in commonsense explanations of how a new topic relates to what has been mentioned before. We first collect a new dataset of human one-turn topic transitions, which we callOTTers. We then explore different strategies used by humans when asked to complete such a task, and notice that the use of a bridging utterance to connect the two topics is the approach used the most. We finally show how existing state-of-the-art text generation models can be adapted to this task and examine the performance of these baselines on different splits of the OTTers data",
    "checked": true,
    "id": "61be0c4cf65855fc5d79ea0e3996108464bf1aec",
    "semantic_title": "",
    "citation_count": 17,
    "authors": [
      "Karin Sevegnani",
      "David M. Howcroft",
      "Ioannis Konstas",
      "Verena Rieser"
    ]
  },
  "https://aclanthology.org/2021.acl-long.195": {
    "title": "Towards Robustness of Text-to-SQL Models against Synonym Substitution",
    "volume": "long",
    "abstract": "Recently, there has been significant progress in studying neural networks to translate text descriptions into SQL queries. Despite achieving good performance on some public benchmarks, existing text-to-SQL models typically rely on the lexical matching between words in natural language (NL) questions and tokens in table schemas, which may render the models vulnerable to attacks that break the schema linking mechanism. In this work, we investigate the robustness of text-to-SQL models to synonym substitution. In particular, we introduce Spider-Syn, a human-curated dataset based on the Spider benchmark for text-to-SQL translation. NL questions in Spider-Syn are modified from Spider, by replacing their schema-related words with manually selected synonyms that reflect real-world question paraphrases. We observe that the accuracy dramatically drops by eliminating such explicit correspondence between NL questions and table schemas, even if the synonyms are not adversarially selected to conduct worst-case attacks. Finally, we present two categories of approaches to improve the model robustness. The first category of approaches utilizes additional synonym annotations for table schemas by modifying the model input, while the second category is based on adversarial training. We demonstrate that both categories of approaches significantly outperform their counterparts without the defense, and the first category of approaches are more effective",
    "checked": true,
    "id": "84b26030b648b6d79177bdafd3e896b1dda9f91e",
    "semantic_title": "",
    "citation_count": 32,
    "authors": [
      "Yujian Gan",
      "Xinyun Chen",
      "Qiuping Huang",
      "Matthew Purver",
      "John R. Woodward",
      "Jinxia Xie",
      "Pengsheng Huang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.196": {
    "title": "KACE: Generating Knowledge Aware Contrastive Explanations for Natural Language Inference",
    "volume": "long",
    "abstract": "In order to better understand the reason behind model behaviors (i.e., making predictions), most recent works have exploited generative models to provide complementary explanations. However, existing approaches in NLP mainly focus on “WHY A” rather than contrastive “WHY A NOT B”, which is shown to be able to better distinguish confusing candidates and improve data efficiency in other research fields.In this paper, we focus on generating contrastive explanations with counterfactual examples in NLI and propose a novel Knowledge-Aware Contrastive Explanation generation framework (KACE).Specifically, we first identify rationales (i.e., key phrases) from input sentences, and use them as key perturbations for generating counterfactual examples. After obtaining qualified counterfactual examples, we take them along with original examples and external knowledge as input, and employ a knowledge-aware generative pre-trained language model to generate contrastive explanations. Experimental results show that contrastive explanations are beneficial to fit the scenarios by clarifying the difference between the predicted answer and other possible wrong ones. Moreover, we train an NLI model enhanced with contrastive explanations and achieves an accuracy of 91.9% on SNLI, gaining improvements of 5.7% against ETPA (“Explain-Then-Predict-Attention”) and 0.6% against NILE (“WHY A”)",
    "checked": true,
    "id": "62452e2bd6f6aebb88baa53890a0edb29f297cb1",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Qianglong Chen",
      "Feng Ji",
      "Xiangji Zeng",
      "Feng-Lin Li",
      "Ji Zhang",
      "Haiqing Chen",
      "Yin Zhang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.197": {
    "title": "Self-Guided Contrastive Learning for BERT Sentence Representations",
    "volume": "long",
    "abstract": "Although BERT and its variants have reshaped the NLP landscape, it still remains unclear how best to derive sentence embeddings from such pre-trained Transformers. In this work, we propose a contrastive learning method that utilizes self-guidance for improving the quality of BERT sentence representations. Our method fine-tunes BERT in a self-supervised fashion, does not rely on data augmentation, and enables the usual [CLS] token embeddings to function as sentence vectors. Moreover, we redesign the contrastive learning objective (NT-Xent) and apply it to sentence representation learning. We demonstrate with extensive experiments that our approach is more effective than competitive baselines on diverse sentence-related tasks. We also show it is efficient at inference and robust to domain shifts",
    "checked": true,
    "id": "795c5774ee1d1efe5c4aa4ff7465374d0e6b3bda",
    "semantic_title": "",
    "citation_count": 96,
    "authors": [
      "Taeuk Kim",
      "Kang Min Yoo",
      "Sang-goo Lee"
    ]
  },
  "https://aclanthology.org/2021.acl-long.198": {
    "title": "LGESQL: Line Graph Enhanced Text-to-SQL Model with Mixed Local and Non-Local Relations",
    "volume": "long",
    "abstract": "This work aims to tackle the challenging heterogeneous graph encoding problem in the text-to-SQL task. Previous methods are typically node-centric and merely utilize different weight matrices to parameterize edge types, which 1) ignore the rich semantics embedded in the topological structure of edges, and 2) fail to distinguish local and non-local relations for each node. To this end, we propose a Line Graph Enhanced Text-to-SQL (LGESQL) model to mine the underlying relational features without constructing meta-paths. By virtue of the line graph, messages propagate more efficiently through not only connections between nodes, but also the topology of directed edges. Furthermore, both local and non-local relations are integrated distinctively during the graph iteration. We also design an auxiliary task called graph pruning to improve the discriminative capability of the encoder. Our framework achieves state-of-the-art results (62.8% with Glove, 72.0% with Electra) on the cross-domain text-to-SQL benchmark Spider at the time of writing",
    "checked": true,
    "id": "50db74aa7e662b640ccbf37788af62cd8af3e930",
    "semantic_title": "",
    "citation_count": 63,
    "authors": [
      "Ruisheng Cao",
      "Lu Chen",
      "Zhi Chen",
      "Yanbin Zhao",
      "Su Zhu",
      "Kai Yu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.199": {
    "title": "Multi-stage Pre-training over Simplified Multimodal Pre-training Models",
    "volume": "long",
    "abstract": "Multimodal pre-training models, such as LXMERT, have achieved excellent results in downstream tasks. However, current pre-trained models require large amounts of training data and have huge model sizes, which make them impossible to apply in low-resource situations. How to obtain similar or even better performance than a larger model under the premise of less pre-training data and smaller model size has become an important problem. In this paper, we propose a new Multi-stage Pre-training (MSP) method, which uses information at different granularities from word, phrase to sentence in both texts and images to pre-train a model in stages. We also design several different pre-training tasks suitable for the information granularity in different stage in order to efficiently capture the diverse knowledge from a limited corpus. We take a Simplified LXMERT (LXMERT-S) which is with 45.9% parameters of the original LXMERT model and only 11.44% of the original pre-training data as the testbed of our MSP method. Experimental results show that our method achieves comparable performance to the original LXMERT model in all downstream tasks, and even outperforms the original model in Image-Text Retrieval task",
    "checked": true,
    "id": "5c585330f0e75121204092bbca3515cf31803b37",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Tongtong Liu",
      "Fangxiang Feng",
      "Xiaojie Wang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.200": {
    "title": "Beyond Sentence-Level End-to-End Speech Translation: Context Helps",
    "volume": "long",
    "abstract": "Document-level contextual information has shown benefits to text-based machine translation, but whether and how context helps end-to-end (E2E) speech translation (ST) is still under-studied. We fill this gap through extensive experiments using a simple concatenation-based context-aware ST model, paired with adaptive feature selection on speech encodings for computational efficiency. We investigate several decoding approaches, and introduce in-model ensemble decoding which jointly performs document- and sentence-level translation using the same model. Our results on the MuST-C benchmark with Transformer demonstrate the effectiveness of context to E2E ST. Compared to sentence-level ST, context-aware ST obtains better translation quality (+0.18-2.61 BLEU), improves pronoun and homophone translation, shows better robustness to (artificial) audio segmentation errors, and reduces latency and flicker to deliver higher quality for simultaneous translation",
    "checked": true,
    "id": "b60e273e336f31f6d061a58d71d5cef7766e8b6f",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Biao Zhang",
      "Ivan Titov",
      "Barry Haddow",
      "Rico Sennrich"
    ]
  },
  "https://aclanthology.org/2021.acl-long.201": {
    "title": "LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding",
    "volume": "long",
    "abstract": "Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to its effective model architecture and the advantage of large-scale unlabeled scanned/digital-born documents. We propose LayoutLMv2 architecture with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework. Specifically, with a two-stream multi-modal Transformer encoder, LayoutLMv2 uses not only the existing masked visual-language modeling task but also the new text-image alignment and text-image matching tasks, which make it better capture the cross-modality interaction in the pre-training stage. Meanwhile, it also integrates a spatial-aware self-attention mechanism into the Transformer architecture so that the model can fully understand the relative positional relationship among different text blocks. Experiment results show that LayoutLMv2 outperforms LayoutLM by a large margin and achieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks, including FUNSD (0.7895 to 0.8420), CORD (0.9493 to 0.9601), SROIE (0.9524 to 0.9781), Kleister-NDA (0.8340 to 0.8520), RVL-CDIP (0.9443 to 0.9564), and DocVQA (0.7295 to 0.8672)",
    "checked": true,
    "id": "0197abda042e6de87b5f716caa708a6a459f078c",
    "semantic_title": "",
    "citation_count": 199,
    "authors": [
      "Yang Xu",
      "Yiheng Xu",
      "Tengchao Lv",
      "Lei Cui",
      "Furu Wei",
      "Guoxin Wang",
      "Yijuan Lu",
      "Dinei Florencio",
      "Cha Zhang",
      "Wanxiang Che",
      "Min Zhang",
      "Lidong Zhou"
    ]
  },
  "https://aclanthology.org/2021.acl-long.202": {
    "title": "UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning",
    "volume": "long",
    "abstract": "Existed pre-training methods either focus on single-modal tasks or multi-modal tasks, and cannot effectively adapt to each other. They can only utilize single-modal data (i.e., text or image) or limited multi-modal data (i.e., image-text pairs). In this work, we propose a UNIfied-MOdal pre-training architecture, namely UNIMO, which can effectively adapt to both single-modal and multi-modal understanding and generation tasks. Large scale of free text corpus and image collections are utilized to improve the capability of visual and textual understanding, and cross-modal contrastive learning (CMCL) is leveraged to align the textual and visual information into a unified semantic space, over a corpus of image-text pairs augmented with related images and texts. With the help of rich non-paired single-modal data, our model is able to learn more generalizable representations, by allowing textual knowledge and visual knowledge to enhance each other in the unified semantic space. The experimental results show that UNIMO greatly improves the performance of several single-modal and multi-modal downstream tasks. Our code and pre-trained models are public at https://github.com/PaddlePaddle/Research/tree/master/NLP/UNIMO",
    "checked": true,
    "id": "5e5fbc41106db9acaaf3a365801051e477f0e984",
    "semantic_title": "",
    "citation_count": 211,
    "authors": [
      "Wei Li",
      "Can Gao",
      "Guocheng Niu",
      "Xinyan Xiao",
      "Hao Liu",
      "Jiachen Liu",
      "Hua Wu",
      "Haifeng Wang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.203": {
    "title": "Missing Modality Imagination Network for Emotion Recognition with Uncertain Missing Modalities",
    "volume": "long",
    "abstract": "Multimodal fusion has been proved to improve emotion recognition performance in previous works. However, in real-world applications, we often encounter the problem of missing modality, and which modalities will be missing is uncertain. It makes the fixed multimodal fusion fail in such cases. In this work, we propose a unified model, Missing Modality Imagination Network (MMIN), to deal with the uncertain missing modality problem. MMIN learns robust joint multimodal representations, which can predict the representation of any missing modality given available modalities under different missing modality conditions.Comprehensive experiments on two benchmark datasets demonstrate that the unified MMIN model significantly improves emotion recognition performance under both uncertain missing-modality testing conditions and full-modality ideal testing condition. The code will be available at https://github.com/AIM3-RUC/MMIN",
    "checked": true,
    "id": "33a6bbb2b36da34b503b7fa79b6e084a45d992cf",
    "semantic_title": "",
    "citation_count": 21,
    "authors": [
      "Jinming Zhao",
      "Ruichen Li",
      "Qin Jin"
    ]
  },
  "https://aclanthology.org/2021.acl-long.204": {
    "title": "Stacked Acoustic-and-Textual Encoding: Integrating the Pre-trained Models into Speech Translation Encoders",
    "volume": "long",
    "abstract": "Encoder pre-training is promising in end-to-end Speech Translation (ST), given the fact that speech-to-translation data is scarce. But ST encoders are not simple instances of Automatic Speech Recognition (ASR) or Machine Translation (MT) encoders. For example, we find that ASR encoders lack the global context representation, which is necessary for translation, whereas MT encoders are not designed to deal with long but locally attentive acoustic sequences. In this work, we propose a Stacked Acoustic-and-Textual Encoding (SATE) method for speech translation. Our encoder begins with processing the acoustic sequence as usual, but later behaves more like an MT encoder for a global representation of the input sequence. In this way, it is straightforward to incorporate the pre-trained models into the system. Also, we develop an adaptor module to alleviate the representation inconsistency between the pre-trained ASR encoder and MT encoder, and develop a multi-teacher knowledge distillation method to preserve the pre-training knowledge. Experimental results on the LibriSpeech En-Fr and MuST-C En-De ST tasks show that our method achieves state-of-the-art BLEU scores of 18.3 and 25.2. To our knowledge, we are the first to develop an end-to-end ST system that achieves comparable or even better BLEU performance than the cascaded ST counterpart when large-scale ASR and MT data is available",
    "checked": true,
    "id": "2f0aa51f0062061c7db1accdf9dd2be5bccfdd26",
    "semantic_title": "",
    "citation_count": 32,
    "authors": [
      "Chen Xu",
      "Bojie Hu",
      "Yanyang Li",
      "Yuhao Zhang",
      "Shen Huang",
      "Qi Ju",
      "Tong Xiao",
      "Jingbo Zhu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.205": {
    "title": "N-ary Constituent Tree Parsing with Recursive Semi-Markov Model",
    "volume": "long",
    "abstract": "In this paper, we study the task of graph-based constituent parsing in the setting that binarization is not conducted as a pre-processing step, where a constituent tree may consist of nodes with more than two children. Previous graph-based methods on this setting typically generate hidden nodes with the dummy label inside the n-ary nodes, in order to transform the tree into a binary tree for prediction. The limitation is that the hidden nodes break the sibling relations of the n-ary node’s children. Consequently, the dependencies of such sibling constituents might not be accurately modeled and is being ignored. To solve this limitation, we propose a novel graph-based framework, which is called “recursive semi-Markov model”. The main idea is to utilize 1-order semi-Markov model to predict the immediate children sequence of a constituent candidate, which then recursively serves as a child candidate of its parent. In this manner, the dependencies of sibling constituents can be described by 1-order transition features, which solves the above limitation. Through experiments, the proposed framework obtains the F1 of 95.92% and 92.50% on the datasets of PTB and CTB 5.1 respectively. Specially, the recursive semi-Markov model shows advantages in modeling nodes with more than two children, whose average F1 can be improved by 0.3-1.1 points in PTB and 2.3-6.8 points in CTB 5.1",
    "checked": true,
    "id": "627792fa7367e884995f688dad4d3b0c10f7d345",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Xin Xin",
      "Jinlong Li",
      "Zeqi Tan"
    ]
  },
  "https://aclanthology.org/2021.acl-long.206": {
    "title": "Automated Concatenation of Embeddings for Structured Prediction",
    "volume": "long",
    "abstract": "Pretrained contextualized embeddings are powerful word representations for structured prediction tasks. Recent work found that better word representations can be obtained by concatenating different types of embeddings. However, the selection of embeddings to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings, and the ever-increasing number of embedding types makes it a more difficult problem. In this paper, we propose Automated Concatenation of Embeddings (ACE) to automate the process of finding better concatenations of embeddings for structured prediction tasks, based on a formulation inspired by recent progress on neural architecture search. Specifically, a controller alternately samples a concatenation of embeddings, according to its current belief of the effectiveness of individual embedding types in consideration for a task, and updates the belief based on a reward. We follow strategies in reinforcement learning to optimize the parameters of the controller and compute the reward based on the accuracy of a task model, which is fed with the sampled concatenation as input and trained on a task dataset. Empirical results on 6 tasks and 21 datasets show that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in all the evaluations",
    "checked": true,
    "id": "dd0f388c57ede2aa86b8a40fee018a288c81b821",
    "semantic_title": "",
    "citation_count": 80,
    "authors": [
      "Xinyu Wang",
      "Yong Jiang",
      "Nguyen Bach",
      "Tao Wang",
      "Zhongqiang Huang",
      "Fei Huang",
      "Kewei Tu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.207": {
    "title": "Multi-View Cross-Lingual Structured Prediction with Minimum Supervision",
    "volume": "long",
    "abstract": "In structured prediction problems, cross-lingual transfer learning is an efficient way to train quality models for low-resource languages, and further improvement can be obtained by learning from multiple source languages. However, not all source models are created equal and some may hurt performance on the target language. Previous work has explored the similarity between source and target sentences as an approximate measure of strength for different source models. In this paper, we propose a multi-view framework, by leveraging a small number of labeled target sentences, to effectively combine multiple source models into an aggregated source view at different granularity levels (language, sentence, or sub-structure), and transfer it to a target view based on a task-specific model. By encouraging the two views to interact with each other, our framework can dynamically adjust the confidence level of each source model and improve the performance of both views during training. Experiments for three structured prediction tasks on sixteen data sets show that our framework achieves significant improvement over all existing approaches, including these with access to additional source language data",
    "checked": true,
    "id": "f97c1d9ab0f4a8941c2aec319fbfc89a23b70422",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Zechuan Hu",
      "Yong Jiang",
      "Nguyen Bach",
      "Tao Wang",
      "Zhongqiang Huang",
      "Fei Huang",
      "Kewei Tu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.208": {
    "title": "The Limitations of Limited Context for Constituency Parsing",
    "volume": "long",
    "abstract": "Incorporating syntax into neural approaches in NLP has a multitude of practical and scientific benefits. For instance, a language model that is syntax-aware is likely to be able to produce better samples; even a discriminative model like BERT with a syntax module could be used for core NLP tasks like unsupervised syntactic parsing. Rapid progress in recent years was arguably spurred on by the empirical success of the Parsing-Reading-Predict architecture of (Shen et al., 2018a), later simplified by the Order Neuron LSTM of (Shen et al., 2019). Most notably, this is the first time neural approaches were able to successfully perform unsupervised syntactic parsing (evaluated by various metrics like F-1 score). However, even heuristic (much less fully mathematical) understanding of why and when these architectures work is lagging severely behind. In this work, we answer representational questions raised by the architectures in (Shen et al., 2018a, 2019), as well as some transition-based syntax-aware language models (Dyer et al., 2016): what kind of syntactic structure can current neural approaches to syntax represent? Concretely, we ground this question in the sandbox of probabilistic context-free-grammars (PCFGs), and identify a key aspect of the representational power of these approaches: the amount and directionality of context that the predictor has access to when forced to make parsing decision. We show that with limited context (either bounded, or unidirectional), there are PCFGs, for which these approaches cannot represent the max-likelihood parse; conversely, if the context is unlimited, they can represent the max-likelihood parse of any PCFG",
    "checked": true,
    "id": "0c47b149f1e0cce163e3bb5eb4135cfec5f6d16b",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Yuchen Li",
      "Andrej Risteski"
    ]
  },
  "https://aclanthology.org/2021.acl-long.209": {
    "title": "Neural Bi-Lexicalized PCFG Induction",
    "volume": "long",
    "abstract": "Neural lexicalized PCFGs (L-PCFGs) have been shown effective in grammar induction. However, to reduce computational complexity, they make a strong independence assumption on the generation of the child word and thus bilexical dependencies are ignored. In this paper, we propose an approach to parameterize L-PCFGs without making implausible independence assumptions. Our approach directly models bilexical dependencies and meanwhile reduces both learning and representation complexities of L-PCFGs. Experimental results on the English WSJ dataset confirm the effectiveness of our approach in improving both running speed and unsupervised parsing performance",
    "checked": true,
    "id": "1e2ed0b70d5f3cbf4a1166eb3982eb7a0ae5ce89",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Songlin Yang",
      "Yanpeng Zhao",
      "Kewei Tu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.210": {
    "title": "Ruddit: Norms of Offensiveness for English Reddit Comments",
    "volume": "long",
    "abstract": "On social media platforms, hateful and offensive language negatively impact the mental well-being of users and the participation of people from diverse backgrounds. Automatic methods to detect offensive language have largely relied on datasets with categorical labels. However, comments can vary in their degree of offensiveness. We create the first dataset of English language Reddit comments that has fine-grained, real-valued scores between -1 (maximally supportive) and 1 (maximally offensive). The dataset was annotated using Best–Worst Scaling, a form of comparative annotation that has been shown to alleviate known biases of using rating scales. We show that the method produces highly reliable offensiveness scores. Finally, we evaluate the ability of widely-used neural models to predict offensiveness scores on this new dataset",
    "checked": true,
    "id": "ad8f0170bde61a8c1ca56507bf624f9392bb59b7",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Rishav Hada",
      "Sohi Sudhir",
      "Pushkar Mishra",
      "Helen Yannakoudakis",
      "Saif M. Mohammad",
      "Ekaterina Shutova"
    ]
  },
  "https://aclanthology.org/2021.acl-long.211": {
    "title": "Towards Quantifiable Dialogue Coherence Evaluation",
    "volume": "long",
    "abstract": "Automatic dialogue coherence evaluation has attracted increasing attention and is crucial for developing promising dialogue systems. However, existing metrics have two major limitations: (a) they are mostly trained in a simplified two-level setting (coherent vs. incoherent), while humans give Likert-type multi-level coherence scores, dubbed as “quantifiable”; (b) their predicted coherence scores cannot align with the actual human rating standards due to the absence of human guidance during training. To address these limitations, we propose Quantifiable Dialogue Coherence Evaluation (QuantiDCE), a novel framework aiming to train a quantifiable dialogue coherence metric that can reflect the actual human rating standards. Specifically, QuantiDCE includes two training stages, Multi-Level Ranking (MLR) pre-training and Knowledge Distillation (KD) fine-tuning. During MLR pre-training, a new MLR loss is proposed for enabling the model to learn the coarse judgement of coherence degrees. Then, during KD fine-tuning, the pretrained model is further finetuned to learn the actual human rating standards with only very few human-annotated data. To advocate the generalizability even with limited fine-tuning data, a novel KD regularization is introduced to retain the knowledge learned at the pre-training stage. Experimental results show that the model trained by QuantiDCE presents stronger correlations with human judgements than the other state-of-the-art metrics",
    "checked": true,
    "id": "c8419114a8972e4945052e9699b69dfa858ab17c",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Zheng Ye",
      "Liucun Lu",
      "Lishan Huang",
      "Liang Lin",
      "Xiaodan Liang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.212": {
    "title": "Assessing the Representations of Idiomaticity in Vector Models with a Noun Compound Dataset Labeled at Type and Token Levels",
    "volume": "long",
    "abstract": "Accurate assessment of the ability of embedding models to capture idiomaticity may require evaluation at token rather than type level, to account for degrees of idiomaticity and possible ambiguity between literal and idiomatic usages. However, most existing resources with annotation of idiomaticity include ratings only at type level. This paper presents the Noun Compound Type and Token Idiomaticity (NCTTI) dataset, with human annotations for 280 noun compounds in English and 180 in Portuguese at both type and token level. We compiled 8,725 and 5,091 token level annotations for English and Portuguese, respectively, which are strongly correlated with the corresponding scores obtained at type level. The NCTTI dataset is used to explore how vector space models reflect the variability of idiomaticity across sentences. Several experiments using state-of-the-art contextualised models suggest that their representations are not capturing the noun compounds idiomaticity as human annotators. This new multilingual resource also contains suggestions for paraphrases of the noun compounds both at type and token levels, with uses for lexical substitution or disambiguation in context",
    "checked": true,
    "id": "b460b6c2591b7f0a5a2629d7007be074ee31a78f",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Marcos Garcia",
      "Tiago Kramer Vieira",
      "Carolina Scarton",
      "Marco Idiart",
      "Aline Villavicencio"
    ]
  },
  "https://aclanthology.org/2021.acl-long.213": {
    "title": "Factoring Statutory Reasoning as Language Understanding Challenges",
    "volume": "long",
    "abstract": "Statutory reasoning is the task of determining whether a legal statute, stated in natural language, applies to the text description of a case. Prior work introduced a resource that approached statutory reasoning as a monolithic textual entailment problem, with neural baselines performing nearly at-chance. To address this challenge, we decompose statutory reasoning into four types of language-understanding challenge problems, through the introduction of concepts and structure found in Prolog programs. Augmenting an existing benchmark, we provide annotations for the four tasks, and baselines for three of them. Models for statutory reasoning are shown to benefit from the additional structure, improving on prior baselines. Further, the decomposition into subtasks facilitates finer-grained model diagnostics and clearer incremental progress",
    "checked": true,
    "id": "a792a19c6364f9279e9dfaf5fae5fdffeed027fc",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Nils Holzenberger",
      "Benjamin Van Durme"
    ]
  },
  "https://aclanthology.org/2021.acl-long.214": {
    "title": "Evaluating Evaluation Measures for Ordinal Classification and Ordinal Quantification",
    "volume": "long",
    "abstract": "Ordinal Classification (OC) is an important classification task where the classes are ordinal. For example, an OC task for sentiment analysis could have the following classes: highly positive, positive, neutral, negative, highly negative. Clearly, evaluation measures for an OC task should penalise misclassifications by considering the ordinal nature of the classes. Ordinal Quantification (OQ) is a related task where the gold data is a distribution over ordinal classes, and the system is required to estimate this distribution. Evaluation measures for an OQ task should also take the ordinal nature of the classes into account. However, for both OC and OQ, there are only a small number of known evaluation measures that meet this basic requirement. In the present study, we utilise data from the SemEval and NTCIR communities to clarify the properties of nine evaluation measures in the context of OC tasks, and six measures in the context of OQ tasks",
    "checked": true,
    "id": "87383ce37910067a7f01f827b5313b69774505a8",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Tetsuya Sakai"
    ]
  },
  "https://aclanthology.org/2021.acl-long.215": {
    "title": "Interpretable and Low-Resource Entity Matching via Decoupling Feature Learning from Decision Making",
    "volume": "long",
    "abstract": "Entity Matching (EM) aims at recognizing entity records that denote the same real-world object. Neural EM models learn vector representation of entity descriptions and match entities end-to-end. Though robust, these methods require many annotated resources for training, and lack of interpretability. In this paper, we propose a novel EM framework that consists of Heterogeneous Information Fusion (HIF) and Key Attribute Tree (KAT) Induction to decouple feature representation from matching decision. Using self-supervised learning and mask mechanism in pre-trained language modeling, HIF learns the embeddings of noisy attribute values by inter-attribute attention with unlabeled data. Using a set of comparison features and a limited amount of annotated data, KAT Induction learns an efficient decision tree that can be interpreted by generating entity matching rules whose structure is advocated by domain experts. Experiments on 6 public datasets and 3 industrial datasets show that our method is highly efficient and outperforms SOTA EM models in most cases. We will release the codes upon acceptance",
    "checked": true,
    "id": "635bc85e4b9877614e5116193bd14b384629752f",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Zijun Yao",
      "Chengjiang Li",
      "Tiansi Dong",
      "Xin Lv",
      "Jifan Yu",
      "Lei Hou",
      "Juanzi Li",
      "Yichi Zhang",
      "Zelin Dai"
    ]
  },
  "https://aclanthology.org/2021.acl-long.216": {
    "title": "Locate and Label: A Two-stage Identifier for Nested Named Entity Recognition",
    "volume": "long",
    "abstract": "Named entity recognition (NER) is a well-studied task in natural language processing. Traditional NER research only deals with flat entities and ignores nested entities. The span-based methods treat entity recognition as a span classification task. Although these methods have the innate ability to handle nested NER, they suffer from high computational cost, ignorance of boundary information, under-utilization of the spans that partially match with entities, and difficulties in long entity recognition. To tackle these issues, we propose a two-stage entity identifier. First we generate span proposals by filtering and boundary regression on the seed spans to locate the entities, and then label the boundary-adjusted span proposals with the corresponding categories. Our method effectively utilizes the boundary information of entities and partially matched spans during training. Through boundary regression, entities of any length can be covered theoretically, which improves the ability to recognize long entities. In addition, many low-quality seed spans are filtered out in the first stage, which reduces the time complexity of inference. Experiments on nested NER datasets demonstrate that our proposed method outperforms previous state-of-the-art models",
    "checked": true,
    "id": "0ab855d5a81fdcb33a884465df6598570a1d0a21",
    "semantic_title": "",
    "citation_count": 66,
    "authors": [
      "Yongliang Shen",
      "Xinyin Ma",
      "Zeqi Tan",
      "Shuai Zhang",
      "Wen Wang",
      "Weiming Lu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.217": {
    "title": "Text2Event: Controllable Sequence-to-Structure Generation for End-to-end Event Extraction",
    "volume": "long",
    "abstract": "Event extraction is challenging due to the complex structure of event records and the semantic gap between text and event. Traditional methods usually extract event records by decomposing the complex structure prediction task into multiple subtasks. In this paper, we propose Text2Event, a sequence-to-structure generation paradigm that can directly extract events from the text in an end-to-end manner. Specifically, we design a sequence-to-structure network for unified event extraction, a constrained decoding algorithm for event knowledge injection during inference, and a curriculum learning algorithm for efficient model learning. Experimental results show that, by uniformly modeling all tasks in a single model and universally predicting different labels, our method can achieve competitive performance using only record-level annotations in both supervised learning and transfer learning settings",
    "checked": true,
    "id": "a0c78661e4a5278c65cefc5b4fdef669dcae7e17",
    "semantic_title": "",
    "citation_count": 110,
    "authors": [
      "Yaojie Lu",
      "Hongyu Lin",
      "Jin Xu",
      "Xianpei Han",
      "Jialong Tang",
      "Annan Li",
      "Le Sun",
      "Meng Liao",
      "Shaoyi Chen"
    ]
  },
  "https://aclanthology.org/2021.acl-long.218": {
    "title": "A Large-Scale Chinese Multimodal NER Dataset with Speech Clues",
    "volume": "long",
    "abstract": "In this paper, we aim to explore an uncharted territory, which is Chinese multimodal named entity recognition (NER) with both textual and acoustic contents. To achieve this, we construct a large-scale human-annotated Chinese multimodal NER dataset, named CNERTA. Our corpus totally contains 42,987 annotated sentences accompanying by 71 hours of speech data. Based on this dataset, we propose a family of strong and representative baseline models, which can leverage textual features or multimodal features. Upon these baselines, to capture the natural monotonic alignment between the textual modality and the acoustic modality, we further propose a simple multimodal multitask model by introducing a speech-to-text alignment auxiliary task. Through extensive experiments, we observe that: (1) Progressive performance boosts as we move from unimodal to multimodal, verifying the necessity of integrating speech clues into Chinese NER. (2) Our proposed model yields state-of-the-art (SoTA) results on CNERTA, demonstrating its effectiveness. For further research, the annotated dataset is publicly available at http://github.com/DianboWork/CNERTA",
    "checked": true,
    "id": "dbd76b51c70135858cf27e24c01dd5b48fc160bf",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Dianbo Sui",
      "Zhengkun Tian",
      "Yubo Chen",
      "Kang Liu",
      "Jun Zhao"
    ]
  },
  "https://aclanthology.org/2021.acl-long.219": {
    "title": "A Neural Transition-based Joint Model for Disease Named Entity Recognition and Normalization",
    "volume": "long",
    "abstract": "Disease is one of the fundamental entities in biomedical research. Recognizing such entities from biomedical text and then normalizing them to a standardized disease vocabulary offer a tremendous opportunity for many downstream applications. Previous studies have demonstrated that joint modeling of the two sub-tasks has superior performance than the pipelined counterpart. Although the neural joint model based on multi-task learning framework has achieved state-of-the-art performance, it suffers from the boundary inconsistency problem due to the separate decoding procedures. Moreover, it ignores the rich information (e.g., the text surface form) of each candidate concept in the vocabulary, which is quite essential for entity normalization. In this work, we propose a neural transition-based joint model to alleviate these two issues. We transform the end-to-end disease recognition and normalization task as an action sequence prediction task, which not only jointly learns the model with shared representations of the input, but also jointly searches the output by state transitions in one search space. Moreover, we introduce attention mechanisms to take advantage of the text surface form of each candidate concept for better normalization performance. Experimental results conducted on two publicly available datasets show the effectiveness of the proposed method",
    "checked": true,
    "id": "29c1f6620bae04b12b59ec8dff38c9f0a5826936",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Zongcheng Ji",
      "Tian Xia",
      "Mei Han",
      "Jing Xiao"
    ]
  },
  "https://aclanthology.org/2021.acl-long.220": {
    "title": "OntoED: Low-resource Event Detection with Ontology Embedding",
    "volume": "long",
    "abstract": "Event Detection (ED) aims to identify event trigger words from a given text and classify it into an event type. Most current methods to ED rely heavily on training instances, and almost ignore the correlation of event types. Hence, they tend to suffer from data scarcity and fail to handle new unseen event types. To address these problems, we formulate ED as a process of event ontology population: linking event instances to pre-defined event types in event ontology, and propose a novel ED framework entitled OntoED with ontology embedding. We enrich event ontology with linkages among event types, and further induce more event-event correlations. Based on the event ontology, OntoED can leverage and propagate correlation knowledge, particularly from data-rich to data-poor event types. Furthermore, OntoED can be applied to new unseen event types, by establishing linkages to existing ones. Experiments indicate that OntoED is more predominant and robust than previous approaches to ED, especially in data-scarce scenarios",
    "checked": true,
    "id": "e6a1cfe8ed7d65d623b745fa57e6259460874676",
    "semantic_title": "",
    "citation_count": 25,
    "authors": [
      "Shumin Deng",
      "Ningyu Zhang",
      "Luoqiu Li",
      "Chen Hui",
      "Tou Huaixiao",
      "Mosha Chen",
      "Fei Huang",
      "Huajun Chen"
    ]
  },
  "https://aclanthology.org/2021.acl-long.221": {
    "title": "Self-Training Sampling with Monolingual Data Uncertainty for Neural Machine Translation",
    "volume": "long",
    "abstract": "Self-training has proven effective for improving NMT performance by augmenting model training with synthetic parallel data. The common practice is to construct synthetic data based on a randomly sampled subset of large-scale monolingual data, which we empirically show is sub-optimal. In this work, we propose to improve the sampling procedure by selecting the most informative monolingual sentences to complement the parallel data. To this end, we compute the uncertainty of monolingual sentences using the bilingual dictionary extracted from the parallel data. Intuitively, monolingual sentences with lower uncertainty generally correspond to easy-to-translate patterns which may not provide additional gains. Accordingly, we design an uncertainty-based sampling strategy to efficiently exploit the monolingual data for self-training, in which monolingual sentences with higher uncertainty would be sampled with higher probability. Experimental results on large-scale WMT English⇒German and English⇒Chinese datasets demonstrate the effectiveness of the proposed approach. Extensive analyses suggest that emphasizing the learning on uncertain monolingual sentences by our approach does improve the translation quality of high-uncertainty sentences and also benefits the prediction of low-frequency words at the target side",
    "checked": true,
    "id": "6b1450a2a68fb8352a7a763409dedc76ae65b254",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Wenxiang Jiao",
      "Xing Wang",
      "Zhaopeng Tu",
      "Shuming Shi",
      "Michael Lyu",
      "Irwin King"
    ]
  },
  "https://aclanthology.org/2021.acl-long.222": {
    "title": "Breaking the Corpus Bottleneck for Context-Aware Neural Machine Translation with Cross-Task Pre-training",
    "volume": "long",
    "abstract": "Context-aware neural machine translation (NMT) remains challenging due to the lack of large-scale document-level parallel corpora. To break the corpus bottleneck, in this paper we aim to improve context-aware NMT by taking the advantage of the availability of both large-scale sentence-level parallel dataset and source-side monolingual documents. To this end, we propose two pre-training tasks. One learns to translate a sentence from source language to target language on the sentence-level parallel dataset while the other learns to translate a document from deliberately noised to original on the monolingual documents. Importantly, the two pre-training tasks are jointly and simultaneously learned via the same model, thereafter fine-tuned on scale-limited parallel documents from both sentence-level and document-level perspectives. Experimental results on four translation tasks show that our approach significantly improves translation performance. One nice property of our approach is that the fine-tuned model can be used to translate both sentences and documents",
    "checked": true,
    "id": "7569bf6fb3b018662e05503bc6854df02d31e686",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Linqing Chen",
      "Junhui Li",
      "Zhengxian Gong",
      "Boxing Chen",
      "Weihua Luo",
      "Min Zhang",
      "Guodong Zhou"
    ]
  },
  "https://aclanthology.org/2021.acl-long.223": {
    "title": "Guiding Teacher Forcing with Seer Forcing for Neural Machine Translation",
    "volume": "long",
    "abstract": "Although teacher forcing has become the main training paradigm for neural machine translation, it usually makes predictions only conditioned on past information, and hence lacks global planning for the future. To address this problem, we introduce another decoder, called seer decoder, into the encoder-decoder framework during training, which involves future information in target predictions. Meanwhile, we force the conventional decoder to simulate the behaviors of the seer decoder via knowledge distillation. In this way, at test the conventional decoder can perform like the seer decoder without the attendance of it. Experiment results on the Chinese-English, English-German and English-Romanian translation tasks show our method can outperform competitive baselines significantly and achieves greater improvements on the bigger data sets. Besides, the experiments also prove knowledge distillation the best way to transfer knowledge from the seer decoder to the conventional decoder compared to adversarial learning and L2 regularization",
    "checked": true,
    "id": "11138d99feedf8b7db4a5dd1b6ea9d636999c1db",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Yang Feng",
      "Shuhao Gu",
      "Dengji Guo",
      "Zhengxin Yang",
      "Chenze Shao"
    ]
  },
  "https://aclanthology.org/2021.acl-long.224": {
    "title": "Cascade versus Direct Speech Translation: Do the Differences Still Make a Difference?",
    "volume": "long",
    "abstract": "Five years after the first published proofs of concept, direct approaches to speech translation (ST) are now competing with traditional cascade solutions. In light of this steady progress, can we claim that the performance gap between the two is closed? Starting from this question, we present a systematic comparison between state-of-the-art systems representative of the two paradigms. Focusing on three language directions (English-German/Italian/Spanish), we conduct automatic and manual evaluations, exploiting high-quality professional post-edits and annotations. Our multi-faceted analysis on one of the few publicly available ST benchmarks attests for the first time that: i) the gap between the two paradigms is now closed, and ii) the subtle differences observed in their behavior are not sufficient for humans neither to distinguish them nor to prefer one over the other",
    "checked": true,
    "id": "96a09e1ab9b01bc123fa9239eb35278b9d8a9e0f",
    "semantic_title": "",
    "citation_count": 34,
    "authors": [
      "Luisa Bentivogli",
      "Mauro Cettolo",
      "Marco Gaido",
      "Alina Karakanta",
      "Alberto Martinelli",
      "Matteo Negri",
      "Marco Turchi"
    ]
  },
  "https://aclanthology.org/2021.acl-long.225": {
    "title": "Unsupervised Neural Machine Translation for Low-Resource Domains via Meta-Learning",
    "volume": "long",
    "abstract": "Unsupervised machine translation, which utilizes unpaired monolingual corpora as training data, has achieved comparable performance against supervised machine translation. However, it still suffers from data-scarce domains. To address this issue, this paper presents a novel meta-learning algorithm for unsupervised neural machine translation (UNMT) that trains the model to adapt to another domain by utilizing only a small amount of training data. We assume that domain-general knowledge is a significant factor in handling data-scarce domains. Hence, we extend the meta-learning algorithm, which utilizes knowledge learned from high-resource domains, to boost the performance of low-resource UNMT. Our model surpasses a transfer learning-based approach by up to 2-3 BLEU scores. Extensive experimental results show that our proposed algorithm is pertinent for fast adaptation and consistently outperforms other baselines",
    "checked": true,
    "id": "7a9348b148a01b13a24cde828cfbaf81438e7e17",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Cheonbok Park",
      "Yunwon Tae",
      "TaeHee Kim",
      "Soyoung Yang",
      "Mohammad Azam Khan",
      "Lucy Park",
      "Jaegul Choo"
    ]
  },
  "https://aclanthology.org/2021.acl-long.226": {
    "title": "Lightweight Cross-Lingual Sentence Representation Learning",
    "volume": "long",
    "abstract": "Large-scale models for learning fixed-dimensional cross-lingual sentence representations like LASER (Artetxe and Schwenk, 2019b) lead to significant improvement in performance on downstream tasks. However, further increases and modifications based on such large-scale models are usually impractical due to memory limitations. In this work, we introduce a lightweight dual-transformer architecture with just 2 layers for generating memory-efficient cross-lingual sentence representations. We explore different training tasks and observe that current cross-lingual training tasks leave a lot to be desired for this shallow architecture. To ameliorate this, we propose a novel cross-lingual language model, which combines the existing single-word masked language model with the newly proposed cross-lingual token-level reconstruction task. We further augment the training task by the introduction of two computationally-lite sentence-level contrastive learning tasks to enhance the alignment of cross-lingual sentence representation space, which compensates for the learning bottleneck of the lightweight transformer for generative tasks. Our comparisons with competing models on cross-lingual sentence retrieval and multilingual document classification confirm the effectiveness of the newly proposed training tasks for a shallow model",
    "checked": true,
    "id": "3153375485fe706481cd3a4ca74b7b1f554598b2",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Zhuoyuan Mao",
      "Prakhar Gupta",
      "Chenhui Chu",
      "Martin Jaggi",
      "Sadao Kurohashi"
    ]
  },
  "https://aclanthology.org/2021.acl-long.227": {
    "title": "ERNIE-Doc: A Retrospective Long-Document Modeling Transformer",
    "volume": "long",
    "abstract": "Transformers are not suited for processing long documents, due to their quadratically increasing memory and time consumption. Simply truncating a long document or applying the sparse attention mechanism will incur the context fragmentation problem or lead to an inferior modeling capability against comparable model sizes. In this paper, we propose ERNIE-Doc, a document-level language pretraining model based on Recurrence Transformers. Two well-designed techniques, namely the retrospective feed mechanism and the enhanced recurrence mechanism, enable ERNIE-Doc, which has a much longer effective context length, to capture the contextual information of a complete document. We pretrain ERNIE-Doc to explicitly learn the relationships among segments with an additional document-aware segment-reordering objective. Various experiments were conducted on both English and Chinese document-level tasks. ERNIE-Doc improved the state-of-the-art language modeling result of perplexity to 16.8 on WikiText-103. Moreover, it outperformed competitive pretraining models by a large margin on most language understanding tasks, such as text classification and question answering",
    "checked": true,
    "id": "7434fb6c79a50bfedd4ad1c872f9e4fb1f25571a",
    "semantic_title": "",
    "citation_count": 21,
    "authors": [
      "SiYu Ding",
      "Junyuan Shang",
      "Shuohuan Wang",
      "Yu Sun",
      "Hao Tian",
      "Hua Wu",
      "Haifeng Wang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.228": {
    "title": "Marginal Utility Diminishes: Exploring the Minimum Knowledge for BERT Knowledge Distillation",
    "volume": "long",
    "abstract": "Recently, knowledge distillation (KD) has shown great success in BERT compression. Instead of only learning from the teacher’s soft label as in conventional KD, researchers find that the rich information contained in the hidden layers of BERT is conducive to the student’s performance. To better exploit the hidden knowledge, a common practice is to force the student to deeply mimic the teacher’s hidden states of all the tokens in a layer-wise manner. In this paper, however, we observe that although distilling the teacher’s hidden state knowledge (HSK) is helpful, the performance gain (marginal utility) diminishes quickly as more HSK is distilled. To understand this effect, we conduct a series of analysis. Specifically, we divide the HSK of BERT into three dimensions, namely depth, length and width. We first investigate a variety of strategies to extract crucial knowledge for each single dimension and then jointly compress the three dimensions. In this way, we show that 1) the student’s performance can be improved by extracting and distilling the crucial HSK, and 2) using a tiny fraction of HSK can achieve the same performance as extensive HSK distillation. Based on the second finding, we further propose an efficient KD paradigm to compress BERT, which does not require loading the teacher during the training of student. For two kinds of student models and computing devices, the proposed KD paradigm gives rise to training speedup of 2.7x 3.4x",
    "checked": true,
    "id": "f7ea3744478c35c89005cc69f4ce5393dfba8268",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Yuanxin Liu",
      "Fandong Meng",
      "Zheng Lin",
      "Weiping Wang",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2021.acl-long.229": {
    "title": "Rational LAMOL: A Rationale-based Lifelong Learning Framework",
    "volume": "long",
    "abstract": "Lifelong learning (LL) aims to train a neural network on a stream of tasks while retaining knowledge from previous tasks. However, many prior attempts in NLP still suffer from the catastrophic forgetting issue, where the model completely forgets what it just learned in the previous tasks. In this paper, we introduce Rational LAMOL, a novel end-to-end LL framework for language models. In order to alleviate catastrophic forgetting, Rational LAMOL enhances LAMOL, a recent LL model, by applying critical freezing guided by human rationales. When the human rationales are not available, we propose exploiting unsupervised generated rationales as substitutions. In the experiment, we tested Rational LAMOL on permutations of three datasets from the ERASER benchmark. The results show that our proposed framework outperformed vanilla LAMOL on most permutations. Furthermore, unsupervised rationale generation was able to consistently improve the overall LL performance from the baseline without relying on human-annotated rationales",
    "checked": true,
    "id": "f9a5ed41efbdd9642104772fe7304842a87d1104",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Kasidis Kanwatchara",
      "Thanapapas Horsuwan",
      "Piyawat Lertvittayakumjorn",
      "Boonserm Kijsirikul",
      "Peerapon Vateekul"
    ]
  },
  "https://aclanthology.org/2021.acl-long.230": {
    "title": "EnsLM: Ensemble Language Model for Data Diversity by Semantic Clustering",
    "volume": "long",
    "abstract": "Natural language processing (NLP) often faces the problem of data diversity such as different domains, themes, styles, and so on. Therefore, a single language model (LM) is insufficient to learn all knowledge from diverse samples. To solve this problem, we firstly propose an autoencoding topic model with a mixture prior (mATM) to perform clustering for the data, where the clusters defined in semantic space describes the data diversity. Having obtained the clustering assignment for each sample, we develop the ensemble LM (EnsLM) with the technique of weight modulation. Specifically, EnsLM contains a backbone that is adjusted by a few modulated weights to fit for different sample clusters. As a result, the backbone learns the shared knowledge among all clusters while modulated weights extract the cluster-specific features. EnsLM can be trained jointly with mATM with a flexible LM backbone. We evaluate the effectiveness of both mATM and EnsLM on various tasks",
    "checked": true,
    "id": "5829f1c62a82c96548b3b0bbfc6b3cab99fcbb0c",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Zhibin Duan",
      "Hao Zhang",
      "Chaojie Wang",
      "Zhengjue Wang",
      "Bo Chen",
      "Mingyuan Zhou"
    ]
  },
  "https://aclanthology.org/2021.acl-long.231": {
    "title": "LeeBERT: Learned Early Exit for BERT with cross-level optimization",
    "volume": "long",
    "abstract": "Pre-trained language models like BERT are performant in a wide range of natural language tasks. However, they are resource exhaustive and computationally expensive for industrial scenarios. Thus, early exits are adopted at each layer of BERT to perform adaptive computation by predicting easier samples with the first few layers to speed up the inference. In this work, to improve efficiency without performance drop, we propose a novel training scheme called Learned Early Exit for BERT (LeeBERT). First, we ask each exit to learn from each other, rather than learning only from the last layer. Second, the weights of different loss terms are learned, thus balancing off different objectives. We formulate the optimization of LeeBERT as a bi-level optimization problem, and we propose a novel cross-level optimization (CLO) algorithm to improve the optimization results. Experiments on the GLUE benchmark show that our proposed methods improve the performance of the state-of-the-art (SOTA) early exit methods for pre-trained models",
    "checked": true,
    "id": "84310f76cf8909f87c6a7f2ed30ae28214cc9eab",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Wei Zhu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.232": {
    "title": "Unsupervised Extractive Summarization-Based Representations for Accurate and Explainable Collaborative Filtering",
    "volume": "long",
    "abstract": "We pioneer the first extractive summarization-based collaborative filtering model called ESCOFILT. Our proposed model specifically produces extractive summaries for each item and user. Unlike other types of explanations, summary-level explanations closely resemble real-life explanations. The strength of ESCOFILT lies in the fact that it unifies representation and explanation. In other words, extractive summaries both represent and explain the items and users. Our model uniquely integrates BERT, K-Means embedding clustering, and multilayer perceptron to learn sentence embeddings, representation-explanations, and user-item interactions, respectively. We argue that our approach enhances both rating prediction accuracy and user/item explainability. Our experiments illustrate that ESCOFILT’s prediction accuracy is better than the other state-of-the-art recommender models. Furthermore, we propose a comprehensive set of criteria that assesses the real-life explainability of explanations. Our explainability study demonstrates the superiority of and preference for summary-level explanations over other explanation types",
    "checked": true,
    "id": "189596813a51c99f6ab9c8589103782c4f8807f0",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Reinald Adrian Pugoy",
      "Hung-Yu Kao"
    ]
  },
  "https://aclanthology.org/2021.acl-long.233": {
    "title": "PLOME: Pre-training with Misspelled Knowledge for Chinese Spelling Correction",
    "volume": "long",
    "abstract": "Chinese spelling correction (CSC) is a task to detect and correct spelling errors in texts. CSC is essentially a linguistic problem, thus the ability of language understanding is crucial to this task. In this paper, we propose a Pre-trained masked Language model with Misspelled knowledgE (PLOME) for CSC, which jointly learns how to understand language and correct spelling errors. To this end, PLOME masks the chosen tokens with similar characters according to a confusion set rather than the fixed token “[MASK]” as in BERT. Besides character prediction, PLOME also introduces pronunciation prediction to learn the misspelled knowledge on phonic level. Moreover, phonological and visual similarity knowledge is important to this task. PLOME utilizes GRU networks to model such knowledge based on characters’ phonics and strokes. Experiments are conducted on widely used benchmarks. Our method achieves superior performance against state-of-the-art approaches by a remarkable margin. We release the source code and pre-trained model for further use by the community (https://github.com/liushulinle/PLOME)",
    "checked": true,
    "id": "cf6be786c307a8c4e523daa455019a6b7650537c",
    "semantic_title": "",
    "citation_count": 36,
    "authors": [
      "Shulin Liu",
      "Tao Yang",
      "Tianchi Yue",
      "Feng Zhang",
      "Di Wang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.234": {
    "title": "Competence-based Multimodal Curriculum Learning for Medical Report Generation",
    "volume": "long",
    "abstract": "Medical report generation task, which targets to produce long and coherent descriptions of medical images, has attracted growing research interests recently. Different from the general image captioning tasks, medical report generation is more challenging for data-driven neural models. This is mainly due to 1) the serious data bias and 2) the limited medical data. To alleviate the data bias and make best use of available data, we propose a Competence-based Multimodal Curriculum Learning framework (CMCL). Specifically, CMCL simulates the learning process of radiologists and optimizes the model in a step by step manner. Firstly, CMCL estimates the difficulty of each training instance and evaluates the competence of current model; Secondly, CMCL selects the most suitable batch of training instances considering current model competence. By iterating above two steps, CMCL can gradually improve the model’s performance. The experiments on the public IU-Xray and MIMIC-CXR datasets show that CMCL can be incorporated into existing models to improve their performance",
    "checked": true,
    "id": "a816733a342b68b75957cdda927424ff9cb04d42",
    "semantic_title": "",
    "citation_count": 39,
    "authors": [
      "Fenglin Liu",
      "Shen Ge",
      "Xian Wu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.235": {
    "title": "Learning Syntactic Dense Embedding with Correlation Graph for Automatic Readability Assessment",
    "volume": "long",
    "abstract": "Deep learning models for automatic readability assessment generally discard linguistic features traditionally used in machine learning models for the task. We propose to incorporate linguistic features into neural network models by learning syntactic dense embeddings based on linguistic features. To cope with the relationships between the features, we form a correlation graph among features and use it to learn their embeddings so that similar features will be represented by similar embeddings. Experiments with six data sets of two proficiency levels demonstrate that our proposed methodology can complement BERT-only model to achieve significantly better performances for automatic readability assessment",
    "checked": true,
    "id": "4fd7ab34f8db5fc646e06625cb480c022a4fd9e5",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Xinying Qiu",
      "Yuan Chen",
      "Hanwu Chen",
      "Jian-Yun Nie",
      "Yuming Shen",
      "Dawei Lu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.236": {
    "title": "Meta-KD: A Meta Knowledge Distillation Framework for Language Model Compression across Domains",
    "volume": "long",
    "abstract": "Pre-trained language models have been applied to various NLP tasks with considerable performance gains. However, the large model sizes, together with the long inference time, limit the deployment of such models in real-time applications. One line of model compression approaches considers knowledge distillation to distill large teacher models into small student models. Most of these studies focus on single-domain only, which ignores the transferable knowledge from other domains. We notice that training a teacher with transferable knowledge digested across domains can achieve better generalization capability to help knowledge distillation. Hence we propose a Meta-Knowledge Distillation (Meta-KD) framework to build a meta-teacher model that captures transferable knowledge across domains and passes such knowledge to students. Specifically, we explicitly force the meta-teacher to capture transferable knowledge at both instance-level and feature-level from multiple domains, and then propose a meta-distillation algorithm to learn single-domain student models with guidance from the meta-teacher. Experiments on public multi-domain NLP tasks show the effectiveness and superiority of the proposed Meta-KD framework. Further, we also demonstrate the capability of Meta-KD in the settings where the training data is scarce",
    "checked": true,
    "id": "4008920e495f3e73c1324d48fea85aef2e2f2c5b",
    "semantic_title": "",
    "citation_count": 21,
    "authors": [
      "Haojie Pan",
      "Chengyu Wang",
      "Minghui Qiu",
      "Yichang Zhang",
      "Yaliang Li",
      "Jun Huang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.237": {
    "title": "A Semantic-based Method for Unsupervised Commonsense Question Answering",
    "volume": "long",
    "abstract": "Unsupervised commonsense question answering is appealing since it does not rely on any labeled task data. Among existing work, a popular solution is to use pre-trained language models to score candidate choices directly conditioned on the question or context. However, such scores from language models can be easily affected by irrelevant factors, such as word frequencies, sentence structures, etc. These distracting factors may not only mislead the model to choose a wrong answer but also make it oversensitive to lexical perturbations in candidate answers. In this paper, we present a novel SEmantic-based Question Answering method (SEQA) for unsupervised commonsense question answering. Instead of directly scoring each answer choice, our method first generates a set of plausible answers with generative models (e.g., GPT-2), and then uses these plausible answers to select the correct choice by considering the semantic similarity between each plausible answer and each choice. We devise a simple, yet sound formalism for this idea and verify its effectiveness and robustness with extensive experiments. We evaluate the proposed method on four benchmark datasets, and our method achieves the best results in unsupervised settings. Moreover, when attacked by TextFooler with synonym replacement, SEQA demonstrates much less performance drops than baselines, thereby indicating stronger robustness",
    "checked": true,
    "id": "8e80a1ee0093771343ed8f0bed6eb6dbe7d4d7d8",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Yilin Niu",
      "Fei Huang",
      "Jiaming Liang",
      "Wenkai Chen",
      "Xiaoyan Zhu",
      "Minlie Huang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.238": {
    "title": "Explanations for CommonsenseQA: New Dataset and Models",
    "volume": "long",
    "abstract": "CommonsenseQA (CQA) (Talmor et al., 2019) dataset was recently released to advance the research on common-sense question answering (QA) task. Whereas the prior work has mostly focused on proposing QA models for this dataset, our aim is to retrieve as well as generate explanation for a given (question, correct answer choice, incorrect answer choices) tuple from this dataset. Our explanation definition is based on certain desiderata, and translates an explanation into a set of positive and negative common-sense properties (aka facts) which not only explain the correct answer choice but also refute the incorrect ones. We human-annotate a first-of-its-kind dataset (called ECQA) of positive and negative properties, as well as free-flow explanations, for 11K QA pairs taken from the CQA dataset. We propose a latent representation based property retrieval model as well as a GPT-2 based property generation model with a novel two step fine-tuning procedure. We also propose a free-flow explanation generation model. Extensive experiments show that our retrieval model beats BM25 baseline by a relative gain of 100% in F1 score, property generation model achieves a respectable F1 score of 36.4, and free-flow generation model achieves a similarity score of 61.9, where last two scores are based on a human correlated semantic similarity metric",
    "checked": true,
    "id": "d9e0b07313a04b033a9f2dcc74c41b2bed8c3614",
    "semantic_title": "",
    "citation_count": 52,
    "authors": [
      "Shourya Aggarwal",
      "Divyanshu Mandowara",
      "Vishwajeet Agrawal",
      "Dinesh Khandelwal",
      "Parag Singla",
      "Dinesh Garg"
    ]
  },
  "https://aclanthology.org/2021.acl-long.239": {
    "title": "Few-Shot Question Answering by Pretraining Span Selection",
    "volume": "long",
    "abstract": "In several question answering benchmarks, pretrained models have reached human parity through fine-tuning on an order of 100,000 annotated questions and answers. We explore the more realistic few-shot setting, where only a few hundred training examples are available, and observe that standard models perform poorly, highlighting the discrepancy between current pretraining objectives and question answering. We propose a new pretraining scheme tailored for question answering: recurring span selection. Given a passage with multiple sets of recurring spans, we mask in each set all recurring spans but one, and ask the model to select the correct span in the passage for each masked span. Masked spans are replaced with a special token, viewed as a question representation, that is later used during fine-tuning to select the answer span. The resulting model obtains surprisingly good results on multiple benchmarks (e.g., 72.7 F1 on SQuAD with only 128 training examples), while maintaining competitive performance in the high-resource setting",
    "checked": true,
    "id": "06047017f7a2b4dee6d8078786a21c8f67590a22",
    "semantic_title": "",
    "citation_count": 63,
    "authors": [
      "Ori Ram",
      "Yuval Kirstain",
      "Jonathan Berant",
      "Amir Globerson",
      "Omer Levy"
    ]
  },
  "https://aclanthology.org/2021.acl-long.240": {
    "title": "UnitedQA: A Hybrid Approach for Open Domain Question Answering",
    "volume": "long",
    "abstract": "To date, most of recent work under the retrieval-reader framework for open-domain QA focuses on either extractive or generative reader exclusively. In this paper, we study a hybrid approach for leveraging the strengths of both models. We apply novel techniques to enhance both extractive and generative readers built upon recent pretrained neural language models, and find that proper training methods can provide large improvement over previous state-of-the-art models. We demonstrate that a simple hybrid approach by combining answers from both readers can efficiently take advantages of extractive and generative answer inference strategies and outperforms single models as well as homogeneous ensembles. Our approach outperforms previous state-of-the-art models by 3.3 and 2.7 points in exact match on NaturalQuestions and TriviaQA respectively",
    "checked": true,
    "id": "17d680f0dc9211492107eac5ff62b08b627f107a",
    "semantic_title": "",
    "citation_count": 27,
    "authors": [
      "Hao Cheng",
      "Yelong Shen",
      "Xiaodong Liu",
      "Pengcheng He",
      "Weizhu Chen",
      "Jianfeng Gao"
    ]
  },
  "https://aclanthology.org/2021.acl-long.241": {
    "title": "Database reasoning over text",
    "volume": "long",
    "abstract": "Neural models have shown impressive performance gains in answering queries from natural language text. However, existing works are unable to support database queries, such as “List/Count all female athletes who were born in 20th century”, which require reasoning over sets of relevant facts with operations such as join, filtering and aggregation. We show that while state-of-the-art transformer models perform very well for small databases, they exhibit limitations in processing noisy data, numerical operations, and queries that aggregate facts. We propose a modular architecture to answer these database-style queries over multiple spans from text and aggregating these at scale. We evaluate the architecture using WikiNLDB, a novel dataset for exploring such queries. Our architecture scales to databases containing thousands of facts whereas contemporary models are limited by how many facts can be encoded. In direct comparison on small databases, our approach increases overall answer accuracy from 85% to 90%. On larger databases, our approach retains its accuracy whereas transformer baselines could not encode the context",
    "checked": true,
    "id": "16529f7194bf7faee8a4e43fd54aefeb8730f236",
    "semantic_title": "",
    "citation_count": 22,
    "authors": [
      "James Thorne",
      "Majid Yazdani",
      "Marzieh Saeidi",
      "Fabrizio Silvestri",
      "Sebastian Riedel",
      "Alon Halevy"
    ]
  },
  "https://aclanthology.org/2021.acl-long.242": {
    "title": "Online Learning Meets Machine Translation Evaluation: Finding the Best Systems with the Least Human Effort",
    "volume": "long",
    "abstract": "In Machine Translation, assessing the quality of a large amount of automatic translations can be challenging. Automatic metrics are not reliable when it comes to high performing systems. In addition, resorting to human evaluators can be expensive, especially when evaluating multiple systems. To overcome the latter challenge, we propose a novel application of online learning that, given an ensemble of Machine Translation systems, dynamically converges to the best systems, by taking advantage of the human feedback available. Our experiments on WMT’19 datasets show that our online approach quickly converges to the top-3 ranked systems for the language pairs considered, despite the lack of human feedback for many translations",
    "checked": true,
    "id": "60c713cf96246f6b8bbce6d1b713a8f76196cf58",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Vânia Mendonça",
      "Ricardo Rei",
      "Luisa Coheur",
      "Alberto Sardinha",
      "Ana Lúcia Santos"
    ]
  },
  "https://aclanthology.org/2021.acl-long.243": {
    "title": "How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models",
    "volume": "long",
    "abstract": "In this work, we provide a systematic and comprehensive empirical comparison of pretrained multilingual language models versus their monolingual counterparts with regard to their monolingual task performance. We study a set of nine typologically diverse languages with readily available pretrained monolingual models on a set of five diverse monolingual downstream tasks. We first aim to establish, via fair and controlled comparisons, if a gap between the multilingual and the corresponding monolingual representation of that language exists, and subsequently investigate the reason for any performance difference. To disentangle conflating factors, we train new monolingual models on the same data, with monolingually and multilingually trained tokenizers. We find that while the pretraining data size is an important factor, a designated monolingual tokenizer plays an equally important role in the downstream performance. Our results show that languages that are adequately represented in the multilingual model’s vocabulary exhibit negligible performance decreases over their monolingual counterparts. We further find that replacing the original multilingual tokenizer with the specialized monolingual tokenizer improves the downstream performance of the multilingual model for almost every task and language",
    "checked": true,
    "id": "0d4b5c9a071557f4eb12f63f785dbc89071d4272",
    "semantic_title": "",
    "citation_count": 108,
    "authors": [
      "Phillip Rust",
      "Jonas Pfeiffer",
      "Ivan Vulić",
      "Sebastian Ruder",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2021.acl-long.244": {
    "title": "Evaluating morphological typology in zero-shot cross-lingual transfer",
    "volume": "long",
    "abstract": "Cross-lingual transfer has improved greatly through multi-lingual language model pretraining, reducing the need for parallel data and increasing absolute performance. However, this progress has also brought to light the differences in performance across languages. Specifically, certain language families and typologies seem to consistently perform worse in these models. In this paper, we address what effects morphological typology has on zero-shot cross-lingual transfer for two tasks: Part-of-speech tagging and sentiment analysis. We perform experiments on 19 languages from four language typologies (fusional, isolating, agglutinative, and introflexive) and find that transfer to another morphological type generally implies a higher loss than transfer to another language with the same morphological typology. Furthermore, POS tagging is more sensitive to morphological typology than sentiment analysis and, on this task, models perform much better on fusional languages than on the other typologies",
    "checked": true,
    "id": "7ab91b21e2ef42293ad0036e55d526ea9924dede",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Antonio Martínez-García",
      "Toni Badia",
      "Jeremy Barnes"
    ]
  },
  "https://aclanthology.org/2021.acl-long.245": {
    "title": "From Machine Translation to Code-Switching: Generating High-Quality Code-Switched Text",
    "volume": "long",
    "abstract": "Generating code-switched text is a problem of growing interest, especially given the scarcity of corpora containing large volumes of real code-switched text. In this work, we adapt a state-of-the-art neural machine translation model to generate Hindi-English code-switched sentences starting from monolingual Hindi sentences. We outline a carefully designed curriculum of pretraining steps, including the use of synthetic code-switched text, that enable the model to generate high-quality code-switched text. Using text generated from our model as data augmentation, we show significant reductions in perplexity on a language modeling task, compared to using text from other generative models of CS text. We also show improvements using our text for a downstream code-switched natural language inference task. Our generated text is further subjected to a rigorous evaluation using a human evaluation study and a range of objective metrics, where we show performance comparable (and sometimes even superior) to code-switched text obtained via crowd workers who are native Hindi speakers",
    "checked": true,
    "id": "a729aca88222c459909942c51cf52b1a1797329a",
    "semantic_title": "",
    "citation_count": 21,
    "authors": [
      "Ishan Tarunesh",
      "Syamantak Kumar",
      "Preethi Jyothi"
    ]
  },
  "https://aclanthology.org/2021.acl-long.246": {
    "title": "Fast and Accurate Neural Machine Translation with Translation Memory",
    "volume": "long",
    "abstract": "It is generally believed that a translation memory (TM) should be beneficial for machine translation tasks. Unfortunately, existing wisdom demonstrates the superiority of TM-based neural machine translation (NMT) only on the TM-specialized translation tasks rather than general tasks, with a non-negligible computational overhead. In this paper, we propose a fast and accurate approach to TM-based NMT within the Transformer framework: the model architecture is simple and employs a single bilingual sentence as its TM, leading to efficient training and inference; and its parameters are effectively optimized through a novel training criterion. Extensive experiments on six TM-specialized tasks show that the proposed approach substantially surpasses several strong baselines that use multiple TMs, in terms of BLEU and running time. In particular, the proposed approach also advances the strong baselines on two general tasks (WMT news Zh->En and En->De)",
    "checked": true,
    "id": "780b2c43f596ba072abeb04a771606b4d8c2a935",
    "semantic_title": "",
    "citation_count": 27,
    "authors": [
      "Qiuxiang He",
      "Guoping Huang",
      "Qu Cui",
      "Li Li",
      "Lemao Liu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.247": {
    "title": "Annotating Online Misogyny",
    "volume": "long",
    "abstract": "Online misogyny, a category of online abusive language, has serious and harmful social consequences. Automatic detection of misogynistic language online, while imperative, poses complicated challenges to both data gathering, data annotation, and bias mitigation, as this type of data is linguistically complex and diverse. This paper makes three contributions in this area: Firstly, we describe the detailed design of our iterative annotation process and codebook. Secondly, we present a comprehensive taxonomy of labels for annotating misogyny in natural written language, and finally, we introduce a high-quality dataset of annotated posts sampled from social media posts",
    "checked": true,
    "id": "bbe109dbc96dab47a3fd049065b82115922d7a52",
    "semantic_title": "",
    "citation_count": 24,
    "authors": [
      "Philine Zeinert",
      "Nanna Inie",
      "Leon Derczynski"
    ]
  },
  "https://aclanthology.org/2021.acl-long.248": {
    "title": "Few-NERD: A Few-shot Named Entity Recognition Dataset",
    "volume": "long",
    "abstract": "Recently, considerable literature has grown up around the theme of few-shot named entity recognition (NER), but little published benchmark data specifically focused on the practical and challenging task. Current approaches collect existing supervised NER datasets and re-organize them to the few-shot setting for empirical study. These strategies conventionally aim to recognize coarse-grained entity types with few examples, while in practice, most unseen entity types are fine-grained. In this paper, we present Few-NERD, a large-scale human-annotated few-shot NER dataset with a hierarchy of 8 coarse-grained and 66 fine-grained entity types. Few-NERD consists of 188,238 sentences from Wikipedia, 4,601,160 words are included and each is annotated as context or a part of the two-level entity type. To the best of our knowledge, this is the first few-shot NER dataset and the largest human-crafted NER dataset. We construct benchmark tasks with different emphases to comprehensively assess the generalization capability of models. Extensive empirical results and analysis show that Few-NERD is challenging and the problem requires further research. The Few-NERD dataset and the baselines will be publicly available to facilitate the research on this problem",
    "checked": true,
    "id": "a293a01ddd639b25360cf4f23e2df8dd0d1caa8e",
    "semantic_title": "",
    "citation_count": 93,
    "authors": [
      "Ning Ding",
      "Guangwei Xu",
      "Yulin Chen",
      "Xiaobin Wang",
      "Xu Han",
      "Pengjun Xie",
      "Haitao Zheng",
      "Zhiyuan Liu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.249": {
    "title": "MultiMET: A Multimodal Dataset for Metaphor Understanding",
    "volume": "long",
    "abstract": "Metaphor involves not only a linguistic phenomenon, but also a cognitive phenomenon structuring human thought, which makes understanding it challenging. As a means of cognition, metaphor is rendered by more than texts alone, and multimodal information in which vision/audio content is integrated with the text can play an important role in expressing and understanding metaphor. However, previous metaphor processing and understanding has focused on texts, partly due to the unavailability of large-scale datasets with ground truth labels of multimodal metaphor. In this paper, we introduce MultiMET, a novel multimodal metaphor dataset to facilitate understanding metaphorical information from multimodal text and image. It contains 10,437 text-image pairs from a range of sources with multimodal annotations of the occurrence of metaphors, domain relations, sentiments metaphors convey, and author intents. MultiMET opens the door to automatic metaphor understanding by investigating multimodal cues and their interplay. Moreover, we propose a range of strong baselines and show the importance of combining multimodal cues for metaphor understanding. MultiMET will be released publicly for research",
    "checked": true,
    "id": "b4985722eeb0f0a60b8545b770ea94c7e6adfccf",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Dongyu Zhang",
      "Minghao Zhang",
      "Heting Zhang",
      "Liang Yang",
      "Hongfei Lin"
    ]
  },
  "https://aclanthology.org/2021.acl-long.250": {
    "title": "Human-in-the-Loop for Data Collection: a Multi-Target Counter Narrative Dataset to Fight Online Hate Speech",
    "volume": "long",
    "abstract": "Undermining the impact of hateful content with informed and non-aggressive responses, called counter narratives, has emerged as a possible solution for having healthier online communities. Thus, some NLP studies have started addressing the task of counter narrative generation. Although such studies have made an effort to build hate speech / counter narrative (HS/CN) datasets for neural generation, they fall short in reaching either high-quality and/or high-quantity. In this paper, we propose a novel human-in-the-loop data collection methodology in which a generative language model is refined iteratively by using its own data from the previous loops to generate new training samples that experts review and/or post-edit. Our experiments comprised several loops including diverse dynamic variations. Results show that the methodology is scalable and facilitates diverse, novel, and cost-effective data collection. To our knowledge, the resulting dataset is the only expert-based multi-target HS/CN dataset available to the community",
    "checked": true,
    "id": "efdb6e2691decd4de9b58d3967b5a592a6b0a809",
    "semantic_title": "",
    "citation_count": 27,
    "authors": [
      "Margherita Fanton",
      "Helena Bonaldi",
      "Serra Sinem Tekiroğlu",
      "Marco Guerini"
    ]
  },
  "https://aclanthology.org/2021.acl-long.251": {
    "title": "Can Generative Pre-trained Language Models Serve As Knowledge Bases for Closed-book QA?",
    "volume": "long",
    "abstract": "Recent work has investigated the interesting question using pre-trained language models (PLMs) as knowledge bases for answering open questions. However, existing work is limited in using small benchmarks with high test-train overlaps. We construct a new dataset of closed-book QA using SQuAD, and investigate the performance of BART. Experiments show that it is challenging for BART to remember training facts in high precision, and also challenging to answer closed-book questions even if relevant knowledge is retained. Some promising directions are found, including decoupling the knowledge memorizing process and the QA finetune process, forcing the model to recall relevant knowledge when question answering",
    "checked": true,
    "id": "f62acd332fd7a6f35b117ed4ffaf93b19483dcf7",
    "semantic_title": "",
    "citation_count": 39,
    "authors": [
      "Cunxiang Wang",
      "Pai Liu",
      "Yue Zhang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.252": {
    "title": "Joint Models for Answer Verification in Question Answering Systems",
    "volume": "long",
    "abstract": "This paper studies joint models for selecting correct answer sentences among the top k provided by answer sentence selection (AS2) modules, which are core components of retrieval-based Question Answering (QA) systems. Our work shows that a critical step to effectively exploiting an answer set regards modeling the interrelated information between pair of answers. For this purpose, we build a three-way multi-classifier, which decides if an answer supports, refutes, or is neutral with respect to another one. More specifically, our neural architecture integrates a state-of-the-art AS2 module with the multi-classifier, and a joint layer connecting all components. We tested our models on WikiQA, TREC-QA, and a real-world dataset. The results show that our models obtain the new state of the art in AS2",
    "checked": true,
    "id": "af4453d721cffea3d393572f3803d5f3a1864e7c",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Zeyu Zhang",
      "Thuy Vu",
      "Alessandro Moschitti"
    ]
  },
  "https://aclanthology.org/2021.acl-long.253": {
    "title": "Answering Ambiguous Questions through Generative Evidence Fusion and Round-Trip Prediction",
    "volume": "long",
    "abstract": "In open-domain question answering, questions are highly likely to be ambiguous because users may not know the scope of relevant topics when formulating them. Therefore, a system needs to find possible interpretations of the question, and predict one or multiple plausible answers. When multiple plausible answers are found, the system should rewrite the question for each answer to resolve the ambiguity. In this paper, we present a model that aggregates and combines evidence from multiple passages to adaptively predict a single answer or a set of question-answer pairs for ambiguous questions. In addition, we propose a novel round-trip prediction approach to iteratively generate additional interpretations that our model fails to find in the first pass, and then verify and filter out the incorrect question-answer pairs to arrive at the final disambiguated output. Our model, named Refuel, achieves a new state-of-the-art performance on the AmbigQA dataset, and shows competitive performance on NQ-Open and TriviaQA. The proposed round-trip prediction is a model-agnostic general approach for answering ambiguous open-domain questions, which improves our Refuel as well as several baseline models. We release source code for our models and experiments at https://github.com/amzn/refuel-open-domain-qa",
    "checked": true,
    "id": "17ccd81f77d8cf798f12608ade1d838ffb96b66c",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Yifan Gao",
      "Henghui Zhu",
      "Patrick Ng",
      "Cicero Nogueira dos Santos",
      "Zhiguo Wang",
      "Feng Nan",
      "Dejiao Zhang",
      "Ramesh Nallapati",
      "Andrew O. Arnold",
      "Bing Xiang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.254": {
    "title": "TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance",
    "volume": "long",
    "abstract": "Hybrid data combining both tabular and textual content (e.g., financial reports) are quite pervasive in the real world. However, Question Answering (QA) over such hybrid data is largely neglected in existing research. In this work, we extract samples from real financial reports to build a new large-scale QA dataset containing both Tabular And Textual data, named TAT-QA, where numerical reasoning is usually required to infer the answer, such as addition, subtraction, multiplication, division, counting, comparison/sorting, and the compositions. We further propose a novel QA model termed TAGOP, which is capable of reasoning over both tables and text. It adopts sequence tagging to extract relevant cells from the table along with relevant spans from the text to infer their semantics, and then applies symbolic reasoning over them with a set of aggregation operators to arrive at the final answer. TAGOP achieves 58.0% inF1, which is an 11.1% absolute increase over the previous best baseline model, according to our experiments on TAT-QA. But this result still lags far behind performance of expert human, i.e.90.8% in F1. It is demonstrated that our TAT-QA is very challenging and can serve as a benchmark for training and testing powerful QA models that address hybrid form data",
    "checked": true,
    "id": "b3213c84a6ff7a2f11099de783c93166e4fc02a4",
    "semantic_title": "",
    "citation_count": 77,
    "authors": [
      "Fengbin Zhu",
      "Wenqiang Lei",
      "Youcheng Huang",
      "Chao Wang",
      "Shuo Zhang",
      "Jiancheng Lv",
      "Fuli Feng",
      "Tat-Seng Chua"
    ]
  },
  "https://aclanthology.org/2021.acl-long.255": {
    "title": "Modeling Transitions of Focal Entities for Conversational Knowledge Base Question Answering",
    "volume": "long",
    "abstract": "Conversational KBQA is about answering a sequence of questions related to a KB. Follow-up questions in conversational KBQA often have missing information referring to entities from the conversation history. In this paper, we propose to model these implied entities, which we refer to as the focal entities of the conversation. We propose a novel graph-based model to capture the transitions of focal entities and apply a graph neural network to derive a probability distribution of focal entities for each question, which is then combined with a standard KBQA module to perform answer ranking. Our experiments on two datasets demonstrate the effectiveness of our proposed method",
    "checked": true,
    "id": "ce4ed8a9a21e19997f3b371db988aae8cf2684a6",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Yunshi Lan",
      "Jing Jiang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.256": {
    "title": "Evidence-based Factual Error Correction",
    "volume": "long",
    "abstract": "This paper introduces the task of factual error correction: performing edits to a claim so that the generated rewrite is better supported by evidence. This extends the well-studied task of fact verification by providing a mechanism to correct written texts that are refuted or only partially supported by evidence. We demonstrate that it is feasible to train factual error correction systems from existing fact checking datasets which only contain labeled claims accompanied by evidence, but not the correction. We achieve this by employing a two-stage distant supervision approach that incorporates evidence into masked claims when generating corrections. Our approach, based on the T5 transformer and using retrieved evidence, achieved better results than existing work which used a pointer copy network and gold evidence, producing accurate factual error corrections for 5x more instances in human evaluation and a .125 increase in SARI score. The evaluation is conducted on a dataset of 65,000 instances based on a recent fact verification shared task and we release it to enable further work on the task",
    "checked": true,
    "id": "03cb8234036dedd356901f574c1771a88e3578d8",
    "semantic_title": "",
    "citation_count": 21,
    "authors": [
      "James Thorne",
      "Andreas Vlachos"
    ]
  },
  "https://aclanthology.org/2021.acl-long.257": {
    "title": "Probabilistic, Structure-Aware Algorithms for Improved Variety, Accuracy, and Coverage of AMR Alignments",
    "volume": "long",
    "abstract": "We present algorithms for aligning components of Abstract Meaning Representation (AMR) graphs to spans in English sentences. We leverage unsupervised learning in combination with heuristics, taking the best of both worlds from previous AMR aligners. Our unsupervised models, however, are more sensitive to graph substructures, without requiring a separate syntactic parse. Our approach covers a wider variety of AMR substructures than previously considered, achieves higher coverage of nodes and edges, and does so with higher accuracy. We will release our LEAMR datasets and aligner for use in research on AMR parsing, generation, and evaluation",
    "checked": true,
    "id": "5382d681722a3b4a79b8ef638cd251f07f6316d3",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Austin Blodgett",
      "Nathan Schneider"
    ]
  },
  "https://aclanthology.org/2021.acl-long.258": {
    "title": "Meta-Learning to Compositionally Generalize",
    "volume": "long",
    "abstract": "Natural language is compositional; the meaning of a sentence is a function of the meaning of its parts. This property allows humans to create and interpret novel sentences, generalizing robustly outside their prior experience. Neural networks have been shown to struggle with this kind of generalization, in particular performing poorly on tasks designed to assess compositional generalization (i.e. where training and testing distributions differ in ways that would be trivial for a compositional strategy to resolve). Their poor performance on these tasks may in part be due to the nature of supervised learning which assumes training and testing data to be drawn from the same distribution. We implement a meta-learning augmented version of supervised learning whose objective directly optimizes for out-of-distribution generalization. We construct pairs of tasks for meta-learning by sub-sampling existing training data. Each pair of tasks is constructed to contain relevant examples, as determined by a similarity metric, in an effort to inhibit models from memorizing their input. Experimental results on the COGS and SCAN datasets show that our similarity-driven meta-learning can improve generalization performance",
    "checked": true,
    "id": "70a136547d81290b9f4dbc1fac49d31bc010bd3c",
    "semantic_title": "",
    "citation_count": 44,
    "authors": [
      "Henry Conklin",
      "Bailin Wang",
      "Kenny Smith",
      "Ivan Titov"
    ]
  },
  "https://aclanthology.org/2021.acl-long.259": {
    "title": "Taming Pre-trained Language Models with N-gram Representations for Low-Resource Domain Adaptation",
    "volume": "long",
    "abstract": "Large pre-trained models such as BERT are known to improve different downstream NLP tasks, even when such a model is trained on a generic domain. Moreover, recent studies have shown that when large domain-specific corpora are available, continued pre-training on domain-specific data can further improve the performance of in-domain tasks. However, this practice requires significant domain-specific data and computational resources which may not always be available. In this paper, we aim to adapt a generic pretrained model with a relatively small amount of domain-specific data. We demonstrate that by explicitly incorporating multi-granularity information of unseen and domain-specific words via the adaptation of (word based) n-grams, the performance of a generic pretrained model can be greatly improved. Specifically, we introduce a Transformer-based Domain-aware N-gram Adaptor, T-DNA, to effectively learn and incorporate the semantic representation of different combinations of words in the new domain. Experimental results illustrate the effectiveness of T-DNA on eight low-resource downstream tasks from four domains. We show that T-DNA is able to achieve significant improvements compared to existing methods on most tasks using limited data with lower computational costs. Moreover, further analyses demonstrate the importance and effectiveness of both unseen words and the information of different granularities. Our code is available at https://github.com/shizhediao/T-DNA",
    "checked": true,
    "id": "959927c3bdc254d0c02583d47f35654d53c4d6ef",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Shizhe Diao",
      "Ruijia Xu",
      "Hongjin Su",
      "Yilei Jiang",
      "Yan Song",
      "Tong Zhang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.260": {
    "title": "ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning",
    "volume": "long",
    "abstract": "Pre-trained Language Models (PLMs) have shown superior performance on various downstream Natural Language Processing (NLP) tasks. However, conventional pre-training objectives do not explicitly model relational facts in text, which are crucial for textual understanding. To address this issue, we propose a novel contrastive learning framework ERICA to obtain a deep understanding of the entities and their relations in text. Specifically, we define two novel pre-training tasks to better understand entities and relations: (1) the entity discrimination task to distinguish which tail entity can be inferred by the given head entity and relation; (2) the relation discrimination task to distinguish whether two relations are close or not semantically, which involves complex relational reasoning. Experimental results demonstrate that ERICA can improve typical PLMs (BERT and RoBERTa) on several language understanding tasks, including relation extraction, entity typing and question answering, especially under low-resource settings",
    "checked": true,
    "id": "25c3b294b9ed2786c4476a25e8b36ebf49fd5b4b",
    "semantic_title": "",
    "citation_count": 66,
    "authors": [
      "Yujia Qin",
      "Yankai Lin",
      "Ryuichi Takanobu",
      "Zhiyuan Liu",
      "Peng Li",
      "Heng Ji",
      "Minlie Huang",
      "Maosong Sun",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2021.acl-long.261": {
    "title": "Position Bias Mitigation: A Knowledge-Aware Graph Model for Emotion Cause Extraction",
    "volume": "long",
    "abstract": "The Emotion Cause Extraction (ECE) task aims to identify clauses which contain emotion-evoking information for a particular emotion expressed in text. We observe that a widely-used ECE dataset exhibits a bias that the majority of annotated cause clauses are either directly before their associated emotion clauses or are the emotion clauses themselves. Existing models for ECE tend to explore such relative position information and suffer from the dataset bias. To investigate the degree of reliance of existing ECE models on clause relative positions, we propose a novel strategy to generate adversarial examples in which the relative position information is no longer the indicative feature of cause clauses. We test the performance of existing models on such adversarial examples and observe a significant performance drop. To address the dataset bias, we propose a novel graph-based method to explicitly model the emotion triggering paths by leveraging the commonsense knowledge to enhance the semantic dependencies between a candidate clause and an emotion clause. Experimental results show that our proposed approach performs on par with the existing state-of-the-art methods on the original ECE dataset, and is more robust against adversarial attacks compared to existing models",
    "checked": true,
    "id": "02ea66066e0f257aee147766a0707e352639e5c0",
    "semantic_title": "",
    "citation_count": 22,
    "authors": [
      "Hanqi Yan",
      "Lin Gui",
      "Gabriele Pergola",
      "Yulan He"
    ]
  },
  "https://aclanthology.org/2021.acl-long.262": {
    "title": "Every Bite Is an Experience: Key Point Analysis of Business Reviews",
    "volume": "long",
    "abstract": "Previous work on review summarization focused on measuring the sentiment toward the main aspects of the reviewed product or business, or on creating a textual summary. These approaches provide only a partial view of the data: aspect-based sentiment summaries lack sufficient explanation or justification for the aspect rating, while textual summaries do not quantify the significance of each element, and are not well-suited for representing conflicting views. Recently, Key Point Analysis (KPA) has been proposed as a summarization framework that provides both textual and quantitative summary of the main points in the data. We adapt KPA to review data by introducing Collective Key Point Mining for better key point extraction; integrating sentiment analysis into KPA; identifying good key point candidates for review summaries; and leveraging the massive amount of available reviews and their metadata. We show empirically that these novel extensions of KPA substantially improve its performance. We demonstrate that promising results can be achieved without any domain-specific annotation, while human supervision can lead to further improvement",
    "checked": true,
    "id": "86cceb149bc903724fa4eafbfc6e532ae18850d0",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Roy Bar-Haim",
      "Lilach Eden",
      "Yoav Kantor",
      "Roni Friedman",
      "Noam Slonim"
    ]
  },
  "https://aclanthology.org/2021.acl-long.263": {
    "title": "Structured Sentiment Analysis as Dependency Graph Parsing",
    "volume": "long",
    "abstract": "Structured sentiment analysis attempts to extract full opinion tuples from a text, but over time this task has been subdivided into smaller and smaller sub-tasks, e.g., target extraction or targeted polarity classification. We argue that this division has become counterproductive and propose a new unified framework to remedy the situation. We cast the structured sentiment problem as dependency graph parsing, where the nodes are spans of sentiment holders, targets and expressions, and the arcs are the relations between them. We perform experiments on five datasets in four languages (English, Norwegian, Basque, and Catalan) and show that this approach leads to strong improvements over state-of-the-art baselines. Our analysis shows that refining the sentiment graphs with syntactic dependency information further improves results",
    "checked": true,
    "id": "fd288ade5654d3e485e0102386d290a7e6fcae95",
    "semantic_title": "",
    "citation_count": 39,
    "authors": [
      "Jeremy Barnes",
      "Robin Kurtz",
      "Stephan Oepen",
      "Lilja Øvrelid",
      "Erik Velldal"
    ]
  },
  "https://aclanthology.org/2021.acl-long.264": {
    "title": "Consistency Regularization for Cross-Lingual Fine-Tuning",
    "volume": "long",
    "abstract": "Fine-tuning pre-trained cross-lingual language models can transfer task-specific supervision from one language to the others. In this work, we propose to improve cross-lingual fine-tuning with consistency regularization. Specifically, we use example consistency regularization to penalize the prediction sensitivity to four types of data augmentations, i.e., subword sampling, Gaussian noise, code-switch substitution, and machine translation. In addition, we employ model consistency to regularize the models trained with two augmented versions of the same training set. Experimental results on the XTREME benchmark show that our method significantly improves cross-lingual fine-tuning across various tasks, including text classification, question answering, and sequence labeling",
    "checked": true,
    "id": "1da75c7a8915c0794bc9857f93c6cf31aac17ad2",
    "semantic_title": "",
    "citation_count": 32,
    "authors": [
      "Bo Zheng",
      "Li Dong",
      "Shaohan Huang",
      "Wenhui Wang",
      "Zewen Chi",
      "Saksham Singhal",
      "Wanxiang Che",
      "Ting Liu",
      "Xia Song",
      "Furu Wei"
    ]
  },
  "https://aclanthology.org/2021.acl-long.265": {
    "title": "Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word Alignment",
    "volume": "long",
    "abstract": "The cross-lingual language models are typically pretrained with masked language modeling on multilingual text or parallel sentences. In this paper, we introduce denoising word alignment as a new cross-lingual pre-training task. Specifically, the model first self-label word alignments for parallel sentences. Then we randomly mask tokens in a bitext pair. Given a masked token, the model uses a pointer network to predict the aligned token in the other language. We alternately perform the above two steps in an expectation-maximization manner. Experimental results show that our method improves cross-lingual transferability on various datasets, especially on the token-level tasks, such as question answering, and structured prediction. Moreover, the model can serve as a pretrained word aligner, which achieves reasonably low error rate on the alignment benchmarks. The code and pretrained parameters are available at github.com/CZWin32768/XLM-Align",
    "checked": true,
    "id": "5539127e3907a492d97181c9e62e43f91d7cf19e",
    "semantic_title": "",
    "citation_count": 32,
    "authors": [
      "Zewen Chi",
      "Li Dong",
      "Bo Zheng",
      "Shaohan Huang",
      "Xian-Ling Mao",
      "Heyan Huang",
      "Furu Wei"
    ]
  },
  "https://aclanthology.org/2021.acl-long.266": {
    "title": "Rejuvenating Low-Frequency Words: Making the Most of Parallel Data in Non-Autoregressive Translation",
    "volume": "long",
    "abstract": "Knowledge distillation (KD) is commonly used to construct synthetic data for training non-autoregressive translation (NAT) models. However, there exists a discrepancy on low-frequency words between the distilled and the original data, leading to more errors on predicting low-frequency words. To alleviate the problem, we directly expose the raw data into NAT by leveraging pretraining. By analyzing directed alignments, we found that KD makes low-frequency source words aligned with targets more deterministically but fails to align sufficient low-frequency words from target to source. Accordingly, we propose reverse KD to rejuvenate more alignments for low-frequency target words. To make the most of authentic and synthetic data, we combine these complementary approaches as a new training strategy for further boosting NAT performance. We conduct experiments on five translation benchmarks over two advanced architectures. Results demonstrate that the proposed approach can significantly and universally improve translation quality by reducing translation errors on low-frequency words. Encouragingly, our approach achieves 28.2 and 33.9 BLEU points on the WMT14 English-German and WMT16 Romanian-English datasets, respectively. Our code, data, and trained models are available at https://github.com/longyuewangdcu/RLFW-NAT",
    "checked": true,
    "id": "d2197b6b3453d9bcb460588e077853020e2e0cdc",
    "semantic_title": "",
    "citation_count": 31,
    "authors": [
      "Liang Ding",
      "Longyue Wang",
      "Xuebo Liu",
      "Derek F. Wong",
      "Dacheng Tao",
      "Zhaopeng Tu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.267": {
    "title": "G-Transformer for Document-Level Machine Translation",
    "volume": "long",
    "abstract": "Document-level MT models are still far from satisfactory. Existing work extend translation unit from single sentence to multiple sentences. However, study shows that when we further enlarge the translation unit to a whole document, supervised training of Transformer can fail. In this paper, we find such failure is not caused by overfitting, but by sticking around local minima during training. Our analysis shows that the increased complexity of target-to-source attention is a reason for the failure. As a solution, we propose G-Transformer, introducing locality assumption as an inductive bias into Transformer, reducing the hypothesis space of the attention from target to source. Experiments show that G-Transformer converges faster and more stably than Transformer, achieving new state-of-the-art BLEU scores for both nonpretraining and pre-training settings on three benchmark datasets",
    "checked": true,
    "id": "b0de1d5fe394226cec0a59d783ab739eb52da76f",
    "semantic_title": "",
    "citation_count": 31,
    "authors": [
      "Guangsheng Bao",
      "Yue Zhang",
      "Zhiyang Teng",
      "Boxing Chen",
      "Weihua Luo"
    ]
  },
  "https://aclanthology.org/2021.acl-long.268": {
    "title": "Prevent the Language Model from being Overconfident in Neural Machine Translation",
    "volume": "long",
    "abstract": "The Neural Machine Translation (NMT) model is essentially a joint language model conditioned on both the source sentence and partial translation. Therefore, the NMT model naturally involves the mechanism of the Language Model (LM) that predicts the next token only based on partial translation. Despite its success, NMT still suffers from the hallucination problem, generating fluent but inadequate translations. The main reason is that NMT pays excessive attention to the partial translation while neglecting the source sentence to some extent, namely overconfidence of the LM. Accordingly, we define the Margin between the NMT and the LM, calculated by subtracting the predicted probability of the LM from that of the NMT model for each token. The Margin is negatively correlated to the overconfidence degree of the LM. Based on the property, we propose a Margin-based Token-level Objective (MTO) and a Margin-based Sentence-level Objective (MSO) to maximize the Margin for preventing the LM from being overconfident. Experiments on WMT14 English-to-German, WMT19 Chinese-to-English, and WMT14 English-to-French translation tasks demonstrate the effectiveness of our approach, with 1.36, 1.50, and 0.63 BLEU improvements, respectively, compared to the Transformer baseline. The human evaluation further verifies that our approaches improve translation adequacy as well as fluency",
    "checked": true,
    "id": "82b57e0ed286fd8bc591c77b5301c1414055244c",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Mengqi Miao",
      "Fandong Meng",
      "Yijin Liu",
      "Xiao-Hua Zhou",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2021.acl-long.269": {
    "title": "Towards Emotional Support Dialog Systems",
    "volume": "long",
    "abstract": "Emotional support is a crucial ability for many conversation scenarios, including social interactions, mental health support, and customer service chats. Following reasonable procedures and using various support skills can help to effectively provide support. However, due to the lack of a well-designed task and corpora of effective emotional support conversations, research on building emotional support into dialog systems remains lacking. In this paper, we define the Emotional Support Conversation (ESC) task and propose an ESC Framework, which is grounded on the Helping Skills Theory. We construct an Emotion Support Conversation dataset (ESConv) with rich annotation (especially support strategy) in a help-seeker and supporter mode. To ensure a corpus of high-quality conversations that provide examples of effective emotional support, we take extensive effort to design training tutorials for supporters and several mechanisms for quality control during data collection. Finally, we evaluate state-of-the-art dialog models with respect to the ability to provide emotional support. Our results show the importance of support strategies in providing effective emotional support and the utility of ESConv in training more emotional support systems",
    "checked": true,
    "id": "96e7f77ed0101ac3c7c4dc41601563ce0bc8889b",
    "semantic_title": "",
    "citation_count": 66,
    "authors": [
      "Siyang Liu",
      "Chujie Zheng",
      "Orianna Demasi",
      "Sahand Sabour",
      "Yu Li",
      "Zhou Yu",
      "Yong Jiang",
      "Minlie Huang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.270": {
    "title": "Novel Slot Detection: A Benchmark for Discovering Unknown Slot Types in the Task-Oriented Dialogue System",
    "volume": "long",
    "abstract": "Existing slot filling models can only recognize pre-defined in-domain slot types from a limited slot set. In the practical application, a reliable dialogue system should know what it does not know. In this paper, we introduce a new task, Novel Slot Detection (NSD), in the task-oriented dialogue system. NSD aims to discover unknown or out-of-domain slot types to strengthen the capability of a dialogue system based on in-domain training data. Besides, we construct two public NSD datasets, propose several strong NSD baselines, and establish a benchmark for future work. Finally, we conduct exhaustive experiments and qualitative analysis to comprehend key challenges and provide new guidance for future directions",
    "checked": true,
    "id": "60edc2750936fcea191f0bb835119743b11b76d2",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Yanan Wu",
      "Zhiyuan Zeng",
      "Keqing He",
      "Hong Xu",
      "Yuanmeng Yan",
      "Huixing Jiang",
      "Weiran Xu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.271": {
    "title": "GTM: A Generative Triple-wise Model for Conversational Question Generation",
    "volume": "long",
    "abstract": "Generating some appealing questions in open-domain conversations is an effective way to improve human-machine interactions and lead the topic to a broader or deeper direction. To avoid dull or deviated questions, some researchers tried to utilize answer, the “future” information, to guide question generation. However, they separate a post-question-answer (PQA) triple into two parts: post-question (PQ) and question-answer (QA) pairs, which may hurt the overall coherence. Besides, the QA relationship is modeled as a one-to-one mapping that is not reasonable in open-domain conversations. To tackle these problems, we propose a generative triple-wise model with hierarchical variations for open-domain conversational question generation (CQG). Latent variables in three hierarchies are used to represent the shared background of a triple and one-to-many semantic mappings in both PQ and QA pairs. Experimental results on a large-scale CQG dataset show that our method significantly improves the quality of questions in terms of fluency, coherence and diversity over competitive baselines",
    "checked": true,
    "id": "b7ca0b0fa14f8a01596f027ec0eb91e2635bd15c",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Lei Shen",
      "Fandong Meng",
      "Jinchao Zhang",
      "Yang Feng",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2021.acl-long.272": {
    "title": "Diversifying Dialog Generation via Adaptive Label Smoothing",
    "volume": "long",
    "abstract": "Neural dialogue generation models trained with the one-hot target distribution suffer from the over-confidence issue, which leads to poor generation diversity as widely reported in the literature. Although existing approaches such as label smoothing can alleviate this issue, they fail to adapt to diverse dialog contexts. In this paper, we propose an Adaptive Label Smoothing (AdaLabel) approach that can adaptively estimate a target label distribution at each time step for different contexts. The maximum probability in the predicted distribution is used to modify the soft target distribution produced by a novel light-weight bi-directional decoder module. The resulting target distribution is aware of both previous and future contexts and is adjusted to avoid over-training the dialogue model. Our model can be trained in an endto-end manner. Extensive experiments on two benchmark datasets show that our approach outperforms various competitive baselines in producing diverse responses",
    "checked": true,
    "id": "94772377a9c08ad81e506240f844534b6669b8e9",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Yida Wang",
      "Yinhe Zheng",
      "Yong Jiang",
      "Minlie Huang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.273": {
    "title": "Out-of-Scope Intent Detection with Self-Supervision and Discriminative Training",
    "volume": "long",
    "abstract": "Out-of-scope intent detection is of practical importance in task-oriented dialogue systems. Since the distribution of outlier utterances is arbitrary and unknown in the training stage, existing methods commonly rely on strong assumptions on data distribution such as mixture of Gaussians to make inference, resulting in either complex multi-step training procedures or hand-crafted rules such as confidence threshold selection for outlier detection. In this paper, we propose a simple yet effective method to train an out-of-scope intent classifier in a fully end-to-end manner by simulating the test scenario in training, which requires no assumption on data distribution and no additional post-processing or threshold setting. Specifically, we construct a set of pseudo outliers in the training stage, by generating synthetic outliers using inliner features via self-supervision and sampling out-of-scope sentences from easily available open-domain datasets. The pseudo outliers are used to train a discriminative classifier that can be directly applied to and generalize well on the test task. We evaluate our method extensively on four benchmark dialogue datasets and observe significant improvements over state-of-the-art approaches. Our code has been released at https://github.com/liam0949/DCLOOS",
    "checked": true,
    "id": "68c506d3d7e830df15226c020638af2e5fdd7a98",
    "semantic_title": "",
    "citation_count": 31,
    "authors": [
      "Li-Ming Zhan",
      "Haowen Liang",
      "Bo Liu",
      "Lu Fan",
      "Xiao-Ming Wu",
      "Albert Y.S. Lam"
    ]
  },
  "https://aclanthology.org/2021.acl-long.274": {
    "title": "Document-level Event Extraction via Heterogeneous Graph-based Interaction Model with a Tracker",
    "volume": "long",
    "abstract": "Document-level event extraction aims to recognize event information from a whole piece of article. Existing methods are not effective due to two challenges of this task: a) the target event arguments are scattered across sentences; b) the correlation among events in a document is non-trivial to model. In this paper, we propose Heterogeneous Graph-based Interaction Model with a Tracker (GIT) to solve the aforementioned two challenges. For the first challenge, GIT constructs a heterogeneous graph interaction network to capture global interactions among different sentences and entity mentions. For the second, GIT introduces a Tracker module to track the extracted events and hence capture the interdependency among the events. Experiments on a large-scale dataset (Zheng et al, 2019) show GIT outperforms the previous methods by 2.8 F1. Further analysis reveals is effective in extracting multiple correlated events and event arguments that scatter across the document",
    "checked": true,
    "id": "67615300b2bd2aa0f0566ee1e3a5e7d5bd6450b3",
    "semantic_title": "",
    "citation_count": 27,
    "authors": [
      "Runxin Xu",
      "Tianyu Liu",
      "Lei Li",
      "Baobao Chang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.275": {
    "title": "Nested Named Entity Recognition via Explicitly Excluding the Influence of the Best Path",
    "volume": "long",
    "abstract": "This paper presents a novel method for nested named entity recognition. As a layered method, our method extends the prior second-best path recognition method by explicitly excluding the influence of the best path. Our method maintains a set of hidden states at each time step and selectively leverages them to build a different potential function for recognition at each level. In addition, we demonstrate that recognizing innermost entities first results in better performance than the conventional outermost entities first scheme. We provide extensive experimental results on ACE2004, ACE2005, and GENIA datasets to show the effectiveness and efficiency of our proposed method",
    "checked": true,
    "id": "2a964485a195bb2f65e129dec6976d26ef2803e3",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Yiran Wang",
      "Hiroyuki Shindo",
      "Yuji Matsumoto",
      "Taro Watanabe"
    ]
  },
  "https://aclanthology.org/2021.acl-long.276": {
    "title": "LearnDA: Learnable Knowledge-Guided Data Augmentation for Event Causality Identification",
    "volume": "long",
    "abstract": "Modern models for event causality identification (ECI) are mainly based on supervised learning, which are prone to the data lacking problem. Unfortunately, the existing NLP-related augmentation methods cannot directly produce available data required for this task. To solve the data lacking problem, we introduce a new approach to augment training data for event causality identification, by iteratively generating new examples and classifying event causality in a dual learning framework. On the one hand, our approach is knowledge guided, which can leverage existing knowledge bases to generate well-formed new sentences. On the other hand, our approach employs a dual mechanism, which is a learnable augmentation framework, and can interactively adjust the generation process to generate task-related sentences. Experimental results on two benchmarks EventStoryLine and Causal-TimeBank show that 1) our method can augment suitable task-related training data for ECI; 2) our method outperforms previous methods on EventStoryLine and Causal-TimeBank (+2.5 and +2.1 points on F1 value respectively)",
    "checked": true,
    "id": "1351a827a5039586a3b3e27865ab8ceda342a235",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Xinyu Zuo",
      "Pengfei Cao",
      "Yubo Chen",
      "Kang Liu",
      "Jun Zhao",
      "Weihua Peng",
      "Yuguang Chen"
    ]
  },
  "https://aclanthology.org/2021.acl-long.277": {
    "title": "Revisiting the Negative Data of Distantly Supervised Relation Extraction",
    "volume": "long",
    "abstract": "Distantly supervision automatically generates plenty of training samples for relation extraction. However, it also incurs two major problems: noisy labels and imbalanced training data. Previous works focus more on reducing wrongly labeled relations (false positives) while few explore the missing relations that are caused by incompleteness of knowledge base (false negatives). Furthermore, the quantity of negative labels overwhelmingly surpasses the positive ones in previous problem formulations. In this paper, we first provide a thorough analysis of the above challenges caused by negative data. Next, we formulate the problem of relation extraction into as a positive unlabeled learning task to alleviate false negative problem. Thirdly, we propose a pipeline approach, dubbed ReRe, that first performs sentence classification with relational labels and then extracts the subjects/objects. Experimental results show that the proposed method consistently outperforms existing approaches and remains excellent performance even learned with a large quantity of false positive samples. Source code is available online at https://github.com/redreamality/RERE-relation-extraction",
    "checked": true,
    "id": "ddbab45dd9dbd5672730042ebb592be9cd6b19fb",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Chenhao Xie",
      "Jiaqing Liang",
      "Jingping Liu",
      "Chengsong Huang",
      "Wenhao Huang",
      "Yanghua Xiao"
    ]
  },
  "https://aclanthology.org/2021.acl-long.278": {
    "title": "Knowing the No-match: Entity Alignment with Dangling Cases",
    "volume": "long",
    "abstract": "This paper studies a new problem setting of entity alignment for knowledge graphs (KGs). Since KGs possess different sets of entities, there could be entities that cannot find alignment across them, leading to the problem of dangling entities. As the first attempt to this problem, we construct a new dataset and design a multi-task learning framework for both entity alignment and dangling entity detection. The framework can opt to abstain from predicting alignment for the detected dangling entities. We propose three techniques for dangling entity detection that are based on the distribution of nearest-neighbor distances, i.e., nearest neighbor classification, marginal ranking and background ranking. After detecting and removing dangling entities, an incorporated entity alignment model in our framework can provide more robust alignment for remaining entities. Comprehensive experiments and analyses demonstrate the effectiveness of our framework. We further discover that the dangling entity detection module can, in turn, improve alignment learning and the final performance. The contributed resource is publicly available to foster further research",
    "checked": true,
    "id": "30a677b3e4d1a52fffe84463f832929c48e27856",
    "semantic_title": "",
    "citation_count": 21,
    "authors": [
      "Zequn Sun",
      "Muhao Chen",
      "Wei Hu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.279": {
    "title": "Superbizarre Is Not Superb: Derivational Morphology Improves BERT's Interpretation of Complex Words",
    "volume": "long",
    "abstract": "How does the input segmentation of pretrained language models (PLMs) affect their interpretations of complex words? We present the first study investigating this question, taking BERT as the example PLM and focusing on its semantic representations of English derivatives. We show that PLMs can be interpreted as serial dual-route models, i.e., the meanings of complex words are either stored or else need to be computed from the subwords, which implies that maximally meaningful input tokens should allow for the best generalization on new words. This hypothesis is confirmed by a series of semantic probing tasks on which DelBERT (Derivation leveraging BERT), a model with derivational input segmentation, substantially outperforms BERT with WordPiece segmentation. Our results suggest that the generalization capabilities of PLMs could be further improved if a morphologically-informed vocabulary of input tokens were used",
    "checked": true,
    "id": "fcfeaba7f8aacc4198d3028d0c7ab6d6b7679224",
    "semantic_title": "",
    "citation_count": 17,
    "authors": [
      "Valentin Hofmann",
      "Janet Pierrehumbert",
      "Hinrich Schütze"
    ]
  },
  "https://aclanthology.org/2021.acl-long.280": {
    "title": "BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models Identify Analogies?",
    "volume": "long",
    "abstract": "Analogies play a central role in human commonsense reasoning. The ability to recognize analogies such as “eye is to seeing what ear is to hearing”, sometimes referred to as analogical proportions, shape how we structure knowledge and understand language. Surprisingly, however, the task of identifying such analogies has not yet received much attention in the language model era. In this paper, we analyze the capabilities of transformer-based language models on this unsupervised task, using benchmarks obtained from educational settings, as well as more commonly used datasets. We find that off-the-shelf language models can identify analogies to a certain extent, but struggle with abstract and complex relations, and results are highly sensitive to model architecture and hyperparameters. Overall the best results were obtained with GPT-2 and RoBERTa, while configurations using BERT were not able to outperform word embedding models. Our results raise important questions for future work about how, and to what extent, pre-trained language models capture knowledge about abstract semantic relations",
    "checked": true,
    "id": "465491b0507e107f248a8277ac17248c2ff8f915",
    "semantic_title": "",
    "citation_count": 34,
    "authors": [
      "Asahi Ushio",
      "Luis Espinosa Anke",
      "Steven Schockaert",
      "Jose Camacho-Collados"
    ]
  },
  "https://aclanthology.org/2021.acl-long.281": {
    "title": "Exploring the Representation of Word Meanings in Context: A Case Study on Homonymy and Synonymy",
    "volume": "long",
    "abstract": "This paper presents a multilingual study of word meaning representations in context. We assess the ability of both static and contextualized models to adequately represent different lexical-semantic relations, such as homonymy and synonymy. To do so, we created a new multilingual dataset that allows us to perform a controlled evaluation of several factors such as the impact of the surrounding context or the overlap between words, conveying the same or different senses. A systematic assessment on four scenarios shows that the best monolingual models based on Transformers can adequately disambiguate homonyms in context. However, as they rely heavily on context, these models fail at representing words with different senses when occurring in similar sentences. Experiments are performed in Galician, Portuguese, English, and Spanish, and both the dataset (with more than 3,000 evaluation items) and new models are freely released with this study",
    "checked": true,
    "id": "4ea7c7e911b0d96838bd57a3f5e79028e0f9a1b4",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Marcos Garcia"
    ]
  },
  "https://aclanthology.org/2021.acl-long.282": {
    "title": "Measuring Fine-Grained Domain Relevance of Terms: A Hierarchical Core-Fringe Approach",
    "volume": "long",
    "abstract": "We propose to measure fine-grained domain relevance– the degree that a term is relevant to a broad (e.g., computer science) or narrow (e.g., deep learning) domain. Such measurement is crucial for many downstream tasks in natural language processing. To handle long-tail terms, we build a core-anchored semantic graph, which uses core terms with rich description information to bridge the vast remaining fringe terms semantically. To support a fine-grained domain without relying on a matching corpus for supervision, we develop hierarchical core-fringe learning, which learns core and fringe terms jointly in a semi-supervised manner contextualized in the hierarchy of the domain. To reduce expensive human efforts, we employ automatic annotation and hierarchical positive-unlabeled learning. Our approach applies to big or small domains, covers head or tail terms, and requires little human effort. Extensive experiments demonstrate that our methods outperform strong baselines and even surpass professional human performance",
    "checked": true,
    "id": "c09d3857e732c7c44980115b4c0a0d9e4129d584",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Jie Huang",
      "Kevin Chang",
      "JinJun Xiong",
      "Wen-mei Hwu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.283": {
    "title": "HERALD: An Annotation Efficient Method to Detect User Disengagement in Social Conversations",
    "volume": "long",
    "abstract": "Open-domain dialog systems have a user-centric goal: to provide humans with an engaging conversation experience. User engagement is one of the most important metrics for evaluating open-domain dialog systems, and could also be used as real-time feedback to benefit dialog policy learning. Existing work on detecting user disengagement typically requires hand-labeling many dialog samples. We propose HERALD, an efficient annotation framework that reframes the training data annotation process as a denoising problem. Specifically, instead of manually labeling training samples, we first use a set of labeling heuristics to label training samples automatically. We then denoise the weakly labeled data using the Shapley algorithm. Finally, we use the denoised data to train a user engagement detector. Our experiments show that HERALD improves annotation efficiency significantly and achieves 86% user disengagement detection accuracy in two dialog corpora",
    "checked": true,
    "id": "e6a7f3344685cde0acfc2a6be13242c0c441eb05",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Weixin Liang",
      "Kai-Hui Liang",
      "Zhou Yu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.284": {
    "title": "Value-Agnostic Conversational Semantic Parsing",
    "volume": "long",
    "abstract": "Conversational semantic parsers map user utterances to executable programs given dialogue histories composed of previous utterances, programs, and system responses. Existing parsers typically condition on rich representations of history that include the complete set of values and computations previously discussed. We propose a model that abstracts over values to focus prediction on type- and function-level context. This approach provides a compact encoding of dialogue histories and predicted programs, improving generalization and computational efficiency. Our model incorporates several other components, including an atomic span copy operation and structural enforcement of well-formedness constraints on predicted programs, that are particularly advantageous in the low-data regime. Trained on the SMCalFlow and TreeDST datasets, our model outperforms prior work by 7.3% and 10.6% respectively in terms of absolute accuracy. Trained on only a thousand examples from each dataset, it outperforms strong baselines by 12.4% and 6.4%. These results indicate that simple representations are key to effective generalization in conversational semantic parsing",
    "checked": true,
    "id": "1a272eb83fbf3082ca6d6009963186bb23254b03",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Emmanouil Antonios Platanios",
      "Adam Pauls",
      "Subhro Roy",
      "Yuchen Zhang",
      "Alexander Kyte",
      "Alan Guo",
      "Sam Thomson",
      "Jayant Krishnamurthy",
      "Jason Wolfe",
      "Jacob Andreas",
      "Dan Klein"
    ]
  },
  "https://aclanthology.org/2021.acl-long.285": {
    "title": "MPC-BERT: A Pre-Trained Language Model for Multi-Party Conversation Understanding",
    "volume": "long",
    "abstract": "Recently, various neural models for multi-party conversation (MPC) have achieved impressive improvements on a variety of tasks such as addressee recognition, speaker identification and response prediction. However, these existing methods on MPC usually represent interlocutors and utterances individually and ignore the inherent complicated structure in MPC which may provide crucial interlocutor and utterance semantics and would enhance the conversation understanding process. To this end, we present MPC-BERT, a pre-trained model for MPC understanding that considers learning who says what to whom in a unified model with several elaborated self-supervised tasks. Particularly, these tasks can be generally categorized into (1) interlocutor structure modeling including reply-to utterance recognition, identical speaker searching and pointer consistency distinction, and (2) utterance semantics modeling including masked shared utterance restoration and shared node detection. We evaluate MPC-BERT on three downstream tasks including addressee recognition, speaker identification and response selection. Experimental results show that MPC-BERT outperforms previous methods by large margins and achieves new state-of-the-art performance on all three downstream tasks at two benchmarks",
    "checked": true,
    "id": "51b9f8aef39de4b6db820b5c4b5bca14fc32aa4d",
    "semantic_title": "",
    "citation_count": 19,
    "authors": [
      "Jia-Chen Gu",
      "Chongyang Tao",
      "Zhenhua Ling",
      "Can Xu",
      "Xiubo Geng",
      "Daxin Jiang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.286": {
    "title": "Best of Both Worlds: Making High Accuracy Non-incremental Transformer-based Disfluency Detection Incremental",
    "volume": "long",
    "abstract": "While Transformer-based text classifiers pre-trained on large volumes of text have yielded significant improvements on a wide range of computational linguistics tasks, their implementations have been unsuitable for live incremental processing thus far, operating only on the level of complete sentence inputs. We address the challenge of introducing methods for word-by-word left-to-right incremental processing to Transformers such as BERT, models without an intrinsic sense of linear order. We modify the training method and live decoding of non-incremental models to detect speech disfluencies with minimum latency and without pre-segmentation of dialogue acts. We experiment with several decoding methods to predict the rightward context of the word currently being processed using a GPT-2 language model and apply a BERT-based disfluency detector to sequences, including predicted words. We show our method of incrementalising Transformers maintains most of their high non-incremental performance while operating strictly incrementally. We also evaluate our models’ incremental performance to establish the trade-off between incremental performance and final performance, using different prediction strategies. We apply our system to incremental speech recognition results as they arrive into a live system and achieve state-of-the-art results in this setting",
    "checked": true,
    "id": "6c7b337f535eb32d47c651c5464c5a52e9a453cb",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Morteza Rohanian",
      "Julian Hough"
    ]
  },
  "https://aclanthology.org/2021.acl-long.287": {
    "title": "NeuralWOZ: Learning to Collect Task-Oriented Dialogue via Model-Based Simulation",
    "volume": "long",
    "abstract": "We propose NeuralWOZ, a novel dialogue collection framework that uses model-based dialogue simulation. NeuralWOZ has two pipelined models, Collector and Labeler. Collector generates dialogues from (1) user’s goal instructions, which are the user context and task constraints in natural language, and (2) system’s API call results, which is a list of possible query responses for user requests from the given knowledge base. Labeler annotates the generated dialogue by formulating the annotation as a multiple-choice problem, in which the candidate labels are extracted from goal instructions and API call results. We demonstrate the effectiveness of the proposed method in the zero-shot domain transfer learning for dialogue state tracking. In the evaluation, the synthetic dialogue corpus generated from NeuralWOZ achieves a new state-of-the-art with improvements of 4.4% point joint goal accuracy on average across domains, and improvements of 5.7% point of zero-shot coverage against the MultiWOZ 2.1 dataset",
    "checked": true,
    "id": "7ccd4960b0dd1d72f87b12a739442bf4c87e70dc",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Sungdong Kim",
      "Minsuk Chang",
      "Sang-Woo Lee"
    ]
  },
  "https://aclanthology.org/2021.acl-long.288": {
    "title": "CDRNN: Discovering Complex Dynamics in Human Language Processing",
    "volume": "long",
    "abstract": "The human mind is a dynamical system, yet many analysis techniques used to study it are limited in their ability to capture the complex dynamics that may characterize mental processes. This study proposes the continuous-time deconvolutional regressive neural network (CDRNN), a deep neural extension of continuous-time deconvolutional regression (Shain & Schuler, 2021) that jointly captures time-varying, non-linear, and delayed influences of predictors (e.g. word surprisal) on the response (e.g. reading time). Despite this flexibility, CDRNN is interpretable and able to illuminate patterns in human cognition that are otherwise difficult to study. Behavioral and fMRI experiments reveal detailed and plausible estimates of human language processing dynamics that generalize better than CDR and other baselines, supporting a potential role for CDRNN in studying human language processing",
    "checked": true,
    "id": "030865ddd9d8615d5e9b96e293f02a1a62fd95f6",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Cory Shain"
    ]
  },
  "https://aclanthology.org/2021.acl-long.289": {
    "title": "Structural Guidance for Transformer Language Models",
    "volume": "long",
    "abstract": "Transformer-based language models pre-trained on large amounts of text data have proven remarkably successful in learning generic transferable linguistic representations. Here we study whether structural guidance leads to more human-like systematic linguistic generalization in Transformer language models without resorting to pre-training on very large amounts of data. We explore two general ideas. The “Generative Parsing” idea jointly models the incremental parse and word sequence as part of the same sequence modeling task. The “Structural Scaffold” idea guides the language model’s representation via additional structure loss that separately predicts the incremental constituency parse. We train the proposed models along with a vanilla Transformer language model baseline on a 14 million-token and a 46 million-token subset of the BLLIP dataset, and evaluate models’ syntactic generalization performances on SG Test Suites and sized BLiMP. Experiment results across two benchmarks suggest converging evidence that generative structural supervisions can induce more robust and humanlike linguistic generalization in Transformer language models without the need for data intensive pre-training",
    "checked": true,
    "id": "f75fd3288a60233bbc766037941f5b370fae9168",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Peng Qian",
      "Tahira Naseem",
      "Roger Levy",
      "Ramón Fernandez Astudillo"
    ]
  },
  "https://aclanthology.org/2021.acl-long.290": {
    "title": "Surprisal Estimators for Human Reading Times Need Character Models",
    "volume": "long",
    "abstract": "While the use of character models has been popular in NLP applications, it has not been explored much in the context of psycholinguistic modeling. This paper presents a character model that can be applied to a structural parser-based processing model to calculate word generation probabilities. Experimental results show that surprisal estimates from a structural processing model using this character model deliver substantially better fits to self-paced reading, eye-tracking, and fMRI data than those from large-scale language models trained on much more data. This may suggest that the proposed processing model provides a more humanlike account of sentence processing, which assumes a larger role of morphology, phonotactics, and orthographic complexity than was previously thought",
    "checked": true,
    "id": "72c007a987ea26bec00ba150b725e5eb54cd6c8c",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Byung-Doh Oh",
      "Christian Clark",
      "William Schuler"
    ]
  },
  "https://aclanthology.org/2021.acl-long.291": {
    "title": "CogAlign: Learning to Align Textual Neural Representations to Cognitive Language Processing Signals",
    "volume": "long",
    "abstract": "Most previous studies integrate cognitive language processing signals (e.g., eye-tracking or EEG data) into neural models of natural language processing (NLP) just by directly concatenating word embeddings with cognitive features, ignoring the gap between the two modalities (i.e., textual vs. cognitive) and noise in cognitive features. In this paper, we propose a CogAlign approach to these issues, which learns to align textual neural representations to cognitive features. In CogAlign, we use a shared encoder equipped with a modality discriminator to alternatively encode textual and cognitive inputs to capture their differences and commonalities. Additionally, a text-aware attention mechanism is proposed to detect task-related information and to avoid using noise in cognitive features. Experimental results on three NLP tasks, namely named entity recognition, sentiment analysis and relation extraction, show that CogAlign achieves significant improvements with multiple cognitive features over state-of-the-art models on public datasets. Moreover, our model is able to transfer cognitive information to other datasets that do not have any cognitive processing signals",
    "checked": true,
    "id": "ce0feb3e46f82e491bf2178ee6d86a284d9c3e79",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Yuqi Ren",
      "Deyi Xiong"
    ]
  },
  "https://aclanthology.org/2021.acl-long.292": {
    "title": "Self-Attention Networks Can Process Bounded Hierarchical Languages",
    "volume": "long",
    "abstract": "Despite their impressive performance in NLP, self-attention networks were recently proved to be limited for processing formal languages with hierarchical structure, such as Dyck-k, the language consisting of well-nested parentheses of k types. This suggested that natural language can be approximated well with models that are too weak for formal languages, or that the role of hierarchy and recursion in natural language might be limited. We qualify this implication by proving that self-attention networks can process Dyck-(k, D), the subset of Dyck-k with depth bounded by D, which arguably better captures the bounded hierarchical structure of natural language. Specifically, we construct a hard-attention network with D+1 layers and O(log k) memory size (per token per layer) that recognizes Dyck-(k, D), and a soft-attention network with two layers and O(log k) memory size that generates Dyck-(k, D). Experiments show that self-attention networks trained on Dyck-(k, D) generalize to longer inputs with near-perfect accuracy, and also verify the theoretical memory advantage of self-attention networks over recurrent networks",
    "checked": true,
    "id": "957424a1f3319a2aab1a91539a00e712477b4b4a",
    "semantic_title": "",
    "citation_count": 17,
    "authors": [
      "Shunyu Yao",
      "Binghui Peng",
      "Christos Papadimitriou",
      "Karthik Narasimhan"
    ]
  },
  "https://aclanthology.org/2021.acl-long.293": {
    "title": "TextSETTR: Few-Shot Text Style Extraction and Tunable Targeted Restyling",
    "volume": "long",
    "abstract": "We present a novel approach to the problem of text style transfer. Unlike previous approaches requiring style-labeled training data, our method makes use of readily-available unlabeled text by relying on the implicit connection in style between adjacent sentences, and uses labeled data only at inference time. We adapt T5 (Raffel et al., 2020), a strong pretrained text-to-text model, to extract a style vector from text and use it to condition the decoder to perform style transfer. As our label-free training results in a style vector space encoding many facets of style, we recast transfers as “targeted restyling” vector operations that adjust specific attributes of the input while preserving others. We demonstrate that training on unlabeled Amazon reviews data results in a model that is competitive on sentiment transfer, even compared to models trained fully on labeled data. Furthermore, applying our novel method to a diverse corpus of unlabeled web text results in a single model capable of transferring along multiple dimensions of style (dialect, emotiveness, formality, politeness, sentiment) despite no additional training and using only a handful of exemplars at inference time",
    "checked": true,
    "id": "d31b5b60e1b3af84cd977da8db0ed4faeb79e7f7",
    "semantic_title": "",
    "citation_count": 28,
    "authors": [
      "Parker Riley",
      "Noah Constant",
      "Mandy Guo",
      "Girish Kumar",
      "David Uthus",
      "Zarana Parekh"
    ]
  },
  "https://aclanthology.org/2021.acl-long.294": {
    "title": "H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences",
    "volume": "long",
    "abstract": "We describe an efficient hierarchical method to compute attention in the Transformer architecture. The proposed attention mechanism exploits a matrix structure similar to the Hierarchical Matrix (H-Matrix) developed by the numerical analysis community, and has linear run time and memory complexity. We perform extensive experiments to show that the inductive bias embodied by our hierarchical attention is effective in capturing the hierarchical structure in the sequences typical for natural language and vision tasks. Our method is superior to alternative sub-quadratic proposals by over +6 points on average on the Long Range Arena benchmark. It also sets a new SOTA test perplexity on One-Billion Word dataset with 5x fewer model parameters than that of the previous-best Transformer-based models",
    "checked": true,
    "id": "dbf53ece1a6a8860e41ff5f721c72ceb0fb18dd6",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Zhenhai Zhu",
      "Radu Soricut"
    ]
  },
  "https://aclanthology.org/2021.acl-long.295": {
    "title": "Making Pre-trained Language Models Better Few-shot Learners",
    "volume": "long",
    "abstract": "The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF—better few-shot fine-tuning of language models—a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30% absolute improvement, and 11% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning",
    "checked": true,
    "id": "85e7d63f75c0916bd350a229e040c5fbb1472e7a",
    "semantic_title": "",
    "citation_count": 808,
    "authors": [
      "Tianyu Gao",
      "Adam Fisch",
      "Danqi Chen"
    ]
  },
  "https://aclanthology.org/2021.acl-long.296": {
    "title": "A Sweet Rabbit Hole by DARCY: Using Honeypots to Detect Universal Trigger's Adversarial Attacks",
    "volume": "long",
    "abstract": "The Universal Trigger (UniTrigger) is a recently-proposed powerful adversarial textual attack method. Utilizing a learning-based mechanism, UniTrigger generates a fixed phrase that, when added to any benign inputs, can drop the prediction accuracy of a textual neural network (NN) model to near zero on a target class. To defend against this attack that can cause significant harm, in this paper, we borrow the “honeypot” concept from the cybersecurity community and propose DARCY, a honeypot-based defense framework against UniTrigger. DARCY greedily searches and injects multiple trapdoors into an NN model to “bait and catch” potential attacks. Through comprehensive experiments across four public datasets, we show that DARCY detects UniTrigger’s adversarial attacks with up to 99% TPR and less than 2% FPR in most cases, while maintaining the prediction accuracy (in F1) for clean inputs within a 1% margin. We also demonstrate that DARCY with multiple trapdoors is also robust to a diverse set of attack scenarios with attackers’ varying levels of knowledge and skills. We release the source code of DARCY at: https://github.com/lethaiq/ACL2021-DARCY-HoneypotDefenseNLP",
    "checked": true,
    "id": "c94529aff09763b607b7594197f1bbf01c006759",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Thai Le",
      "Noseong Park",
      "Dongwon Lee"
    ]
  },
  "https://aclanthology.org/2021.acl-long.297": {
    "title": "Towards Propagation Uncertainty: Edge-enhanced Bayesian Graph Convolutional Networks for Rumor Detection",
    "volume": "long",
    "abstract": "Detecting rumors on social media is a very critical task with significant implications to the economy, public health, etc. Previous works generally capture effective features from texts and the propagation structure. However, the uncertainty caused by unreliable relations in the propagation structure is common and inevitable due to wily rumor producers and the limited collection of spread data. Most approaches neglect it and may seriously limit the learning of features. Towards this issue, this paper makes the first attempt to explore propagation uncertainty for rumor detection. Specifically, we propose a novel Edge-enhanced Bayesian Graph Convolutional Network (EBGCN) to capture robust structural features. The model adaptively rethinks the reliability of latent relations by adopting a Bayesian approach. Besides, we design a new edge-wise consistency training framework to optimize the model by enforcing consistency on relations. Experiments on three public benchmark datasets demonstrate that the proposed model achieves better performance than baseline methods on both rumor detection and early rumor detection tasks",
    "checked": true,
    "id": "10825c7414dd839be051682a5ae653aa7ba8e08c",
    "semantic_title": "",
    "citation_count": 19,
    "authors": [
      "Lingwei Wei",
      "Dou Hu",
      "Wei Zhou",
      "Zhaojuan Yue",
      "Songlin Hu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.298": {
    "title": "Label-Specific Dual Graph Neural Network for Multi-Label Text Classification",
    "volume": "long",
    "abstract": "Multi-label text classification is one of the fundamental tasks in natural language processing. Previous studies have difficulties to distinguish similar labels well because they learn the same document representations for different labels, that is they do not explicitly extract label-specific semantic components from documents. Moreover, they do not fully explore the high-order interactions among these semantic components, which is very helpful to predict tail labels. In this paper, we propose a novel label-specific dual graph neural network (LDGN), which incorporates category information to learn label-specific components from documents, and employs dual Graph Convolution Network (GCN) to model complete and adaptive interactions among these components based on the statistical label co-occurrence and dynamic reconstruction graph in a joint way. Experimental results on three benchmark datasets demonstrate that LDGN significantly outperforms the state-of-the-art models, and also achieves better performance with respect to tail labels",
    "checked": true,
    "id": "efb570104b0a148315a1eedd8e9a5dc106143514",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Qianwen Ma",
      "Chunyuan Yuan",
      "Wei Zhou",
      "Songlin Hu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.299": {
    "title": "TAN-NTM: Topic Attention Networks for Neural Topic Modeling",
    "volume": "long",
    "abstract": "Topic models have been widely used to learn text representations and gain insight into document corpora. To perform topic discovery, most existing neural models either take document bag-of-words (BoW) or sequence of tokens as input followed by variational inference and BoW reconstruction to learn topic-word distribution. However, leveraging topic-word distribution for learning better features during document encoding has not been explored much. To this end, we develop a framework TAN-NTM, which processes document as a sequence of tokens through a LSTM whose contextual outputs are attended in a topic-aware manner. We propose a novel attention mechanism which factors in topic-word distribution to enable the model to attend on relevant words that convey topic related cues. The output of topic attention module is then used to carry out variational inference. We perform extensive ablations and experiments resulting in ~9-15 percentage improvement over score of existing SOTA topic models in NPMI coherence on several benchmark datasets - 20Newsgroups, Yelp Review Polarity and AGNews. Further, we show that our method learns better latent document-topic features compared to existing topic models through improvement on two downstream tasks: document classification and topic guided keyphrase generation",
    "checked": true,
    "id": "68768d4dca5b71e88094d63d546da9574b09c7b2",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Madhur Panwar",
      "Shashank Shailabh",
      "Milan Aggarwal",
      "Balaji Krishnamurthy"
    ]
  },
  "https://aclanthology.org/2021.acl-long.300": {
    "title": "Cross-language Sentence Selection via Data Augmentation and Rationale Training",
    "volume": "long",
    "abstract": "This paper proposes an approach to cross-language sentence selection in a low-resource setting. It uses data augmentation and negative sampling techniques on noisy parallel sentence data to directly learn a cross-lingual embedding-based query relevance model. Results show that this approach performs as well as or better than multiple state-of-the-art machine translation + monolingual retrieval systems trained on the same parallel data. Moreover, when a rationale training secondary objective is applied to encourage the model to match word alignment hints from a phrase-based statistical machine translation model, consistent improvements are seen across three language pairs (English-Somali, English-Swahili and English-Tagalog) over a variety of state-of-the-art baselines",
    "checked": true,
    "id": "3df7970d24ac31744b772455307feb71d3d092b0",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Yanda Chen",
      "Chris Kedzie",
      "Suraj Nair",
      "Petra Galuscakova",
      "Rui Zhang",
      "Douglas Oard",
      "Kathleen McKeown"
    ]
  },
  "https://aclanthology.org/2021.acl-long.301": {
    "title": "A Neural Model for Joint Document and Snippet Ranking in Question Answering for Large Document Collections",
    "volume": "long",
    "abstract": "Question answering (QA) systems for large document collections typically use pipelines that (i) retrieve possibly relevant documents, (ii) re-rank them, (iii) rank paragraphs or other snippets of the top-ranked documents, and (iv) select spans of the top-ranked snippets as exact answers. Pipelines are conceptually simple, but errors propagate from one component to the next, without later components being able to revise earlier decisions. We present an architecture for joint document and snippet ranking, the two middle stages, which leverages the intuition that relevant documents have good snippets and good snippets come from relevant documents. The architecture is general and can be used with any neural text relevance ranker. We experiment with two main instantiations of the architecture, based on POSIT-DRMM (PDRMM) and a BERT-based ranker. Experiments on biomedical data from BIOASQ show that our joint models vastly outperform the pipelines in snippet retrieval, the main goal for QA, with fewer trainable parameters, also remaining competitive in document retrieval. Furthermore, our joint PDRMM-based model is competitive with BERT-based models, despite using orders of magnitude fewer parameters. These claims are also supported by human evaluation on two test batches of BIOASQ. To test our key findings on another dataset, we modified the Natural Questions dataset so that it can also be used for document and snippet retrieval. Our joint PDRMM-based model again outperforms the corresponding pipeline in snippet retrieval on the modified Natural Questions dataset, even though it performs worse than the pipeline in document retrieval. We make our code and the modified Natural Questions dataset publicly available",
    "checked": true,
    "id": "068eb6b3797d5807b744d9326e7ebb50769e4b4d",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Dimitris Pappas",
      "Ion Androutsopoulos"
    ]
  },
  "https://aclanthology.org/2021.acl-long.302": {
    "title": "W-RST: Towards a Weighted RST-style Discourse Framework",
    "volume": "long",
    "abstract": "Aiming for a better integration of data-driven and linguistically-inspired approaches, we explore whether RST Nuclearity, assigning a binary assessment of importance between text segments, can be replaced by automatically generated, real-valued scores, in what we call a Weighted-RST framework. In particular, we find that weighted discourse trees from auxiliary tasks can benefit key NLP downstream applications, compared to nuclearity-centered approaches. We further show that real-valued importance distributions partially and interestingly align with the assessment and uncertainty of human annotators",
    "checked": true,
    "id": "04821f01347bd58be330d5dc12e510f586b23286",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Patrick Huber",
      "Wen Xiao",
      "Giuseppe Carenini"
    ]
  },
  "https://aclanthology.org/2021.acl-long.303": {
    "title": "ABCD: A Graph Framework to Convert Complex Sentences to a Covering Set of Simple Sentences",
    "volume": "long",
    "abstract": "Atomic clauses are fundamental text units for understanding complex sentences. Identifying the atomic sentences within complex sentences is important for applications such as summarization, argument mining, discourse analysis, discourse parsing, and question answering. Previous work mainly relies on rule-based methods dependent on parsing. We propose a new task to decompose each complex sentence into simple sentences derived from the tensed clauses in the source, and a novel problem formulation as a graph edit task. Our neural model learns to Accept, Break, Copy or Drop elements of a graph that combines word adjacency and grammatical dependencies. The full processing pipeline includes modules for graph construction, graph editing, and sentence generation from the output graph. We introduce DeSSE, a new dataset designed to train and evaluate complex sentence decomposition, and MinWiki, a subset of MinWikiSplit. ABCD achieves comparable performance as two parsing baselines on MinWiki. On DeSSE, which has a more even balance of complex sentence types, our model achieves higher accuracy on the number of atomic sentences than an encoder-decoder baseline. Results include a detailed error analysis",
    "checked": true,
    "id": "862b9f75e4b249ea20b3598ea3a4375b3dfff368",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Yanjun Gao",
      "Ting-Hao Huang",
      "Rebecca J. Passonneau"
    ]
  },
  "https://aclanthology.org/2021.acl-long.304": {
    "title": "Which Linguist Invented the Lightbulb? Presupposition Verification for Question-Answering",
    "volume": "long",
    "abstract": "Many Question-Answering (QA) datasets contain unanswerable questions, but their treatment in QA systems remains primitive. Our analysis of the Natural Questions (Kwiatkowski et al. 2019) dataset reveals that a substantial portion of unanswerable questions (~21%) can be explained based on the presence of unverifiable presuppositions. Through a user preference study, we demonstrate that the oracle behavior of our proposed system—which provides responses based on presupposition failure—is preferred over the oracle behavior of existing QA systems. Then, we present a novel framework for implementing such a system in three steps: presupposition generation, presupposition verification, and explanation generation, reporting progress on each. Finally, we show that a simple modification of adding presuppositions and their verifiability to the input of a competitive end-to-end QA system yields modest gains in QA performance and unanswerability detection, demonstrating the promise of our approach",
    "checked": true,
    "id": "c2aa448fea1d02ede55f0cbfeebe7ba9d08fe16d",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Najoung Kim",
      "Ellie Pavlick",
      "Burcu Karagol Ayan",
      "Deepak Ramachandran"
    ]
  },
  "https://aclanthology.org/2021.acl-long.305": {
    "title": "Adversarial Learning for Discourse Rhetorical Structure Parsing",
    "volume": "long",
    "abstract": "Text-level discourse rhetorical structure (DRS) parsing is known to be challenging due to the notorious lack of training data. Although recent top-down DRS parsers can better leverage global document context and have achieved certain success, the performance is still far from perfect. To our knowledge, all previous DRS parsers make local decisions for either bottom-up node composition or top-down split point ranking at each time step, and largely ignore DRS parsing from the global view point. Obviously, it is not sufficient to build an entire DRS tree only through these local decisions. In this work, we present our insight on evaluating the pros and cons of the entire DRS tree for global optimization. Specifically, based on recent well-performing top-down frameworks, we introduce a novel method to transform both gold standard and predicted constituency trees into tree diagrams with two color channels. After that, we learn an adversarial bot between gold and fake tree diagrams to estimate the generated DRS trees from a global perspective. We perform experiments on both RST-DT and CDTB corpora and use the original Parseval for performance evaluation. The experimental results show that our parser can substantially improve the performance when compared with previous state-of-the-art parsers",
    "checked": true,
    "id": "75ca384b69494c16ad7e814d069745be854082bb",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Longyin Zhang",
      "Fang Kong",
      "Guodong Zhou"
    ]
  },
  "https://aclanthology.org/2021.acl-long.306": {
    "title": "Exploring Discourse Structures for Argument Impact Classification",
    "volume": "long",
    "abstract": "Discourse relations among arguments reveal logical structures of a debate conversation. However, no prior work has explicitly studied how the sequence of discourse relations influence a claim’s impact. This paper empirically shows that the discourse relations between two arguments along the context path are essential factors for identifying the persuasive power of an argument. We further propose DisCOC to inject and fuse the sentence-level structural discourse information with contextualized features derived from large-scale language models. Experimental results and extensive analysis show that the attention and gate mechanisms that explicitly model contexts and texts can indeed help the argument impact classification task defined by Durmus et al. (2019), and discourse structures among the context path of the claim to be classified can further boost the performance",
    "checked": true,
    "id": "d484fb6b39bc8a5ae3ae1039ea551b031c215e20",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Xin Liu",
      "Jiefu Ou",
      "Yangqiu Song",
      "Xin Jiang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.307": {
    "title": "Point, Disambiguate and Copy: Incorporating Bilingual Dictionaries for Neural Machine Translation",
    "volume": "long",
    "abstract": "This paper proposes a sophisticated neural architecture to incorporate bilingual dictionaries into Neural Machine Translation (NMT) models. By introducing three novel components: Pointer, Disambiguator, and Copier, our method PDC achieves the following merits inherently compared with previous efforts: (1) Pointer leverages the semantic information from bilingual dictionaries, for the first time, to better locate source words whose translation in dictionaries can potentially be used; (2) Disambiguator synthesizes contextual information from the source view and the target view, both of which contribute to distinguishing the proper translation of a specific source word from multiple candidates in dictionaries; (3) Copier systematically connects Pointer and Disambiguator based on a hierarchical copy mechanism seamlessly integrated with Transformer, thereby building an end-to-end architecture that could avoid error propagation problems in alternative pipe-line methods. The experimental results on Chinese-English and English-Japanese benchmarks demonstrate the PDC’s overall superiority and effectiveness of each component",
    "checked": true,
    "id": "1105f26529bd6ab73e866b13513b36b591470985",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Tong Zhang",
      "Long Zhang",
      "Wei Ye",
      "Bo Li",
      "Jinan Sun",
      "Xiaoyu Zhu",
      "Wen Zhao",
      "Shikun Zhang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.308": {
    "title": "VECO: Variable and Flexible Cross-lingual Pre-training for Language Understanding and Generation",
    "volume": "long",
    "abstract": "Existing work in multilingual pretraining has demonstrated the potential of cross-lingual transferability by training a unified Transformer encoder for multiple languages. However, much of this work only relies on the shared vocabulary and bilingual contexts to encourage the correlation across languages, which is loose and implicit for aligning the contextual representations between languages. In this paper, we plug a cross-attention module into the Transformer encoder to explicitly build the interdependence between languages. It can effectively avoid the degeneration of predicting masked words only conditioned on the context in its own language. More importantly, when fine-tuning on downstream tasks, the cross-attention module can be plugged in or out on-demand, thus naturally benefiting a wider range of cross-lingual tasks, from language understanding to generation. As a result, the proposed cross-lingual model delivers new state-of-the-art results on various cross-lingual understanding tasks of the XTREME benchmark, covering text classification, sequence labeling, question answering, and sentence retrieval. For cross-lingual generation tasks, it also outperforms all existing cross-lingual models and state-of-the-art Transformer variants on WMT14 English-to-German and English-to-French translation datasets, with gains of up to 1 2 BLEU",
    "checked": true,
    "id": "cd5a4c3ad315a40ff3d4456a550be818bc5ec7af",
    "semantic_title": "",
    "citation_count": 37,
    "authors": [
      "Fuli Luo",
      "Wei Wang",
      "Jiahao Liu",
      "Yijia Liu",
      "Bin Bi",
      "Songfang Huang",
      "Fei Huang",
      "Luo Si"
    ]
  },
  "https://aclanthology.org/2021.acl-long.309": {
    "title": "A unified approach to sentence segmentation of punctuated text in many languages",
    "volume": "long",
    "abstract": "The sentence is a fundamental unit of text processing. Yet sentences in the wild are commonly encountered not in isolation, but unsegmented within larger paragraphs and documents. Therefore, the first step in many NLP pipelines is sentence segmentation. Despite its importance, this step is the subject of relatively little research. There are no standard test sets or even methods for evaluation, leaving researchers and engineers without a clear footing for evaluating and selecting models for the task. Existing tools have relatively small language coverage, and efforts to extend them to other languages are often ad hoc. We introduce a modern context-based modeling approach that provides a solution to the problem of segmenting punctuated text in many languages, and show how it can be trained on noisily-annotated data. We also establish a new 23-language multilingual evaluation set. Our approach exceeds high baselines set by existing methods on prior English corpora (WSJ and Brown corpora), and also performs well on average on our new evaluation set. We release our tool, ersatz, as open source",
    "checked": true,
    "id": "b95e1b0b716e36b7a594031192948956fc20fdf6",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Rachel Wicks",
      "Matt Post"
    ]
  },
  "https://aclanthology.org/2021.acl-long.310": {
    "title": "Towards User-Driven Neural Machine Translation",
    "volume": "long",
    "abstract": "A good translation should not only translate the original content semantically, but also incarnate personal traits of the original text. For a real-world neural machine translation (NMT) system, these user traits (e.g., topic preference, stylistic characteristics and expression habits) can be preserved in user behavior (e.g., historical inputs). However, current NMT systems marginally consider the user behavior due to: 1) the difficulty of modeling user portraits in zero-shot scenarios, and 2) the lack of user-behavior annotated parallel dataset. To fill this gap, we introduce a novel framework called user-driven NMT. Specifically, a cache-based module and a user-driven contrastive learning method are proposed to offer NMT the ability to capture potential user traits from their historical inputs under a zero-shot learning fashion. Furthermore, we contribute the first Chinese-English parallel corpus annotated with user behavior called UDT-Corpus. Experimental results confirm that the proposed user-driven NMT can generate user-specific translations",
    "checked": true,
    "id": "147974cbbe6fb6a5ab6372210a10ae8bbe139842",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Huan Lin",
      "Liang Yao",
      "Baosong Yang",
      "Dayiheng Liu",
      "Haibo Zhang",
      "Weihua Luo",
      "Degen Huang",
      "Jinsong Su"
    ]
  },
  "https://aclanthology.org/2021.acl-long.311": {
    "title": "End-to-End Lexically Constrained Machine Translation for Morphologically Rich Languages",
    "volume": "long",
    "abstract": "Lexically constrained machine translation allows the user to manipulate the output sentence by enforcing the presence or absence of certain words and phrases. Although current approaches can enforce terms to appear in the translation, they often struggle to make the constraint word form agree with the rest of the generated output. Our manual analysis shows that 46% of the errors in the output of a baseline constrained model for English to Czech translation are related to agreement. We investigate mechanisms to allow neural machine translation to infer the correct word inflection given lemmatized constraints. In particular, we focus on methods based on training the model with constraints provided as part of the input sequence. Our experiments on English-Czech language pair show that this approach improves translation of constrained terms in both automatic and manual evaluation by reducing errors in agreement. Our approach thus eliminates inflection errors, without introducing new errors or decreasing overall quality of the translation",
    "checked": true,
    "id": "9d030241eb35c38bacf689df6e0dfa9dadc789da",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Josef Jon",
      "João Paulo Aires",
      "Dusan Varis",
      "Ondřej Bojar"
    ]
  },
  "https://aclanthology.org/2021.acl-long.312": {
    "title": "Handling Extreme Class Imbalance in Technical Logbook Datasets",
    "volume": "long",
    "abstract": "Technical logbooks are a challenging and under-explored text type in automated event identification. These texts are typically short and written in non-standard yet technical language, posing challenges to off-the-shelf NLP pipelines. The granularity of issue types described in these datasets additionally leads to class imbalance, making it challenging for models to accurately predict which issue each logbook entry describes. In this paper we focus on the problem of technical issue classification by considering logbook datasets from the automotive, aviation, and facilities maintenance domains. We adapt a feedback strategy from computer vision for handling extreme class imbalance, which resamples the training data based on its error in the prediction process. Our experiments show that with statistical significance this feedback strategy provides the best results for four different neural network models trained across a suite of seven different technical logbook datasets from distinct technical domains. The feedback strategy is also generic and could be applied to any learning problem with substantial class imbalances",
    "checked": true,
    "id": "300450616c609f97aea9dbf44891f3e5e2fe7dae",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Farhad Akhbardeh",
      "Cecilia Ovesdotter Alm",
      "Marcos Zampieri",
      "Travis Desell"
    ]
  },
  "https://aclanthology.org/2021.acl-long.313": {
    "title": "ILDC for CJPE: Indian Legal Documents Corpus for Court Judgment Prediction and Explanation",
    "volume": "long",
    "abstract": "An automated system that could assist a judge in predicting the outcome of a case would help expedite the judicial process. For such a system to be practically useful, predictions by the system should be explainable. To promote research in developing such a system, we introduce ILDC (Indian Legal Documents Corpus). ILDC is a large corpus of 35k Indian Supreme Court cases annotated with original court decisions. A portion of the corpus (a separate test set) is annotated with gold standard explanations by legal experts. Based on ILDC, we propose the task of Court Judgment Prediction and Explanation (CJPE). The task requires an automated system to predict an explainable outcome of a case. We experiment with a battery of baseline models for case predictions and propose a hierarchical occlusion based model for explainability. Our best prediction model has an accuracy of 78% versus 94% for human legal experts, pointing towards the complexity of the prediction task. The analysis of explanations by the proposed algorithm reveals a significant difference in the point of view of the algorithm and legal experts for explaining the judgments, pointing towards scope for future research",
    "checked": true,
    "id": "61a5ae33fc34efcdbc710004554ec57e607ce75e",
    "semantic_title": "",
    "citation_count": 37,
    "authors": [
      "Vijit Malik",
      "Rishabh Sanjay",
      "Shubham Kumar Nigam",
      "Kripabandhu Ghosh",
      "Shouvik Kumar Guha",
      "Arnab Bhattacharya",
      "Ashutosh Modi"
    ]
  },
  "https://aclanthology.org/2021.acl-long.314": {
    "title": "Supporting Cognitive and Emotional Empathic Writing of Students",
    "volume": "long",
    "abstract": "We present an annotation approach to capturing emotional and cognitive empathy in student-written peer reviews on business models in German. We propose an annotation scheme that allows us to model emotional and cognitive empathy scores based on three types of review components. Also, we conducted an annotation study with three annotators based on 92 student essays to evaluate our annotation scheme. The obtained inter-rater agreement of α=0.79 for the components and the multi-π=0.41 for the empathy scores indicate that the proposed annotation scheme successfully guides annotators to a substantial to moderate agreement. Moreover, we trained predictive models to detect the annotated empathy structures and embedded them in an adaptive writing support system for students to receive individual empathy feedback independent of an instructor, time, and location. We evaluated our tool in a peer learning exercise with 58 students and found promising results for perceived empathy skill learning, perceived feedback accuracy, and intention to use. Finally, we present our freely available corpus of 500 empathy-annotated, student-written peer reviews on business models and our annotation guidelines to encourage future research on the design and development of empathy support systems",
    "checked": true,
    "id": "d899c0150ecec3f967030f5cde717c2a0d1ede6d",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Thiemo Wambsganss",
      "Christina Niklaus",
      "Matthias Söllner",
      "Siegfried Handschuh",
      "Jan Marco Leimeister"
    ]
  },
  "https://aclanthology.org/2021.acl-long.315": {
    "title": "Dual Reader-Parser on Hybrid Textual and Tabular Evidence for Open Domain Question Answering",
    "volume": "long",
    "abstract": "The current state-of-the-art generative models for open-domain question answering (ODQA) have focused on generating direct answers from unstructured textual information. However, a large amount of world’s knowledge is stored in structured databases, and need to be accessed using query languages such as SQL. Furthermore, query languages can answer questions that require complex reasoning, as well as offering full explainability. In this paper, we propose a hybrid framework that takes both textual and tabular evidences as input and generates either direct answers or SQL queries depending on which form could better answer the question. The generated SQL queries can then be executed on the associated databases to obtain the final answers. To the best of our knowledge, this is the first paper that applies Text2SQL to ODQA tasks. Empirically, we demonstrate that on several ODQA datasets, the hybrid methods consistently outperforms the baseline models that only takes homogeneous input by a large margin. Specifically we achieve the state-of-the-art performance on OpenSQuAD dataset using a T5-base model. In a detailed analysis, we demonstrate that the being able to generate structural SQL queries can always bring gains, especially for those questions that requires complex reasoning",
    "checked": true,
    "id": "285428daf56a1530b11b6deb515f10c8f1dc5739",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Alexander Hanbo Li",
      "Patrick Ng",
      "Peng Xu",
      "Henghui Zhu",
      "Zhiguo Wang",
      "Bing Xiang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.316": {
    "title": "Generation-Augmented Retrieval for Open-Domain Question Answering",
    "volume": "long",
    "abstract": "We propose Generation-Augmented Retrieval (GAR) for answering open-domain questions, which augments a query through text generation of heuristically discovered relevant contexts without external resources as supervision. We demonstrate that the generated contexts substantially enrich the semantics of the queries and GAR with sparse representations (BM25) achieves comparable or better performance than state-of-the-art dense retrieval methods such as DPR. We show that generating diverse contexts for a query is beneficial as fusing their results consistently yields better retrieval accuracy. Moreover, as sparse and dense representations are often complementary, GAR can be easily combined with DPR to achieve even better performance. GAR achieves state-of-the-art performance on Natural Questions and TriviaQA datasets under the extractive QA setup when equipped with an extractive reader, and consistently outperforms other retrieval methods when the same generative reader is used",
    "checked": true,
    "id": "7d429ad73fc311a0a29ab9d02482b4e6b059d81f",
    "semantic_title": "",
    "citation_count": 95,
    "authors": [
      "Yuning Mao",
      "Pengcheng He",
      "Xiaodong Liu",
      "Yelong Shen",
      "Jianfeng Gao",
      "Jiawei Han",
      "Weizhu Chen"
    ]
  },
  "https://aclanthology.org/2021.acl-long.317": {
    "title": "Check It Again:Progressive Visual Question Answering via Visual Entailment",
    "volume": "long",
    "abstract": "While sophisticated neural-based models have achieved remarkable success in Visual Question Answering (VQA), these models tend to answer questions only according to superficial correlations between question and answer. Several recent approaches have been developed to address this language priors problem. However, most of them predict the correct answer according to one best output without checking the authenticity of answers. Besides, they only explore the interaction between image and question, ignoring the semantics of candidate answers. In this paper, we propose a select-and-rerank (SAR) progressive framework based on Visual Entailment. Specifically, we first select the candidate answers relevant to the question or the image, then we rerank the candidate answers by a visual entailment task, which verifies whether the image semantically entails the synthetic statement of the question and each candidate answer. Experimental results show the effectiveness of our proposed framework, which establishes a new state-of-the-art accuracy on VQA-CP v2 with a 7.55% improvement",
    "checked": true,
    "id": "b8cb9b5c2964f7f2e0783d328801420ec74e0365",
    "semantic_title": "",
    "citation_count": 17,
    "authors": [
      "Qingyi Si",
      "Zheng Lin",
      "Ming yu Zheng",
      "Peng Fu",
      "Weiping Wang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.318": {
    "title": "A Mutual Information Maximization Approach for the Spurious Solution Problem in Weakly Supervised Question Answering",
    "volume": "long",
    "abstract": "Weakly supervised question answering usually has only the final answers as supervision signals while the correct solutions to derive the answers are not provided. This setting gives rise to the spurious solution problem: there may exist many spurious solutions that coincidentally derive the correct answer, but training on such solutions can hurt model performance (e.g., producing wrong solutions or answers). For example, for discrete reasoning tasks as on DROP, there may exist many equations to derive a numeric answer, and typically only one of them is correct. Previous learning methods mostly filter out spurious solutions with heuristics or using model confidence, but do not explicitly exploit the semantic correlations between a question and its solution. In this paper, to alleviate the spurious solution problem, we propose to explicitly exploit such semantic correlations by maximizing the mutual information between question-answer pairs and predicted solutions. Extensive experiments on four question answering datasets show that our method significantly outperforms previous learning methods in terms of task performance and is more effective in training models to produce correct solutions",
    "checked": true,
    "id": "3fd92feb43d61645429ec4a2c80b1304a3c8bd69",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Zhihong Shao",
      "Lifeng Shang",
      "Qun Liu",
      "Minlie Huang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.319": {
    "title": "Breaking Down Walls of Text: How Can NLP Benefit Consumer Privacy?",
    "volume": "long",
    "abstract": "Privacy plays a crucial role in preserving democratic ideals and personal autonomy. The dominant legal approach to privacy in many jurisdictions is the “Notice and Choice” paradigm, where privacy policies are the primary instrument used to convey information to users. However, privacy policies are long and complex documents that are difficult for users to read and comprehend. We discuss how language technologies can play an important role in addressing this information gap, reporting on initial progress towards helping three specific categories of stakeholders take advantage of digital privacy policies: consumers, enterprises, and regulators. Our goal is to provide a roadmap for the development and use of language technologies to empower users to reclaim control over their privacy, limit privacy harms, and rally research efforts from the community towards addressing an issue with large social impact. We highlight many remaining opportunities to develop language technologies that are more precise or nuanced in the way in which they use the text of privacy policies",
    "checked": true,
    "id": "c39fc494cb84b260eaa33cceda2b76512f3701a5",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Abhilasha Ravichander",
      "Alan W Black",
      "Thomas Norton",
      "Shomir Wilson",
      "Norman Sadeh"
    ]
  },
  "https://aclanthology.org/2021.acl-long.320": {
    "title": "Supporting Land Reuse of Former Open Pit Mining Sites using Text Classification and Active Learning",
    "volume": "long",
    "abstract": "Open pit mines left many regions worldwide inhospitable or uninhabitable. Many sites are left behind in a hazardous or contaminated state, show remnants of waste, or have other restrictions imposed upon them, e.g., for the protection of human or nature. Such information has to be permanently managed in order to reuse those areas in the future. In this work we present and evaluate an automated workflow for supporting the post-mining management of former lignite open pit mines in the eastern part of Germany, where prior to any planned land reuse, aforementioned information has to be acquired to ensure the safety and validity of such an endeavor. Usually, this information is found in expert reports, either in the form of paper documents, or in the best case as digitized unstructured text—all of them in German language. However, due to the size and complexity of these documents, any inquiry is tedious and time-consuming, thereby slowing down or even obstructing the reuse of related areas. Since no training data is available, we employ active learning in order to perform multi-label sentence classification for two categories of restrictions and seven categories of topics. The final system integrates optical character recognition (OCR), active-learning-based text classification, and geographic information system visualization in order to effectively extract, query, and visualize this information for any area of interest. Active learning and text classification results are twofold: Whereas the restriction categories were reasonably accurate (>0.85 F1), the seven topic-oriented categories seemed to be complex even for human annotators and achieved mediocre evaluation scores (<0.70 F1)",
    "checked": true,
    "id": "02e1d548c243ad4053b6795f009011ac38ed8457",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Christopher Schröder",
      "Kim Bürgl",
      "Yves Annanias",
      "Andreas Niekler",
      "Lydia Müller",
      "Daniel Wiegreffe",
      "Christian Bender",
      "Christoph Mengs",
      "Gerik Scheuermann",
      "Gerhard Heyer"
    ]
  },
  "https://aclanthology.org/2021.acl-long.321": {
    "title": "Reliability Testing for Natural Language Processing Systems",
    "volume": "long",
    "abstract": "Questions of fairness, robustness, and transparency are paramount to address before deploying NLP systems. Central to these concerns is the question of reliability: Can NLP systems reliably treat different demographics fairly and function correctly in diverse and noisy environments? To address this, we argue for the need for reliability testing and contextualize it among existing work on improving accountability. We show how adversarial attacks can be reframed for this goal, via a framework for developing reliability tests. We argue that reliability testing — with an emphasis on interdisciplinary collaboration — will enable rigorous and targeted testing, and aid in the enactment and enforcement of industry standards",
    "checked": true,
    "id": "0f71a4fa9736ae916e6aef53045f6be4c901b0ff",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Samson Tan",
      "Shafiq Joty",
      "Kathy Baxter",
      "Araz Taeihagh",
      "Gregory A. Bennett",
      "Min-Yen Kan"
    ]
  },
  "https://aclanthology.org/2021.acl-long.322": {
    "title": "Learning Language and Multimodal Privacy-Preserving Markers of Mood from Mobile Data",
    "volume": "long",
    "abstract": "Mental health conditions remain underdiagnosed even in countries with common access to advanced medical care. The ability to accurately and efficiently predict mood from easily collectible data has several important implications for the early detection, intervention, and treatment of mental health disorders. One promising data source to help monitor human behavior is daily smartphone usage. However, care must be taken to summarize behaviors without identifying the user through personal (e.g., personally identifiable information) or protected (e.g., race, gender) attributes. In this paper, we study behavioral markers of daily mood using a recent dataset of mobile behaviors from adolescent populations at high risk of suicidal behaviors. Using computational models, we find that language and multimodal representations of mobile typed text (spanning typed characters, words, keystroke timings, and app usage) are predictive of daily mood. However, we find that models trained to predict mood often also capture private user identities in their intermediate representations. To tackle this problem, we evaluate approaches that obfuscate user identity while remaining predictive. By combining multimodal representations with privacy-preserving learning, we are able to push forward the performance-privacy frontier",
    "checked": true,
    "id": "cc760dd9f909db13a1725e3c1f4975f47be358ef",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Paul Pu Liang",
      "Terrance Liu",
      "Anna Cai",
      "Michal Muszynski",
      "Ryo Ishii",
      "Nick Allen",
      "Randy Auerbach",
      "David Brent",
      "Ruslan Salakhutdinov",
      "Louis-Philippe Morency"
    ]
  },
  "https://aclanthology.org/2021.acl-long.323": {
    "title": "Anonymisation Models for Text Data: State of the art, Challenges and Future Directions",
    "volume": "long",
    "abstract": "This position paper investigates the problem of automated text anonymisation, which is a prerequisite for secure sharing of documents containing sensitive information about individuals. We summarise the key concepts behind text anonymisation and provide a review of current approaches. Anonymisation methods have so far been developed in two fields with little mutual interaction, namely natural language processing and privacy-preserving data publishing. Based on a case study, we outline the benefits and limitations of these approaches and discuss a number of open challenges, such as (1) how to account for multiple types of semantic inferences, (2) how to strike a balance between disclosure risk and data utility and (3) how to evaluate the quality of the resulting anonymisation. We lay out a case for moving beyond sequence labelling models and incorporate explicit measures of disclosure risk into the text anonymisation process",
    "checked": true,
    "id": "9262b3fdd11e1fa73bc96cf4db4de6bfb5f6f6a2",
    "semantic_title": "",
    "citation_count": 24,
    "authors": [
      "Pierre Lison",
      "Ildikó Pilán",
      "David Sanchez",
      "Montserrat Batet",
      "Lilja Øvrelid"
    ]
  },
  "https://aclanthology.org/2021.acl-long.324": {
    "title": "End-to-End AMR Coreference Resolution",
    "volume": "long",
    "abstract": "Although parsing to Abstract Meaning Representation (AMR) has become very popular and AMR has been shown effective on the many sentence-level downstream tasks, little work has studied how to generate AMRs that can represent multi-sentence information. We introduce the first end-to-end AMR coreference resolution model in order to build multi-sentence AMRs. Compared with the previous pipeline and rule-based approaches, our model alleviates error propagation and it is more robust for both in-domain and out-domain situations. Besides, the document-level AMRs obtained by our model can significantly improve over the AMRs generated by a rule-based method (Liu et al., 2015) on text summarization",
    "checked": true,
    "id": "e8acb757b782ff0fa12656dd4486efd4a49662ed",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Qiankun Fu",
      "Linfeng Song",
      "Wenyu Du",
      "Yue Zhang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.325": {
    "title": "How is BERT surprised? Layerwise detection of linguistic anomalies",
    "volume": "long",
    "abstract": "Transformer language models have shown remarkable ability in detecting when a word is anomalous in context, but likelihood scores offer no information about the cause of the anomaly. In this work, we use Gaussian models for density estimation at intermediate layers of three language models (BERT, RoBERTa, and XLNet), and evaluate our method on BLiMP, a grammaticality judgement benchmark. In lower layers, surprisal is highly correlated to low token frequency, but this correlation diminishes in upper layers. Next, we gather datasets of morphosyntactic, semantic, and commonsense anomalies from psycholinguistic studies; we find that the best performing model RoBERTa exhibits surprisal in earlier layers when the anomaly is morphosyntactic than when it is semantic, while commonsense anomalies do not exhibit surprisal at any intermediate layer. These results suggest that language models employ separate mechanisms to detect different types of linguistic anomalies",
    "checked": true,
    "id": "528cd7c546315ae57e532ff9f57a674378a5ad6f",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Bai Li",
      "Zining Zhu",
      "Guillaume Thomas",
      "Yang Xu",
      "Frank Rudzicz"
    ]
  },
  "https://aclanthology.org/2021.acl-long.326": {
    "title": "Psycholinguistic Tripartite Graph Network for Personality Detection",
    "volume": "long",
    "abstract": "Most of the recent work on personality detection from online posts adopts multifarious deep neural networks to represent the posts and builds predictive models in a data-driven manner, without the exploitation of psycholinguistic knowledge that may unveil the connections between one’s language use and his psychological traits. In this paper, we propose a psycholinguistic knowledge-based tripartite graph network, TrigNet, which consists of a tripartite graph network and a BERT-based graph initializer. The graph network injects structural psycholinguistic knowledge in LIWC, a computerized instrument for psycholinguistic analysis, by constructing a heterogeneous tripartite graph. The initializer is employed to provide initial embeddings for the graph nodes. To reduce the computational cost in graph learning, we further propose a novel flow graph attention network (GAT) that only transmits messages between neighboring parties in the tripartite graph. Benefiting from the tripartite graph, TrigNet can aggregate post information from a psychological perspective, which is a novel way of exploiting domain knowledge. Extensive experiments on two datasets show that TrigNet outperforms the existing state-of-art model by 3.47 and 2.10 points in average F1. Moreover, the flow GAT reduces the FLOPS and Memory measures by 38% and 32%, respectively, in comparison to the original GAT in our setting",
    "checked": true,
    "id": "c651019b28c5e4f358d72779487bcc17e5ef726b",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Tao Yang",
      "Feifan Yang",
      "Haolan Ouyang",
      "Xiaojun Quan"
    ]
  },
  "https://aclanthology.org/2021.acl-long.327": {
    "title": "Verb Metaphor Detection via Contextual Relation Learning",
    "volume": "long",
    "abstract": "Correct natural language understanding requires computers to distinguish the literal and metaphorical senses of a word. Recent neu- ral models achieve progress on verb metaphor detection by viewing it as sequence labeling. In this paper, we argue that it is appropriate to view this task as relation classification between a verb and its various contexts. We propose the Metaphor-relation BERT (Mr-BERT) model, which explicitly models the relation between a verb and its grammatical, sentential and semantic contexts. We evaluate our method on the VUA, MOH-X and TroFi datasets. Our method gets competitive results compared with state-of-the-art approaches",
    "checked": true,
    "id": "1c181f93490bc2a411b1ff6266e9b4c65ddefdb2",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Wei Song",
      "Shuhui Zhou",
      "Ruiji Fu",
      "Ting Liu",
      "Lizhen Liu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.328": {
    "title": "Improving Speech Translation by Understanding and Learning from the Auxiliary Text Translation Task",
    "volume": "long",
    "abstract": "Pretraining and multitask learning are widely used to improve the speech translation performance. In this study, we are interested in training a speech translation model along with an auxiliary text translation task. We conduct a detailed analysis to understand the impact of the auxiliary task on the primary task within the multitask learning framework. Our analysis confirms that multitask learning tends to generate similar decoder representations from different modalities and preserve more information from the pretrained text translation modules. We observe minimal negative transfer effect between the two tasks and sharing more parameters is helpful to transfer knowledge from the text task to the speech task. The analysis also reveals that the modality representation difference at the top decoder layers is still not negligible, and those layers are critical for the translation quality. Inspired by these findings, we propose three methods to improve translation quality. First, a parameter sharing and initialization strategy is proposed to enhance information sharing between the tasks. Second, a novel attention-based regularization is proposed for the encoders and pulls the representations from different modalities closer. Third, an online knowledge distillation is proposed to enhance the knowledge transfer from the text to the speech task. Our experiments show that the proposed approach improves translation performance by more than 2 BLEU over a strong baseline and achieves state-of-the-art results on the MuST-C English-German, English-French and English-Spanish language pairs",
    "checked": true,
    "id": "95e838c886346c563005bde16af96b3173b0e016",
    "semantic_title": "",
    "citation_count": 42,
    "authors": [
      "Yun Tang",
      "Juan Pino",
      "Xian Li",
      "Changhan Wang",
      "Dmitriy Genzel"
    ]
  },
  "https://aclanthology.org/2021.acl-long.329": {
    "title": "Probing Toxic Content in Large Pre-Trained Language Models",
    "volume": "long",
    "abstract": "Large pre-trained language models (PTLMs) have been shown to carry biases towards different social groups which leads to the reproduction of stereotypical and toxic content by major NLP systems. We propose a method based on logistic regression classifiers to probe English, French, and Arabic PTLMs and quantify the potentially harmful content that they convey with respect to a set of templates. The templates are prompted by a name of a social group followed by a cause-effect relation. We use PTLMs to predict masked tokens at the end of a sentence in order to examine how likely they enable toxicity towards specific communities. We shed the light on how such negative content can be triggered within unrelated and benign contexts based on evidence from a large-scale study, then we explain how to take advantage of our methodology to assess and mitigate the toxicity transmitted by PTLMs",
    "checked": true,
    "id": "080df61ee1c15ff3c8e5d0d82d60bfd80e372e38",
    "semantic_title": "",
    "citation_count": 32,
    "authors": [
      "Nedjma Ousidhoum",
      "Xinran Zhao",
      "Tianqing Fang",
      "Yangqiu Song",
      "Dit-Yan Yeung"
    ]
  },
  "https://aclanthology.org/2021.acl-long.330": {
    "title": "Societal Biases in Language Generation: Progress and Challenges",
    "volume": "long",
    "abstract": "Technology for language generation has advanced rapidly, spurred by advancements in pre-training large models on massive amounts of data and the need for intelligent agents to communicate in a natural manner. While techniques can effectively generate fluent text, they can also produce undesirable societal biases that can have a disproportionately negative impact on marginalized populations. Language generation presents unique challenges for biases in terms of direct user interaction and the structure of decoding techniques. To better understand these challenges, we present a survey on societal biases in language generation, focusing on how data and techniques contribute to biases and progress towards reducing biases. Motivated by a lack of studies on biases from decoding techniques, we also conduct experiments to quantify the effects of these techniques. By further discussing general trends and open challenges, we call to attention promising directions for research and the importance of fairness and inclusivity considerations for language generation applications",
    "checked": true,
    "id": "76a786b1acd6d1aca56e12a8a1db34569fdf9f3a",
    "semantic_title": "",
    "citation_count": 82,
    "authors": [
      "Emily Sheng",
      "Kai-Wei Chang",
      "Prem Natarajan",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2021.acl-long.331": {
    "title": "Reservoir Transformers",
    "volume": "long",
    "abstract": "We demonstrate that transformers obtain impressive performance even when some of the layers are randomly initialized and never updated. Inspired by old and well-established ideas in machine learning, we explore a variety of non-linear “reservoir” layers interspersed with regular transformer layers, and show improvements in wall-clock compute time until convergence, as well as overall performance, on various machine translation and (masked) language modelling tasks",
    "checked": true,
    "id": "4b30dd65a26e573df9796beb582ba1e1a69f23f7",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Sheng Shen",
      "Alexei Baevski",
      "Ari Morcos",
      "Kurt Keutzer",
      "Michael Auli",
      "Douwe Kiela"
    ]
  },
  "https://aclanthology.org/2021.acl-long.332": {
    "title": "Subsequence Based Deep Active Learning for Named Entity Recognition",
    "volume": "long",
    "abstract": "Active Learning (AL) has been successfully applied to Deep Learning in order to drastically reduce the amount of data required to achieve high performance. Previous works have shown that lightweight architectures for Named Entity Recognition (NER) can achieve optimal performance with only 25% of the original training data. However, these methods do not exploit the sequential nature of language and the heterogeneity of uncertainty within each instance, requiring the labelling of whole sentences. Additionally, this standard method requires that the annotator has access to the full sentence when labelling. In this work, we overcome these limitations by allowing the AL algorithm to query subsequences within sentences, and propagate their labels to other sentences. We achieve highly efficient results on OntoNotes 5.0, only requiring 13% of the original training data, and CoNLL 2003, requiring only 27%. This is an improvement of 39% and 37% compared to querying full sentences",
    "checked": true,
    "id": "45303f4cdc54d27ed0b39753ef77ea3761d94870",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Puria Radmard",
      "Yassir Fathullah",
      "Aldo Lipani"
    ]
  },
  "https://aclanthology.org/2021.acl-long.333": {
    "title": "Convolutions and Self-Attention: Re-interpreting Relative Positions in Pre-trained Language Models",
    "volume": "long",
    "abstract": "In this paper, we detail the relationship between convolutions and self-attention in natural language tasks. We show that relative position embeddings in self-attention layers are equivalent to recently-proposed dynamic lightweight convolutions, and we consider multiple new ways of integrating convolutions into Transformer self-attention. Specifically, we propose composite attention, which unites previous relative position encoding methods under a convolutional framework. We conduct experiments by training BERT with composite attention, finding that convolutions consistently improve performance on multiple downstream tasks, replacing absolute position embeddings. To inform future work, we present results comparing lightweight convolutions, dynamic convolutions, and depthwise-separable convolutions in language model pre-training, considering multiple injection points for convolutions in self-attention layers",
    "checked": true,
    "id": "1dbb523a6555d6e0c5727620e2b57daaa5b79dc0",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Tyler Chang",
      "Yifan Xu",
      "Weijian Xu",
      "Zhuowen Tu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.334": {
    "title": "BinaryBERT: Pushing the Limit of BERT Quantization",
    "volume": "long",
    "abstract": "The rapid development of large pre-trained language models has greatly increased the demand for model compression techniques, among which quantization is a popular solution. In this paper, we propose BinaryBERT, which pushes BERT quantization to the limit by weight binarization. We find that a binary BERT is hard to be trained directly than a ternary counterpart due to its complex and irregular loss landscape. Therefore, we propose ternary weight splitting, which initializes BinaryBERT by equivalently splitting from a half-sized ternary network. The binary model thus inherits the good performance of the ternary one, and can be further enhanced by fine-tuning the new architecture after splitting. Empirical results show that our BinaryBERT has only a slight performance drop compared with the full-precision model while being 24x smaller, achieving the state-of-the-art compression results on the GLUE and SQuAD benchmarks. Code will be released",
    "checked": true,
    "id": "c375e121926db9551f224ff235018ea38bb159b7",
    "semantic_title": "",
    "citation_count": 103,
    "authors": [
      "Haoli Bai",
      "Wei Zhang",
      "Lu Hou",
      "Lifeng Shang",
      "Jin Jin",
      "Xin Jiang",
      "Qun Liu",
      "Michael Lyu",
      "Irwin King"
    ]
  },
  "https://aclanthology.org/2021.acl-long.335": {
    "title": "Are Pretrained Convolutions Better than Pretrained Transformers?",
    "volume": "long",
    "abstract": "In the era of pre-trained language models, Transformers are the de facto choice of model architectures. While recent research has shown promise in entirely convolutional, or CNN, architectures, they have not been explored using the pre-train-fine-tune paradigm. In the context of language models, are convolutional models competitive to Transformers when pre-trained? This paper investigates this research question and presents several interesting findings. Across an extensive set of experiments on 8 datasets/tasks, we find that CNN-based pre-trained models are competitive and outperform their Transformer counterpart in certain scenarios, albeit with caveats. Overall, the findings outlined in this paper suggest that conflating pre-training and architectural advances is misguided and that both advances should be considered independently. We believe our research paves the way for a healthy amount of optimism in alternative architectures",
    "checked": true,
    "id": "642492003112a47b0bf86d60fac5507bc3b35a49",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Yi Tay",
      "Mostafa Dehghani",
      "Jai Prakash Gupta",
      "Vamsi Aribandi",
      "Dara Bahri",
      "Zhen Qin",
      "Donald Metzler"
    ]
  },
  "https://aclanthology.org/2021.acl-long.336": {
    "title": "PairRE: Knowledge Graph Embeddings via Paired Relation Vectors",
    "volume": "long",
    "abstract": "Distance based knowledge graph embedding methods show promising results on link prediction task, on which two topics have been widely studied: one is the ability to handle complex relations, such as N-to-1, 1-to-N and N-to-N, the other is to encode various relation patterns, such as symmetry/antisymmetry. However, the existing methods fail to solve these two problems at the same time, which leads to unsatisfactory results. To mitigate this problem, we propose PairRE, a model with paired vectors for each relation representation. The paired vectors enable an adaptive adjustment of the margin in loss function to fit for different complex relations. Besides, PairRE is capable of encoding three important relation patterns, symmetry/antisymmetry, inverse and composition. Given simple constraints on relation representations, PairRE can encode subrelation further. Experiments on link prediction benchmarks demonstrate the proposed key capabilities of PairRE. Moreover, We set a new state-of-the-art on two knowledge graph datasets of the challenging Open Graph Benchmark",
    "checked": true,
    "id": "1a8a8504722d9a39f17bbaa2968a89acc5cd0c48",
    "semantic_title": "",
    "citation_count": 52,
    "authors": [
      "Linlin Chao",
      "Jianshan He",
      "Taifeng Wang",
      "Wei Chu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.337": {
    "title": "Hierarchy-aware Label Semantics Matching Network for Hierarchical Text Classification",
    "volume": "long",
    "abstract": "Hierarchical text classification is an important yet challenging task due to the complex structure of the label hierarchy. Existing methods ignore the semantic relationship between text and labels, so they cannot make full use of the hierarchical information. To this end, we formulate the text-label semantics relationship as a semantic matching problem and thus propose a hierarchy-aware label semantics matching network (HiMatch). First, we project text semantics and label semantics into a joint embedding space. We then introduce a joint embedding loss and a matching learning loss to model the matching relationship between the text semantics and the label semantics. Our model captures the text-label semantics matching relationship among coarse-grained labels and fine-grained labels in a hierarchy-aware manner. The experimental results on various benchmark datasets verify that our model achieves state-of-the-art results",
    "checked": true,
    "id": "57516ba4a5356154b81a9332010544dce24ee494",
    "semantic_title": "",
    "citation_count": 18,
    "authors": [
      "Haibin Chen",
      "Qianli Ma",
      "Zhenxi Lin",
      "Jiangyue Yan"
    ]
  },
  "https://aclanthology.org/2021.acl-long.338": {
    "title": "HiddenCut: Simple Data Augmentation for Natural Language Understanding with Better Generalizability",
    "volume": "long",
    "abstract": "Fine-tuning large pre-trained models with task-specific data has achieved great success in NLP. However, it has been demonstrated that the majority of information within the self-attention networks is redundant and not utilized effectively during the fine-tuning stage. This leads to inferior results when generalizing the obtained models to out-of-domain distributions. To this end, we propose a simple yet effective data augmentation technique, HiddenCut, to better regularize the model and encourage it to learn more generalizable features. Specifically, contiguous spans within the hidden space are dynamically and strategically dropped during training. Experiments show that our HiddenCut method outperforms the state-of-the-art augmentation methods on the GLUE benchmark, and consistently exhibits superior generalization performances on out-of-distribution and challenging counterexamples. We have publicly released our code at https://github.com/GT-SALT/HiddenCut",
    "checked": true,
    "id": "291016368158f28829c06c0a037e0ca1a6548cca",
    "semantic_title": "",
    "citation_count": 22,
    "authors": [
      "Jiaao Chen",
      "Dinghan Shen",
      "Weizhu Chen",
      "Diyi Yang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.339": {
    "title": "Neural Stylistic Response Generation with Disentangled Latent Variables",
    "volume": "long",
    "abstract": "Generating open-domain conversational responses in the desired style usually suffers from the lack of parallel data in the style. Meanwhile, using monolingual stylistic data to increase style intensity often leads to the expense of decreasing content relevance. In this paper, we propose to disentangle the content and style in latent space by diluting sentence-level information in style representations. Combining the desired style representation and a response content representation will then obtain a stylistic response. Our approach achieves a higher BERT-based style intensity score and comparable BLEU scores, compared with baselines. Human evaluation results show that our approach significantly improves style intensity and maintains content relevance",
    "checked": true,
    "id": "8a88313f341ed2f98ead0f0ec03b5aba576c6efe",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Qingfu Zhu",
      "Wei-Nan Zhang",
      "Ting Liu",
      "William Yang Wang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.340": {
    "title": "Intent Classification and Slot Filling for Privacy Policies",
    "volume": "long",
    "abstract": "Understanding privacy policies is crucial for users as it empowers them to learn about the information that matters to them. Sentences written in a privacy policy document explain privacy practices, and the constituent text spans convey further specific information about that practice. We refer to predicting the privacy practice explained in a sentence as intent classification and identifying the text spans sharing specific information as slot filling. In this work, we propose PolicyIE, an English corpus consisting of 5,250 intent and 11,788 slot annotations spanning 31 privacy policies of websites and mobile applications. PolicyIE corpus is a challenging real-world benchmark with limited labeled examples reflecting the cost of collecting large-scale annotations from domain experts. We present two alternative neural approaches as baselines, (1) intent classification and slot filling as a joint sequence tagging and (2) modeling them as a sequence-to-sequence (Seq2Seq) learning task. The experiment results show that both approaches perform comparably in intent classification, while the Seq2Seq method outperforms the sequence tagging approach in slot filling by a large margin. We perform a detailed error analysis to reveal the challenges of the proposed corpus",
    "checked": true,
    "id": "95d48c41daf7121a4ae815fee21afa375612f57f",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Wasi Ahmad",
      "Jianfeng Chi",
      "Tu Le",
      "Thomas Norton",
      "Yuan Tian",
      "Kai-Wei Chang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.341": {
    "title": "RADDLE: An Evaluation Benchmark and Analysis Platform for Robust Task-oriented Dialog Systems",
    "volume": "long",
    "abstract": "For task-oriented dialog systems to be maximally useful, it must be able to process conversations in a way that is (1) generalizable with a small number of training examples for new task domains, and (2) robust to user input in various styles, modalities, or domains. In pursuit of these goals, we introduce the RADDLE benchmark, a collection of corpora and tools for evaluating the performance of models across a diverse set of domains. By including tasks with limited training data, RADDLE is designed to favor and encourage models with a strong generalization ability. RADDLE also includes a diagnostic checklist that facilitates detailed robustness analysis in aspects such as language variations, speech errors, unseen entities, and out-of-domain utterances. We evaluate recent state-of-the-art systems based on pre-training and fine-tuning, and find that grounded pre-training on heterogeneous dialog corpora performs better than training a separate model per domain. Adversarial training is also proposed to improve model robustness against noisy inputs. Overall, existing models are less than satisfactory in robustness evaluation, which suggests opportunities for future improvement",
    "checked": true,
    "id": "e885821da51a4370b4eb8ecd9f62047f6ee604db",
    "semantic_title": "",
    "citation_count": 34,
    "authors": [
      "Baolin Peng",
      "Chunyuan Li",
      "Zhu Zhang",
      "Chenguang Zhu",
      "Jinchao Li",
      "Jianfeng Gao"
    ]
  },
  "https://aclanthology.org/2021.acl-long.342": {
    "title": "Semantic Representation for Dialogue Modeling",
    "volume": "long",
    "abstract": "Although neural models have achieved competitive results in dialogue systems, they have shown limited ability in representing core semantics, such as ignoring important entities. To this end, we exploit Abstract Meaning Representation (AMR) to help dialogue modeling. Compared with the textual input, AMR explicitly provides core semantic knowledge and reduces data sparsity. We develop an algorithm to construct dialogue-level AMR graphs from sentence-level AMRs and explore two ways to incorporate AMRs into dialogue systems. Experimental results on both dialogue understanding and response generation tasks show the superiority of our model. To our knowledge, we are the first to leverage a formal semantic representation into neural dialogue modeling",
    "checked": true,
    "id": "1c2499f11b4d061f6a5f0a0a50504a31a7d83090",
    "semantic_title": "",
    "citation_count": 24,
    "authors": [
      "Xuefeng Bai",
      "Yulong Chen",
      "Linfeng Song",
      "Yue Zhang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.343": {
    "title": "A Pre-training Strategy for Zero-Resource Response Selection in Knowledge-Grounded Conversations",
    "volume": "long",
    "abstract": "Recently, many studies are emerging towards building a retrieval-based dialogue system that is able to effectively leverage background knowledge (e.g., documents) when conversing with humans. However, it is non-trivial to collect large-scale dialogues that are naturally grounded on the background documents, which hinders the effective and adequate training of knowledge selection and response matching. To overcome the challenge, we consider decomposing the training of the knowledge-grounded response selection into three tasks including: 1) query-passage matching task; 2) query-dialogue history matching task; 3) multi-turn response matching task, and joint learning all these tasks in a unified pre-trained language model. The former two tasks could help the model in knowledge selection and comprehension, while the last task is designed for matching the proper response with the given query and background knowledge (dialogue history). By this means, the model can be learned to select relevant knowledge and distinguish proper response, with the help of ad-hoc retrieval corpora and a large number of ungrounded multi-turn dialogues. Experimental results on two benchmarks of knowledge-grounded response selection indicate that our model can achieve comparable performance with several existing methods that rely on crowd-sourced data for training",
    "checked": true,
    "id": "34a0306bdac2b06b757bbbe2573297171e523616",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Chongyang Tao",
      "Changyu Chen",
      "Jiazhan Feng",
      "Ji-Rong Wen",
      "Rui Yan"
    ]
  },
  "https://aclanthology.org/2021.acl-long.344": {
    "title": "Dependency-driven Relation Extraction with Attentive Graph Convolutional Networks",
    "volume": "long",
    "abstract": "Syntactic information, especially dependency trees, has been widely used by existing studies to improve relation extraction with better semantic guidance for analyzing the context information associated with the given entities. However, most existing studies suffer from the noise in the dependency trees, especially when they are automatically generated, so that intensively leveraging dependency information may introduce confusions to relation classification and necessary pruning is of great importance in this task. In this paper, we propose a dependency-driven approach for relation extraction with attentive graph convolutional networks (A-GCN). In this approach, an attention mechanism upon graph convolutional networks is applied to different contextual words in the dependency tree obtained from an off-the-shelf dependency parser, to distinguish the importance of different word dependencies. Consider that dependency types among words also contain important contextual guidance, which is potentially helpful for relation extraction, we also include the type information in A-GCN modeling. Experimental results on two English benchmark datasets demonstrate the effectiveness of our A-GCN, which outperforms previous studies and achieves state-of-the-art performance on both datasets",
    "checked": true,
    "id": "d1a99b5e200f535b306995fa119884c95fd4516f",
    "semantic_title": "",
    "citation_count": 34,
    "authors": [
      "Yuanhe Tian",
      "Guimin Chen",
      "Yan Song",
      "Xiang Wan"
    ]
  },
  "https://aclanthology.org/2021.acl-long.345": {
    "title": "Evaluating Entity Disambiguation and the Role of Popularity in Retrieval-Based NLP",
    "volume": "long",
    "abstract": "Retrieval is a core component for open-domain NLP tasks. In open-domain tasks, multiple entities can share a name, making disambiguation an inherent yet under-explored problem. We propose an evaluation benchmark for assessing the entity disambiguation capabilities of these retrievers, which we call Ambiguous Entity Retrieval (AmbER) sets. We define an AmbER set as a collection of entities that share a name along with queries about those entities. By covering the set of entities for polysemous names, AmbER sets act as a challenging test of entity disambiguation. We create AmbER sets for three popular open-domain tasks: fact checking, slot filling, and question answering, and evaluate a diverse set of retrievers. We find that the retrievers exhibit popularity bias, significantly under-performing on rarer entities that share a name, e.g., they are twice as likely to retrieve erroneous documents on queries for the less popular entity under the same name. These experiments on AmbER sets show their utility as an evaluation tool and highlight the weaknesses of popular retrieval systems",
    "checked": true,
    "id": "4291fe672cf6dc73e237ca0942fa49beb8c98711",
    "semantic_title": "",
    "citation_count": 19,
    "authors": [
      "Anthony Chen",
      "Pallavi Gudipati",
      "Shayne Longpre",
      "Xiao Ling",
      "Sameer Singh"
    ]
  },
  "https://aclanthology.org/2021.acl-long.346": {
    "title": "Evaluation Examples are not Equally Informative: How should that change NLP Leaderboards?",
    "volume": "long",
    "abstract": "Leaderboards are widely used in NLP and push the field forward. While leaderboards are a straightforward ranking of NLP models, this simplicity can mask nuances in evaluation items (examples) and subjects (NLP models). Rather than replace leaderboards, we advocate a re-imagining so that they better highlight if and where progress is made. Building on educational testing, we create a Bayesian leaderboard model where latent subject skill and latent item difficulty predict correct responses. Using this model, we analyze the ranking reliability of leaderboards. Afterwards, we show the model can guide what to annotate, identify annotation errors, detect overfitting, and identify informative examples. We conclude with recommendations for future benchmark tasks",
    "checked": true,
    "id": "30f233eecca2239ee1dd754914324092e53f8f19",
    "semantic_title": "",
    "citation_count": 34,
    "authors": [
      "Pedro Rodriguez",
      "Joe Barrow",
      "Alexander Miserlis Hoyle",
      "John P. Lalor",
      "Robin Jia",
      "Jordan Boyd-Graber"
    ]
  },
  "https://aclanthology.org/2021.acl-long.347": {
    "title": "Claim Matching Beyond English to Scale Global Fact-Checking",
    "volume": "long",
    "abstract": "Manual fact-checking does not scale well to serve the needs of the internet. This issue is further compounded in non-English contexts. In this paper, we discuss claim matching as a possible solution to scale fact-checking. We define claim matching as the task of identifying pairs of textual messages containing claims that can be served with one fact-check. We construct a novel dataset of WhatsApp tipline and public group messages alongside fact-checked claims that are first annotated for containing “claim-like statements” and then matched with potentially similar items and annotated for claim matching. Our dataset contains content in high-resource (English, Hindi) and lower-resource (Bengali, Malayalam, Tamil) languages. We train our own embedding model using knowledge distillation and a high-quality “teacher” model in order to address the imbalance in embedding quality between the low- and high-resource languages in our dataset. We provide evaluations on the performance of our solution and compare with baselines and existing state-of-the-art multilingual embedding models, namely LASER and LaBSE. We demonstrate that our performance exceeds LASER and LaBSE in all settings. We release our annotated datasets, codebooks, and trained embedding model to allow for further research",
    "checked": true,
    "id": "5775a40e51d79c1ecdec7f7c336667da658914f7",
    "semantic_title": "",
    "citation_count": 26,
    "authors": [
      "Ashkan Kazemi",
      "Kiran Garimella",
      "Devin Gaffney",
      "Scott Hale"
    ]
  },
  "https://aclanthology.org/2021.acl-long.348": {
    "title": "SemFace: Pre-training Encoder and Decoder with a Semantic Interface for Neural Machine Translation",
    "volume": "long",
    "abstract": "While pre-training techniques are working very well in natural language processing, how to pre-train a decoder and effectively use it for neural machine translation (NMT) still remains a tricky issue. The main reason is that the cross-attention module between the encoder and decoder cannot be pre-trained, and the combined encoder-decoder model cannot work well in the fine-tuning stage because the inputs of the decoder cross-attention come from unknown encoder outputs. In this paper, we propose a better pre-training method for NMT by defining a semantic interface (SemFace) between the pre-trained encoder and the pre-trained decoder. Specifically, we propose two types of semantic interfaces, including CL-SemFace which regards cross-lingual embeddings as an interface, and VQ-SemFace which employs vector quantized embeddings to constrain the encoder outputs and decoder inputs in the same language-independent space. We conduct massive experiments on six supervised translation pairs and three unsupervised pairs. Experimental results demonstrate that our proposed SemFace can effectively connect the pre-trained encoder and decoder, and achieves significant improvement by 3.7 and 1.5 BLEU points on the two tasks respectively compared with previous pre-training-based NMT models",
    "checked": true,
    "id": "1180bf7c1f15d5d472123a7b4aa5969baa5d5e7f",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Shuo Ren",
      "Long Zhou",
      "Shujie Liu",
      "Furu Wei",
      "Ming Zhou",
      "Shuai Ma"
    ]
  },
  "https://aclanthology.org/2021.acl-long.349": {
    "title": "Energy-Based Reranking: Improving Neural Machine Translation Using Energy-Based Models",
    "volume": "long",
    "abstract": "The discrepancy between maximum likelihood estimation (MLE) and task measures such as BLEU score has been studied before for autoregressive neural machine translation (NMT) and resulted in alternative training algorithms (Ranzato et al., 2016; Norouzi et al., 2016; Shen et al., 2016; Wu et al., 2018). However, MLE training remains the de facto approach for autoregressive NMT because of its computational efficiency and stability. Despite this mismatch between the training objective and task measure, we notice that the samples drawn from an MLE-based trained NMT support the desired distribution – there are samples with much higher BLEU score comparing to the beam decoding output. To benefit from this observation, we train an energy-based model to mimic the behavior of the task measure (i.e., the energy-based model assigns lower energy to samples with higher BLEU score), which is resulted in a re-ranking algorithm based on the samples drawn from NMT: energy-based re-ranking (EBR). We use both marginal energy models (over target sentence) and joint energy models (over both source and target sentences). Our EBR with the joint energy model consistently improves the performance of the Transformer-based NMT: +3.7 BLEU points on IWSLT’14 German-English, +3.37 BELU points on Sinhala-English, +1.4 BLEU points on WMT’16 English-German tasks",
    "checked": true,
    "id": "42fc352a0db1e742b0248a02b812db4aaf7b2cd3",
    "semantic_title": "",
    "citation_count": 32,
    "authors": [
      "Sumanta Bhattacharyya",
      "Amirmohammad Rooshenas",
      "Subhajit Naskar",
      "Simeng Sun",
      "Mohit Iyyer",
      "Andrew McCallum"
    ]
  },
  "https://aclanthology.org/2021.acl-long.350": {
    "title": "Syntax-augmented Multilingual BERT for Cross-lingual Transfer",
    "volume": "long",
    "abstract": "In recent years, we have seen a colossal effort in pre-training multilingual text encoders using large-scale corpora in many languages to facilitate cross-lingual transfer learning. However, due to typological differences across languages, the cross-lingual transfer is challenging. Nevertheless, language syntax, e.g., syntactic dependencies, can bridge the typological gap. Previous works have shown that pre-trained multilingual encoders, such as mBERT (CITATION), capture language syntax, helping cross-lingual transfer. This work shows that explicitly providing language syntax and training mBERT using an auxiliary objective to encode the universal dependency tree structure helps cross-lingual transfer. We perform rigorous experiments on four NLP tasks, including text classification, question answering, named entity recognition, and task-oriented semantic parsing. The experiment results show that syntax-augmented mBERT improves cross-lingual transfer on popular benchmarks, such as PAWS-X and MLQA, by 1.4 and 1.6 points on average across all languages. In the generalized transfer setting, the performance boosted significantly, with 3.9 and 3.1 points on average in PAWS-X and MLQA",
    "checked": true,
    "id": "dda0bce7baee7175f7b0e5f0ac81669ed1c13f07",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Wasi Ahmad",
      "Haoran Li",
      "Kai-Wei Chang",
      "Yashar Mehdad"
    ]
  },
  "https://aclanthology.org/2021.acl-long.351": {
    "title": "How to Adapt Your Pretrained Multilingual Model to 1600 Languages",
    "volume": "long",
    "abstract": "Pretrained multilingual models (PMMs) enable zero-shot learning via cross-lingual transfer, performing best for languages seen during pretraining. While methods exist to improve performance for unseen languages, they have almost exclusively been evaluated using amounts of raw text only available for a small fraction of the world’s languages. In this paper, we evaluate the performance of existing methods to adapt PMMs to new languages using a resource available for close to 1600 languages: the New Testament. This is challenging for two reasons: (1) the small corpus size, and (2) the narrow domain. While performance drops for all approaches, we surprisingly still see gains of up to 17.69% accuracy for part-of-speech tagging and 6.29 F1 for NER on average over all languages as compared to XLM-R. Another unexpected finding is that continued pretraining, the simplest approach, performs best. Finally, we perform a case study to disentangle the effects of domain and size and to shed light on the influence of the finetuning source language",
    "checked": true,
    "id": "c89ff2a0416a6d0870e724953a61a798deee238f",
    "semantic_title": "",
    "citation_count": 29,
    "authors": [
      "Abteen Ebrahimi",
      "Katharina Kann"
    ]
  },
  "https://aclanthology.org/2021.acl-long.352": {
    "title": "Weakly Supervised Named Entity Tagging with Learnable Logical Rules",
    "volume": "long",
    "abstract": "We study the problem of building entity tagging systems by using a few rules as weak supervision. Previous methods mostly focus on disambiguating entity types based on contexts and expert-provided rules, while assuming entity spans are given. In this work, we propose a novel method TALLOR that bootstraps high-quality logical rules to train a neural tagger in a fully automated manner. Specifically, we introduce compound rules that are composed from simple rules to increase the precision of boundary detection and generate more diverse pseudo labels. We further design a dynamic label selection strategy to ensure pseudo label quality and therefore avoid overfitting the neural tagger. Experiments on three datasets demonstrate that our method outperforms other weakly supervised methods and even rivals a state-of-the-art distantly supervised tagger with a lexicon of over 2,000 terms when starting from only 20 simple rules. Our method can serve as a tool for rapidly building taggers in emerging domains and tasks. Case studies show that learned rules can potentially explain the predicted entities",
    "checked": true,
    "id": "c19d03b7488823050b6f9b208461462648edf1fd",
    "semantic_title": "",
    "citation_count": 18,
    "authors": [
      "Jiacheng Li",
      "Haibo Ding",
      "Jingbo Shang",
      "Julian McAuley",
      "Zhe Feng"
    ]
  },
  "https://aclanthology.org/2021.acl-long.353": {
    "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
    "volume": "long",
    "abstract": "Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were “virtual tokens”. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training",
    "checked": true,
    "id": "53d8b356551a2361020a948f64454a6d599af69f",
    "semantic_title": "",
    "citation_count": 1130,
    "authors": [
      "Xiang Lisa Li",
      "Percy Liang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.354": {
    "title": "One2Set: Generating Diverse Keyphrases as a Set",
    "volume": "long",
    "abstract": "Recently, the sequence-to-sequence models have made remarkable progress on the task of keyphrase generation (KG) by concatenating multiple keyphrases in a predefined order as a target sequence during training. However, the keyphrases are inherently an unordered set rather than an ordered sequence. Imposing a predefined order will introduce wrong bias during training, which can highly penalize shifts in the order between keyphrases. In this work, we propose a new training paradigm One2Set without predefining an order to concatenate the keyphrases. To fit this paradigm, we propose a novel model that utilizes a fixed set of learned control codes as conditions to generate a set of keyphrases in parallel. To solve the problem that there is no correspondence between each prediction and target during training, we propose a K-step label assignment mechanism via bipartite matching, which greatly increases the diversity and reduces the repetition rate of generated keyphrases. The experimental results on multiple benchmarks demonstrate that our approach significantly outperforms the state-of-the-art methods",
    "checked": true,
    "id": "e08c6f15565c9f70e5e3375e293d9a5cfe888e74",
    "semantic_title": "",
    "citation_count": 34,
    "authors": [
      "Jiacheng Ye",
      "Tao Gui",
      "Yichao Luo",
      "Yige Xu",
      "Qi Zhang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.355": {
    "title": "Continuous Language Generative Flow",
    "volume": "long",
    "abstract": "Recent years have witnessed various types of generative models for natural language generation (NLG), especially RNNs or transformer based sequence-to-sequence models, as well as variational autoencoder (VAE) and generative adversarial network (GAN) based models. However, flow-based generative models, which achieve strong performance in image generation due to their invertibility and exact density estimation properties, have been less explored for NLG. In this paper, we propose a flow-based language generation model by adapting previous flow generative models to language generation via continuous input embeddings, adapted affine coupling structures, and a novel architecture for autoregressive text generation. We also apply our framework to Sequence-to-Sequence generation, including text- and video-based Question Generation (QG) and Neural Machine Translation (NMT), and data augmentation for Question Answering (QA). We use our language flow model to provide extra input features for QG and NMT, which achieves improvements over the strong QG baselines on SQuAD and TVQA and NMT baseline on WMT16. We also augment QA data with new context by injecting noise to the latent features of the language flow and show this augmentation leads to a large performance improvement from strong baselines on SQuAD and TVQA",
    "checked": true,
    "id": "68c252eaf81d24f5d04eb8317492f6efc664eb3c",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Zineng Tang",
      "Shiyue Zhang",
      "Hyounghun Kim",
      "Mohit Bansal"
    ]
  },
  "https://aclanthology.org/2021.acl-long.356": {
    "title": "TWAG: A Topic-Guided Wikipedia Abstract Generator",
    "volume": "long",
    "abstract": "Wikipedia abstract generation aims to distill a Wikipedia abstract from web sources and has met significant success by adopting multi-document summarization techniques. However, previous works generally view the abstract as plain text, ignoring the fact that it is a description of a certain entity and can be decomposed into different topics. In this paper, we propose a two-stage model TWAG that guides the abstract generation with topical information. First, we detect the topic of each input paragraph with a classifier trained on existing Wikipedia articles to divide input documents into different topics. Then, we predict the topic distribution of each abstract sentence, and decode the sentence from topic-aware representations with a Pointer-Generator network. We evaluate our model on the WikiCatSum dataset, and the results show that TWAG outperforms various existing baselines and is capable of generating comprehensive abstracts",
    "checked": true,
    "id": "0ed8c60150106473ed8777b1f4797b1f0b4c613f",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Fangwei Zhu",
      "Shangqing Tu",
      "Jiaxin Shi",
      "Juanzi Li",
      "Lei Hou",
      "Tong Cui"
    ]
  },
  "https://aclanthology.org/2021.acl-long.357": {
    "title": "ForecastQA: A Question Answering Challenge for Event Forecasting with Temporal Text Data",
    "volume": "long",
    "abstract": "Event forecasting is a challenging, yet important task, as humans seek to constantly plan for the future. Existing automated forecasting studies rely mostly on structured data, such as time-series or event-based knowledge graphs, to help predict future events. In this work, we aim to formulate a task, construct a dataset, and provide benchmarks for developing methods for event forecasting with large volumes of unstructured text data. To simulate the forecasting scenario on temporal news documents, we formulate the problem as a restricted-domain, multiple-choice, question-answering (QA) task. Unlike existing QA tasks, our task limits accessible information, and thus a model has to make a forecasting judgement. To showcase the usefulness of this task formulation, we introduce ForecastQA, a question-answering dataset consisting of 10,392 event forecasting questions, which have been collected and verified via crowdsourcing efforts. We present our experiments on ForecastQA using BERTbased models and find that our best model achieves 61.0% accuracy on the dataset, which still lags behind human performance by about 19%. We hope ForecastQA will support future research efforts in bridging this gap",
    "checked": true,
    "id": "30602e3382df3abedb5f225b55b7efce8580f74d",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Woojeong Jin",
      "Rahul Khanna",
      "Suji Kim",
      "Dong-Ho Lee",
      "Fred Morstatter",
      "Aram Galstyan",
      "Xiang Ren"
    ]
  },
  "https://aclanthology.org/2021.acl-long.358": {
    "title": "Recursive Tree-Structured Self-Attention for Answer Sentence Selection",
    "volume": "long",
    "abstract": "Syntactic structure is an important component of natural language text. Recent top-performing models in Answer Sentence Selection (AS2) use self-attention and transfer learning, but not syntactic structure. Tree structures have shown strong performance in tasks with sentence pair input like semantic relatedness. We investigate whether tree structures can boost performance in AS2. We introduce the Tree Aggregation Transformer: a novel recursive, tree-structured self-attention model for AS2. The recursive nature of our model is able to represent all levels of syntactic parse trees with only one additional self-attention layer. Without transfer learning, we establish a new state of the art on the popular TrecQA and WikiQA benchmark datasets. Additionally, we evaluate our method on four Community Question Answering datasets, and find that tree-structured representations have limitations with noisy user-generated text. We conduct probing experiments to evaluate how our models leverage tree structures across datasets. Our findings show that the ability of tree-structured models to successfully absorb syntactic information is strongly correlated with a higher performance in AS2",
    "checked": true,
    "id": "061c712d9bb32439c70d9d3c01882f4097fc3df3",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Khalil Mrini",
      "Emilia Farcas",
      "Ndapa Nakashole"
    ]
  },
  "https://aclanthology.org/2021.acl-long.359": {
    "title": "How Knowledge Graph and Attention Help? A Qualitative Analysis into Bag-level Relation Extraction",
    "volume": "long",
    "abstract": "Knowledge Graph (KG) and attention mechanism have been demonstrated effective in introducing and selecting useful information for weakly supervised methods. However, only qualitative analysis and ablation study are provided as evidence. In this paper, we contribute a dataset and propose a paradigm to quantitatively evaluate the effect of attention and KG on bag-level relation extraction (RE). We find that (1) higher attention accuracy may lead to worse performance as it may harm the model’s ability to extract entity mention features; (2) the performance of attention is largely influenced by various noise distribution patterns, which is closely related to real-world datasets; (3) KG-enhanced attention indeed improves RE performance, while not through enhanced attention but by incorporating entity prior; and (4) attention mechanism may exacerbate the issue of insufficient training data. Based on these findings, we show that a straightforward variant of RE model can achieve significant improvements (6% AUC on average) on two real-world datasets as compared with three state-of-the-art baselines. Our codes and datasets are available at https://github.com/zig-kwin-hu/how-KG-ATT-help",
    "checked": true,
    "id": "7f3e27731f96e85de069cdcfd4a9be3de4a6d636",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Zikun Hu",
      "Yixin Cao",
      "Lifu Huang",
      "Tat-Seng Chua"
    ]
  },
  "https://aclanthology.org/2021.acl-long.360": {
    "title": "Trigger is Not Sufficient: Exploiting Frame-aware Knowledge for Implicit Event Argument Extraction",
    "volume": "long",
    "abstract": "Implicit Event Argument Extraction seeks to identify arguments that play direct or implicit roles in a given event. However, most prior works focus on capturing direct relations between arguments and the event trigger. The lack of reasoning ability brings many challenges to the extraction of implicit arguments. In this work, we present a Frame-aware Event Argument Extraction (FEAE) learning framework to tackle this issue through reasoning in event frame-level scope. The proposed method leverages related arguments of the expected one as clues to guide the reasoning process. To bridge the gap between oracle knowledge used in the training phase and the imperfect related arguments in the test stage, we further introduce a curriculum knowledge distillation strategy to drive a final model that could operate without extra inputs through mimicking the behavior of a well-informed teacher model. Experimental results demonstrate FEAE obtains new state-of-the-art performance on the RAMS dataset",
    "checked": true,
    "id": "89c32bb4da87815564081be435e54dbd09b4a426",
    "semantic_title": "",
    "citation_count": 17,
    "authors": [
      "Kaiwen Wei",
      "Xian Sun",
      "Zequn Zhang",
      "Jingyuan Zhang",
      "Guo Zhi",
      "Li Jin"
    ]
  },
  "https://aclanthology.org/2021.acl-long.361": {
    "title": "Element Intervention for Open Relation Extraction",
    "volume": "long",
    "abstract": "Open relation extraction aims to cluster relation instances referring to the same underlying relation, which is a critical step for general relation extraction. Current OpenRE models are commonly trained on the datasets generated from distant supervision, which often results in instability and makes the model easily collapsed. In this paper, we revisit the procedure of OpenRE from a causal view. By formulating OpenRE using a structural causal model, we identify that the above-mentioned problems stem from the spurious correlations from entities and context to the relation type. To address this issue, we conduct Element Intervention, which intervene on the context and entities respectively to obtain the underlying causal effects of them. We also provide two specific implementations of the interventions based on entity ranking and context contrasting. Experimental results on unsupervised relation extraction datasets show our method to outperform previous state-of-the-art methods and is robust across different datasets",
    "checked": true,
    "id": "bf2079d4fca3844747a9ffcfe79736a6fa4dba90",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Fangchao Liu",
      "Lingyong Yan",
      "Hongyu Lin",
      "Xianpei Han",
      "Le Sun"
    ]
  },
  "https://aclanthology.org/2021.acl-long.362": {
    "title": "AdaTag: Multi-Attribute Value Extraction from Product Profiles with Adaptive Decoding",
    "volume": "long",
    "abstract": "Automatic extraction of product attribute values is an important enabling technology in e-Commerce platforms. This task is usually modeled using sequence labeling architectures, with several extensions to handle multi-attribute extraction. One line of previous work constructs attribute-specific models, through separate decoders or entirely separate models. However, this approach constrains knowledge sharing across different attributes. Other contributions use a single multi-attribute model, with different techniques to embed attribute information. But sharing the entire network parameters across all attributes can limit the model’s capacity to capture attribute-specific characteristics. In this paper we present AdaTag, which uses adaptive decoding to handle extraction. We parameterize the decoder with pretrained attribute embeddings, through a hypernetwork and a Mixture-of-Experts (MoE) module. This allows for separate, but semantically correlated, decoders to be generated on the fly for different attributes. This approach facilitates knowledge sharing, while maintaining the specificity of each attribute. Our experiments on a real-world e-Commerce dataset show marked improvements over previous methods",
    "checked": true,
    "id": "b2eec2dc3c870b2d3f37564579c45434be128383",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Jun Yan",
      "Nasser Zalmout",
      "Yan Liang",
      "Christan Grant",
      "Xiang Ren",
      "Xin Luna Dong"
    ]
  },
  "https://aclanthology.org/2021.acl-long.363": {
    "title": "CoRI: Collective Relation Integration with Data Augmentation for Open Information Extraction",
    "volume": "long",
    "abstract": "Integrating extracted knowledge from the Web to knowledge graphs (KGs) can facilitate tasks like question answering. We study relation integration that aims to align free-text relations in subject-relation-object extractions to relations in a target KG. To address the challenge that free-text relations are ambiguous, previous methods exploit neighbor entities and relations for additional context. However, the predictions are made independently, which can be mutually inconsistent. We propose a two-stage Collective Relation Integration (CoRI) model, where the first stage independently makes candidate predictions, and the second stage employs a collective model that accesses all candidate predictions to make globally coherent predictions. We further improve the collective model with augmented data from the portion of the target KG that is otherwise unused. Experiment results on two datasets show that CoRI can significantly outperform the baselines, improving AUC from .677 to .748 and from .716 to .780, respectively",
    "checked": true,
    "id": "0c04a256d858f3d03e52f01fb55a5fa2c4c9aa42",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Zhengbao Jiang",
      "Jialong Han",
      "Bunyamin Sisman",
      "Xin Luna Dong"
    ]
  },
  "https://aclanthology.org/2021.acl-long.364": {
    "title": "Benchmarking Scalable Methods for Streaming Cross Document Entity Coreference",
    "volume": "long",
    "abstract": "Streaming cross document entity coreference (CDC) systems disambiguate mentions of named entities in a scalable manner via incremental clustering. Unlike other approaches for named entity disambiguation (e.g., entity linking), streaming CDC allows for the disambiguation of entities that are unknown at inference time. Thus, it is well-suited for processing streams of data where new entities are frequently introduced. Despite these benefits, this task is currently difficult to study, as existing approaches are either evaluated on datasets that are no longer available, or omit other crucial details needed to ensure fair comparison. In this work, we address this issue by compiling a large benchmark adapted from existing free datasets, and performing a comprehensive evaluation of a number of novel and existing baseline models. We investigate: how to best encode mentions, which clustering algorithms are most effective for grouping mentions, how models transfer to different domains, and how bounding the number of mentions tracked during inference impacts performance. Our results show that the relative performance of neural and feature-based mention encoders varies across different domains, and in most cases the best performance is achieved using a combination of both approaches. We also find that performance is minimally impacted by limiting the number of tracked mentions",
    "checked": true,
    "id": "973f1092f3eea20254eb290925b41f8b619e1a03",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Robert L Logan IV",
      "Andrew McCallum",
      "Sameer Singh",
      "Dan Bikel"
    ]
  },
  "https://aclanthology.org/2021.acl-long.365": {
    "title": "Search from History and Reason for Future: Two-stage Reasoning on Temporal Knowledge Graphs",
    "volume": "long",
    "abstract": "Temporal Knowledge Graphs (TKGs) have been developed and used in many different areas. Reasoning on TKGs that predicts potential facts (events) in the future brings great challenges to existing models. When facing a prediction task, human beings usually search useful historical information (i.e., clues) in their memories and then reason for future meticulously. Inspired by this mechanism, we propose CluSTeR to predict future facts in a two-stage manner, Clue Searching and Temporal Reasoning, accordingly. Specifically, at the clue searching stage, CluSTeR learns a beam search policy via reinforcement learning (RL) to induce multiple clues from historical facts. At the temporal reasoning stage, it adopts a graph convolution network based sequence method to deduce answers from clues. Experiments on four datasets demonstrate the substantial advantages of CluSTeR compared with the state-of-the-art methods. Moreover, the clues found by CluSTeR further provide interpretability for the results",
    "checked": true,
    "id": "a697045644788d9c975bf5ac544ed211f6fc9d5b",
    "semantic_title": "",
    "citation_count": 30,
    "authors": [
      "Zixuan Li",
      "Xiaolong Jin",
      "Saiping Guan",
      "Wei Li",
      "Jiafeng Guo",
      "Yuanzhuo Wang",
      "Xueqi Cheng"
    ]
  },
  "https://aclanthology.org/2021.acl-long.366": {
    "title": "Employing Argumentation Knowledge Graphs for Neural Argument Generation",
    "volume": "long",
    "abstract": "Generating high-quality arguments, while being challenging, may benefit a wide range of downstream applications, such as writing assistants and argument search engines. Motivated by the effectiveness of utilizing knowledge graphs for supporting general text generation tasks, this paper investigates the usage of argumentation-related knowledge graphs to control the generation of arguments. In particular, we construct and populate three knowledge graphs, employing several compositions of them to encode various knowledge into texts of debate portals and relevant paragraphs from Wikipedia. Then, the texts with the encoded knowledge are used to fine-tune a pre-trained text generation model, GPT-2. We evaluate the newly created arguments manually and automatically, based on several dimensions important in argumentative contexts, including argumentativeness and plausibility. The results demonstrate the positive impact of encoding the graphs’ knowledge into debate portal texts for generating arguments with superior quality than those generated without knowledge",
    "checked": true,
    "id": "1a5760e164e858ac991d06b7840362a5fc0db4d2",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Khalid Al Khatib",
      "Lukas Trautner",
      "Henning Wachsmuth",
      "Yufang Hou",
      "Benno Stein"
    ]
  },
  "https://aclanthology.org/2021.acl-long.367": {
    "title": "Learning Span-Level Interactions for Aspect Sentiment Triplet Extraction",
    "volume": "long",
    "abstract": "Aspect Sentiment Triplet Extraction (ASTE) is the most recent subtask of ABSA which outputs triplets of an aspect target, its associated sentiment, and the corresponding opinion term. Recent models perform the triplet extraction in an end-to-end manner but heavily rely on the interactions between each target word and opinion word. Thereby, they cannot perform well on targets and opinions which contain multiple words. Our proposed span-level approach explicitly considers the interaction between the whole spans of targets and opinions when predicting their sentiment relation. Thus, it can make predictions with the semantics of whole spans, ensuring better sentiment consistency. To ease the high computational cost caused by span enumeration, we propose a dual-channel span pruning strategy by incorporating supervision from the Aspect Term Extraction (ATE) and Opinion Term Extraction (OTE) tasks. This strategy not only improves computational efficiency but also distinguishes the opinion and target spans more properly. Our framework simultaneously achieves strong performance for the ASTE as well as ATE and OTE tasks. In particular, our analysis shows that our span-level approach achieves more significant improvements over the baselines on triplets with multi-word targets or opinions",
    "checked": true,
    "id": "1abaa34bd4cd780f723c23b8fa819e5aa4d6ae08",
    "semantic_title": "",
    "citation_count": 55,
    "authors": [
      "Lu Xu",
      "Yew Ken Chia",
      "Lidong Bing"
    ]
  },
  "https://aclanthology.org/2021.acl-long.368": {
    "title": "On Compositional Generalization of Neural Machine Translation",
    "volume": "long",
    "abstract": "Modern neural machine translation (NMT) models have achieved competitive performance in standard benchmarks such as WMT. However, there still exist significant issues such as robustness, domain generalization, etc. In this paper, we study NMT models from the perspective of compositional generalization by building a benchmark dataset, CoGnition, consisting of 216k clean and consistent sentence pairs. We quantitatively analyze effects of various factors using compound translation error rate, then demonstrate that the NMT model fails badly on compositional generalization, although it performs remarkably well under traditional metrics",
    "checked": true,
    "id": "03ad126cfe495933f7bb769f27c03e5f31caedf8",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Yafu Li",
      "Yongjing Yin",
      "Yulong Chen",
      "Yue Zhang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.369": {
    "title": "Mask-Align: Self-Supervised Neural Word Alignment",
    "volume": "long",
    "abstract": "Word alignment, which aims to align translationally equivalent words between source and target sentences, plays an important role in many natural language processing tasks. Current unsupervised neural alignment methods focus on inducing alignments from neural machine translation models, which does not leverage the full context in the target sequence. In this paper, we propose Mask-Align, a self-supervised word alignment model that takes advantage of the full context on the target side. Our model masks out each target token and predicts it conditioned on both source and the remaining target tokens. This two-step process is based on the assumption that the source token contributing most to recovering the masked target token should be aligned. We also introduce an attention variant called leaky attention, which alleviates the problem of unexpected high cross-attention weights on special tokens such as periods. Experiments on four language pairs show that our model outperforms previous unsupervised neural aligners and obtains new state-of-the-art results",
    "checked": true,
    "id": "61fb9e3709aa396dd234ed96e4e4333905f69092",
    "semantic_title": "",
    "citation_count": 23,
    "authors": [
      "Chi Chen",
      "Maosong Sun",
      "Yang Liu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.370": {
    "title": "GWLAN: General Word-Level AutocompletioN for Computer-Aided Translation",
    "volume": "long",
    "abstract": "Computer-aided translation (CAT), the use of software to assist a human translator in the translation process, has been proven to be useful in enhancing the productivity of human translators. Autocompletion, which suggests translation results according to the text pieces provided by human translators, is a core function of CAT. There are two limitations in previous research in this line. First, most research works on this topic focus on sentence-level autocompletion (i.e., generating the whole translation as a sentence based on human input), but word-level autocompletion is under-explored so far. Second, almost no public benchmarks are available for the autocompletion task of CAT. This might be among the reasons why research progress in CAT is much slower compared to automatic MT. In this paper, we propose the task of general word-level autocompletion (GWLAN) from a real-world CAT scenario, and construct the first public benchmark to facilitate research in this topic. In addition, we propose an effective method for GWLAN and compare it with several strong baselines. Experiments demonstrate that our proposed method can give significantly more accurate predictions than the baseline methods on our benchmark datasets",
    "checked": true,
    "id": "35e7760adfbf1d3f4da6246c9bb9b20f51de96a5",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Huayang Li",
      "Lemao Liu",
      "Guoping Huang",
      "Shuming Shi"
    ]
  },
  "https://aclanthology.org/2021.acl-long.371": {
    "title": "De-biasing Distantly Supervised Named Entity Recognition via Causal Intervention",
    "volume": "long",
    "abstract": "Distant supervision tackles the data bottleneck in NER by automatically generating training instances via dictionary matching. Unfortunately, the learning of DS-NER is severely dictionary-biased, which suffers from spurious correlations and therefore undermines the effectiveness and the robustness of the learned models. In this paper, we fundamentally explain the dictionary bias via a Structural Causal Model (SCM), categorize the bias into intra-dictionary and inter-dictionary biases, and identify their causes. Based on the SCM, we learn de-biased DS-NER via causal interventions. For intra-dictionary bias, we conduct backdoor adjustment to remove the spurious correlations introduced by the dictionary confounder. For inter-dictionary bias, we propose a causal invariance regularizer which will make DS-NER models more robust to the perturbation of dictionaries. Experiments on four datasets and three DS-NER models show that our method can significantly improve the performance of DS-NER",
    "checked": true,
    "id": "3aa14f1d49db181a6cef26faa4b7cf60e5afeefb",
    "semantic_title": "",
    "citation_count": 22,
    "authors": [
      "Wenkai Zhang",
      "Hongyu Lin",
      "Xianpei Han",
      "Le Sun"
    ]
  },
  "https://aclanthology.org/2021.acl-long.372": {
    "title": "A Span-Based Model for Joint Overlapped and Discontinuous Named Entity Recognition",
    "volume": "long",
    "abstract": "Research on overlapped and discontinuous named entity recognition (NER) has received increasing attention. The majority of previous work focuses on either overlapped or discontinuous entities. In this paper, we propose a novel span-based model that can recognize both overlapped and discontinuous entities jointly. The model includes two major steps. First, entity fragments are recognized by traversing over all possible text spans, thus, overlapped entities can be recognized. Second, we perform relation classification to judge whether a given pair of entity fragments to be overlapping or succession. In this way, we can recognize not only discontinuous entities, and meanwhile doubly check the overlapped entities. As a whole, our model can be regarded as a relation extraction paradigm essentially. Experimental results on multiple benchmark datasets (i.e., CLEF, GENIA and ACE05) show that our model is highly competitive for overlapped and discontinuous NER",
    "checked": true,
    "id": "22b6983a68d901925c83244dad1991c2bc2294b8",
    "semantic_title": "",
    "citation_count": 34,
    "authors": [
      "Fei Li",
      "ZhiChao Lin",
      "Meishan Zhang",
      "Donghong Ji"
    ]
  },
  "https://aclanthology.org/2021.acl-long.373": {
    "title": "MLBiNet: A Cross-Sentence Collective Event Detection Network",
    "volume": "long",
    "abstract": "We consider the problem of collectively detecting multiple events, particularly in cross-sentence settings. The key to dealing with the problem is to encode semantic information and model event inter-dependency at a document-level. In this paper, we reformulate it as a Seq2Seq task and propose a Multi-Layer Bidirectional Network (MLBiNet) to capture the document-level association of events and semantic information simultaneously. Specifically, a bidirectional decoder is firstly devised to model event inter-dependency within a sentence when decoding the event tag vector sequence. Secondly, an information aggregation module is employed to aggregate sentence-level semantic and event tag information. Finally, we stack multiple bidirectional decoders and feed cross-sentence information, forming a multi-layer bidirectional tagging architecture to iteratively propagate information across sentences. We show that our approach provides significant improvement in performance compared to the current state-of-the-art results",
    "checked": true,
    "id": "ab5f4c07b627ee1cc3b349bf11ca385b8ffc505e",
    "semantic_title": "",
    "citation_count": 31,
    "authors": [
      "Dongfang Lou",
      "Zhilin Liao",
      "Shumin Deng",
      "Ningyu Zhang",
      "Huajun Chen"
    ]
  },
  "https://aclanthology.org/2021.acl-long.374": {
    "title": "Exploiting Document Structures and Cluster Consistencies for Event Coreference Resolution",
    "volume": "long",
    "abstract": "We study the problem of event coreference resolution (ECR) that seeks to group coreferent event mentions into the same clusters. Deep learning methods have recently been applied for this task to deliver state-of-the-art performance. However, existing deep learning models for ECR are limited in that they cannot exploit important interactions between relevant objects for ECR, e.g., context words and entity mentions, to support the encoding of document-level context. In addition, consistency constraints between golden and predicted clusters of event mentions have not been considered to improve representation learning in prior deep learning models for ECR. This work addresses such limitations by introducing a novel deep learning model for ECR. At the core of our model are document structures to explicitly capture relevant objects for ECR. Our document structures introduce diverse knowledge sources (discourse, syntax, semantics) to compute edges/interactions between structure nodes for document-level representation learning. We also present novel regularization techniques based on consistencies of golden and predicted clusters for event mentions in documents. Extensive experiments show that our model achieve state-of-the-art performance on two benchmark datasets",
    "checked": true,
    "id": "28e207ba02f3eab16803f3ee0653788b710d5b7d",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Hieu Minh Tran",
      "Duy Phung",
      "Thien Huu Nguyen"
    ]
  },
  "https://aclanthology.org/2021.acl-long.375": {
    "title": "StereoRel: Relational Triple Extraction from a Stereoscopic Perspective",
    "volume": "long",
    "abstract": "Relational triple extraction is critical to understanding massive text corpora and constructing large-scale knowledge graph, which has attracted increasing research interest. However, existing studies still face some challenging issues, including information loss, error propagation and ignoring the interaction between entity and relation. To intuitively explore the above issues and address them, in this paper, we provide a revealing insight into relational triple extraction from a stereoscopic perspective, which rationalizes the occurrence of these issues and exposes the shortcomings of existing methods. Further, a novel model is proposed for relational triple extraction, which maps relational triples to a three-dimension (3-D) space and leverages three decoders to extract them, aimed at simultaneously handling the above issues. A series of experiments are conducted on five public datasets, demonstrating that the proposed model outperforms the recent advanced baselines",
    "checked": true,
    "id": "576899ec8352a7fdfe5047fbe2949b7fe86feca6",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Xuetao Tian",
      "Liping Jing",
      "Lu He",
      "Feng Liu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.376": {
    "title": "Knowledge-Enriched Event Causality Identification via Latent Structure Induction Networks",
    "volume": "long",
    "abstract": "Identifying causal relations of events is an important task in natural language processing area. However, the task is very challenging, because event causality is usually expressed in diverse forms that often lack explicit causal clues. Existing methods cannot handle well the problem, especially in the condition of lacking training data. Nonetheless, humans can make a correct judgement based on their background knowledge, including descriptive knowledge and relational knowledge. Inspired by it, we propose a novel Latent Structure Induction Network (LSIN) to incorporate the external structural knowledge into this task. Specifically, to make use of the descriptive knowledge, we devise a Descriptive Graph Induction module to obtain and encode the graph-structured descriptive knowledge. To leverage the relational knowledge, we propose a Relational Graph Induction module which is able to automatically learn a reasoning structure for event causality reasoning. Experimental results on two widely used datasets indicate that our approach significantly outperforms previous state-of-the-art methods",
    "checked": true,
    "id": "3bc0095232c098e737be3be5dd8110ab6b2936a9",
    "semantic_title": "",
    "citation_count": 19,
    "authors": [
      "Pengfei Cao",
      "Xinyu Zuo",
      "Yubo Chen",
      "Kang Liu",
      "Jun Zhao",
      "Yuguang Chen",
      "Weihua Peng"
    ]
  },
  "https://aclanthology.org/2021.acl-long.377": {
    "title": "Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word Substitution",
    "volume": "long",
    "abstract": "Recent studies show that neural natural language processing (NLP) models are vulnerable to backdoor attacks. Injected with backdoors, models perform normally on benign examples but produce attacker-specified predictions when the backdoor is activated, presenting serious security threats to real-world applications. Since existing textual backdoor attacks pay little attention to the invisibility of backdoors, they can be easily detected and blocked. In this work, we present invisible backdoors that are activated by a learnable combination of word substitution. We show that NLP models can be injected with backdoors that lead to a nearly 100% attack success rate, whereas being highly invisible to existing defense strategies and even human inspections. The results raise a serious alarm to the security of NLP models, which requires further research to be resolved. All the data and code of this paper are released at https://github.com/thunlp/BkdAtk-LWS",
    "checked": true,
    "id": "d7c0aff686894b370a13840890d42fe7992d6b95",
    "semantic_title": "",
    "citation_count": 61,
    "authors": [
      "Fanchao Qi",
      "Yuan Yao",
      "Sophia Xu",
      "Zhiyuan Liu",
      "Maosong Sun"
    ]
  },
  "https://aclanthology.org/2021.acl-long.378": {
    "title": "Parameter-Efficient Transfer Learning with Diff Pruning",
    "volume": "long",
    "abstract": "The large size of pretrained networks makes them difficult to deploy for multiple tasks in storage-constrained settings. Diff pruning enables parameter-efficient transfer learning that scales well with new tasks. The approach learns a task-specific “diff” vector that extends the original pretrained parameters. This diff vector is adaptively pruned during training with a differentiable approximation to the L0-norm penalty to encourage sparsity. As the number of tasks increases, diff pruning remains parameter-efficient, as it requires storing only a small diff vector for each task. Since it does not require access to all tasks during training, it is attractive in on-device deployment settings where tasks arrive in stream or even from different providers. Diff pruning can match the performance of finetuned baselines on the GLUE benchmark while only modifying 0.5% of the pretrained model’s parameters per task and scales favorably in comparison to popular pruning approaches",
    "checked": true,
    "id": "d22e4cc3a501c17881b9478621f29760e429e76e",
    "semantic_title": "",
    "citation_count": 126,
    "authors": [
      "Demi Guo",
      "Alexander Rush",
      "Yoon Kim"
    ]
  },
  "https://aclanthology.org/2021.acl-long.379": {
    "title": "R2D2: Recursive Transformer based on Differentiable Tree for Interpretable Hierarchical Language Modeling",
    "volume": "long",
    "abstract": "Human language understanding operates at multiple levels of granularity (e.g., words, phrases, and sentences) with increasing levels of abstraction that can be hierarchically combined. However, existing deep models with stacked layers do not explicitly model any sort of hierarchical process. In this paper, we propose a recursive Transformer model based on differentiable CKY style binary trees to emulate this composition process, and we extend the bidirectional language model pre-training objective to this architecture, attempting to predict each word given its left and right abstraction nodes. To scale up our approach, we also introduce an efficient pruning and growing algorithm to reduce the time complexity and enable encoding in linear time. Experimental results on language modeling and unsupervised parsing show the effectiveness of our approach",
    "checked": true,
    "id": "dab75d54f477c39529f2a88ae8ab26a1ea5bdb31",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Xiang Hu",
      "Haitao Mi",
      "Zujie Wen",
      "Yafang Wang",
      "Yi Su",
      "Jing Zheng",
      "Gerard de Melo"
    ]
  },
  "https://aclanthology.org/2021.acl-long.380": {
    "title": "Risk Minimization for Zero-shot Sequence Labeling",
    "volume": "long",
    "abstract": "Zero-shot sequence labeling aims to build a sequence labeler without human-annotated datasets. One straightforward approach is utilizing existing systems (source models) to generate pseudo-labeled datasets and train a target sequence labeler accordingly. However, due to the gap between the source and the target languages/domains, this approach may fail to recover the true labels. In this paper, we propose a novel unified framework for zero-shot sequence labeling with minimum risk training and design a new decomposable risk function that models the relations between the predicted labels from the source models and the true labels. By making the risk function trainable, we draw a connection between minimum risk training and latent variable model learning. We propose a unified learning algorithm based on the expectation maximization (EM) algorithm. We extensively evaluate our proposed approaches on cross-lingual/domain sequence labeling tasks over twenty-one datasets. The results show that our approaches outperform state-of-the-art baseline systems",
    "checked": true,
    "id": "961a9f9529a37484f216d5c4086d277c2fffa89e",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Zechuan Hu",
      "Yong Jiang",
      "Nguyen Bach",
      "Tao Wang",
      "Zhongqiang Huang",
      "Fei Huang",
      "Kewei Tu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.381": {
    "title": "WARP: Word-level Adversarial ReProgramming",
    "volume": "long",
    "abstract": "Transfer learning from pretrained language models recently became the dominant approach for solving many NLP tasks. A common approach to transfer learning for multiple tasks that maximize parameter sharing trains one or more task-specific layers on top of the language model. In this paper, we present an alternative approach based on adversarial reprogramming, which extends earlier work on automatic prompt generation. Adversarial reprogramming attempts to learn task-specific word embeddings that, when concatenated to the input text, instruct the language model to solve the specified task. Using up to 25K trainable parameters per task, this approach outperforms all existing methods with up to 25M trainable parameters on the public leaderboard of the GLUE benchmark. Our method, initialized with task-specific human-readable prompts, also works in a few-shot setting, outperforming GPT-3 on two SuperGLUE tasks with just 32 training samples",
    "checked": true,
    "id": "5a2e45ce35fb26ab70a61b424a49f8e5b4532a8e",
    "semantic_title": "",
    "citation_count": 144,
    "authors": [
      "Karen Hambardzumyan",
      "Hrant Khachatrian",
      "Jonathan May"
    ]
  },
  "https://aclanthology.org/2021.acl-long.382": {
    "title": "Lexicon Learning for Few Shot Sequence Modeling",
    "volume": "long",
    "abstract": "Sequence-to-sequence transduction is the core problem in language processing applications as diverse as semantic parsing, machine translation, and instruction following. The neural network models that provide the dominant solution to these problems are brittle, especially in low-resource settings: they fail to generalize correctly or systematically from small datasets. Past work has shown that many failures of systematic generalization arise from neural models’ inability to disentangle lexical phenomena from syntactic ones. To address this, we augment neural decoders with a lexical translation mechanism that generalizes existing copy mechanisms to incorporate learned, decontextualized, token-level translation rules. We describe how to initialize this mechanism using a variety of lexicon learning algorithms, and show that it improves systematic generalization on a diverse set of sequence modeling tasks drawn from cognitive science, formal semantics, and machine translation",
    "checked": true,
    "id": "83a028e00b8b3d9ec7d38056ebd0f3a96d0d7f34",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Ekin Akyurek",
      "Jacob Andreas"
    ]
  },
  "https://aclanthology.org/2021.acl-long.383": {
    "title": "Personalized Transformer for Explainable Recommendation",
    "volume": "long",
    "abstract": "Personalization of natural language generation plays a vital role in a large spectrum of tasks, such as explainable recommendation, review summarization and dialog systems. In these tasks, user and item IDs are important identifiers for personalization. Transformer, which is demonstrated with strong language modeling capability, however, is not personalized and fails to make use of the user and item IDs since the ID tokens are not even in the same semantic space as the words. To address this problem, we present a PErsonalized Transformer for Explainable Recommendation (PETER), on which we design a simple and effective learning objective that utilizes the IDs to predict the words in the target explanation, so as to endow the IDs with linguistic meanings and to achieve personalized Transformer. Besides generating explanations, PETER can also make recommendations, which makes it a unified model for the whole recommendation-explanation pipeline. Extensive experiments show that our small unpretrained model outperforms fine-tuned BERT on the generation task, in terms of both effectiveness and efficiency, which highlights the importance and the nice utility of our design",
    "checked": true,
    "id": "0c9a2adda11ed49d091948211fcfd517113b5243",
    "semantic_title": "",
    "citation_count": 53,
    "authors": [
      "Lei Li",
      "Yongfeng Zhang",
      "Li Chen"
    ]
  },
  "https://aclanthology.org/2021.acl-long.384": {
    "title": "Generating SOAP Notes from Doctor-Patient Conversations Using Modular Summarization Techniques",
    "volume": "long",
    "abstract": "Following each patient visit, physicians draft long semi-structured clinical summaries called SOAP notes. While invaluable to clinicians and researchers, creating digital SOAP notes is burdensome, contributing to physician burnout. In this paper, we introduce the first complete pipelines to leverage deep summarization models to generate these notes based on transcripts of conversations between physicians and patients. After exploring a spectrum of methods across the extractive-abstractive spectrum, we propose Cluster2Sent, an algorithm that (i) extracts important utterances relevant to each summary section; (ii) clusters together related utterances; and then (iii) generates one summary sentence per cluster. Cluster2Sent outperforms its purely abstractive counterpart by 8 ROUGE-1 points, and produces significantly more factual and coherent sentences as assessed by expert human evaluators. For reproducibility, we demonstrate similar benefits on the publicly available AMI dataset. Our results speak to the benefits of structuring summaries into sections and annotating supporting evidence when constructing summarization corpora",
    "checked": true,
    "id": "732a2f3532e41e05465e2b3a9b5eebc2ceb483c1",
    "semantic_title": "",
    "citation_count": 34,
    "authors": [
      "Kundan Krishna",
      "Sopan Khosla",
      "Jeffrey Bigham",
      "Zachary C. Lipton"
    ]
  },
  "https://aclanthology.org/2021.acl-long.385": {
    "title": "Tail-to-Tail Non-Autoregressive Sequence Prediction for Chinese Grammatical Error Correction",
    "volume": "long",
    "abstract": "We investigate the problem of Chinese Grammatical Error Correction (CGEC) and present a new framework named Tail-to-Tail (TtT) non-autoregressive sequence prediction to address the deep issues hidden in CGEC. Considering that most tokens are correct and can be conveyed directly from source to target, and the error positions can be estimated and corrected based on the bidirectional context information, thus we employ a BERT-initialized Transformer Encoder as the backbone model to conduct information modeling and conveying. Considering that only relying on the same position substitution cannot handle the variable-length correction cases, various operations such substitution, deletion, insertion, and local paraphrasing are required jointly. Therefore, a Conditional Random Fields (CRF) layer is stacked on the up tail to conduct non-autoregressive sequence prediction by modeling the token dependencies. Since most tokens are correct and easily to be predicted/conveyed to the target, then the models may suffer from a severe class imbalance issue. To alleviate this problem, focal loss penalty strategies are integrated into the loss functions. Moreover, besides the typical fix-length error correction datasets, we also construct a variable-length corpus to conduct experiments. Experimental results on standard datasets, especially on the variable-length datasets, demonstrate the effectiveness of TtT in terms of sentence-level Accuracy, Precision, Recall, and F1-Measure on tasks of error Detection and Correction",
    "checked": true,
    "id": "133a5c9a15b734195d4ecb41c9f3fee01a8e8fd9",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Piji Li",
      "Shuming Shi"
    ]
  },
  "https://aclanthology.org/2021.acl-long.386": {
    "title": "Early Detection of Sexual Predators in Chats",
    "volume": "long",
    "abstract": "An important risk that children face today is online grooming, where a so-called sexual predator establishes an emotional connection with a minor online with the objective of sexual abuse. Prior work has sought to automatically identify grooming chats, but only after an incidence has already happened in the context of legal prosecution. In this work, we instead investigate this problem from the point of view of prevention. We define and study the task of early sexual predator detection (eSPD) in chats, where the goal is to analyze a running chat from its beginning and predict grooming attempts as early and as accurately as possible. We survey existing datasets and their limitations regarding eSPD, and create a new dataset called PANC for more realistic evaluations. We present strong baselines built on BERT that also reach state-of-the-art results for conventional SPD. Finally, we consider coping with limited computational resources, as real-life applications require eSPD on mobile devices",
    "checked": true,
    "id": "fe9d8baf4dd0c5973f11094fbab95d34fd3a36fd",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Matthias Vogt",
      "Ulf Leser",
      "Alan Akbik"
    ]
  },
  "https://aclanthology.org/2021.acl-long.387": {
    "title": "Writing by Memorizing: Hierarchical Retrieval-based Medical Report Generation",
    "volume": "long",
    "abstract": "Medical report generation is one of the most challenging tasks in medical image analysis. Although existing approaches have achieved promising results, they either require a predefined template database in order to retrieve sentences or ignore the hierarchical nature of medical report generation. To address these issues, we propose MedWriter that incorporates a novel hierarchical retrieval mechanism to automatically extract both report and sentence-level templates for clinically accurate report generation. MedWriter first employs the Visual-Language Retrieval (VLR) module to retrieve the most relevant reports for the given images. To guarantee the logical coherence between generated sentences, the Language-Language Retrieval (LLR) module is introduced to retrieve relevant sentences based on the previous generated description. At last, a language decoder fuses image features and features from retrieved reports and sentences to generate meaningful medical reports. We verified the effectiveness of our model by automatic evaluation and human evaluation on two datasets, i.e., Open-I and MIMIC-CXR",
    "checked": true,
    "id": "84bab90970575f07eb2380453b51191f3aff6fcd",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Xingyi Yang",
      "Muchao Ye",
      "Quanzeng You",
      "Fenglong Ma"
    ]
  },
  "https://aclanthology.org/2021.acl-long.388": {
    "title": "Concept-Based Label Embedding via Dynamic Routing for Hierarchical Text Classification",
    "volume": "long",
    "abstract": "Hierarchical Text Classification (HTC) is a challenging task that categorizes a textual description within a taxonomic hierarchy. Most of the existing methods focus on modeling the text. Recently, researchers attempt to model the class representations with some resources (e.g., external dictionaries). However, the concept shared among classes which is a kind of domain-specific and fine-grained information has been ignored in previous work. In this paper, we propose a novel concept-based label embedding method that can explicitly represent the concept and model the sharing mechanism among classes for the hierarchical text classification. Experimental results on two widely used datasets prove that the proposed model outperforms several state-of-the-art methods. We release our complementary resources (concepts and definitions of classes) for these two datasets to benefit the research on HTC",
    "checked": true,
    "id": "2fe7d9a0983b11fa817f14cb2da4855a6267a945",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Xuepeng Wang",
      "Li Zhao",
      "Bing Liu",
      "Tao Chen",
      "Feng Zhang",
      "Di Wang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.389": {
    "title": "VisualSparta: An Embarrassingly Simple Approach to Large-scale Text-to-Image Search with Weighted Bag-of-words",
    "volume": "long",
    "abstract": "Text-to-image retrieval is an essential task in cross-modal information retrieval, i.e., retrieving relevant images from a large and unlabelled dataset given textual queries. In this paper, we propose VisualSparta, a novel (Visual-text Sparse Transformer Matching) model that shows significant improvement in terms of both accuracy and efficiency. VisualSparta is capable of outperforming previous state-of-the-art scalable methods in MSCOCO and Flickr30K. We also show that it achieves substantial retrieving speed advantages, i.e., for a 1 million image index, VisualSparta using CPU gets ~391X speedup compared to CPU vector search and ~5.4X speedup compared to vector search with GPU acceleration. Experiments show that this speed advantage even gets bigger for larger datasets because VisualSparta can be efficiently implemented as an inverted index. To the best of our knowledge, VisualSparta is the first transformer-based text-to-image retrieval model that can achieve real-time searching for large-scale datasets, with significant accuracy improvement compared to previous state-of-the-art methods",
    "checked": true,
    "id": "30b21df1ae0c2b918e48435e739410e49db7638a",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Xiaopeng Lu",
      "Tiancheng Zhao",
      "Kyusong Lee"
    ]
  },
  "https://aclanthology.org/2021.acl-long.390": {
    "title": "Few-Shot Text Ranking with Meta Adapted Synthetic Weak Supervision",
    "volume": "long",
    "abstract": "The effectiveness of Neural Information Retrieval (Neu-IR) often depends on a large scale of in-domain relevance training signals, which are not always available in real-world ranking scenarios. To democratize the benefits of Neu-IR, this paper presents MetaAdaptRank, a domain adaptive learning method that generalizes Neu-IR models from label-rich source domains to few-shot target domains. Drawing on source-domain massive relevance supervision, MetaAdaptRank contrastively synthesizes a large number of weak supervision signals for target domains and meta-learns to reweight these synthetic “weak” data based on their benefits to the target-domain ranking accuracy of Neu-IR models. Experiments on three TREC benchmarks in the web, news, and biomedical domains show that MetaAdaptRank significantly improves the few-shot ranking accuracy of Neu-IR models. Further analyses indicate that MetaAdaptRank thrives from both its contrastive weak data synthesis and meta-reweighted data selection. The code and data of this paper can be obtained from https://github.com/thunlp/MetaAdaptRank",
    "checked": true,
    "id": "65c2d2ffe45569101860a7defc7cccbd36b3602a",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Si Sun",
      "Yingzhuo Qian",
      "Zhenghao Liu",
      "Chenyan Xiong",
      "Kaitao Zhang",
      "Jie Bao",
      "Zhiyuan Liu",
      "Paul Bennett"
    ]
  },
  "https://aclanthology.org/2021.acl-long.391": {
    "title": "Semi-Supervised Text Classification with Balanced Deep Representation Distributions",
    "volume": "long",
    "abstract": "Semi-Supervised Text Classification (SSTC) mainly works under the spirit of self-training. They initialize the deep classifier by training over labeled texts; and then alternatively predict unlabeled texts as their pseudo-labels and train the deep classifier over the mixture of labeled and pseudo-labeled texts. Naturally, their performance is largely affected by the accuracy of pseudo-labels for unlabeled texts. Unfortunately, they often suffer from low accuracy because of the margin bias problem caused by the large difference between representation distributions of labels in SSTC. To alleviate this problem, we apply the angular margin loss, and perform Gaussian linear transformation to achieve balanced label angle variances, i.e., the variance of label angles of texts within the same label. More accuracy of predicted pseudo-labels can be achieved by constraining all label angle variances balanced, where they are estimated over both labeled and pseudo-labeled texts during self-training loops. With this insight, we propose a novel SSTC method, namely Semi-Supervised Text Classification with Balanced Deep representation Distributions (S2TC-BDD). To evaluate S2TC-BDD, we compare it against the state-of-the-art SSTC methods. Empirical results demonstrate the effectiveness of S2TC-BDD, especially when the labeled texts are scarce",
    "checked": true,
    "id": "0c049234515bfd17c642862af4cded5ba6d2fb82",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Changchun Li",
      "Ximing Li",
      "Jihong Ouyang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.392": {
    "title": "Improving Document Representations by Generating Pseudo Query Embeddings for Dense Retrieval",
    "volume": "long",
    "abstract": "Recently, the retrieval models based on dense representations have been gradually applied in the first stage of the document retrieval tasks, showing better performance than traditional sparse vector space models. To obtain high efficiency, the basic structure of these models is Bi-encoder in most cases. However, this simple structure may cause serious information loss during the encoding of documents since the queries are agnostic. To address this problem, we design a method to mimic the queries to each of the documents by an iterative clustering process and represent the documents by multiple pseudo queries (i.e., the cluster centroids). To boost the retrieval process using approximate nearest neighbor search library, we also optimize the matching function with a two-step score calculation procedure. Experimental results on several popular ranking and QA datasets show that our model can achieve state-of-the-art results while still remaining high efficiency",
    "checked": true,
    "id": "2a72251ed53ffe138b87d7c9c63b9dac65cdd65d",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Hongyin Tang",
      "Xingwu Sun",
      "Beihong Jin",
      "Jingang Wang",
      "Fuzheng Zhang",
      "Wei Wu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.393": {
    "title": "ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer",
    "volume": "long",
    "abstract": "Learning high-quality sentence representations benefits a wide range of natural language processing tasks. Though BERT-based pre-trained language models achieve high performance on many downstream tasks, the native derived sentence representations are proved to be collapsed and thus produce a poor performance on the semantic textual similarity (STS) tasks. In this paper, we present ConSERT, a Contrastive Framework for Self-Supervised SEntence Representation Transfer, that adopts contrastive learning to fine-tune BERT in an unsupervised and effective way. By making use of unlabeled texts, ConSERT solves the collapse issue of BERT-derived sentence representations and make them more applicable for downstream tasks. Experiments on STS datasets demonstrate that ConSERT achieves an 8% relative improvement over the previous state-of-the-art, even comparable to the supervised SBERT-NLI. And when further incorporating NLI supervision, we achieve new state-of-the-art performance on STS tasks. Moreover, ConSERT obtains comparable results with only 1000 samples available, showing its robustness in data scarcity scenarios",
    "checked": true,
    "id": "077c713bccd9d2c7fde68d4cbde06ab0f07a6855",
    "semantic_title": "",
    "citation_count": 232,
    "authors": [
      "Yuanmeng Yan",
      "Rumei Li",
      "Sirui Wang",
      "Fuzheng Zhang",
      "Wei Wu",
      "Weiran Xu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.394": {
    "title": "Exploring Dynamic Selection of Branch Expansion Orders for Code Generation",
    "volume": "long",
    "abstract": "Due to the great potential in facilitating software development, code generation has attracted increasing attention recently. Generally, dominant models are Seq2Tree models, which convert the input natural language description into a sequence of tree-construction actions corresponding to the pre-order traversal of an Abstract Syntax Tree (AST). However, such a traversal order may not be suitable for handling all multi-branch nodes. In this paper, we propose to equip the Seq2Tree model with a context-based Branch Selector, which is able to dynamically determine optimal expansion orders of branches for multi-branch nodes. Particularly, since the selection of expansion orders is a non-differentiable multi-step operation, we optimize the selector through reinforcement learning, and formulate the reward function as the difference of model losses obtained through different expansion orders. Experimental results and in-depth analysis on several commonly-used datasets demonstrate the effectiveness and generality of our approach. We have released our code at https://github.com/DeepLearnXMU/CG-RL",
    "checked": true,
    "id": "3ca1430fb5bbf6ffa8f377f6b603648268f7546e",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Hui Jiang",
      "Chulun Zhou",
      "Fandong Meng",
      "Biao Zhang",
      "Jie Zhou",
      "Degen Huang",
      "Qingqiang Wu",
      "Jinsong Su"
    ]
  },
  "https://aclanthology.org/2021.acl-long.395": {
    "title": "COINS: Dynamically Generating COntextualized Inference Rules for Narrative Story Completion",
    "volume": "long",
    "abstract": "Despite recent successes of large pre-trained language models in solving reasoning tasks, their inference capabilities remain opaque. We posit that such models can be made more interpretable by explicitly generating interim inference rules, and using them to guide the generation of task-specific textual outputs. In this paper we present Coins, a recursive inference framework that i) iteratively reads context sentences, ii) dynamically generates contextualized inference rules, encodes them, and iii) uses them to guide task-specific output generation. We apply to a Narrative Story Completion task that asks a model to complete a story with missing sentences, to produce a coherent story with plausible logical connections, causal relationships, and temporal dependencies. By modularizing inference and sentence generation steps in a recurrent model, we aim to make reasoning steps and their effects on next sentence generation transparent. Our automatic and manual evaluations show that the model generates better story sentences than SOTA baselines, especially in terms of coherence. We further demonstrate improved performance over strong pre-trained LMs in generating commonsense inference rules. The recursive nature of holds the potential for controlled generation of longer sequences",
    "checked": true,
    "id": "411a621e9822c2bd33e5022ee2627a9de5c4d238",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Debjit Paul",
      "Anette Frank"
    ]
  },
  "https://aclanthology.org/2021.acl-long.396": {
    "title": "Reasoning over Entity-Action-Location Graph for Procedural Text Understanding",
    "volume": "long",
    "abstract": "Procedural text understanding aims at tracking the states (e.g., create, move, destroy) and locations of the entities mentioned in a given paragraph. To effectively track the states and locations, it is essential to capture the rich semantic relations between entities, actions, and locations in the paragraph. Although recent works have achieved substantial progress, most of them focus on leveraging the inherent constraints or incorporating external knowledge for state prediction. The rich semantic relations in the given paragraph are largely overlooked. In this paper, we propose a novel approach (REAL) to procedural text understanding, where we build a general framework to systematically model the entity-entity, entity-action, and entity-location relations using a graph neural network. We further develop algorithms for graph construction, representation learning, and state and location tracking. We evaluate the proposed approach on two benchmark datasets, ProPara, and Recipes. The experimental results show that our method outperforms strong baselines by a large margin, i.e., 5.0% on ProPara and 3.2% on Recipes, illustrating the utility of semantic relations and the effectiveness of the graph-based reasoning model",
    "checked": true,
    "id": "ebd088efdf62348fe4b6b353e3c84dec93bee54f",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Hao Huang",
      "Xiubo Geng",
      "Jian Pei",
      "Guodong Long",
      "Daxin Jiang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.397": {
    "title": "From Paraphrasing to Semantic Parsing: Unsupervised Semantic Parsing via Synchronous Semantic Decoding",
    "volume": "long",
    "abstract": "Semantic parsing is challenging due to the structure gap and the semantic gap between utterances and logical forms. In this paper, we propose an unsupervised semantic parsing method - Synchronous Semantic Decoding (SSD), which can simultaneously resolve the semantic gap and the structure gap by jointly leveraging paraphrasing and grammar-constrained decoding. Specifically, we reformulate semantic parsing as a constrained paraphrasing problem: given an utterance, our model synchronously generates its canonical utterancel and meaning representation. During synchronously decoding: the utterance paraphrasing is constrained by the structure of the logical form, therefore the canonical utterance can be paraphrased controlledly; the semantic decoding is guided by the semantics of the canonical utterance, therefore its logical form can be generated unsupervisedly. Experimental results show that SSD is a promising approach and can achieve state-of-the-art unsupervised semantic parsing performance on multiple datasets",
    "checked": true,
    "id": "f8a7aa9c556703d27dd5bf98265de1ff89175635",
    "semantic_title": "",
    "citation_count": 17,
    "authors": [
      "Shan Wu",
      "Bo Chen",
      "Chunlei Xin",
      "Xianpei Han",
      "Le Sun",
      "Weipeng Zhang",
      "Jiansong Chen",
      "Fan Yang",
      "Xunliang Cai"
    ]
  },
  "https://aclanthology.org/2021.acl-long.398": {
    "title": "Pre-training Universal Language Representation",
    "volume": "long",
    "abstract": "Despite the well-developed cut-edge representation learning for language, most language representation models usually focus on specific levels of linguistic units. This work introduces universal language representation learning, i.e., embeddings of different levels of linguistic units or text with quite diverse lengths in a uniform vector space. We propose the training objective MiSAD that utilizes meaningful n-grams extracted from large unlabeled corpus by a simple but effective algorithm for pre-trained language models. Then we empirically verify that well designed pre-training scheme may effectively yield universal language representation, which will bring great convenience when handling multiple layers of linguistic objects in a unified way. Especially, our model achieves the highest accuracy on analogy tasks in different language levels and significantly improves the performance on downstream tasks in the GLUE benchmark and a question answering dataset",
    "checked": true,
    "id": "ccd42cb2b846336f93a79d93a162c3dc936f6db2",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Yian Li",
      "Hai Zhao"
    ]
  },
  "https://aclanthology.org/2021.acl-long.399": {
    "title": "Structural Pre-training for Dialogue Comprehension",
    "volume": "long",
    "abstract": "Pre-trained language models (PrLMs) have demonstrated superior performance due to their strong ability to learn universal language representations from self-supervised pre-training. However, even with the help of the powerful PrLMs, it is still challenging to effectively capture task-related knowledge from dialogue texts which are enriched by correlations among speaker-aware utterances. In this work, we present SPIDER, Structural Pre-traIned DialoguE Reader, to capture dialogue exclusive features. To simulate the dialogue-like features, we propose two training objectives in addition to the original LM objectives: 1) utterance order restoration, which predicts the order of the permuted utterances in dialogue context; 2) sentence backbone regularization, which regularizes the model to improve the factual correctness of summarized subject-verb-object triplets. Experimental results on widely used dialogue benchmarks verify the effectiveness of the newly introduced self-supervised tasks",
    "checked": true,
    "id": "a03844a1cb957feae7ded3a327cd3a445e2175ad",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Zhuosheng Zhang",
      "Hai Zhao"
    ]
  },
  "https://aclanthology.org/2021.acl-long.400": {
    "title": "AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models",
    "volume": "long",
    "abstract": "Pre-trained language models (PLMs) have achieved great success in natural language processing. Most of PLMs follow the default setting of architecture hyper-parameters (e.g., the hidden dimension is a quarter of the intermediate dimension in feed-forward sub-networks) in BERT. Few studies have been conducted to explore the design of architecture hyper-parameters in BERT, especially for the more efficient PLMs with tiny sizes, which are essential for practical deployment on resource-constrained devices. In this paper, we adopt the one-shot Neural Architecture Search (NAS) to automatically search architecture hyper-parameters. Specifically, we carefully design the techniques of one-shot learning and the search space to provide an adaptive and efficient development way of tiny PLMs for various latency constraints. We name our method AutoTinyBERT and evaluate its effectiveness on the GLUE and SQuAD benchmarks. The extensive experiments show that our method outperforms both the SOTA search-based baseline (NAS-BERT) and the SOTA distillation-based methods (such as DistilBERT, TinyBERT, MiniLM, and MobileBERT). In addition, based on the obtained architectures, we propose a more efficient development method that is even faster than the development of a single PLM. The source code and models will be publicly available upon publication",
    "checked": true,
    "id": "ef18db2a18ac61e72783a613328842ce86ef00bf",
    "semantic_title": "",
    "citation_count": 22,
    "authors": [
      "Yichun Yin",
      "Cheng Chen",
      "Lifeng Shang",
      "Xin Jiang",
      "Xiao Chen",
      "Qun Liu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.401": {
    "title": "Data Augmentation with Adversarial Training for Cross-Lingual NLI",
    "volume": "long",
    "abstract": "Due to recent pretrained multilingual representation models, it has become feasible to exploit labeled data from one language to train a cross-lingual model that can then be applied to multiple new languages. In practice, however, we still face the problem of scarce labeled data, leading to subpar results. In this paper, we propose a novel data augmentation strategy for better cross-lingual natural language inference by enriching the data to reflect more diversity in a semantically faithful way. To this end, we propose two methods of training a generative model to induce synthesized examples, and then leverage the resulting data using an adversarial training regimen for more robustness. In a series of detailed experiments, we show that this fruitful combination leads to substantial gains in cross-lingual inference",
    "checked": true,
    "id": "0ce24e44400f91d86ba337377a5093d5ff8923c8",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Xin Dong",
      "Yaxin Zhu",
      "Zuohui Fu",
      "Dongkuan Xu",
      "Gerard de Melo"
    ]
  },
  "https://aclanthology.org/2021.acl-long.402": {
    "title": "Bootstrapped Unsupervised Sentence Representation Learning",
    "volume": "long",
    "abstract": "As high-quality labeled data is scarce, unsupervised sentence representation learning has attracted much attention. In this paper, we propose a new framework with a two-branch Siamese Network which maximizes the similarity between two augmented views of each sentence. Specifically, given one augmented view of the input sentence, the online network branch is trained by predicting the representation yielded by the target network of the same sentence under another augmented view. Meanwhile, the target network branch is bootstrapped with a moving average of the online network. The proposed method significantly outperforms other state-of-the-art unsupervised methods on semantic textual similarity (STS) and classification tasks. It can be adopted as a post-training procedure to boost the performance of the supervised methods. We further extend our method for learning multilingual sentence representations and demonstrate its effectiveness on cross-lingual STS tasks. Our code is available at https://github.com/yanzhangnlp/BSL",
    "checked": true,
    "id": "a08fcd1b32ed7f6f0a7ad5500a4871516d175b7a",
    "semantic_title": "",
    "citation_count": 21,
    "authors": [
      "Yan Zhang",
      "Ruidan He",
      "Zuozhu Liu",
      "Lidong Bing",
      "Haizhou Li"
    ]
  },
  "https://aclanthology.org/2021.acl-long.403": {
    "title": "Learning Event Graph Knowledge for Abductive Reasoning",
    "volume": "long",
    "abstract": "Abductive reasoning aims at inferring the most plausible explanation for observed events, which would play critical roles in various NLP applications, such as reading comprehension and question answering. To facilitate this task, a narrative text based abductive reasoning task 𝛼NLI is proposed, together with explorations about building reasoning framework using pretrained language models. However, abundant event commonsense knowledge is not well exploited for this task. To fill this gap, we propose a variational autoencoder based model ege-RoBERTa, which employs a latent variable to capture the necessary commonsense knowledge from event graph for guiding the abductive reasoning task. Experimental results show that through learning the external event graph knowledge, our approach outperforms the baseline methods on the 𝛼NLI task",
    "checked": true,
    "id": "ee4385aaae742a14a5db828a5076208c9b33da88",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Li Du",
      "Xiao Ding",
      "Ting Liu",
      "Bing Qin"
    ]
  },
  "https://aclanthology.org/2021.acl-long.404": {
    "title": "A Cognitive Regularizer for Language Modeling",
    "volume": "long",
    "abstract": "The uniform information density (UID) hypothesis, which posits that speakers behaving optimally tend to distribute information uniformly across a linguistic signal, has gained traction in psycholinguistics as an explanation for certain syntactic, morphological, and prosodic choices. In this work, we explore whether the UID hypothesis can be operationalized as an inductive bias for statistical language modeling. Specifically, we augment the canonical MLE objective for training language models with a regularizer that encodes UID. In experiments on ten languages spanning five language families, we find that using UID regularization consistently improves perplexity in language models, having a larger effect when training data is limited. Moreover, via an analysis of generated sequences, we find that UID-regularized language models have other desirable properties, e.g., they generate text that is more lexically diverse. Our results not only suggest that UID is a reasonable inductive bias for language modeling, but also provide an alternative validation of the UID hypothesis using modern-day NLP tools",
    "checked": true,
    "id": "06994c2810c2720b302153fc73f8c4459f05fda7",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Jason Wei",
      "Clara Meister",
      "Ryan Cotterell"
    ]
  },
  "https://aclanthology.org/2021.acl-long.405": {
    "title": "Lower Perplexity is Not Always Human-Like",
    "volume": "long",
    "abstract": "In computational psycholinguistics, various language models have been evaluated against human reading behavior (e.g., eye movement) to build human-like computational models. However, most previous efforts have focused almost exclusively on English, despite the recent trend towards linguistic universal within the general community. In order to fill the gap, this paper investigates whether the established results in computational psycholinguistics can be generalized across languages. Specifically, we re-examine an established generalization —the lower perplexity a language model has, the more human-like the language model is— in Japanese with typologically different structures from English. Our experiments demonstrate that this established generalization exhibits a surprising lack of universality; namely, lower perplexity is not always human-like. Moreover, this discrepancy between English and Japanese is further explored from the perspective of (non-)uniform information density. Overall, our results suggest that a cross-lingual evaluation will be necessary to construct human-like computational models",
    "checked": true,
    "id": "36936e63fb81e05a775fa4d0b7513ec8313f5e56",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Tatsuki Kuribayashi",
      "Yohei Oseki",
      "Takumi Ito",
      "Ryo Yoshida",
      "Masayuki Asahara",
      "Kentaro Inui"
    ]
  },
  "https://aclanthology.org/2021.acl-long.406": {
    "title": "Word Sense Disambiguation: Towards Interactive Context Exploitation from Both Word and Sense Perspectives",
    "volume": "long",
    "abstract": "Lately proposed Word Sense Disambiguation (WSD) systems have approached the estimated upper bound of the task on standard evaluation benchmarks. However, these systems typically implement the disambiguation of words in a document almost independently, underutilizing sense and word dependency in context. In this paper, we convert the nearly isolated decisions into interrelated ones by exposing senses in context when learning sense embeddings in a similarity-based Sense Aware Context Exploitation (SACE) architecture. Meanwhile, we enhance the context embedding learning with selected sentences from the same document, rather than utilizing only the sentence where each ambiguous word appears. Experiments on both English and multilingual WSD datasets have shown the effectiveness of our approach, surpassing previous state-of-the-art by large margins (3.7% and 1.2% respectively), especially on few-shot (14.3%) and zero-shot (35.9%) scenarios",
    "checked": true,
    "id": "c0e3964e451a75b4bcf4defb5324ca91f54c4b83",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Ming Wang",
      "Yinglin Wang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.407": {
    "title": "A Knowledge-Guided Framework for Frame Identification",
    "volume": "long",
    "abstract": "Frame Identification (FI) is a fundamental and challenging task in frame semantic parsing. The task aims to find the exact frame evoked by a target word in a given sentence. It is generally regarded as a classification task in existing work, where frames are treated as discrete labels or represented using onehot embeddings. However, the valuable knowledge about frames is neglected. In this paper, we propose a Knowledge-Guided Frame Identification framework (KGFI) that integrates three types frame knowledge, including frame definitions, frame elements and frame-to-frame relations, to learn better frame representation, which guides the KGFI to jointly map target words and frames into the same embedding space and subsequently identify the best frame by calculating the dot-product similarity scores between the target word embedding and all of the frame embeddings. The extensive experimental results demonstrate KGFI significantly outperforms the state-of-the-art methods on two benchmark datasets",
    "checked": true,
    "id": "169082d38acf2af7efd6f76a320f6f5e2e0f799f",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Xuefeng Su",
      "Ru Li",
      "Xiaoli Li",
      "Jeff Z. Pan",
      "Hu Zhang",
      "Qinghua Chai",
      "Xiaoqi Han"
    ]
  },
  "https://aclanthology.org/2021.acl-long.408": {
    "title": "Obtaining Better Static Word Embeddings Using Contextual Embedding Models",
    "volume": "long",
    "abstract": "The advent of contextual word embeddings — representations of words which incorporate semantic and syntactic information from their context—has led to tremendous improvements on a wide variety of NLP tasks. However, recent contextual models have prohibitively high computational cost in many use-cases and are often hard to interpret. In this work, we demonstrate that our proposed distillation method, which is a simple extension of CBOW-based training, allows to significantly improve computational efficiency of NLP applications, while outperforming the quality of existing static embeddings trained from scratch as well as those distilled from previously proposed methods. As a side-effect, our approach also allows a fair comparison of both contextual and static embeddings via standard lexical evaluation tasks",
    "checked": true,
    "id": "440a7cd665293cfb149a7434668fb3079e43e2aa",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Prakhar Gupta",
      "Martin Jaggi"
    ]
  },
  "https://aclanthology.org/2021.acl-long.409": {
    "title": "Meta-Learning with Variational Semantic Memory for Word Sense Disambiguation",
    "volume": "long",
    "abstract": "A critical challenge faced by supervised word sense disambiguation (WSD) is the lack of large annotated datasets with sufficient coverage of words in their diversity of senses. This inspired recent research on few-shot WSD using meta-learning. While such work has successfully applied meta-learning to learn new word senses from very few examples, its performance still lags behind its fully-supervised counterpart. Aiming to further close this gap, we propose a model of semantic memory for WSD in a meta-learning setting. Semantic memory encapsulates prior experiences seen throughout the lifetime of the model, which aids better generalization in limited data settings. Our model is based on hierarchical variational inference and incorporates an adaptive memory update rule via a hypernetwork. We show our model advances the state of the art in few-shot WSD, supports effective learning in extremely data scarce (e.g. one-shot) scenarios and produces meaning prototypes that capture similar senses of distinct words",
    "checked": true,
    "id": "74592257b9812f7d3a4a8a1cec31d6a1fd174c2f",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Yingjun Du",
      "Nithin Holla",
      "Xiantong Zhen",
      "Cees Snoek",
      "Ekaterina Shutova"
    ]
  },
  "https://aclanthology.org/2021.acl-long.410": {
    "title": "LexFit: Lexical Fine-Tuning of Pretrained Language Models",
    "volume": "long",
    "abstract": "Transformer-based language models (LMs) pretrained on large text collections implicitly store a wealth of lexical semantic knowledge, but it is non-trivial to extract that knowledge effectively from their parameters. Inspired by prior work on semantic specialization of static word embedding (WE) models, we show that it is possible to expose and enrich lexical knowledge from the LMs, that is, to specialize them to serve as effective and universal “decontextualized” word encoders even when fed input words “in isolation” (i.e., without any context). Their transformation into such word encoders is achieved through a simple and efficient lexical fine-tuning procedure (termed LexFit) based on dual-encoder network structures. Further, we show that LexFit can yield effective word encoders even with limited lexical supervision and, via cross-lingual transfer, in different languages without any readily available external knowledge. Our evaluation over four established, structurally different lexical-level tasks in 8 languages indicates the superiority of LexFit-based WEs over standard static WEs (e.g., fastText) and WEs from vanilla LMs. Other extensive experiments and ablation studies further profile the LexFit framework, and indicate best practices and performance variations across LexFit variants, languages, and lexical tasks, also directly questioning the usefulness of traditional WE models in the era of large neural models",
    "checked": true,
    "id": "9d31cfe09f93a5d87657313c503b17619c2ae107",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Ivan Vulić",
      "Edoardo Maria Ponti",
      "Anna Korhonen",
      "Goran Glavaš"
    ]
  },
  "https://aclanthology.org/2021.acl-long.411": {
    "title": "Text-Free Image-to-Speech Synthesis Using Learned Segmental Units",
    "volume": "long",
    "abstract": "In this paper we present the first model for directly synthesizing fluent, natural-sounding spoken audio captions for images that does not require natural language text as an intermediate representation or source of supervision. Instead, we connect the image captioning module and the speech synthesis module with a set of discrete, sub-word speech units that are discovered with a self-supervised visual grounding task. We conduct experiments on the Flickr8k spoken caption dataset in addition to a novel corpus of spoken audio captions collected for the popular MSCOCO dataset, demonstrating that our generated captions also capture diverse visual semantics of the images they describe. We investigate several different intermediate speech representations, and empirically find that the representation must satisfy several important properties to serve as drop-in replacements for text",
    "checked": true,
    "id": "5df12460d9a742b08f82e8b79cb102a8be5dd9b4",
    "semantic_title": "",
    "citation_count": 32,
    "authors": [
      "Wei-Ning Hsu",
      "David Harwath",
      "Tyler Miller",
      "Christopher Song",
      "James Glass"
    ]
  },
  "https://aclanthology.org/2021.acl-long.412": {
    "title": "CTFN: Hierarchical Learning for Multimodal Sentiment Analysis Using Coupled-Translation Fusion Network",
    "volume": "long",
    "abstract": "Multimodal sentiment analysis is the challenging research area that attends to the fusion of multiple heterogeneous modalities. The main challenge is the occurrence of some missing modalities during the multimodal fusion procedure. However, the existing techniques require all modalities as input, thus are sensitive to missing modalities at predicting time. In this work, the coupled-translation fusion network (CTFN) is firstly proposed to model bi-direction interplay via couple learning, ensuring the robustness in respect to missing modalities. Specifically, the cyclic consistency constraint is presented to improve the translation performance, allowing us directly to discard decoder and only embraces encoder of Transformer. This could contribute to a much lighter model. Due to the couple learning, CTFN is able to conduct bi-direction cross-modality intercorrelation parallelly. Based on CTFN, a hierarchical architecture is further established to exploit multiple bi-direction translations, leading to double multimodal fusing embeddings compared with traditional translation methods. Moreover, the convolution block is utilized to further highlight explicit interactions among those translations. For evaluation, CTFN was verified on two multimodal benchmarks with extensive ablation studies. The experiments demonstrate that the proposed framework achieves state-of-the-art or often competitive performance. Additionally, CTFN still maintains robustness when considering missing modality",
    "checked": true,
    "id": "e42cbd2dd928367da98b4fa056de2b9b29e39ed8",
    "semantic_title": "",
    "citation_count": 18,
    "authors": [
      "Jiajia Tang",
      "Kang Li",
      "Xuanyu Jin",
      "Andrzej Cichocki",
      "Qibin Zhao",
      "Wanzeng Kong"
    ]
  },
  "https://aclanthology.org/2021.acl-long.413": {
    "title": "Positional Artefacts Propagate Through Masked Language Model Embeddings",
    "volume": "long",
    "abstract": "In this work, we demonstrate that the contextualized word vectors derived from pretrained masked language model-based encoders share a common, perhaps undesirable pattern across layers. Namely, we find cases of persistent outlier neurons within BERT and RoBERTa’s hidden state vectors that consistently bear the smallest or largest values in said vectors. In an attempt to investigate the source of this information, we introduce a neuron-level analysis method, which reveals that the outliers are closely related to information captured by positional embeddings. We also pre-train the RoBERTa-base models from scratch and find that the outliers disappear without using positional embeddings. These outliers, we find, are the major cause of anisotropy of encoders’ raw vector spaces, and clipping them leads to increased similarity across vectors. We demonstrate this in practice by showing that clipped vectors can more accurately distinguish word senses, as well as lead to better sentence embeddings when mean pooling. In three supervised tasks, we find that clipping does not affect the performance",
    "checked": true,
    "id": "829580d6fc73fa601c4982e2b1b6832f2796270b",
    "semantic_title": "",
    "citation_count": 24,
    "authors": [
      "Ziyang Luo",
      "Artur Kulmizev",
      "Xiaoxi Mao"
    ]
  },
  "https://aclanthology.org/2021.acl-long.414": {
    "title": "Language Model Evaluation Beyond Perplexity",
    "volume": "long",
    "abstract": "We propose an alternate approach to quantifying how well language models learn natural language: we ask how well they match the statistical tendencies of natural language. To answer this question, we analyze whether text generated from language models exhibits the statistical tendencies present in the human-generated text on which they were trained. We provide a framework–paired with significance tests–for evaluating the fit of language models to these trends. We find that neural language models appear to learn only a subset of the tendencies considered, but align much more closely with empirical trends than proposed theoretical distributions (when present). Further, the fit to different distributions is highly-dependent on both model architecture and generation strategy. As concrete examples, text generated under the nucleus sampling scheme adheres more closely to the type–token relationship of natural language than text produced using standard ancestral sampling; text from LSTMs reflects the natural language distributions over length, stopwords, and symbols surprisingly well",
    "checked": true,
    "id": "1fa487376af5d4293ec482d193ca1790176c4dbc",
    "semantic_title": "",
    "citation_count": 29,
    "authors": [
      "Clara Meister",
      "Ryan Cotterell"
    ]
  },
  "https://aclanthology.org/2021.acl-long.415": {
    "title": "Learning to Explain: Generating Stable Explanations Fast",
    "volume": "long",
    "abstract": "The importance of explaining the outcome of a machine learning model, especially a black-box model, is widely acknowledged. Recent approaches explain an outcome by identifying the contributions of input features to this outcome. In environments involving large black-box models or complex inputs, this leads to computationally demanding algorithms. Further, these algorithms often suffer from low stability, with explanations varying significantly across similar examples. In this paper, we propose a Learning to Explain (L2E) approach that learns the behaviour of an underlying explanation algorithm simultaneously from all training examples. Once the explanation algorithm is distilled into an explainer network, it can be used to explain new instances. Our experiments on three classification tasks, which compare our approach to six explanation algorithms, show that L2E is between 5 and 7.5×10ˆ4 times faster than these algorithms, while generating more stable explanations, and having comparable faithfulness to the black-box model",
    "checked": true,
    "id": "0ea9df70ae9e4c9c4e99e0e53046eb041c18f2cf",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Xuelin Situ",
      "Ingrid Zukerman",
      "Cecile Paris",
      "Sameen Maruf",
      "Gholamreza Haffari"
    ]
  },
  "https://aclanthology.org/2021.acl-long.416": {
    "title": "StereoSet: Measuring stereotypical bias in pretrained language models",
    "volume": "long",
    "abstract": "A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or African Americans are athletic. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real-world data, they are known to capture stereotypical biases. It is important to quantify to what extent these biases are present in them. Although this is a rapidly growing area of research, existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences, even though these models are trained on natural data 2) current evaluations focus on measuring bias without considering the language modeling ability of a model, which could lead to misleading trust on a model even if it is a poor language model. We address both these problems. We present StereoSet, a large-scale natural English dataset to measure stereotypical biases in four domains: gender, profession, race, and religion. We contrast both stereotypical bias and language modeling ability of popular models like BERT, GPT-2, RoBERTa, and XLnet. We show that these models exhibit strong stereotypical biases. Our data and code are available at https://stereoset.mit.edu",
    "checked": true,
    "id": "babeda48b10a4d638252118f2238d05a06f4ec55",
    "semantic_title": "",
    "citation_count": 342,
    "authors": [
      "Moin Nadeem",
      "Anna Bethke",
      "Siva Reddy"
    ]
  },
  "https://aclanthology.org/2021.acl-long.417": {
    "title": "Alignment Rationale for Natural Language Inference",
    "volume": "long",
    "abstract": "Deep learning models have achieved great success on the task of Natural Language Inference (NLI), though only a few attempts try to explain their behaviors. Existing explanation methods usually pick prominent features such as words or phrases from the input text. However, for NLI, alignments among words or phrases are more enlightening clues to explain the model. To this end, this paper presents AREC, a post-hoc approach to generate alignment rationale explanations for co-attention based models in NLI. The explanation is based on feature selection, which keeps few but sufficient alignments while maintaining the same prediction of the target model. Experimental results show that our method is more faithful and human-readable compared with many existing approaches. We further study and re-evaluate three typical models through our explanation beyond accuracy, and propose a simple method that greatly improves the model robustness",
    "checked": true,
    "id": "648a5496a4be5690adaa888b04469de4799fff4f",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Zhongtao Jiang",
      "Yuanzhe Zhang",
      "Zhao Yang",
      "Jun Zhao",
      "Kang Liu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.418": {
    "title": "Enabling Lightweight Fine-tuning for Pre-trained Language Model Compression based on Matrix Product Operators",
    "volume": "long",
    "abstract": "This paper presents a novel pre-trained language models (PLM) compression approach based on the matrix product operator (short as MPO) from quantum many-body physics. It can decompose an original matrix into central tensors (containing the core information) and auxiliary tensors (with only a small proportion of parameters). With the decomposed MPO structure, we propose a novel fine-tuning strategy by only updating the parameters from the auxiliary tensors, and design an optimization algorithm for MPO-based approximation over stacked network architectures. Our approach can be applied to the original or the compressed PLMs in a general way, which derives a lighter network and significantly reduces the parameters to be fine-tuned. Extensive experiments have demonstrated the effectiveness of the proposed approach in model compression, especially the reduction in fine-tuning parameters (91% reduction on average). The code to reproduce the results of this paper can be found at https://github.com/RUCAIBox/MPOP",
    "checked": true,
    "id": "e89804a4d2611450893a001c5ab5dcf8b5ad2b3a",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Peiyu Liu",
      "Ze-Feng Gao",
      "Wayne Xin Zhao",
      "Zhi-Yuan Xie",
      "Zhong-Yi Lu",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2021.acl-long.419": {
    "title": "On Sample Based Explanation Methods for NLP: Faithfulness, Efficiency and Semantic Evaluation",
    "volume": "long",
    "abstract": "In the recent advances of natural language processing, the scale of the state-of-the-art models and datasets is usually extensive, which challenges the application of sample-based explanation methods in many aspects, such as explanation interpretability, efficiency, and faithfulness. In this work, for the first time, we can improve the interpretability of explanations by allowing arbitrary text sequences as the explanation unit. On top of this, we implement a hessian-free method with a model faithfulness guarantee. Finally, to compare our method with the others, we propose a semantic-based evaluation metric that can better align with humans’ judgment of explanations than the widely adopted diagnostic or re-training measures. The empirical results on multiple real data sets demonstrate the proposed method’s superior performance to popular explanation techniques such as Influence Function or TracIn on semantic evaluation",
    "checked": true,
    "id": "1b5f117513da0e22b1c1282e831303938650919d",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Wei Zhang",
      "Ziming Huang",
      "Yada Zhu",
      "Guangnan Ye",
      "Xiaodong Cui",
      "Fan Zhang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.420": {
    "title": "Syntax-Enhanced Pre-trained Model",
    "volume": "long",
    "abstract": "We study the problem of leveraging the syntactic structure of text to enhance pre-trained models such as BERT and RoBERTa. Existing methods utilize syntax of text either in the pre-training stage or in the fine-tuning stage, so that they suffer from discrepancy between the two stages. Such a problem would lead to the necessity of having human-annotated syntactic information, which limits the application of existing methods to broader scenarios. To address this, we present a model that utilizes the syntax of text in both pre-training and fine-tuning stages. Our model is based on Transformer with a syntax-aware attention layer that considers the dependency tree of the text. We further introduce a new pre-training task of predicting the syntactic distance among tokens in the dependency tree. We evaluate the model on three downstream tasks, including relation classification, entity typing, and question answering. Results show that our model achieves state-of-the-art performance on six public benchmark datasets. We have two major findings. First, we demonstrate that infusing automatically produced syntax of text improves pre-trained models. Second, global syntactic distances among tokens bring larger performance gains compared to local head relations between contiguous tokens",
    "checked": true,
    "id": "634e8fbeba53d45828846dd541ce0a0078c57b68",
    "semantic_title": "",
    "citation_count": 28,
    "authors": [
      "Zenan Xu",
      "Daya Guo",
      "Duyu Tang",
      "Qinliang Su",
      "Linjun Shou",
      "Ming Gong",
      "Wanjun Zhong",
      "Xiaojun Quan",
      "Daxin Jiang",
      "Nan Duan"
    ]
  },
  "https://aclanthology.org/2021.acl-long.421": {
    "title": "Matching Distributions between Model and Data: Cross-domain Knowledge Distillation for Unsupervised Domain Adaptation",
    "volume": "long",
    "abstract": "Unsupervised Domain Adaptation (UDA) aims to transfer the knowledge of source domain to the unlabeled target domain. Existing methods typically require to learn to adapt the target model by exploiting the source data and sharing the network architecture across domains. However, this pipeline makes the source data risky and is inflexible for deploying the target model. This paper tackles a novel setting where only a trained source model is available and different network architectures can be adapted for target domain in terms of deployment environments. We propose a generic framework named Cross-domain Knowledge Distillation (CdKD) without needing any source data. CdKD matches the joint distributions between a trained source model and a set of target data during distilling the knowledge from the source model to the target domain. As a type of important knowledge in the source domain, for the first time, the gradient information is exploited to boost the transfer performance. Experiments on cross-domain text classification demonstrate that CdKD achieves superior performance, which verifies the effectiveness in this novel setting",
    "checked": true,
    "id": "3d06221331d8e61df2027b59f47846921294295b",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Bo Zhang",
      "Xiaoming Zhang",
      "Yun Liu",
      "Lei Cheng",
      "Zhoujun Li"
    ]
  },
  "https://aclanthology.org/2021.acl-long.422": {
    "title": "Counterfactual Inference for Text Classification Debiasing",
    "volume": "long",
    "abstract": "Today’s text classifiers inevitably suffer from unintended dataset biases, especially the document-level label bias and word-level keyword bias, which may hurt models’ generalization. Many previous studies employed data-level manipulations or model-level balancing mechanisms to recover unbiased distributions and thus prevent models from capturing the two types of biases. Unfortunately, they either suffer from the extra cost of data collection/selection/annotation or need an elaborate design of balancing strategies. Different from traditional factual inference in which debiasing occurs before or during training, counterfactual inference mitigates the influence brought by unintended confounders after training, which can make unbiased decisions with biased observations. Inspired by this, we propose a model-agnostic text classification debiasing framework – Corsair, which can effectively avoid employing data manipulations or designing balancing mechanisms. Concretely, Corsair first trains a base model on a training set directly, allowing the dataset biases ‘poison’ the trained model. In inference, given a factual input document, Corsair imagines its two counterfactual counterparts to distill and mitigate the two biases captured by the poisonous model. Extensive experiments demonstrate Corsair’s effectiveness, generalizability and fairness",
    "checked": true,
    "id": "cea684bfe852e6fccdcb7c4d8676f8cab04dccc2",
    "semantic_title": "",
    "citation_count": 31,
    "authors": [
      "Chen Qian",
      "Fuli Feng",
      "Lijie Wen",
      "Chunping Ma",
      "Pengjun Xie"
    ]
  },
  "https://aclanthology.org/2021.acl-long.423": {
    "title": "HieRec: Hierarchical User Interest Modeling for Personalized News Recommendation",
    "volume": "long",
    "abstract": "User interest modeling is critical for personalized news recommendation. Existing news recommendation methods usually learn a single user embedding for each user from their previous behaviors to represent their overall interest. However, user interest is usually diverse and multi-grained, which is difficult to be accurately modeled by a single user embedding. In this paper, we propose a news recommendation method with hierarchical user interest modeling, named HieRec. Instead of a single user embedding, in our method each user is represented in a hierarchical interest tree to better capture their diverse and multi-grained interest in news. We use a three-level hierarchy to represent 1) overall user interest; 2) user interest in coarse-grained topics like sports; and 3) user interest in fine-grained topics like football. Moreover, we propose a hierarchical user interest matching framework to match candidate news with different levels of user interest for more accurate user interest targeting. Extensive experiments on two real-world datasets validate our method can effectively improve the performance of user modeling for personalized news recommendation",
    "checked": true,
    "id": "c09c9a507846b9ea0adff1afd1cc92513bba32f7",
    "semantic_title": "",
    "citation_count": 35,
    "authors": [
      "Tao Qi",
      "Fangzhao Wu",
      "Chuhan Wu",
      "Peiru Yang",
      "Yang Yu",
      "Xing Xie",
      "Yongfeng Huang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.424": {
    "title": "PP-Rec: News Recommendation with Personalized User Interest and Time-aware News Popularity",
    "volume": "long",
    "abstract": "Personalized news recommendation methods are widely used in online news services. These methods usually recommend news based on the matching between news content and user interest inferred from historical behaviors. However, these methods usually have difficulties in making accurate recommendations to cold-start users, and tend to recommend similar news with those users have read. In general, popular news usually contain important information and can attract users with different interests. Besides, they are usually diverse in content and topic. Thus, in this paper we propose to incorporate news popularity information to alleviate the cold-start and diversity problems for personalized news recommendation. In our method, the ranking score for recommending a candidate news to a target user is the combination of a personalized matching score and a news popularity score. The former is used to capture the personalized user interest in news. The latter is used to measure time-aware popularity of candidate news, which is predicted based on news content, recency, and real-time CTR using a unified framework. Besides, we propose a popularity-aware user encoder to eliminate the popularity bias in user behaviors for accurate interest modeling. Experiments on two real-world datasets show our method can effectively improve the accuracy and diversity for news recommendation",
    "checked": true,
    "id": "c7923f0e978310d7b7e38d7a1ba798b8bc50690a",
    "semantic_title": "",
    "citation_count": 29,
    "authors": [
      "Tao Qi",
      "Fangzhao Wu",
      "Chuhan Wu",
      "Yongfeng Huang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.425": {
    "title": "Article Reranking by Memory-Enhanced Key Sentence Matching for Detecting Previously Fact-Checked Claims",
    "volume": "long",
    "abstract": "False claims that have been previously fact-checked can still spread on social media. To mitigate their continual spread, detecting previously fact-checked claims is indispensable. Given a claim, existing works focus on providing evidence for detection by reranking candidate fact-checking articles (FC-articles) retrieved by BM25. However, these performances may be limited because they ignore the following characteristics of FC-articles: (1) claims are often quoted to describe the checked events, providing lexical information besides semantics; (2) sentence templates to introduce or debunk claims are common across articles, providing pattern information. Models that ignore the two aspects only leverage semantic relevance and may be misled by sentences that describe similar but irrelevant events. In this paper, we propose a novel reranker, MTM (Memory-enhanced Transformers for Matching) to rank FC-articles using key sentences selected with event (lexical and semantic) and pattern information. For event information, we propose a ROUGE-guided Transformer which is finetuned with regression of ROUGE. For pattern information, we generate pattern vectors for matching with sentences. By fusing event and pattern information, we select key sentences to represent an article and then predict if the article fact-checks the given claim using the claim, key sentences, and patterns. Experiments on two real-world datasets show that MTM outperforms existing methods. Human evaluation proves that MTM can capture key sentences for explanations",
    "checked": true,
    "id": "119c33321fc0e1db837ce293f1b65cc26c1cc34e",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Qiang Sheng",
      "Juan Cao",
      "Xueyao Zhang",
      "Xirong Li",
      "Lei Zhong"
    ]
  },
  "https://aclanthology.org/2021.acl-long.426": {
    "title": "Defense against Synonym Substitution-based Adversarial Attacks via Dirichlet Neighborhood Ensemble",
    "volume": "long",
    "abstract": "Although deep neural networks have achieved prominent performance on many NLP tasks, they are vulnerable to adversarial examples. We propose Dirichlet Neighborhood Ensemble (DNE), a randomized method for training a robust model to defense synonym substitution-based attacks. During training, DNE forms virtual sentences by sampling embedding vectors for each word in an input sentence from a convex hull spanned by the word and its synonyms, and it augments them with the training data. In such a way, the model is robust to adversarial attacks while maintaining the performance on the original clean data. DNE is agnostic to the network architectures and scales to large models (e.g., BERT) for NLP applications. Through extensive experimentation, we demonstrate that our method consistently outperforms recently proposed defense methods by a significant margin across different network architectures and multiple data sets",
    "checked": true,
    "id": "0cca27a289b595763d33b0a66ac1b3fc5b3ddc73",
    "semantic_title": "",
    "citation_count": 33,
    "authors": [
      "Yi Zhou",
      "Xiaoqing Zheng",
      "Cho-Jui Hsieh",
      "Kai-Wei Chang",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.427": {
    "title": "Shortformer: Better Language Modeling using Shorter Inputs",
    "volume": "long",
    "abstract": "Increasing the input length has been a driver of progress in language modeling with transformers. We identify conditions where shorter inputs are not harmful, and achieve perplexity and efficiency improvements through two new methods that decrease input length. First, we show that initially training a model on short subsequences before moving on to longer ones both reduces overall training time and, surprisingly, substantially improves perplexity. Second, we show how to improve the efficiency of recurrence methods in transformers, which let models condition on previously processed tokens when generating sequences that exceed the maximal length the transformer can handle at once. Existing methods require computationally expensive relative position embeddings; we introduce a simple alternative of adding absolute position embeddings to queries and keys instead of to word embeddings, which efficiently produces superior results. We show that these recurrent models also benefit from short input lengths. Combining these techniques speeds up training by a factor of 1.65, reduces memory usage, and substantially improves perplexity on WikiText-103, without adding any parameters",
    "checked": true,
    "id": "0822f8d7e6a72a65e65f147d3a8d8fccd485da40",
    "semantic_title": "",
    "citation_count": 43,
    "authors": [
      "Ofir Press",
      "Noah A. Smith",
      "Mike Lewis"
    ]
  },
  "https://aclanthology.org/2021.acl-long.428": {
    "title": "BanditMTL: Bandit-based Multi-task Learning for Text Classification",
    "volume": "long",
    "abstract": "Task variance regularization, which can be used to improve the generalization of Multi-task Learning (MTL) models, remains unexplored in multi-task text classification. Accordingly, to fill this gap, this paper investigates how the task might be effectively regularized, and consequently proposes a multi-task learning method based on adversarial multi-armed bandit. The proposed method, named BanditMTL, regularizes the task variance by means of a mirror gradient ascent-descent algorithm. Adopting BanditMTL in the multi-task text classification context is found to achieve state-of-the-art performance. The results of extensive experiments back up our theoretical analysis and validate the superiority of our proposals",
    "checked": true,
    "id": "26db43bc04dd8ca827f7700e3eaac04b5e5b7bad",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Yuren Mao",
      "Zekai Wang",
      "Weiwei Liu",
      "Xuemin Lin",
      "Wenbin Hu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.429": {
    "title": "Unified Interpretation of Softmax Cross-Entropy and Negative Sampling: With Case Study for Knowledge Graph Embedding",
    "volume": "long",
    "abstract": "In knowledge graph embedding, the theoretical relationship between the softmax cross-entropy and negative sampling loss functions has not been investigated. This makes it difficult to fairly compare the results of the two different loss functions. We attempted to solve this problem by using the Bregman divergence to provide a unified interpretation of the softmax cross-entropy and negative sampling loss functions. Under this interpretation, we can derive theoretical findings for fair comparison. Experimental results on the FB15k-237 and WN18RR datasets show that the theoretical findings are valid in practical settings",
    "checked": true,
    "id": "8527319f39e4bc0a85d404bdf4c71eb444aac395",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Hidetaka Kamigaito",
      "Katsuhiko Hayashi"
    ]
  },
  "https://aclanthology.org/2021.acl-long.430": {
    "title": "De-Confounded Variational Encoder-Decoder for Logical Table-to-Text Generation",
    "volume": "long",
    "abstract": "Logical table-to-text generation aims to automatically generate fluent and logically faithful text from tables. The task remains challenging where deep learning models often generated linguistically fluent but logically inconsistent text. The underlying reason may be that deep learning models often capture surface-level spurious correlations rather than the causal relationships between the table x and the sentence y. Specifically, in the training stage, a model can get a low empirical loss without understanding x and use spurious statistical cues instead. In this paper, we propose a de-confounded variational encoder-decoder (DCVED) based on causal intervention, learning the objective p(y|do(x)). Firstly, we propose to use variational inference to estimate the confounders in the latent space and cooperate with the causal intervention based on Pearl’s do-calculus to alleviate the spurious correlations. Secondly, to make the latent confounder meaningful, we propose a back-prediction process to predict the not-used entities but linguistically similar to the exactly selected ones. Finally, since our variational model can generate multiple candidates, we train a table-text selector to find out the best candidate sentence for the given table. An extensive set of experiments show that our model outperforms the baselines and achieves new state-of-the-art performance on two logical table-to-text datasets in terms of logical fidelity",
    "checked": true,
    "id": "98a00dbf695da32a63350fad4d6750acfe160eec",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Wenqing Chen",
      "Jidong Tian",
      "Yitian Li",
      "Hao He",
      "Yaohui Jin"
    ]
  },
  "https://aclanthology.org/2021.acl-long.431": {
    "title": "Rethinking Stealthiness of Backdoor Attack against NLP Models",
    "volume": "long",
    "abstract": "Recent researches have shown that large natural language processing (NLP) models are vulnerable to a kind of security threat called the Backdoor Attack. Backdoor attacked models can achieve good performance on clean test sets but perform badly on those input sentences injected with designed trigger words. In this work, we point out a potential problem of current backdoor attacking research: its evaluation ignores the stealthiness of backdoor attacks, and most of existing backdoor attacking methods are not stealthy either to system deployers or to system users. To address this issue, we first propose two additional stealthiness-based metrics to make the backdoor attacking evaluation more credible. We further propose a novel word-based backdoor attacking method based on negative data augmentation and modifying word embeddings, making an important step towards achieving stealthy backdoor attacking. Experiments on sentiment analysis and toxic detection tasks show that our method is much stealthier while maintaining pretty good attacking performance. Our code is available at https://github.com/lancopku/SOS",
    "checked": true,
    "id": "68a3d32416977e88cf1bfa4ad548d403f5f089d6",
    "semantic_title": "",
    "citation_count": 40,
    "authors": [
      "Wenkai Yang",
      "Yankai Lin",
      "Peng Li",
      "Jie Zhou",
      "Xu Sun"
    ]
  },
  "https://aclanthology.org/2021.acl-long.432": {
    "title": "Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition",
    "volume": "long",
    "abstract": "Crowdsourcing is regarded as one prospective solution for effective supervised learning, aiming to build large-scale annotated training data by crowd workers. Previous studies focus on reducing the influences from the noises of the crowdsourced annotations for supervised models. We take a different point in this work, regarding all crowdsourced annotations as gold-standard with respect to the individual annotators. In this way, we find that crowdsourcing could be highly similar to domain adaptation, and then the recent advances of cross-domain methods can be almost directly applied to crowdsourcing. Here we take named entity recognition (NER) as a study case, suggesting an annotator-aware representation learning model that inspired by the domain adaptation methods which attempt to capture effective domain-aware features. We investigate both unsupervised and supervised crowdsourcing learning, assuming that no or only small-scale expert annotations are available. Experimental results on a benchmark crowdsourced NER dataset show that our method is highly effective, leading to a new state-of-the-art performance. In addition, under the supervised setting, we can achieve impressive performance gains with only a very small scale of expert annotations",
    "checked": true,
    "id": "7df731af8fa55d8613d388ce68ed6d87f01835e6",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Xin Zhang",
      "Guangwei Xu",
      "Yueheng Sun",
      "Meishan Zhang",
      "Pengjun Xie"
    ]
  },
  "https://aclanthology.org/2021.acl-long.433": {
    "title": "Exploring Distantly-Labeled Rationales in Neural Network Models",
    "volume": "long",
    "abstract": "Recent studies strive to incorporate various human rationales into neural networks to improve model performance, but few pay attention to the quality of the rationales. Most existing methods distribute their models’ focus to distantly-labeled rationale words entirely and equally, while ignoring the potential important non-rationale words and not distinguishing the importance of different rationale words. In this paper, we propose two novel auxiliary loss functions to make better use of distantly-labeled rationales, which encourage models to maintain their focus on important words beyond labeled rationales (PINs) and alleviate redundant training on non-helpful rationales (NoIRs). Experiments on two representative classification tasks show that our proposed methods can push a classification model to effectively learn crucial clues from non-perfect rationales while maintaining the ability to spread its focus to other unlabeled important words, thus significantly outperform existing methods",
    "checked": true,
    "id": "d3bd93b8fb79290a803d00fe32c30e53fbcff6de",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Quzhe Huang",
      "Shengqi Zhu",
      "Yansong Feng",
      "Dongyan Zhao"
    ]
  },
  "https://aclanthology.org/2021.acl-long.434": {
    "title": "Learning to Perturb Word Embeddings for Out-of-distribution QA",
    "volume": "long",
    "abstract": "QA models based on pretrained language models have achieved remarkable performance on various benchmark datasets. However, QA models do not generalize well to unseen data that falls outside the training distribution, due to distributional shifts. Data augmentation (DA) techniques which drop/replace words have shown to be effective in regularizing the model from overfitting to the training data. Yet, they may adversely affect the QA tasks since they incur semantic changes that may lead to wrong answers for the QA task. To tackle this problem, we propose a simple yet effective DA method based on a stochastic noise generator, which learns to perturb the word embedding of the input questions and context without changing their semantics. We validate the performance of the QA models trained with our word embedding perturbation on a single source dataset, on five different target domains. The results show that our method significantly outperforms the baseline DA methods. Notably, the model trained with ours outperforms the model trained with more than 240K artificially generated QA pairs",
    "checked": true,
    "id": "193054e7e3a83d3e36926268e9c3706721973c57",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Seanie Lee",
      "Minki Kang",
      "Juho Lee",
      "Sung Ju Hwang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.435": {
    "title": "Maria: A Visual Experience Powered Conversational Agent",
    "volume": "long",
    "abstract": "Arguably, the visual perception of conversational agents to the physical world is a key way for them to exhibit the human-like intelligence. Image-grounded conversation is thus proposed to address this challenge. Existing works focus on exploring the multimodal dialog models that ground the conversation on a given image. In this paper, we take a step further to study image-grounded conversation under a fully open-ended setting where no paired dialog and image are assumed available. Specifically, we present Maria, a neural conversation agent powered by the visual world experiences which are retrieved from a large-scale image index. Maria consists of three flexible components, i.e., text-to-image retriever, visual concept detector and visual-knowledge-grounded response generator. The retriever aims to retrieve a correlated image to the dialog from an image index, while the visual concept detector extracts rich visual knowledge from the image. Then, the response generator is grounded on the extracted visual knowledge and dialog context to generate the target response. Extensive experiments demonstrate Maria outperforms previous state-of-the-art methods on automatic metrics and human evaluation, and can generate informative responses that have some visual commonsense of the physical world",
    "checked": true,
    "id": "f5aa169162f2c7fdb311c4a7fedca182a07f05d9",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Zujie Liang",
      "Huang Hu",
      "Can Xu",
      "Chongyang Tao",
      "Xiubo Geng",
      "Yining Chen",
      "Fan Liang",
      "Daxin Jiang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.436": {
    "title": "A Human-machine Collaborative Framework for Evaluating Malevolence in Dialogues",
    "volume": "long",
    "abstract": "Conversational dialogue systems (CDSs) are hard to evaluate due to the complexity of natural language. Automatic evaluation of dialogues often shows insufficient correlation with human judgements. Human evaluation is reliable but labor-intensive. We introduce a human-machine collaborative framework, HMCEval, that can guarantee reliability of the evaluation outcomes with reduced human effort. HMCEval casts dialogue evaluation as a sample assignment problem, where we need to decide to assign a sample to a human or a machine for evaluation. HMCEval includes a model confidence estimation module to estimate the confidence of the predicted sample assignment, and a human effort estimation module to estimate the human effort should the sample be assigned to human evaluation, as well as a sample assignment execution module that finds the optimum assignment solution based on the estimated confidence and effort. We assess the performance of HMCEval on the task of evaluating malevolence in dialogues. The experimental results show that HMCEval achieves around 99% evaluation accuracy with half of the human effort spared, showing that HMCEval provides reliable evaluation outcomes while reducing human effort by a large amount",
    "checked": true,
    "id": "b1999b5ede6820e7a17a5567009f4aadb3649f64",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Yangjun Zhang",
      "Pengjie Ren",
      "Maarten de Rijke"
    ]
  },
  "https://aclanthology.org/2021.acl-long.437": {
    "title": "Generating Relevant and Coherent Dialogue Responses using Self-Separated Conditional Variational AutoEncoders",
    "volume": "long",
    "abstract": "Conditional Variational AutoEncoder (CVAE) effectively increases the diversity and informativeness of responses in open-ended dialogue generation tasks through enriching the context vector with sampled latent variables. However, due to the inherent one-to-many and many-to-one phenomena in human dialogues, the sampled latent variables may not correctly reflect the contexts’ semantics, leading to irrelevant and incoherent generated responses. To resolve this problem, we propose Self-separated Conditional Variational AutoEncoder (abbreviated as SepaCVAE) that introduces group information to regularize the latent variables, which enhances CVAE by improving the responses’ relevance and coherence while maintaining their diversity and informativeness. SepaCVAE actively divides the input data into groups, and then widens the absolute difference between data pairs from distinct groups, while narrowing the relative distance between data pairs in the same group. Empirical results from automatic evaluation and detailed analysis demonstrate that SepaCVAE can significantly boost responses in well-established open-domain dialogue datasets",
    "checked": true,
    "id": "0472f016ccf6609d73e11432883101b54b015150",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Bin Sun",
      "Shaoxiong Feng",
      "Yiwei Li",
      "Jiamou Liu",
      "Kan Li"
    ]
  },
  "https://aclanthology.org/2021.acl-long.438": {
    "title": "Learning to Ask Conversational Questions by Optimizing Levenshtein Distance",
    "volume": "long",
    "abstract": "Conversational Question Simplification (CQS) aims to simplify self-contained questions into conversational ones by incorporating some conversational characteristics, e.g., anaphora and ellipsis. Existing maximum likelihood estimation based methods often get trapped in easily learned tokens as all tokens are treated equally during training. In this work, we introduce a Reinforcement Iterative Sequence Editing (RISE) framework that optimizes the minimum Levenshtein distance through explicit editing actions. RISE is able to pay attention to tokens that are related to conversational characteristics. To train RISE, we devise an Iterative Reinforce Training (IRT) algorithm with a Dynamic Programming based Sampling (DPS) process to improve exploration. Experimental results on two benchmark datasets show that RISE significantly outperforms state-of-the-art methods and generalizes well on unseen data",
    "checked": true,
    "id": "bb1839548939385aed7a8a8dfdf5e85c63a3e177",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Zhongkun Liu",
      "Pengjie Ren",
      "Zhumin Chen",
      "Zhaochun Ren",
      "Maarten de Rijke",
      "Ming Zhou"
    ]
  },
  "https://aclanthology.org/2021.acl-long.439": {
    "title": "DVD: A Diagnostic Dataset for Multi-step Reasoning in Video Grounded Dialogue",
    "volume": "long",
    "abstract": "A video-grounded dialogue system is required to understand both dialogue, which contains semantic dependencies from turn to turn, and video, which contains visual cues of spatial and temporal scene variations. Building such dialogue systems is a challenging problem, involving various reasoning types on both visual and language inputs. Existing benchmarks do not have enough annotations to thoroughly analyze dialogue systems and understand their capabilities and limitations in isolation. These benchmarks are also not explicitly designed to minimise biases that models can exploit without actual reasoning. To address these limitations, in this paper, we present DVD, a Diagnostic Dataset for Video-grounded Dialogue. The dataset is designed to contain minimal biases and has detailed annotations for the different types of reasoning over the spatio-temporal space of video. Dialogues are synthesized over multiple question turns, each of which is injected with a set of cross-turn semantic relationships. We use DVD to analyze existing approaches, providing interesting insights into their abilities and limitations. In total, DVD is built from 11k CATER synthetic videos and contains 10 instances of 10-round dialogues for each video, resulting in more than 100k dialogues and 1M question-answer pairs. Our code and dataset are publicly available",
    "checked": true,
    "id": "8b8d0e74a21486930b89c0ad356d60aa60d6322d",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Hung Le",
      "Chinnadhurai Sankar",
      "Seungwhan Moon",
      "Ahmad Beirami",
      "Alborz Geramifard",
      "Satwik Kottur"
    ]
  },
  "https://aclanthology.org/2021.acl-long.440": {
    "title": "MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation",
    "volume": "long",
    "abstract": "Emotion recognition in conversation (ERC) is a crucial component in affective dialogue systems, which helps the system understand users’ emotions and generate empathetic responses. However, most works focus on modeling speaker and contextual information primarily on the textual modality or simply leveraging multimodal information through feature concatenation. In order to explore a more effective way of utilizing both multimodal and long-distance contextual information, we propose a new model based on multimodal fused graph convolutional network, MMGCN, in this work. MMGCN can not only make use of multimodal dependencies effectively, but also leverage speaker information to model inter-speaker and intra-speaker dependency. We evaluate our proposed model on two public benchmark datasets, IEMOCAP and MELD, and the results prove the effectiveness of MMGCN, which outperforms other SOTA methods by a significant margin under the multimodal conversation setting",
    "checked": true,
    "id": "64374ad854dba86b087ed14d96a376e646103904",
    "semantic_title": "",
    "citation_count": 36,
    "authors": [
      "Jingwen Hu",
      "Yuchen Liu",
      "Jinming Zhao",
      "Qin Jin"
    ]
  },
  "https://aclanthology.org/2021.acl-long.441": {
    "title": "DynaEval: Unifying Turn and Dialogue Level Evaluation",
    "volume": "long",
    "abstract": "A dialogue is essentially a multi-turn interaction among interlocutors. Effective evaluation metrics should reflect the dynamics of such interaction. Existing automatic metrics are focused very much on the turn-level quality, while ignoring such dynamics. To this end, we propose DynaEval, a unified automatic evaluation framework which is not only capable of performing turn-level evaluation, but also holistically considers the quality of the entire dialogue. In DynaEval, the graph convolutional network (GCN) is adopted to model a dialogue in totality, where the graph nodes denote each individual utterance and the edges represent the dependency between pairs of utterances. A contrastive loss is then applied to distinguish well-formed dialogues from carefully constructed negative samples. Experiments show that DynaEval significantly outperforms the state-of-the-art dialogue coherence model, and correlates strongly with human judgements across multiple dialogue evaluation aspects at both turn and dialogue level",
    "checked": true,
    "id": "753baa88a7f49f6605ae30f77a54dbb9e074c8b4",
    "semantic_title": "",
    "citation_count": 29,
    "authors": [
      "Chen Zhang",
      "Yiming Chen",
      "Luis Fernando D’Haro",
      "Yan Zhang",
      "Thomas Friedrichs",
      "Grandee Lee",
      "Haizhou Li"
    ]
  },
  "https://aclanthology.org/2021.acl-long.442": {
    "title": "CoSQA: 20,000+ Web Queries for Code Search and Question Answering",
    "volume": "long",
    "abstract": "Finding codes given natural language query is beneficial to the productivity of software developers. Future progress towards better semantic matching between query and code requires richer supervised training resources. To remedy this, we introduce CoSQA dataset. It includes 20,604 labels for pairs of natural language queries and codes, each annotated by at least 3 human annotators. We further introduce a contrastive learning method dubbed CoCLR to enhance text-code matching, which works as a data augmenter to bring more artificially generated training instances. We show that, evaluated on CodeXGLUE with the same CodeBERT model, training on CoSQA improves the accuracy of code question answering by 5.1% and incorporating CoCLR brings a further improvement of 10.5%",
    "checked": true,
    "id": "0c21334be0228431d619a180c809b43be0065bdd",
    "semantic_title": "",
    "citation_count": 42,
    "authors": [
      "Junjie Huang",
      "Duyu Tang",
      "Linjun Shou",
      "Ming Gong",
      "Ke Xu",
      "Daxin Jiang",
      "Ming Zhou",
      "Nan Duan"
    ]
  },
  "https://aclanthology.org/2021.acl-long.443": {
    "title": "Rewriter-Evaluator Architecture for Neural Machine Translation",
    "volume": "long",
    "abstract": "A few approaches have been developed to improve neural machine translation (NMT) models with multiple passes of decoding. However, their performance gains are limited because of lacking proper policies to terminate the multi-pass process. To address this issue, we introduce a novel architecture of Rewriter-Evaluator. Translating a source sentence involves multiple rewriting passes. In every pass, a rewriter generates a new translation to improve the past translation. Termination of this multi-pass process is determined by a score of translation quality estimated by an evaluator. We also propose prioritized gradient descent (PGD) to jointly and efficiently train the rewriter and the evaluator. Extensive experiments on three machine translation tasks show that our architecture notably improves the performances of NMT models and significantly outperforms prior methods. An oracle experiment reveals that it can largely reduce performance gaps to the oracle policy. Experiments confirm that the evaluator trained with PGD is more accurate than prior methods in determining proper numbers of rewriting",
    "checked": true,
    "id": "148ed5a01f5d32957bebbdebea59a8f4531da5b9",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangming Li",
      "Kaisheng Yao"
    ]
  },
  "https://aclanthology.org/2021.acl-long.444": {
    "title": "Modeling Bilingual Conversational Characteristics for Neural Chat Translation",
    "volume": "long",
    "abstract": "Neural chat translation aims to translate bilingual conversational text, which has a broad application in international exchanges and cooperation. Despite the impressive performance of sentence-level and context-aware Neural Machine Translation (NMT), there still remain challenges to translate bilingual conversational text due to its inherent characteristics such as role preference, dialogue coherence, and translation consistency. In this paper, we aim to promote the translation quality of conversational text by modeling the above properties. Specifically, we design three latent variational modules to learn the distributions of bilingual conversational characteristics. Through sampling from these learned distributions, the latent variables, tailored for role preference, dialogue coherence, and translation consistency, are incorporated into the NMT model for better translation. We evaluate our approach on the benchmark dataset BConTrasT (English<->German) and a self-collected bilingual dialogue corpus, named BMELD (English<->Chinese). Extensive experiments show that our approach notably boosts the performance over strong baselines by a large margin and significantly surpasses some state-of-the-art context-aware NMT models in terms of BLEU and TER. Additionally, we make the BMELD dataset publicly available for the research community",
    "checked": true,
    "id": "aa2bd42be1a96f3b8c5d9df70e6073a4799e0140",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Yunlong Liang",
      "Fandong Meng",
      "Yufeng Chen",
      "Jinan Xu",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2021.acl-long.445": {
    "title": "Importance-based Neuron Allocation for Multilingual Neural Machine Translation",
    "volume": "long",
    "abstract": "Multilingual neural machine translation with a single model has drawn much attention due to its capability to deal with multiple languages. However, the current multilingual translation paradigm often makes the model tend to preserve the general knowledge, but ignore the language-specific knowledge. Some previous works try to solve this problem by adding various kinds of language-specific modules to the model, but they suffer from the parameter explosion problem and require specialized manual design. To solve these problems, we propose to divide the model neurons into general and language-specific parts based on their importance across languages. The general part is responsible for preserving the general knowledge and participating in the translation of all the languages, while the language-specific part is responsible for preserving the language-specific knowledge and participating in the translation of some specific languages. Experimental results on several language pairs, covering IWSLT and Europarl corpus datasets, demonstrate the effectiveness and universality of the proposed method",
    "checked": true,
    "id": "88a72a9bc844ceb809f0fcf94ae3a0ba69e558de",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Wanying Xie",
      "Yang Feng",
      "Shuhao Gu",
      "Dong Yu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.446": {
    "title": "Transfer Learning for Sequence Generation: from Single-source to Multi-source",
    "volume": "long",
    "abstract": "Multi-source sequence generation (MSG) is an important kind of sequence generation tasks that takes multiple sources, including automatic post-editing, multi-source translation, multi-document summarization, etc. As MSG tasks suffer from the data scarcity problem and recent pretrained models have been proven to be effective for low-resource downstream tasks, transferring pretrained sequence-to-sequence models to MSG tasks is essential. Although directly finetuning pretrained models on MSG tasks and concatenating multiple sources into a single long sequence is regarded as a simple method to transfer pretrained models to MSG tasks, we conjecture that the direct finetuning method leads to catastrophic forgetting and solely relying on pretrained self-attention layers to capture cross-source information is not sufficient. Therefore, we propose a two-stage finetuning method to alleviate the pretrain-finetune discrepancy and introduce a novel MSG model with a fine encoder to learn better representations in MSG tasks. Experiments show that our approach achieves new state-of-the-art results on the WMT17 APE task and multi-source translation task using the WMT14 test set. When adapted to document-level translation, our framework outperforms strong baselines significantly",
    "checked": true,
    "id": "fafd06196304bccd23d221cc8bfb431a48a288fc",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Xuancheng Huang",
      "Jingfang Xu",
      "Maosong Sun",
      "Yang Liu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.447": {
    "title": "A Closer Look at Few-Shot Crosslingual Transfer: The Choice of Shots Matters",
    "volume": "long",
    "abstract": "Few-shot crosslingual transfer has been shown to outperform its zero-shot counterpart with pretrained encoders like multilingual BERT. Despite its growing popularity, little to no attention has been paid to standardizing and analyzing the design of few-shot experiments. In this work, we highlight a fundamental risk posed by this shortcoming, illustrating that the model exhibits a high degree of sensitivity to the selection of few shots. We conduct a large-scale experimental study on 40 sets of sampled few shots for six diverse NLP tasks across up to 40 languages. We provide an analysis of success and failure cases of few-shot transfer, which highlights the role of lexical features. Additionally, we show that a straightforward full model finetuning approach is quite effective for few-shot transfer, outperforming several state-of-the-art few-shot approaches. As a step towards standardizing few-shot crosslingual experimental designs, we make our sampled few shots publicly available",
    "checked": true,
    "id": "7317dccaf8023b2719a2d0fe787a31b20a3232e1",
    "semantic_title": "",
    "citation_count": 38,
    "authors": [
      "Mengjie Zhao",
      "Yi Zhu",
      "Ehsan Shareghi",
      "Ivan Vulić",
      "Roi Reichart",
      "Anna Korhonen",
      "Hinrich Schütze"
    ]
  },
  "https://aclanthology.org/2021.acl-long.448": {
    "title": "Coreference Reasoning in Machine Reading Comprehension",
    "volume": "long",
    "abstract": "Coreference resolution is essential for natural language understanding and has been long studied in NLP. In recent years, as the format of Question Answering (QA) became a standard for machine reading comprehension (MRC), there have been data collection efforts, e.g., Dasigi et al. (2019), that attempt to evaluate the ability of MRC models to reason about coreference. However, as we show, coreference reasoning in MRC is a greater challenge than earlier thought; MRC datasets do not reflect the natural distribution and, consequently, the challenges of coreference reasoning. Specifically, success on these datasets does not reflect a model’s proficiency in coreference reasoning. We propose a methodology for creating MRC datasets that better reflect the challenges of coreference reasoning and use it to create a sample evaluation set. The results on our dataset show that state-of-the-art models still struggle with these phenomena. Furthermore, we develop an effective way to use naturally occurring coreference phenomena from existing coreference resolution datasets when training MRC models. This allows us to show an improvement in the coreference reasoning abilities of state-of-the-art models",
    "checked": true,
    "id": "bda345366be3960bbe451cbc57e386dbcb08903a",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Mingzhu Wu",
      "Nafise Sadat Moosavi",
      "Dan Roth",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2021.acl-long.449": {
    "title": "Adapting Unsupervised Syntactic Parsing Methodology for Discourse Dependency Parsing",
    "volume": "long",
    "abstract": "One of the main bottlenecks in developing discourse dependency parsers is the lack of annotated training data. A potential solution is to utilize abundant unlabeled data by using unsupervised techniques, but there is so far little research in unsupervised discourse dependency parsing. Fortunately, unsupervised syntactic dependency parsing has been studied by decades, which could potentially be adapted for discourse parsing. In this paper, we propose a simple yet effective method to adapt unsupervised syntactic dependency parsing methodology for unsupervised discourse dependency parsing. We apply the method to adapt two state-of-the-art unsupervised syntactic dependency parsing methods. Experimental results demonstrate that our adaptation is effective. Moreover, we extend the adapted methods to the semi-supervised and supervised setting and surprisingly, we find that they outperform previous methods specially designed for supervised discourse parsing. Further analysis shows our adaptations result in superiority not only in parsing accuracy but also in time and space efficiency",
    "checked": true,
    "id": "80b52d5834dd1ee61c5d164dcb372af6993f81b8",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Liwen Zhang",
      "Ge Wang",
      "Wenjuan Han",
      "Kewei Tu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.450": {
    "title": "A Conditional Splitting Framework for Efficient Constituency Parsing",
    "volume": "long",
    "abstract": "We introduce a generic seq2seq parsing framework that casts constituency parsing problems (syntactic and discourse parsing) into a series of conditional splitting decisions. Our parsing model estimates the conditional probability distribution of possible splitting points in a given text span and supports efficient top-down decoding, which is linear in number of nodes. The conditional splitting formulation together with efficient beam search inference facilitate structural consistency without relying on expensive structured inference. Crucially, for discourse analysis we show that in our formulation, discourse segmentation can be framed as a special case of parsing which allows us to perform discourse parsing without requiring segmentation as a pre-requisite. Experiments show that our model achieves good results on the standard syntactic parsing tasks under settings with/without pre-trained representations and rivals state-of-the-art (SoTA) methods that are more computationally expensive than ours. In discourse parsing, our method outperforms SoTA by a good margin",
    "checked": true,
    "id": "4f5922b8ab49f4c5529e415656d13bab6c28ee52",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Thanh-Tung Nguyen",
      "Xuan-Phi Nguyen",
      "Shafiq Joty",
      "Xiaoli Li"
    ]
  },
  "https://aclanthology.org/2021.acl-long.451": {
    "title": "A Unified Generative Framework for Various NER Subtasks",
    "volume": "long",
    "abstract": "Named Entity Recognition (NER) is the task of identifying spans that represent entities in sentences. Whether the entity spans are nested or discontinuous, the NER task can be categorized into the flat NER, nested NER, and discontinuous NER subtasks. These subtasks have been mainly solved by the token-level sequence labelling or span-level classification. However, these solutions can hardly tackle the three kinds of NER subtasks concurrently. To that end, we propose to formulate the NER subtasks as an entity span sequence generation task, which can be solved by a unified sequence-to-sequence (Seq2Seq) framework. Based on our unified framework, we can leverage the pre-trained Seq2Seq model to solve all three kinds of NER subtasks without the special design of the tagging schema or ways to enumerate spans. We exploit three types of entity representations to linearize entities into a sequence. Our proposed framework is easy-to-implement and achieves state-of-the-art (SoTA) or near SoTA performance on eight English NER datasets, including two flat NER datasets, three nested NER datasets, and three discontinuous NER datasets",
    "checked": true,
    "id": "c09ff6965322ece56bce383266c75159234f59c4",
    "semantic_title": "",
    "citation_count": 117,
    "authors": [
      "Hang Yan",
      "Tao Gui",
      "Junqi Dai",
      "Qipeng Guo",
      "Zheng Zhang",
      "Xipeng Qiu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.452": {
    "title": "An In-depth Study on Internal Structure of Chinese Words",
    "volume": "long",
    "abstract": "Unlike English letters, Chinese characters have rich and specific meanings. Usually, the meaning of a word can be derived from its constituent characters in some way. Several previous works on syntactic parsing propose to annotate shallow word-internal structures for better utilizing character-level information. This work proposes to model the deep internal structures of Chinese words as dependency trees with 11 labels for distinguishing syntactic relationships. First, based on newly compiled annotation guidelines, we manually annotate a word-internal structure treebank (WIST) consisting of over 30K multi-char words from Chinese Penn Treebank. To guarantee quality, each word is independently annotated by two annotators and inconsistencies are handled by a third senior annotator. Second, we present detailed and interesting analysis on WIST to reveal insights on Chinese word formation. Third, we propose word-internal structure parsing as a new task, and conduct benchmark experiments using a competitive dependency parser. Finally, we present two simple ways to encode word-internal structures, leading to promising gains on the sentence-level syntactic parsing task",
    "checked": true,
    "id": "38d052313e6cce936926f2168987b9edeb85a496",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Chen Gong",
      "Saihao Huang",
      "Houquan Zhou",
      "Zhenghua Li",
      "Min Zhang",
      "Zhefeng Wang",
      "Baoxing Huai",
      "Nicholas Jing Yuan"
    ]
  },
  "https://aclanthology.org/2021.acl-long.453": {
    "title": "MulDA: A Multilingual Data Augmentation Framework for Low-Resource Cross-Lingual NER",
    "volume": "long",
    "abstract": "Named Entity Recognition (NER) for low-resource languages is a both practical and challenging research problem. This paper addresses zero-shot transfer for cross-lingual NER, especially when the amount of source-language training data is also limited. The paper first proposes a simple but effective labeled sequence translation method to translate source-language training data to target languages and avoids problems such as word order change and entity span determination. With the source-language data as well as the translated data, a generation-based multilingual data augmentation method is introduced to further increase diversity by generating synthetic labeled data in multiple languages. These augmented data enable the language model based NER models to generalize better with both the language-specific features from the target-language synthetic data and the language-independent features from multilingual synthetic data. An extensive set of experiments were conducted to demonstrate encouraging cross-lingual transfer performance of the new research on a wide variety of target languages",
    "checked": true,
    "id": "0df9c19659388e55745c290ace520491c2985d9b",
    "semantic_title": "",
    "citation_count": 45,
    "authors": [
      "Linlin Liu",
      "Bosheng Ding",
      "Lidong Bing",
      "Shafiq Joty",
      "Luo Si",
      "Chunyan Miao"
    ]
  },
  "https://aclanthology.org/2021.acl-long.454": {
    "title": "Lexicon Enhanced Chinese Sequence Labeling Using BERT Adapter",
    "volume": "long",
    "abstract": "Lexicon information and pre-trained models, such as BERT, have been combined to explore Chinese sequence labeling tasks due to their respective strengths. However, existing methods solely fuse lexicon features via a shallow and random initialized sequence layer and do not integrate them into the bottom layers of BERT. In this paper, we propose Lexicon Enhanced BERT (LEBERT) for Chinese sequence labeling, which integrates external lexicon knowledge into BERT layers directly by a Lexicon Adapter layer. Compared with existing methods, our model facilitates deep lexicon knowledge fusion at the lower layers of BERT. Experiments on ten Chinese datasets of three tasks including Named Entity Recognition, Word Segmentation, and Part-of-Speech Tagging, show that LEBERT achieves state-of-the-art results",
    "checked": true,
    "id": "d0c383acba2f94caecff6f809debc032ec029d62",
    "semantic_title": "",
    "citation_count": 54,
    "authors": [
      "Wei Liu",
      "Xiyan Fu",
      "Yue Zhang",
      "Wenming Xiao"
    ]
  },
  "https://aclanthology.org/2021.acl-long.455": {
    "title": "Math Word Problem Solving with Explicit Numerical Values",
    "volume": "long",
    "abstract": "In recent years, math word problem solving has received considerable attention and achieved promising results, but previous methods rarely take numerical values into consideration. Most methods treat the numerical values in the problems as number symbols, and ignore the prominent role of the numerical values in solving the problem. In this paper, we propose a novel approach called NumS2T, which enhances math word problem solving performance by explicitly incorporating numerical values into a sequence-to-tree network. In addition, a numerical properties prediction mechanism is used to capture the category and comparison information of numerals and measure their importance in global expressions. Experimental results on the Math23K and APE datasets demonstrate that our model achieves better performance than existing state-of-the-art models",
    "checked": true,
    "id": "ec15ff1fc5c9780fd91902f286a9e3fcd00b890d",
    "semantic_title": "",
    "citation_count": 25,
    "authors": [
      "Qinzhuo Wu",
      "Qi Zhang",
      "Zhongyu Wei",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.456": {
    "title": "Neural-Symbolic Solver for Math Word Problems with Auxiliary Tasks",
    "volume": "long",
    "abstract": "Previous math word problem solvers following the encoder-decoder paradigm fail to explicitly incorporate essential math symbolic constraints, leading to unexplainable and unreasonable predictions. Herein, we propose Neural-Symbolic Solver (NS-Solver) to explicitly and seamlessly incorporate different levels of symbolic constraints by auxiliary tasks. Our NS-Solver consists of a problem reader to encode problems, a programmer to generate symbolic equations, and a symbolic executor to obtain answers. Along with target expression supervision, our solver is also optimized via 4 new auxiliary objectives to enforce different symbolic reasoning: a) self-supervised number prediction task predicting both number quantity and number locations; b) commonsense constant prediction task predicting what prior knowledge (e.g. how many legs a chicken has) is required; c) program consistency checker computing the semantic loss between predicted equation and target equation to ensure reasonable equation mapping; d) duality exploiting task exploiting the quasi-duality between symbolic equation generation and problem’s part-of-speech generation to enhance the understanding ability of a solver. Besides, to provide a more realistic and challenging benchmark for developing a universal and scalable solver, we also construct a new largescale MWP benchmark CM17K consisting of 4 kinds of MWPs (arithmetic, one-unknown linear, one-unknown non-linear, equation set) with more than 17K samples. Extensive experiments on Math23K and our CM17k demonstrate the superiority of our NS-Solver compared to state-of-the-art methods",
    "checked": true,
    "id": "7654dbd372d8b65e730e3bd477ff9fec96c16dc5",
    "semantic_title": "",
    "citation_count": 27,
    "authors": [
      "Jinghui Qin",
      "Xiaodan Liang",
      "Yining Hong",
      "Jianheng Tang",
      "Liang Lin"
    ]
  },
  "https://aclanthology.org/2021.acl-long.457": {
    "title": "SMedBERT: A Knowledge-Enhanced Pre-trained Language Model with Structured Semantics for Medical Text Mining",
    "volume": "long",
    "abstract": "Recently, the performance of Pre-trained Language Models (PLMs) has been significantly improved by injecting knowledge facts to enhance their abilities of language understanding. For medical domains, the background knowledge sources are especially useful, due to the massive medical terms and their complicated relations are difficult to understand in text. In this work, we introduce SMedBERT, a medical PLM trained on large-scale medical corpora, incorporating deep structured semantic knowledge from neighbours of linked-entity. In SMedBERT, the mention-neighbour hybrid attention is proposed to learn heterogeneous-entity information, which infuses the semantic representations of entity types into the homogeneous neighbouring entity structure. Apart from knowledge integration as external features, we propose to employ the neighbors of linked-entities in the knowledge graph as additional global contexts of text mentions, allowing them to communicate via shared neighbors, thus enrich their semantic representations. Experiments demonstrate that SMedBERT significantly outperforms strong baselines in various knowledge-intensive Chinese medical tasks. It also improves the performance of other tasks such as question answering, question matching and natural language inference",
    "checked": true,
    "id": "4fdaf912ad474e4d52b2b8915535ab5a9e8ea94d",
    "semantic_title": "",
    "citation_count": 24,
    "authors": [
      "Taolin Zhang",
      "Zerui Cai",
      "Chengyu Wang",
      "Minghui Qiu",
      "Bite Yang",
      "Xiaofeng He"
    ]
  },
  "https://aclanthology.org/2021.acl-long.458": {
    "title": "What is Your Article Based On? Inferring Fine-grained Provenance",
    "volume": "long",
    "abstract": "When evaluating an article and the claims it makes, a critical reader must be able to assess where the information presented comes from, and whether the various claims are mutually consistent and support the conclusion. This motivates the study of claim provenance, which seeks to trace and explain the origins of claims. In this paper, we introduce new techniques to model and reason about the provenance of multiple interacting claims, including how to capture fine-grained information about the context. Our solution hinges on first identifying the sentences that potentially contain important external information. We then develop a query generator with our novel rank-aware cross attention mechanism, which aims at generating metadata for the source article, based on the context and the signals collected from a search engine. This establishes relevant search queries, and it allows us to obtain source article candidates for each identified sentence and propose an ILP based algorithm to infer the best sources. We experiment with a newly created evaluation dataset, Politi-Prov, based on fact-checking articles from www.politifact.com; our experimental results show that our solution leads to a significant improvement over baselines",
    "checked": true,
    "id": "aebdd7aab992437ba66cd81eab9306cc38bb8357",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Yi Zhang",
      "Zachary Ives",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/2021.acl-long.459": {
    "title": "Cross-modal Memory Networks for Radiology Report Generation",
    "volume": "long",
    "abstract": "Medical imaging plays a significant role in clinical practice of medical diagnosis, where the text reports of the images are essential in understanding them and facilitating later treatments. By generating the reports automatically, it is beneficial to help lighten the burden of radiologists and significantly promote clinical automation, which already attracts much attention in applying artificial intelligence to medical domain. Previous studies mainly follow the encoder-decoder paradigm and focus on the aspect of text generation, with few studies considering the importance of cross-modal mappings and explicitly exploit such mappings to facilitate radiology report generation. In this paper, we propose a cross-modal memory networks (CMN) to enhance the encoder-decoder framework for radiology report generation, where a shared memory is designed to record the alignment between images and texts so as to facilitate the interaction and generation across modalities. Experimental results illustrate the effectiveness of our proposed model, where state-of-the-art performance is achieved on two widely used benchmark datasets, i.e., IU X-Ray and MIMIC-CXR. Further analyses also prove that our model is able to better align information from radiology images and texts so as to help generating more accurate reports in terms of clinical indicators",
    "checked": true,
    "id": "db2bd466953f3ea49280988e1659b6ac3f639e45",
    "semantic_title": "",
    "citation_count": 36,
    "authors": [
      "Zhihong Chen",
      "Yaling Shen",
      "Yan Song",
      "Xiang Wan"
    ]
  },
  "https://aclanthology.org/2021.acl-long.460": {
    "title": "Controversy and Conformity: from Generalized to Personalized Aggressiveness Detection",
    "volume": "long",
    "abstract": "There is content such as hate speech, offensive, toxic or aggressive documents, which are perceived differently by their consumers. They are commonly identified using classifiers solely based on textual content that generalize pre-agreed meanings of difficult problems. Such models provide the same results for each user, which leads to high misclassification rate observable especially for contentious, aggressive documents. Both document controversy and user nonconformity require new solutions. Therefore, we propose novel personalized approaches that respect individual beliefs expressed by either user conformity-based measures or various embeddings of their previous text annotations. We found that only a few annotations of most controversial documents are enough for all our personalization methods to significantly outperform classic, generalized solutions. The more controversial the content, the greater the gain. The personalized solutions may be used to efficiently filter unwanted aggressive content in the way adjusted to a given person",
    "checked": true,
    "id": "4026ae4e613e3e235ac9c999cccce31edeadf5f5",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Kamil Kanclerz",
      "Alicja Figas",
      "Marcin Gruza",
      "Tomasz Kajdanowicz",
      "Jan Kocon",
      "Daria Puchalska",
      "Przemyslaw Kazienko"
    ]
  },
  "https://aclanthology.org/2021.acl-long.461": {
    "title": "Multi-perspective Coherent Reasoning for Helpfulness Prediction of Multimodal Reviews",
    "volume": "long",
    "abstract": "As more and more product reviews are posted in both text and images, Multimodal Review Analysis (MRA) becomes an attractive research topic. Among the existing review analysis tasks, helpfulness prediction on review text has become predominant due to its importance for e-commerce platforms and online shops, i.e. helping customers quickly acquire useful product information. This paper proposes a new task Multimodal Review Helpfulness Prediction (MRHP) aiming to analyze the review helpfulness from text and visual modalities. Meanwhile, a novel Multi-perspective Coherent Reasoning method (MCR) is proposed to solve the MRHP task, which conducts joint reasoning over texts and images from both the product and the review, and aggregates the signals to predict the review helpfulness. Concretely, we first propose a product-review coherent reasoning module to measure the intra- and inter-modal coherence between the target product and the review. In addition, we also devise an intra-review coherent reasoning module to identify the coherence between the text content and images of the review, which is a piece of strong evidence for review helpfulness prediction. To evaluate the effectiveness of MCR, we present two newly collected multimodal review datasets as benchmark evaluation resources for the MRHP task. Experimental results show that our MCR method can lead to a performance increase of up to 8.5% as compared to the best performing text-only model. The source code and datasets can be obtained from https://github.com/jhliu17/MCR",
    "checked": true,
    "id": "73850065ad0a76b3ca85f356e11bed7eb71409f9",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Junhao Liu",
      "Zhen Hai",
      "Min Yang",
      "Lidong Bing"
    ]
  },
  "https://aclanthology.org/2021.acl-long.462": {
    "title": "Instantaneous Grammatical Error Correction with Shallow Aggressive Decoding",
    "volume": "long",
    "abstract": "In this paper, we propose Shallow Aggressive Decoding (SAD) to improve the online inference efficiency of the Transformer for instantaneous Grammatical Error Correction (GEC). SAD optimizes the online inference efficiency for GEC by two innovations: 1) it aggressively decodes as many tokens as possible in parallel instead of always decoding only one token in each step to improve computational parallelism; 2) it uses a shallow decoder instead of the conventional Transformer architecture with balanced encoder-decoder depth to reduce the computational cost during inference. Experiments in both English and Chinese GEC benchmarks show that aggressive decoding could yield identical predictions to greedy decoding but with significant speedup for online inference. Its combination with the shallow decoder could offer an even higher online inference speedup over the powerful Transformer baseline without quality loss. Not only does our approach allow a single model to achieve the state-of-the-art results in English GEC benchmarks: 66.4 F0.5 in the CoNLL-14 and 72.9 F0.5 in the BEA-19 test set with an almost 10x online inference speedup over the Transformer-big model, but also it is easily adapted to other languages. Our code is available at https://github.com/AutoTemp/Shallow-Aggressive-Decoding",
    "checked": true,
    "id": "3efd4b048dd7544333092332bccc3f0aea79f5c7",
    "semantic_title": "",
    "citation_count": 21,
    "authors": [
      "Xin Sun",
      "Tao Ge",
      "Furu Wei",
      "Houfeng Wang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.463": {
    "title": "Automatic ICD Coding via Interactive Shared Representation Networks with Self-distillation Mechanism",
    "volume": "long",
    "abstract": "The ICD coding task aims at assigning codes of the International Classification of Diseases in clinical notes. Since manual coding is very laborious and prone to errors, many methods have been proposed for the automatic ICD coding task. However, existing works either ignore the long-tail of code frequency or the noisy clinical notes. To address the above issues, we propose an Interactive Shared Representation Network with Self-Distillation Mechanism. Specifically, an interactive shared representation network targets building connections among codes while modeling the co-occurrence, consequently alleviating the long-tail problem. Moreover, to cope with the noisy text issue, we encourage the model to focus on the clinical note’s noteworthy part and extract valuable information through a self-distillation learning mechanism. Experimental results on two MIMIC datasets demonstrate the effectiveness of our method",
    "checked": true,
    "id": "ce2ed0c0fc58ff34061bd1f8e200c8c5aff71908",
    "semantic_title": "",
    "citation_count": 21,
    "authors": [
      "Tong Zhou",
      "Pengfei Cao",
      "Yubo Chen",
      "Kang Liu",
      "Jun Zhao",
      "Kun Niu",
      "Weifeng Chong",
      "Shengping Liu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.464": {
    "title": "PHMOSpell: Phonological and Morphological Knowledge Guided Chinese Spelling Check",
    "volume": "long",
    "abstract": "Chinese Spelling Check (CSC) is a challenging task due to the complex characteristics of Chinese characters. Statistics reveal that most Chinese spelling errors belong to phonological or visual errors. However, previous methods rarely utilize phonological and morphological knowledge of Chinese characters or heavily rely on external resources to model their similarities. To address the above issues, we propose a novel end-to-end trainable model called PHMOSpell, which promotes the performance of CSC with multi-modal information. Specifically, we derive pinyin and glyph representations for Chinese characters from audio and visual modalities respectively, which are integrated into a pre-trained language model by a well-designed adaptive gating mechanism. To verify its effectiveness, we conduct comprehensive experiments and ablation tests. Experimental results on three shared benchmarks demonstrate that our model consistently outperforms previous state-of-the-art models",
    "checked": true,
    "id": "9998875164c1d46c8c41be6f22171e90abf0ccbe",
    "semantic_title": "",
    "citation_count": 27,
    "authors": [
      "Li Huang",
      "Junjie Li",
      "Weiwei Jiang",
      "Zhiyu Zhang",
      "Minchuan Chen",
      "Shaojun Wang",
      "Jing Xiao"
    ]
  },
  "https://aclanthology.org/2021.acl-long.465": {
    "title": "Guiding the Growth: Difficulty-Controllable Question Generation through Step-by-Step Rewriting",
    "volume": "long",
    "abstract": "This paper explores the task of Difficulty-Controllable Question Generation (DCQG), which aims at generating questions with required difficulty levels. Previous research on this task mainly defines the difficulty of a question as whether it can be correctly answered by a Question Answering (QA) system, lacking interpretability and controllability. In our work, we redefine question difficulty as the number of inference steps required to answer it and argue that Question Generation (QG) systems should have stronger control over the logic of generated questions. To this end, we propose a novel framework that progressively increases question difficulty through step-by-step rewriting under the guidance of an extracted reasoning chain. A dataset is automatically constructed to facilitate the research, on which extensive experiments are conducted to test the performance of our method",
    "checked": true,
    "id": "694d83d3430d054a99eed027f58ebfe00de6e46f",
    "semantic_title": "",
    "citation_count": 17,
    "authors": [
      "Yi Cheng",
      "Siyao Li",
      "Bang Liu",
      "Ruihui Zhao",
      "Sujian Li",
      "Chenghua Lin",
      "Yefeng Zheng"
    ]
  },
  "https://aclanthology.org/2021.acl-long.466": {
    "title": "Improving Encoder by Auxiliary Supervision Tasks for Table-to-Text Generation",
    "volume": "long",
    "abstract": "Table-to-text generation aims at automatically generating natural text to help people conveniently obtain salient information in tables. Although neural models for table-to-text have achieved remarkable progress, some problems are still overlooked. Previous methods cannot deduce the factual results from the entity’s (player or team) performance and the relations between entities. To solve this issue, we first build an entity graph from the input tables and introduce a reasoning module to perform reasoning on the graph. Moreover, there are different relations (e.g., the numeric size relation and the importance relation) between records in different dimensions. And these relations may contribute to the data-to-text generation. However, it is hard for a vanilla encoder to capture these. Consequently, we propose to utilize two auxiliary tasks, Number Ranking (NR) and Importance Ranking (IR), to supervise the encoder to capture the different relations. Experimental results on ROTOWIRE and RW-FG show that our method not only has a good generalization but also outperforms previous methods on several metrics: BLEU, Content Selection, Content Ordering",
    "checked": true,
    "id": "ef358603d387ca392b889b18b8314e08cdc5d83b",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Liang Li",
      "Can Ma",
      "Yinliang Yue",
      "Dayong Hu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.467": {
    "title": "POS-Constrained Parallel Decoding for Non-autoregressive Generation",
    "volume": "long",
    "abstract": "The multimodality problem has become a major challenge of existing non-autoregressive generation (NAG) systems. A common solution often resorts to sequence-level knowledge distillation by rebuilding the training dataset through autoregressive generation (hereinafter known as “teacher AG”). The success of such methods may largely depend on a latent assumption, i.e., the teacher AG is superior to the NAG model. However, in this work, we experimentally reveal that this assumption does not always hold for the text generation tasks like text summarization and story ending generation. To provide a feasible solution to the multimodality problem of NAG, we propose incorporating linguistic structure (Part-of-Speech sequence in particular) into NAG inference instead of relying on teacher AG. More specifically, the proposed POS-constrained Parallel Decoding (POSPD) method aims at providing a specific POS sequence to constrain the NAG model during decoding. Our experiments demonstrate that POSPD consistently improves NAG models on four text generation tasks to a greater extent compared to knowledge distillation. This observation validates the necessity of exploring the alternatives for sequence-level knowledge distillation",
    "checked": true,
    "id": "3a25583c170d7b3eef568890ccffd8a500ac9b3e",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Kexin Yang",
      "Wenqiang Lei",
      "Dayiheng Liu",
      "Weizhen Qi",
      "Jiancheng Lv"
    ]
  },
  "https://aclanthology.org/2021.acl-long.468": {
    "title": "Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation",
    "volume": "long",
    "abstract": "A well-known limitation in pretrain-finetune paradigm lies in its inflexibility caused by the one-size-fits-all vocabulary.This potentially weakens the effect when applying pretrained models into natural language generation (NLG) tasks, especially for the subword distributions between upstream and downstream tasks with significant discrepancy. Towards approaching this problem, we extend the vanilla pretrain-finetune pipeline with an extra embedding transfer step. Specifically, a plug-and-play embedding generator is introduced to produce the representation of any input token, according to pre-trained embeddings of its morphologically similar ones.Thus, embeddings of mismatch tokens in downstream tasks can also be efficiently initialized.We conduct experiments on a variety of NLG tasks under the pretrain-finetune fashion. Experimental results and extensive analyses show that the proposed strategy offers us opportunities to feel free to transfer the vocabulary, leading to more efficient and better performed downstream NLG models",
    "checked": true,
    "id": "58e13e1fed9b28e50efcaff611772801d7980c80",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Xin Liu",
      "Baosong Yang",
      "Dayiheng Liu",
      "Haibo Zhang",
      "Weihua Luo",
      "Min Zhang",
      "Haiying Zhang",
      "Jinsong Su"
    ]
  },
  "https://aclanthology.org/2021.acl-long.469": {
    "title": "TGEA: An Error-Annotated Dataset and Benchmark Tasks for TextGeneration from Pretrained Language Models",
    "volume": "long",
    "abstract": "In order to deeply understand the capability of pretrained language models in text generation and conduct a diagnostic evaluation, we propose TGEA, an error-annotated dataset with multiple benchmark tasks for text generation from pretrained language models (PLMs). We use carefully selected prompt words to guide GPT-2 to generate candidate sentences, from which we select 47K for error annotation. Crowdsourced workers manually check each of these sentences and detect 12k erroneous sentences. We create an error taxonomy to cover 24 types of errors occurring in these erroneous sentences according to the nature of errors with respect to linguistics and knowledge (e.g., common sense). For each erroneous span in PLM-generated sentences, we also detect another span that is closely associated with it. Each error is hence manually labeled with comprehensive annotations, including the span of the error, the associated span, minimal correction to the error, the type of the error, and rationale behind the error. Apart from the fully annotated dataset, we also present a detailed description of the data collection procedure, statistics and analysis of the dataset. This is the first dataset with comprehensive annotations for PLM-generated texts, which facilitates the diagnostic evaluation of PLM-based text generation. Furthermore, we use TGEA as a benchmark dataset and propose a series of automatic diagnosis tasks, including error detection, error type classification, associated span detection, error rationale generation, to further promote future study on the automatic error detection and correction on texts generated by pretrained language models",
    "checked": true,
    "id": "1db86e01300e2f30fd08b46e63ea11656cb6dcf5",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Jie He",
      "Bo Peng",
      "Yi Liao",
      "Qun Liu",
      "Deyi Xiong"
    ]
  },
  "https://aclanthology.org/2021.acl-long.470": {
    "title": "Long-Span Summarization via Local Attention and Content Selection",
    "volume": "long",
    "abstract": "Transformer-based models have achieved state-of-the-art results in a wide range of natural language processing (NLP) tasks including document summarization. Typically these systems are trained by fine-tuning a large pre-trained model to the target task. One issue with these transformer-based models is that they do not scale well in terms of memory and compute requirements as the input length grows. Thus, for long document summarization, it can be challenging to train or fine-tune these models. In this work, we exploit large pre-trained transformer-based models and address long-span dependencies in abstractive summarization using two methods: local self-attention; and explicit content selection. These approaches are compared on a range of network configurations. Experiments are carried out on standard long-span summarization tasks, including Spotify Podcast, arXiv, and PubMed datasets. We demonstrate that by combining these methods, we can achieve state-of-the-art results on all three tasks in the ROUGE scores. Moreover, without a large-scale GPU card, our approach can achieve comparable or better results than existing approaches",
    "checked": true,
    "id": "f4566761fe39c4b5273d696d9bc3f4195c9325bb",
    "semantic_title": "",
    "citation_count": 21,
    "authors": [
      "Potsawee Manakul",
      "Mark Gales"
    ]
  },
  "https://aclanthology.org/2021.acl-long.471": {
    "title": "RepSum: Unsupervised Dialogue Summarization based on Replacement Strategy",
    "volume": "long",
    "abstract": "In the field of dialogue summarization, due to the lack of training data, it is often difficult for supervised summary generation methods to learn vital information from dialogue context with limited data. Several attempts on unsupervised summarization for text by leveraging semantic information solely or auto-encoder strategy (i.e., sentence compression), it however cannot be adapted to the dialogue scene due to the limited words in utterances and huge gap between the dialogue and its summary. In this study, we propose a novel unsupervised strategy to address this challenge, which roots from the hypothetical foundation that a superior summary approximates a replacement of the original dialogue, and they are roughly equivalent for auxiliary (self-supervised) tasks, e.g., dialogue generation. The proposed strategy RepSum is applied to generate both extractive and abstractive summary with the guidance of the followed nˆth utterance generation and classification tasks. Extensive experiments on various datasets demonstrate the superiority of the proposed model compared with the state-of-the-art methods",
    "checked": true,
    "id": "fe4b825d049c4e982da70cc10a2db8cbe5851cd8",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Xiyan Fu",
      "Yating Zhang",
      "Tianyi Wang",
      "Xiaozhong Liu",
      "Changlong Sun",
      "Zhenglu Yang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.472": {
    "title": "BASS: Boosting Abstractive Summarization with Unified Semantic Graph",
    "volume": "long",
    "abstract": "Abstractive summarization for long-document or multi-document remains challenging for the Seq2Seq architecture, as Seq2Seq is not good at analyzing long-distance relations in text. In this paper, we present BASS, a novel framework for Boosting Abstractive Summarization based on a unified Semantic graph, which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases. Further, a graph-based encoder-decoder model is proposed to improve both the document representation and summary generation process by leveraging the graph structure. Specifically, several graph augmentation methods are designed to encode both the explicit and implicit relations in the text while the graph-propagation attention mechanism is developed in the decoder to select salient content into the summary. Empirical results show that the proposed architecture brings substantial improvements for both long-document and multi-document summarization tasks",
    "checked": true,
    "id": "a62209a3f90a5bb23054b0a126f5f5f23b9e4b53",
    "semantic_title": "",
    "citation_count": 29,
    "authors": [
      "Wenhao Wu",
      "Wei Li",
      "Xinyan Xiao",
      "Jiachen Liu",
      "Ziqiang Cao",
      "Sujian Li",
      "Hua Wu",
      "Haifeng Wang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.473": {
    "title": "Capturing Relations between Scientific Papers: An Abstractive Model for Related Work Section Generation",
    "volume": "long",
    "abstract": "Given a set of related publications, related work section generation aims to provide researchers with an overview of the specific research area by summarizing these works and introducing them in a logical order. Most of existing related work generation models follow the inflexible extractive style, which directly extract sentences from multiple original papers to form a related work discussion. Hence, in this paper, we propose a Relation-aware Related work Generator (RRG), which generates an abstractive related work from the given multiple scientific papers in the same research area. Concretely, we propose a relation-aware multi-document encoder that relates one document to another according to their content dependency in a relation graph. The relation graph and the document representation are interacted and polished iteratively, complementing each other in the training process. We also contribute two public datasets composed of related work sections and their corresponding papers. Extensive experiments on the two datasets show that the proposed model brings substantial improvements over several strong baselines. We hope that this work will promote advances in related work generation task",
    "checked": true,
    "id": "85bc3c3707e514430ed29dfedc454d5f77630f46",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Xiuying Chen",
      "Hind Alamro",
      "Mingzhe Li",
      "Shen Gao",
      "Xiangliang Zhang",
      "Dongyan Zhao",
      "Rui Yan"
    ]
  },
  "https://aclanthology.org/2021.acl-long.474": {
    "title": "Focus Attention: Promoting Faithfulness and Diversity in Summarization",
    "volume": "long",
    "abstract": "Professional summaries are written with document-level information, such as the theme of the document, in mind. This is in contrast with most seq2seq decoders which simultaneously learn to focus on salient content, while deciding what to generate, at each decoding step. With the motivation to narrow this gap, we introduce Focus Attention Mechanism, a simple yet effective method to encourage decoders to proactively generate tokens that are similar or topical to the input document. Further, we propose a Focus Sampling method to enable generation of diverse summaries, an area currently understudied in summarization. When evaluated on the BBC extreme summarization task, two state-of-the-art models augmented with Focus Attention generate summaries that are closer to the target and more faithful to their input documents, outperforming their vanilla counterparts on ROUGE and multiple faithfulness measures. We also empirically demonstrate that Focus Sampling is more effective in generating diverse and faithful summaries than top-k or nucleus sampling-based decoding methods",
    "checked": true,
    "id": "630b19f8a856177dc9fcab3222cacb47f77985c3",
    "semantic_title": "",
    "citation_count": 24,
    "authors": [
      "Rahul Aralikatte",
      "Shashi Narayan",
      "Joshua Maynez",
      "Sascha Rothe",
      "Ryan McDonald"
    ]
  },
  "https://aclanthology.org/2021.acl-long.475": {
    "title": "Generating Query Focused Summaries from Query-Free Resources",
    "volume": "long",
    "abstract": "The availability of large-scale datasets has driven the development of neural models that create generic summaries from single or multiple documents. In this work we consider query focused summarization (QFS), a task for which training data in the form of queries, documents, and summaries is not readily available. We propose to decompose QFS into (1) query modeling (i.e., finding supportive evidence within a set of documents for a query) and (2) conditional language modeling (i.e., summary generation). We introduce MaRGE, a Masked ROUGE Regression framework for evidence estimation and ranking which relies on a unified representation for summaries and queries, so that summaries in generic data can be converted into proxy queries for learning a query model. Experiments across QFS benchmarks and query types show that our model achieves state-of-the-art performance despite learning from weak supervision",
    "checked": true,
    "id": "94b1bb4c23affc1b02a479e1b0347c1de0b17b11",
    "semantic_title": "",
    "citation_count": 17,
    "authors": [
      "Yumo Xu",
      "Mirella Lapata"
    ]
  },
  "https://aclanthology.org/2021.acl-long.476": {
    "title": "Robustifying Multi-hop QA through Pseudo-Evidentiality Training",
    "volume": "long",
    "abstract": "This paper studies the bias problem of multi-hop question answering models, of answering correctly without correct reasoning. One way to robustify these models is by supervising to not only answer right, but also with right reasoning chains. An existing direction is to annotate reasoning chains to train models, requiring expensive additional annotations. In contrast, we propose a new approach to learn evidentiality, deciding whether the answer prediction is supported by correct evidences, without such annotations. Instead, we compare counterfactual changes in answer confidence with and without evidence sentences, to generate “pseudo-evidentiality” annotations. We validate our proposed model on an original set and challenge set in HotpotQA, showing that our method is accurate and robust in multi-hop reasoning",
    "checked": true,
    "id": "a0c51191f7b3fa8207b4401903c10b87e2458129",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Kyungjae Lee",
      "Seung-won Hwang",
      "Sang-eun Han",
      "Dohyeon Lee"
    ]
  },
  "https://aclanthology.org/2021.acl-long.477": {
    "title": "xMoCo: Cross Momentum Contrastive Learning for Open-Domain Question Answering",
    "volume": "long",
    "abstract": "Dense passage retrieval has been shown to be an effective approach for information retrieval tasks such as open domain question answering. Under this paradigm, a dual-encoder model is learned to encode questions and passages separately into vector representations, and all the passage vectors are then pre-computed and indexed, which can be efficiently retrieved by vector space search during inference time. In this paper, we propose a new contrastive learning method called Cross Momentum Contrastive learning (xMoCo), for learning a dual-encoder model for question-passage matching. Our method efficiently maintains a large pool of negative samples like the original MoCo, and by jointly optimizing question-to-passage and passage-to-question matching tasks, enables using separate encoders for questions and passages. We evaluate our method on various open-domain question answering dataset, and the experimental results show the effectiveness of the proposed method",
    "checked": true,
    "id": "cd2f3398727bf3d5ccef42b33ad9097aeb1c44f1",
    "semantic_title": "",
    "citation_count": 21,
    "authors": [
      "Nan Yang",
      "Furu Wei",
      "Binxing Jiao",
      "Daxing Jiang",
      "Linjun Yang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.478": {
    "title": "Learn to Resolve Conversational Dependency: A Consistency Training Framework for Conversational Question Answering",
    "volume": "long",
    "abstract": "One of the main challenges in conversational question answering (CQA) is to resolve the conversational dependency, such as anaphora and ellipsis. However, existing approaches do not explicitly train QA models on how to resolve the dependency, and thus these models are limited in understanding human dialogues. In this paper, we propose a novel framework, ExCorD (Explicit guidance on how to resolve Conversational Dependency) to enhance the abilities of QA models in comprehending conversational context. ExCorD first generates self-contained questions that can be understood without the conversation history, then trains a QA model with the pairs of original and self-contained questions using a consistency-based regularizer. In our experiments, we demonstrate that ExCorD significantly improves the QA models’ performance by up to 1.2 F1 on QuAC, and 5.2 F1 on CANARD, while addressing the limitations of the existing approaches",
    "checked": true,
    "id": "653927114923a82bbe92e4872e5dd555f078c056",
    "semantic_title": "",
    "citation_count": 23,
    "authors": [
      "Gangwoo Kim",
      "Hyunjae Kim",
      "Jungsoo Park",
      "Jaewoo Kang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.479": {
    "title": "PhotoChat: A Human-Human Dialogue Dataset With Photo Sharing Behavior For Joint Image-Text Modeling",
    "volume": "long",
    "abstract": "We present a new human-human dialogue dataset - PhotoChat, the first dataset that casts light on the photo sharing behavior in online messaging. PhotoChat contains 12k dialogues, each of which is paired with a user photo that is shared during the conversation. Based on this dataset, we propose two tasks to facilitate research on image-text modeling: a photo-sharing intent prediction task that predicts whether one intends to share a photo in the next conversation turn, and a photo retrieval task that retrieves the most relevant photo according to the dialogue context. In addition, for both tasks, we provide baseline models using the state-of-the-art models and report their benchmark performances. The best image retrieval model achieves 10.4% recall@1 (out of 1000 candidates) and the best photo intent prediction model achieves 58.1% F1 score, indicating that the dataset presents interesting yet challenging real-world problems. We are releasing PhotoChat to facilitate future research work among the community",
    "checked": true,
    "id": "cdd00f4a6741011313f331a6705c841ce2b4479c",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Xiaoxue Zang",
      "Lijuan Liu",
      "Maria Wang",
      "Yang Song",
      "Hao Zhang",
      "Jindong Chen"
    ]
  },
  "https://aclanthology.org/2021.acl-long.480": {
    "title": "Good for Misconceived Reasons: An Empirical Revisiting on the Need for Visual Context in Multimodal Machine Translation",
    "volume": "long",
    "abstract": "A neural multimodal machine translation (MMT) system is one that aims to perform better translation by extending conventional text-only translation models with multimodal information. Many recent studies report improvements when equipping their models with the multimodal module, despite the controversy of whether such improvements indeed come from the multimodal part. We revisit the contribution of multimodal information in MMT by devising two interpretable MMT models. To our surprise, although our models replicate similar gains as recently developed multimodal-integrated systems achieved, our models learn to ignore the multimodal information. Upon further investigation, we discover that the improvements achieved by the multimodal models over text-only counterparts are in fact results of the regularization effect. We report empirical findings that highlight the importance of MMT models’ interpretability, and discuss how our findings will benefit future research",
    "checked": true,
    "id": "65ebd6debde06e4e41dab9fed0ff142e5ec12561",
    "semantic_title": "",
    "citation_count": 29,
    "authors": [
      "Zhiyong Wu",
      "Lingpeng Kong",
      "Wei Bi",
      "Xiang Li",
      "Ben Kao"
    ]
  },
  "https://aclanthology.org/2021.acl-long.481": {
    "title": "Attend What You Need: Motion-Appearance Synergistic Networks for Video Question Answering",
    "volume": "long",
    "abstract": "Video Question Answering is a task which requires an AI agent to answer questions grounded in video. This task entails three key challenges: (1) understand the intention of various questions, (2) capturing various elements of the input video (e.g., object, action, causality), and (3) cross-modal grounding between language and vision information. We propose Motion-Appearance Synergistic Networks (MASN), which embed two cross-modal features grounded on motion and appearance information and selectively utilize them depending on the question’s intentions. MASN consists of a motion module, an appearance module, and a motion-appearance fusion module. The motion module computes the action-oriented cross-modal joint representations, while the appearance module focuses on the appearance aspect of the input video. Finally, the motion-appearance fusion module takes each output of the motion module and the appearance module as input, and performs question-guided fusion. As a result, MASN achieves new state-of-the-art performance on the TGIF-QA and MSVD-QA datasets. We also conduct qualitative analysis by visualizing the inference results of MASN",
    "checked": true,
    "id": "9c2f872cad55d691d484b42c2a8dd95b1222a958",
    "semantic_title": "",
    "citation_count": 26,
    "authors": [
      "Ahjeong Seo",
      "Gi-Cheon Kang",
      "Joonhan Park",
      "Byoung-Tak Zhang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.482": {
    "title": "BERTifying the Hidden Markov Model for Multi-Source Weakly Supervised Named Entity Recognition",
    "volume": "long",
    "abstract": "We study the problem of learning a named entity recognition (NER) tagger using noisy labels from multiple weak supervision sources. Though cheap to obtain, the labels from weak supervision sources are often incomplete, inaccurate, and contradictory, making it difficult to learn an accurate NER model. To address this challenge, we propose a conditional hidden Markov model (CHMM), which can effectively infer true labels from multi-source noisy labels in an unsupervised way. CHMM enhances the classic hidden Markov model with the contextual representation power of pre-trained language models. Specifically, CHMM learns token-wise transition and emission probabilities from the BERT embeddings of the input tokens to infer the latent true labels from noisy observations. We further refine CHMM with an alternate-training approach (CHMM-ALT). It fine-tunes a BERT-NER model with the labels inferred by CHMM, and this BERT-NER’s output is regarded as an additional weak source to train the CHMM in return. Experiments on four NER benchmarks from various domains show that our method outperforms state-of-the-art weakly supervised NER models by wide margins",
    "checked": true,
    "id": "92d9231cf56d7d5cf388cb33c4a2cfc00f8cbef8",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Yinghao Li",
      "Pranav Shetty",
      "Lucas Liu",
      "Chao Zhang",
      "Le Song"
    ]
  },
  "https://aclanthology.org/2021.acl-long.483": {
    "title": "CIL: Contrastive Instance Learning Framework for Distantly Supervised Relation Extraction",
    "volume": "long",
    "abstract": "The journey of reducing noise from distant supervision (DS) generated training data has been started since the DS was first introduced into the relation extraction (RE) task. For the past decade, researchers apply the multi-instance learning (MIL) framework to find the most reliable feature from a bag of sentences. Although the pattern of MIL bags can greatly reduce DS noise, it fails to represent many other useful sentence features in the datasets. In many cases, these sentence features can only be acquired by extra sentence-level human annotation with heavy costs. Therefore, the performance of distantly supervised RE models is bounded. In this paper, we go beyond typical MIL framework and propose a novel contrastive instance learning (CIL) framework. Specifically, we regard the initial MIL as the relational triple encoder and constraint positive pairs against negative pairs for each instance. Experiments demonstrate the effectiveness of our proposed framework, with significant improvements over the previous methods on NYT10, GDS and KBP",
    "checked": true,
    "id": "04885b8ff6cad73eba0905e515428d1bdf3088b3",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Tao Chen",
      "Haizhou Shi",
      "Siliang Tang",
      "Zhigang Chen",
      "Fei Wu",
      "Yueting Zhuang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.484": {
    "title": "SENT: Sentence-level Distant Relation Extraction via Negative Training",
    "volume": "long",
    "abstract": "Distant supervision for relation extraction provides uniform bag labels for each sentence inside the bag, while accurate sentence labels are important for downstream applications that need the exact relation type. Directly using bag labels for sentence-level training will introduce much noise, thus severely degrading performance. In this work, we propose the use of negative training (NT), in which a model is trained using complementary labels regarding that “the instance does not belong to these complementary labels”. Since the probability of selecting a true label as a complementary label is low, NT provides less noisy information. Furthermore, the model trained with NT is able to separate the noisy data from the training data. Based on NT, we propose a sentence-level framework, SENT, for distant relation extraction. SENT not only filters the noisy data to construct a cleaner dataset, but also performs a re-labeling process to transform the noisy data into useful training data, thus further benefiting the model’s performance. Experimental results show the significant improvement of the proposed method over previous methods on sentence-level evaluation and de-noise effect",
    "checked": true,
    "id": "72bcf9667e9809f5391a29c7adc5bb6bf702a471",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Ruotian Ma",
      "Tao Gui",
      "Linyang Li",
      "Qi Zhang",
      "Xuanjing Huang",
      "Yaqian Zhou"
    ]
  },
  "https://aclanthology.org/2021.acl-long.485": {
    "title": "An End-to-End Progressive Multi-Task Learning Framework for Medical Named Entity Recognition and Normalization",
    "volume": "long",
    "abstract": "Medical named entity recognition (NER) and normalization (NEN) are fundamental for constructing knowledge graphs and building QA systems. Existing implementations for medical NER and NEN are suffered from the error propagation between the two tasks. The mispredicted mentions from NER will directly influence the results of NEN. Therefore, the NER module is the bottleneck of the whole system. Besides, the learnable features for both tasks are beneficial to improving the model performance. To avoid the disadvantages of existing models and exploit the generalized representation across the two tasks, we design an end-to-end progressive multi-task learning model for jointly modeling medical NER and NEN in an effective way. There are three level tasks with progressive difficulty in the framework. The progressive tasks can reduce the error propagation with the incremental task settings which implies the lower level tasks gain the supervised signals other than errors from the higher level tasks to improve their performances. Besides, the context features are exploited to enrich the semantic information of entity mentions extracted by NER. The performance of NEN profits from the enhanced entity mention features. The standard entities from knowledge bases are introduced into the NER module for extracting corresponding entity mentions correctly. The empirical results on two publicly available medical literature datasets demonstrate the superiority of our method over nine typical methods",
    "checked": true,
    "id": "49804b6edc7644af7482171239ab9945c35cb77f",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Baohang Zhou",
      "Xiangrui Cai",
      "Ying Zhang",
      "Xiaojie Yuan"
    ]
  },
  "https://aclanthology.org/2021.acl-long.486": {
    "title": "PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction",
    "volume": "long",
    "abstract": "Joint extraction of entities and relations from unstructured texts is a crucial task in information extraction. Recent methods achieve considerable performance but still suffer from some inherent limitations, such as redundancy of relation prediction, poor generalization of span-based extraction and inefficiency. In this paper, we decompose this task into three subtasks, Relation Judgement, Entity Extraction and Subject-object Alignment from a novel perspective and then propose a joint relational triple extraction framework based on Potential Relation and Global Correspondence (PRGC). Specifically, we design a component to predict potential relations, which constrains the following entity extraction to the predicted relation subset rather than all relations; then a relation-specific sequence tagging component is applied to handle the overlapping problem between subjects and objects; finally, a global correspondence component is designed to align the subject and object into a triple with low-complexity. Extensive experiments show that PRGC achieves state-of-the-art performance on public benchmarks with higher efficiency and delivers consistent performance gain on complex scenarios of overlapping triples. The source code has been submitted as the supplementary material and will be made publicly available after the blind review",
    "checked": true,
    "id": "7b66a5dd12d4b262138dc5864e908bc87f2d919a",
    "semantic_title": "",
    "citation_count": 55,
    "authors": [
      "Hengyi Zheng",
      "Rui Wen",
      "Xi Chen",
      "Yifan Yang",
      "Yunyan Zhang",
      "Ziheng Zhang",
      "Ningyu Zhang",
      "Bin Qin",
      "Xu Ming",
      "Yefeng Zheng"
    ]
  },
  "https://aclanthology.org/2021.acl-long.487": {
    "title": "Learning from Miscellaneous Other-Class Words for Few-shot Named Entity Recognition",
    "volume": "long",
    "abstract": "Few-shot Named Entity Recognition (NER) exploits only a handful of annotations to iden- tify and classify named entity mentions. Pro- totypical network shows superior performance on few-shot NER. However, existing prototyp- ical methods fail to differentiate rich seman- tics in other-class words, which will aggravate overfitting under few shot scenario. To address the issue, we propose a novel model, Mining Undefined Classes from Other-class (MUCO), that can automatically induce different unde- fined classes from the other class to improve few-shot NER. With these extra-labeled unde- fined classes, our method will improve the dis- criminative ability of NER classifier and en- hance the understanding of predefined classes with stand-by semantic knowledge. Experi- mental results demonstrate that our model out- performs five state-of-the-art models in both 1- shot and 5-shots settings on four NER bench- marks. We will release the code upon accep- tance. The source code is released on https: //github.com/shuaiwa16/OtherClassNER.git",
    "checked": true,
    "id": "4a534218cde7fd2a2efcc05b6f541b4c34596079",
    "semantic_title": "",
    "citation_count": 18,
    "authors": [
      "Meihan Tong",
      "Shuai Wang",
      "Bin Xu",
      "Yixin Cao",
      "Minghui Liu",
      "Lei Hou",
      "Juanzi Li"
    ]
  },
  "https://aclanthology.org/2021.acl-long.488": {
    "title": "Joint Biomedical Entity and Relation Extraction with Knowledge-Enhanced Collective Inference",
    "volume": "long",
    "abstract": "Compared to the general news domain, information extraction (IE) from biomedical text requires much broader domain knowledge. However, many previous IE methods do not utilize any external knowledge during inference. Due to the exponential growth of biomedical publications, models that do not go beyond their fixed set of parameters will likely fall behind. Inspired by how humans look up relevant information to comprehend a scientific text, we present a novel framework that utilizes external knowledge for joint entity and relation extraction named KECI (Knowledge-Enhanced Collective Inference). Given an input text, KECI first constructs an initial span graph representing its initial understanding of the text. It then uses an entity linker to form a knowledge graph containing relevant background knowledge for the the entity mentions in the text. To make the final predictions, KECI fuses the initial span graph and the knowledge graph into a more refined graph using an attention mechanism. KECI takes a collective approach to link mention spans to entities by integrating global relational information into local representations using graph convolutional networks. Our experimental results show that the framework is highly effective, achieving new state-of-the-art results in two different benchmark datasets: BioRelEx (binding interaction detection) and ADE (adverse drug event extraction). For example, KECI achieves absolute improvements of 4.59% and 4.91% in F1 scores over the state-of-the-art on the BioRelEx entity and relation extraction tasks",
    "checked": true,
    "id": "983fdd94067ff52972a931595bffb8933d6df968",
    "semantic_title": "",
    "citation_count": 25,
    "authors": [
      "Tuan Lai",
      "Heng Ji",
      "ChengXiang Zhai",
      "Quan Hung Tran"
    ]
  },
  "https://aclanthology.org/2021.acl-long.489": {
    "title": "Fine-grained Information Extraction from Biomedical Literature based on Knowledge-enriched Abstract Meaning Representation",
    "volume": "long",
    "abstract": "Biomedical Information Extraction from scientific literature presents two unique and non-trivial challenges. First, compared with general natural language texts, sentences from scientific papers usually possess wider contexts between knowledge elements. Moreover, comprehending the fine-grained scientific entities and events urgently requires domain-specific background knowledge. In this paper, we propose a novel biomedical Information Extraction (IE) model to tackle these two challenges and extract scientific entities and events from English research papers. We perform Abstract Meaning Representation (AMR) to compress the wide context to uncover a clear semantic structure for each complex sentence. Besides, we construct the sentence-level knowledge graph from an external knowledge base and use it to enrich the AMR graph to improve the model’s understanding of complex scientific concepts. We use an edge-conditioned graph attention network to encode the knowledge-enriched AMR graph for biomedical IE tasks. Experiments on the GENIA 2011 dataset show that the AMR and external knowledge have contributed 1.8% and 3.0% absolute F-score gains respectively. In order to evaluate the impact of our approach on real-world problems that involve topic-specific fine-grained knowledge elements, we have also created a new ontology and annotated corpus for entity and event extraction for the COVID-19 scientific literature, which can serve as a new benchmark for the biomedical IE community",
    "checked": true,
    "id": "41350b24e0766b40a90fede59adec1531340dea9",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Zixuan Zhang",
      "Nikolaus Parulian",
      "Heng Ji",
      "Ahmed Elsayed",
      "Skatje Myers",
      "Martha Palmer"
    ]
  },
  "https://aclanthology.org/2021.acl-long.490": {
    "title": "Unleash GPT-2 Power for Event Detection",
    "volume": "long",
    "abstract": "Event Detection (ED) aims to recognize mentions of events (i.e., event triggers) and their types in text. Recently, several ED datasets in various domains have been proposed. However, the major limitation of these resources is the lack of enough training data for individual event types which hinders the efficient training of data-hungry deep learning models. To overcome this issue, we propose to exploit the powerful pre-trained language model GPT-2 to generate training samples for ED. To prevent the noises inevitable in automatically generated data from hampering training process, we propose to exploit a teacher-student architecture in which the teacher is supposed to learn anchor knowledge from the original data. The student is then trained on combination of the original and GPT-generated data while being led by the anchor knowledge from the teacher. Optimal transport is introduced to facilitate the anchor knowledge-based guidance between the two networks. We evaluate the proposed model on multiple ED benchmark datasets, gaining consistent improvement and establishing state-of-the-art results for ED",
    "checked": true,
    "id": "2cfeea5d8aadaacaba1c693a6af8c3d651ab4fa5",
    "semantic_title": "",
    "citation_count": 27,
    "authors": [
      "Amir Pouran Ben Veyseh",
      "Viet Lai",
      "Franck Dernoncourt",
      "Thien Huu Nguyen"
    ]
  },
  "https://aclanthology.org/2021.acl-long.491": {
    "title": "CLEVE: Contrastive Pre-training for Event Extraction",
    "volume": "long",
    "abstract": "Event extraction (EE) has considerably benefited from pre-trained language models (PLMs) by fine-tuning. However, existing pre-training methods have not involved modeling event characteristics, resulting in the developed EE models cannot take full advantage of large-scale unsupervised data. To this end, we propose CLEVE, a contrastive pre-training framework for EE to better learn event knowledge from large unsupervised data and their semantic structures (e.g. AMR) obtained with automatic parsers. CLEVE contains a text encoder to learn event semantics and a graph encoder to learn event structures respectively. Specifically, the text encoder learns event semantic representations by self-supervised contrastive learning to represent the words of the same events closer than those unrelated words; the graph encoder learns event structure representations by graph contrastive pre-training on parsed event-related semantic structures. The two complementary representations then work together to improve both the conventional supervised EE and the unsupervised “liberal” EE, which requires jointly extracting events and discovering event schemata without any annotated data. Experiments on ACE 2005 and MAVEN datasets show that CLEVE achieves significant improvements, especially in the challenging unsupervised setting. The source code and pre-trained checkpoints can be obtained from https://github.com/THU-KEG/CLEVE",
    "checked": true,
    "id": "2580aed3ac10d971f86d21f4c06db2de0cfb3c22",
    "semantic_title": "",
    "citation_count": 48,
    "authors": [
      "Ziqi Wang",
      "Xiaozhi Wang",
      "Xu Han",
      "Yankai Lin",
      "Lei Hou",
      "Zhiyuan Liu",
      "Peng Li",
      "Juanzi Li",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2021.acl-long.492": {
    "title": "Document-level Event Extraction via Parallel Prediction Networks",
    "volume": "long",
    "abstract": "Document-level event extraction (DEE) is indispensable when events are described throughout a document. We argue that sentence-level extractors are ill-suited to the DEE task where event arguments always scatter across sentences and multiple events may co-exist in a document. It is a challenging task because it requires a holistic understanding of the document and an aggregated ability to assemble arguments across multiple sentences. In this paper, we propose an end-to-end model, which can extract structured events from a document in a parallel manner. Specifically, we first introduce a document-level encoder to obtain the document-aware representations. Then, a multi-granularity non-autoregressive decoder is used to generate events in parallel. Finally, to train the entire model, a matching loss function is proposed, which can bootstrap a global optimization. The empirical results on the widely used DEE dataset show that our approach significantly outperforms current state-of-the-art methods in the challenging DEE task. Code will be available at https://github.com/HangYang-NLP/DE-PPN",
    "checked": true,
    "id": "c952b73fd8ac6a7d4ad00d78f6b3b8d1caed6a8f",
    "semantic_title": "",
    "citation_count": 18,
    "authors": [
      "Hang Yang",
      "Dianbo Sui",
      "Yubo Chen",
      "Kang Liu",
      "Jun Zhao",
      "Taifeng Wang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.493": {
    "title": "StructuralLM: Structural Pre-training for Form Understanding",
    "volume": "long",
    "abstract": "Large pre-trained language models achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, they almost exclusively focus on text-only representation, while neglecting cell-level layout information that is important for form image understanding. In this paper, we propose a new pre-training approach, StructuralLM, to jointly leverage cell and layout information from scanned documents. Specifically, we pre-train StructuralLM with two new designs to make the most of the interactions of cell and layout information: 1) each cell as a semantic unit; 2) classification of cell positions. The pre-trained StructuralLM achieves new state-of-the-art results in different types of downstream tasks, including form understanding (from 78.95 to 85.14), document visual question answering (from 72.59 to 83.94) and document image classification (from 94.43 to 96.08)",
    "checked": true,
    "id": "0fc66c750d06da8dc47e7a3ae9f24af9ff3d6617",
    "semantic_title": "",
    "citation_count": 57,
    "authors": [
      "Chenliang Li",
      "Bin Bi",
      "Ming Yan",
      "Wei Wang",
      "Songfang Huang",
      "Fei Huang",
      "Luo Si"
    ]
  },
  "https://aclanthology.org/2021.acl-long.494": {
    "title": "Dual Graph Convolutional Networks for Aspect-based Sentiment Analysis",
    "volume": "long",
    "abstract": "Aspect-based sentiment analysis is a fine-grained sentiment classification task. Recently, graph neural networks over dependency trees have been explored to explicitly model connections between aspects and opinion words. However, the improvement is limited due to the inaccuracy of the dependency parsing results and the informal expressions and complexity of online reviews. To overcome these challenges, in this paper, we propose a dual graph convolutional networks (DualGCN) model that considers the complementarity of syntax structures and semantic correlations simultaneously. Particularly, to alleviate dependency parsing errors, we design a SynGCN module with rich syntactic knowledge. To capture semantic correlations, we design a SemGCN module with self-attention mechanism. Furthermore, we propose orthogonal and differential regularizers to capture semantic correlations between words precisely by constraining attention scores in the SemGCN module. The orthogonal regularizer encourages the SemGCN to learn semantically correlated words with less overlap for each word. The differential regularizer encourages the SemGCN to learn semantic features that the SynGCN fails to capture. Experimental results on three public datasets show that our DualGCN model outperforms state-of-the-art methods and verify the effectiveness of our model",
    "checked": true,
    "id": "02b1bc3601aafbafc4cf8b27557e599a1690f295",
    "semantic_title": "",
    "citation_count": 103,
    "authors": [
      "Ruifan Li",
      "Hao Chen",
      "Fangxiang Feng",
      "Zhanyu Ma",
      "Xiaojie Wang",
      "Eduard Hovy"
    ]
  },
  "https://aclanthology.org/2021.acl-long.495": {
    "title": "Multi-Label Few-Shot Learning for Aspect Category Detection",
    "volume": "long",
    "abstract": "Aspect category detection (ACD) in sentiment analysis aims to identify the aspect categories mentioned in a sentence. In this paper, we formulate ACD in the few-shot learning scenario. However, existing few-shot learning approaches mainly focus on single-label predictions. These methods can not work well for the ACD task since a sentence may contain multiple aspect categories. Therefore, we propose a multi-label few-shot learning method based on the prototypical network. To alleviate the noise, we design two effective attention mechanisms. The support-set attention aims to extract better prototypes by removing irrelevant aspects. The query-set attention computes multiple prototype-specific representations for each query instance, which are then used to compute accurate distances with the corresponding prototypes. To achieve multi-label inference, we further learn a dynamic threshold per instance by a policy network. Extensive experimental results on three datasets demonstrate that the proposed method significantly outperforms strong baselines",
    "checked": true,
    "id": "4bcd811142901c798c785a6aee4b6ae40bdabe19",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Mengting Hu",
      "Shiwan Zhao",
      "Honglei Guo",
      "Chao Xue",
      "Hang Gao",
      "Tiegang Gao",
      "Renhong Cheng",
      "Zhong Su"
    ]
  },
  "https://aclanthology.org/2021.acl-long.496": {
    "title": "Argument Pair Extraction via Attention-guided Multi-Layer Multi-Cross Encoding",
    "volume": "long",
    "abstract": "Argument pair extraction (APE) is a research task for extracting arguments from two passages and identifying potential argument pairs. Prior research work treats this task as a sequence labeling problem and a binary classification problem on two passages that are directly concatenated together, which has a limitation of not fully utilizing the unique characteristics and inherent relations of two different passages. This paper proposes a novel attention-guided multi-layer multi-cross encoding scheme to address the challenges. The new model processes two passages with two individual sequence encoders and updates their representations using each other’s representations through attention. In addition, the pair prediction part is formulated as a table-filling problem by updating the representations of two sequences’ Cartesian product. Furthermore, an auxiliary attention loss is introduced to guide each argument to align to its paired argument. An extensive set of experiments show that the new model significantly improves the APE performance over several alternatives",
    "checked": true,
    "id": "bdec7620d1df7fc9a32f19d7d90a5e47ee4bddb2",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Liying Cheng",
      "Tianyu Wu",
      "Lidong Bing",
      "Luo Si"
    ]
  },
  "https://aclanthology.org/2021.acl-long.497": {
    "title": "A Neural Transition-based Model for Argumentation Mining",
    "volume": "long",
    "abstract": "The goal of argumentation mining is to automatically extract argumentation structures from argumentative texts. Most existing methods determine argumentative relations by exhaustively enumerating all possible pairs of argument components, which suffer from low efficiency and class imbalance. Moreover, due to the complex nature of argumentation, there is, so far, no universal method that can address both tree and non-tree structured argumentation. Towards these issues, we propose a neural transition-based model for argumentation mining, which incrementally builds an argumentation graph by generating a sequence of actions, avoiding inefficient enumeration operations. Furthermore, our model can handle both tree and non-tree structured argumentation without introducing any structural constraints. Experimental results show that our model achieves the best performance on two public datasets of different structures",
    "checked": true,
    "id": "975d02b643fb13f13a478f7b7a5bc4e3748baf5b",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Jianzhu Bao",
      "Chuang Fan",
      "Jipeng Wu",
      "Yixue Dang",
      "Jiachen Du",
      "Ruifeng Xu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.498": {
    "title": "Keep It Simple: Unsupervised Simplification of Multi-Paragraph Text",
    "volume": "long",
    "abstract": "This work presents Keep it Simple (KiS), a new approach to unsupervised text simplification which learns to balance a reward across three properties: fluency, salience and simplicity. We train the model with a novel algorithm to optimize the reward (k-SCST), in which the model proposes several candidate simplifications, computes each candidate’s reward, and encourages candidates that outperform the mean reward. Finally, we propose a realistic text comprehension task as an evaluation method for text simplification. When tested on the English news domain, the KiS model outperforms strong supervised baselines by more than 4 SARI points, and can help people complete a comprehension task an average of 18% faster while retaining accuracy, when compared to the original text",
    "checked": true,
    "id": "f07029549bdb29a7afce1acd824fbe4e3dfb25d5",
    "semantic_title": "",
    "citation_count": 25,
    "authors": [
      "Philippe Laban",
      "Tobias Schnabel",
      "Paul Bennett",
      "Marti A. Hearst"
    ]
  },
  "https://aclanthology.org/2021.acl-long.499": {
    "title": "Long Text Generation by Modeling Sentence-Level and Discourse-Level Coherence",
    "volume": "long",
    "abstract": "Generating long and coherent text is an important but challenging task, particularly for open-ended language generation tasks such as story generation. Despite the success in modeling intra-sentence coherence, existing generation models (e.g., BART) still struggle to maintain a coherent event sequence throughout the generated text. We conjecture that this is because of the difficulty for the decoder to capture the high-level semantics and discourse structures in the context beyond token-level co-occurrence. In this paper, we propose a long text generation model, which can represent the prefix sentences at sentence level and discourse level in the decoding process. To this end, we propose two pretraining objectives to learn the representations by predicting inter-sentence semantic similarity and distinguishing between normal and shuffled sentence orders. Extensive experiments show that our model can generate more coherent texts than state-of-the-art baselines",
    "checked": true,
    "id": "798c61b2b985e918a74b9aa154e6bc3f01040763",
    "semantic_title": "",
    "citation_count": 32,
    "authors": [
      "Jian Guan",
      "Xiaoxi Mao",
      "Changjie Fan",
      "Zitao Liu",
      "Wenbiao Ding",
      "Minlie Huang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.500": {
    "title": "OpenMEVA: A Benchmark for Evaluating Open-ended Story Generation Metrics",
    "volume": "long",
    "abstract": "Automatic metrics are essential for developing natural language generation (NLG) models, particularly for open-ended language generation tasks such as story generation. However, existing automatic metrics are observed to correlate poorly with human evaluation. The lack of standardized benchmark datasets makes it difficult to fully evaluate the capabilities of a metric and fairly compare different metrics. Therefore, we propose OpenMEVA, a benchmark for evaluating open-ended story generation metrics. OpenMEVA provides a comprehensive test suite to assess the capabilities of metrics, including (a) the correlation with human judgments, (b) the generalization to different model outputs and datasets, (c) the ability to judge story coherence, and (d) the robustness to perturbations. To this end, OpenMEVA includes both manually annotated stories and auto-constructed test examples. We evaluate existing metrics on OpenMEVA and observe that they have poor correlation with human judgments, fail to recognize discourse-level incoherence, and lack inferential knowledge (e.g., causal order between events), the generalization ability and robustness. Our study presents insights for developing NLG models and metrics in further research",
    "checked": true,
    "id": "7ffc1b425026e916cd6db37c79df3e08e8f47ee6",
    "semantic_title": "",
    "citation_count": 18,
    "authors": [
      "Jian Guan",
      "Zhexin Zhang",
      "Zhuoer Feng",
      "Zitao Liu",
      "Wenbiao Ding",
      "Xiaoxi Mao",
      "Changjie Fan",
      "Minlie Huang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.501": {
    "title": "DYPLOC: Dynamic Planning of Content Using Mixed Language Models for Text Generation",
    "volume": "long",
    "abstract": "We study the task of long-form opinion text generation, which faces at least two distinct challenges. First, existing neural generation models fall short of coherence, thus requiring efficient content planning. Second, diverse types of information are needed to guide the generator to cover both subjective and objective content. To this end, we propose DYPLOC, a generation framework that conducts dynamic planning of content while generating the output based on a novel design of mixed language models. To enrich the generation with diverse content, we further propose to use large pre-trained models to predict relevant concepts and to generate claims. We experiment with two challenging tasks on newly collected datasets: (1) argument generation with Reddit ChangeMyView, and (2) writing articles using New York Times’ Opinion section. Automatic evaluation shows that our model significantly outperforms competitive comparisons. Human judges further confirm that our generations are more coherent with richer content",
    "checked": true,
    "id": "07cf32655f229ed0bfa76aad7e1afc60ea5bc9a5",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Xinyu Hua",
      "Ashwin Sreevatsa",
      "Lu Wang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.502": {
    "title": "Controllable Open-ended Question Generation with A New Question Type Ontology",
    "volume": "long",
    "abstract": "We investigate the less-explored task of generating open-ended questions that are typically answered by multiple sentences. We first define a new question type ontology which differentiates the nuanced nature of questions better than widely used question words. A new dataset with 4,959 questions is labeled based on the new ontology. We then propose a novel question type-aware question generation framework, augmented by a semantic graph representation, to jointly predict question focuses and produce the question. Based on this framework, we further use both exemplars and automatically generated templates to improve controllability and diversity. Experiments on two newly collected large-scale datasets show that our model improves question quality over competitive comparisons based on automatic metrics. Human judges also rate our model outputs highly in answerability, coverage of scope, and overall quality. Finally, our model variants with templates can produce questions with enhanced controllability and diversity",
    "checked": true,
    "id": "0f60df76da1f55c4a904e3d9a40d30798c171103",
    "semantic_title": "",
    "citation_count": 21,
    "authors": [
      "Shuyang Cao",
      "Lu Wang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.503": {
    "title": "BERTGen: Multi-task Generation through BERT",
    "volume": "long",
    "abstract": "We present BERTGen, a novel, generative, decoder-only model which extends BERT by fusing multimodal and multilingual pre-trained models VL-BERT and M-BERT, respectively. BERTGen is auto-regressively trained for language generation tasks, namely image captioning, machine translation and multimodal machine translation, under a multi-task setting. With a comprehensive set of evaluations, we show that BERTGen outperforms many strong baselines across the tasks explored. We also show BERTGen’s ability for zero-shot language generation, where it exhibits competitive performance to supervised counterparts. Finally, we conduct ablation studies which demonstrate that BERTGen substantially benefits from multi-tasking and effectively transfers relevant inductive biases from the pre-trained models",
    "checked": true,
    "id": "1fa1cbc15101ef0ff05c6a5c4f6d0b926f6d4d67",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Faidon Mitzalis",
      "Ozan Caglayan",
      "Pranava Madhyastha",
      "Lucia Specia"
    ]
  },
  "https://aclanthology.org/2021.acl-long.504": {
    "title": "Selective Knowledge Distillation for Neural Machine Translation",
    "volume": "long",
    "abstract": "Neural Machine Translation (NMT) models achieve state-of-the-art performance on many translation benchmarks. As an active research field in NMT, knowledge distillation is widely applied to enhance the model’s performance by transferring teacher model’s knowledge on each training sample. However, previous work rarely discusses the different impacts and connections among these samples, which serve as the medium for transferring teacher knowledge. In this paper, we design a novel protocol that can effectively analyze the different impacts of samples by comparing various samples’ partitions. Based on above protocol, we conduct extensive experiments and find that the teacher’s knowledge is not the more, the better. Knowledge over specific samples may even hurt the whole performance of knowledge distillation. Finally, to address these issues, we propose two simple yet effective strategies, i.e., batch-level and global-level selections, to pick suitable samples for distillation. We evaluate our approaches on two large-scale machine translation tasks, WMT’14 English-German and WMT’19 Chinese-English. Experimental results show that our approaches yield up to +1.28 and +0.89 BLEU points improvements over the Transformer baseline, respectively",
    "checked": true,
    "id": "926eb6dbb08791dad76e4a0468731b02a85a5bba",
    "semantic_title": "",
    "citation_count": 25,
    "authors": [
      "Fusheng Wang",
      "Jianhao Yan",
      "Fandong Meng",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2021.acl-long.505": {
    "title": "Measuring and Increasing Context Usage in Context-Aware Machine Translation",
    "volume": "long",
    "abstract": "Recent work in neural machine translation has demonstrated both the necessity and feasibility of using inter-sentential context, context from sentences other than those currently being translated. However, while many current methods present model architectures that theoretically can use this extra context, it is often not clear how much they do actually utilize it at translation time. In this paper, we introduce a new metric, conditional cross-mutual information, to quantify usage of context by these models. Using this metric, we measure how much document-level machine translation systems use particular varieties of context. We find that target context is referenced more than source context, and that including more context has a diminishing affect on results. We then introduce a new, simple training method, context-aware word dropout, to increase the usage of context by context-aware models. Experiments show that our method not only increases context usage, but also improves the translation quality according to metrics such as BLEU and COMET, as well as performance on anaphoric pronoun resolution and lexical cohesion contrastive datasets",
    "checked": true,
    "id": "83145b7a391b792e24d8d38f74ed6b6ae7a149dc",
    "semantic_title": "",
    "citation_count": 23,
    "authors": [
      "Patrick Fernandes",
      "Kayo Yin",
      "Graham Neubig",
      "André F. T. Martins"
    ]
  },
  "https://aclanthology.org/2021.acl-long.506": {
    "title": "Beyond Offline Mapping: Learning Cross-lingual Word Embeddings through Context Anchoring",
    "volume": "long",
    "abstract": "Recent research on cross-lingual word embeddings has been dominated by unsupervised mapping approaches that align monolingual embeddings. Such methods critically rely on those embeddings having a similar structure, but it was recently shown that the separate training in different languages causes departures from this assumption. In this paper, we propose an alternative approach that does not have this limitation, while requiring a weak seed dictionary (e.g., a list of identical words) as the only form of supervision. Rather than aligning two fixed embedding spaces, our method works by fixing the target language embeddings, and learning a new set of embeddings for the source language that are aligned with them. To that end, we use an extension of skip-gram that leverages translated context words as anchor points, and incorporates self-learning and iterative restarts to reduce the dependency on the initial dictionary. Our approach outperforms conventional mapping methods on bilingual lexicon induction, and obtains competitive results in the downstream XNLI task",
    "checked": true,
    "id": "a8e44f0488ac573d0c9f5f8ba046f03212b34921",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Aitor Ormazabal",
      "Mikel Artetxe",
      "Aitor Soroa",
      "Gorka Labaka",
      "Eneko Agirre"
    ]
  },
  "https://aclanthology.org/2021.acl-long.507": {
    "title": "CCMatrix: Mining Billions of High-Quality Parallel Sentences on the Web",
    "volume": "long",
    "abstract": "We show that margin-based bitext mining in a multilingual sentence space can be successfully scaled to operate on monolingual corpora of billions of sentences. We use 32 snapshots of a curated common crawl corpus (Wenzel et al, 2019) totaling 71 billion unique sentences. Using one unified approach for 90 languages, we were able to mine 10.8 billion parallel sentences, out of which only 2.9 billions are aligned with English. We illustrate the capability of our scalable mining system to create high quality training sets from one language to any other by training hundreds of different machine translation models and evaluating them on the many-to-many TED benchmark. Further, we evaluate on competitive translation benchmarks such as WMT and WAT. Using only mined bitext, we set a new state of the art for a single system on the WMT’19 test set for English-German/Russian/Chinese. In particular, our English/German and English/Russian systems outperform the best single ones by over 4 BLEU points and are on par with best WMT’19 systems, which train on the WMT training data and augment it with backtranslation. We also achieve excellent results for distant languages pairs like Russian/Japanese, outperforming the best submission at the 2020 WAT workshop. All of the mined bitext will be freely available",
    "checked": true,
    "id": "16dbfa7ad452277d2c568913d2550a9a58a43b62",
    "semantic_title": "",
    "citation_count": 147,
    "authors": [
      "Holger Schwenk",
      "Guillaume Wenzek",
      "Sergey Edunov",
      "Edouard Grave",
      "Armand Joulin",
      "Angela Fan"
    ]
  },
  "https://aclanthology.org/2021.acl-long.508": {
    "title": "Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search",
    "volume": "long",
    "abstract": "Despite transformers’ impressive accuracy, their computational cost is often prohibitive to use with limited computational resources. Most previous approaches to improve inference efficiency require a separate model for each possible computational budget. In this paper, we extend PoWER-BERT (Goyal et al., 2020) and propose Length-Adaptive Transformer that can be used for various inference scenarios after one-shot training. We train a transformer with LengthDrop, a structural variant of dropout, which stochastically determines a sequence length at each layer. We then conduct a multi-objective evolutionary search to find a length configuration that maximizes the accuracy and minimizes the efficiency metric under any given computational budget. Additionally, we significantly extend the applicability of PoWER-BERT beyond sequence-level classification into token-level classification with Drop-and-Restore process that drops word-vectors temporarily in intermediate layers and restores at the last layer if necessary. We empirically verify the utility of the proposed approach by demonstrating the superior accuracy-efficiency trade-off under various setups, including span-based question answering and text classification. Code is available at https://github.com/clovaai/lengthadaptive-transformer",
    "checked": true,
    "id": "1a6c5f6ce26914c2a7af0217e8cd3e844f2b2f37",
    "semantic_title": "",
    "citation_count": 32,
    "authors": [
      "Gyuwan Kim",
      "Kyunghyun Cho"
    ]
  },
  "https://aclanthology.org/2021.acl-long.509": {
    "title": "GhostBERT: Generate More Features with Cheap Operations for BERT",
    "volume": "long",
    "abstract": "Transformer-based pre-trained language models like BERT, though powerful in many tasks, are expensive in both memory and computation, due to their large number of parameters. Previous works show that some parameters in these models can be pruned away without severe accuracy drop. However, these redundant features contribute to a comprehensive understanding of the training data and removing them weakens the model’s representation ability. In this paper, we propose GhostBERT, which generates more features with very cheap operations from the remaining features. In this way, GhostBERT has similar memory and computational cost as the pruned model, but enjoys much larger representation power. The proposed ghost module can also be applied to unpruned BERT models to enhance their performance with negligible additional parameters and computation. Empirical results on the GLUE benchmark on three backbone models (i.e., BERT, RoBERTa and ELECTRA) verify the efficacy of our proposed method",
    "checked": true,
    "id": "ef7c03ccd3a044d92a8c457c9bb969c6344f9d5c",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Zhiqi Huang",
      "Lu Hou",
      "Lifeng Shang",
      "Xin Jiang",
      "Xiao Chen",
      "Qun Liu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.510": {
    "title": "Super Tickets in Pre-Trained Language Models: From Model Compression to Improving Generalization",
    "volume": "long",
    "abstract": "The Lottery Ticket Hypothesis suggests that an over-parametrized network consists of ”lottery tickets”, and training a certain collection of them (i.e., a subnetwork) can match the performance of the full model. In this paper, we study such a collection of tickets, which is referred to as ”winning tickets”, in extremely over-parametrized models, e.g., pre-trained language models. We observe that at certain compression ratios, the generalization performance of the winning tickets can not only match but also exceed that of the full model. In particular, we observe a phase transition phenomenon: As the compression ratio increases, generalization performance of the winning tickets first improves then deteriorates after a certain threshold. We refer to the tickets on the threshold as ”super tickets”. We further show that the phase transition is task and model dependent — as the model size becomes larger and the training data set becomes smaller, the transition becomes more pronounced. Our experiments on the GLUE benchmark show that the super tickets improve single task fine-tuning by 0.9 points on BERT-base and 1.0 points on BERT-large, in terms of task-average score. We also demonstrate that adaptively sharing the super tickets across tasks benefits multi-task learning",
    "checked": true,
    "id": "e638b9e6ee09ab4fa748b748099e0f03d471d803",
    "semantic_title": "",
    "citation_count": 31,
    "authors": [
      "Chen Liang",
      "Simiao Zuo",
      "Minshuo Chen",
      "Haoming Jiang",
      "Xiaodong Liu",
      "Pengcheng He",
      "Tuo Zhao",
      "Weizhu Chen"
    ]
  },
  "https://aclanthology.org/2021.acl-long.511": {
    "title": "A Novel Estimator of Mutual Information for Learning to Disentangle Textual Representations",
    "volume": "long",
    "abstract": "Learning disentangled representations of textual data is essential for many natural language tasks such as fair classification, style transfer and sentence generation, among others. The existent dominant approaches in the context of text data either rely on training an adversary (discriminator) that aims at making attribute values difficult to be inferred from the latent code or rely on minimising variational bounds of the mutual information between latent code and the value attribute. However, the available methods suffer of the impossibility to provide a fine-grained control of the degree (or force) of disentanglement. In contrast to adversarial methods, which are remarkably simple, although the adversary seems to be performing perfectly well during the training phase, after it is completed a fair amount of information about the undesired attribute still remains. This paper introduces a novel variational upper bound to the mutual information between an attribute and the latent code of an encoder. Our bound aims at controlling the approximation error via the Renyi’s divergence, leading to both better disentangled representations and in particular, a precise control of the desirable degree of disentanglement than state-of-the-art methods proposed for textual data. Furthermore, it does not suffer from the degeneracy of other losses in multi-class scenarios. We show the superiority of this method on fair classification and on textual style transfer tasks. Additionally, we provide new insights illustrating various trade-offs in style transfer when attempting to learn disentangled representations and quality of the generated sentence",
    "checked": true,
    "id": "7018b5d9e98fb9a2c4d6b5b14594e9187e234a82",
    "semantic_title": "",
    "citation_count": 26,
    "authors": [
      "Pierre Colombo",
      "Pablo Piantanida",
      "Chloé Clavel"
    ]
  },
  "https://aclanthology.org/2021.acl-long.512": {
    "title": "Determinantal Beam Search",
    "volume": "long",
    "abstract": "Beam search is a go-to strategy for decoding neural sequence models. The algorithm can naturally be viewed as a subset optimization problem, albeit one where the corresponding set function does not reflect interactions between candidates. Empirically, this leads to sets often exhibiting high overlap, e.g., strings may differ by only a single word. Yet in use-cases that call for multiple solutions, a diverse or representative set is often desired. To address this issue, we propose a reformulation of beam search, which we call determinantal beam search. Determinantal beam search has a natural relationship to determinantal point processes (DPPs), models over sets that inherently encode intra-set interactions. By posing iterations in beam search as a series of subdeterminant maximization problems, we can turn the algorithm into a diverse subset selection process. In a case study, we use the string subsequence kernel to explicitly encourage n-gram coverage in text generated from a sequence model. We observe that our algorithm offers competitive performance against other diverse set generation strategies in the context of language generation, while providing a more general approach to optimizing for diversity",
    "checked": true,
    "id": "2d842b01d9e07f7d883f75f6b140eb3304ea78c0",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Clara Meister",
      "Martina Forster",
      "Ryan Cotterell"
    ]
  },
  "https://aclanthology.org/2021.acl-long.513": {
    "title": "Multi-hop Graph Convolutional Network with High-order Chebyshev Approximation for Text Reasoning",
    "volume": "long",
    "abstract": "Graph convolutional network (GCN) has become popular in various natural language processing (NLP) tasks with its superiority in long-term and non-consecutive word interactions. However, existing single-hop graph reasoning in GCN may miss some important non-consecutive dependencies. In this study, we define the spectral graph convolutional network with the high-order dynamic Chebyshev approximation (HDGCN), which augments the multi-hop graph reasoning by fusing messages aggregated from direct and long-term dependencies into one convolutional layer. To alleviate the over-smoothing in high-order Chebyshev approximation, a multi-vote-based cross-attention (MVCAttn) with linear computation complexity is also proposed. The empirical results on four transductive and inductive NLP tasks and the ablation study verify the efficacy of the proposed model",
    "checked": true,
    "id": "bf07a4087e53738a0ed5aa263e4a5ff730ae74e1",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuoran Jiang",
      "Qingcai Chen",
      "Xin Liu",
      "Baotian Hu",
      "Lisai Zhang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.514": {
    "title": "Accelerating Text Communication via Abbreviated Sentence Input",
    "volume": "long",
    "abstract": "Typing every character in a text message may require more time or effort than strictly necessary. Skipping spaces or other characters may be able to speed input and reduce a user’s physical input effort. This can be particularly important for people with motor impairments. In a large crowdsourced study, we found workers frequently abbreviated text by omitting mid-word vowels. We designed a recognizer optimized for expanding noisy abbreviated input where users often omit spaces and mid-word vowels. We show using neural language models for selecting conversational-style training text and for rescoring the recognizer’s n-best sentences improved accuracy. On noisy touchscreen data collected from hundreds of users, we found accurate abbreviated input was possible even if a third of characters was omitted. Finally, in a study where users had to dwell for a second on each key, sentence abbreviated input was competitive with a conventional keyboard with word predictions. After practice, users wrote abbreviated sentences at 9.6 words-per-minute versus word input at 9.9 words-per-minute",
    "checked": true,
    "id": "578c9b095b166549625d2221ea4c1446eeb114f6",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Jiban Adhikary",
      "Jamie Berger",
      "Keith Vertanen"
    ]
  },
  "https://aclanthology.org/2021.acl-long.515": {
    "title": "Regression Bugs Are In Your Model! Measuring, Reducing and Analyzing Regressions In NLP Model Updates",
    "volume": "long",
    "abstract": "Behavior of deep neural networks can be inconsistent between different versions. Regressions during model update are a common cause of concern that often over-weigh the benefits in accuracy or efficiency gain. This work focuses on quantifying, reducing and analyzing regression errors in the NLP model updates. Using negative flip rate as regression measure, we show that regression has a prevalent presence across tasks in the GLUE benchmark. We formulate the regression-free model updates into a constrained optimization problem, and further reduce it into a relaxed form which can be approximately optimized through knowledge distillation training method. We empirically analyze how model ensemble reduces regression. Finally, we conduct CheckList behavioral testing to understand the distribution of regressions across linguistic phenomena, and the efficacy of ensemble and distillation methods",
    "checked": true,
    "id": "4f92221c7cedb1bd6212276e1c122dcac9860750",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Yuqing Xie",
      "Yi-An Lai",
      "Yuanjun Xiong",
      "Yi Zhang",
      "Stefano Soatto"
    ]
  },
  "https://aclanthology.org/2021.acl-long.516": {
    "title": "Detecting Propaganda Techniques in Memes",
    "volume": "long",
    "abstract": "Propaganda can be defined as a form of communication that aims to influence the opinions or the actions of people towards a specific goal; this is achieved by means of well-defined rhetorical and psychological devices. Propaganda, in the form we know it today, can be dated back to the beginning of the 17th century. However, it is with the advent of the Internet and the social media that propaganda has started to spread on a much larger scale than before, thus becoming major societal and political issue. Nowadays, a large fraction of propaganda in social media is multimodal, mixing textual with visual content. With this in mind, here we propose a new multi-label multimodal task: detecting the type of propaganda techniques used in memes. We further create and release a new corpus of 950 memes, carefully annotated with 22 propaganda techniques, which can appear in the text, in the image, or in both. Our analysis of the corpus shows that understanding both modalities together is essential for detecting these techniques. This is further confirmed in our experiments with several state-of-the-art multimodal models",
    "checked": true,
    "id": "ed7391f944d413d8d2e67d8d03bc1d4f8e83a327",
    "semantic_title": "",
    "citation_count": 44,
    "authors": [
      "Dimitar Dimitrov",
      "Bishr Bin Ali",
      "Shaden Shaar",
      "Firoj Alam",
      "Fabrizio Silvestri",
      "Hamed Firooz",
      "Preslav Nakov",
      "Giovanni Da San Martino"
    ]
  },
  "https://aclanthology.org/2021.acl-long.517": {
    "title": "On the Efficacy of Adversarial Data Collection for Question Answering: Results from a Large-Scale Randomized Study",
    "volume": "long",
    "abstract": "In adversarial data collection (ADC), a human workforce interacts with a model in real time, attempting to produce examples that elicit incorrect predictions. Researchers hope that models trained on these more challenging datasets will rely less on superficial patterns, and thus be less brittle. However, despite ADC’s intuitive appeal, it remains unclear when training on adversarial datasets produces more robust models. In this paper, we conduct a large-scale controlled study focused on question answering, assigning workers at random to compose questions either (i) adversarially (with a model in the loop); or (ii) in the standard fashion (without a model). Across a variety of models and datasets, we find that models trained on adversarial data usually perform better on other adversarial datasets but worse on a diverse collection of out-of-domain evaluation sets. Finally, we provide a qualitative analysis of adversarial (vs standard) data, identifying key differences and offering guidance for future research",
    "checked": true,
    "id": "c5e4eafd85949e6aac9d8e98d5e03b2acf444046",
    "semantic_title": "",
    "citation_count": 26,
    "authors": [
      "Divyansh Kaushik",
      "Douwe Kiela",
      "Zachary C. Lipton",
      "Wen-tau Yih"
    ]
  },
  "https://aclanthology.org/2021.acl-long.518": {
    "title": "Learning Dense Representations of Phrases at Scale",
    "volume": "long",
    "abstract": "Open-domain question answering can be reformulated as a phrase retrieval problem, without the need for processing documents on-demand during inference (Seo et al., 2019). However, current phrase retrieval models heavily depend on sparse representations and still underperform retriever-reader approaches. In this work, we show for the first time that we can learn dense representations of phrases alone that achieve much stronger performance in open-domain QA. We present an effective method to learn phrase representations from the supervision of reading comprehension tasks, coupled with novel negative sampling methods. We also propose a query-side fine-tuning strategy, which can support transfer learning and reduce the discrepancy between training and inference. On five popular open-domain QA datasets, our model DensePhrases improves over previous phrase retrieval models by 15%-25% absolute accuracy and matches the performance of state-of-the-art retriever-reader models. Our model is easy to parallelize due to pure dense representations and processes more than 10 questions per second on CPUs. Finally, we directly use our pre-indexed dense phrase representations for two slot filling tasks, showing the promise of utilizing DensePhrases as a dense knowledge base for downstream tasks",
    "checked": true,
    "id": "1283ca87e6215b7393eba1653a4a2e4bf28d2868",
    "semantic_title": "",
    "citation_count": 66,
    "authors": [
      "Jinhyuk Lee",
      "Mujeen Sung",
      "Jaewoo Kang",
      "Danqi Chen"
    ]
  },
  "https://aclanthology.org/2021.acl-long.519": {
    "title": "End-to-End Training of Neural Retrievers for Open-Domain Question Answering",
    "volume": "long",
    "abstract": "Recent work on training neural retrievers for open-domain question answering (OpenQA) has employed both supervised and unsupervised approaches. However, it remains unclear how unsupervised and supervised methods can be used most effectively for neural retrievers. In this work, we systematically study retriever pre-training. We first propose an approach of unsupervised pre-training with the Inverse Cloze Task and masked salient spans, followed by supervised finetuning using question-context pairs. This approach leads to absolute gains of 2+ points over the previous best result in the top-20 retrieval accuracy on Natural Questions and TriviaQA datasets. We next explore two approaches for end-to-end training of the reader and retriever components in OpenQA models, which differ in the manner the reader ingests the retrieved documents. Our experiments demonstrate the effectiveness of these approaches as we obtain state-of-the-art results. On the Natural Questions dataset, we obtain a top-20 retrieval accuracy of 84%, an improvement of 5 points over the recent DPR model. We also achieve good results on answer extraction, outperforming recent models like REALM and RAG by 3+ points",
    "checked": true,
    "id": "15df0e2c602ae8ccedcf50accea080c4ba76f8ba",
    "semantic_title": "",
    "citation_count": 54,
    "authors": [
      "Devendra Sachan",
      "Mostofa Patwary",
      "Mohammad Shoeybi",
      "Neel Kant",
      "Wei Ping",
      "William L. Hamilton",
      "Bryan Catanzaro"
    ]
  },
  "https://aclanthology.org/2021.acl-long.520": {
    "title": "Question Answering Over Temporal Knowledge Graphs",
    "volume": "long",
    "abstract": "Temporal Knowledge Graphs (Temporal KGs) extend regular Knowledge Graphs by providing temporal scopes (start and end times) on each edge in the KG. While Question Answering over KG (KGQA) has received some attention from the research community, QA over Temporal KGs (Temporal KGQA) is a relatively unexplored area. Lack of broad coverage datasets has been another factor limiting progress in this area. We address this challenge by presenting CRONQUESTIONS, the largest known Temporal KGQA dataset, clearly stratified into buckets of structural complexity. CRONQUESTIONS expands the only known previous dataset by a factor of 340x. We find that various state-of-the-art KGQA methods fall far short of the desired performance on this new dataset. In response, we also propose CRONKGQA, a transformer-based solution that exploits recent advances in Temporal KG embeddings, and achieves performance superior to all baselines, with an increase of 120% in accuracy over the next best performing method. Through extensive experiments, we give detailed insights into the workings of CRONKGQA, as well as situations where significant further improvements appear possible. In addition to the dataset, we have released our code as well",
    "checked": true,
    "id": "47fe46e561e3b270407b7bffa176e35291f165a7",
    "semantic_title": "",
    "citation_count": 43,
    "authors": [
      "Apoorv Saxena",
      "Soumen Chakrabarti",
      "Partha Talukdar"
    ]
  },
  "https://aclanthology.org/2021.acl-long.521": {
    "title": "Language Model Augmented Relevance Score",
    "volume": "long",
    "abstract": "Although automated metrics are commonly used to evaluate NLG systems, they often correlate poorly with human judgements. Newer metrics such as BERTScore have addressed many weaknesses in prior metrics such as BLEU and ROUGE, which rely on n-gram matching. These newer methods, however, are still limited in that they do not consider the generation context, so they cannot properly reward generated text that is correct but deviates from the given reference. In this paper, we propose Language Model Augmented Relevance Score (MARS), a new context-aware metric for NLG evaluation. MARS leverages off-the-shelf language models, guided by reinforcement learning, to create augmented references that consider both the generation context and available human references, which are then used as additional references to score generated text. Compared with seven existing metrics in three common NLG tasks, MARS not only achieves higher correlation with human reference judgements, but also differentiates well-formed candidates from adversarial samples to a larger degree",
    "checked": true,
    "id": "ea45423c8f44025a276f6e88aafffbf87a898c89",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Ruibo Liu",
      "Jason Wei",
      "Soroush Vosoughi"
    ]
  },
  "https://aclanthology.org/2021.acl-long.522": {
    "title": "DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts",
    "volume": "long",
    "abstract": "Despite recent advances in natural language generation, it remains challenging to control attributes of generated text. We propose DExperts: Decoding-time Experts, a decoding-time method for controlled text generation that combines a pretrained language model with “expert” LMs and/or “anti-expert” LMs in a product of experts. Intuitively, under the ensemble, tokens only get high probability if they are considered likely by the experts, and unlikely by the anti-experts. We apply DExperts to language detoxification and sentiment-controlled generation, where we outperform existing controllable generation methods on both automatic and human evaluations. Moreover, because DExperts operates only on the output of the pretrained LM, it is effective with (anti-)experts of smaller size, including when operating on GPT-3. Our work highlights the promise of tuning small LMs on text with (un)desirable attributes for efficient decoding-time steering",
    "checked": true,
    "id": "02f033482b8045c687316ef81ba7aaae9f0a2e1c",
    "semantic_title": "",
    "citation_count": 87,
    "authors": [
      "Alisa Liu",
      "Maarten Sap",
      "Ximing Lu",
      "Swabha Swayamdipta",
      "Chandra Bhagavatula",
      "Noah A. Smith",
      "Yejin Choi"
    ]
  },
  "https://aclanthology.org/2021.acl-long.523": {
    "title": "Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and Improving Models",
    "volume": "long",
    "abstract": "While counterfactual examples are useful for analysis and training of NLP models, current generation methods either rely on manual labor to create very few counterfactuals, or only instantiate limited types of perturbations such as paraphrases or word substitutions. We present Polyjuice, a general-purpose counterfactual generator that allows for control over perturbation types and locations, trained by finetuning GPT-2 on multiple datasets of paired sentences. We show that Polyjuice produces diverse sets of realistic counterfactuals, which in turn are useful in various distinct applications: improving training and evaluation on three different tasks (with around 70% less annotation effort than manual generation), augmenting state-of-the-art explanation techniques, and supporting systematic counterfactual error analysis by revealing behaviors easily missed by human experts",
    "checked": true,
    "id": "15e71e497a67423bfedd0d63efe423c4660e5053",
    "semantic_title": "",
    "citation_count": 100,
    "authors": [
      "Tongshuang Wu",
      "Marco Tulio Ribeiro",
      "Jeffrey Heer",
      "Daniel Weld"
    ]
  },
  "https://aclanthology.org/2021.acl-long.524": {
    "title": "Metaphor Generation with Conceptual Mappings",
    "volume": "long",
    "abstract": "Generating metaphors is a difficult task as it requires understanding nuanced relationships between abstract concepts. In this paper, we aim to generate a metaphoric sentence given a literal expression by replacing relevant verbs. Guided by conceptual metaphor theory, we propose to control the generation process by encoding conceptual mappings between cognitive domains to generate meaningful metaphoric expressions. To achieve this, we develop two methods: 1) using FrameNet-based embeddings to learn mappings between domains and applying them at the lexical level (CM-Lex), and 2) deriving source/target pairs to train a controlled seq-to-seq generation model (CM-BART). We assess our methods through automatic and human evaluation for basic metaphoricity and conceptual metaphor presence. We show that the unsupervised CM-Lex model is competitive with recent deep learning metaphor generation systems, and CM-BART outperforms all other models both in automatic and human evaluations",
    "checked": true,
    "id": "432da56e21e0d4c85f7ea58159f363732c4ecdd4",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Kevin Stowe",
      "Tuhin Chakrabarty",
      "Nanyun Peng",
      "Smaranda Muresan",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2021.acl-long.525": {
    "title": "Learning Latent Structures for Cross Action Phrase Relations in Wet Lab Protocols",
    "volume": "long",
    "abstract": "Wet laboratory protocols (WLPs) are critical for conveying reproducible procedures in biological research. They are composed of instructions written in natural language describing the step-wise processing of materials by specific actions. This process flow description for reagents and materials synthesis in WLPs can be captured by material state transfer graphs (MSTGs), which encode global temporal and causal relationships between actions. Here, we propose methods to automatically generate a MSTG for a given protocol by extracting all action relationships across multiple sentences. We also note that previous corpora and methods focused primarily on local intra-sentence relationships between actions and entities and did not address two critical issues: (i) resolution of implicit arguments and (ii) establishing long-range dependencies across sentences. We propose a new model that incrementally learns latent structures and is better suited to resolving inter-sentence relations and implicit arguments. This model draws upon a new corpus WLP-MSTG which was created by extending annotations in the WLP corpora for inter-sentence relations and implicit arguments. Our model achieves an F1 score of 54.53% for temporal and causal relations in protocols from our corpus, which is a significant improvement over previous models - DyGIE++:28.17%; spERT:27.81%. We make our annotated WLP-MSTG corpus available to the research community",
    "checked": true,
    "id": "bd235d6ea116eec1fdb8a5410b61c4a92e19727f",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Chaitanya Kulkarni",
      "Jany Chan",
      "Eric Fosler-Lussier",
      "Raghu Machiraju"
    ]
  },
  "https://aclanthology.org/2021.acl-long.526": {
    "title": "Multimodal Multi-Speaker Merger & Acquisition Financial Modeling: A New Task, Dataset, and Neural Baselines",
    "volume": "long",
    "abstract": "Risk prediction is an essential task in financial markets. Merger and Acquisition (M&A) calls provide key insights into the claims made by company executives about the restructuring of the financial firms. Extracting vocal and textual cues from M&A calls can help model the risk associated with such financial activities. To aid the analysis of M&A calls, we curate a dataset of conference call transcripts and their corresponding audio recordings for the time period ranging from 2016 to 2020. We introduce M3ANet, a baseline architecture that takes advantage of the multimodal multi-speaker input to forecast the financial risk associated with the M&A calls. Empirical results prove that the task is challenging, with the pro-posed architecture performing marginally better than strong BERT-based baselines. We release the M3A dataset and benchmark models to motivate future research on this challenging problem domain",
    "checked": true,
    "id": "15322b1ed9e8c062b9077a2bc48cd965f8d41d72",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Ramit Sawhney",
      "Mihir Goyal",
      "Prakhar Goel",
      "Puneet Mathur",
      "Rajiv Ratn Shah"
    ]
  },
  "https://aclanthology.org/2021.acl-long.527": {
    "title": "Mid-Air Hand Gestures for Post-Editing of Machine Translation",
    "volume": "long",
    "abstract": "To translate large volumes of text in a globally connected world, more and more translators are integrating machine translation (MT) and post-editing (PE) into their translation workflows to generate publishable quality translations. While this process has been shown to save time and reduce errors, the task of translation is changing from mostly text production from scratch to fixing errors within useful but partly incorrect MT output. This is affecting the interface design of translation tools, where better support for text editing tasks is required. Here, we present the first study that investigates the usefulness of mid-air hand gestures in combination with the keyboard (GK) for text editing in PE of MT. Guided by a gesture elicitation study with 14 freelance translators, we develop a prototype supporting mid-air hand gestures for cursor placement, text selection, deletion, and reordering. These gestures combined with the keyboard facilitate all editing types required for PE. An evaluation of the prototype shows that the average editing duration of GK is only slightly slower than the standard mouse and keyboard (MK), even though participants are very familiar with the latter, and relative novices to the former. Furthermore, the qualitative analysis shows positive attitudes towards hand gestures for PE, especially when manipulating single words",
    "checked": true,
    "id": "a2844b687ad11315a6a8d7c528560c21d40d7260",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Rashad Albo Jamara",
      "Nico Herbig",
      "Antonio Krüger",
      "Josef van Genabith"
    ]
  },
  "https://aclanthology.org/2021.acl-long.528": {
    "title": "Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning",
    "volume": "long",
    "abstract": "Geometry problem solving has attracted much attention in the NLP community recently. The task is challenging as it requires abstract problem understanding and symbolic reasoning with axiomatic knowledge. However, current datasets are either small in scale or not publicly available. Thus, we construct a new large-scale benchmark, Geometry3K, consisting of 3,002 geometry problems with dense annotation in formal language. We further propose a novel geometry solving approach with formal language and symbolic reasoning, called Interpretable Geometry Problem Solver (Inter-GPS). Inter-GPS first parses the problem text and diagram into formal language automatically via rule-based text parsing and neural object detecting, respectively. Unlike implicit learning in existing methods, Inter-GPS incorporates theorem knowledge as conditional rules and performs symbolic reasoning step by step. Also, a theorem predictor is designed to infer the theorem application sequence fed to the symbolic solver for the more efficient and reasonable searching path. Extensive experiments on the Geometry3K and GEOS datasets demonstrate that Inter-GPS achieves significant improvements over existing methods. The project with code and data is available at https://lupantech.github.io/inter-gps",
    "checked": true,
    "id": "fb1c90806fc5ec72987f58110aa255edbce6620d",
    "semantic_title": "",
    "citation_count": 25,
    "authors": [
      "Pan Lu",
      "Ran Gong",
      "Shibiao Jiang",
      "Liang Qiu",
      "Siyuan Huang",
      "Xiaodan Liang",
      "Song-Chun Zhu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.529": {
    "title": "Joint Verification and Reranking for Open Fact Checking Over Tables",
    "volume": "long",
    "abstract": "Structured information is an important knowledge source for automatic verification of factual claims. Nevertheless, the majority of existing research into this task has focused on textual data, and the few recent inquiries into structured data have been for the closed-domain setting where appropriate evidence for each claim is assumed to have already been retrieved. In this paper, we investigate verification over structured data in the open-domain setting, introducing a joint reranking-and-verification model which fuses evidence documents in the verification component. Our open-domain model achieves performance comparable to the closed-domain state-of-the-art on the TabFact dataset, and demonstrates performance gains from the inclusion of multiple tables as well as a significant improvement over a heuristic retrieval baseline",
    "checked": true,
    "id": "3ce32d492aa14a2048d928957e9d15bcd92e8294",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Michael Sejr Schlichtkrull",
      "Vladimir Karpukhin",
      "Barlas Oguz",
      "Mike Lewis",
      "Wen-tau Yih",
      "Sebastian Riedel"
    ]
  },
  "https://aclanthology.org/2021.acl-long.530": {
    "title": "Evaluation of Thematic Coherence in Microblogs",
    "volume": "long",
    "abstract": "Collecting together microblogs representing opinions about the same topics within the same timeframe is useful to a number of different tasks and practitioners. A major question is how to evaluate the quality of such thematic clusters. Here we create a corpus of microblog clusters from three different domains and time windows and define the task of evaluating thematic coherence. We provide annotation guidelines and human annotations of thematic coherence by journalist experts. We subsequently investigate the efficacy of different automated evaluation metrics for the task. We consider a range of metrics including surface level metrics, ones for topic model coherence and text generation metrics (TGMs). While surface level metrics perform well, outperforming topic coherence metrics, they are not as consistent as TGMs. TGMs are more reliable than all other metrics considered for capturing thematic coherence in microblog clusters due to being less sensitive to the effect of time windows",
    "checked": true,
    "id": "99211c5cfe0da6e18330c38f64b2587bbbc4c389",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Iman Munire Bilal",
      "Bo Wang",
      "Maria Liakata",
      "Rob Procter",
      "Adam Tsakalidis"
    ]
  },
  "https://aclanthology.org/2021.acl-long.531": {
    "title": "Neural semi-Markov CRF for Monolingual Word Alignment",
    "volume": "long",
    "abstract": "Monolingual word alignment is important for studying fine-grained editing operations (i.e., deletion, addition, and substitution) in text-to-text generation tasks, such as paraphrase generation, text simplification, neutralizing biased language, etc. In this paper, we present a novel neural semi-Markov CRF alignment model, which unifies word and phrase alignments through variable-length spans. We also create a new benchmark with human annotations that cover four different text genres to evaluate monolingual word alignment models in more realistic settings. Experimental results show that our proposed model outperforms all previous approaches for monolingual word alignment as well as a competitive QA-based baseline, which was previously only applied to bilingual data. Our model demonstrates good generalizability to three out-of-domain datasets and shows great utility in two downstream applications: automatic text simplification and sentence pair classification tasks",
    "checked": true,
    "id": "c552c158776ad85814ef4fbca5b040d443573dda",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Wuwei Lan",
      "Chao Jiang",
      "Wei Xu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.532": {
    "title": "Privacy at Scale: Introducing the PrivaSeer Corpus of Web Privacy Policies",
    "volume": "long",
    "abstract": "Organisations disclose their privacy practices by posting privacy policies on their websites. Even though internet users often care about their digital privacy, they usually do not read privacy policies, since understanding them requires a significant investment of time and effort. Natural language processing has been used to create experimental tools to interpret privacy policies, but there has been a lack of large privacy policy corpora to facilitate the creation of large-scale semi-supervised and unsupervised models to interpret and simplify privacy policies. Thus, we present the PrivaSeer Corpus of 1,005,380 English language website privacy policies collected from the web. The number of unique websites represented in PrivaSeer is about ten times larger than the next largest public collection of web privacy policies, and it surpasses the aggregate of unique websites represented in all other publicly available privacy policy corpora combined. We describe a corpus creation pipeline with stages that include a web crawler, language detection, document classification, duplicate and near-duplicate removal, and content extraction. We employ an unsupervised topic modelling approach to investigate the contents of policy documents in the corpus and discuss the distribution of topics in privacy policies at web scale. We further investigate the relationship between privacy policy domain PageRanks and text features of the privacy policies. Finally, we use the corpus to pretrain PrivBERT, a transformer-based privacy policy language model, and obtain state of the art results on the data practice classification and question answering tasks",
    "checked": true,
    "id": "ebeed3d81649ab67e6220d6db0c2c361cbc20784",
    "semantic_title": "",
    "citation_count": 22,
    "authors": [
      "Mukund Srinath",
      "Shomir Wilson",
      "C Lee Giles"
    ]
  },
  "https://aclanthology.org/2021.acl-long.533": {
    "title": "The statistical advantage of automatic NLG metrics at the system level",
    "volume": "long",
    "abstract": "Estimating the expected output quality of generation systems is central to NLG. This paper qualifies the notion that automatic metrics are not as good as humans in estimating system-level quality. Statistically, humans are unbiased, high variance estimators, while metrics are biased, low variance estimators. We compare these estimators by their error in pairwise prediction (which generation system is better?) using the bootstrap. Measuring this error is complicated: predictions are evaluated against noisy, human predicted labels instead of the ground truth, and metric predictions fluctuate based on the test sets they were calculated on. By applying a bias-variance-noise decomposition, we adjust this error to a noise-free, infinite test set setting. Our analysis compares the adjusted error of metrics to humans and a derived, perfect segment-level annotator, both of which are unbiased estimators dependent on the number of judgments collected. In MT, we identify two settings where metrics outperform humans due to a statistical advantage in variance: when the number of human judgments used is small, and when the quality difference between compared systems is small",
    "checked": true,
    "id": "1cedc9f229368ef87c37b76ad5a686cd7c180610",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Johnny Wei",
      "Robin Jia"
    ]
  },
  "https://aclanthology.org/2021.acl-long.534": {
    "title": "Are Missing Links Predictable? An Inferential Benchmark for Knowledge Graph Completion",
    "volume": "long",
    "abstract": "We present InferWiki, a Knowledge Graph Completion (KGC) dataset that improves upon existing benchmarks in inferential ability, assumptions, and patterns. First, each testing sample is predictable with supportive data in the training set. To ensure it, we propose to utilize rule-guided train/test generation, instead of conventional random split. Second, InferWiki initiates the evaluation following the open-world assumption and improves the inferential difficulty of the closed-world assumption, by providing manually annotated negative and unknown triples. Third, we include various inference patterns (e.g., reasoning path length and types) for comprehensive evaluation. In experiments, we curate two settings of InferWiki varying in sizes and structures, and apply the construction process on CoDEx as comparative datasets. The results and empirical analyses demonstrate the necessity and high-quality of InferWiki. Nevertheless, the performance gap among various inferential assumptions and patterns presents the difficulty and inspires future research direction. Our datasets can be found in https://github.com/TaoMiner/inferwiki",
    "checked": true,
    "id": "18979d2dfa2c15a8930c494517407c7c7b6e3526",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Yixin Cao",
      "Xiang Ji",
      "Xin Lv",
      "Juanzi Li",
      "Yonggang Wen",
      "Hanwang Zhang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.535": {
    "title": "ConvoSumm: Conversation Summarization Benchmark and Improved Abstractive Summarization with Argument Mining",
    "volume": "long",
    "abstract": "While online conversations can cover a vast amount of information in many different formats, abstractive text summarization has primarily focused on modeling solely news articles. This research gap is due, in part, to the lack of standardized datasets for summarizing online discussions. To address this gap, we design annotation protocols motivated by an issues–viewpoints–assertions framework to crowdsource four new datasets on diverse online conversation forms of news comments, discussion forums, community question answering forums, and email threads. We benchmark state-of-the-art models on our datasets and analyze characteristics associated with the data. To create a comprehensive benchmark, we also evaluate these models on widely-used conversation summarization datasets to establish strong baselines in this domain. Furthermore, we incorporate argument mining through graph construction to directly model the issues, viewpoints, and assertions present in a conversation and filter noisy input, showing comparable or improved results according to automatic and human evaluations",
    "checked": true,
    "id": "fa51076458b7bcf9a60f476d525755e47199a6d8",
    "semantic_title": "",
    "citation_count": 29,
    "authors": [
      "Alexander Fabbri",
      "Faiaz Rahman",
      "Imad Rizvi",
      "Borui Wang",
      "Haoran Li",
      "Yashar Mehdad",
      "Dragomir Radev"
    ]
  },
  "https://aclanthology.org/2021.acl-long.536": {
    "title": "Improving Factual Consistency of Abstractive Summarization via Question Answering",
    "volume": "long",
    "abstract": "A commonly observed problem with the state-of-the art abstractive summarization models is that the generated summaries can be factually inconsistent with the input documents. The fact that automatic summarization may produce plausible-sounding yet inaccurate summaries is a major concern that limits its wide application. In this paper we present an approach to address factual consistency in summarization. We first propose an efficient automatic evaluation metric to measure factual consistency; next, we propose a novel learning algorithm that maximizes the proposed metric during model training. Through extensive experiments, we confirm that our method is effective in improving factual consistency and even overall quality of the summaries, as judged by both automatic metrics and human evaluation",
    "checked": true,
    "id": "73cb36eb79e8d21a0e154e7984e4fe8a47cc0be9",
    "semantic_title": "",
    "citation_count": 52,
    "authors": [
      "Feng Nan",
      "Cicero Nogueira dos Santos",
      "Henghui Zhu",
      "Patrick Ng",
      "Kathleen McKeown",
      "Ramesh Nallapati",
      "Dejiao Zhang",
      "Zhiguo Wang",
      "Andrew O. Arnold",
      "Bing Xiang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.537": {
    "title": "EmailSum: Abstractive Email Thread Summarization",
    "volume": "long",
    "abstract": "Recent years have brought about an interest in the challenging task of summarizing conversation threads (meetings, online discussions, etc.). Such summaries help analysis of the long text to quickly catch up with the decisions made and thus improve our work or communication efficiency. To spur research in thread summarization, we have developed an abstractive Email Thread Summarization (EmailSum) dataset, which contains human-annotated short (<30 words) and long (<100 words) summaries of 2,549 email threads (each containing 3 to 10 emails) over a wide variety of topics. We perform a comprehensive empirical study to explore different summarization techniques (including extractive and abstractive methods, single-document and hierarchical models, as well as transfer and semisupervised learning) and conduct human evaluations on both short and long summary generation tasks. Our results reveal the key challenges of current abstractive summarization models in this task, such as understanding the sender’s intent and identifying the roles of sender and receiver. Furthermore, we find that widely used automatic evaluation metrics (ROUGE, BERTScore) are weakly correlated with human judgments on this email thread summarization task. Hence, we emphasize the importance of human evaluation and the development of better metrics by the community",
    "checked": true,
    "id": "f03afeef6f36da2f5b6531b65c996a7b8ea521dd",
    "semantic_title": "",
    "citation_count": 18,
    "authors": [
      "Shiyue Zhang",
      "Asli Celikyilmaz",
      "Jianfeng Gao",
      "Mohit Bansal"
    ]
  },
  "https://aclanthology.org/2021.acl-long.538": {
    "title": "Cross-Lingual Abstractive Summarization with Limited Parallel Resources",
    "volume": "long",
    "abstract": "Parallel cross-lingual summarization data is scarce, requiring models to better use the limited available cross-lingual resources. Existing methods to do so often adopt sequence-to-sequence networks with multi-task frameworks. Such approaches apply multiple decoders, each of which is utilized for a specific task. However, these independent decoders share no parameters, hence fail to capture the relationships between the discrete phrases of summaries in different languages, breaking the connections in order to transfer the knowledge of the high-resource languages to low-resource languages. To bridge these connections, we propose a novel Multi-Task framework for Cross-Lingual Abstractive Summarization (MCLAS) in a low-resource setting. Employing one unified decoder to generate the sequential concatenation of monolingual and cross-lingual summaries, MCLAS makes the monolingual summarization task a prerequisite of the CLS task. In this way, the shared decoder learns interactions involving alignments and summary patterns across languages, which encourages attaining knowledge transfer. Experiments on two CLS datasets demonstrate that our model significantly outperforms three baseline models in both low-resource and full-dataset scenarios. Moreover, in-depth analysis on the generated summaries and attention heads verifies that interactions are learned well using MCLAS, which benefits the CLS task under limited parallel resources",
    "checked": true,
    "id": "146b054c3389213862eb00dde623ca7edba888e1",
    "semantic_title": "",
    "citation_count": 28,
    "authors": [
      "Yu Bai",
      "Yang Gao",
      "Heyan Huang"
    ]
  },
  "https://aclanthology.org/2021.acl-long.539": {
    "title": "Dissecting Generation Modes for Abstractive Summarization Models via Ablation and Attribution",
    "volume": "long",
    "abstract": "Despite the prominence of neural abstractive summarization models, we know little about how they actually form summaries and how to understand where their decisions come from. We propose a two-step method to interpret summarization model decisions. We first analyze the model’s behavior by ablating the full model to categorize each decoder decision into one of several generation modes: roughly, is the model behaving like a language model, is it relying heavily on the input, or is it somewhere in between? After isolating decisions that do depend on the input, we explore interpreting these decisions using several different attribution methods. We compare these techniques based on their ability to select content and reconstruct the model’s predicted token from perturbations of the input, thus revealing whether highlighted attributions are truly important for the generation of the next token. While this machinery can be broadly useful even beyond summarization, we specifically demonstrate its capability to identify phrases the summarization model has memorized and determine where in the training pipeline this memorization happened, as well as study complex generation phenomena like sentence fusion on a per-instance basis",
    "checked": true,
    "id": "143310c074eb09d5e60adea4c42250dbe03bf9f2",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Jiacheng Xu",
      "Greg Durrett"
    ]
  },
  "https://aclanthology.org/2021.acl-long.540": {
    "title": "Learning Prototypical Functions for Physical Artifacts",
    "volume": "long",
    "abstract": "Humans create things for a reason. Ancient people created spears for hunting, knives for cutting meat, pots for preparing food, etc. The prototypical function of a physical artifact is a kind of commonsense knowledge that we rely on to understand natural language. For example, if someone says “She borrowed the book” then you would assume that she intends to read the book, or if someone asks “Can I use your knife?” then you would assume that they need to cut something. In this paper, we introduce a new NLP task of learning the prototypical uses for human-made physical objects. We use frames from FrameNet to represent a set of common functions for objects, and describe a manually annotated data set of physical objects labeled with their prototypical function. We also present experimental results for this task, including BERT-based models that use predictions from masked patterns as well as artifact sense definitions from WordNet and frame definitions from FrameNet",
    "checked": true,
    "id": "8c748fbc290f7748cbf223a8c3674936808fc45d",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Tianyu Jiang",
      "Ellen Riloff"
    ]
  },
  "https://aclanthology.org/2021.acl-long.541": {
    "title": "Verb Knowledge Injection for Multilingual Event Processing",
    "volume": "long",
    "abstract": "Linguistic probing of pretrained Transformer-based language models (LMs) revealed that they encode a range of syntactic and semantic properties of a language. However, they are still prone to fall back on superficial cues and simple heuristics to solve downstream tasks, rather than leverage deeper linguistic information. In this paper, we target a specific facet of linguistic knowledge, the interplay between verb meaning and argument structure. We investigate whether injecting explicit information on verbs’ semantic-syntactic behaviour improves the performance of pretrained LMs in event extraction tasks, where accurate verb processing is paramount. Concretely, we impart the verb knowledge from curated lexical resources into dedicated adapter modules (verb adapters), allowing it to complement, in downstream tasks, the language knowledge obtained during LM-pretraining. We first demonstrate that injecting verb knowledge leads to performance gains in English event extraction. We then explore the utility of verb adapters for event extraction in other languages: we investigate 1) zero-shot language transfer with multilingual Transformers and 2) transfer via (noisy automatic) translation of English verb-based lexical knowledge. Our results show that the benefits of verb knowledge injection indeed extend to other languages, even when relying on noisily translated lexical knowledge",
    "checked": true,
    "id": "ce2e6a287e2b1898665035b1a734d3a559c0933c",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Olga Majewska",
      "Ivan Vulić",
      "Goran Glavaš",
      "Edoardo Maria Ponti",
      "Anna Korhonen"
    ]
  },
  "https://aclanthology.org/2021.acl-long.542": {
    "title": "Dynamic Contextualized Word Embeddings",
    "volume": "long",
    "abstract": "Static word embeddings that represent words by a single vector cannot capture the variability of word meaning in different linguistic and extralinguistic contexts. Building on prior work on contextualized and dynamic word embeddings, we introduce dynamic contextualized word embeddings that represent words as a function of both linguistic and extralinguistic context. Based on a pretrained language model (PLM), dynamic contextualized word embeddings model time and social space jointly, which makes them attractive for a range of NLP tasks involving semantic variability. We highlight potential application scenarios by means of qualitative and quantitative analyses on four English datasets",
    "checked": true,
    "id": "33661b3345cb29e37c85cc702d2a2a3b428e1445",
    "semantic_title": "",
    "citation_count": 24,
    "authors": [
      "Valentin Hofmann",
      "Janet Pierrehumbert",
      "Hinrich Schütze"
    ]
  },
  "https://aclanthology.org/2021.acl-long.543": {
    "title": "Lexical Semantic Change Discovery",
    "volume": "long",
    "abstract": "While there is a large amount of research in the field of Lexical Semantic Change Detection, only few approaches go beyond a standard benchmark evaluation of existing models. In this paper, we propose a shift of focus from change detection to change discovery, i.e., discovering novel word senses over time from the full corpus vocabulary. By heavily fine-tuning a type-based and a token-based approach on recently published German data, we demonstrate that both models can successfully be applied to discover new words undergoing meaning change. Furthermore, we provide an almost fully automated framework for both evaluation and discovery",
    "checked": true,
    "id": "3be4340615904d3b2452a5cba98d7f265b63a6f4",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Sinan Kurtyigit",
      "Maike Park",
      "Dominik Schlechtweg",
      "Jonas Kuhn",
      "Sabine Schulte im Walde"
    ]
  },
  "https://aclanthology.org/2021.acl-long.544": {
    "title": "The R-U-A-Robot Dataset: Helping Avoid Chatbot Deception by Detecting User Questions About Human or Non-Human Identity",
    "volume": "long",
    "abstract": "Humans are increasingly interacting with machines through language, sometimes in contexts where the user may not know they are talking to a machine (like over the phone or a text chatbot). We aim to understand how system designers and researchers might allow their systems to confirm its non-human identity. We collect over 2,500 phrasings related to the intent of “Are you a robot?”. This is paired with over 2,500 adversarially selected utterances where only confirming the system is non-human would be insufficient or disfluent. We compare classifiers to recognize the intent and discuss the precision/recall and model complexity tradeoffs. Such classifiers could be integrated into dialog systems to avoid undesired deception. We then explore how both a generative research model (Blender) as well as two deployed systems (Amazon Alexa, Google Assistant) handle this intent, finding that systems often fail to confirm their non-human identity. Finally, we try to understand what a good response to the intent would be, and conduct a user study to compare the important aspects when responding to this intent",
    "checked": true,
    "id": "4f9ced6a7a7b19e8607fc9028e8a8d55f89e9a52",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "David Gros",
      "Yu Li",
      "Zhou Yu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.545": {
    "title": "Using Meta-Knowledge Mined from Identifiers to Improve Intent Recognition in Conversational Systems",
    "volume": "long",
    "abstract": "In this paper we explore the improvement of intent recognition in conversational systems by the use of meta-knowledge embedded in intent identifiers. Developers often include such knowledge, structure as taxonomies, in the documentation of chatbots. By using neuro-symbolic algorithms to incorporate those taxonomies into embeddings of the output space, we were able to improve accuracy in intent recognition. In datasets with intents and example utterances from 200 professional chatbots, we saw decreases in the equal error rate (EER) in more than 40% of the chatbots in comparison to the baseline of the same algorithm without the meta-knowledge. The meta-knowledge proved also to be effective in detecting out-of-scope utterances, improving the false acceptance rate (FAR) in two thirds of the chatbots, with decreases of 0.05 or more in FAR in almost 40% of the chatbots. When considering only the well-developed workspaces with a high level use of taxonomies, FAR decreased more than 0.05 in 77% of them, and more than 0.1 in 39% of the chatbots",
    "checked": true,
    "id": "72a281af1054753d9db50f585139300abd7b515d",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Claudio Pinhanez",
      "Paulo Cavalin",
      "Victor Henrique Alves Ribeiro",
      "Ana Appel",
      "Heloisa Candello",
      "Julio Nogima",
      "Mauro Pichiliani",
      "Melina Guerra",
      "Maira de Bayser",
      "Gabriel Malfatti",
      "Henrique Ferreira"
    ]
  },
  "https://aclanthology.org/2021.acl-long.546": {
    "title": "Space Efficient Context Encoding for Non-Task-Oriented Dialogue Generation with Graph Attention Transformer",
    "volume": "long",
    "abstract": "To improve the coherence and knowledge retrieval capabilities of non-task-oriented dialogue systems, recent Transformer-based models aim to integrate fixed background context. This often comes in the form of knowledge graphs, and the integration is done by creating pseudo utterances through paraphrasing knowledge triples, added into the accumulated dialogue context. However, the context length is fixed in these architectures, which restricts how much background or dialogue context can be kept. In this work, we propose a more concise encoding for background context structured in the form of knowledge graphs, by expressing the graph connections through restrictions on the attention weights. The results of our human evaluation show that this encoding reduces space requirements without negative effects on the precision of reproduction of knowledge and perceived consistency. Further, models trained with our proposed context encoding generate dialogues that are judged to be more comprehensive and interesting",
    "checked": true,
    "id": "14312b5c345a0c7c210773436579500c507852e3",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Fabian Galetzka",
      "Jewgeni Rose",
      "David Schlangen",
      "Jens Lehmann"
    ]
  },
  "https://aclanthology.org/2021.acl-long.547": {
    "title": "DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in Conversations",
    "volume": "long",
    "abstract": "Emotion Recognition in Conversations (ERC) has gained increasing attention for developing empathetic machines. Recently, many approaches have been devoted to perceiving conversational context by deep learning models. However, these approaches are insufficient in understanding the context due to lacking the ability to extract and integrate emotional clues. In this work, we propose novel Contextual Reasoning Networks (DialogueCRN) to fully understand the conversational context from a cognitive perspective. Inspired by the Cognitive Theory of Emotion, we design multi-turn reasoning modules to extract and integrate emotional clues. The reasoning module iteratively performs an intuitive retrieving process and a conscious reasoning process, which imitates human unique cognitive thinking. Extensive experiments on three public benchmark datasets demonstrate the effectiveness and superiority of the proposed model",
    "checked": true,
    "id": "72b2f7b5ebde06779b9842908fbae902cc4b9674",
    "semantic_title": "",
    "citation_count": 46,
    "authors": [
      "Dou Hu",
      "Lingwei Wei",
      "Xiaoyong Huai"
    ]
  },
  "https://aclanthology.org/2021.acl-long.548": {
    "title": "Cross-replication Reliability - An Empirical Approach to Interpreting Inter-rater Reliability",
    "volume": "long",
    "abstract": "When collecting annotations and labeled data from humans, a standard practice is to use inter-rater reliability (IRR) as a measure of data goodness (Hallgren, 2012). Metrics such as Krippendorff’s alpha or Cohen’s kappa are typically required to be above a threshold of 0.6 (Landis and Koch, 1977). These absolute thresholds are unreasonable for crowdsourced data from annotators with high cultural and training variances, especially on subjective topics. We present a new alternative to interpreting IRR that is more empirical and contextualized. It is based upon benchmarking IRR against baseline measures in a replication, one of which is a novel cross-replication reliability (xRR) measure based on Cohen’s (1960) kappa. We call this approach the xRR framework. We opensource a replication dataset of 4 million human judgements of facial expressions and analyze it with the proposed framework. We argue this framework can be used to measure the quality of crowdsourced datasets",
    "checked": true,
    "id": "3130fa914c733c9fffcf4c92d8dcaca93fadada6",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Ka Wong",
      "Praveen Paritosh",
      "Lora Aroyo"
    ]
  },
  "https://aclanthology.org/2021.acl-long.549": {
    "title": "TIMEDIAL: Temporal Commonsense Reasoning in Dialog",
    "volume": "long",
    "abstract": "Everyday conversations require understanding everyday events, which in turn, requires understanding temporal commonsense concepts interwoven with those events. Despite recent progress with massive pre-trained language models (LMs) such as T5 and GPT-3, their capability of temporal reasoning in dialogs remains largely under-explored. In this paper, we present the first study to investigate pre-trained LMs for their temporal reasoning capabilities in dialogs by introducing a new task and a crowd-sourced English challenge set, TimeDial. We formulate TimeDial as a multiple choice cloze task with over 1.1K carefully curated dialogs. Empirical results demonstrate that even the best performing models struggle on this task compared to humans, with 23 absolute points of gap in accuracy. Furthermore, our analysis reveals that the models fail to reason about dialog context correctly; instead, they rely on shallow cues based on existing temporal patterns in context, motivating future research for modeling temporal concepts in text and robust contextual reasoning about them. The dataset is publicly available at https://github.com/google-research-datasets/timedial",
    "checked": true,
    "id": "62953ca1252c9febe07c7007a10911726f37792d",
    "semantic_title": "",
    "citation_count": 28,
    "authors": [
      "Lianhui Qin",
      "Aditya Gupta",
      "Shyam Upadhyay",
      "Luheng He",
      "Yejin Choi",
      "Manaal Faruqui"
    ]
  },
  "https://aclanthology.org/2021.acl-long.550": {
    "title": "RAW-C: Relatedness of Ambiguous Words in Context (A New Lexical Resource for English)",
    "volume": "long",
    "abstract": "Most words are ambiguous—-i.e., they convey distinct meanings in different contexts—-and even the meanings of unambiguous words are context-dependent. Both phenomena present a challenge for NLP. Recently, the advent of contextualized word embeddings has led to success on tasks involving lexical ambiguity, such as Word Sense Disambiguation. However, there are few tasks that directly evaluate how well these contextualized embeddings accommodate the more continuous, dynamic nature of word meaning—-particularly in a way that matches human intuitions. We introduce RAW-C, a dataset of graded, human relatedness judgments for 112 ambiguous words in context (with 672 sentence pairs total), as well as human estimates of sense dominance. The average inter-annotator agreement (assessed using a leave-one-annotator-out method) was 0.79. We then show that a measure of cosine distance, computed using contextualized embeddings from BERT and ELMo, correlates with human judgments, but that cosine distance also systematically underestimates how similar humans find uses of the same sense of a word to be, and systematically overestimates how similar humans find uses of different-sense homonyms. Finally, we propose a synthesis between psycholinguistic theories of the mental lexicon and computational models of lexical semantics",
    "checked": true,
    "id": "55d760c0c9000378edc53ad75f77a72bfc50dc88",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Sean Trott",
      "Benjamin Bergen"
    ]
  },
  "https://aclanthology.org/2021.acl-long.551": {
    "title": "ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic",
    "volume": "long",
    "abstract": "Pre-trained language models (LMs) are currently integral to many natural language processing systems. Although multilingual LMs were also introduced to serve many languages, these have limitations such as being costly at inference time and the size and diversity of non-English data involved in their pre-training. We remedy these issues for a collection of diverse Arabic varieties by introducing two powerful deep bidirectional transformer-based models, ARBERT and MARBERT. To evaluate our models, we also introduce ARLUE, a new benchmark for multi-dialectal Arabic language understanding evaluation. ARLUE is built using 42 datasets targeting six different task clusters, allowing us to offer a series of standardized experiments under rich conditions. When fine-tuned on ARLUE, our models collectively achieve new state-of-the-art results across the majority of tasks (37 out of 48 classification tasks, on the 42 datasets). Our best model acquires the highest ARLUE score (77.40) across all six task clusters, outperforming all other models including XLM-R Large ( 3.4x larger size). Our models are publicly available at https://github.com/UBC-NLP/marbert and ARLUE will be released through the same repository",
    "checked": true,
    "id": "1e4cda8be54999ced1324777fa462a85e2c9746c",
    "semantic_title": "",
    "citation_count": 166,
    "authors": [
      "Muhammad Abdul-Mageed",
      "AbdelRahim Elmadany",
      "El Moatez Billah Nagoudi"
    ]
  },
  "https://aclanthology.org/2021.acl-long.552": {
    "title": "Improving Paraphrase Detection with the Adversarial Paraphrasing Task",
    "volume": "long",
    "abstract": "If two sentences have the same meaning, it should follow that they are equivalent in their inferential properties, i.e., each sentence should textually entail the other. However, many paraphrase datasets currently in widespread use rely on a sense of paraphrase based on word overlap and syntax. Can we teach them instead to identify paraphrases in a way that draws on the inferential properties of the sentences, and is not over-reliant on lexical and syntactic similarities of a sentence pair? We apply the adversarial paradigm to this question, and introduce a new adversarial method of dataset creation for paraphrase identification: the Adversarial Paraphrasing Task (APT), which asks participants to generate semantically equivalent (in the sense of mutually implicative) but lexically and syntactically disparate paraphrases. These sentence pairs can then be used both to test paraphrase identification models (which get barely random accuracy) and then improve their performance. To accelerate dataset generation, we explore automation of APT using T5, and show that the resulting dataset also improves accuracy. We discuss implications for paraphrase detection and release our dataset in the hope of making paraphrase detection models better able to detect sentence-level meaning equivalence",
    "checked": true,
    "id": "191635f4457f06193920baaceba57d4c462b452d",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Animesh Nighojkar",
      "John Licato"
    ]
  },
  "https://aclanthology.org/2021.acl-long.553": {
    "title": "ADEPT: An Adjective-Dependent Plausibility Task",
    "volume": "long",
    "abstract": "A false contract is more likely to be rejected than a contract is, yet a false key is less likely than a key to open doors. While correctly interpreting and assessing the effects of such adjective-noun pairs (e.g., false key) on the plausibility of given events (e.g., opening doors) underpins many natural language understanding tasks, doing so often requires a significant degree of world knowledge and common-sense reasoning. We introduce ADEPT – a large-scale semantic plausibility task consisting of over 16 thousand sentences that are paired with slightly modified versions obtained by adding an adjective to a noun. Overall, we find that while the task appears easier for human judges (85% accuracy), it proves more difficult for transformer-based models like RoBERTa (71% accuracy). Our experiments also show that neither the adjective itself nor its taxonomic class suffice in determining the correct plausibility judgement, emphasizing the importance of endowing automatic natural language understanding systems with more context sensitivity and common-sense reasoning",
    "checked": true,
    "id": "c8206c0450c6928614577899b92fa389365c423d",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Ali Emami",
      "Ian Porada",
      "Alexandra Olteanu",
      "Kaheer Suleman",
      "Adam Trischler",
      "Jackie Chi Kit Cheung"
    ]
  },
  "https://aclanthology.org/2021.acl-long.554": {
    "title": "ReadOnce Transformers: Reusable Representations of Text for Transformers",
    "volume": "long",
    "abstract": "We present ReadOnce Transformers, an approach to convert a transformer-based model into one that can build an information-capturing, task-independent, and compressed representation of text. The resulting representation is reusable across different examples and tasks, thereby requiring a document shared across many examples or tasks to only be read once. This leads to faster training and evaluation of models. Additionally, we extend standard text-to-text transformer models to Representation+Text-to-text models, and evaluate on multiple downstream tasks: multi-hop QA, abstractive QA, and long-document summarization. Our one-time computed representation results in a 2x-5x speedup compared to standard text-to-text models, while the compression also allows existing language models to handle longer documents without the need for designing new pre-trained models",
    "checked": true,
    "id": "207a8328ab05d3a23f4d2882e70819bb1ad91b10",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Shih-Ting Lin",
      "Ashish Sabharwal",
      "Tushar Khot"
    ]
  },
  "https://aclanthology.org/2021.acl-long.555": {
    "title": "Conditional Generation of Temporally-ordered Event Sequences",
    "volume": "long",
    "abstract": "Models of narrative schema knowledge have proven useful for a range of event-related tasks, but they typically do not capture the temporal relationships between events. We propose a single model that addresses both temporal ordering, sorting given events into the order they occurred, and event infilling, predicting new events which fit into an existing temporally-ordered sequence. We use a BART-based conditional generation model that can capture both temporality and common event co-occurrence, meaning it can be flexibly applied to different tasks in this space. Our model is trained as a denoising autoencoder: we take temporally-ordered event sequences, shuffle them, delete some events, and then attempt to recover the original event sequence. This task teaches the model to make inferences given incomplete knowledge about the events in an underlying scenario. On the temporal ordering task, we show that our model is able to unscramble event sequences from existing datasets without access to explicitly labeled temporal training data, outperforming both a BERT-based pairwise model and a BERT-based pointer network. On event infilling, human evaluation shows that our model is able to generate events that fit better temporally into the input events when compared to GPT-2 story completion models",
    "checked": true,
    "id": "33b06c74eea3f400b6f5ef14ef163aef1db42d16",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Shih-Ting Lin",
      "Nathanael Chambers",
      "Greg Durrett"
    ]
  },
  "https://aclanthology.org/2021.acl-long.556": {
    "title": "Hate Speech Detection Based on Sentiment Knowledge Sharing",
    "volume": "long",
    "abstract": "The wanton spread of hate speech on the internet brings great harm to society and families. It is urgent to establish and improve automatic detection and active avoidance mechanisms for hate speech. While there exist methods for hate speech detection, they stereotype words and hence suffer from inherently biased training. In other words, getting more affective features from other affective resources will significantly affect the performance of hate speech detection. In this paper, we propose a hate speech detection framework based on sentiment knowledge sharing. While extracting the affective features of the target sentence itself, we make better use of the sentiment features from external resources, and finally fuse features from different feature extraction units to detect hate speech. Experimental results on two public datasets demonstrate the effectiveness of our model",
    "checked": true,
    "id": "afaee3bd71a3ca31b883179270589e1588ee4eee",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Xianbing Zhou",
      "Yang Yong",
      "Xiaochao Fan",
      "Ge Ren",
      "Yunfeng Song",
      "Yufeng Diao",
      "Liang Yang",
      "Hongfei Lin"
    ]
  },
  "https://aclanthology.org/2021.acl-long.557": {
    "title": "Transition-based Bubble Parsing: Improvements on Coordination Structure Prediction",
    "volume": "long",
    "abstract": "We propose a transition-based bubble parser to perform coordination structure identification and dependency-based syntactic analysis simultaneously. Bubble representations were proposed in the formal linguistics literature decades ago; they enhance dependency trees by encoding coordination boundaries and internal relationships within coordination structures explicitly. In this paper, we introduce a transition system and neural models for parsing these bubble-enhanced structures. Experimental results on the English Penn Treebank and the English GENIA corpus show that our parsers beat previous state-of-the-art approaches on the task of coordination structure prediction, especially for the subset of sentences with complex coordination structures",
    "checked": true,
    "id": "a61e3e99cc10bc3a5b58e420cf66a9954d3cb43e",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianze Shi",
      "Lillian Lee"
    ]
  },
  "https://aclanthology.org/2021.acl-long.558": {
    "title": "SpanNER: Named Entity Re-/Recognition as Span Prediction",
    "volume": "long",
    "abstract": "Recent years have seen the paradigm shift of Named Entity Recognition (NER) systems from sequence labeling to span prediction. Despite its preliminary effectiveness, the span prediction model’s architectural bias has not been fully understood. In this paper, we first investigate the strengths and weaknesses when the span prediction model is used for named entity recognition compared with the sequence labeling framework and how to further improve it, which motivates us to make complementary advantages of systems based on different paradigms. We then reveal that span prediction, simultaneously, can serve as a system combiner to re-recognize named entities from different systems’ outputs. We experimentally implement 154 systems on 11 datasets, covering three languages, comprehensive results show the effectiveness of span prediction models that both serve as base NER systems and system combiners. We make all codes and datasets available: https://github.com/neulab/spanner, as well as an online system demo: http://spanner.sh. Our model also has been deployed into the ExplainaBoard platform, which allows users to flexibly perform a system combination of top-scoring systems in an interactive way: http://explainaboard.nlpedia.ai/leaderboard/task-ner/",
    "checked": true,
    "id": "6efec9c3b3bb05b71c58786ffab6d40c31bcae96",
    "semantic_title": "",
    "citation_count": 44,
    "authors": [
      "Jinlan Fu",
      "Xuanjing Huang",
      "Pengfei Liu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.559": {
    "title": "StructFormer: Joint Unsupervised Induction of Dependency and Constituency Structure from Masked Language Modeling",
    "volume": "long",
    "abstract": "There are two major classes of natural language grammars — the dependency grammar that models one-to-one correspondences between words and the constituency grammar that models the assembly of one or several corresponded words. While previous unsupervised parsing methods mostly focus on only inducing one class of grammars, we introduce a novel model, StructFormer, that can induce dependency and constituency structure at the same time. To achieve this, we propose a new parsing framework that can jointly generate a constituency tree and dependency graph. Then we integrate the induced dependency relations into the transformer, in a differentiable manner, through a novel dependency-constrained self-attention mechanism. Experimental results show that our model can achieve strong results on unsupervised constituency parsing, unsupervised dependency parsing, and masked language modeling at the same time",
    "checked": true,
    "id": "5042235979c7e91f0252e1d0bb182a55564a18f5",
    "semantic_title": "",
    "citation_count": 23,
    "authors": [
      "Yikang Shen",
      "Yi Tay",
      "Che Zheng",
      "Dara Bahri",
      "Donald Metzler",
      "Aaron Courville"
    ]
  },
  "https://aclanthology.org/2021.acl-long.560": {
    "title": "Language Embeddings for Typology and Cross-lingual Transfer Learning",
    "volume": "long",
    "abstract": "Cross-lingual language tasks typically require a substantial amount of annotated data or parallel translation data. We explore whether language representations that capture relationships among languages can be learned and subsequently leveraged in cross-lingual tasks without the use of parallel data. We generate dense embeddings for 29 languages using a denoising autoencoder, and evaluate the embeddings using the World Atlas of Language Structures (WALS) and two extrinsic tasks in a zero-shot setting: cross-lingual dependency parsing and cross-lingual natural language inference",
    "checked": true,
    "id": "ec844466feda60d664d5173ff011fea1c275f1fa",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Dian Yu",
      "Taiqi He",
      "Kenji Sagae"
    ]
  },
  "https://aclanthology.org/2021.acl-long.561": {
    "title": "Can Sequence-to-Sequence Models Crack Substitution Ciphers?",
    "volume": "long",
    "abstract": "Decipherment of historical ciphers is a challenging problem. The language of the target plaintext might be unknown, and ciphertext can have a lot of noise. State-of-the-art decipherment methods use beam search and a neural language model to score candidate plaintext hypotheses for a given cipher, assuming the plaintext language is known. We propose an end-to-end multilingual model for solving simple substitution ciphers. We test our model on synthetic and real historical ciphers and show that our proposed method can decipher text without explicit language identification while still being robust to noise",
    "checked": true,
    "id": "6ac0dcfd9302ed128a035dfe77ebb9e665c01185",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Nada Aldarrab",
      "Jonathan May"
    ]
  },
  "https://aclanthology.org/2021.acl-long.562": {
    "title": "Beyond Noise: Mitigating the Impact of Fine-grained Semantic Divergences on Neural Machine Translation",
    "volume": "long",
    "abstract": "While it has been shown that Neural Machine Translation (NMT) is highly sensitive to noisy parallel training samples, prior work treats all types of mismatches between source and target as noise. As a result, it remains unclear how samples that are mostly equivalent but contain a small number of semantically divergent tokens impact NMT training. To close this gap, we analyze the impact of different types of fine-grained semantic divergences on Transformer models. We show that models trained on synthetic divergences output degenerated text more frequently and are less confident in their predictions. Based on these findings, we introduce a divergent-aware NMT framework that uses factors to help NMT recover from the degradation caused by naturally occurring divergences, improving both translation quality and model calibration on EN-FR tasks",
    "checked": true,
    "id": "1d7e3bc217fe097ca39b366e98fbbe59fa6bff43",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Eleftheria Briakou",
      "Marine Carpuat"
    ]
  },
  "https://aclanthology.org/2021.acl-long.563": {
    "title": "Discriminative Reranking for Neural Machine Translation",
    "volume": "long",
    "abstract": "Reranking models enable the integration of rich features to select a better output hypothesis within an n-best list or lattice. These models have a long history in NLP, and we revisit discriminative reranking for modern neural machine translation models by training a large transformer architecture. This takes as input both the source sentence as well as a list of hypotheses to output a ranked list. The reranker is trained to predict the observed distribution of a desired metric, e.g. BLEU, over the n-best list. Since such a discriminator contains hundreds of millions of parameters, we improve its generalization using pre-training and data augmentation techniques. Experiments on four WMT directions show that our discriminative reranking approach is effective and complementary to existing generative reranking approaches, yielding improvements of up to 4 BLEU over the beam search output",
    "checked": true,
    "id": "c47cac224ff59892abfd6af316b0f9e082f97012",
    "semantic_title": "",
    "citation_count": 19,
    "authors": [
      "Ann Lee",
      "Michael Auli",
      "Marc’Aurelio Ranzato"
    ]
  },
  "https://aclanthology.org/2021.acl-long.564": {
    "title": "Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering",
    "volume": "long",
    "abstract": "Active learning promises to alleviate the massive data needs of supervised machine learning: it has successfully improved sample efficiency by an order of magnitude on traditional tasks like topic classification and object recognition. However, we uncover a striking contrast to this promise: across 5 models and 4 datasets on the task of visual question answering, a wide variety of active learning approaches fail to outperform random selection. To understand this discrepancy, we profile 8 active learning methods on a per-example basis, and identify the problem as collective outliers – groups of examples that active learning methods prefer to acquire but models fail to learn (e.g., questions that ask about text in images or require external knowledge). Through systematic ablation experiments and qualitative visualizations, we verify that collective outliers are a general phenomenon responsible for degrading pool-based active learning. Notably, we show that active learning sample efficiency increases significantly as the number of collective outliers in the active learning pool decreases. We conclude with a discussion and prescriptive recommendations for mitigating the effects of these outliers in future work",
    "checked": true,
    "id": "5441598e2b690a15198b7a38359e5936e4a46114",
    "semantic_title": "",
    "citation_count": 47,
    "authors": [
      "Siddharth Karamcheti",
      "Ranjay Krishna",
      "Li Fei-Fei",
      "Christopher Manning"
    ]
  },
  "https://aclanthology.org/2021.acl-long.565": {
    "title": "All That's ‘Human' Is Not Gold: Evaluating Human Evaluation of Generated Text",
    "volume": "long",
    "abstract": "Human evaluations are typically considered the gold standard in natural language generation, but as models’ fluency improves, how well can evaluators detect and judge machine-generated text? We run a study assessing non-experts’ ability to distinguish between human- and machine-authored text (GPT2 and GPT3) in three domains (stories, news articles, and recipes). We find that, without training, evaluators distinguished between GPT3- and human-authored text at random chance level. We explore three approaches for quickly training evaluators to better identify GPT3-authored text (detailed instructions, annotated examples, and paired examples) and find that while evaluators’ accuracy improved up to 55%, it did not significantly improve across the three domains. Given the inconsistent results across text domains and the often contradictory reasons evaluators gave for their judgments, we examine the role untrained human evaluations play in NLG evaluation and provide recommendations to NLG researchers for improving human evaluations of text generated from state-of-the-art models",
    "checked": true,
    "id": "a16ae67070de155789a871cb27ecbf9eaa98b379",
    "semantic_title": "",
    "citation_count": 114,
    "authors": [
      "Elizabeth Clark",
      "Tal August",
      "Sofia Serrano",
      "Nikita Haduong",
      "Suchin Gururangan",
      "Noah A. Smith"
    ]
  },
  "https://aclanthology.org/2021.acl-long.566": {
    "title": "Scientific Credibility of Machine Translation Research: A Meta-Evaluation of 769 Papers",
    "volume": "long",
    "abstract": "This paper presents the first large-scale meta-evaluation of machine translation (MT). We annotated MT evaluations conducted in 769 research papers published from 2010 to 2020. Our study shows that practices for automatic MT evaluation have dramatically changed during the past decade and follow concerning trends. An increasing number of MT evaluations exclusively rely on differences between BLEU scores to draw conclusions, without performing any kind of statistical significance testing nor human evaluation, while at least 108 metrics claiming to be better than BLEU have been proposed. MT evaluations in recent papers tend to copy and compare automatic metric scores from previous work to claim the superiority of a method or an algorithm without confirming neither exactly the same training, validating, and testing data have been used nor the metric scores are comparable. Furthermore, tools for reporting standardized metric scores are still far from being widely adopted by the MT community. After showing how the accumulation of these pitfalls leads to dubious evaluation, we propose a guideline to encourage better automatic MT evaluation along with a simple meta-evaluation scoring method to assess its credibility",
    "checked": true,
    "id": "e399e78f2c236802aa50aef95554a3768079edb1",
    "semantic_title": "",
    "citation_count": 61,
    "authors": [
      "Benjamin Marie",
      "Atsushi Fujita",
      "Raphael Rubino"
    ]
  },
  "https://aclanthology.org/2021.acl-long.567": {
    "title": "Neural Machine Translation with Monolingual Translation Memory",
    "volume": "long",
    "abstract": "Prior work has proved that Translation Memory (TM) can boost the performance of Neural Machine Translation (NMT). In contrast to existing work that uses bilingual corpus as TM and employs source-side similarity search for memory retrieval, we propose a new framework that uses monolingual memory and performs learnable memory retrieval in a cross-lingual manner. Our framework has unique advantages. First, the cross-lingual memory retriever allows abundant monolingual data to be TM. Second, the memory retriever and NMT model can be jointly optimized for the ultimate translation goal. Experiments show that the proposed method obtains substantial improvements. Remarkably, it even outperforms strong TM-augmented NMT baselines using bilingual TM. Owning to the ability to leverage monolingual data, our model also demonstrates effectiveness in low-resource and domain adaptation scenarios",
    "checked": true,
    "id": "73ee65c312e64d7c5e581d55082d61c0f26707fb",
    "semantic_title": "",
    "citation_count": 43,
    "authors": [
      "Deng Cai",
      "Yan Wang",
      "Huayang Li",
      "Wai Lam",
      "Lemao Liu"
    ]
  },
  "https://aclanthology.org/2021.acl-long.568": {
    "title": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning",
    "volume": "long",
    "abstract": "Although pretrained language models can be fine-tuned to produce state-of-the-art results for a very wide range of language understanding tasks, the dynamics of this process are not well understood, especially in the low data regime. Why can we use relatively vanilla gradient descent algorithms (e.g., without strong regularization) to tune a model with hundreds of millions of parameters on datasets with only hundreds or thousands of labeled examples? In this paper, we argue that analyzing fine-tuning through the lens of intrinsic dimension provides us with empirical and theoretical intuitions to explain this remarkable phenomenon. We empirically show that common pre-trained models have a very low intrinsic dimension; in other words, there exists a low dimension reparameterization that is as effective for fine-tuning as the full parameter space. For example, by optimizing only 200 trainable parameters randomly projected back into the full space, we can tune a RoBERTa model to achieve 90% of the full parameter performance levels on MRPC. Furthermore, we empirically show that pre-training implicitly minimizes intrinsic dimension and, perhaps surprisingly, larger models tend to have lower intrinsic dimension after a fixed number of pre-training updates, at least in part explaining their extreme effectiveness. Lastly, we connect intrinsic dimensionality with low dimensional task representations and compression based generalization bounds to provide intrinsic-dimension-based generalization bounds that are independent of the full parameter count",
    "checked": true,
    "id": "e54ffc76d805c48660bb0fd20019ca82ac94ba0d",
    "semantic_title": "",
    "citation_count": 112,
    "authors": [
      "Armen Aghajanyan",
      "Sonal Gupta",
      "Luke Zettlemoyer"
    ]
  },
  "https://aclanthology.org/2021.acl-long.569": {
    "title": "UnNatural Language Inference",
    "volume": "long",
    "abstract": "Recent investigations into the inner-workings of state-of-the-art large-scale pre-trained Transformer-based Natural Language Understanding (NLU) models indicate that they appear to understand human-like syntax, at least to some extent. We provide novel evidence that complicates this claim: we find that state-of-the-art Natural Language Inference (NLI) models assign the same labels to permuted examples as they do to the original, i.e. they are invariant to random word-order permutations. This behavior notably differs from that of humans; we struggle to understand the meaning of ungrammatical sentences. To measure the severity of this issue, we propose a suite of metrics and investigate which properties of particular permutations lead models to be word order invariant. For example, in MNLI dataset we find almost all (98.7%) examples contain at least one permutation which elicits the gold label. Models are even able to assign gold labels to permutations that they originally failed to predict correctly. We provide a comprehensive empirical evaluation of this phenomenon, and further show that this issue exists in pre-Transformer RNN / ConvNet based encoders, as well as across multiple languages (English and Chinese). Our code and data are available at https://github.com/facebookresearch/unlu",
    "checked": true,
    "id": "2e3a7760c543a181b47245ffece91bff027c43c9",
    "semantic_title": "",
    "citation_count": 47,
    "authors": [
      "Koustuv Sinha",
      "Prasanna Parthasarathi",
      "Joelle Pineau",
      "Adina Williams"
    ]
  },
  "https://aclanthology.org/2021.acl-long.570": {
    "title": "Including Signed Languages in Natural Language Processing",
    "volume": "long",
    "abstract": "Signed languages are the primary means of communication for many deaf and hard of hearing individuals. Since signed languages exhibit all the fundamental linguistic properties of natural language, we believe that tools and theories of Natural Language Processing (NLP) are crucial towards its modeling. However, existing research in Sign Language Processing (SLP) seldom attempt to explore and leverage the linguistic organization of signed languages. This position paper calls on the NLP community to include signed languages as a research area with high social and scientific impact. We first discuss the linguistic properties of signed languages to consider during their modeling. Then, we review the limitations of current SLP models and identify the open challenges to extend NLP to signed languages. Finally, we urge (1) the adoption of an efficient tokenization method; (2) the development of linguistically-informed models; (3) the collection of real-world signed language data; (4) the inclusion of local signed language communities as an active and leading voice in the direction of research",
    "checked": true,
    "id": "c4358134954d8e62939c3a8b9ba8e953d951f73b",
    "semantic_title": "",
    "citation_count": 44,
    "authors": [
      "Kayo Yin",
      "Amit Moryossef",
      "Julie Hochgesang",
      "Yoav Goldberg",
      "Malihe Alikhani"
    ]
  },
  "https://aclanthology.org/2021.acl-long.571": {
    "title": "Vocabulary Learning via Optimal Transport for Neural Machine Translation",
    "volume": "long",
    "abstract": "The choice of token vocabulary affects the performance of machine translation. This paper aims to figure out what is a good vocabulary and whether we can find the optimal vocabulary without trial training. To answer these questions, we first provide an alternative understanding of vocabulary from the perspective of information theory. It motivates us to formulate the quest of vocabularization – finding the best token dictionary with a proper size – as an optimal transport (OT) problem. We propose VOLT, a simple and efficient solution without trial training. Empirical results show that VOLT beats widely-used vocabularies in diverse scenarios, including WMT-14 English-German translation, TED bilingual translation, and TED multilingual translation. For example, VOLT achieves 70% vocabulary size reduction and 0.5 BLEU gain on English-German translation. Also, compared to BPE-search, VOLT reduces the search time from 384 GPU hours to 30 GPU hours on English-German translation. Codes are available at https://github.com/Jingjing-NLP/VOLT",
    "checked": true,
    "id": "b086b812c867b1d07eb65bcdd206dd0891733f9d",
    "semantic_title": "",
    "citation_count": 43,
    "authors": [
      "Jingjing Xu",
      "Hao Zhou",
      "Chun Gan",
      "Zaixiang Zheng",
      "Lei Li"
    ]
  },
  "https://aclanthology.org/2021.acl-short.1": {
    "title": "Catchphrase: Automatic Detection of Cultural References",
    "volume": "short",
    "abstract": "A snowclone is a customizable phrasal template that can be realized in multiple, instantly recognized variants. For example, “* is the new *\" (Orange is the new black, 40 is the new 30). Snowclones are extensively used in social media. In this paper, we study snowclones originating from pop-culture quotes; our goal is to automatically detect cultural references in text. We introduce a new, publicly available data set of pop-culture quotes and their corresponding snowclone usages and train models on them. We publish code for Catchphrase, an internet browser plugin to automatically detect and mark references in real-time, and examine its performance via a user study. Aside from assisting people to better comprehend cultural references, we hope that detecting snowclones can complement work on paraphrasing and help tackling long-standing questions in social science about the dynamics of information propagation",
    "checked": true,
    "id": "50058cb053ede6c630dc39d4db07cea2b668367d",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Nir Sweed",
      "Dafna Shahaf"
    ]
  },
  "https://aclanthology.org/2021.acl-short.2": {
    "title": "On Training Instance Selection for Few-Shot Neural Text Generation",
    "volume": "short",
    "abstract": "Large-scale pretrained language models have led to dramatic improvements in text generation. Impressive performance can be achieved by finetuning only on a small number of instances (few-shot setting). Nonetheless, almost all previous work simply applies random sampling to select the few-shot training instances. Little to no attention has been paid to the selection strategies and how they would affect model performance. In this work, we present a study on training instance selection in few-shot neural text generation. The selection decision is made based only on the unlabeled data so as to identify the most worthwhile data points that should be annotated under some budget of labeling cost. Based on the intuition that the few-shot training instances should be diverse and representative of the entire data distribution, we propose a simple selection strategy with K-means clustering. We show that even with the naive clustering-based approach, the generation models consistently outperform random sampling on three text generation tasks: data-to-text generation, document summarization and question generation. The code and training data are made available. We hope that this work will call for more attention on this largely unexplored area",
    "checked": true,
    "id": "e3c5ef26c54203c6b370e6d37ef8178ef231e5e1",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Ernie Chang",
      "Xiaoyu Shen",
      "Hui-Syuan Yeh",
      "Vera Demberg"
    ]
  },
  "https://aclanthology.org/2021.acl-short.3": {
    "title": "Coreference Resolution without Span Representations",
    "volume": "short",
    "abstract": "The introduction of pretrained language models has reduced many complex task-specific NLP models to simple lightweight layers. An exception to this trend is coreference resolution, where a sophisticated task-specific model is appended to a pretrained transformer encoder. While highly effective, the model has a very large memory footprint – primarily due to dynamically-constructed span and span-pair representations – which hinders the processing of complete documents and the ability to train on multiple instances in a single batch. We introduce a lightweight end-to-end coreference model that removes the dependency on span representations, handcrafted features, and heuristics. Our model performs competitively with the current standard model, while being simpler and more efficient",
    "checked": true,
    "id": "3029263ca51e6c2907f9f99277083cf6afb1adb7",
    "semantic_title": "",
    "citation_count": 43,
    "authors": [
      "Yuval Kirstain",
      "Ori Ram",
      "Omer Levy"
    ]
  },
  "https://aclanthology.org/2021.acl-short.4": {
    "title": "Enhancing Entity Boundary Detection for Better Chinese Named Entity Recognition",
    "volume": "short",
    "abstract": "In comparison with English, due to the lack of explicit word boundary and tenses information, Chinese Named Entity Recognition (NER) is much more challenging. In this paper, we propose a boundary enhanced approach for better Chinese NER. In particular, our approach enhances the boundary information from two perspectives. On one hand, we enhance the representation of the internal dependency of phrases by an additional Graph Attention Network(GAT) layer. On the other hand, taking the entity head-tail prediction (i.e., boundaries) as an auxiliary task, we propose an unified framework to learn the boundary information and recognize the NE jointly. Experiments on both the OntoNotes and the Weibo corpora show the effectiveness of our approach",
    "checked": true,
    "id": "086c78fd5b53f016a2a9f067b12f0c47966382d7",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Chun Chen",
      "Fang Kong"
    ]
  },
  "https://aclanthology.org/2021.acl-short.5": {
    "title": "Difficulty-Aware Machine Translation Evaluation",
    "volume": "short",
    "abstract": "The high-quality translation results produced by machine translation (MT) systems still pose a huge challenge for automatic evaluation. Current MT evaluation pays the same attention to each sentence component, while the questions of real-world examinations (e.g., university examinations) have different difficulties and weightings. In this paper, we propose a novel difficulty-aware MT evaluation metric, expanding the evaluation dimension by taking translation difficulty into consideration. A translation that fails to be predicted by most MT systems will be treated as a difficult one and assigned a large weight in the final score function, and conversely. Experimental results on the WMT19 English-German Metrics shared tasks show that our proposed method outperforms commonly used MT metrics in terms of human correlation. In particular, our proposed method performs well even when all the MT systems are very competitive, which is when most existing metrics fail to distinguish between them. The source code is freely available at https://github.com/NLP2CT/Difficulty-Aware-MT-Evaluation",
    "checked": true,
    "id": "3e724faf78a51d5cc3852d79a339b820150628d8",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Runzhe Zhan",
      "Xuebo Liu",
      "Derek F. Wong",
      "Lidia S. Chao"
    ]
  },
  "https://aclanthology.org/2021.acl-short.6": {
    "title": "Uncertainty and Surprisal Jointly Deliver the Punchline: Exploiting Incongruity-Based Features for Humor Recognition",
    "volume": "short",
    "abstract": "Humor recognition has been widely studied as a text classification problem using data-driven approaches. However, most existing work does not examine the actual joke mechanism to understand humor. We break down any joke into two distinct components: the set-up and the punchline, and further explore the special relationship between them. Inspired by the incongruity theory of humor, we model the set-up as the part developing semantic uncertainty, and the punchline disrupting audience expectations. With increasingly powerful language models, we were able to feed the set-up along with the punchline into the GPT-2 language model, and calculate the uncertainty and surprisal values of the jokes. By conducting experiments on the SemEval 2021 Task 7 dataset, we found that these two features have better capabilities of telling jokes from non-jokes, compared with existing baselines",
    "checked": true,
    "id": "daae582be667cc24fafb2b93208e377c2c5dac37",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Yubo Xie",
      "Junze Li",
      "Pearl Pu"
    ]
  },
  "https://aclanthology.org/2021.acl-short.7": {
    "title": "Counterfactuals to Control Latent Disentangled Text Representations for Style Transfer",
    "volume": "short",
    "abstract": "Disentanglement of latent representations into content and style spaces has been a commonly employed method for unsupervised text style transfer. These techniques aim to learn the disentangled representations and tweak them to modify the style of a sentence. In this paper, we propose a counterfactual-based method to modify the latent representation, by posing a ‘what-if’ scenario. This simple and disciplined approach also enables a fine-grained control on the transfer strength. We conduct experiments with the proposed methodology on multiple attribute transfer tasks like Sentiment, Formality and Excitement to support our hypothesis",
    "checked": true,
    "id": "1cb9df8d24eb0e346bc08b2e5ca3827807b6aff0",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Sharmila Reddy Nangi",
      "Niyati Chhaya",
      "Sopan Khosla",
      "Nikhil Kaushik",
      "Harshit Nyati"
    ]
  },
  "https://aclanthology.org/2021.acl-short.8": {
    "title": "Attention Flows are Shapley Value Explanations",
    "volume": "short",
    "abstract": "Shapley Values, a solution to the credit assignment problem in cooperative game theory, are a popular type of explanation in machine learning, having been used to explain the importance of features, embeddings, and even neurons. In NLP, however, leave-one-out and attention-based explanations still predominate. Can we draw a connection between these different methods? We formally prove that — save for the degenerate case — attention weights and leave-one-out values cannot be Shapley Values. Attention flow is a post-processed variant of attention weights obtained by running the max-flow algorithm on the attention graph. Perhaps surprisingly, we prove that attention flows are indeed Shapley Values, at least at the layerwise level. Given the many desirable theoretical qualities of Shapley Values — which has driven their adoption among the ML community — we argue that NLP practitioners should, when possible, adopt attention flow explanations alongside more traditional ones",
    "checked": true,
    "id": "da130d6538eeeacfb3a0da4cff106c098f74cdd4",
    "semantic_title": "",
    "citation_count": 17,
    "authors": [
      "Kawin Ethayarajh",
      "Dan Jurafsky"
    ]
  },
  "https://aclanthology.org/2021.acl-short.9": {
    "title": "Video Paragraph Captioning as a Text Summarization Task",
    "volume": "short",
    "abstract": "Video paragraph captioning aims to generate a set of coherent sentences to describe a video that contains several events. Most previous methods simplify this task by using ground-truth event segments. In this work, we propose a novel framework by taking this task as a text summarization task. We first generate lots of sentence-level captions focusing on different video clips and then summarize these captions to obtain the final paragraph caption. Our method does not depend on ground-truth event segments. Experiments on two popular datasets ActivityNet Captions and YouCookII demonstrate the advantages of our new framework. On the ActivityNet dataset, our method even outperforms some previous methods using ground-truth event segment labels",
    "checked": true,
    "id": "3d9c86d3bfd856299fcd1152e59d3f2fb7cded83",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Hui Liu",
      "Xiaojun Wan"
    ]
  },
  "https://aclanthology.org/2021.acl-short.10": {
    "title": "Are VQA Systems RAD? Measuring Robustness to Augmented Data with Focused Interventions",
    "volume": "short",
    "abstract": "Deep learning algorithms have shown promising results in visual question answering (VQA) tasks, but a more careful look reveals that they often do not understand the rich signal they are being fed with. To understand and better measure the generalization capabilities of VQA systems, we look at their robustness to counterfactually augmented data. Our proposed augmentations are designed to make a focused intervention on a specific property of the question such that the answer changes. Using these augmentations, we propose a new robustness measure, Robustness to Augmented Data (RAD), which measures the consistency of model predictions between original and augmented examples. Through extensive experimentation, we show that RAD, unlike classical accuracy measures, can quantify when state-of-the-art systems are not robust to counterfactuals. We find substantial failure cases which reveal that current VQA systems are still brittle. Finally, we connect between robustness and generalization, demonstrating the predictive power of RAD for performance on unseen augmentations",
    "checked": true,
    "id": "abaa9cc845d3b18328d2014d780235fdd9695d1e",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Daniel Rosenberg",
      "Itai Gat",
      "Amir Feder",
      "Roi Reichart"
    ]
  },
  "https://aclanthology.org/2021.acl-short.11": {
    "title": "How Helpful is Inverse Reinforcement Learning for Table-to-Text Generation?",
    "volume": "short",
    "abstract": "Existing approaches for the Table-to-Text task suffer from issues such as missing information, hallucination and repetition. Many approaches to this problem use Reinforcement Learning (RL), which maximizes a single manually defined reward, such as BLEU. In this work, we instead pose the Table-to-Text task as Inverse Reinforcement Learning (IRL) problem. We explore using multiple interpretable unsupervised reward components that are combined linearly to form a composite reward function. The composite reward function and the description generator are learned jointly. We find that IRL outperforms strong RL baselines marginally. We further study the generalization of learned IRL rewards in scenarios involving domain adaptation. Our experiments reveal significant challenges in using IRL for this task",
    "checked": true,
    "id": "dffd39f36deaf68e9916b945800757993895baf8",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Sayan Ghosh",
      "Zheng Qi",
      "Snigdha Chaturvedi",
      "Shashank Srivastava"
    ]
  },
  "https://aclanthology.org/2021.acl-short.12": {
    "title": "Automatic Fake News Detection: Are Models Learning to Reason?",
    "volume": "short",
    "abstract": "Most fact checking models for automatic fake news detection are based on reasoning: given a claim with associated evidence, the models aim to estimate the claim veracity based on the supporting or refuting content within the evidence. When these models perform well, it is generally assumed to be due to the models having learned to reason over the evidence with regards to the claim. In this paper, we investigate this assumption of reasoning, by exploring the relationship and importance of both claim and evidence. Surprisingly, we find on political fact checking datasets that most often the highest effectiveness is obtained by utilizing only the evidence, as the impact of including the claim is either negligible or harmful to the effectiveness. This highlights an important problem in what constitutes evidence in existing approaches for automatic fake news detection",
    "checked": true,
    "id": "f830aae938f3a7611a0ceabe2a6cd432660e5116",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Casper Hansen",
      "Christian Hansen",
      "Lucas Chaves Lima"
    ]
  },
  "https://aclanthology.org/2021.acl-short.13": {
    "title": "Saying No is An Art: Contextualized Fallback Responses for Unanswerable Dialogue Queries",
    "volume": "short",
    "abstract": "Despite end-to-end neural systems making significant progress in the last decade for task-oriented as well as chit-chat based dialogue systems, most dialogue systems rely on hybrid approaches which use a combination of rule-based, retrieval and generative approaches for generating a set of ranked responses. Such dialogue systems need to rely on a fallback mechanism to respond to out-of-domain or novel user queries which are not answerable within the scope of the dialogue system. While, dialogue systems today rely on static and unnatural responses like “I don’t know the answer to that question” or “I’m not sure about that”, we design a neural approach which generates responses which are contextually aware with the user query as well as say no to the user. Such customized responses provide paraphrasing ability and contextualization as well as improve the interaction with the user and reduce dialogue monotonicity. Our simple approach makes use of rules over dependency parses and a text-to-text transformer fine-tuned on synthetic data of question-response pairs generating highly relevant, grammatical as well as diverse questions. We perform automatic and manual evaluations to demonstrate the efficacy of the system",
    "checked": true,
    "id": "2664a8ff0e5e779ea1d1ae8b4868e6d980bb40a6",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Ashish Shrivastava",
      "Kaustubh Dhole",
      "Abhinav Bhatt",
      "Sharvani Raghunath"
    ]
  },
  "https://aclanthology.org/2021.acl-short.14": {
    "title": "N-Best ASR Transformer: Enhancing SLU Performance using Multiple ASR Hypotheses",
    "volume": "short",
    "abstract": "Spoken Language Understanding (SLU) systems parse speech into semantic structures like dialog acts and slots. This involves the use of an Automatic Speech Recognizer (ASR) to transcribe speech into multiple text alternatives (hypotheses). Transcription errors, ordinary in ASRs, impact downstream SLU performance negatively. Common approaches to mitigate such errors involve using richer information from the ASR, either in form of N-best hypotheses or word-lattices. We hypothesize that transformer models will learn better with a simpler utterance representation using the concatenation of the N-best ASR alternatives, where each alternative is separated by a special delimiter [SEP]. In our work, we test our hypothesis by using the concatenated N-best ASR alternatives as the input to the transformer encoder models, namely BERT and XLM-RoBERTa, and achieve equivalent performance to the prior state-of-the-art model on DSTC2 dataset. We also show that our approach significantly outperforms the prior state-of-the-art when subjected to the low data regime. Additionally, this methodology is accessible to users of third-party ASR APIs which do not provide word-lattice information",
    "checked": true,
    "id": "b1db3f66d7b5cd8c7b72b824bac487c1b6a2b487",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Karthik Ganesan",
      "Pakhi Bamdev",
      "Jaivarsan B",
      "Amresh Venugopal",
      "Abhinav Tushar"
    ]
  },
  "https://aclanthology.org/2021.acl-short.15": {
    "title": "Gender bias amplification during Speed-Quality optimization in Neural Machine Translation",
    "volume": "short",
    "abstract": "Is bias amplified when neural machine translation (NMT) models are optimized for speed and evaluated on generic test sets using BLEU? We investigate architectures and techniques commonly used to speed up decoding in Transformer-based models, such as greedy search, quantization, average attention networks (AANs) and shallow decoder models and show their effect on gendered noun translation. We construct a new gender bias test set, SimpleGEN, based on gendered noun phrases in which there is a single, unambiguous, correct answer. While we find minimal overall BLEU degradation as we apply speed optimizations, we observe that gendered noun translation performance degrades at a much faster rate",
    "checked": true,
    "id": "63052e581f1b272eefdbf109a230c7ec87e1f79a",
    "semantic_title": "",
    "citation_count": 24,
    "authors": [
      "Adithya Renduchintala",
      "Denise Diaz",
      "Kenneth Heafield",
      "Xian Li",
      "Mona Diab"
    ]
  },
  "https://aclanthology.org/2021.acl-short.16": {
    "title": "Machine Translation into Low-resource Language Varieties",
    "volume": "short",
    "abstract": "State-of-the-art machine translation (MT) systems are typically trained to generate “standard” target language; however, many languages have multiple varieties (regional varieties, dialects, sociolects, non-native varieties) that are different from the standard language. Such varieties are often low-resource, and hence do not benefit from contemporary NLP solutions, MT included. We propose a general framework to rapidly adapt MT systems to generate language varieties that are close to, but different from, the standard target language, using no parallel (source–variety) data. This also includes adaptation of MT systems to low-resource typologically-related target languages. We experiment with adapting an English–Russian MT system to generate Ukrainian and Belarusian, an English–Norwegian Bokmål system to generate Nynorsk, and an English–Arabic system to generate four Arabic dialects, obtaining significant improvements over competitive baselines",
    "checked": true,
    "id": "a3d22d18905e09eea53bcc2cd619037f4db736ea",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Sachin Kumar",
      "Antonios Anastasopoulos",
      "Shuly Wintner",
      "Yulia Tsvetkov"
    ]
  },
  "https://aclanthology.org/2021.acl-short.17": {
    "title": "Is Sparse Attention more Interpretable?",
    "volume": "short",
    "abstract": "Sparse attention has been claimed to increase model interpretability under the assumption that it highlights influential inputs. Yet the attention distribution is typically over representations internal to the model rather than the inputs themselves, suggesting this assumption may not have merit. We build on the recent work exploring the interpretability of attention; we design a set of experiments to help us understand how sparsity affects our ability to use attention as an explainability tool. On three text classification tasks, we verify that only a weak relationship between inputs and co-indexed intermediate representations exists—under sparse attention and otherwise. Further, we do not find any plausible mappings from sparse attention distributions to a sparse set of influential inputs through other avenues. Rather, we observe in this setting that inducing sparsity may make it less plausible that attention can be used as a tool for understanding model behavior",
    "checked": true,
    "id": "bc73d53ba859c56ab08c41c475d45a9ae6d021cb",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Clara Meister",
      "Stefan Lazov",
      "Isabelle Augenstein",
      "Ryan Cotterell"
    ]
  },
  "https://aclanthology.org/2021.acl-short.18": {
    "title": "The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models",
    "volume": "short",
    "abstract": "Mechanisms for encoding positional information are central for transformer-based language models. In this paper, we analyze the position embeddings of existing language models, finding strong evidence of translation invariance, both for the embeddings themselves and for their effect on self-attention. The degree of translation invariance increases during training and correlates positively with model performance. Our findings lead us to propose translation-invariant self-attention (TISA), which accounts for the relative position between tokens in an interpretable fashion without needing conventional position embeddings. Our proposal has several theoretical advantages over existing position-representation approaches. Proof-of-concept experiments show that it improves on regular ALBERT on GLUE tasks, while only adding orders of magnitude less positional parameters",
    "checked": true,
    "id": "d8e7bad2681ce70277c900c77a22181d4b03d705",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Ulme Wennberg",
      "Gustav Eje Henter"
    ]
  },
  "https://aclanthology.org/2021.acl-short.19": {
    "title": "Relative Importance in Sentence Processing",
    "volume": "short",
    "abstract": "Determining the relative importance of the elements in a sentence is a key factor for effortless natural language understanding. For human language processing, we can approximate patterns of relative importance by measuring reading fixations using eye-tracking technology. In neural language models, gradient-based saliency methods indicate the relative importance of a token for the target objective. In this work, we compare patterns of relative importance in English language processing by humans and models and analyze the underlying linguistic patterns. We find that human processing patterns in English correlate strongly with saliency-based importance in language models and not with attention-based importance. Our results indicate that saliency could be a cognitively more plausible metric for interpreting neural language models. The code is available on github: https://github.com/beinborn/relative_importance",
    "checked": true,
    "id": "e1afe296314493791b1c2c0af88cfe97279bdc69",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Nora Hollenstein",
      "Lisa Beinborn"
    ]
  },
  "https://aclanthology.org/2021.acl-short.20": {
    "title": "Doing Good or Doing Right? Exploring the Weakness of Commonsense Causal Reasoning Models",
    "volume": "short",
    "abstract": "Pretrained language models (PLM) achieve surprising performance on the Choice of Plausible Alternatives (COPA) task. However, whether PLMs have truly acquired the ability of causal reasoning remains a question. In this paper, we investigate the problem of semantic similarity bias and reveal the vulnerability of current COPA models by certain attacks. Previous solutions that tackle the superficial cues of unbalanced token distribution still encounter the same problem of semantic bias, even more seriously due to the utilization of more training data. We mitigate this problem by simply adding a regularization loss and experimental results show that this solution not only improves the model’s generalization ability, but also assists the models to perform more robustly on a challenging dataset, BCOPA-CE, which has unbiased token distribution and is more difficult for models to distinguish cause and effect",
    "checked": true,
    "id": "f9ff3facd992951415e67ce08cd31c5ba8c04b4e",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Mingyue Han",
      "Yinglin Wang"
    ]
  },
  "https://aclanthology.org/2021.acl-short.21": {
    "title": "AND does not mean OR: Using Formal Languages to Study Language Models' Representations",
    "volume": "short",
    "abstract": "A current open question in natural language processing is to what extent language models, which are trained with access only to the form of language, are able to capture the meaning of language. This question is challenging to answer in general, as there is no clear line between meaning and form, but rather meaning constrains form in consistent ways. The goal of this study is to offer insights into a narrower but critical subquestion: Under what conditions should we expect that meaning and form covary sufficiently, such that a language model with access only to form might nonetheless succeed in emulating meaning? Focusing on several formal languages (propositional logic and a set of programming languages), we generate training corpora using a variety of motivated constraints, and measure a distributional language model’s ability to differentiate logical symbols (AND, OR, and NOT). Our findings are largely negative: none of our simulated training corpora result in models which definitively differentiate meaningfully different symbols (e.g., AND vs. OR), suggesting a limitation to the types of semantic signals that current models are able to exploit",
    "checked": true,
    "id": "7ddb18fc67f13ff8ea5467bc04eb41666f8e7cb8",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Aaron Traylor",
      "Roman Feiman",
      "Ellie Pavlick"
    ]
  },
  "https://aclanthology.org/2021.acl-short.22": {
    "title": "Enforcing Consistency in Weakly Supervised Semantic Parsing",
    "volume": "short",
    "abstract": "The predominant challenge in weakly supervised semantic parsing is that of spurious programs that evaluate to correct answers for the wrong reasons. Prior work uses elaborate search strategies to mitigate the prevalence of spurious programs; however, they typically consider only one input at a time. In this work we explore the use of consistency between the output programs for related inputs to reduce the impact of spurious programs. We bias the program search (and thus the model’s training signal) towards programs that map the same phrase in related inputs to the same sub-parts in their respective programs. Additionally, we study the importance of designing logical formalisms that facilitate this kind of consistency-based training. We find that a more consistent formalism leads to improved model performance even without consistency-based training. When combined together, these two insights lead to a 10% absolute improvement over the best prior result on the Natural Language Visual Reasoning dataset",
    "checked": true,
    "id": "447a36764df662076e0be38e2ed14aed7f5bace6",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Nitish Gupta",
      "Sameer Singh",
      "Matt Gardner"
    ]
  },
  "https://aclanthology.org/2021.acl-short.23": {
    "title": "An Improved Model for Voicing Silent Speech",
    "volume": "short",
    "abstract": "In this paper, we present an improved model for voicing silent speech, where audio is synthesized from facial electromyography (EMG) signals. To give our model greater flexibility to learn its own input features, we directly use EMG signals as input in the place of hand-designed features used by prior work. Our model uses convolutional layers to extract features from the signals and Transformer layers to propagate information across longer distances. To provide better signal for learning, we also introduce an auxiliary task of predicting phoneme labels in addition to predicting speech audio features. On an open vocabulary intelligibility evaluation, our model improves the state of the art for this task by an absolute 25.8%",
    "checked": true,
    "id": "4dc2c552bfb0abcf15db9cfe795da9e97551ee42",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "David Gaddy",
      "Dan Klein"
    ]
  },
  "https://aclanthology.org/2021.acl-short.24": {
    "title": "What's in the Box? An Analysis of Undesirable Content in the Common Crawl Corpus",
    "volume": "short",
    "abstract": "Whereas much of the success of the current generation of neural language models has been driven by increasingly large training corpora, relatively little research has been dedicated to analyzing these massive sources of textual data. In this exploratory analysis, we delve deeper into the Common Crawl, a colossal web corpus that is extensively used for training language models. We find that it contains a significant amount of undesirable content, including hate speech and sexually explicit content, even after filtering procedures. We discuss the potential impacts of this content on language models and conclude with future research directions and a more mindful approach to corpus collection and analysis",
    "checked": true,
    "id": "5b1641b7661b4d9ec2826e847ebf1b36f2d5bdec",
    "semantic_title": "",
    "citation_count": 33,
    "authors": [
      "Alexandra Luccioni",
      "Joseph Viviano"
    ]
  },
  "https://aclanthology.org/2021.acl-short.25": {
    "title": "Continual Quality Estimation with Online Bayesian Meta-Learning",
    "volume": "short",
    "abstract": "Most current quality estimation (QE) models for machine translation are trained and evaluated in a static setting where training and test data are assumed to be from a fixed distribution. However, in real-life settings, the test data that a deployed QE model would be exposed to may differ from its training data. In particular, training samples are often labelled by one or a small set of annotators, whose perceptions of translation quality and needs may differ substantially from those of end-users, who will employ predictions in practice. To address this challenge, we propose an online Bayesian meta-learning framework for the continuous training of QE models that is able to adapt them to the needs of different users, while being robust to distributional shifts in training and test data. Experiments on data with varying number of users and language characteristics validate the effectiveness of the proposed approach",
    "checked": true,
    "id": "7bf530a2d01cd29b7c09116f2437b799aadd1b05",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Abiola Obamuyide",
      "Marina Fomicheva",
      "Lucia Specia"
    ]
  },
  "https://aclanthology.org/2021.acl-short.26": {
    "title": "A Span-based Dynamic Local Attention Model for Sequential Sentence Classification",
    "volume": "short",
    "abstract": "Sequential sentence classification aims to classify each sentence in the document based on the context in which sentences appear. Most existing work addresses this problem using a hierarchical sequence labeling network. However, they ignore considering the latent segment structure of the document, in which contiguous sentences often have coherent semantics. In this paper, we proposed a span-based dynamic local attention model that could explicitly capture the structural information by the proposed supervised dynamic local attention. We further introduce an auxiliary task called span-based classification to explore the span-level representations. Extensive experiments show that our model achieves better or competitive performance against state-of-the-art baselines on two benchmark datasets",
    "checked": true,
    "id": "6515d5dc6a04050a9dd46ccebff9ef745ffe95f0",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Xichen Shang",
      "Qianli Ma",
      "Zhenxi Lin",
      "Jiangyue Yan",
      "Zipeng Chen"
    ]
  },
  "https://aclanthology.org/2021.acl-short.27": {
    "title": "How effective is BERT without word ordering? Implications for language understanding and data privacy",
    "volume": "short",
    "abstract": "Ordered word sequences contain the rich structures that define language. However, it’s often not clear if or how modern pretrained language models utilize these structures. We show that the token representations and self-attention activations within BERT are surprisingly resilient to shuffling the order of input tokens, and that for several GLUE language understanding tasks, shuffling only minimally degrades performance, e.g., by 4% for QNLI. While bleak from the perspective of language understanding, our results have positive implications for cases where copyright or ethics necessitates the consideration of bag-of-words data (vs. full documents). We simulate such a scenario for three sensitive classification tasks, demonstrating minimal performance degradation vs. releasing full language sequences",
    "checked": true,
    "id": "3757de51963e31d0d88dc7a0fbffb8e5d6ba0a79",
    "semantic_title": "",
    "citation_count": 19,
    "authors": [
      "Jack Hessel",
      "Alexandra Schofield"
    ]
  },
  "https://aclanthology.org/2021.acl-short.28": {
    "title": "WikiSum: Coherent Summarization Dataset for Efficient Human-Evaluation",
    "volume": "short",
    "abstract": "Recent works made significant advances on summarization tasks, facilitated by summarization datasets. Several existing datasets have the form of coherent-paragraph summaries. However, these datasets were curated from academic documents that were written for experts, thus making the essential step of assessing the summarization output through human-evaluation very demanding. To overcome these limitations, we present a dataset based on article summaries appearing on the WikiHow website, composed of how-to articles and coherent-paragraph summaries written in plain language. We compare our dataset attributes to existing ones, including readability and world-knowledge, showing our dataset makes human evaluation significantly easier and thus, more effective. A human evaluation conducted on PubMed and the proposed dataset reinforces our findings",
    "checked": true,
    "id": "224ef724ca1ca3eed6475cf46a72109709371d81",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Nachshon Cohen",
      "Oren Kalinsky",
      "Yftah Ziser",
      "Alessandro Moschitti"
    ]
  },
  "https://aclanthology.org/2021.acl-short.29": {
    "title": "UMIC: An Unreferenced Metric for Image Captioning via Contrastive Learning",
    "volume": "short",
    "abstract": "Despite the success of various text generation metrics such as BERTScore, it is still difficult to evaluate the image captions without enough reference captions due to the diversity of the descriptions. In this paper, we introduce a new metric UMIC, an Unreferenced Metric for Image Captioning which does not require reference captions to evaluate image captions. Based on Vision-and-Language BERT, we train UMIC to discriminate negative captions via contrastive learning. Also, we observe critical problems of the previous benchmark dataset (i.e., human annotations) on image captioning metric, and introduce a new collection of human annotations on the generated captions. We validate UMIC on four datasets, including our new dataset, and show that UMIC has a higher correlation than all previous metrics that require multiple references",
    "checked": true,
    "id": "389b98518980f218cdc0869fd852428686eef6dd",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Hwanhee Lee",
      "Seunghyun Yoon",
      "Franck Dernoncourt",
      "Trung Bui",
      "Kyomin Jung"
    ]
  },
  "https://aclanthology.org/2021.acl-short.30": {
    "title": "Anchor-based Bilingual Word Embeddings for Low-Resource Languages",
    "volume": "short",
    "abstract": "Good quality monolingual word embeddings (MWEs) can be built for languages which have large amounts of unlabeled text. MWEs can be aligned to bilingual spaces using only a few thousand word translation pairs. For low resource languages training MWEs monolingually results in MWEs of poor quality, and thus poor bilingual word embeddings (BWEs) as well. This paper proposes a new approach for building BWEs in which the vector space of the high resource source language is used as a starting point for training an embedding space for the low resource target language. By using the source vectors as anchors the vector spaces are automatically aligned during training. We experiment on English-German, English-Hiligaynon and English-Macedonian. We show that our approach results not only in improved BWEs and bilingual lexicon induction performance, but also in improved target language MWE quality as measured using monolingual word similarity",
    "checked": true,
    "id": "5e161653faa3a7739841d28c2d57ac76718b1e17",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Tobias Eder",
      "Viktor Hangya",
      "Alexander Fraser"
    ]
  },
  "https://aclanthology.org/2021.acl-short.31": {
    "title": "Multilingual Agreement for Multilingual Neural Machine Translation",
    "volume": "short",
    "abstract": "Although multilingual neural machine translation (MNMT) enables multiple language translations, the training process is based on independent multilingual objectives. Most multilingual models can not explicitly exploit different language pairs to assist each other, ignoring the relationships among them. In this work, we propose a novel agreement-based method to encourage multilingual agreement among different translation directions, which minimizes the differences among them. We combine the multilingual training objectives with the agreement term by randomly substituting some fragments of the source language with their counterpart translations of auxiliary languages. To examine the effectiveness of our method, we conduct experiments on the multilingual translation task of 10 language pairs. Experimental results show that our method achieves significant improvements over the previous multilingual baselines",
    "checked": true,
    "id": "8db80de081890299c1bd6a6134cf1d7750fc80cf",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Jian Yang",
      "Yuwei Yin",
      "Shuming Ma",
      "Haoyang Huang",
      "Dongdong Zhang",
      "Zhoujun Li",
      "Furu Wei"
    ]
  },
  "https://aclanthology.org/2021.acl-short.32": {
    "title": "Higher-order Derivatives of Weighted Finite-state Machines",
    "volume": "short",
    "abstract": "Weighted finite-state machines are a fundamental building block of NLP systems. They have withstood the test of time—from their early use in noisy channel models in the 1990s up to modern-day neurally parameterized conditional random fields. This work examines the computation of higher-order derivatives with respect to the normalization constant for weighted finite-state machines. We provide a general algorithm for evaluating derivatives of all orders, which has not been previously described in the literature. In the case of second-order derivatives, our scheme runs in the optimal O(Aˆ2 Nˆ4) time where A is the alphabet size and N is the number of states. Our algorithm is significantly faster than prior algorithms. Additionally, our approach leads to a significantly faster algorithm for computing second-order expectations, such as covariance matrices and gradients of first-order expectations",
    "checked": true,
    "id": "3b67cbfcbb8840755e25bbb2907215ca8754e3e9",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ran Zmigrod",
      "Tim Vieira",
      "Ryan Cotterell"
    ]
  },
  "https://aclanthology.org/2021.acl-short.33": {
    "title": "Reinforcement Learning for Abstractive Question Summarization with Question-aware Semantic Rewards",
    "volume": "short",
    "abstract": "The growth of online consumer health questions has led to the necessity for reliable and accurate question answering systems. A recent study showed that manual summarization of consumer health questions brings significant improvement in retrieving relevant answers. However, the automatic summarization of long questions is a challenging task due to the lack of training data and the complexity of the related subtasks, such as the question focus and type recognition. In this paper, we introduce a reinforcement learning-based framework for abstractive question summarization. We propose two novel rewards obtained from the downstream tasks of (i) question-type identification and (ii) question-focus recognition to regularize the question generation model. These rewards ensure the generation of semantically valid questions and encourage the inclusion of key medical entities/foci in the question summary. We evaluated our proposed method on two benchmark datasets and achieved higher performance over state-of-the-art models. The manual evaluation of the summaries reveals that the generated questions are more diverse and have fewer factual inconsistencies than the baseline summaries. The source code is available here: https://github.com/shwetanlp/CHQ-Summ",
    "checked": true,
    "id": "5011b61b11fe88d2f79dd5bb330691d23b73dddc",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Shweta Yadav",
      "Deepak Gupta",
      "Asma Ben Abacha",
      "Dina Demner-Fushman"
    ]
  },
  "https://aclanthology.org/2021.acl-short.34": {
    "title": "A Semantics-aware Transformer Model of Relation Linking for Knowledge Base Question Answering",
    "volume": "short",
    "abstract": "Relation linking is a crucial component of Knowledge Base Question Answering systems. Existing systems use a wide variety of heuristics, or ensembles of multiple systems, heavily relying on the surface question text. However, the explicit semantic parse of the question is a rich source of relation information that is not taken advantage of. We propose a simple transformer-based neural model for relation linking that leverages the AMR semantic parse of a sentence. Our system significantly outperforms the state-of-the-art on 4 popular benchmark datasets. These are based on either DBpedia or Wikidata, demonstrating that our approach is effective across KGs",
    "checked": true,
    "id": "f980bd9bcf0fac1d132cc4e1e0d6ebd51ec6fe9c",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Tahira Naseem",
      "Srinivas Ravishankar",
      "Nandana Mihindukulasooriya",
      "Ibrahim Abdelaziz",
      "Young-Suk Lee",
      "Pavan Kapanipathi",
      "Salim Roukos",
      "Alfio Gliozzo",
      "Alexander Gray"
    ]
  },
  "https://aclanthology.org/2021.acl-short.35": {
    "title": "Neural Retrieval for Question Answering with Cross-Attention Supervised Data Augmentation",
    "volume": "short",
    "abstract": "Early fusion models with cross-attention have shown better-than-human performance on some question answer benchmarks, while it is a poor fit for retrieval since it prevents pre-computation of the answer representations. We present a supervised data mining method using an accurate early fusion model to improve the training of an efficient late fusion retrieval model. We first train an accurate classification model with cross-attention between questions and answers. The cross-attention model is then used to annotate additional passages in order to generate weighted training examples for a neural retrieval model. The resulting retrieval model with additional data significantly outperforms retrieval models directly trained with gold annotations on Precision at N (P@N) and Mean Reciprocal Rank (MRR)",
    "checked": true,
    "id": "5679431425a81c07bcafa521e5609cc05b3ec5dc",
    "semantic_title": "",
    "citation_count": 17,
    "authors": [
      "Yinfei Yang",
      "Ning Jin",
      "Kuo Lin",
      "Mandy Guo",
      "Daniel Cer"
    ]
  },
  "https://aclanthology.org/2021.acl-short.36": {
    "title": "Enhancing Descriptive Image Captioning with Natural Language Inference",
    "volume": "short",
    "abstract": "Generating descriptive sentences that convey non-trivial, detailed, and salient information about images is an important goal of image captioning. In this paper we propose a novel approach to encourage captioning models to produce more detailed captions using natural language inference, based on the motivation that, among different captions of an image, descriptive captions are more likely to entail less descriptive captions. Specifically, we construct directed inference graphs for reference captions based on natural language inference. A PageRank algorithm is then employed to estimate the descriptiveness score of each node. Built on that, we use reference sampling and weighted designated rewards to guide captioning to generate descriptive captions. The results on MSCOCO show that the proposed method outperforms the baselines significantly on a wide range of conventional and descriptiveness-related evaluation metrics",
    "checked": true,
    "id": "e761a0d40cc7d6efddd0123d11045eba72c3d760",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Zhan Shi",
      "Hui Liu",
      "Xiaodan Zhu"
    ]
  },
  "https://aclanthology.org/2021.acl-short.37": {
    "title": "MOLEMAN: Mention-Only Linking of Entities with a Mention Annotation Network",
    "volume": "short",
    "abstract": "We present an instance-based nearest neighbor approach to entity linking. In contrast to most prior entity retrieval systems which represent each entity with a single vector, we build a contextualized mention-encoder that learns to place similar mentions of the same entity closer in vector space than mentions of different entities. This approach allows all mentions of an entity to serve as “class prototypes” as inference involves retrieving from the full set of labeled entity mentions in the training set and applying the nearest mention neighbor’s entity label. Our model is trained on a large multilingual corpus of mention pairs derived from Wikipedia hyperlinks, and performs nearest neighbor inference on an index of 700 million mentions. It is simpler to train, gives more interpretable predictions, and outperforms all other systems on two multilingual entity linking benchmarks",
    "checked": true,
    "id": "f7d6bf58f5960c4ad18a698410f5bd9c15d092ae",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Nicholas FitzGerald",
      "Dan Bikel",
      "Jan Botha",
      "Daniel Gillick",
      "Tom Kwiatkowski",
      "Andrew McCallum"
    ]
  },
  "https://aclanthology.org/2021.acl-short.38": {
    "title": "eMLM: A New Pre-training Objective for Emotion Related Tasks",
    "volume": "short",
    "abstract": "BERT has been shown to be extremely effective on a wide variety of natural language processing tasks, including sentiment analysis and emotion detection. However, the proposed pretraining objectives of BERT do not induce any sentiment or emotion-specific biases into the model. In this paper, we present Emotion Masked Language Modelling, a variation of Masked Language Modelling aimed at improving the BERT language representation model for emotion detection and sentiment analysis tasks. Using the same pre-training corpora as the original model, Wikipedia and BookCorpus, our BERT variation manages to improve the downstream performance on 4 tasks from emotion detection and sentiment analysis by an average of 1.2% F-1. Moreover, our approach shows an increased performance in our task-specific robustness tests",
    "checked": true,
    "id": "dbacc7a4e6739bc85d2495220bcbc3f223eab013",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Tiberiu Sosea",
      "Cornelia Caragea"
    ]
  },
  "https://aclanthology.org/2021.acl-short.39": {
    "title": "On Positivity Bias in Negative Reviews",
    "volume": "short",
    "abstract": "Prior work has revealed that positive words occur more frequently than negative words in human expressions, which is typically attributed to positivity bias, a tendency for people to report positive views of reality. But what about the language used in negative reviews? Consistent with prior work, we show that English negative reviews tend to contain more positive words than negative words, using a variety of datasets. We reconcile this observation with prior findings on the pragmatics of negation, and show that negations are commonly associated with positive words in negative reviews. Furthermore, in negative reviews, the majority of sentences with positive words express negative opinions based on sentiment classifiers, indicating some form of negation",
    "checked": true,
    "id": "06463fb4232539f8e098fdab680fae01e7b1d4c7",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Madhusudhan Aithal",
      "Chenhao Tan"
    ]
  },
  "https://aclanthology.org/2021.acl-short.40": {
    "title": "PRAL: A Tailored Pre-Training Model for Task-Oriented Dialog Generation",
    "volume": "short",
    "abstract": "Large pre-trained language generation models such as GPT-2 have demonstrated their effectiveness as language priors by reaching state-of-the-art results in various language generation tasks. However, the performance of pre-trained models on task-oriented dialog tasks is still under-explored. We propose a Pre-trainedRole Alternating Language model (PRAL), explicitly designed for task-oriented conversational systems. We design several techniques: start position randomization, knowledge distillation, and history discount to improve pre-training performance. In addition, we introduce a high-quality large-scale task-oriented dialog pre-training dataset by post-prossessing13 dialog datasets. We effectively adapt PRALon three downstream tasks. The results show that PRAL outperforms or is on par with state-of-the-art models",
    "checked": true,
    "id": "bcba2d80ee681394c0ba43520e32bf56ab5ff3bd",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Jing Gu",
      "Qingyang Wu",
      "Chongruo Wu",
      "Weiyan Shi",
      "Zhou Yu"
    ]
  },
  "https://aclanthology.org/2021.acl-short.41": {
    "title": "ROPE: Reading Order Equivariant Positional Encoding for Graph-based Document Information Extraction",
    "volume": "short",
    "abstract": "Natural reading orders of words are crucial for information extraction from form-like documents. Despite recent advances in Graph Convolutional Networks (GCNs) on modeling spatial layout patterns of documents, they have limited ability to capture reading orders of given word-level node representations in a graph. We propose Reading Order Equivariant Positional Encoding (ROPE), a new positional encoding technique designed to apprehend the sequential presentation of words in documents. ROPE generates unique reading order codes for neighboring words relative to the target word given a word-level graph connectivity. We study two fundamental document entity extraction tasks including word labeling and word grouping on the public FUNSD dataset and a large-scale payment dataset. We show that ROPE consistently improves existing GCNs with a margin up to 8.4% F1-score",
    "checked": true,
    "id": "7cf872dbad1cd780a2ebf56f8bee76fb9497c018",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Chen-Yu Lee",
      "Chun-Liang Li",
      "Chu Wang",
      "Renshen Wang",
      "Yasuhisa Fujii",
      "Siyang Qin",
      "Ashok Popat",
      "Tomas Pfister"
    ]
  },
  "https://aclanthology.org/2021.acl-short.42": {
    "title": "Zero-shot Event Extraction via Transfer Learning: Challenges and Insights",
    "volume": "short",
    "abstract": "Event extraction has long been a challenging task, addressed mostly with supervised methods that require expensive annotation and are not extensible to new event ontologies. In this work, we explore the possibility of zero-shot event extraction by formulating it as a set of Textual Entailment (TE) and/or Question Answering (QA) queries (e.g. “A city was attacked” entails “There is an attack”), exploiting pretrained TE/QA models for direct transfer. On ACE-2005 and ERE, our system achieves acceptable results, yet there is still a large gap from supervised approaches, showing that current QA and TE technologies fail in transferring to a different domain. To investigate the reasons behind the gap, we analyze the remaining key challenges, their respective impact, and possible improvement directions",
    "checked": true,
    "id": "3047c31521e919b8bad3e9682358e33b0e3aa8bd",
    "semantic_title": "",
    "citation_count": 47,
    "authors": [
      "Qing Lyu",
      "Hongming Zhang",
      "Elior Sulem",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/2021.acl-short.43": {
    "title": "Using Adversarial Attacks to Reveal the Statistical Bias in Machine Reading Comprehension Models",
    "volume": "short",
    "abstract": "Pre-trained language models have achieved human-level performance on many Machine Reading Comprehension (MRC) tasks, but it remains unclear whether these models truly understand language or answer questions by exploiting statistical biases in datasets. Here, we demonstrate a simple yet effective method to attack MRC models and reveal the statistical biases in these models. We apply the method to the RACE dataset, for which the answer to each MRC question is selected from 4 options. It is found that several pre-trained language models, including BERT, ALBERT, and RoBERTa, show consistent preference to some options, even when these options are irrelevant to the question. When interfered by these irrelevant options, the performance of MRC models can be reduced from human-level performance to the chance-level performance. Human readers, however, are not clearly affected by these irrelevant options. Finally, we propose an augmented training method that can greatly reduce models’ statistical biases",
    "checked": true,
    "id": "620f2d75b08aa0788d3215d545c23bdd3bc1b337",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Jieyu Lin",
      "Jiajie Zou",
      "Nai Ding"
    ]
  },
  "https://aclanthology.org/2021.acl-short.44": {
    "title": "Quantifying and Avoiding Unfair Qualification Labour in Crowdsourcing",
    "volume": "short",
    "abstract": "Extensive work has argued in favour of paying crowd workers a wage that is at least equivalent to the U.S. federal minimum wage. Meanwhile, research on collecting high quality annotations suggests using a qualification that requires workers to have previously completed a certain number of tasks. If most requesters who pay fairly require workers to have completed a large number of tasks already then workers need to complete a substantial amount of poorly paid work before they can earn a fair wage. Through analysis of worker discussions and guidance for researchers, we estimate that workers spend approximately 2.25 months of full time effort on poorly paid tasks in order to get the qualifications needed for better paid tasks. We discuss alternatives to this qualification and conduct a study of the correlation between qualifications and work quality on two NLP tasks. We find that it is possible to reduce the burden on workers while still collecting high quality data",
    "checked": true,
    "id": "410164c34d3677ce5cf51571d7fbdfcb81eb1086",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Jonathan K. Kummerfeld"
    ]
  },
  "https://aclanthology.org/2021.acl-short.45": {
    "title": "Men Are Elected, Women Are Married: Events Gender Bias on Wikipedia",
    "volume": "short",
    "abstract": "Human activities can be seen as sequences of events, which are crucial to understanding societies. Disproportional event distribution for different demographic groups can manifest and amplify social stereotypes, and potentially jeopardize the ability of members in some groups to pursue certain goals. In this paper, we present the first event-centric study of gender biases in a Wikipedia corpus. To facilitate the study, we curate a corpus of career and personal life descriptions with demographic information consisting of 7,854 fragments from 10,412 celebrities. Then we detect events with a state-of-the-art event detection model, calibrate the results using strategically generated templates, and extract events that have asymmetric associations with genders. Our study discovers that the Wikipedia pages tend to intermingle personal life events with professional events for females but not for males, which calls for the awareness of the Wikipedia community to formalize guidelines and train the editors to mind the implicit biases that contributors carry. Our work also lays the foundation for future works on quantifying and discovering event biases at the corpus level",
    "checked": true,
    "id": "35511226f4e0e8aa39ebbfd22844fbfbd0070799",
    "semantic_title": "",
    "citation_count": 21,
    "authors": [
      "Jiao Sun",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2021.acl-short.46": {
    "title": "Modeling Task-Aware MIMO Cardinality for Efficient Multilingual Neural Machine Translation",
    "volume": "short",
    "abstract": "Neural machine translation has achieved great success in bilingual settings, as well as in multilingual settings. With the increase of the number of languages, multilingual systems tend to underperform their bilingual counterparts. Model capacity has been found crucial for massively multilingual NMT to support language pairs with varying typological characteristics. Previous work increases the modeling capacity by deepening or widening the Transformer. However, modeling cardinality based on aggregating a set of transformations with the same topology has been proven more effective than going deeper or wider when increasing capacity. In this paper, we propose to efficiently increase the capacity for multilingual NMT by increasing the cardinality. Unlike previous work which feeds the same input to several transformations and merges their outputs into one, we present a Multi-Input-Multi-Output (MIMO) architecture that allows each transformation of the block to have its own input. We also present a task-aware attention mechanism to learn to selectively utilize individual transformations from a set of transformations for different translation directions. Our model surpasses previous work and establishes a new state-of-the-art on the large scale OPUS-100 corpus while being 1.31 times as fast",
    "checked": true,
    "id": "d60119f643be6294470f26c20eeaa2582a64e336",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Hongfei Xu",
      "Qiuhui Liu",
      "Josef van Genabith",
      "Deyi Xiong"
    ]
  },
  "https://aclanthology.org/2021.acl-short.47": {
    "title": "Adaptive Nearest Neighbor Machine Translation",
    "volume": "short",
    "abstract": "kNN-MT, recently proposed by Khandelwal et al. (2020a), successfully combines pre-trained neural machine translation (NMT) model with token-level k-nearest-neighbor (kNN) retrieval to improve the translation accuracy. However, the traditional kNN algorithm used in kNN-MT simply retrieves a same number of nearest neighbors for each target token, which may cause prediction errors when the retrieved neighbors include noises. In this paper, we propose Adaptive kNN-MT to dynamically determine the number of k for each target token. We achieve this by introducing a light-weight Meta-k Network, which can be efficiently trained with only a few training samples. On four benchmark machine translation datasets, we demonstrate that the proposed method is able to effectively filter out the noises in retrieval results and significantly outperforms the vanilla kNN-MT model. Even more noteworthy is that the Meta-k Network learned on one domain could be directly applied to other domains and obtain consistent improvements, illustrating the generality of our method. Our implementation is open-sourced at https://github.com/zhengxxn/adaptive-knn-mt",
    "checked": true,
    "id": "6551ba18a52809f057518f3ebc627c77689bfc93",
    "semantic_title": "",
    "citation_count": 43,
    "authors": [
      "Xin Zheng",
      "Zhirui Zhang",
      "Junliang Guo",
      "Shujian Huang",
      "Boxing Chen",
      "Weihua Luo",
      "Jiajun Chen"
    ]
  },
  "https://aclanthology.org/2021.acl-short.48": {
    "title": "On Orthogonality Constraints for Transformers",
    "volume": "short",
    "abstract": "Orthogonality constraints encourage matrices to be orthogonal for numerical stability. These plug-and-play constraints, which can be conveniently incorporated into model training, have been studied for popular architectures in natural language processing, such as convolutional neural networks and recurrent neural networks. However, a dedicated study on such constraints for transformers has been absent. To fill this gap, this paper studies orthogonality constraints for transformers, showing the effectiveness with empirical evidence from ten machine translation tasks and two dialogue generation tasks. For example, on the large-scale WMT’16 En→De benchmark, simply plugging-and-playing orthogonality constraints on the original transformer model (Vaswani et al., 2017) increases the BLEU from 28.4 to 29.6, coming close to the 29.7 BLEU achieved by the very competitive dynamic convolution (Wu et al., 2019)",
    "checked": true,
    "id": "e382a73c4f87020e5ee8af8feacc6053347a129f",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Aston Zhang",
      "Alvin Chan",
      "Yi Tay",
      "Jie Fu",
      "Shuohang Wang",
      "Shuai Zhang",
      "Huajie Shao",
      "Shuochao Yao",
      "Roy Ka-Wei Lee"
    ]
  },
  "https://aclanthology.org/2021.acl-short.49": {
    "title": "Measuring and Improving BERT's Mathematical Abilities by Predicting the Order of Reasoning",
    "volume": "short",
    "abstract": "Imagine you are in a supermarket. You have two bananas in your basket and want to buy four apples. How many fruits do you have in total? This seemingly straightforward question can be challenging for data-driven language models, even if trained at scale. However, we would expect such generic language models to possess some mathematical abilities in addition to typical linguistic competence. Towards this goal, we investigate if a commonly used language model, BERT, possesses such mathematical abilities and, if so, to what degree. For that, we fine-tune BERT on a popular dataset for word math problems, AQuA-RAT, and conduct several tests to understand learned representations better. Since we teach models trained on natural language to do formal mathematics, we hypothesize that such models would benefit from training on semi-formal steps that explain how math results are derived. To better accommodate such training, we also propose new pretext tasks for learning mathematical rules. We call them (Neighbor) Reasoning Order Prediction (ROP or NROP). With this new model, we achieve significantly better outcomes than data-driven baselines and even on-par with more tailored models",
    "checked": true,
    "id": "57a1258571a21817d89197dc84c986861fb6e580",
    "semantic_title": "",
    "citation_count": 18,
    "authors": [
      "Piotr Piękos",
      "Mateusz Malinowski",
      "Henryk Michalewski"
    ]
  },
  "https://aclanthology.org/2021.acl-short.50": {
    "title": "Happy Dance, Slow Clap: Using Reaction GIFs to Predict Induced Affect on Twitter",
    "volume": "short",
    "abstract": "Datasets with induced emotion labels are scarce but of utmost importance for many NLP tasks. We present a new, automated method for collecting texts along with their induced reaction labels. The method exploits the online use of reaction GIFs, which capture complex affective states. We show how to augment the data with induced emotion and induced sentiment labels. We use our method to create and publish ReactionGIF, a first-of-its-kind affective dataset of 30K tweets. We provide baselines for three new tasks, including induced sentiment prediction and multilabel classification of induced emotions. Our method and dataset open new research opportunities in emotion detection and affective computing",
    "checked": true,
    "id": "9880c8e9ce969e002cbd9e49ed5d1d830445492b",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Boaz Shmueli",
      "Soumya Ray",
      "Lun-Wei Ku"
    ]
  },
  "https://aclanthology.org/2021.acl-short.51": {
    "title": "Exploring Listwise Evidence Reasoning with T5 for Fact Verification",
    "volume": "short",
    "abstract": "This work explores a framework for fact verification that leverages pretrained sequence-to-sequence transformer models for sentence selection and label prediction, two key sub-tasks in fact verification. Most notably, improving on previous pointwise aggregation approaches for label prediction, we take advantage of T5 using a listwise approach coupled with data augmentation. With this enhancement, we observe that our label prediction stage is more robust to noise and capable of verifying complex claims by jointly reasoning over multiple pieces of evidence. Experimental results on the FEVER task show that our system attains a FEVER score of 75.87% on the blind test set. This puts our approach atop the competitive FEVER leaderboard at the time of our work, scoring higher than the second place submission by almost two points in label accuracy and over one point in FEVER score",
    "checked": true,
    "id": "0c1d53fc87ca482037360b3547111158b505b26e",
    "semantic_title": "",
    "citation_count": 24,
    "authors": [
      "Kelvin Jiang",
      "Ronak Pradeep",
      "Jimmy Lin"
    ]
  },
  "https://aclanthology.org/2021.acl-short.52": {
    "title": "DefSent: Sentence Embeddings using Definition Sentences",
    "volume": "short",
    "abstract": "Sentence embedding methods using natural language inference (NLI) datasets have been successfully applied to various tasks. However, these methods are only available for limited languages due to relying heavily on the large NLI datasets. In this paper, we propose DefSent, a sentence embedding method that uses definition sentences from a word dictionary, which performs comparably on unsupervised semantics textual similarity (STS) tasks and slightly better on SentEval tasks than conventional methods. Since dictionaries are available for many languages, DefSent is more broadly applicable than methods using NLI datasets without constructing additional datasets. We demonstrate that DefSent performs comparably on unsupervised semantics textual similarity (STS) tasks and slightly better on SentEval tasks to the methods using large NLI datasets. Our code is publicly available at https://github.com/hpprc/defsent",
    "checked": true,
    "id": "314d9c650a6aebdbca90313a156b6cd28f85ba72",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Hayato Tsukagoshi",
      "Ryohei Sasano",
      "Koichi Takeda"
    ]
  },
  "https://aclanthology.org/2021.acl-short.53": {
    "title": "Discrete Cosine Transform as Universal Sentence Encoder",
    "volume": "short",
    "abstract": "Modern sentence encoders are used to generate dense vector representations that capture the underlying linguistic characteristics for a sequence of words, including phrases, sentences, or paragraphs. These kinds of representations are ideal for training a classifier for an end task such as sentiment analysis, question answering and text classification. Different models have been proposed to efficiently generate general purpose sentence representations to be used in pretraining protocols. While averaging is the most commonly used efficient sentence encoder, Discrete Cosine Transform (DCT) was recently proposed as an alternative that captures the underlying syntactic characteristics of a given text without compromising practical efficiency compared to averaging. However, as with most other sentence encoders, the DCT sentence encoder was only evaluated in English. To this end, we utilize DCT encoder to generate universal sentence representation for different languages such as German, French, Spanish and Russian. The experimental results clearly show the superior effectiveness of DCT encoding in which consistent performance improvements are achieved over strong baselines on multiple standardized datasets",
    "checked": true,
    "id": "c4d3794279e98f2c1a1c91c8b07121bffc13ae89",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nada Almarwani",
      "Mona Diab"
    ]
  },
  "https://aclanthology.org/2021.acl-short.54": {
    "title": "AligNarr: Aligning Narratives on Movies",
    "volume": "short",
    "abstract": "High-quality alignment between movie scripts and plot summaries is an asset for learning to summarize stories and to generate dialogues. The alignment task is challenging as scripts and summaries substantially differ in details and abstraction levels as well as in linguistic register. This paper addresses the alignment problem by devising a fully unsupervised approach based on a global optimization model. Experimental results on ten movies show the viability of our method with 76% F1-score and its superiority over a previous baseline. We publish alignments for 914 movies to foster research in this new topic",
    "checked": true,
    "id": "26afaeefbd0c38c5367919a0d5cf9c0ca6e91986",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Paramita Mirza",
      "Mostafa Abouhamra",
      "Gerhard Weikum"
    ]
  },
  "https://aclanthology.org/2021.acl-short.55": {
    "title": "An Exploratory Analysis of Multilingual Word-Level Quality Estimation with Cross-Lingual Transformers",
    "volume": "short",
    "abstract": "Most studies on word-level Quality Estimation (QE) of machine translation focus on language-specific models. The obvious disadvantages of these approaches are the need for labelled data for each language pair and the high cost required to maintain several language-specific models. To overcome these problems, we explore different approaches to multilingual, word-level QE. We show that multilingual QE models perform on par with the current language-specific models. In the cases of zero-shot and few-shot QE, we demonstrate that it is possible to accurately predict word-level quality for any given new language pair from models trained on other language pairs. Our findings suggest that the word-level QE models based on powerful pre-trained transformers that we propose in this paper generalise well across languages, making them more useful in real-world scenarios",
    "checked": true,
    "id": "d6218ce8a829c705678884d77ad791753755175e",
    "semantic_title": "",
    "citation_count": 17,
    "authors": [
      "Tharindu Ranasinghe",
      "Constantin Orasan",
      "Ruslan Mitkov"
    ]
  },
  "https://aclanthology.org/2021.acl-short.56": {
    "title": "Exploration and Exploitation: Two Ways to Improve Chinese Spelling Correction Models",
    "volume": "short",
    "abstract": "A sequence-to-sequence learning with neural networks has empirically proven to be an effective framework for Chinese Spelling Correction (CSC), which takes a sentence with some spelling errors as input and outputs the corrected one. However, CSC models may fail to correct spelling errors covered by the confusion sets, and also will encounter unseen ones. We propose a method, which continually identifies the weak spots of a model to generate more valuable training instances, and apply a task-specific pre-training strategy to enhance the model. The generated adversarial examples are gradually added to the training set. Experimental results show that such an adversarial training method combined with the pre-training strategy can improve both the generalization and robustness of multiple CSC models across three different datasets, achieving state-of-the-art performance for CSC task",
    "checked": true,
    "id": "755038a42abf9a9fa6d40ac7c0eb9fbe6298d3f0",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Chong Li",
      "Cenyuan Zhang",
      "Xiaoqing Zheng",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2021.acl-short.57": {
    "title": "Training Adaptive Computation for Open-Domain Question Answering with Computational Constraints",
    "volume": "short",
    "abstract": "Adaptive Computation (AC) has been shown to be effective in improving the efficiency of Open-Domain Question Answering (ODQA) systems. However, the current AC approaches require tuning of all model parameters, and training state-of-the-art ODQA models requires significant computational resources that may not be available for most researchers. We propose Adaptive Passage Encoder, an AC method that can be applied to an existing ODQA model and can be trained efficiently on a single GPU. It keeps the parameters of the base ODQA model fixed, but it overrides the default layer-by-layer computation of the encoder with an AC policy that is trained to optimise the computational efficiency of the model. Our experimental results show that our method improves upon a state-of-the-art model on two datasets, and is also more accurate than previous AC methods due to the stronger base ODQA model. All source code and datasets are available at https://github.com/uclnlp/APE",
    "checked": true,
    "id": "4572ce690e5eff7d7d38c6f3f67561d0b263b34f",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Yuxiang Wu",
      "Pasquale Minervini",
      "Pontus Stenetorp",
      "Sebastian Riedel"
    ]
  },
  "https://aclanthology.org/2021.acl-short.58": {
    "title": "An Empirical Study on Adversarial Attack on NMT: Languages and Positions Matter",
    "volume": "short",
    "abstract": "In this paper, we empirically investigate adversarial attack on NMT from two aspects: languages (the source vs. the target language) and positions (front vs. rear). For autoregressive NMT models that generate target words from left to right, we observe that adversarial attack on the source language is more effective than on the target language, and that attacking front positions of target sentences or positions of source sentences aligned to the front positions of corresponding target sentences is more effective than attacking other positions. We further exploit the attention distribution of the victim model to attack source sentences at positions that have a strong association with front target words. Experiment results demonstrate that our attention-based adversarial attack is more effective than adversarial attacks by sampling positions randomly or according to gradients",
    "checked": true,
    "id": "2effdadfa3723abfa39db66e844a3c485203849a",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Zhiyuan Zeng",
      "Deyi Xiong"
    ]
  },
  "https://aclanthology.org/2021.acl-short.59": {
    "title": "OntoGUM: Evaluating Contextualized SOTA Coreference Resolution on 12 More Genres",
    "volume": "short",
    "abstract": "SOTA coreference resolution produces increasingly impressive scores on the OntoNotes benchmark. However lack of comparable data following the same scheme for more genres makes it difficult to evaluate generalizability to open domain data. This paper provides a dataset and comprehensive evaluation showing that the latest neural LM based end-to-end systems degrade very substantially out of domain. We make an OntoNotes-like coreference dataset called OntoGUM publicly available, converted from GUM, an English corpus covering 12 genres, using deterministic rules, which we evaluate. Thanks to the rich syntactic and discourse annotations in GUM, we are able to create the largest human-annotated coreference corpus following the OntoNotes guidelines, and the first to be evaluated for consistency with the OntoNotes scheme. Out-of-domain evaluation across 12 genres shows nearly 15-20% degradation for both deterministic and deep learning systems, indicating a lack of generalizability or covert overfitting in existing coreference resolution models",
    "checked": true,
    "id": "037adc1f49e42b681a482c181df46bab054944ff",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Yilun Zhu",
      "Sameer Pradhan",
      "Amir Zeldes"
    ]
  },
  "https://aclanthology.org/2021.acl-short.60": {
    "title": "In Factuality: Efficient Integration of Relevant Facts for Visual Question Answering",
    "volume": "short",
    "abstract": "Visual Question Answering (VQA) methods aim at leveraging visual input to answer questions that may require complex reasoning over entities. Current models are trained on labelled data that may be insufficient to learn complex knowledge representations. In this paper, we propose a new method to enhance the reasoning capabilities of a multi-modal pretrained model (Vision+Language BERT) by integrating facts extracted from an external knowledge base. Evaluation on the KVQA dataset benchmark demonstrates that our method outperforms competitive baselines by 19%, achieving new state-of-the-art results. We also perform an extensive analysis highlighting the limitations of our best performing model through an ablation study",
    "checked": true,
    "id": "0f9b5b32229a7245e43754430c0c88f8e7f0d8af",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Peter Vickers",
      "Nikolaos Aletras",
      "Emilio Monti",
      "Loïc Barrault"
    ]
  },
  "https://aclanthology.org/2021.acl-short.61": {
    "title": "Zero-shot Fact Verification by Claim Generation",
    "volume": "short",
    "abstract": "Neural models for automated fact verification have achieved promising results thanks to the availability of large, human-annotated datasets. However, for each new domain that requires fact verification, creating a dataset by manually writing claims and linking them to their supporting evidence is expensive. We develop QACG, a framework for training a robust fact verification model by using automatically generated claims that can be supported, refuted, or unverifiable from evidence from Wikipedia. QACG generates question-answer pairs from the evidence and then converts them into different types of claims. Experiments on the FEVER dataset show that our QACG framework significantly reduces the demand for human-annotated training data. In a zero-shot scenario, QACG improves a RoBERTa model’s F1 from 50% to 77%, equivalent in performance to 2K+ manually-curated examples. Our QACG code is publicly available",
    "checked": true,
    "id": "25c9dbcad66fedec76a8941db46bda2e9bd8f698",
    "semantic_title": "",
    "citation_count": 24,
    "authors": [
      "Liangming Pan",
      "Wenhu Chen",
      "Wenhan Xiong",
      "Min-Yen Kan",
      "William Yang Wang"
    ]
  },
  "https://aclanthology.org/2021.acl-short.62": {
    "title": "Thank you BART! Rewarding Pre-Trained Models Improves Formality Style Transfer",
    "volume": "short",
    "abstract": "Scarcity of parallel data causes formality style transfer models to have scarce success in preserving content. We show that fine-tuning pre-trained language (GPT-2) and sequence-to-sequence (BART) models boosts content preservation, and that this is possible even with limited amounts of parallel data. Augmenting these models with rewards that target style and content –the two core aspects of the task– we achieve a new state-of-the-art",
    "checked": true,
    "id": "0bf230ee96c7ec49fe9d2aadcaec598d68ad94f1",
    "semantic_title": "",
    "citation_count": 30,
    "authors": [
      "Huiyuan Lai",
      "Antonio Toral",
      "Malvina Nissim"
    ]
  },
  "https://aclanthology.org/2021.acl-short.63": {
    "title": "Deep Context- and Relation-Aware Learning for Aspect-based Sentiment Analysis",
    "volume": "short",
    "abstract": "Existing works for aspect-based sentiment analysis (ABSA) have adopted a unified approach, which allows the interactive relations among subtasks. However, we observe that these methods tend to predict polarities based on the literal meaning of aspect and opinion terms and mainly consider relations implicitly among subtasks at the word level. In addition, identifying multiple aspect–opinion pairs with their polarities is much more challenging. Therefore, a comprehensive understanding of contextual information w.r.t. the aspect and opinion are further required in ABSA. In this paper, we propose Deep Contextualized Relation-Aware Network (DCRAN), which allows interactive relations among subtasks with deep contextual information based on two modules (i.e., Aspect and Opinion Propagation and Explicit Self-Supervised Strategies). Especially, we design novel self-supervised strategies for ABSA, which have strengths in dealing with multiple aspects. Experimental results show that DCRAN significantly outperforms previous state-of-the-art methods by large margins on three widely used benchmarks",
    "checked": true,
    "id": "725f0f75f0b210c31378ee1358345dbfb6322578",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Shinhyeok Oh",
      "Dongyub Lee",
      "Taesun Whang",
      "IlNam Park",
      "Seo Gaeun",
      "EungGyun Kim",
      "Harksoo Kim"
    ]
  },
  "https://aclanthology.org/2021.acl-short.64": {
    "title": "Towards Generative Aspect-Based Sentiment Analysis",
    "volume": "short",
    "abstract": "Aspect-based sentiment analysis (ABSA) has received increasing attention recently. Most existing work tackles ABSA in a discriminative manner, designing various task-specific classification networks for the prediction. Despite their effectiveness, these methods ignore the rich label semantics in ABSA problems and require extensive task-specific designs. In this paper, we propose to tackle various ABSA tasks in a unified generative framework. Two types of paradigms, namely annotation-style and extraction-style modeling, are designed to enable the training process by formulating each ABSA task as a text generation problem. We conduct experiments on four ABSA tasks across multiple benchmark datasets where our proposed generative approach achieves new state-of-the-art results in almost all cases. This also validates the strong generality of the proposed framework which can be easily adapted to arbitrary ABSA task without additional task-specific model design",
    "checked": true,
    "id": "768ff532981f47a954d8bc6563a5b3e909cfbcad",
    "semantic_title": "",
    "citation_count": 47,
    "authors": [
      "Wenxuan Zhang",
      "Xin Li",
      "Yang Deng",
      "Lidong Bing",
      "Wai Lam"
    ]
  },
  "https://aclanthology.org/2021.acl-short.65": {
    "title": "Bilingual Mutual Information Based Adaptive Training for Neural Machine Translation",
    "volume": "short",
    "abstract": "Recently, token-level adaptive training has achieved promising improvement in machine translation, where the cross-entropy loss function is adjusted by assigning different training weights to different tokens, in order to alleviate the token imbalance problem. However, previous approaches only use static word frequency information in the target language without considering the source language, which is insufficient for bilingual tasks like machine translation. In this paper, we propose a novel bilingual mutual information (BMI) based adaptive objective, which measures the learning difficulty for each target token from the perspective of bilingualism, and assigns an adaptive weight accordingly to improve token-level adaptive training. This method assigns larger training weights to tokens with higher BMI, so that easy tokens are updated with coarse granularity while difficult tokens are updated with fine granularity. Experimental results on WMT14 English-to-German and WMT19 Chinese-to-English demonstrate the superiority of our approach compared with the Transformer baseline and previous token-level adaptive training approaches. Further analyses confirm that our method can improve the lexical diversity",
    "checked": true,
    "id": "36ced528de0e87a02ca997a223aeabd7647cd0d9",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Yangyifan Xu",
      "Yijin Liu",
      "Fandong Meng",
      "Jiajun Zhang",
      "Jinan Xu",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2021.acl-short.66": {
    "title": "Continual Learning for Task-oriented Dialogue System with Iterative Network Pruning, Expanding and Masking",
    "volume": "short",
    "abstract": "This ability to learn consecutive tasks without forgetting how to perform previously trained problems is essential for developing an online dialogue system. This paper proposes an effective continual learning method for the task-oriented dialogue system with iterative network pruning, expanding, and masking (TPEM), which preserves performance on previously encountered tasks while accelerating learning progress on subsequent tasks. Specifically, TPEM (i) leverages network pruning to keep the knowledge for old tasks, (ii) adopts network expanding to create free weights for new tasks, and (iii) introduces task-specific network masking to alleviate the negative impact of fixed weights of old tasks on new tasks. We conduct extensive experiments on seven different tasks from three benchmark datasets and show empirically that TPEM leads to significantly improved results over the strong competitors",
    "checked": true,
    "id": "95f3b5b09e3f0e0d2038c57b57325bfe03279fbf",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Binzong Geng",
      "Fajie Yuan",
      "Qiancheng Xu",
      "Ying Shen",
      "Ruifeng Xu",
      "Min Yang"
    ]
  },
  "https://aclanthology.org/2021.acl-short.67": {
    "title": "TIMERS: Document-level Temporal Relation Extraction",
    "volume": "short",
    "abstract": "We present TIMERS - a TIME, Rhetorical and Syntactic-aware model for document-level temporal relation classification in the English language. Our proposed method leverages rhetorical discourse features and temporal arguments from semantic role labels, in addition to traditional local syntactic features, trained through a Gated Relational-GCN. Extensive experiments show that the proposed model outperforms previous methods by 5-18% on the TDDiscourse, TimeBank-Dense, and MATRES datasets due to our discourse-level modeling",
    "checked": true,
    "id": "ce9e010c5cb2323dfed3af687b12875f01c759bc",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Puneet Mathur",
      "Rajiv Jain",
      "Franck Dernoncourt",
      "Vlad Morariu",
      "Quan Hung Tran",
      "Dinesh Manocha"
    ]
  },
  "https://aclanthology.org/2021.acl-short.68": {
    "title": "Improving Arabic Diacritization with Regularized Decoding and Adversarial Training",
    "volume": "short",
    "abstract": "Arabic diacritization is a fundamental task for Arabic language processing. Previous studies have demonstrated that automatically generated knowledge can be helpful to this task. However, these studies regard the auto-generated knowledge instances as gold references, which limits their effectiveness since such knowledge is not always accurate and inferior instances can lead to incorrect predictions. In this paper, we propose to use regularized decoding and adversarial training to appropriately learn from such noisy knowledge for diacritization. Experimental results on two benchmark datasets show that, even with quite flawed auto-generated knowledge, our model can still learn adequate diacritics and outperform all previous studies, on both datasets",
    "checked": true,
    "id": "c21f366b6e024318e57e6b972ba697c769e88957",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Han Qin",
      "Guimin Chen",
      "Yuanhe Tian",
      "Yan Song"
    ]
  },
  "https://aclanthology.org/2021.acl-short.69": {
    "title": "When is Char Better Than Subword: A Systematic Study of Segmentation Algorithms for Neural Machine Translation",
    "volume": "short",
    "abstract": "Subword segmentation algorithms have been a de facto choice when building neural machine translation systems. However, most of them need to learn a segmentation model based on some heuristics, which may produce sub-optimal segmentation. This can be problematic in some scenarios when the target language has rich morphological changes or there is not enough data for learning compact composition rules. Translating at fully character level has the potential to alleviate the issue, but empirical performances of character-based models has not been fully explored. In this paper, we present an in-depth comparison between character-based and subword-based NMT systems under three settings: translating to typologically diverse languages, training with low resource, and adapting to unseen domains. Experiment results show strong competitiveness of character-based models. Further analyses show that compared to subword-based models, character-based models are better at handling morphological phenomena, generating rare and unknown words, and more suitable for transferring to unseen domains",
    "checked": true,
    "id": "0bfa9275561f45bc772c4d544fd75f5d65143c5e",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Jiahuan Li",
      "Yutong Shen",
      "Shujian Huang",
      "Xinyu Dai",
      "Jiajun Chen"
    ]
  },
  "https://aclanthology.org/2021.acl-short.70": {
    "title": "More than Text: Multi-modal Chinese Word Segmentation",
    "volume": "short",
    "abstract": "Chinese word segmentation (CWS) is undoubtedly an important basic task in natural language processing. Previous works only focus on the textual modality, but there are often audio and video utterances (such as news broadcast and face-to-face dialogues), where textual, acoustic and visual modalities normally exist. To this end, we attempt to combine the multi-modality (mainly the converted text and actual voice information) to perform CWS. In this paper, we annotate a new dataset for CWS containing text and audio. Moreover, we propose a time-dependent multi-modal interactive model based on Transformer framework to integrate multi-modal information for word sequence labeling. The experimental results on three different training sets show the effectiveness of our approach with fusing text and audio",
    "checked": true,
    "id": "e7ff0dedf94fbba7abb36f80ebb75716eba51e09",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Dong Zhang",
      "Zheng Hu",
      "Shoushan Li",
      "Hanqian Wu",
      "Qiaoming Zhu",
      "Guodong Zhou"
    ]
  },
  "https://aclanthology.org/2021.acl-short.71": {
    "title": "A Mixture-of-Experts Model for Antonym-Synonym Discrimination",
    "volume": "short",
    "abstract": "Discrimination between antonyms and synonyms is an important and challenging NLP task. Antonyms and synonyms often share the same or similar contexts and thus are hard to make a distinction. This paper proposes two underlying hypotheses and employs the mixture-of-experts framework as a solution. It works on the basis of a divide-and-conquer strategy, where a number of localized experts focus on their own domains (or subspaces) to learn their specialties, and a gating mechanism determines the space partitioning and the expert mixture. Experimental results have shown that our method achieves the state-of-the-art performance on the task",
    "checked": true,
    "id": "0de74abad261ba383786b06065a15f025edf048a",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Zhipeng Xie",
      "Nan Zeng"
    ]
  },
  "https://aclanthology.org/2021.acl-short.72": {
    "title": "Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking",
    "volume": "short",
    "abstract": "Injecting external domain-specific knowledge (e.g., UMLS) into pretrained language models (LMs) advances their capability to handle specialised in-domain tasks such as biomedical entity linking (BEL). However, such abundant expert knowledge is available only for a handful of languages (e.g., English). In this work, by proposing a novel cross-lingual biomedical entity linking task (XL-BEL) and establishing a new XL-BEL benchmark spanning 10 typologically diverse languages, we first investigate the ability of standard knowledge-agnostic as well as knowledge-enhanced monolingual and multilingual LMs beyond the standard monolingual English BEL task. The scores indicate large gaps to English performance. We then address the challenge of transferring domain-specific knowledge in resource-rich languages to resource-poor ones. To this end, we propose and evaluate a series of cross-lingual transfer methods for the XL-BEL task, and demonstrate that general-domain bitext helps propagate the available English knowledge to languages with little to no in-domain data. Remarkably, we show that our proposed domain-specific transfer methods yield consistent gains across all target languages, sometimes up to 20 Precision@1 points, without any in-domain knowledge in the target language, and without any in-domain parallel data",
    "checked": true,
    "id": "0e2dd43d4487f9707eba1482a593293a5934a59e",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Fangyu Liu",
      "Ivan Vulić",
      "Anna Korhonen",
      "Nigel Collier"
    ]
  },
  "https://aclanthology.org/2021.acl-short.73": {
    "title": "A Cluster-based Approach for Improving Isotropy in Contextual Embedding Space",
    "volume": "short",
    "abstract": "The representation degeneration problem in Contextual Word Representations (CWRs) hurts the expressiveness of the embedding space by forming an anisotropic cone where even unrelated words have excessively positive correlations. Existing techniques for tackling this issue require a learning process to re-train models with additional objectives and mostly employ a global assessment to study isotropy. Our quantitative analysis over isotropy shows that a local assessment could be more accurate due to the clustered structure of CWRs. Based on this observation, we propose a local cluster-based method to address the degeneration issue in contextual embedding spaces. We show that in clusters including punctuations and stop words, local dominant directions encode structural information, removing which can improve CWRs performance on semantic tasks. Moreover, we find that tense information in verb representations dominates sense semantics. We show that removing dominant directions of verb representations can transform the space to better suit semantic applications. Our experiments demonstrate that the proposed cluster-based method can mitigate the degeneration problem on multiple tasks",
    "checked": true,
    "id": "36e69a05b315c7f2c51379a261b70154482a4c74",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Sara Rajaee",
      "Mohammad Taher Pilehvar"
    ]
  },
  "https://aclanthology.org/2021.acl-short.74": {
    "title": "Unsupervised Enrichment of Persona-grounded Dialog with Background Stories",
    "volume": "short",
    "abstract": "Humans often refer to personal narratives, life experiences, and events to make a conversation more engaging and rich. While persona-grounded dialog models are able to generate responses that follow a given persona, they often miss out on stating detailed experiences or events related to a persona, often leaving conversations shallow and dull. In this work, we equip dialog models with ‘background stories’ related to a persona by leveraging fictional narratives from existing story datasets (e.g. ROCStories). Since current dialog datasets do not contain such narratives as responses, we perform an unsupervised adaptation of a retrieved story for generating a dialog response using a gradient-based rewriting technique. Our proposed method encourages the generated response to be fluent (i.e., highly likely) with the dialog history, minimally different from the retrieved story to preserve event ordering and consistent with the original persona. We demonstrate that our method can generate responses that are more diverse, and are rated more engaging and human-like by human evaluators, compared to outputs from existing dialog models",
    "checked": true,
    "id": "8f0686ccf1a706a4cf797e0fd9ee0b9ac007dc91",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Bodhisattwa Prasad Majumder",
      "Taylor Berg-Kirkpatrick",
      "Julian McAuley",
      "Harsh Jhamtani"
    ]
  },
  "https://aclanthology.org/2021.acl-short.75": {
    "title": "Beyond Laurel/Yanny: An Autoencoder-Enabled Search for Polyperceivable Audio",
    "volume": "short",
    "abstract": "The famous “laurel/yanny” phenomenon references an audio clip that elicits dramatically different responses from different listeners. For the original clip, roughly half the population hears the word “laurel,” while the other half hears “yanny.” How common are such “polyperceivable” audio clips? In this paper we apply ML techniques to study the prevalence of polyperceivability in spoken language. We devise a metric that correlates with polyperceivability of audio clips, use it to efficiently find new “laurel/yanny”-type examples, and validate these results with human experiments. Our results suggest that polyperceivable examples are surprisingly prevalent in natural language, existing for >2% of English words",
    "checked": true,
    "id": "1a1807c0458d0b2d754b5659111691df0c3a2241",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Kartik Chandra",
      "Chuma Kabaghe",
      "Gregory Valiant"
    ]
  },
  "https://aclanthology.org/2021.acl-short.76": {
    "title": "Don't Let Discourse Confine Your Model: Sequence Perturbations for Improved Event Language Models",
    "volume": "short",
    "abstract": "Event language models represent plausible sequences of events. Most existing approaches train autoregressive models on text, which successfully capture event co-occurrence but unfortunately constrain the model to follow the discourse order in which events are presented. Other domains may employ different discourse orders, and for many applications, we may care about different notions of ordering (e.g., temporal) or not care about ordering at all (e.g., when predicting related events in a schema). We propose a simple yet surprisingly effective strategy for improving event language models by perturbing event sequences so we can relax model dependence on text order. Despite generating completely synthetic event orderings, we show that this technique improves the performance of the event language models on both applications and out-of-domain events data",
    "checked": true,
    "id": "47019e041ee30165a1dcc36ce9847561ade4df59",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Mahnaz Koupaee",
      "Greg Durrett",
      "Nathanael Chambers",
      "Niranjan Balasubramanian"
    ]
  },
  "https://aclanthology.org/2021.acl-short.77": {
    "title": "The Curse of Dense Low-Dimensional Information Retrieval for Large Index Sizes",
    "volume": "short",
    "abstract": "Information Retrieval using dense low-dimensional representations recently became popular and showed out-performance to traditional sparse-representations like BM25. However, no previous work investigated how dense representations perform with large index sizes. We show theoretically and empirically that the performance for dense representations decreases quicker than sparse representations for increasing index sizes. In extreme cases, this can even lead to a tipping point where at a certain index size sparse representations outperform dense representations. We show that this behavior is tightly connected to the number of dimensions of the representations: The lower the dimension, the higher the chance for false positives, i.e. returning irrelevant documents",
    "checked": true,
    "id": "0f49917a1dfafac3d8353d084467ce04cd58aec2",
    "semantic_title": "",
    "citation_count": 30,
    "authors": [
      "Nils Reimers",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2021.acl-short.78": {
    "title": "Cross-lingual Text Classification with Heterogeneous Graph Neural Network",
    "volume": "short",
    "abstract": "Cross-lingual text classification aims at training a classifier on the source language and transferring the knowledge to target languages, which is very useful for low-resource languages. Recent multilingual pretrained language models (mPLM) achieve impressive results in cross-lingual classification tasks, but rarely consider factors beyond semantic similarity, causing performance degradation between some language pairs. In this paper we propose a simple yet effective method to incorporate heterogeneous information within and across languages for cross-lingual text classification using graph convolutional networks (GCN). In particular, we construct a heterogeneous graph by treating documents and words as nodes, and linking nodes with different relations, which include part-of-speech roles, semantic similarity, and document translations. Extensive experiments show that our graph-based method significantly outperforms state-of-the-art models on all tasks, and also achieves consistent performance gain over baselines in low-resource settings where external tools like translators are unavailable",
    "checked": true,
    "id": "413c6974ee908c4f54ed1970ce554db133c4a63d",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Ziyun Wang",
      "Xuan Liu",
      "Peiji Yang",
      "Shixing Liu",
      "Zhisheng Wang"
    ]
  },
  "https://aclanthology.org/2021.acl-short.79": {
    "title": "Towards more equitable question answering systems: How much more data do you need?",
    "volume": "short",
    "abstract": "Question answering (QA) in English has been widely explored, but multilingual datasets are relatively new, with several methods attempting to bridge the gap between high- and low-resourced languages using data augmentation through translation and cross-lingual transfer. In this project we take a step back and study which approaches allow us to take the most advantage of existing resources in order to produce QA systems in many languages. Specifically, we perform extensive analysis to measure the efficacy of few-shot approaches augmented with automatic translations and permutations of context-question-answer pairs. In addition, we make suggestions for future dataset development efforts that make better use of a fixed annotation budget, with a goal of increasing the language coverage of QA datasets and systems",
    "checked": true,
    "id": "7cfcbf280a3d6ee52a3a3900673bcc3ec3608fb4",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Arnab Debnath",
      "Navid Rajabi",
      "Fardina Fathmiul Alam",
      "Antonios Anastasopoulos"
    ]
  },
  "https://aclanthology.org/2021.acl-short.80": {
    "title": "Embedding Time Differences in Context-sensitive Neural Networks for Learning Time to Event",
    "volume": "short",
    "abstract": "We propose an effective context-sensitive neural model for time to event (TTE) prediction task, which aims to predict the amount of time to/from the occurrence of given events in streaming content. We investigate this problem in the context of a multi-task learning framework, which we enrich with time difference embeddings. In addition, we develop a multi-genre dataset of English events about soccer competitions and academy awards ceremonies, and their relevant tweets obtained from Twitter. Our model is 1.4 and 3.3 hours more accurate than the current state-of-the-art model in estimating TTE on English and Dutch tweets respectively. We examine different aspects of our model to illustrate its source of improvement",
    "checked": true,
    "id": "f62601045de4f0db88aac0c7d84a61ea8de69bc4",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nazanin Dehghani",
      "Hassan Hajipoor",
      "Hadi Amiri"
    ]
  },
  "https://aclanthology.org/2021.acl-short.81": {
    "title": "Improving Compositional Generalization in Classification Tasks via Structure Annotations",
    "volume": "short",
    "abstract": "Compositional generalization is the ability to generalize systematically to a new data distribution by combining known components. Although humans seem to have a great ability to generalize compositionally, state-of-the-art neural models struggle to do so. In this work, we study compositional generalization in classification tasks and present two main contributions. First, we study ways to convert a natural language sequence-to-sequence dataset to a classification dataset that also requires compositional generalization. Second, we show that providing structural hints (specifically, providing parse trees and entity links as attention masks for a Transformer model) helps compositional generalization",
    "checked": true,
    "id": "523745e29f6cb1890f18352d449fd3597910c485",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Juyong Kim",
      "Pradeep Ravikumar",
      "Joshua Ainslie",
      "Santiago Ontanon"
    ]
  },
  "https://aclanthology.org/2021.acl-short.82": {
    "title": "Learning to Generate Task-Specific Adapters from Task Description",
    "volume": "short",
    "abstract": "Pre-trained text-to-text transformers such as BART have achieved impressive performance across a range of NLP tasks. Recent study further shows that they can learn to generalize to novel tasks, by including task descriptions as part of the source sequence and training the model with (source, target) examples. At test time, these fine-tuned models can make inferences on new tasks using the new task descriptions as part of the input. However, this approach has potential limitations, as the model learns to solve individual (source, target) examples (i.e., at the instance level), instead of learning to solve tasks by taking all examples within a task as a whole (i.e., at the task level). To this end, we introduce Hypter, a framework that improves text-to-text transformer’s generalization ability to unseen tasks by training a hypernetwork to generate task-specific, light-weight adapters from task descriptions. Experiments on ZEST dataset and a synthetic SQuAD dataset demonstrate that Hypter improves upon fine-tuning baselines. Notably, when using BART-Large as the main network, Hypter brings 11.3% comparative improvement on ZEST dataset",
    "checked": true,
    "id": "9e42cb2b133bc41600824a1002cb05844c6a46a9",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Qinyuan Ye",
      "Xiang Ren"
    ]
  },
  "https://aclanthology.org/2021.acl-short.83": {
    "title": "QA-Driven Zero-shot Slot Filling with Weak Supervision Pretraining",
    "volume": "short",
    "abstract": "Slot-filling is an essential component for building task-oriented dialog systems. In this work, we focus on the zero-shot slot-filling problem, where the model needs to predict slots and their values, given utterances from new domains without training on the target domain. Prior methods directly encode slot descriptions to generalize to unseen slot types. However, raw slot descriptions are often ambiguous and do not encode enough semantic information, limiting the models’ zero-shot capability. To address this problem, we introduce QA-driven slot filling (QASF), which extracts slot-filler spans from utterances with a span-based QA model. We use a linguistically motivated questioning strategy to turn descriptions into questions, allowing the model to generalize to unseen slot types. Moreover, our QASF model can benefit from weak supervision signals from QA pairs synthetically generated from unlabeled conversations. Our full system substantially outperforms baselines by over 5% on the SNIPS benchmark",
    "checked": true,
    "id": "525d5835b66612b11c2714645d92297b221e9daf",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Xinya Du",
      "Luheng He",
      "Qi Li",
      "Dian Yu",
      "Panupong Pasupat",
      "Yuan Zhang"
    ]
  },
  "https://aclanthology.org/2021.acl-short.84": {
    "title": "Domain-Adaptive Pretraining Methods for Dialogue Understanding",
    "volume": "short",
    "abstract": "Language models like BERT and SpanBERT pretrained on open-domain data have obtained impressive gains on various NLP tasks. In this paper, we probe the effectiveness of domain-adaptive pretraining objectives on downstream tasks. In particular, three objectives, including a novel objective focusing on modeling predicate-argument relations, are evaluated on two challenging dialogue understanding tasks. Experimental results demonstrate that domain-adaptive pretraining with proper objectives can significantly improve the performance of a strong baseline on these tasks, achieving the new state-of-the-art performances",
    "checked": true,
    "id": "9fd8fbc97890909869197a888cfc42bde48e94ec",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Han Wu",
      "Kun Xu",
      "Linfeng Song",
      "Lifeng Jin",
      "Haisong Zhang",
      "Linqi Song"
    ]
  },
  "https://aclanthology.org/2021.acl-short.85": {
    "title": "Targeting the Benchmark: On Methodology in Current Natural Language Processing Research",
    "volume": "short",
    "abstract": "It has become a common pattern in our field: One group introduces a language task, exemplified by a dataset, which they argue is challenging enough to serve as a benchmark. They also provide a baseline model for it, which then soon is improved upon by other groups. Often, research efforts then move on, and the pattern repeats itself. What is typically left implicit is the argumentation for why this constitutes progress, and progress towards what. In this paper, we try to step back for a moment from this pattern and work out possible argumentations and their parts",
    "checked": true,
    "id": "8e8c9e907f9be0c098d90eec387da88dd0111238",
    "semantic_title": "",
    "citation_count": 36,
    "authors": [
      "David Schlangen"
    ]
  },
  "https://aclanthology.org/2021.acl-short.86": {
    "title": "X-Fact: A New Benchmark Dataset for Multilingual Fact Checking",
    "volume": "short",
    "abstract": "In this work, we introduce : the largest publicly available multilingual dataset for factual verification of naturally existing real-world claims. The dataset contains short statements in 25 languages and is labeled for veracity by expert fact-checkers. The dataset includes a multilingual evaluation benchmark that measures both out-of-domain generalization, and zero-shot capabilities of the multilingual models. Using state-of-the-art multilingual transformer-based models, we develop several automated fact-checking models that, along with textual claims, make use of additional metadata and evidence from news stories retrieved using a search engine. Empirically, our best model attains an F-score of around 40%, suggesting that our dataset is a challenging benchmark for the evaluation of multilingual fact-checking models",
    "checked": true,
    "id": "378e2dc920f312fdb1c5ab095dfc2a0de9208199",
    "semantic_title": "",
    "citation_count": 34,
    "authors": [
      "Ashim Gupta",
      "Vivek Srikumar"
    ]
  },
  "https://aclanthology.org/2021.acl-short.87": {
    "title": "nmT5 - Is parallel data still relevant for pre-training massively multilingual language models?",
    "volume": "short",
    "abstract": "Recently, mT5 - a massively multilingual version of T5 - leveraged a unified text-to-text format to attain state-of-the-art results on a wide variety of multilingual NLP tasks. In this paper, we investigate the impact of incorporating parallel data into mT5 pre-training. We find that multi-tasking language modeling with objectives such as machine translation during pre-training is a straightforward way to improve performance on downstream multilingual and cross-lingual tasks. However, the gains start to diminish as the model capacity increases, suggesting that parallel data might not be as essential for larger models. At the same time, even at larger model sizes, we find that pre-training with parallel data still provides benefits in the limited labelled data regime",
    "checked": true,
    "id": "a1d578646cf42f2f69ee996742af484d03cc9121",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Mihir Kale",
      "Aditya Siddhant",
      "Rami Al-Rfou",
      "Linting Xue",
      "Noah Constant",
      "Melvin Johnson"
    ]
  },
  "https://aclanthology.org/2021.acl-short.88": {
    "title": "Question Generation for Adaptive Education",
    "volume": "short",
    "abstract": "Intelligent and adaptive online education systems aim to make high-quality education available for a diverse range of students. However, existing systems usually depend on a pool of hand-made questions, limiting how fine-grained and open-ended they can be in adapting to individual students. We explore targeted question generation as a controllable sequence generation task. We first show how to fine-tune pre-trained language models for deep knowledge tracing (LM-KT). This model accurately predicts the probability of a student answering a question correctly, and generalizes to questions not seen in training. We then use LM-KT to specify the objective and data for training a model to generate questions conditioned on the student and target difficulty. Our results show we succeed at generating novel, well-calibrated language translation questions for second language learners from a real online education platform",
    "checked": true,
    "id": "390f174d102c72172249254f3f1048721c0c3161",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Megha Srivastava",
      "Noah Goodman"
    ]
  },
  "https://aclanthology.org/2021.acl-short.89": {
    "title": "A Simple Recipe for Multilingual Grammatical Error Correction",
    "volume": "short",
    "abstract": "This paper presents a simple recipe to trainstate-of-the-art multilingual Grammatical Error Correction (GEC) models. We achieve this by first proposing a language-agnostic method to generate a large number of synthetic examples. The second ingredient is to use large-scale multilingual language models (up to 11B parameters). Once fine-tuned on language-specific supervised sets we surpass the previous state-of-the-art results on GEC benchmarks in four languages: English, Czech, German and Russian. Having established a new set of baselines for GEC, we make our results easily reproducible and accessible by releasing a CLANG-8 dataset. It is produced by using our best model, which we call gT5, to clean the targets of a widely used yet noisy Lang-8 dataset. cLang-8 greatly simplifies typical GEC training pipelines composed of multiple fine-tuning stages – we demonstrate that performing a single fine-tuning stepon cLang-8 with the off-the-shelf language models yields further accuracy improvements over an already top-performing gT5 model for English",
    "checked": true,
    "id": "c3661fc0091545c6b71f78141bdd2075614fe266",
    "semantic_title": "",
    "citation_count": 51,
    "authors": [
      "Sascha Rothe",
      "Jonathan Mallinson",
      "Eric Malmi",
      "Sebastian Krause",
      "Aliaksei Severyn"
    ]
  },
  "https://aclanthology.org/2021.acl-short.90": {
    "title": "Towards Visual Question Answering on Pathology Images",
    "volume": "short",
    "abstract": "Pathology imaging is broadly used for identifying the causes and effects of diseases or injuries. Given a pathology image, being able to answer questions about the clinical findings contained in the image is very important for medical decision making. In this paper, we aim to develop a pathological visual question answering framework to analyze pathology images and answer medical questions related to these images. To build such a framework, we create PathVQA, a VQA dataset with 32,795 questions asked from 4,998 pathology images. We also propose a three-level optimization framework which performs self-supervised pretraining and VQA finetuning end-to-end to learn powerful visual and textual representations jointly and automatically identifies and excludes noisy self-supervised examples from pretraining. We perform experiments on our created PathVQA dataset and the results demonstrate the effectiveness of our proposed methods. The datasets and code are available at https://github.com/UCSD-AI4H/PathVQA",
    "checked": true,
    "id": "2551990a1ccdffb1a4d1d9040b2d493ba6d26dd1",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Xuehai He",
      "Zhuo Cai",
      "Wenlan Wei",
      "Yichen Zhang",
      "Luntian Mou",
      "Eric Xing",
      "Pengtao Xie"
    ]
  },
  "https://aclanthology.org/2021.acl-short.91": {
    "title": "Efficient Text-based Reinforcement Learning by Jointly Leveraging State and Commonsense Graph Representations",
    "volume": "short",
    "abstract": "Text-based games (TBGs) have emerged as useful benchmarks for evaluating progress at the intersection of grounded language understanding and reinforcement learning (RL). Recent work has proposed the use of external knowledge to improve the efficiency of RL agents for TBGs. In this paper, we posit that to act efficiently in TBGs, an agent must be able to track the state of the game while retrieving and using relevant commonsense knowledge. Thus, we propose an agent for TBGs that induces a graph representation of the game state and jointly grounds it with a graph of commonsense knowledge from ConceptNet. This combination is achieved through bidirectional knowledge graph attention between the two symbolic representations. We show that agents that incorporate commonsense into the game state graph outperform baseline agents",
    "checked": true,
    "id": "027353b63d9bf423ff675d751ad7505c3fb53614",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Keerthiram Murugesan",
      "Mattia Atzeni",
      "Pavan Kapanipathi",
      "Kartik Talamadupula",
      "Mrinmaya Sachan",
      "Murray Campbell"
    ]
  },
  "https://aclanthology.org/2021.acl-short.92": {
    "title": "mTVR: Multilingual Moment Retrieval in Videos",
    "volume": "short",
    "abstract": "We introduce mTVR, a large-scale multilingual video moment retrieval dataset, containing 218K English and Chinese queries from 21.8K TV show video clips. The dataset is collected by extending the popular TVR dataset (in English) with paired Chinese queries and subtitles. Compared to existing moment retrieval datasets, mTVR is multilingual, larger, and comes with diverse annotations. We further propose mXML, a multilingual moment retrieval model that learns and operates on data from both languages, via encoder parameter sharing and language neighborhood constraints. We demonstrate the effectiveness of mXML on the newly collected mTVR dataset, where mXML outperforms strong monolingual baselines while using fewer parameters. In addition, we also provide detailed dataset analyses and model ablations. Data and code are publicly available at https://github.com/jayleicn/mTVRetrieval",
    "checked": true,
    "id": "5eb4dc9b012286f22724bafff0edd3dacff09fc6",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Jie Lei",
      "Tamara Berg",
      "Mohit Bansal"
    ]
  },
  "https://aclanthology.org/2021.acl-short.93": {
    "title": "Explicitly Capturing Relations between Entity Mentions via Graph Neural Networks for Domain-specific Named Entity Recognition",
    "volume": "short",
    "abstract": "Named entity recognition (NER) is well studied for the general domain, and recent systems have achieved human-level performance for identifying common entity types. However, the NER performance is still moderate for specialized domains that tend to feature complicated contexts and jargonistic entity types. To address these challenges, we propose explicitly connecting entity mentions based on both global coreference relations and local dependency relations for building better entity mention representations. In our experiments, we incorporate entity mention relations by Graph Neural Networks and show that our system noticeably improves the NER performance on two datasets from different domains. We further show that the proposed lightweight system can effectively elevate the NER performance to a higher level even when only a tiny amount of labeled data is available, which is desirable for domain-specific NER",
    "checked": true,
    "id": "626576bcda2404c328c8848331e0b34f64394571",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Pei Chen",
      "Haibo Ding",
      "Jun Araki",
      "Ruihong Huang"
    ]
  },
  "https://aclanthology.org/2021.acl-short.94": {
    "title": "Improving Lexically Constrained Neural Machine Translation with Source-Conditioned Masked Span Prediction",
    "volume": "short",
    "abstract": "Accurate terminology translation is crucial for ensuring the practicality and reliability of neural machine translation (NMT) systems. To address this, lexically constrained NMT explores various methods to ensure pre-specified words and phrases appear in the translation output. However, in many cases, those methods are studied on general domain corpora, where the terms are mostly uni- and bi-grams (>98%). In this paper, we instead tackle a more challenging setup consisting of domain-specific corpora with much longer n-gram and highly specialized terms. Inspired by the recent success of masked span prediction models, we propose a simple and effective training strategy that achieves consistent improvements on both terminology and sentence-level translation for three domain-specific corpora in two language pairs",
    "checked": true,
    "id": "cbc0a2ddb8df70ae7e68b20038dfb8274f84b850",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Gyubok Lee",
      "Seongjun Yang",
      "Edward Choi"
    ]
  },
  "https://aclanthology.org/2021.acl-short.95": {
    "title": "Quotation Recommendation and Interpretation Based on Transformation from Queries to Quotations",
    "volume": "short",
    "abstract": "To help individuals express themselves better, quotation recommendation is receiving growing attention. Nevertheless, most prior efforts focus on modeling quotations and queries separately and ignore the relationship between the quotations and the queries. In this work, we introduce a transformation matrix that directly maps the query representations to quotation representations. To better learn the mapping relationship, we employ a mapping loss that minimizes the distance of two semantic spaces (one for quotation and another for mapped-query). Furthermore, we explore using the words in history queries to interpret the figurative language of quotations, where quotation-aware attention is applied on top of history queries to highlight the indicator words. Experiments on two datasets in English and Chinese show that our model outperforms previous state-of-the-art models",
    "checked": true,
    "id": "64a16a16a869d4396c658c32293647f28168f7e0",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Lingzhi Wang",
      "Xingshan Zeng",
      "Kam-Fai Wong"
    ]
  },
  "https://aclanthology.org/2021.acl-short.96": {
    "title": "Pre-training is a Hot Topic: Contextualized Document Embeddings Improve Topic Coherence",
    "volume": "short",
    "abstract": "Topic models extract groups of words from documents, whose interpretation as a topic hopefully allows for a better understanding of the data. However, the resulting word groups are often not coherent, making them harder to interpret. Recently, neural topic models have shown improvements in overall coherence. Concurrently, contextual embeddings have advanced the state of the art of neural models in general. In this paper, we combine contextualized representations with neural topic models. We find that our approach produces more meaningful and coherent topics than traditional bag-of-words topic models and recent neural models. Our results indicate that future improvements in language models will translate into better topic models",
    "checked": true,
    "id": "07d85a6c8a31f43ee6e128f7ef0a1bf1494567cd",
    "semantic_title": "",
    "citation_count": 119,
    "authors": [
      "Federico Bianchi",
      "Silvia Terragni",
      "Dirk Hovy"
    ]
  },
  "https://aclanthology.org/2021.acl-short.97": {
    "title": "Input Representations for Parsing Discourse Representation Structures: Comparing English with Chinese",
    "volume": "short",
    "abstract": "Neural semantic parsers have obtained acceptable results in the context of parsing DRSs (Discourse Representation Structures). In particular models with character sequences as input showed remarkable performance for English. But how does this approach perform on languages with a different writing system, like Chinese, a language with a large vocabulary of characters? Does rule-based tokenisation of the input help, and which granularity is preferred: characters, or words? The results are promising. Even with DRSs based on English, good results for Chinese are obtained. Tokenisation offers a small advantage for English, but not for Chinese. Overall, characters are preferred as input, both for English and Chinese",
    "checked": true,
    "id": "d7040675dcd91157a4fbf4f8e4f34a93eec81f29",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Chunliu Wang",
      "Rik van Noord",
      "Arianna Bisazza",
      "Johan Bos"
    ]
  },
  "https://aclanthology.org/2021.acl-short.98": {
    "title": "Code Generation from Natural Language with Less Prior Knowledge and More Monolingual Data",
    "volume": "short",
    "abstract": "Training datasets for semantic parsing are typically small due to the higher expertise required for annotation than most other NLP tasks. As a result, models for this application usually need additional prior knowledge to be built into the architecture or algorithm. The increased dependency on human experts hinders automation and raises the development and maintenance costs in practice. This work investigates whether a generic transformer-based seq2seq model can achieve competitive performance with minimal code-generation-specific inductive bias design. By exploiting a relatively sizeable monolingual corpus of the target programming language, which is cheap to mine from the web, we achieved 81.03% exact match accuracy on Django and 32.57 BLEU score on CoNaLa. Both are SOTA to the best of our knowledge. This positive evidence highlights a potentially easier path toward building accurate semantic parsers in practice",
    "checked": true,
    "id": "b029346bfa85fa2fc7a12498ae5f018922ce55f9",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Sajad Norouzi",
      "Keyi Tang",
      "Yanshuai Cao"
    ]
  },
  "https://aclanthology.org/2021.acl-short.99": {
    "title": "Issues with Entailment-based Zero-shot Text Classification",
    "volume": "short",
    "abstract": "The general format of natural language inference (NLI) makes it tempting to be used for zero-shot text classification by casting any target label into a sentence of hypothesis and verifying whether or not it could be entailed by the input, aiming at generic classification applicable on any specified label space. In this opinion piece, we point out a few overlooked issues that are yet to be discussed in this line of work. We observe huge variance across different classification datasets amongst standard BERT-based NLI models and surprisingly find that pre-trained BERT without any fine-tuning can yield competitive performance against BERT fine-tuned for NLI. With the concern that these models heavily rely on spurious lexical patterns for prediction, we also experiment with preliminary approaches for more robust NLI, but the results are in general negative. Our observations reveal implicit but challenging difficulties in entailment-based zero-shot text classification",
    "checked": true,
    "id": "5a3826bfed12bbff27f8dd701928d741e5d3d960",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Tingting Ma",
      "Jin-Ge Yao",
      "Chin-Yew Lin",
      "Tiejun Zhao"
    ]
  },
  "https://aclanthology.org/2021.acl-short.100": {
    "title": "Neural-Symbolic Commonsense Reasoner with Relation Predictors",
    "volume": "short",
    "abstract": "Commonsense reasoning aims to incorporate sets of commonsense facts, retrieved from Commonsense Knowledge Graphs (CKG), to draw conclusion about ordinary situations. The dynamic nature of commonsense knowledge postulates models capable of performing multi-hop reasoning over new situations. This feature also results in having large-scale sparse Knowledge Graphs, where such reasoning process is needed to predict relations between new events. However, existing approaches in this area are limited by considering CKGs as a limited set of facts, thus rendering them unfit for reasoning over new unseen situations and events. In this paper, we present a neural-symbolic reasoner, which is capable of reasoning over large-scale dynamic CKGs. The logic rules for reasoning over CKGs are learned during training by our model. In addition to providing interpretable explanation, the learned logic rules help to generalise prediction to newly introduced events. Experimental results on the task of link prediction on CKGs prove the effectiveness of our model by outperforming the state-of-the-art models",
    "checked": true,
    "id": "513e6d3ac68f170feebaee7c9b53a1b8e9533773",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Farhad Moghimifar",
      "Lizhen Qu",
      "Terry Yue Zhuo",
      "Gholamreza Haffari",
      "Mahsa Baktashmotlagh"
    ]
  },
  "https://aclanthology.org/2021.acl-short.101": {
    "title": "What Motivates You? Benchmarking Automatic Detection of Basic Needs from Short Posts",
    "volume": "short",
    "abstract": "According to the self-determination theory, the levels of satisfaction of three basic needs (competence, autonomy and relatedness) have implications on people’s everyday life and career. We benchmark the novel task of automatically detecting those needs on short posts in English, by modelling it as a ternary classification task, and as three binary classification tasks. A detailed manual analysis shows that the latter has advantages in the real-world scenario, and that our best models achieve similar performances as a trained human annotator",
    "checked": true,
    "id": "b6bffec600168834ae031491796e20b2feb1702f",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanja Stajner",
      "Seren Yenikent",
      "Bilal Ghanem",
      "Marc Franco-Salvador"
    ]
  },
  "https://aclanthology.org/2021.acl-short.102": {
    "title": "Semantic Frame Induction using Masked Word Embeddings and Two-Step Clustering",
    "volume": "short",
    "abstract": "Recent studies on semantic frame induction show that relatively high performance has been achieved by using clustering-based methods with contextualized word embeddings. However, there are two potential drawbacks to these methods: one is that they focus too much on the superficial information of the frame-evoking verb and the other is that they tend to divide the instances of the same verb into too many different frame clusters. To overcome these drawbacks, we propose a semantic frame induction method using masked word embeddings and two-step clustering. Through experiments on the English FrameNet data, we demonstrate that using the masked word embeddings is effective for avoiding too much reliance on the surface information of frame-evoking verbs and that two-step clustering can improve the number of resulting frame clusters for the instances of the same verb",
    "checked": true,
    "id": "765976a7035bb93b2cb58e1d86fc731b5fd0ae48",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Kosuke Yamada",
      "Ryohei Sasano",
      "Koichi Takeda"
    ]
  },
  "https://aclanthology.org/2021.acl-short.103": {
    "title": "Lightweight Adapter Tuning for Multilingual Speech Translation",
    "volume": "short",
    "abstract": "Adapter modules were recently introduced as an efficient alternative to fine-tuning in NLP. Adapter tuning consists in freezing pre-trained parameters of a model and injecting lightweight modules between layers, resulting in the addition of only a small number of task-specific trainable parameters. While adapter tuning was investigated for multilingual neural machine translation, this paper proposes a comprehensive analysis of adapters for multilingual speech translation (ST). Starting from different pre-trained models (a multilingual ST trained on parallel data or a multilingual BART (mBART) trained on non parallel multilingual data), we show that adapters can be used to: (a) efficiently specialize ST to specific language pairs with a low extra cost in terms of parameters, and (b) transfer from an automatic speech recognition (ASR) task and an mBART pre-trained model to a multilingual ST task. Experiments show that adapter tuning offer competitive results to full fine-tuning, while being much more parameter-efficient",
    "checked": true,
    "id": "eacb5dc57a167aeda3b23c28abfc2b51095f1b7c",
    "semantic_title": "",
    "citation_count": 40,
    "authors": [
      "Hang Le",
      "Juan Pino",
      "Changhan Wang",
      "Jiatao Gu",
      "Didier Schwab",
      "Laurent Besacier"
    ]
  },
  "https://aclanthology.org/2021.acl-short.104": {
    "title": "Parameter Selection: Why We Should Pay More Attention to It",
    "volume": "short",
    "abstract": "The importance of parameter selection in supervised learning is well known. However, due to the many parameter combinations, an incomplete or an insufficient procedure is often applied. This situation may cause misleading or confusing conclusions. In this opinion paper, through an intriguing example we point out that the seriousness goes beyond what is generally recognized. In the topic of multilabel classification for medical code prediction, one influential paper conducted a proper parameter selection on a set, but when moving to a subset of frequently occurring labels, the authors used the same parameters without a separate tuning. The set of frequent labels became a popular benchmark in subsequent studies, which kept pushing the state of the art. However, we discovered that most of the results in these studies cannot surpass the approach in the original paper if a parameter tuning had been conducted at the time. Thus it is unclear how much progress the subsequent developments have actually brought. The lesson clearly indicates that without enough attention on parameter selection, the research progress in our field can be uncertain or even illusive",
    "checked": true,
    "id": "3762b6b18f78d6e67fd4570812274994b889bd5d",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Jie-Jyun Liu",
      "Tsung-Han Yang",
      "Si-An Chen",
      "Chih-Jen Lin"
    ]
  },
  "https://aclanthology.org/2021.acl-short.105": {
    "title": "Distinct Label Representations for Few-Shot Text Classification",
    "volume": "short",
    "abstract": "Few-shot text classification aims to classify inputs whose label has only a few examples. Previous studies overlooked the semantic relevance between label representations. Therefore, they are easily confused by labels that are relevant. To address this problem, we propose a method that generates distinct label representations that embed information specific to each label. Our method is applicable to conventional few-shot classification models. Experimental results show that our method significantly improved the performance of few-shot text classification across models and datasets",
    "checked": true,
    "id": "4b4788db41d21f531465524c582b2a65fa2d33c7",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Sora Ohashi",
      "Junya Takayama",
      "Tomoyuki Kajiwara",
      "Yuki Arase"
    ]
  },
  "https://aclanthology.org/2021.acl-short.106": {
    "title": "Learning to Solve NLP Tasks in an Incremental Number of Languages",
    "volume": "short",
    "abstract": "In real scenarios, a multilingual model trained to solve NLP tasks on a set of languages can be required to support new languages over time. Unfortunately, the straightforward retraining on a dataset containing annotated examples for all the languages is both expensive and time-consuming, especially when the number of target languages grows. Moreover, the original annotated material may no longer be available due to storage or business constraints. Re-training only with the new language data will inevitably result in Catastrophic Forgetting of previously acquired knowledge. We propose a Continual Learning strategy that updates a model to support new languages over time, while maintaining consistent results on previously learned languages. We define a Teacher-Student framework where the existing model “teaches” to a student model its knowledge about the languages it supports, while the student is also trained on a new language. We report an experimental evaluation in several tasks including Sentence Classification, Relational Learning and Sequence Labeling",
    "checked": true,
    "id": "448c377dfd28ceb9c1e55dbdc55872fb6e34ba57",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Giuseppe Castellucci",
      "Simone Filice",
      "Danilo Croce",
      "Roberto Basili"
    ]
  },
  "https://aclanthology.org/2021.acl-short.107": {
    "title": "Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling",
    "volume": "short",
    "abstract": "Transformer is important for text modeling. However, it has difficulty in handling long documents due to the quadratic complexity with input text length. In order to handle this problem, we propose a hierarchical interactive Transformer (Hi-Transformer) for efficient and effective long document modeling. Hi-Transformer models documents in a hierarchical way, i.e., first learns sentence representations and then learns document representations. It can effectively reduce the complexity and meanwhile capture global document context in the modeling of each sentence. More specifically, we first use a sentence Transformer to learn the representations of each sentence. Then we use a document Transformer to model the global document context from these sentence representations. Next, we use another sentence Transformer to enhance sentence modeling using the global document context. Finally, we use hierarchical pooling method to obtain document embedding. Extensive experiments on three benchmark datasets validate the efficiency and effectiveness of Hi-Transformer in long document modeling",
    "checked": true,
    "id": "84daddd294fa3cc12596b5785f81c2a153d2fb1d",
    "semantic_title": "",
    "citation_count": 24,
    "authors": [
      "Chuhan Wu",
      "Fangzhao Wu",
      "Tao Qi",
      "Yongfeng Huang"
    ]
  },
  "https://aclanthology.org/2021.acl-short.108": {
    "title": "Robust Transfer Learning with Pretrained Language Models through Adapters",
    "volume": "short",
    "abstract": "Transfer learning with large pretrained transformer-based language models like BERT has become a dominating approach for most NLP tasks. Simply fine-tuning those large language models on downstream tasks or combining it with task-specific pretraining is often not robust. In particular, the performance considerably varies as the random seed changes or the number of pretraining and/or fine-tuning iterations varies, and the fine-tuned model is vulnerable to adversarial attack. We propose a simple yet effective adapter-based approach to mitigate these issues. Specifically, we insert small bottleneck layers (i.e., adapter) within each layer of a pretrained model, then fix the pretrained layers and train the adapter layers on the downstream task data, with (1) task-specific unsupervised pretraining and then (2) task-specific supervised training (e.g., classification, sequence labeling). Our experiments demonstrate that such a training scheme leads to improved stability and adversarial robustness in transfer learning to various downstream tasks",
    "checked": true,
    "id": "cdcffb2f1678d7252bfd9b902d3cd676a5217005",
    "semantic_title": "",
    "citation_count": 27,
    "authors": [
      "Wenjuan Han",
      "Bo Pang",
      "Ying Nian Wu"
    ]
  },
  "https://aclanthology.org/2021.acl-short.109": {
    "title": "Embracing Ambiguity: Shifting the Training Target of NLI Models",
    "volume": "short",
    "abstract": "Natural Language Inference (NLI) datasets contain examples with highly ambiguous labels. While many research works do not pay much attention to this fact, several recent efforts have been made to acknowledge and embrace the existence of ambiguity, such as UNLI and ChaosNLI. In this paper, we explore the option of training directly on the estimated label distribution of the annotators in the NLI task, using a learning loss based on this ambiguity distribution instead of the gold-labels. We prepare AmbiNLI, a trial dataset obtained from readily available sources, and show it is possible to reduce ChaosNLI divergence scores when finetuning on this data, a promising first step towards learning how to capture linguistic ambiguity. Additionally, we show that training on the same amount of data but targeting the ambiguity distribution instead of gold-labels can result in models that achieve higher performance and learn better representations for downstream tasks",
    "checked": true,
    "id": "fb51dc284e42927d018858fcc6618d16cbdfc042",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Johannes Mario Meissner",
      "Napat Thumwanit",
      "Saku Sugawara",
      "Akiko Aizawa"
    ]
  },
  "https://aclanthology.org/2021.acl-short.110": {
    "title": "Modeling Discriminative Representations for Out-of-Domain Detection with Supervised Contrastive Learning",
    "volume": "short",
    "abstract": "Detecting Out-of-Domain (OOD) or unknown intents from user queries is essential in a task-oriented dialog system. A key challenge of OOD detection is to learn discriminative semantic features. Traditional cross-entropy loss only focuses on whether a sample is correctly classified, and does not explicitly distinguish the margins between categories. In this paper, we propose a supervised contrastive learning objective to minimize intra-class variance by pulling together in-domain intents belonging to the same class and maximize inter-class variance by pushing apart samples from different classes. Besides, we employ an adversarial augmentation mechanism to obtain pseudo diverse views of a sample in the latent space. Experiments on two public datasets prove the effectiveness of our method capturing discriminative representations for OOD detection",
    "checked": true,
    "id": "8c54e2bc0dde115a00886cb026a04daff3f4b4de",
    "semantic_title": "",
    "citation_count": 34,
    "authors": [
      "Zhiyuan Zeng",
      "Keqing He",
      "Yuanmeng Yan",
      "Zijun Liu",
      "Yanan Wu",
      "Hong Xu",
      "Huixing Jiang",
      "Weiran Xu"
    ]
  },
  "https://aclanthology.org/2021.acl-short.111": {
    "title": "Preview, Attend and Review: Schema-Aware Curriculum Learning for Multi-Domain Dialogue State Tracking",
    "volume": "short",
    "abstract": "Existing dialog state tracking (DST) models are trained with dialog data in a random order, neglecting rich structural information in a dataset. In this paper, we propose to use curriculum learning (CL) to better leverage both the curriculum structure and schema structure for task-oriented dialogs. Specifically, we propose a model-agnostic framework called Schema-aware Curriculum Learning for Dialog State Tracking (SaCLog), which consists of a preview module that pre-trains a DST model with schema information, a curriculum module that optimizes the model with CL, and a review module that augments mispredicted data to reinforce the CL training. We show that our proposed approach improves DST performance over both a transformer-based and RNN-based DST model (TripPy and TRADE) and achieves new state-of-the-art results on WOZ2.0 and MultiWOZ2.1",
    "checked": true,
    "id": "eadaf893e56adc383e07c119fefe85a2d1b9832c",
    "semantic_title": "",
    "citation_count": 36,
    "authors": [
      "Yinpei Dai",
      "Hangyu Li",
      "Yongbin Li",
      "Jian Sun",
      "Fei Huang",
      "Luo Si",
      "Xiaodan Zhu"
    ]
  },
  "https://aclanthology.org/2021.acl-short.112": {
    "title": "On the Generation of Medical Dialogs for COVID-19",
    "volume": "short",
    "abstract": "Under the pandemic of COVID-19, people experiencing COVID19-related symptoms have a pressing need to consult doctors. Because of the shortage of medical professionals, many people cannot receive online consultations timely. To address this problem, we aim to develop a medical dialog system that can provide COVID19-related consultations. We collected two dialog datasets – CovidDialog – (in English and Chinese respectively) containing conversations between doctors and patients about COVID-19. While the largest of their kind, these two datasets are still relatively small compared with general-domain dialog datasets. Training complex dialog generation models on small datasets bears high risk of overfitting. To alleviate overfitting, we develop a multi-task learning approach, which regularizes the data-deficient dialog generation task with a masked token prediction task. Experiments on the CovidDialog datasets demonstrate the effectiveness of our approach. We perform both human evaluation and automatic evaluation of dialogs generated by our method. Results show that the generated responses are promising in being doctor-like, relevant to conversation history, clinically informative and correct. The code and the data are available at https://github.com/UCSD-AI4H/COVID-Dialogue",
    "checked": true,
    "id": "c6304247f04870393d56579cdd5e093a25c121ee",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Meng Zhou",
      "Zechen Li",
      "Bowen Tan",
      "Guangtao Zeng",
      "Wenmian Yang",
      "Xuehai He",
      "Zeqian Ju",
      "Subrato Chakravorty",
      "Shu Chen",
      "Xingyi Yang",
      "Yichen Zhang",
      "Qingyang Wu",
      "Zhou Yu",
      "Kun Xu",
      "Eric Xing",
      "Pengtao Xie"
    ]
  },
  "https://aclanthology.org/2021.acl-short.113": {
    "title": "Constructing Multi-Modal Dialogue Dataset by Replacing Text with Semantically Relevant Images",
    "volume": "short",
    "abstract": "In multi-modal dialogue systems, it is important to allow the use of images as part of a multi-turn conversation. Training such dialogue systems generally requires a large-scale dataset consisting of multi-turn dialogues that involve images, but such datasets rarely exist. In response, this paper proposes a 45k multi-modal dialogue dataset created with minimal human intervention. Our method to create such a dataset consists of (1) preparing and pre-processing text dialogue datasets, (2) creating image-mixed dialogues by using a text-to-image replacement technique, and (3) employing a contextual-similarity-based filtering step to ensure the contextual coherence of the dataset. To evaluate the validity of our dataset, we devise a simple retrieval model for dialogue sentence prediction tasks. Automatic metrics and human evaluation results on such tasks show that our dataset can be effectively used as training data for multi-modal dialogue systems which require an understanding of images and text in a context-aware manner. Our dataset and generation code is available at https://github.com/shh1574/multi-modal-dialogue-dataset",
    "checked": true,
    "id": "8f31038de5cadc3171735c0410511c044d216463",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Nyoungwoo Lee",
      "Suwon Shin",
      "Jaegul Choo",
      "Ho-Jin Choi",
      "Sung-Hyon Myaeng"
    ]
  },
  "https://aclanthology.org/2021.acl-short.114": {
    "title": "Exposing the limits of Zero-shot Cross-lingual Hate Speech Detection",
    "volume": "short",
    "abstract": "Reducing and counter-acting hate speech on Social Media is a significant concern. Most of the proposed automatic methods are conducted exclusively on English and very few consistently labeled, non-English resources have been proposed. Learning to detect hate speech on English and transferring to unseen languages seems an immediate solution. This work is the first to shed light on the limits of this zero-shot, cross-lingual transfer learning framework for hate speech detection. We use benchmark data sets in English, Italian, and Spanish to detect hate speech towards immigrants and women. Investigating post-hoc explanations of the model, we discover that non-hateful, language-specific taboo interjections are misinterpreted as signals of hate speech. Our findings demonstrate that zero-shot, cross-lingual models cannot be used as they are, but need to be carefully designed",
    "checked": true,
    "id": "f683f1d7c3366f3c9fd967540112900c6918387a",
    "semantic_title": "",
    "citation_count": 36,
    "authors": [
      "Debora Nozza"
    ]
  },
  "https://aclanthology.org/2021.acl-short.115": {
    "title": "BERTTune: Fine-Tuning Neural Machine Translation with BERTScore",
    "volume": "short",
    "abstract": "Neural machine translation models are often biased toward the limited translation references seen during training. To amend this form of overfitting, in this paper we propose fine-tuning the models with a novel training objective based on the recently-proposed BERTScore evaluation metric. BERTScore is a scoring function based on contextual embeddings that overcomes the typical limitations of n-gram-based metrics (e.g. synonyms, paraphrases), allowing translations that are different from the references, yet close in the contextual embedding space, to be treated as substantially correct. To be able to use BERTScore as a training objective, we propose three approaches for generating soft predictions, allowing the network to remain completely differentiable end-to-end. Experiments carried out over four, diverse language pairs show improvements of up to 0.58 pp (3.28%) in BLEU score and up to 0.76 pp (0.98%) in BERTScore (F_BERT) when fine-tuning a strong baseline",
    "checked": true,
    "id": "75decd34aeed7057ab3962be5310ec2faa7ec9ba",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Inigo Jauregi Unanue",
      "Jacob Parnell",
      "Massimo Piccardi"
    ]
  },
  "https://aclanthology.org/2021.acl-short.116": {
    "title": "Entity Enhancement for Implicit Discourse Relation Classification in the Biomedical Domain",
    "volume": "short",
    "abstract": "Implicit discourse relation classification is a challenging task, in particular when the text domain is different from the standard Penn Discourse Treebank (PDTB; Prasad et al., 2008) training corpus domain (Wall Street Journal in 1990s). We here tackle the task of implicit discourse relation classification on the biomedical domain, for which the Biomedical Discourse Relation Bank (BioDRB; Prasad et al., 2011) is available. We show that entity information can be used to improve discourse relational argument representation. In a first step, we show that explicitly marked instances that are content-wise similar to the target relations can be used to achieve good performance in the cross-domain setting using a simple unsupervised voting pipeline. As a further step, we show that with the linked entity information from the first step, a transformer which is augmented with entity-related information (KBERT; Liu et al., 2020) sets the new state of the art performance on the dataset, outperforming the large pre-trained BioBERT (Lee et al., 2020) model by 2% points",
    "checked": true,
    "id": "956a9ea85bda349eff58588c1ad64015560414e0",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Wei Shi",
      "Vera Demberg"
    ]
  },
  "https://aclanthology.org/2021.acl-short.117": {
    "title": "Unsupervised Pronoun Resolution via Masked Noun-Phrase Prediction",
    "volume": "short",
    "abstract": "In this work, we propose Masked Noun-Phrase Prediction (MNPP), a pre-training strategy to tackle pronoun resolution in a fully unsupervised setting. Firstly, We evaluate our pre-trained model on various pronoun resolution datasets without any finetuning. Our method outperforms all previous unsupervised methods on all datasets by large margins. Secondly, we proceed to a few-shot setting where we finetune our pre-trained model on WinoGrande-S and XS separately. Our method outperforms RoBERTa-large baseline with large margins, meanwhile, achieving a higher AUC score after further finetuning on the remaining three official splits of WinoGrande",
    "checked": true,
    "id": "7064225b8736e4687d1d0ad37ee3e0c88b780b3e",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Ming Shen",
      "Pratyay Banerjee",
      "Chitta Baral"
    ]
  },
  "https://aclanthology.org/2021.acl-short.118": {
    "title": "Addressing Semantic Drift in Generative Question Answering with Auxiliary Extraction",
    "volume": "short",
    "abstract": "Recently, question answering (QA) based on machine reading comprehension has become popular. This work focuses on generative QA which aims to generate an abstractive answer to a given question instead of extracting an answer span from a provided passage. Generative QA often suffers from two critical problems: (1) summarizing content irrelevant to a given question, (2) drifting away from a correct answer during generation. In this paper, we address these problems by a novel Rationale-Enriched Answer Generator (REAG), which incorporates an extractive mechanism into a generative model. Specifically, we add an extraction task on the encoder to obtain the rationale for an answer, which is the most relevant piece of text in an input document to a given question. Based on the extracted rationale and original input, the decoder is expected to generate an answer with high confidence. We jointly train REAG on the MS MARCO QA+NLG task and the experimental results show that REAG improves the quality and semantic accuracy of answers over baseline models",
    "checked": true,
    "id": "c61641ee5849efb5bafc786f2fd3e976e19f81c8",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Chenliang Li",
      "Bin Bi",
      "Ming Yan",
      "Wei Wang",
      "Songfang Huang"
    ]
  },
  "https://aclanthology.org/2021.acl-short.119": {
    "title": "Demoting the Lead Bias in News Summarization via Alternating Adversarial Learning",
    "volume": "short",
    "abstract": "In news articles the lead bias is a common phenomenon that usually dominates the learning signals for neural extractive summarizers, severely limiting their performance on data with different or even no bias. In this paper, we introduce a novel technique to demote lead bias and make the summarizer focus more on the content semantics. Experiments on two news corpora with different degrees of lead bias show that our method can effectively demote the model’s learned lead bias and improve its generality on out-of-distribution data, with little to no performance loss on in-distribution data",
    "checked": true,
    "id": "ffd2e261889a56c7b54d03679f8827ebb118c1fb",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Linzi Xing",
      "Wen Xiao",
      "Giuseppe Carenini"
    ]
  },
  "https://aclanthology.org/2021.acl-short.120": {
    "title": "DuReader_robust: A Chinese Dataset Towards Evaluating Robustness and Generalization of Machine Reading Comprehension in Real-World Applications",
    "volume": "short",
    "abstract": "Machine reading comprehension (MRC) is a crucial task in natural language processing and has achieved remarkable advancements. However, most of the neural MRC models are still far from robust and fail to generalize well in real-world applications. In order to comprehensively verify the robustness and generalization of MRC models, we introduce a real-world Chinese dataset – DuReader_robust . It is designed to evaluate the MRC models from three aspects: over-sensitivity, over-stability and generalization. Comparing to previous work, the instances in DuReader_robust are natural texts, rather than the altered unnatural texts. It presents the challenges when applying MRC models to real-world applications. The experimental results show that MRC models do not perform well on the challenge test set. Moreover, we analyze the behavior of existing models on the challenge test set, which may provide suggestions for future model development. The dataset and codes are publicly available at https://github.com/baidu/DuReader",
    "checked": true,
    "id": "7718a9b2b33bc6b9c1d4c4b9a3e1d473ffe5c330",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Hongxuan Tang",
      "Hongyu Li",
      "Jing Liu",
      "Yu Hong",
      "Hua Wu",
      "Haifeng Wang"
    ]
  },
  "https://aclanthology.org/2021.acl-short.121": {
    "title": "Sequence to General Tree: Knowledge-Guided Geometry Word Problem Solving",
    "volume": "short",
    "abstract": "With the recent advancements in deep learning, neural solvers have gained promising results in solving math word problems. However, these SOTA solvers only generate binary expression trees that contain basic arithmetic operators and do not explicitly use the math formulas. As a result, the expression trees they produce are lengthy and uninterpretable because they need to use multiple operators and constants to represent one single formula. In this paper, we propose sequence-to-general tree (S2G) that learns to generate interpretable and executable operation trees where the nodes can be formulas with an arbitrary number of arguments. With nodes now allowed to be formulas, S2G can learn to incorporate mathematical domain knowledge into problem-solving, making the results more interpretable. Experiments show that S2G can achieve a better performance against strong baselines on problems that require domain knowledge",
    "checked": true,
    "id": "dd75f713420cd8eb3e0b3ce870b2680cd93b39fd",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Shih-hung Tsai",
      "Chao-Chun Liang",
      "Hsin-Min Wang",
      "Keh-Yih Su"
    ]
  },
  "https://aclanthology.org/2021.acl-short.122": {
    "title": "Multi-Scale Progressive Attention Network for Video Question Answering",
    "volume": "short",
    "abstract": "Understanding the multi-scale visual information in a video is essential for Video Question Answering (VideoQA). Therefore, we propose a novel Multi-Scale Progressive Attention Network (MSPAN) to achieve relational reasoning between cross-scale video information. We construct clips of different lengths to represent different scales of the video. Then, the clip-level features are aggregated into node features by using max-pool, and a graph is generated for each scale of clips. For cross-scale feature interaction, we design a message passing strategy between adjacent scale graphs, i.e., top-down scale interaction and bottom-up scale interaction. Under the question’s guidance of progressive attention, we realize the fusion of all-scale video features. Experimental evaluations on three benchmarks: TGIF-QA, MSVD-QA and MSRVTT-QA show our method has achieved state-of-the-art performance",
    "checked": true,
    "id": "d27d44ed1caa7f973f9fedb11878e4b5ab452320",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Zhicheng Guo",
      "Jiaxuan Zhao",
      "Licheng Jiao",
      "Xu Liu",
      "Lingling Li"
    ]
  },
  "https://aclanthology.org/2021.acl-short.123": {
    "title": "Efficient Passage Retrieval with Hashing for Open-domain Question Answering",
    "volume": "short",
    "abstract": "Most state-of-the-art open-domain question answering systems use a neural retrieval model to encode passages into continuous vectors and extract them from a knowledge source. However, such retrieval models often require large memory to run because of the massive size of their passage index. In this paper, we introduce Binary Passage Retriever (BPR), a memory-efficient neural retrieval model that integrates a learning-to-hash technique into the state-of-the-art Dense Passage Retriever (DPR) to represent the passage index using compact binary codes rather than continuous vectors. BPR is trained with a multi-task objective over two tasks: efficient candidate generation based on binary codes and accurate reranking based on continuous vectors. Compared with DPR, BPR substantially reduces the memory cost from 65GB to 2GB without a loss of accuracy on two standard open-domain question answering benchmarks: Natural Questions and TriviaQA. Our code and trained models are available at https://github.com/studio-ousia/bpr",
    "checked": true,
    "id": "15493bbb5387d60e7c77cee34528acd3acae7b65",
    "semantic_title": "",
    "citation_count": 51,
    "authors": [
      "Ikuya Yamada",
      "Akari Asai",
      "Hannaneh Hajishirzi"
    ]
  },
  "https://aclanthology.org/2021.acl-short.124": {
    "title": "Entity Concept-enhanced Few-shot Relation Extraction",
    "volume": "short",
    "abstract": "Few-shot relation extraction (FSRE) is of great importance in long-tail distribution problem, especially in special domain with low-resource data. Most existing FSRE algorithms fail to accurately classify the relations merely based on the information of the sentences together with the recognized entity pairs, due to limited samples and lack of knowledge. To address this problem, in this paper, we proposed a novel entity CONCEPT-enhanced FEw-shot Relation Extraction scheme (ConceptFERE), which introduces the inherent concepts of entities to provide clues for relation prediction and boost the relations classification performance. Firstly, a concept-sentence attention module is developed to select the most appropriate concept from multiple concepts of each entity by calculating the semantic similarity between sentences and concepts. Secondly, a self-attention based fusion module is presented to bridge the gap of concept embedding and sentence embedding from different semantic spaces. Extensive experiments on the FSRE benchmark dataset FewRel have demonstrated the effectiveness and the superiority of the proposed ConceptFERE scheme as compared to the state-of-the-art baselines. Code is available at https://github.com/LittleGuoKe/ConceptFERE",
    "checked": true,
    "id": "4e9e5d7430b2dbea903fc8d49ff34c0dc043472e",
    "semantic_title": "",
    "citation_count": 25,
    "authors": [
      "Shan Yang",
      "Yongfei Zhang",
      "Guanglin Niu",
      "Qinghua Zhao",
      "Shiliang Pu"
    ]
  },
  "https://aclanthology.org/2021.acl-short.125": {
    "title": "Improving Model Generalization: A Chinese Named Entity Recognition Case Study",
    "volume": "short",
    "abstract": "Generalization is an important ability that helps to ensure that a machine learning model can perform well on unseen data. In this paper, we study the effect of data bias on model generalization, using Chinese Named Entity Recognition (NER) as a case study. Specifically, we analyzed five benchmarking datasets for Chinese NER, and observed the following two types of data bias that can compromise model generalization ability. Firstly, the test sets of all the five datasets contain a significant proportion of entities that have been seen in the training sets. Such test data would therefore not be able to reflect the true generalization ability of a model. Secondly, all datasets are dominated by a few fat-head entities, i.e., entities appearing with particularly high frequency. As a result, a model might be able to produce high prediction accuracy simply by keyword memorization without leveraging context knowledge. To address these data biases, we first refine each test set by excluding seen entities from it, so as to better evaluate a model’s generalization ability. Then, we propose a simple yet effective entity resampling method to make entities within the same category distributed equally, encouraging a model to leverage both name and context knowledge in the training process. Experimental results demonstrate that the proposed entity resampling method significantly improves a model’s ability in detecting unseen entities, especially for company, organization and position categories",
    "checked": true,
    "id": "2f34693c39aaca2617db8965f82515003e359ebf",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Guanqing Liang",
      "Cane Wing-Ki Leung"
    ]
  },
  "https://aclanthology.org/2021.acl-short.126": {
    "title": "Three Sentences Are All You Need: Local Path Enhanced Document Relation Extraction",
    "volume": "short",
    "abstract": "Document-level Relation Extraction (RE) is a more challenging task than sentence RE as it often requires reasoning over multiple sentences. Yet, human annotators usually use a small number of sentences to identify the relationship between a given entity pair. In this paper, we present an embarrassingly simple but effective method to heuristically select evidence sentences for document-level RE, which can be easily combined with BiLSTM to achieve good performance on benchmark datasets, even better than fancy graph neural network based methods. We have released our code at https://github.com/AndrewZhe/Three-Sentences-Are-All-You-Need",
    "checked": true,
    "id": "026f955701579ba6ae0caf972c90729daf5a5e9e",
    "semantic_title": "",
    "citation_count": 17,
    "authors": [
      "Quzhe Huang",
      "Shengqi Zhu",
      "Yansong Feng",
      "Yuan Ye",
      "Yuxuan Lai",
      "Dongyan Zhao"
    ]
  },
  "https://aclanthology.org/2021.acl-short.127": {
    "title": "Unsupervised Cross-Domain Prerequisite Chain Learning using Variational Graph Autoencoders",
    "volume": "short",
    "abstract": "Learning prerequisite chains is an important task for one to pick up knowledge efficiently in both known and unknown domains. For example, one may be an expert in the natural language processing (NLP) domain, but want to determine the best order in which to learn new concepts in an unfamiliar Computer Vision domain (CV). Both domains share some common concepts, such as machine learning basics and deep learning models. In this paper, we solve the task of unsupervised cross-domain concept prerequisite chain learning, using an optimized variational graph autoencoder. Our model learns to transfer concept prerequisite relations from an information-rich domain (source domain) to an information-poor domain (target domain), substantially surpassing other baseline models. In addition, we expand an existing dataset by introducing two new domains—-CV and Bioinformatics (BIO). The annotated data and resources as well as the code will be made publicly available",
    "checked": true,
    "id": "3316bc75e1d46e58008e89f109da1fb2f6b6efb4",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Irene Li",
      "Vanessa Yan",
      "Tianxiao Li",
      "Rihao Qu",
      "Dragomir Radev"
    ]
  },
  "https://aclanthology.org/2021.acl-short.128": {
    "title": "Attentive Multiview Text Representation for Differential Diagnosis",
    "volume": "short",
    "abstract": "We present a text representation approach that can combine different views (representations) of the same input through effective data fusion and attention strategies for ranking purposes. We apply our model to the problem of differential diagnosis, which aims to find the most probable diseases that match with clinical descriptions of patients, using data from the Undiagnosed Diseases Network. Our model outperforms several ranking approaches (including a commercially-supported system) by effectively prioritizing and combining representations obtained from traditional and recent text representation techniques. We elaborate on several aspects of our model and shed light on its improved performance",
    "checked": true,
    "id": "3843f69718a4052da1ec93e83b6a0f5508a69e2e",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Hadi Amiri",
      "Mitra Mohtarami",
      "Isaac Kohane"
    ]
  },
  "https://aclanthology.org/2021.acl-short.129": {
    "title": "MedNLI Is Not Immune: Natural Language Inference Artifacts in the Clinical Domain",
    "volume": "short",
    "abstract": "Crowdworker-constructed natural language inference (NLI) datasets have been found to contain statistical artifacts associated with the annotation process that allow hypothesis-only classifiers to achieve better-than-random performance (CITATION). We investigate whether MedNLI, a physician-annotated dataset with premises extracted from clinical notes, contains such artifacts (CITATION). We find that entailed hypotheses contain generic versions of specific concepts in the premise, as well as modifiers related to responsiveness, duration, and probability. Neutral hypotheses feature conditions and behaviors that co-occur with, or cause, the condition(s) in the premise. Contradiction hypotheses feature explicit negation of the premise and implicit negation via assertion of good health. Adversarial filtering demonstrates that performance degrades when evaluated on the difficult subset. We provide partition information and recommendations for alternative dataset construction strategies for knowledge-intensive domains",
    "checked": true,
    "id": "b9c0d572402d38976d4bf99c78f4cc65e6d8f1e4",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Christine Herlihy",
      "Rachel Rudinger"
    ]
  },
  "https://aclanthology.org/2021.acl-short.130": {
    "title": "Towards a more Robust Evaluation for Conversational Question Answering",
    "volume": "short",
    "abstract": "With the explosion of chatbot applications, Conversational Question Answering (CQA) has generated a lot of interest in recent years. Among proposals, reading comprehension models which take advantage of the conversation history (previous QA) seem to answer better than those which only consider the current question. Nevertheless, we note that the CQA evaluation protocol has a major limitation. In particular, models are allowed, at each turn of the conversation, to access the ground truth answers of the previous turns. Not only does this severely prevent their applications in fully autonomous chatbots, it also leads to unsuspected biases in their behavior. In this paper, we highlight this effect and propose new tools for evaluation and training in order to guard against the noted issues. The new results that we bring come to reinforce methods of the current state of the art",
    "checked": true,
    "id": "06f71821237da3d3b4289618846fe4fab71eca82",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Wissam Siblini",
      "Baris Sayil",
      "Yacine Kessaci"
    ]
  },
  "https://aclanthology.org/2021.acl-short.131": {
    "title": "VAULT: VAriable Unified Long Text Representation for Machine Reading Comprehension",
    "volume": "short",
    "abstract": "Existing models on Machine Reading Comprehension (MRC) require complex model architecture for effectively modeling long texts with paragraph representation and classification, thereby making inference computationally inefficient for production use. In this work, we propose VAULT: a light-weight and parallel-efficient paragraph representation for MRC based on contextualized representation from long document input, trained using a new Gaussian distribution-based objective that pays close attention to the partially correct instances that are close to the ground-truth. We validate our VAULT architecture showing experimental results on two benchmark MRC datasets that require long context modeling; one Wikipedia-based (Natural Questions (NQ)) and the other on TechNotes (TechQA). VAULT can achieve comparable performance on NQ with a state-of-the-art (SOTA) complex document modeling approach while being 16 times faster, demonstrating the efficiency of our proposed model. We also demonstrate that our model can also be effectively adapted to a completely different domain – TechQA – with large improvement over a model fine-tuned on a previously published large PLM",
    "checked": true,
    "id": "a14ef90b8fa01b3ddc231eb491c76a6f7458976e",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Haoyang Wen",
      "Anthony Ferritto",
      "Heng Ji",
      "Radu Florian",
      "Avi Sil"
    ]
  },
  "https://aclanthology.org/2021.acl-short.132": {
    "title": "Avoiding Overlap in Data Augmentation for AMR-to-Text Generation",
    "volume": "short",
    "abstract": "Leveraging additional unlabeled data to boost model performance is common practice in machine learning and natural language processing. For generation tasks, if there is overlap between the additional data and the target text evaluation data, then training on the additional data is training on answers of the test set. This leads to overly-inflated scores with the additional data compared to real-world testing scenarios and problems when comparing models. We study the AMR dataset and Gigaword, which is popularly used for improving AMR-to-text generators, and find significant overlap between Gigaword and a subset of the AMR dataset. We propose methods for excluding parts of Gigaword to remove this overlap, and show that our approach leads to a more realistic evaluation of the task of AMR-to-text generation. Going forward, we give simple best-practice recommendations for leveraging additional data in AMR-to-text generation",
    "checked": true,
    "id": "2e386384fe19600ff130210708c996104bebab67",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Wenchao Du",
      "Jeffrey Flanigan"
    ]
  },
  "https://aclanthology.org/2021.acl-short.133": {
    "title": "Weakly-Supervised Methods for Suicide Risk Assessment: Role of Related Domains",
    "volume": "short",
    "abstract": "Social media has become a valuable resource for the study of suicidal ideation and the assessment of suicide risk. Among social media platforms, Reddit has emerged as the most promising one due to its anonymity and its focus on topic-based communities (subreddits) that can be indicative of someone’s state of mind or interest regarding mental health disorders such as r/SuicideWatch, r/Anxiety, r/depression. A challenge for previous work on suicide risk assessment has been the small amount of labeled data. We propose an empirical investigation into several classes of weakly-supervised approaches, and show that using pseudo-labeling based on related issues around mental health (e.g., anxiety, depression) helps improve model performance for suicide risk assessment",
    "checked": true,
    "id": "1b460f0733a5569280b8e25a89820f32b2a0d687",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Chenghao Yang",
      "Yudong Zhang",
      "Smaranda Muresan"
    ]
  },
  "https://aclanthology.org/2021.acl-short.134": {
    "title": "Can Transformer Models Measure Coherence In Text: Re-Thinking the Shuffle Test",
    "volume": "short",
    "abstract": "The Shuffle Test is the most common task to evaluate whether NLP models can measure coherence in text. Most recent work uses direct supervision on the task; we show that by simply finetuning a RoBERTa model, we can achieve a near perfect accuracy of 97.8%, a state-of-the-art. We argue that this outstanding performance is unlikely to lead to a good model of text coherence, and suggest that the Shuffle Test should be approached in a Zero-Shot setting: models should be evaluated without being trained on the task itself. We evaluate common models in this setting, such as Generative and Bi-directional Transformers, and find that larger architectures achieve high-performance out-of-the-box. Finally, we suggest the k-Block Shuffle Test, a modification of the original by increasing the size of blocks shuffled. Even though human reader performance remains high (around 95% accuracy), model performance drops from 94% to 78% as block size increases, creating a conceptually simple challenge to benchmark NLP models",
    "checked": true,
    "id": "f3bb8cef420df35456d3abc7194ddcc0dd5439fb",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Philippe Laban",
      "Luke Dai",
      "Lucas Bandarkar",
      "Marti A. Hearst"
    ]
  },
  "https://aclanthology.org/2021.acl-short.135": {
    "title": "SimCLS: A Simple Framework for Contrastive Learning of Abstractive Summarization",
    "volume": "short",
    "abstract": "In this paper, we present a conceptually simple while empirically powerful framework for abstractive summarization, SimCLS, which can bridge the gap between the learning objective and evaluation metrics resulting from the currently dominated sequence-to-sequence learning framework by formulating text generation as a reference-free evaluation problem (i.e., quality estimation) assisted by contrastive learning. Experimental results show that, with minor modification over existing top-scoring systems, SimCLS can improve the performance of existing top-performing models by a large margin. Particularly, 2.51 absolute improvement against BART and 2.50 over PEGASUS w.r.t ROUGE-1 on the CNN/DailyMail dataset, driving the state-of-the-art performance to a new level. We have open-sourced our codes and results: https://github.com/yixinL7/SimCLS. Results of our proposed models have been deployed into ExplainaBoard platform, which allows researchers to understand our systems in a more fine-grained way",
    "checked": true,
    "id": "b31eb3428320342dfde042693ff2ca106dabed0d",
    "semantic_title": "",
    "citation_count": 112,
    "authors": [
      "Yixin Liu",
      "Pengfei Liu"
    ]
  },
  "https://aclanthology.org/2021.acl-short.136": {
    "title": "SaRoCo: Detecting Satire in a Novel Romanian Corpus of News Articles",
    "volume": "short",
    "abstract": "In this work, we introduce a corpus for satire detection in Romanian news. We gathered 55,608 public news articles from multiple real and satirical news sources, composing one of the largest corpora for satire detection regardless of language and the only one for the Romanian language. We provide an official split of the text samples, such that training news articles belong to different sources than test news articles, thus ensuring that models do not achieve high performance simply due to overfitting. We conduct experiments with two state-of-the-art deep neural models, resulting in a set of strong baselines for our novel corpus. Our results show that the machine-level accuracy for satire detection in Romanian is quite low (under 73% on the test set) compared to the human-level accuracy (87%), leaving enough room for improvement in future research",
    "checked": true,
    "id": "c0498f3bbba3bca5f23934b61704b028e7dc1705",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Ana-Cristina Rogoz",
      "Gaman Mihaela",
      "Radu Tudor Ionescu"
    ]
  },
  "https://aclanthology.org/2021.acl-short.137": {
    "title": "Bringing Structure into Summaries: a Faceted Summarization Dataset for Long Scientific Documents",
    "volume": "short",
    "abstract": "Faceted summarization provides briefings of a document from different perspectives. Readers can quickly comprehend the main points of a long document with the help of a structured outline. However, little research has been conducted on this subject, partially due to the lack of large-scale faceted summarization datasets. In this study, we present FacetSum, a faceted summarization benchmark built on Emerald journal articles, covering a diverse range of domains. Different from traditional document-summary pairs, FacetSum provides multiple summaries, each targeted at specific sections of a long document, including the purpose, method, findings, and value. Analyses and empirical results on our dataset reveal the importance of bringing structure into summaries. We believe FacetSum will spur further advances in summarization research and foster the development of NLP systems that can leverage the structured information in both long texts and summaries",
    "checked": true,
    "id": "77a25c1577610c0edb2024c902fb2c6610228d9a",
    "semantic_title": "",
    "citation_count": 17,
    "authors": [
      "Rui Meng",
      "Khushboo Thaker",
      "Lei Zhang",
      "Yue Dong",
      "Xingdi Yuan",
      "Tong Wang",
      "Daqing He"
    ]
  },
  "https://aclanthology.org/2021.acl-short.138": {
    "title": "Replicating and Extending \"Because Their Treebanks Leak\": Graph Isomorphism, Covariants, and Parser Performance",
    "volume": "short",
    "abstract": "Søgaard (2020) obtained results suggesting the fraction of trees occurring in the test data isomorphic to trees in the training set accounts for a non-trivial variation in parser performance. Similar to other statistical analyses in NLP, the results were based on evaluating linear regressions. However, the study had methodological issues and was undertaken using a small sample size leading to unreliable results. We present a replication study in which we also bin sentences by length and find that only a small subset of sentences vary in performance with respect to graph isomorphism. Further, the correlation observed between parser performance and graph isomorphism in the wild disappears when controlling for covariants. However, in a controlled experiment, where covariants are kept fixed, we do observe a correlation. We suggest that conclusions drawn from statistical analyses like this need to be tempered and that controlled experiments can complement them by more readily teasing factors apart",
    "checked": true,
    "id": "10102684ef60e2fa06ea0cfa3d034303ad7dfacc",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Mark Anderson",
      "Anders Søgaard",
      "Carlos Gómez-Rodríguez"
    ]
  },
  "https://aclanthology.org/2021.acl-short.139": {
    "title": "Don't Rule Out Monolingual Speakers: A Method For Crowdsourcing Machine Translation Data",
    "volume": "short",
    "abstract": "High-performing machine translation (MT) systems can help overcome language barriers while making it possible for everyone to communicate and use language technologies in the language of their choice. However, such systems require large amounts of parallel sentences for training, and translators can be difficult to find and expensive. Here, we present a data collection strategy for MT which, in contrast, is cheap and simple, as it does not require bilingual speakers. Based on the insight that humans pay specific attention to movements, we use graphics interchange formats (GIFs) as a pivot to collect parallel sentences from monolingual annotators. We use our strategy to collect data in Hindi, Tamil and English. As a baseline, we also collect data using images as a pivot. We perform an intrinsic evaluation by manually evaluating a subset of the sentence pairs and an extrinsic evaluation by finetuning mBART (Liu et al., 2020) on the collected data. We find that sentences collected via GIFs are indeed of higher quality",
    "checked": true,
    "id": "0a54d2fe19878a3381767be62382a9799717934d",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rajat Bhatnagar",
      "Ananya Ganesh",
      "Katharina Kann"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.1": {
    "title": "Explainable Inference Over Grounding-Abstract Chains for Science Questions",
    "volume": "findings",
    "abstract": "We propose an explainable inference approach for science questions by reasoning on grounding and abstract inference chains. This paper frames question answering as a natural language abductive reasoning problem, constructing plausible explanations for each candidate answer and then selecting the candidate with the best explanation as the final answer. Our method, ExplanationLP, elicits explanations by constructing a weighted graph of relevant facts for each candidate answer and employs a linear programming formalism designed to select the optimal subgraph of explanatory facts. The graphs' weighting function is composed of a set of parameters targeting relevance, cohesion and diversity, which we finetune for answer selection via Bayesian Optimisation. We carry out our experiments on the WorldTree and ARC-Challenge datasets to empirically demonstrate the following contributions: (1) ExplanationLP obtains strong performance when compared to transformer-based and multi-hop approaches despite having a significantly lower number of parameters; (2) We show that our model is able to generate plausible explanations for answer prediction; (3) Our model demonstrates better robustness towards semantic drift when compared to transformerbased and multi-hop approaches",
    "checked": true,
    "id": "9e54e3848697c49f4ea76c5998a3ae3820671bab",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Mokanarangan Thayaparan",
      "Marco Valentino",
      "André Freitas"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.2": {
    "title": "LV-BERT: Exploiting Layer Variety for BERT",
    "volume": "findings",
    "abstract": "Modern pre-trained language models are mostly built upon backbones stacking self-attention and feed-forward layers in an interleaved order. In this paper, beyond this stereotyped layer pattern, we aim to improve pre-trained models by exploiting layer variety from two aspects: the layer type set and the layer order. Specifically, besides the original self-attention and feed-forward layers, we introduce convolution into the layer type set, which is experimentally found beneficial to pre-trained models. Furthermore, beyond the original interleaved order, we explore more layer orders to discover more powerful architectures. However, the introduced layer variety leads to a large architecture space of more than billions of candidates, while training a single candidate model from scratch already requires huge computation cost, making it not affordable to search such a space by directly training large amounts of candidate models. To solve this problem, we first pre-train a supernet from which the weights of all candidate models can be inherited, and then adopt an evolutionary algorithm guided by pre-training accuracy to find the optimal architecture. Extensive experiments show that LV-BERT model obtained by our method outperforms BERT and its variants on various downstream tasks. For example, LV-BERT-small achieves 79.8 on the GLUE testing set, 1.8 higher than the strong baseline ELECTRA-small",
    "checked": true,
    "id": "400bbf81e1a30ca35ccb39bef2d25a3c2c28c532",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weihao Yu",
      "Zihang Jiang",
      "Fei Chen",
      "Qibin Hou",
      "Jiashi Feng"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.3": {
    "title": "Few-Shot Event Detection with Prototypical Amortized Conditional Random Field",
    "volume": "findings",
    "abstract": "Event Detection, a fundamental task of Information Extraction, tends to struggle when it needs to recognize novel event types with a few samples, i.e. Few-Shot Event Detection (FSED). Previous identify-then-classify paradigm attempts to solve this problem in the pipeline manner but ignores the trigger discrepancy between event types, thus suffering from the error propagation. In this paper, we present a novel unified joint model which converts the task to a few-shot tagging problem with a double-part tagging scheme. To this end, we first design the Prototypical Amortized Conditional Random Field (PA-CRF) to model the label dependency in the few-shot scenario, which builds prototypical amortization networks to approximate the transition scores between labels based on the label prototypes. Then Gaussian distribution is introduced for the modeling of the transition scores in PA-CRF to alleviate the uncertain estimation resulting from insufficient data. We conduct experiments on the benchmark dataset FewEvent and the experimental results show that the tagging based methods are better than existing pipeline and joint learning methods. In addition, the proposed PA-CRF achieves the best results on the public dataset",
    "checked": true,
    "id": "53bb3924925503986948bd3872efecf77f795a6a",
    "semantic_title": "",
    "citation_count": 30,
    "authors": [
      "Xin Cong",
      "Shiyao Cui",
      "Bowen Yu",
      "Tingwen Liu",
      "Wang Yubin",
      "Bin Wang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.4": {
    "title": "LUX (Linguistic aspects Under eXamination): Discourse Analysis for Automatic Fake News Classification",
    "volume": "findings",
    "abstract": "The democratization/decentralization of both the production and consumption of information has resulted in a subjective and often misleading depiction of facts known as Fake News a phenomenon that is effectively shaping the perception of reality for many individuals. Manual fact-checking is time-consuming and cannot scale and although automatic factchecking, vis a vis machine learning holds promise, it is significantly hindered by a deficit of suitable training data. We present both a novel dataset, VERITAS(VERIfying Textual Aspects), a collection of fact-checked claims, containing their original documents and LUX(Language Under eXamination), a text classifier that makes use of an extensive linguistic analysis to infer the likelihood of the input being a piece of fake-news",
    "checked": true,
    "id": "e3c45dde86be18463b89e4880c715e84befe929e",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Lucas Azevedo",
      "Mathieu d’Aquin",
      "Brian Davis",
      "Manel Zarrouk"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.5": {
    "title": "Diagnosing Transformers in Task-Oriented Semantic Parsing",
    "volume": "findings",
    "abstract": "Modern task-oriented semantic parsing approaches typically use seq2seq transformers to map textual utterances to semantic frames comprised of intents and slots. While these models are empirically strong, their specific strengths and weaknesses have largely remained unexplored. In this work, we study BART (Lewis et al., 2020) and XLM-R (Conneau et al., 2020), two state-of-the-art parsers, across both monolingual and multilingual settings. Our experiments yield several key results: transformer-based parsers struggle not only with disambiguating intents/slots, but surprisingly also with producing syntacticallyvalid frames. Though pre-training imbues transformers with syntactic inductive biases, we find the ambiguity of copying utterance spans into frames often leads to tree invalidity, indicating span extraction is a major bottleneck for current parsers. However, as a silver lining, we show transformer-based parsers give sufficient indicators for whether a frame is likely to be correct or incorrect, making them easier to deploy in production settings",
    "checked": true,
    "id": "73395837a22eb00ce4106be3653460377bed7725",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Shrey Desai",
      "Ahmed Aly"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.6": {
    "title": "Semantic Relation-aware Difference Representation Learning for Change Captioning",
    "volume": "findings",
    "abstract": "Change captioning is to describe the difference in a pair of images with a natural language sentence. In this task, the distractors, such as the illumination or viewpoint change, bring the huge challenges about learning the difference representation. In this paper, we propose a semantic relation-aware difference representation learning network to explicitly learn the difference representation in the existence of distractors. Specifically, we introduce a selfsemantic relation embedding block to explore the underlying changed objects and design a cross-semantic relation measuring block to localize the real change and learn the discriminative difference representation. Besides, relying on the POS of words, we devise an attentionbased visual switch to dynamically use visual information for caption generation. Extensive experiments show that our method achieves the state-of-the-art performances on CLEVRChange and Spot-the-Diff datasets 1",
    "checked": true,
    "id": "bce036f82d266c9781d2cbf64efc190679d4769e",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Yunbin Tu",
      "Tingting Yao",
      "Liang Li",
      "Jiedong Lou",
      "Shengxiang Gao",
      "Zhengtao Yu",
      "Chenggang Yan"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.7": {
    "title": "The Authors Matter: Understanding and Mitigating Implicit Bias in Deep Text Classification",
    "volume": "findings",
    "abstract": "It is evident that deep text classification models trained on human data could be biased. In particular, they produce biased outcomes for texts that explicitly include identity terms of certain demographic groups. We refer to this type of bias as explicit bias, which has been extensively studied. However, deep text classification models can also produce biased outcomes for texts written by authors of certain demographic groups. We refer to such bias as implicit bias of which we still have a rather limited understanding. In this paper, we first demonstrate that implicit bias exists in different text classification tasks for different demographic groups. Then, we build a learningbased interpretation method to deepen our knowledge of implicit bias. Specifically, we verify that classifiers learn to make predictions based on language features that are related to the demographic attributes of the authors. Next, we propose a framework Debiased-TC to train deep text classifiers to make predictions on the right features and consequently mitigate implicit bias. We conduct extensive experiments on three real-world datasets. The results show that the text classification models trained under our proposed framework outperform traditional models significantly in terms of fairness, and also slightly in terms of classification performance",
    "checked": true,
    "id": "07f4920a4a120a4a8644804904d0f5cc215b8ff8",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Haochen Liu",
      "Wei Jin",
      "Hamid Karimi",
      "Zitao Liu",
      "Jiliang Tang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.8": {
    "title": "From What to Why: Improving Relation Extraction with Rationale Graph",
    "volume": "findings",
    "abstract": "Which type of information affects the existing neural relation extraction (RE) models to make correct decisions is an important question. In this paper, we observe that entity type and trigger are the most indicative information for RE in each instance. Moreover, these indicative clues are always constrained to co-occur with specific relations at the corpus level. Motivated by this, we propose a novel RAtionale Graph (RAG) to organize such co-occurrence constraints among entity types, triggers and relations in a holistic graph view. By introducing two subtasks of entity type prediction and trigger labeling, we build the connection between each instance and RAG, and then leverage relevant global co-occurrence knowledge stored in the graph to improve the performance of neural RE models. Extensive experimental results indicate that our method outperforms strong baselines significantly and achieves state-ofthe-art performance on the document-level and sentence-level RE benchmarks",
    "checked": true,
    "id": "d3fcae983f6fa3536b67542c17ae8cc1ba825490",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Zhenyu Zhang",
      "Bowen Yu",
      "Xiaobo Shu",
      "Xue Mengge",
      "Tingwen Liu",
      "Li Guo"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.9": {
    "title": "More Parameters? No Thanks!",
    "volume": "findings",
    "abstract": "This work studies the long-standing problems of model capacity and negative interference in multilingual neural machine translation (MNMT). We use network pruning techniques and observe that pruning 50-70% of the parameters from a trained MNMT model results only in a 0.29-1.98 drop in the BLEU score. Suggesting that there exist large redundancies in MNMT models. These observations motivate us to use the redundant parameters and counter the interference problem efficiently. We propose a novel adaptation strategy, where we iteratively prune and retrain the redundant parameters of an MNMT to improve bilingual representations while retaining the multilinguality. Negative interference severely affects high resource languages, and our method alleviates it without any additional adapter modules. Hence, we call it parameterfree adaptation strategy, paving way for the efficient adaptation of MNMT. We demonstrate the effectiveness of our method on a 9 language MNMT trained on TED talks, and report an average improvement of +1.36 on high resource pairs. Code will be released here",
    "checked": true,
    "id": "348af8247f81d56cae1d9b00a69e242056b171e9",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeeshan Khan",
      "Kartheek Akella",
      "Vinay Namboodiri",
      "C V Jawahar"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.10": {
    "title": "SyGNS: A Systematic Generalization Testbed Based on Natural Language Semantics",
    "volume": "findings",
    "abstract": "Recently, deep neural networks (DNNs) have achieved great success in semantically challenging NLP tasks, yet it remains unclear whether DNN models can capture compositional meanings, those aspects of meaning that have been long studied in formal semantics. To investigate this issue, we propose a Systematic Generalization testbed based on Natural language Semantics (SyGNS), whose challenge is to map natural language sentences to multiple forms of scoped meaning representations, designed to account for various semantic phenomena. Using SyGNS, we test whether neural networks can systematically parse sentences involving novel combinations of logical expressions such as quantifiers and negation. Experiments show that Transformer and GRU models can generalize to unseen combinations of quantifiers, negations, and modifiers that are similar to given training instances in form, but not to the others. We also find that the generalization performance to unseen combinations is better when the form of meaning representations is simpler. The data and code for SyGNS are publicly available at https: //github.com/verypluming/SyGNS",
    "checked": true,
    "id": "577d44a10b424a55165a6bf4839bafce2c695302",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Hitomi Yanaka",
      "Koji Mineshima",
      "Kentaro Inui"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.11": {
    "title": "Fully Non-autoregressive Neural Machine Translation: Tricks of the Trade",
    "volume": "findings",
    "abstract": "Fully non-autoregressive neural machine translation (NAT) simultaneously predicts tokens with single forward of neural networks, which significantly reduces the inference latency at the expense of quality drop compared to the Transformer baseline. In this work, we target on closing the performance gap while maintaining the latency advantage. We first inspect the fundamental issues of fully NAT models, and adopt dependency reduction in the learning space of output tokens as the primary guidance. Then, we revisit methods in four different aspects that have been proven effective for improving NAT models, and carefully combine these techniques with necessary modifications. Our extensive experiments on three translation benchmarks show that the proposed system achieves the state-of-the-art results for fully NAT models, and obtains comparable performance with the autoregressive and iterative NAT systems. For instance, one of the proposed models achieves 27.49 BLEU points on WMT14 En-De with 16.5× speed-up compared to similar sized autoregressive baseline under the same inference condition. The implementation of our model is available here1",
    "checked": true,
    "id": "1784fa1b5ac0b9f9fefe5e0508f91033f5952177",
    "semantic_title": "",
    "citation_count": 74,
    "authors": [
      "Jiatao Gu",
      "Xiang Kong"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.12": {
    "title": "Generate, Prune, Select: A Pipeline for Counterspeech Generation against Online Hate Speech",
    "volume": "findings",
    "abstract": "Warning: this paper contains content that may be offensive or upsetting. Countermeasures to effectively fight the ever increasing hate speech online without blocking freedom of speech is of great social interest. Natural Language Generation (NLG), is uniquely capable of developing scalable solutions. However, off-the-shelf NLG methods are primarily sequence-to-sequence neural models and they are limited in that they generate commonplace, repetitive and safe responses regardless of the hate speech (e.g., \"Please refrain from using such language.\") or irrelevant responses, making them ineffective for de-escalating hateful conversations. In this paper, we design a three-module pipeline approach to effectively improve the diversity and relevance. Our proposed pipeline first generates various counterspeech candidates by a generative model to promote diversity, then filters the ungrammatical ones using a BERT model, and finally selects the most relevant counterspeech response using a novel retrievalbased method. Extensive Experiments on three representative datasets demonstrate the efficacy of our approach in generating diverse and relevant counterspeech",
    "checked": true,
    "id": "1c2b4dac511e37333948d0a2686755ea0ed24ce0",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Wanzheng Zhu",
      "Suma Bhat"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.13": {
    "title": "REPT: Bridging Language Models and Machine Reading Comprehension via Retrieval-Based Pre-training",
    "volume": "findings",
    "abstract": "Pre-trained Language Models (PLMs) have achieved great success on Machine Reading Comprehension (MRC) over the past few years. Although the general language representation learned from large-scale corpora does benefit MRC, the poor support in evidence extraction which requires reasoning across multiple sentences hinders PLMs from further advancing MRC. To bridge the gap between general PLMs and MRC, we present REPT, a REtrieval-based Pre-Training approach. In particular, we introduce two self-supervised tasks to strengthen evidence extraction during pre-training, which is further inherited by downstream MRC tasks through the consistent retrieval operation and model architecture. To evaluate our proposed method, we conduct extensive experiments on five MRC datasets that require collecting evidence from and reasoning across multiple sentences. Experimental results demonstrate the effectiveness of our pre-training approach. Moreover, further analysis shows that our approach is able to enhance the capacity of evidence extraction without explicit supervision.1",
    "checked": true,
    "id": "246fd63312287e3b629d6baa49c8842123d5a7a9",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Fangkai Jiao",
      "Yangyang Guo",
      "Yilin Niu",
      "Feng Ji",
      "Feng-Lin Li",
      "Liqiang Nie"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.14": {
    "title": "CasEE: A Joint Learning Framework with Cascade Decoding for Overlapping Event Extraction",
    "volume": "findings",
    "abstract": "Event extraction (EE) is a crucial information extraction task that aims to extract event information in texts. Most existing methods assume that events appear in sentences without overlaps, which are not applicable to the complicated overlapping event extraction. This work systematically studies the realistic event overlapping problem, where a word may serve as triggers with several types or arguments with different roles. To tackle the above problem, we propose a novel joint learning framework with cascade decoding for overlapping event extraction, termed as CasEE. Particularly, CasEE sequentially performs type detection, trigger extraction and argument extraction, where the overlapped targets are extracted separately conditioned on the specific former prediction. All the subtasks are jointly learned in a framework to capture dependencies among the subtasks. The evaluation on a public event extraction benchmark FewFC demonstrates that CasEE1 achieves significant improvements on overlapping event extraction over previous competitive methods",
    "checked": true,
    "id": "5cd842abb03293e5a3d5023a71f4fc0133b01ae8",
    "semantic_title": "",
    "citation_count": 18,
    "authors": [
      "Jiawei Sheng",
      "Shu Guo",
      "Bowen Yu",
      "Qian Li",
      "Yiming Hei",
      "Lihong Wang",
      "Tingwen Liu",
      "Hongbo Xu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.15": {
    "title": "Discovering Topics in Long-tailed Corpora with Causal Intervention",
    "volume": "findings",
    "abstract": "Topic models are effective in capturing the latent semantics of large-scale textual data while existing methods are normally designed and evaluated on balanced corpora. However, it contradicts the fact that general corpora in our world are naturally long-tailed, and the longtailed bias can highly impair the topic modeling performance. Therefore, in this paper, we propose a causal inference framework to explain and overcome the issues of topic modeling on long-tailed corpora. In a neat and elegant way, causal intervention is applied in training to take out the influence brought by the long-tailed bias. Extensive experiments on manually constructed and naturally collected datasets demonstrate that our model can mitigate the bias effect, greatly improve topic quality and better discover the hidden semantics on the tail",
    "checked": true,
    "id": "a9ee79cda5108f6e927dea5dafa197c9aaf3463d",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Xiaobao Wu",
      "Chunping Li",
      "Yishu Miao"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.16": {
    "title": "More than just Frequency? Demasking Unsupervised Hypernymy Prediction Methods",
    "volume": "findings",
    "abstract": "This paper presents a comparison of unsupervised methods of hypernymy prediction (i.e., to predict which word in a pair of words such as fish–cod is the hypernym and which the hyponym). Most importantly, we demonstrate across datasets for English and for German that the predictions of three methods (WeedsPrec, invCL, SLQS Row) strongly overlap and are highly correlated with frequency-based predictions. In contrast, the second-order method SLQS shows an overall lower accuracy but makes correct predictions where the others go wrong. Our study once more confirms the general need to check the frequency bias of a computational method in order to identify frequency-(un)related effects",
    "checked": true,
    "id": "6f5908d26a5c6928b414a71cdaaf5019668d77eb",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Thomas Bott",
      "Dominik Schlechtweg",
      "Sabine Schulte im Walde"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.17": {
    "title": "WikiTableT: A Large-Scale Data-to-Text Dataset for Generating Wikipedia Article Sections",
    "volume": "findings",
    "abstract": "Datasets for data-to-text generation typically focus either on multi-domain, single-sentence generation or on single-domain, long-form generation. In this work, we cast generating Wikipedia sections as a data-to-text generation task and create a large-scale dataset, WIKITABLET, that pairs Wikipedia sections with their corresponding tabular data and various metadata. WIKITABLET contains millions of instances, covering a broad range of topics, as well as a variety of flavors of generation tasks with different levels of flexibility. We benchmark several training and decoding strategies on WIKITABLET. Our qualitative analysis shows that the best approaches can generate fluent and high quality texts but they struggle with coherence and factuality, showing the potential for our dataset to inspire future work on long-form generation.1",
    "checked": true,
    "id": "e4a38435a08da7af34e801494c318a9ebb699e10",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Mingda Chen",
      "Sam Wiseman",
      "Kevin Gimpel"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.18": {
    "title": "CoDesc: A Large Code–Description Parallel Dataset",
    "volume": "findings",
    "abstract": "Translation between natural language and source code can help software development by enabling developers to comprehend, ideate, search, and write computer programs in natural language. Despite growing interest from the industry and the research community, this task is often difficult due to the lack of large standard datasets suitable for training deep neural models, standard noise removal methods, and evaluation benchmarks. This leaves researchers to collect new small-scale datasets, resulting in inconsistencies across published works. In this study, we present CoDesc a large parallel dataset composed of 4.2 million Java methods and natural language descriptions. With extensive analysis, we identify and remove prevailing noise patterns from the dataset. We demonstrate the proficiency of CoDesc in two complementary tasks for code–description pairs: code summarization and code search. We show that the dataset helps improve code search by up to 22% and achieves the new state-of-the-art in code summarization. Furthermore, we show CoDesc's effectiveness in pre-training–finetuning setup, opening possibilities in building pretrained language models for Java. To facilitate future research, we release the dataset, a data processing tool, and a benchmark at https://github.com/csebuetnlp/CoDesc",
    "checked": true,
    "id": "8c4f89a9ac30cf94186916be1bfaa02dbfb3600d",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Masum Hasan",
      "Tanveer Muttaqueen",
      "Abdullah Al Ishtiaq",
      "Kazi Sajeed Mehrab",
      "Md. Mahim Anjum Haque",
      "Tahmid Hasan",
      "Wasi Ahmad",
      "Anindya Iqbal",
      "Rifat Shahriyar"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.19": {
    "title": "Deep Cognitive Reasoning Network for Multi-hop Question Answering over Knowledge Graphs",
    "volume": "findings",
    "abstract": "Knowledge Graphs (KGs) provide human knowledge with nodes and edges being entities and relations among them, respectively. Multihop question answering over KGs—which aims to find answer entities of given questions through reasoning paths in KGs—has attracted great attention from both academia and industry recently. However, this task remains challenging, as it requires to accurately identify answers in a large candidate entity set, of which the size grows exponentially with the number of reasoning hops. To tackle this problem, we propose a novel Deep Cognitive Reasoning Network (DCRN), which is inspired by the dual process theory in cognitive science. Specifically, DCRN consists of two phases—the unconscious phase and the conscious phase. The unconscious phase first retrieves informative evidence from candidate entities by leveraging their semantic information. Then, the conscious phase accurately identifies answers by performing sequential reasoning according to the graph structure on the retrieved evidence. Experiments demonstrate that DCRN significantly outperforms state-of-the-art methods on benchmark datasets",
    "checked": true,
    "id": "14ee04939eae5610d5d6141ad953021967ab2de5",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Jianyu Cai",
      "Zhanqiu Zhang",
      "Feng Wu",
      "Jie Wang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.20": {
    "title": "GoG: Relation-aware Graph-over-Graph Network for Visual Dialog",
    "volume": "findings",
    "abstract": "Visual dialog, which aims to hold a meaningful conversation with humans about a given image, is a challenging task that requires models to reason the complex dependencies among visual content, dialog history, and current questions. Graph neural networks are recently applied to model the implicit relations between objects in an image or dialog. However, they neglect the importance of 1) coreference relations among dialog history and dependency relations between words for the question representation; and 2) the representation of the image based on the fully represented question. Therefore, we propose a novel relation-aware graph-over-graph network (GoG) for visual dialog. Speciﬁcally, GoG consists of three sequential graphs: 1) H-Graph, which aims to capture coreference relations among dialog history; 2) History-aware Q-Graph, which aims to fully understand the question through capturing dependency relations between words based on coreference resolution on the dialog history; and 3) Question-aware I-Graph, which aims to capture the relations between objects in an image based on fully question representation. As an additional feature representation module, we add GoG to the existing visual dialogue model. Experimental results show that our model outperforms the strong baseline in both generative and discriminative settings by a signiﬁcant margin",
    "checked": true,
    "id": "be35fbde5080831694244a3fe5cfbdf7af8e8734",
    "semantic_title": "",
    "citation_count": 19,
    "authors": [
      "Feilong Chen",
      "Xiuyi Chen",
      "Fandong Meng",
      "Peng Li",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.21": {
    "title": "Joint Optimization of Tokenization and Downstream Model",
    "volume": "findings",
    "abstract": "Since traditional tokenizers are isolated from a downstream task and model, they cannot output an appropriate tokenization depending on the task and model, although recent studies imply that the appropriate tokenization improves the performance. In this paper, we propose a novel method to find an appropriate tokenization to a given downstream model by jointly optimizing a tokenizer and the model. The proposed method has no restriction except for using loss values computed by the downstream model to train the tokenizer, and thus, we can apply the proposed method to any NLP task. Moreover, the proposed method can be used to explore the appropriate tokenization for an already trained model as post-processing. Therefore, the proposed method is applicable to various situations. We evaluated whether our method contributes to improving performance on text classification in three languages and machine translation in eight language pairs. Experimental results show that our proposed method improves the performance by determining appropriate tokenizations",
    "checked": true,
    "id": "e6b252ad22486c10b1b288e0a5e1ad468690be70",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Tatsuya Hiraoka",
      "Sho Takase",
      "Kei Uchiumi",
      "Atsushi Keyaki",
      "Naoaki Okazaki"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.22": {
    "title": "How does Attention Affect the Model?",
    "volume": "findings",
    "abstract": "The attention layer has become a prevalent component in improving the effectiveness of neural network models for NLP tasks. Figuring out why attention is effective and its interpretability has attracted a widespread deliberation. Current studies mostly investigate the effect of attention mechanism based on the attention distribution it generates with one single neural network structure. However they do not consider the changes in semantic capability of different components in the model due to the attention mechanism, which can vary across different network structures. In this paper, we propose a comprehensive analytical framework that exploits a convex hull representation of sequence semantics in an n-dimensional Semantic Euclidean Space and defines a series of indicators to capture the impact of attention on sequence semantics. Through a series of experiments on various NLP tasks and three representative recurrent units, we analyze why and how attention benefits the semantic capacity of different types of recurrent neural networks based on the indicators defined in the proposed framework",
    "checked": true,
    "id": "83216b431d72cb5dcf852dfb6bc1fa60bb6dc55d",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Cheng Zhang",
      "Qiuchi Li",
      "Lingyu Hua",
      "Dawei Song"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.23": {
    "title": "Contrastive Attention for Automatic Chest X-ray Report Generation",
    "volume": "findings",
    "abstract": "Recently, chest X-ray report generation, which aims to automatically generate descriptions of given chest X-ray images, has received grow-ing research interests. The key challenge of chest X-ray report generation is to accurately capture and describe the abnormal regions. In most cases, the normal regions dominate the entire chest X-ray image, and the corresponding descriptions of these normal regions dominate the ﬁnal report. Due to such data bias, learning-based models may fail to attend to abnormal regions. In this work, to effectively capture and describe abnormal regions, we propose the Contrastive Attention (CA) model. Instead of solely focusing on the current input image, the CA model compares the current input image with normal images to distill the contrastive information. The acquired contrastive information can better represent the visual features of abnormal regions. According to the experiments on the public IU-X-ray and MIMIC-CXR datasets, incorporating our CA into several existing models can boost their performance across most metrics. In addition, according to the analysis, the CA model can help existing models better attend to the abnormal regions and provide more accurate descriptions which are crucial for an interpretable diagnosis. Speciﬁcally, we achieve the state-of-the-art results on the two public datasets",
    "checked": true,
    "id": "2eda4a77f667306f1b9f05eee97c4f120491d44a",
    "semantic_title": "",
    "citation_count": 33,
    "authors": [
      "Fenglin Liu",
      "Changchang Yin",
      "Xian Wu",
      "Shen Ge",
      "Ping Zhang",
      "Xu Sun"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.24": {
    "title": "O2NA: An Object-Oriented Non-Autoregressive Approach for Controllable Video Captioning",
    "volume": "findings",
    "abstract": "Video captioning combines video understanding and language generation. Different from image captioning that describes a static image with details of almost every object, video captioning usually considers a sequence of frames and biases towards focused objects, e.g., the objects that stay in focus regardless of the changing background. Therefore, detecting and properly accommodating focused objects is critical in video captioning. To enforce the description of focused objects and achieve controllable video captioning, we propose an Object-Oriented Non-Autoregressive approach (O2NA), which performs caption generation in three steps: 1) identify the focused objects and predict their locations in the target caption; 2) generate the related attribute words and relation words of these focused objects to form a draft caption; and 3) combine video information to refine the draft caption to a fluent final caption. Since the focused objects are generated and located ahead of other words, it is difficult to apply the word-by-word autoregressive generation process; instead, we adopt a non-autoregressive approach. The experiments on two benchmark datasets, i.e., MSR-VTT and MSVD, demonstrate the effectiveness of O2NA, which achieves results competitive with the state-of-the-arts but with both higher diversity and higher inference speed",
    "checked": true,
    "id": "87fa897b3d4a8ec1aa9a6ccdc342fbb9c96bcef7",
    "semantic_title": "",
    "citation_count": 17,
    "authors": [
      "Fenglin Liu",
      "Xuancheng Ren",
      "Xian Wu",
      "Bang Yang",
      "Shen Ge",
      "Xu Sun"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.25": {
    "title": "Better Chinese Sentence Segmentation with Reinforcement Learning",
    "volume": "findings",
    "abstract": "A long-standing challenge in Chinese–English machine translation is that sentence boundaries are ambiguous in Chinese orthography, but inferring good splits is necessary for obtaining high quality translations. To solve this, we use reinforcement learning to train a segmentation policy that splits Chinese texts into segments that can be independently translated so as to maximise the overall translation quality. We compare to a variety of segmentation strategies and find that our approach improves the baseline BLEU score on the WMT2020 Chinese–English news translation task by +0.3 BLEU overall and improves the score on input segments that contain more than 60 words by +3 BLEU",
    "checked": true,
    "id": "95f8cf3dd2cd1d050d6b155e3c056f0e14f496b7",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Srivatsan Srinivasan",
      "Chris Dyer"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.26": {
    "title": "Enhancing Transformers with Gradient Boosted Decision Trees for NLI Fine-Tuning",
    "volume": "findings",
    "abstract": "Transfer learning has become the dominant paradigm for many natural language processing tasks. In addition to models being pretrained on large datasets, they can be further trained on intermediate (supervised) tasks that are similar to the target task. For small Natural Language Inference (NLI) datasets, language modelling is typically followed by pretraining on a large (labelled) NLI dataset before ﬁne-tuning with each NLI subtask. In this work, we explore Gradient Boosted Decision Trees (GBDTs) as an alternative to the commonly used Multi-Layer Perceptron (MLP) classiﬁcation head. GBDTs have de-sirable properties such as good performance on dense, numerical features and are effective where the ratio of the number of samples w.r.t the number of features is low. We then introduce FreeGBDT, a method of ﬁtting a GBDT head on the features computed during ﬁne-tuning to increase performance without additional computation by the neural network. We demonstrate the effectiveness of our method on several NLI datasets using a strong baseline model (RoBERTa-large with MNLI pretrain-ing). The FreeGBDT shows a consistent improvement over the MLP classiﬁcation head",
    "checked": true,
    "id": "6d2773a788067dcce7ac6a82019649528d341b4e",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Minixhofer",
      "Milan Gritta",
      "Ignacio Iacobacci"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.27": {
    "title": "Empirical Error Modeling Improves Robustness of Noisy Neural Sequence Labeling",
    "volume": "findings",
    "abstract": "Despite recent advances, standard sequence labeling systems often fail when processing noisy user-generated text or consuming the output of an Optical Character Recognition (OCR) process. In this paper, we improve the noise-aware training method by proposing an empirical error generation approach that employs a sequence-to-sequence model trained to perform translation from error-free to erroneous text. Using an OCR engine, we generated a large parallel text corpus for training and produced several real-world noisy sequence labeling benchmarks for evaluation. Moreover, to overcome the data sparsity problem that exacerbates in the case of imperfect textual input, we learned noisy language model-based embeddings. Our approach outperformed the baseline noise generation and error correction techniques on the erroneous sequence labeling data sets. To facilitate future research on robustness, we make our code, embeddings, and data conversion scripts publicly available",
    "checked": true,
    "id": "008f036b32e9b118b78334553e9bc51176341543",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Marcin Namysl",
      "Sven Behnke",
      "Joachim Köhler"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.28": {
    "title": "Spatial Dependency Parsing for Semi-Structured Document Information Extraction",
    "volume": "findings",
    "abstract": "Information Extraction (IE) for semi-structured document images is often approached as a sequence tagging problem by classifying each recognized input token into one of the IOB (Inside, Outside, and Beginning) categories. However, such problem setup has two inherent limitations that (1) it cannot easily handle complex spatial relationships and (2) it is not suitable for highly structured information, which are nevertheless frequently observed in real-world document images. To tackle these issues, we first formulate the IE task as spatial dependency parsing problem that focuses on the relationship among text segment nodes in the documents. Under this setup, we then propose SPADE (SPAtial DEpendency parser) that models highly complex spatial relationships and an arbitrary number of information layers in the documents in an end-to-end manner. We evaluate it on various kinds of documents such as receipts, name cards, forms, and invoices, and show that it achieves a similar or better performance compared to strong baselines including BERT-based IOB taggger, with up to 37.7% improvement",
    "checked": true,
    "id": "20d0564fd3fdbc24f266ca2076826a2271c3ea08",
    "semantic_title": "",
    "citation_count": 42,
    "authors": [
      "Wonseok Hwang",
      "Jinyeong Yim",
      "Seunghyun Park",
      "Sohee Yang",
      "Minjoon Seo"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.29": {
    "title": "Reader-Guided Passage Reranking for Open-Domain Question Answering",
    "volume": "findings",
    "abstract": "Current open-domain question answering systems often follow a Retriever-Reader architecture, where the retriever first retrieves relevant passages and the reader then reads the retrieved passages to form an answer. In this paper, we propose a simple and effective passage reranking method, named ReaderguIDEd Reranker (RIDER), which does not involve training and reranks the retrieved passages solely based on the top predictions of the reader before reranking. We show that RIDER, despite its simplicity, achieves 10 to 20 absolute gains in top-1 retrieval accuracy and 1 to 4 Exact Match (EM) gains without refining the retriever or reader. In addition, RIDER, without any training, outperforms state-of-the-art transformer-based supervised rerankers. Remarkably, RIDER achieves 48.3 EM on the Natural Questions dataset and 66.4 EM on the TriviaQA dataset when only 1,024 tokens (7.8 passages on average) are used as the reader input after passage reranking.1",
    "checked": true,
    "id": "0558e6575cbed16a63761a906bbaf91c7843a78d",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Yuning Mao",
      "Pengcheng He",
      "Xiaodong Liu",
      "Yelong Shen",
      "Jianfeng Gao",
      "Jiawei Han",
      "Weizhu Chen"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.30": {
    "title": "Entity-Aware Abstractive Multi-Document Summarization",
    "volume": "findings",
    "abstract": "Entities and their mentions convey significant semantic information in documents. In multidocument summarization, the same entity may appear across different documents. Capturing such cross-document entity information can be beneficial – intuitively, it allows the system to aggregate diverse useful information around the same entity for better summarization. In this paper, we present EMSum, an entityaware model for abstractive multi-document summarization. Our model augments the classical Transformer-based encoder-decoder framework with a heterogeneous graph consisting of text units and entities as nodes, which allows rich cross-document information to be captured. In the decoding process, we design a novel two-level attention mechanism, allowing the model to deal with saliency and redundancy issues explicitly. Our model can also be used together with pre-trained language models, arriving at improved performance. We conduct comprehensive experiments on the standard datasets and the results show the effectiveness of our approach",
    "checked": true,
    "id": "366c40349b2813c4eb281b4747f5210f99a4d62e",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Hao Zhou",
      "Weidong Ren",
      "Gongshen Liu",
      "Bo Su",
      "Wei Lu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.31": {
    "title": "LenAtten: An Effective Length Controlling Unit For Text Summarization",
    "volume": "findings",
    "abstract": "Fixed length summarization aims at generating summaries with a preset number of words or characters. Most recent researches incorporate length information with word embeddings as the input to the recurrent decoding unit, causing a compromise between length controllability and summary quality. In this work, we present an effective length controlling unit Length Attention (LenAtten) to break this trade-off. Experimental results show that LenAtten not only brings improvements in length controllability and ROGUE scores but also has great generalization ability. In the task of generating a summary with the target length, our model is 732 times better than the bestperforming length controllable summarizer in length controllability on the CNN/Daily Mail dataset. 1",
    "checked": true,
    "id": "c30849528d4d75c514a5de7e01c0df25c6040803",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Zhongyi Yu",
      "Zhenghao Wu",
      "Hao Zheng",
      "Zhe XuanYuan",
      "Jefferson Fong",
      "Weifeng Su"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.32": {
    "title": "XeroAlign: Zero-shot cross-lingual transformer alignment",
    "volume": "findings",
    "abstract": "The introduction of transformer-based crosslingual language models brought decisive improvements to multilingual NLP tasks. However, the lack of labelled data has necessitated a variety of methods that aim to close the gap to high-resource languages. Zero-shot methods in particular, often use translated task data as a training signal to bridge the performance gap between the source and target language(s). We introduce XeroAlign, a simple method for taskspecific alignment of cross-lingual pretrained transformers such as XLM-R. XeroAlign uses translated task data to encourage the model to generate similar sentence embeddings for different languages. The XeroAligned XLM-R, called XLM-RA, shows strong improvements over the baseline models to achieve state-ofthe-art zero-shot results on three multilingual natural language understanding tasks. XLMRA performs on par with state-of-the-art models on a cross-lingual adversarial paraphrasing task and its text classification accuracy exceeds that of XLM-R trained with labelled data",
    "checked": true,
    "id": "c782d1f9ac3ead6f7b0952a4d1f9143e978f6b8f",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Milan Gritta",
      "Ignacio Iacobacci"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.33": {
    "title": "Using Word Embeddings to Analyze Teacher Evaluations: An Application to a Filipino Education Non-Profit Organization",
    "volume": "findings",
    "abstract": "Analysis of teacher evaluations is crucial to the development of robust educational programs, particularly through the validation of desirable qualities being reflected on in the text. This research applies Natural Language Processing techniques on a real-world dataset from a Filipino education non-profit to explore insights from analyzing evaluations written by Teacher Fellows who assess their own progress. Prior to this research, only qualitative assessment had been conducted on the text. Inspired by the use of word embedding similarities to capture semantic alignment, we utilize GloVe embeddings to determine to what extent these evaluations reflect concepts critical to measuring the competency of Teacher Fellows and upholding the organization's Vision and Mission. As Fellows' quantitative ratings improved, so too did their demonstration of competency in the text. Further, Teacher Fellow language was consistent with the organization's Vision and Mission. This research therefore showcases the possibilities of NLP in education, improving our understanding of Teacher Fellow evaluations, which can lead to advances in program operations and education efforts",
    "checked": true,
    "id": "23a812dde149021bab611ca4395279b9d41031f1",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesca Vera"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.34": {
    "title": "Relation Classification with Entity Type Restriction",
    "volume": "findings",
    "abstract": "Relation classification aims to predict a relation between two entities in a sentence. The existing methods regard all relations as the candidate relations for the two entities. These methods neglect the restrictions on candidate relations by entity types, which leads to some inappropriate relations being candidate relations. In this paper, we propose a novel paradigm, RElation Classification with ENtity Type restriction (RECENT), which exploits entity types to restrict candidate relations. Specially, the mutual restrictions of relations and entity types are formalized and introduced into relation classification. Besides, the proposed paradigm, RECENT, is model-agnostic. Based on two representative models GCN and SpanBERT respectively, RECENTGCN and RECENTSpanBERT are trained in RECENT1. Experimental results on a standard dataset indicate that RECENT improves the performance of GCN and SpanBERT by 6.9 and 4.4 F1 points, respectively. Especially, RECENTSpanBERT achieves a new state-of-the-art on TACRED",
    "checked": true,
    "id": "97cbc8a78ad588931d7adfe319b4c68f3d167461",
    "semantic_title": "",
    "citation_count": 35,
    "authors": [
      "Shengfei Lyu",
      "Huanhuan Chen"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.35": {
    "title": "Link Prediction on N-ary Relational Facts: A Graph-based Approach",
    "volume": "findings",
    "abstract": "Link prediction on knowledge graphs (KGs) is a key research topic. Previous work mainly focused on binary relations, paying less attention to higher-arity relations although they are ubiquitous in real-world KGs. This paper considers link prediction upon n-ary relational facts and proposes a graph-based approach to this task. The key to our approach is to represent the nary structure of a fact as a small heterogeneous graph, and model this graph with edge-biased fully-connected attention. The fully-connected attention captures universal inter-vertex interactions, while with edge-aware attentive biases to particularly encode the graph structure and its heterogeneity. In this fashion, our approach fully models global and local dependencies in each n-ary fact, and hence can more effectively capture associations therein. Extensive evaluation verifies the effectiveness and superiority of our approach. It performs substantially and consistently better than current state-of-the-art across a variety of n-ary relational benchmarks. Our code is publicly available.1",
    "checked": true,
    "id": "9f863fcb229e762a95b8bb0e4a89d7aeeb3d8640",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Quan Wang",
      "Haifeng Wang",
      "Yajuan Lyu",
      "Yong Zhu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.36": {
    "title": "GLGE: A New General Language Generation Evaluation Benchmark",
    "volume": "findings",
    "abstract": "Multi-task benchmarks such as GLUE and SuperGLUE have driven great progress of pretraining and transfer learning in Natural Language Processing (NLP). These benchmarks mostly focus on a range of Natural Language Understanding (NLU) tasks, without considering the Natural Language Generation (NLG) models. In this paper, we present the General Language Generation Evaluation (GLGE), a new multi-task benchmark for evaluating the generalization capabilities of NLG models across eight language generation tasks. For each task, we continue to design three subtasks in terms of task difficulty (GLGE-Easy, GLGE-Medium, and GLGE-Hard). This introduces 24 subtasks to comprehensively compare model performance. To encourage research on pretraining and transfer learning on NLG models, we make GLGE publicly available and build a leaderboard with strong baselines including MASS, BART, and ProphetNet\\footnote{The source code and dataset will be publicly available at this https URL",
    "checked": true,
    "id": "5fe78eb0f142902237df11cb67c455787a759172",
    "semantic_title": "",
    "citation_count": 50,
    "authors": [
      "Dayiheng Liu",
      "Yu Yan",
      "Yeyun Gong",
      "Weizhen Qi",
      "Hang Zhang",
      "Jian Jiao",
      "Weizhu Chen",
      "Jie Fu",
      "Linjun Shou",
      "Ming Gong",
      "Pengcheng Wang",
      "Jiusheng Chen",
      "Daxin Jiang",
      "Jiancheng Lv",
      "Ruofei Zhang",
      "Winnie Wu",
      "Ming Zhou",
      "Nan Duan"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.37": {
    "title": "AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization",
    "volume": "findings",
    "abstract": "Pre-trained language models such as BERT have exhibited remarkable performances in many tasks in natural language understanding (NLU). The tokens in the models are usually fine-grained in the sense that for languages like English they are words or sub-words and for languages like Chinese they are characters. In English, for example, there are multi-word expressions which form natural lexical units and thus the use of coarse-grained tokenization also appears to be reasonable. In fact, both fine-grained and coarse-grained tokenizations have advantages and disadvantages for learning of pre-trained language models. In this paper, we propose a novel pre-trained language model, referred to as AMBERT (A Multi-grained BERT), on the basis of both fine-grained and coarse-grained tokenizations. For English, AMBERT takes both the sequence of words (fine-grained tokens) and the sequence of phrases (coarse-grained tokens) as input after tokenization, employs one encoder for processing the sequence of words and the other encoder for processing the sequence of the phrases, utilizes shared parameters between the two encoders, and finally creates a sequence of contextualized representations of the words and a sequence of contextualized representations of the phrases. Experiments have been conducted on benchmark datasets for Chinese and English, including CLUE, GLUE, SQuAD and RACE. The results show that AMBERT outperforms the existing best performing models in almost all cases, particularly the improvements are significant for Chinese",
    "checked": true,
    "id": "59c0076b3d814588e320820b95563965733d1875",
    "semantic_title": "",
    "citation_count": 28,
    "authors": [
      "Xinsong Zhang",
      "Pengshuai Li",
      "Hang Li"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.38": {
    "title": "Multimodal Incremental Transformer with Visual Grounding for Visual Dialogue Generation",
    "volume": "findings",
    "abstract": "Visual dialogue is a challenging task since it needs to answer a series of coherent questions on the basis of understanding the visual environment. Previous studies focus on the implicit exploration of multimodal coreference by implicitly attending to spatial image features or object-level image features but neglect the importance of locating the objects explicitly in the visual content, which is associated with entities in the textual content. Therefore, in this paper we propose a Multimodal Incremental Transformer with Visual Grounding, named MITVG, which consists of two key parts: visual grounding and multimodal incremental transformer. Visual grounding aims to explicitly locate related objects in the image guided by textual entities, which helps the model exclude the visual content that does not need attention. On the basis of visual grounding, the multimodal incremental transformer encodes the multi-turn dialogue history combined with visual scene step by step according to the order of the dialogue and then generates a contextually and visually coherent response. Experimental results on the VisDial v0.9 and v1.0 datasets demonstrate the superiority of the proposed model, which achieves comparable performance",
    "checked": true,
    "id": "2da676d2b029fc96823c288b4d208d38f1fee96c",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Feilong Chen",
      "Fandong Meng",
      "Xiuyi Chen",
      "Peng Li",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.39": {
    "title": "Retrieve & Memorize: Dialog Policy Learning with Multi-Action Memory",
    "volume": "findings",
    "abstract": "Dialogue policy learning, a subtask that determines the content of system response generation and then the degree of task completion, is essential for task-oriented dialogue systems. However, the unbalanced distribution of system actions in dialogue datasets often causes difficulty in learning to generate desired actions and responses. In this paper, we propose a retrieve-and-memorize framework to enhance the learning of system actions. Specially, we first design a neural context-aware retrieval module to retrieve multiple candidate system actions from the training set given a dialogue context. Then, we propose a memoryaugmented multi-decoder network to generate the system actions conditioned on the candidate actions, which allows the network to adaptively select key information in the candidate actions and ignore noises. We conduct experiments on the large-scale multi-domain task-oriented dialogue dataset MultiWOZ 2.0 and MultiWOZ 2.1. Experimental results show that our method achieves competitive performance among several state-of-the-art models in the context-to-response generation task",
    "checked": true,
    "id": "273990598a6929bdc7c885f25bacfc6ebf51995e",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "YunHao Li",
      "Yunyi Yang",
      "Xiaojun Quan",
      "Jianxing Yu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.40": {
    "title": "Adapt-and-Distill: Developing Small, Fast and Effective Pretrained Language Models for Domains",
    "volume": "findings",
    "abstract": "Large pretrained models have achieved great success in many natural language processing tasks. However, when they are applied in specific domains, these models suffer from domain shift and bring challenges in fine-tuning and online serving for latency and capacity constraints. In this paper, we present a general approach to developing small, fast and effective pretrained models for specific domains. This is achieved by adapting the offthe-shelf general pretrained models and performing task-agnostic knowledge distillation in target domains. Specifically, we propose domain-specific vocabulary expansion in the adaptation stage and employ corpus level occurrence probability to choose the size of incremental vocabulary automatically. Then we systematically explore different strategies to compress the large pretrained models for specific domains. We conduct our experiments in the biomedical and computer science domain. The experimental results demonstrate that our approach achieves better performance over the BERTBASE model in domain-specific tasks while 3.3× smaller and 5.1× faster than BERTBASE. The code and pretrained models are available at https://aka.ms/adalm",
    "checked": true,
    "id": "cf5e670a79847d9be0eb185fb372d99d30d4d98f",
    "semantic_title": "",
    "citation_count": 18,
    "authors": [
      "Yunzhi Yao",
      "Shaohan Huang",
      "Wenhui Wang",
      "Li Dong",
      "Furu Wei"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.41": {
    "title": "Decoupling Adversarial Training for Fair NLP",
    "volume": "findings",
    "abstract": "Adversarial debiasing can help to learn fairer models. Previous work has assumed that both main task labels and protected attributes are available in the dataset. However, protected labels are often unavailable, or only available in limited numbers. In this paper, we propose a training strategy which needs only a small volume of protected labels in adversarial training, incorporating an estimation method to transfer private-labelled instances from one dataset to another. We demonstrate the inand crossdomain effectiveness of our method through a range of experiments",
    "checked": true,
    "id": "7dc43f7339e636ba49891732e3f20b3b377dfd78",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Xudong Han",
      "Timothy Baldwin",
      "Trevor Cohn"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.42": {
    "title": "GO FIGURE: A Meta Evaluation of Factuality in Summarization",
    "volume": "findings",
    "abstract": "Text generation models can generate factually inconsistent text containing distorted or fabricated facts about the source text. Recent work has focused on building evaluation models to verify the factual correctness of semantically constrained text generation tasks such as document summarization. While the field of factuality evaluation is growing fast, we don't have well-defined criteria for measuring the effectiveness, generalizability, reliability, or sensitivity of the factuality metrics. Focusing on these aspects, in this paper, we introduce a meta-evaluation framework for evaluating factual consistency metrics. We introduce five necessary, common-sense conditions for effective factuality metrics and experiment with nine recent factuality metrics using synthetic and human-labeled factuality data from short news, long news and dialogue summarization domains. Our framework enables assessing the efficiency of any new factual consistency metric on a variety of dimensions over multiple summarization domains and can be easily extended with new meta-evaluation criteria. We also present our conclusions towards standardizing the factuality evaluation metrics",
    "checked": true,
    "id": "266bd8542c87be1030d578638dc1b4e793b5a091",
    "semantic_title": "",
    "citation_count": 58,
    "authors": [
      "Saadia Gabriel",
      "Asli Celikyilmaz",
      "Rahul Jha",
      "Yejin Choi",
      "Jianfeng Gao"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.43": {
    "title": "DNN-driven Gradual Machine Learning for Aspect-term Sentiment Analysis",
    "volume": "findings",
    "abstract": "Recent work has shown that Aspect-Term Sentiment Analysis (ATSA) can be performed by Gradual Machine Learning (GML), which begins with some automatically labeled easy instances, and then gradually labels more challenging instances by iterative factor graph inference without manual intervention. As a non-i.i.d learning paradigm, GML leverages shared features between labeled and unlabeled instances for knowledge conveyance. However, the existing GML solution extracts sentiment features based on pre-specified lexicons, which are usually inaccurate and incomplete and thus lead to inadequate knowledge conveyance. In this paper, we propose a Deep Neural Network (DNN) driven GML approach for ATSA, which exploits the power of DNN in feature representation for gradual learning. It first uses an unsupervised neural network to cluster the automatically extracted features by their sentiment orientation. Then, it models the clustered features as factors to enable implicit knowledge conveyance for gradual inference in a factor graph. To leverage labeled training data, we also present a hybrid solution that fulfills gradual learning by fusing the influence of supervised DNN predictions and implicit knowledge conveyance in a unified factor graph. Finally, we empirically evaluate the performance of the proposed approach on real benchmark data. Our extensive experiments have shown that the proposed approach consistently achieves the state-of-the-art performance across all the test datasets in both unsupervised and supervised settings and the improvement margins are considerable",
    "checked": true,
    "id": "727673f05b65bc842701f69a8c2d8c09b9042f99",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Murtadha Ahmed",
      "Qun Chen",
      "Yanyan Wang",
      "Youcef Nafa",
      "Zhanhuai Li",
      "Tianyi Duan"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.44": {
    "title": "Error Detection in Large-Scale Natural Language Understanding Systems Using Transformer Models",
    "volume": "findings",
    "abstract": "Large-scale conversational assistants like Alexa, Siri, Cortana and Google Assistant process every utterance using multiple models for domain, intent and named entity recognition. Given the decoupled nature of model development and large traffic volumes, it is extremely difficult to identify utterances processed erroneously by such systems. We address this challenge to detect domain classification errors using offline Transformer models. We combine utterance encodings from a RoBERTa model with the Nbest hypothesis produced by the production system. We then fine-tune end-to-end in a multitask setting using a small dataset of humanannotated utterances with domain classification errors. We tested our approach for detecting misclassifications from one domain that accounts for <0.5% of the traffic in a large-scale conversational AI system. Our approach achieves an F1 score of 30% outperforming a biLSTM baseline by 16.9% and a standalone RoBERTa model by 4.8%. We improve this further by 2.2% to 32.2% by ensembling multiple models",
    "checked": true,
    "id": "db899c6191e25818091d4bab1c2da88689554e7f",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Rakesh Chada",
      "Pradeep Natarajan",
      "Darshan Fofadiya",
      "Prathap Ramachandra"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.45": {
    "title": "OutFlip: Generating Examples for Unknown Intent Detection with Natural Language Attack",
    "volume": "findings",
    "abstract": "Out-of-domain (OOD) input detection is vital in a task-oriented dialogue system since the acceptance of unsupported inputs could lead to an incorrect response of the system. This paper proposes OutFlip, a method to generate outof-domain samples using only in-domain training dataset automatically. A white-box natural language attack method HotFlip is revised to generate out-of-domain samples instead of adversarial examples. Our evaluation results showed that integrating OutFlip-generated outof-domain samples into the training dataset could significantly improve an intent classification model's out-of-domain detection performance1",
    "checked": true,
    "id": "6913b90ae451d721886dfae5e563f49b352016a8",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "DongHyun Choi",
      "Myeong Cheol Shin",
      "EungGyun Kim",
      "Dong Ryeol Shin"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.46": {
    "title": "GeoQA: A Geometric Question Answering Benchmark Towards Multimodal Numerical Reasoning",
    "volume": "findings",
    "abstract": "Automatic math problem solving has recently attracted increasing attention as a longstanding AI benchmark. In this paper, we focus on solving geometric problems, which requires a comprehensive understanding of textual descriptions, visual diagrams, and theorem knowledge. However, the existing methods were highly dependent on handcraft rules and were merely evaluated on small-scale datasets. Therefore, we propose a Geometric Question Answering dataset GeoQA, containing 5,010 geometric problems with corresponding annotated programs, which illustrate the solving process of the given problems. Compared with another publicly available dataset GeoS, GeoQA is 25 times larger, in which the program annotations can provide a practical testbed for future research on explicit and explainable numerical reasoning. Moreover, we introduce a Neural Geometric Solver (NGS) to address geometric problems by comprehensively parsing multimodal information and generating interpretable programs. We further add multiple self-supervised auxiliary tasks on NGS to enhance cross-modal semantic representation. Extensive experiments on GeoQA validate the effectiveness of our proposed NGS and auxiliary tasks. However, the results are still significantly lower than human performance, which leaves large room for future research. Our benchmark and code are released at https://github.com/chenjudge/GeoQA",
    "checked": true,
    "id": "291133a657498920451481d3bf784ebbafda8d6e",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Jiaqi Chen",
      "Jianheng Tang",
      "Jinghui Qin",
      "Xiaodan Liang",
      "Lingbo Liu",
      "Eric Xing",
      "Liang Lin"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.47": {
    "title": "SIRE: Separate Intra- and Inter-sentential Reasoning for Document-level Relation Extraction",
    "volume": "findings",
    "abstract": "Document-level relation extraction has attracted much attention in recent years. It is usually formulated as a classification problem that predicts relations for all entity pairs in the document. However, previous works indiscriminately represent intraand inter-sentential relations in the same way, confounding the different patterns for predicting them. Besides, they create a document graph and use paths between entities on the graph as clues for logical reasoning. However, not all entity pairs can be connected with a path and have the correct logical reasoning paths in their graph. Thus many cases of logical reasoning cannot be covered. This paper proposes an effective architecture, SIRE, to represent intraand inter-sentential relations in different ways. We design a new and straightforward form of logical reasoning module that can cover more logical reasoning chains. Experiments on the public datasets show SIRE outperforms the previous state-of-the-art methods. Further analysis shows that our predictions are reliable and explainable. Our code is available at https: //github.com/DreamInvoker/SIRE",
    "checked": true,
    "id": "0df00857af8c2d2c17b368a8008b965243322924",
    "semantic_title": "",
    "citation_count": 26,
    "authors": [
      "Shuang Zeng",
      "Yuting Wu",
      "Baobao Chang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.48": {
    "title": "KGPool: Dynamic Knowledge Graph Context Selection for Relation Extraction",
    "volume": "findings",
    "abstract": "We present a novel method for relation extraction (RE) from a single sentence, mapping the sentence and two given entities to a canonical fact in a knowledge graph (KG). Especially in this presumed sentential RE setting, the context of a single sentence is often sparse. This paper introduces the KGPool method to address this sparsity, dynamically expanding the context with additional facts from the KG. It learns the representation of these facts (entity alias, entity descriptions, etc.) using neural methods, supplementing the sentential context. Unlike existing methods that statically use all expanded facts, KGPool conditions this expansion on the sentence. We study the efficacy of KGPool by evaluating it with different neural models and KGs (Wikidata and NYT Freebase). Our experimental evaluation on standard datasets shows that by feeding the KGPool representation into a Graph Neural Network, the overall method is significantly more accurate than state-of-the-art methods",
    "checked": true,
    "id": "4a7184a8e7a3a944c20c971d9e4523166c290fbc",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Abhishek Nadgeri",
      "Anson Bastos",
      "Kuldeep Singh",
      "Isaiah Onando Mulang’",
      "Johannes Hoffart",
      "Saeedeh Shekarpour",
      "Vijay Saraswat"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.49": {
    "title": "Better Combine Them Together! Integrating Syntactic Constituency and Dependency Representations for Semantic Role Labeling",
    "volume": "findings",
    "abstract": "Structural syntax knowledge has been proven effective for semantic role labeling (SRL), while existing works mostly use only one singleton syntax, such as either syntactic dependency or constituency tree. In this paper, we explore the integration of heterogeneous syntactic representations for SRL. We first consider a TreeLSTM-based integration, collaboratively learning the phrasal boundaries from the constituency and the semantic relations from dependency. We further introduce a labelaware GCN solution for simultaneously modeling the syntactic edges and labels. Experimental results demonstrate that by effectively combining the heterogeneous syntactic representations, our methods yield task improvements on both span-based and dependencybased SRL. Also our system achieves new state-of-the-art SRL performances, meanwhile bringing explainable task improvements",
    "checked": true,
    "id": "c1a84ed0c78bb7e8d063f35802313b1f701beffe",
    "semantic_title": "",
    "citation_count": 26,
    "authors": [
      "Hao Fei",
      "Shengqiong Wu",
      "Yafeng Ren",
      "Fei Li",
      "Donghong Ji"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.50": {
    "title": "Keep the Primary, Rewrite the Secondary: A Two-Stage Approach for Paraphrase Generation",
    "volume": "findings",
    "abstract": "Paraphrase generation is an important and challenging NLG problem. In this work, we propose a new Identification-thenAggregation (IA) framework to tackle this task. In the identification step, the input tokens are sorted into two groups by a novel Primary/Secondary Identification (PSI) algorithm. In the aggregation step, these groups are separately encoded, before being aggregated by a custom designed decoder, which autoregressively generates the paraphrased sentence. In extensive experiments on two benchmark datasets, we demonstrate that our model outperforms previous studies by a notable margin. We also show that the proposed approach can generate paraphrases in an interpretable and controllable way",
    "checked": true,
    "id": "b70d7de2690137986aff7895b763d3cc69c92766",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Yixuan Su",
      "David Vandyke",
      "Simon Baker",
      "Yan Wang",
      "Nigel Collier"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.51": {
    "title": "Contrastive Fine-tuning Improves Robustness for Neural Rankers",
    "volume": "findings",
    "abstract": "The performance of state-of-the-art neural rankers can deteriorate substantially when exposed to noisy inputs or applied to a new domain. In this paper, we present a novel method for fine-tuning neural rankers that can significantly improve their robustness to out-of-domain data and query perturbations. Specifically, a contrastive loss that compares data points in the representation space is combined with the standard ranking loss during fine-tuning. We use relevance labels to denote similar/dissimilar pairs, which allows the model to learn the underlying matching semantics across different query-document pairs and leads to improved robustness. In experiments with four passage ranking datasets, the proposed contrastive fine-tuning method obtains improvements on robustness to query reformulations, noise perturbations, and zeroshot transfer for both BERT and BART-based rankers. Additionally, our experiments show that contrastive fine-tuning outperforms data augmentation for robustifying neural rankers",
    "checked": true,
    "id": "f7bd23eaf95aad340fc87caaa5674bb4f1930184",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Xiaofei Ma",
      "Cicero Nogueira dos Santos",
      "Andrew O. Arnold"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.52": {
    "title": "Cross-Lingual Transfer in Zero-Shot Cross-Language Entity Linking",
    "volume": "findings",
    "abstract": "Cross-language entity linking grounds mentions in multiple languages to a single-language knowledge base. We propose a neural ranking architecture for this task that uses multilingual BERT representations of the mention and the context in a neural network. We find that the multilingual ability of BERT leads to robust performance in monolingual and multilingual settings. Furthermore, we explore zero-shot language transfer and find surprisingly robust performance. We investigate the zero-shot degradation and find that it can be partially mitigated by a proposed auxiliary training objective, but that the remaining error can best be attributed to domain shift rather than language transfer",
    "checked": true,
    "id": "791f31b7ea4976f71dfd63783f6c12def8fbebcc",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Elliot Schumacher",
      "James Mayfield",
      "Mark Dredze"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.53": {
    "title": "TellMeWhy: A Dataset for Answering Why-Questions in Narratives",
    "volume": "findings",
    "abstract": "Answering questions about why characters perform certain actions is central to understanding and reasoning about narratives. De-spite recent progress in QA, it is not clear if existing models have the ability to answer \"why\" questions that may require commonsense knowledge external to the input narrative. In this work, we introduce TellMeWhy , a new crowd-sourced dataset that consists of more than 30k questions and free-form answers concerning why characters in short narratives perform the actions described. For a third of this dataset, the answers are not present within the narrative. Given the limita-tions of automated evaluation for this task, we also present a systematized human evaluation interface for this dataset. Our evaluation of state-of-the-art models show that they are far below human performance on answering such questions. They are especially worse on questions whose answers are external to the narrative, thus providing a challenge for future QA and narrative understanding research",
    "checked": true,
    "id": "3eeedb6651a629a105c1185ada862e2cad7a0522",
    "semantic_title": "",
    "citation_count": 17,
    "authors": [
      "Yash Kumar Lal",
      "Nathanael Chambers",
      "Raymond Mooney",
      "Niranjan Balasubramanian"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.54": {
    "title": "Dialogue in the Wild: Learning from a Deployed Role-Playing Game with Humans and Bots",
    "volume": "findings",
    "abstract": "Much of NLP research has focused on crowdsourced static datasets and the supervised learning paradigm of training once and then evaluating test performance. As argued in de Vries et al. (2020), crowdsourced data has the issues of lack of naturalness and relevance to real-world use cases, while the static dataset paradigm does not allow for a model to learn from its experiences of using language (Silver et al., 2013). In contrast, one might hope for machine learning systems that become more useful as they interact with people. In this work, we build and deploy a role-playing game, whereby human players converse with learning agents situated in an open-domain fantasy world. We show that by training models on the conversations they have with humans in the game the models progressively improve, as measured by automatic metrics and online engagement scores. This learning is shown to be more efficient than crowdsourced data when applied to conversations with real users, as well as being far cheaper to collect",
    "checked": true,
    "id": "b7a64a22f69a17ea0b007f7748580f7641b55cb6",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Kurt Shuster",
      "Jack Urbanek",
      "Emily Dinan",
      "Arthur Szlam",
      "Jason Weston"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.55": {
    "title": "Deep Learning against COVID-19: Respiratory Insufficiency Detection in Brazilian Portuguese Speech",
    "volume": "findings",
    "abstract": "Respiratory insufficiency is a symptom that requires hospitalization. This work investigates whether it is possible to detect this condition by analyzing patient's speech samples; the analysis was performed on data collected during the first wave of the COVID-19 pandemic in 2020, and thus limited to respiratory insufficiency in COVID-19 patients. For that, a dataset was created consisting of speech emissions of both COVID-19 patients affected by respiratory insufficiency and a control group. This dataset was used to build a Convolution Neural Network to detect respiratory insufficiency using speech emission MFCC representations. Methodologically, dealing with background noise was a challenge, so we also collected background noise from COVID-19 wards where patients were located. Due to the difficulty in filtering noise without eliminating crucial information, noise samples were injected in the control group data to prevent bias. Moreover, we investigated (i) two approaches to address the duration variance of audios, and (ii) the ideal number of noise samples to inject in both patients and the control group to prevent bias and overfitting. The techniques developed reached 91.66% accuracy. Thus we validated the project's Leading Hypothesis, namely that it is possible to detect respiratory insufficiency in speech utterances, under real-life environmental conditions; we believe our results justify further enquiries into the use of automated speech analysis to support health professionals in triage procedures",
    "checked": true,
    "id": "6179ebc6e8d535513a71e9db9faabdbdcf96a075",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Edresson Casanova",
      "Lucas Gris",
      "Augusto Camargo",
      "Daniel da Silva",
      "Murilo Gazzola",
      "Ester Sabino",
      "Anna Levin",
      "Arnaldo Candido Jr",
      "Sandra Aluisio",
      "Marcelo Finger"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.56": {
    "title": "Benchmarking Robustness of Machine Reading Comprehension Models",
    "volume": "findings",
    "abstract": "Machine Reading Comprehension (MRC) is an important testbed for evaluating models' natural language understanding (NLU) ability. There has been rapid progress in this area, with new models achieving impressive performance on various MRC benchmarks. However, most of these benchmarks only evaluate models on in-domain test sets without considering their robustness under test-time perturbations. To fill this important gap, we construct AdvRACE (Adversarial RACE), a new model-agnostic benchmark for evaluating the robustness of MRC models under six different types of test-time perturbations, including our novel superimposed attack and distractor construction attack. We show that current state-of-the-art (SOTA) models are vulnerable to these simple black-box attacks. Our benchmark is constructed automatically based on the existing RACE benchmark, and thus the construction pipeline can be easily adopted by other tasks and datasets. We will release the data and source codes to facilitate future work. We hope that our work will encourage more research on improving the robustness of MRC and other NLU models",
    "checked": true,
    "id": "9da6e38f5124b3bb55839e0ad42e1ddcc50f840b",
    "semantic_title": "",
    "citation_count": 29,
    "authors": [
      "Chenglei Si",
      "Ziqing Yang",
      "Yiming Cui",
      "Wentao Ma",
      "Ting Liu",
      "Shijin Wang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.57": {
    "title": "Improving BERT with Syntax-aware Local Attention",
    "volume": "findings",
    "abstract": "Pre-trained Transformer-based neural language models, such as BERT, have achieved remarkable results on varieties of NLP tasks. Recent works have shown that attention-based models can benefit from more focused attention over local regions. Most of them restrict the attention scope within a linear span, or confine to certain tasks such as machine translation and question answering. In this paper, we propose a syntax-aware local attention, where the attention scopes are restrained based on the distances in the syntactic structure. The proposed syntax-aware local attention can be integrated with pretrained language models, such as BERT, to render the model to focus on syntactically relevant words. We conduct experiments on various single-sentence benchmarks, including sentence classification and sequence labeling tasks. Experimental results show consistent gains over BERT on all benchmark datasets. The extensive studies verify that our model achieves better performance owing to more focused attention over syntactically relevant words",
    "checked": true,
    "id": "154b27e548db8a9998b867971794e0615322b0d1",
    "semantic_title": "",
    "citation_count": 29,
    "authors": [
      "Zhongli Li",
      "Qingyu Zhou",
      "Chao Li",
      "Ke Xu",
      "Yunbo Cao"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.58": {
    "title": "A Dialogue-based Information Extraction System for Medical Insurance Assessment",
    "volume": "findings",
    "abstract": "In the Chinese medical insurance industry, the assessor's role is essential and requires significant efforts to converse with the claimant. This is a highly professional job that involves many parts, such as identifying personal information, collecting related evidence, and making a final insurance report. Due to the coronavirus (COVID-19) pandemic, the previous offline insurance assessment has to be conducted online. However, for the junior assessor often lacking practical experience, it is not easy to quickly handle such a complex online procedure, yet this is important as the insurance company needs to decide how much compensation the claimant should receive based on the assessor's feedback. In order to promote assessors' work efficiency and speed up the overall procedure, in this paper, we propose a dialogue-based information extraction system that integrates advanced NLP technologies for medical insurance assessment. With the assistance of our system, the average time cost of the procedure is reduced from 55 minutes to 35 minutes, and the total human resources cost is saved 30% compared with the previous offline procedure. Until now, the system has already served thousands of online claim cases. © 2021 Association for Computational Linguistics",
    "checked": true,
    "id": "65bb703c67822be2f201d2b1fcc1681e580c0861",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Shuang Peng",
      "Mengdi Zhou",
      "Minghui Yang",
      "Haitao Mi",
      "Shaosheng Cao",
      "Zujie Wen",
      "Teng Xu",
      "Hongbin Wang",
      "Lei Liu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.59": {
    "title": "Prediction or Comparison: Toward Interpretable Qualitative Reasoning",
    "volume": "findings",
    "abstract": "Qualitative relationships illustrate how changing one property (e.g., moving velocity) affects another (e.g., kinetic energy) and constitutes a considerable portion of textual knowledge. Current approaches use either semantic parsers to transform natural language inputs into logical expressions or a \"black-box\" model to solve them in one step. The former has a limited application range, while the latter lacks interpretability. In this work, we categorize qualitative reasoning tasks into two types: prediction and comparison. In particular, we adopt neural network modules trained in an end-to-end manner to simulate the two reasoning processes. Experiments on two qualitative reasoning question answering datasets, QuaRTz and QuaRel, show our methods' effectiveness and generalization capability, and the intermediate outputs provided by the modules make the reasoning process interpretable",
    "checked": true,
    "id": "9578cd3422eba0d2ce7fccd4b93c190c0b318efd",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mucheng Ren",
      "Heyan Huang",
      "Yang Gao"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.60": {
    "title": "Boundary Detection with BERT for Span-level Emotion Cause Analysis",
    "volume": "findings",
    "abstract": "Emotion cause analysis (ECA) has been an emerging topic in natural language processing, which aims to identify the reasons behind a certain emotion expressed in the text. Most ECA methods intend to identify the clause which contains the cause of a given emotion, but such clause-level ECA (CECA) can be ambiguous and imprecise. In this paper, we aim at span-level ECA (SECA) by detecting the precise boundaries of text spans conveying accurate emotion causes from the given context. We formulate this task as sequence labeling and position identification problems and design two neural methods to solve them. Experiments on two benchmark ECA datasets show that the proposed methods substantially outperform the existing ECA models 1",
    "checked": true,
    "id": "02c687f366e8d5e622dc39d967995495bdb99bd8",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Xiangju Li",
      "Wei Gao",
      "Shi Feng",
      "Yifei Zhang",
      "Daling Wang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.61": {
    "title": "On Commonsense Cues in BERT for Solving Commonsense Tasks",
    "volume": "findings",
    "abstract": "BERT has been used for solving commonsense tasks such as CommonsenseQA. While prior research has found that BERT does contain commonsense information to some extent, there has been work showing that pre-trained models can rely on spurious associations (e.g., data bias) rather than key cues in solving sentiment classification and other problems. We quantitatively investigate the presence of structural commonsense cues in BERT when solving commonsense tasks, and the importance of such cues for the model prediction. Using two different measures, we find that BERT does use relevant knowledge for solving the task, and the presence of commonsense knowledge is positively correlated to the model accuracy",
    "checked": true,
    "id": "07b95736960731b49b6ce5aa0b29f10bd0586a6d",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Leyang Cui",
      "Sijie Cheng",
      "Yu Wu",
      "Yue Zhang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.62": {
    "title": "Weakly Supervised Pre-Training for Multi-Hop Retriever",
    "volume": "findings",
    "abstract": "In multi-hop QA, answering complex questions entails iterative document retrieval for finding the missing entity of the question. The main steps of this process are sub-question detection, document retrieval for the subquestion, and generation of a new query for the final document retrieval. However, building a dataset that contains complex questions with sub-questions and their corresponding documents requires costly human annotation. To address the issue, we propose a new method for weakly supervised multi-hop retriever pretraining without human efforts. Our method includes 1) a pre-training task for generating vector representations of complex questions, 2) a scalable data generation method that produces the nested structure of question and subquestion as weak supervision for pre-training, and 3) a pre-training model structure based on dense encoders. We conduct experiments to compare the performance of our pre-trained retriever with several state-of-the-art models on end-to-end multi-hop QA as well as document retrieval. The experimental results show that our pre-trained retriever is effective and also robust on limited data and computational resources",
    "checked": true,
    "id": "9651c3f83b9310829622305f5316443253861fba",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Yeon Seonwoo",
      "Sang-Woo Lee",
      "Ji-Hoon Kim",
      "Jung-Woo Ha",
      "Alice Oh"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.63": {
    "title": "Meet The Truth: Leverage Objective Facts and Subjective Views for Interpretable Rumor Detection",
    "volume": "findings",
    "abstract": "Existing rumor detection strategies typically provide detection labels while ignoring their explanation. Nonetheless, providing pieces of evidence to explain why a suspicious tweet is rumor is essential. As such, a novel model, LOSIRD, was proposed in this paper. First, LOSIRD mines appropriate evidence sentences and classifies them by automatically checking the veracity of the relationship of the given claim and its evidence from about 5 million Wikipedia documents. LOSIRD then automatically constructs two heterogeneous graph objects to simulate the propagation layout of the tweets and code the relationship of evidence. Finally, a graphSAGE processing component is used in LOSIRD to provide the label and evidence. To the best of our knowledge, we are the first one who combines objective facts and subjective views to verify rumor. The experimental results on two real-world Twitter datasets showed that our model exhibited the best performance in the early rumor detection task and its rumor detection performance outperformed other baseline and state-of-the-art models. Moreover, we confirmed that both objective information and subjective information are fundamental clues for rumor detection",
    "checked": true,
    "id": "cff464e5dfe16f48559bf6353da311dc8d13a2ed",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Jiawen Li",
      "Shiwen Ni",
      "Hung-Yu Kao"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.64": {
    "title": "Read, Listen, and See: Leveraging Multimodal Information Helps Chinese Spell Checking",
    "volume": "findings",
    "abstract": "Chinese Spell Checking (CSC) aims to detect and correct erroneous characters for usergenerated text in Chinese language. Most of the Chinese spelling errors are misused semantically, phonetically or graphically similar characters. Previous attempts notice this phenomenon and try to utilize the similarity relationship for this task. However, these methods use either heuristics or handcrafted confusion sets to predict the correct character. In this paper, we propose a Chinese spell checker called REALISE, by directly leveraging the multimodal information of the Chinese characters. The REALISE model tackles the CSC task by (1) capturing the semantic, phonetic and graphic information of the input characters, and (2) selectively mixing the information in these modalities to predict the correct output. Experiments1 on the SIGHAN benchmarks show that the proposed model outperforms strong baselines by a large margin",
    "checked": true,
    "id": "b92abf442f5cc4971556b157b8e14d17fd600f4b",
    "semantic_title": "",
    "citation_count": 36,
    "authors": [
      "Heng-Da Xu",
      "Zhongli Li",
      "Qingyu Zhou",
      "Chao Li",
      "Zizhen Wang",
      "Yunbo Cao",
      "Heyan Huang",
      "Xian-Ling Mao"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.65": {
    "title": "TransSum: Translating Aspect and Sentiment Embeddings for Self-Supervised Opinion Summarization",
    "volume": "findings",
    "abstract": "In this paper, we propose a novel selfsupervised opinion summarization framework TransSum, which models opinion summaries as translations operating on the low-dimensional aspect and sentiment embedding spaces. Specifically, we propose two contrastive objectives to learn the crucial aspect and sentiment embeddings of reviews, by taking advantage of the intraand inter-group invariances that have not been considered in previous studies. Furthermore, these embeddings can be used to reduce opinion redundancy and construct highly relevant reviews-summary pairs to train a supervised multi-input opinion summarization model. Experimental results on three different domains show that TransSum outperforms several strong baselines in generating informative, relevant and low-redundant summaries, unveiling the effectiveness of our approach",
    "checked": true,
    "id": "82581fe6523ee22021123aed76c4312e3d291c52",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Ke Wang",
      "Xiaojun Wan"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.66": {
    "title": "Hashing based Efficient Inference for Image-Text Matching",
    "volume": "findings",
    "abstract": "August 1–6, 2021. ©2021 Association for Computational Linguistics 743 Hashing based Efficient Inference for Image-Text Matching Rong-Cheng Tu†, Lei Ji ‡§¶∗ , Huaishao Luo‖, Botian Shio, Heyan Huang†, Nan Duan¶ and Xian-Ling Mao† †School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China, Beijing, China ‡Institute of Computing Technology, CAS, Beijing, China §University of Chinese Academy of Sciences, Beijing, China ¶Microsoft Research Asia, Beijing, China ‖Southwest Jiaotong University, Chengdu, China oShanghai AI lab, Shanghai, China †{turongcheng, hhy63, maoxl}@bit.edu.cn, ¶{leiji,nanduan}@microsoft.com Abstract",
    "checked": true,
    "id": "b7ef3452894180224542c23a647ceed839d829f2",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Rong-Cheng Tu",
      "Lei Ji",
      "Huaishao Luo",
      "Botian Shi",
      "Heyan Huang",
      "Nan Duan",
      "Xian-Ling Mao"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.67": {
    "title": "Can the Transformer Learn Nested Recursion with Symbol Masking?",
    "volume": "findings",
    "abstract": "We investigate if, given a simple symbol masking strategy, self-attention models are capable of learning nested structures and generalise over their depth. We do so in the simplest setting possible, namely languages consisting of nested parentheses of several kinds. We use encoder-only models, which we train to predict randomly masked symbols, in a BERTlike fashion. We find that the accuracy is well above random baseline, with accuracy consistently above 50% both when increasing nesting depth and distances between training and testing. However, we find that the predictions made correspond to a simple parenthesis counting strategy, rather than a push-down automaton. This suggests that self-attention models are not suitable for tasks which require generalisation to more complex instances of recursive structures than those found in the training set",
    "checked": true,
    "id": "1ea54f567c222619754417229d1eafebaa892bd5",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Jean-Philippe Bernardy",
      "Adam Ek",
      "Vladislav Maraev"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.68": {
    "title": "Rationalization through Concepts",
    "volume": "findings",
    "abstract": "Automated predictions require explanations to be interpretable by humans. One type of explanation is a rationale, i.e., a selection of input features such as relevant text snippets from which the model computes the outcome. However, a single overall selection does not provide a complete explanation, e.g., weighing several aspects for decisions. To this end, we present a novel self-interpretable model called ConRAT. Inspired by how human explanations for high-level decisions are often based on key concepts, ConRAT extracts a set of text snippets as concepts and infers which ones are described in the document. Then, it explains the outcome with a linear aggregation of concepts. Two regularizers drive ConRAT to build interpretable concepts. In addition, we propose two techniques to boost the rationale and predictive performance further. Experiments on both singleand multi-aspect sentiment classification tasks show that ConRAT is the first to generate concepts that align with human rationalization while using only the overall label. Further, it outperforms state-of-the-art methods trained on each aspect label independently",
    "checked": true,
    "id": "3097f7d9a2696d4ec06999dee4a50052c01453a4",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Diego Antognini",
      "Boi Faltings"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.69": {
    "title": "Parallel Attention Network with Sequence Matching for Video Grounding",
    "volume": "findings",
    "abstract": "Given a video, video grounding aims to retrieve a temporal moment that semantically corresponds to a language query. In this work, we propose a Parallel Attention Network with Sequence matching (SeqPAN) to address the challenges in this task: multi-modal representation learning, and target moment boundary prediction. We design a self-guided parallel attention module to effectively capture selfmodal contexts and cross-modal attentive information between video and text. Inspired by sequence labeling tasks in natural language processing, we split the ground truth moment into begin, inside, and end regions. We then propose a sequence matching strategy to guide start/end boundary predictions using region labels. Experimental results on three datasets show that SeqPAN is superior to state-of-theart methods. Furthermore, the effectiveness of the self-guided parallel attention module and the sequence matching module is verified.1",
    "checked": true,
    "id": "efe0746d10299ddce2e71c2285f1601b39709a96",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Hao Zhang",
      "Aixin Sun",
      "Wei Jing",
      "Liangli Zhen",
      "Joey Tianyi Zhou",
      "Siow Mong Rick Goh"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.70": {
    "title": "MusicBERT: Symbolic Music Understanding with Large-Scale Pre-Training",
    "volume": "findings",
    "abstract": "Symbolic music understanding, which refers to the understanding of music from the symbolic data (e.g., MIDI format, but not audio), covers many music applications such as genre classification, emotion classification, and music pieces matching. While good music representations are beneficial for these applications, the lack of training data hinders representation learning. Inspired by the success of pre-training models in natural language processing, in this paper, we develop MusicBERT, a large-scale pre-trained model for music understanding. To this end, we construct a large-scale symbolic music corpus that contains more than 1 million music songs. Since symbolic music contains more structural (e.g., bar, position) and diverse information (e.g., tempo, instrument, and pitch), simply adopting the pre-training techniques from NLP to symbolic music only brings marginal gains. Therefore, we design several mechanisms, including OctupleMIDI encoding and bar-level masking strategy, to enhance pre-training with symbolic music data. Experiments demonstrate the advantages of MusicBERT on four music understanding tasks, including melody completion, accompaniment suggestion, genre classification, and style classification. Ablation studies also verify the effectiveness of our designs of OctupleMIDI encoding and barlevel masking strategy in MusicBERT",
    "checked": true,
    "id": "79604e29340a8336003d1e8a348083b6249cc754",
    "semantic_title": "",
    "citation_count": 41,
    "authors": [
      "Mingliang Zeng",
      "Xu Tan",
      "Rui Wang",
      "Zeqian Ju",
      "Tao Qin",
      "Tie-Yan Liu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.71": {
    "title": "Evaluating the Efficacy of Summarization Evaluation across Languages",
    "volume": "findings",
    "abstract": "While automatic summarization evaluation methods developed for English are routinely applied to other languages, this is the first attempt to systematically quantify their panlinguistic efficacy. We take a summarization corpus for eight different languages, and manually annotate generated summaries for focus (precision) and coverage (recall). Based on this, we evaluate 19 summarization evaluation metrics, and find that using multilingual BERT within BERTScore performs well across all languages, at a level above that for English",
    "checked": true,
    "id": "3877fed12c8b3c422c74eaed01f1b8357ca3338b",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Fajri Koto",
      "Jey Han Lau",
      "Timothy Baldwin"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.72": {
    "title": "CoMAE: A Multi-factor Hierarchical Framework for Empathetic Response Generation",
    "volume": "findings",
    "abstract": "The capacity of empathy is crucial to the success of open-domain dialog systems. Due to its nature of multi-dimensionality, there are various factors that relate to empathy expression, such as communication mechanism, dialog act and emotion. However, existing methods for empathetic response generation usually either consider only one empathy factor or ignore the hierarchical relationships between different factors, leading to a weak ability of empathy modeling. In this paper, we propose a multi-factor hierarchical framework, CoMAE, for empathetic response generation, which models the above three key factors of empathy expression in a hierarchical way. We show experimentally that our CoMAEbased model can generate more empathetic responses than previous methods. We also highlight the importance of hierarchical modeling of different factors through both the empirical analysis on a real-life corpus and the extensive experiments. Our codes and used data are available at https://github.com/ chujiezheng/CoMAE",
    "checked": true,
    "id": "e6d03bf7d9b961dff94e71c46086f09791531b7b",
    "semantic_title": "",
    "citation_count": 31,
    "authors": [
      "Chujie Zheng",
      "Yong Liu",
      "Wei Chen",
      "Yongcai Leng",
      "Minlie Huang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.73": {
    "title": "UniKeyphrase: A Unified Extraction and Generation Framework for Keyphrase Prediction",
    "volume": "findings",
    "abstract": "Keyphrase Prediction (KP) task aims at predicting several keyphrases that can summarize the main idea of the given document. Mainstream KP methods can be categorized into purely generative approaches and integrated models with extraction and generation. However, these methods either ignore the diversity among keyphrases or only weakly capture the relation across tasks implicitly. In this paper, we propose UniKeyphrase , a novel end-to-end learning framework that jointly learns to extract and generate keyphrases. In UniKeyphrase, stacked relation layer and bag-of-words constraint are proposed to fully ex-ploit the latent semantic relation between extraction and generation in the view of model structure and training process, respectively. Experiments on KP benchmarks demonstrate that our joint approach outperforms mainstream methods by a large margin. 1",
    "checked": true,
    "id": "73c8cee29fd57097288a6919116b5c8a448f3030",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Huanqin Wu",
      "Wei Liu",
      "Lei Li",
      "Dan Nie",
      "Tao Chen",
      "Feng Zhang",
      "Di Wang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.74": {
    "title": "As Good as New. How to Successfully Recycle English GPT-2 to Make Models for Other Languages",
    "volume": "findings",
    "abstract": "Large generative language models have been very successful for English, but other languages lag behind due to data and computational limitations. We propose a method that may overcome these problems by adapting existing pre-trained language models to new languages. Specifically, we describe the adaptation of English GPT-2 to Italian and Dutch by retraining lexical embeddings without tuning the Transformer layers. As a result, we obtain lexical embeddings for Italian and Dutch that are aligned with the original English lexical embeddings and induce a bilingual lexicon from this alignment. Additionally, we show how to scale up complexity by transforming relearned lexical embeddings of GPT-2 small to the GPT-2 medium embedding space. This method minimises the amount of training and prevents losing information during adaptation that was learned by GPT-2. English GPT-2 models with relearned lexical embeddings can generate realistic sentences in Italian and Dutch, but on average these sentences are still identifiable as artificial by humans. Based on perplexity scores and human judgements, we find that generated sentences become more realistic with some additional full model finetuning, especially for Dutch. For Italian, we see that they are evaluated on par with sentences generated by a GPT-2 model fully trained from scratch. Our work can be conceived as a blueprint for training GPT-2s for other languages, and we provide a 'recipe' to do so",
    "checked": true,
    "id": "56446cb1da48cbe6e19e5051ed80c3861021e5ba",
    "semantic_title": "",
    "citation_count": 22,
    "authors": [
      "Wietse de Vries",
      "Malvina Nissim"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.75": {
    "title": "Can Cognate Prediction Be Modelled as a Low-Resource Machine Translation Task?",
    "volume": "findings",
    "abstract": "Cognate prediction is the task of generating, in a given language, the likely cognates of words in a related language, where cognates are words in related languages that have evolved from a common ancestor word. It is a task for which little data exists and which can aid linguists in the discovery of previously undiscovered relations. Previous work has applied machine translation (MT) techniques to this task, based on the tasks' similarities, without, however, studying their numerous differences or optimising architectural choices and hyper-parameters. In this paper, we investigate whether cognate prediction can benefit from insights from low-resource MT. We first compare statistical MT (SMT) and neural MT (NMT) architectures in a bilingual setup. We then study the impact of employing data augmentation techniques commonly seen to give gains in low-resource MT: monolingual pretraining, backtranslation and multilinguality. Our experiments on several Romance languages show that cognate prediction behaves only to a certain extent like a standard lowresource MT task. In particular, MT architectures, both statistical and neural, can be successfully used for the task, but using supplementary monolingual data is not always as beneficial as using additional language data, contrarily to what is observed for MT",
    "checked": true,
    "id": "37dd6bb2fde54ce667f189ac259b586f3afe2d7c",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Clémentine Fourrier",
      "Rachel Bawden",
      "Benoît Sagot"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.76": {
    "title": "What if This Modified That? Syntactic Interventions with Counterfactual Embeddings",
    "volume": "findings",
    "abstract": "Neural language models exhibit impressive performance on a variety of tasks, but their internal reasoning may be difficult to understand. Prior art aims to uncover meaningful properties within model representations via probes, but it is unclear how faithfully such probes portray information that the models actually use. To overcome such limitations, we propose a technique, inspired by causal analysis, for generating counterfactual embeddings within models. In experiments testing our technique, we produce evidence that suggests some BERT-based models use a tree-distancelike representation of syntax in downstream prediction tasks",
    "checked": true,
    "id": "0e0199c71361471d4517f4af86890b308f564ad5",
    "semantic_title": "",
    "citation_count": 21,
    "authors": [
      "Mycal Tucker",
      "Peng Qian",
      "Roger Levy"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.77": {
    "title": "Investigating Text Simplification Evaluation",
    "volume": "findings",
    "abstract": "Modern text simplification (TS) heavily relies on the availability of gold standard data to build machine learning models. However, existing studies show that parallel TS corpora contain inaccurate simplifications and incorrect alignments. Additionally, evaluation is usually performed by using metrics such as BLEU or SARI to compare system output to the gold standard. A major limitation is that these metrics do not match human judgements and the performance on different datasets and linguistic phenomena vary greatly. Furthermore, our research shows that the test and training subsets of parallel datasets differ significantly. In this work, we investigate existing TS corpora, providing new insights that will motivate the improvement of existing state-ofthe-art TS evaluation methods. Our contributions include the analysis of TS corpora based on existing modifications used for simplification and an empirical study on TS models performance by using better-distributed datasets. We demonstrate that by improving the distribution of TS datasets, we can build more robust TS models",
    "checked": true,
    "id": "7ca6c7af591be3ea45eb1fc63043eaa7afd22c7a",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Laura Vásquez-Rodríguez",
      "Matthew Shardlow",
      "Piotr Przybyła",
      "Sophia Ananiadou"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.78": {
    "title": "COM2SENSE: A Commonsense Reasoning Benchmark with Complementary Sentences",
    "volume": "findings",
    "abstract": "Commonsense reasoning is intuitive for humans but has been a long-term challenge for artificial intelligence (AI). Recent advancements in pretrained language models have shown promising results on several commonsense benchmark datasets. However, the reliability and comprehensiveness of these benchmarks towards assessing model's commonsense reasoning ability remains unclear. To this end, we introduce a new commonsense reasoning benchmark dataset comprising natural language true/false statements, with each sample paired with its complementary counterpart, resulting in 4k sentence pairs. We propose a pairwise accuracy metric to reliably measure an agent's ability to perform commonsense reasoning over a given situation. The dataset is crowdsourced and enhanced with an adversarial model-in-the-loop setup to incentivize challenging samples. To facilitate a systematic analysis of commonsense capabilities, we design our dataset along the dimensions of knowledge domains, reasoning scenarios and numeracy. Experimental results demonstrate that our strongest baseline (UnifiedQA-3B), after fine-tuning, achieves ~71% standard accuracy and ~51% pairwise accuracy, well below human performance (~95% for both metrics). The dataset is available at https://github. com/PlusLabNLP/Com2Sense",
    "checked": true,
    "id": "6597d61bdb531051678c773526758a6dc113b9ce",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Shikhar Singh",
      "Nuan Wen",
      "Yu Hou",
      "Pegah Alipoormolabashi",
      "Te-lin Wu",
      "Xuezhe Ma",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.79": {
    "title": "Towards Knowledge-Grounded Counter Narrative Generation for Hate Speech",
    "volume": "findings",
    "abstract": "Tackling online hatred using informed textual responses – called counter narratives – has been brought under the spotlight recently. Accordingly, a research line has emerged to automatically generate counter narratives in order to facilitate the direct intervention in the hate discussion and to prevent hate content from further spreading. Still, current neural approaches tend to produce generic/repetitive responses and lack grounded and up-to-date evidence such as facts, statistics, or examples. Moreover, these models can create plausible but not necessarily true arguments. In this paper we present the first complete knowledgebound counter narrative generation pipeline, grounded in an external knowledge repository that can provide more informative content to fight online hatred. Together with our approach, we present a series of experiments that show its feasibility to produce suitable and informative counter narratives in in-domain and cross-domain settings",
    "checked": true,
    "id": "473b70bb3c531b2d740fa6d652956e2733b53243",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Yi-Ling Chung",
      "Serra Sinem Tekiroğlu",
      "Marco Guerini"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.80": {
    "title": "SOLID: A Large-Scale Semi-Supervised Dataset for Offensive Language Identification",
    "volume": "findings",
    "abstract": "The use of offensive language is a major problem in social media which has led to an abundance of research in detecting content such as hate speech, cyberbulling, and cyber-aggression. There have been several attempts to consolidate and categorize these efforts. Recently, the OLID dataset used at SemEval-2019 proposed a hierarchical three-level annotation taxonomy which addresses different types of offensive language as well as important information such as the target of such content. The categorization provides meaningful and important information for understanding offensive language. However, the OLID dataset is limited in size, especially for some of the low-level categories, which included only a few hundred instances, thus making it challenging to train robust deep learning models. Here, we address this limitation by creating the largest available dataset for this task, SOLID. SOLID contains over nine million English tweets labeled in a semi-supervised manner. We further demonstrate experimentally that using SOLID along with OLID yields improved performance on the OLID test set for two different models, especially for the lower levels of the taxonomy. Finally, we perform analysis of the models' performance on easy and hard examples of offensive language using data annotated in a semi-supervised way",
    "checked": true,
    "id": "660732e9f04063c4e7f15dc83670033d261e8aaa",
    "semantic_title": "",
    "citation_count": 114,
    "authors": [
      "Sara Rosenthal",
      "Pepa Atanasova",
      "Georgi Karadzhov",
      "Marcos Zampieri",
      "Preslav Nakov"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.81": {
    "title": "RealFormer: Transformer Likes Residual Attention",
    "volume": "findings",
    "abstract": "Transformer is the backbone of modern NLP models. In this paper, we propose RealFormer, a simple Residual Attention Layer Transformer architecture that significantly outperforms canonical Transformers on a spectrum of tasks including Masked Language Modeling, GLUE, and SQuAD. Qualitatively, RealFormer is easy to implement and requires minimal hyper-parameter tuning. It also stabilizes training and leads to models with sparser attentions. Code will be open-sourced upon paper acceptance",
    "checked": true,
    "id": "6914a7997ff4be207fa7b3472a9c5879abaec646",
    "semantic_title": "",
    "citation_count": 44,
    "authors": [
      "Ruining He",
      "Anirudh Ravula",
      "Bhargav Kanagal",
      "Joshua Ainslie"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.82": {
    "title": "Promoting Graph Awareness in Linearized Graph-to-Text Generation",
    "volume": "findings",
    "abstract": "Generating text from structured inputs, such as meaning representations or RDF triples, has often involved the use of specialized graphencoding neural networks. However, recent applications of pretrained transformers to linearizations of graph inputs have yielded stateof-the-art generation results on graph-to-text tasks. Here, we explore the ability of these linearized models to encode local graph structures, in particular their invariance to the graph linearization strategy and their ability to reconstruct corrupted inputs. Our findings motivate solutions to enrich the quality of models' implicit graph encodings via scaffolding. Namely, we use graph-denoising objectives implemented in a multi-task text-to-text framework. We find that these denoising scaffolds lead to substantial improvements in downstream generation in low-resource settings",
    "checked": true,
    "id": "b37afeb5301445546bca3d7293044a77b1a8f25f",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Alexander Miserlis Hoyle",
      "Ana Marasović",
      "Noah A. Smith"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.83": {
    "title": "Predicting cross-linguistic adjective order with information gain",
    "volume": "findings",
    "abstract": "Languages vary in their placement of multiple adjectives before, after, or surrounding the noun, but they typically exhibit strong intralanguage tendencies on the relative order of those adjectives (e.g., the preference for 'big blue box' in English, 'grande boîte bleue' in French, and 'als.undūq al'azraq alkabı̄r' in Arabic). We advance a new quantitative account of adjective order across typologically-distinct languages based on maximizing information gain. Our model addresses the left-right asymmetry of French-type ANA sequences with the same approach as AAN and NAA orderings, without appeal to other mechanisms. We find that, across 32 languages, the preferred order of adjectives mirrors an efficient algorithm of maximizing information gain",
    "checked": true,
    "id": "2c81da90eed8ff8cfb4301c7ed9a3f2cf2b4ea0d",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "William Dyer",
      "Richard Futrell",
      "Zoey Liu",
      "Greg Scontras"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.84": {
    "title": "A Survey of Data Augmentation Approaches for NLP",
    "volume": "findings",
    "abstract": "Data augmentation has recently seen increased interest in NLP due to more work in lowresource domains, new tasks, and the popularity of large-scale neural networks that require large amounts of training data. Despite this recent upsurge, this area is still relatively underexplored, perhaps due to the challenges posed by the discrete nature of language data. In this paper, we present a comprehensive and unifying survey of data augmentation for NLP by summarizing the literature in a structured manner. We first introduce and motivate data augmentation for NLP, and then discuss major methodologically representative approaches. Next, we highlight techniques that are used for popular NLP applications and tasks. We conclude by outlining current challenges and directions for future research. Overall, our paper aims to clarify the landscape of existing literature in data augmentation for NLP and motivate additional work in this area. We also present a GitHub repository with a paper list that will be continuously updated at https://github.com/styfeng/DataAug4NLP",
    "checked": true,
    "id": "63d8426ba1f51a8525dd19fd8ec92934ec71aea5",
    "semantic_title": "",
    "citation_count": 321,
    "authors": [
      "Steven Y. Feng",
      "Varun Gangal",
      "Jason Wei",
      "Sarath Chandar",
      "Soroush Vosoughi",
      "Teruko Mitamura",
      "Eduard Hovy"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.85": {
    "title": "Why Machine Reading Comprehension Models Learn Shortcuts?",
    "volume": "findings",
    "abstract": "Recent studies report that many machine reading comprehension (MRC) models can perform closely to or even better than humans on benchmark datasets. However, existing works indicate that many MRC models may learn shortcuts to outwit these benchmarks, but the performance is unsatisfactory in real-world applications. In this work, we attempt to explore, instead of the expected comprehension skills, why these models learn the shortcuts. Based on the observation that a large portion of questions in current datasets have shortcut solutions, we argue that larger proportion of shortcut questions in training data make models rely on shortcut tricks excessively. To investigate this hypothesis, we carefully design two synthetic datasets with annotations that indicate whether a question can be answered using shortcut solutions. We further propose two new methods to quantitatively analyze the learning difficulty regarding shortcut and challenging questions, and revealing the inherent learning mechanism behind the different performance between the two kinds of questions. A thorough empirical analysis shows that MRC models tend to learn shortcut questions earlier than challenging questions, and the high proportions of shortcut questions in training sets hinder models from exploring the sophisticated reasoning skills in the later stage of training",
    "checked": true,
    "id": "476afc913d63f3ba1882f6419f718984379b2380",
    "semantic_title": "",
    "citation_count": 23,
    "authors": [
      "Yuxuan Lai",
      "Chen Zhang",
      "Yansong Feng",
      "Quzhe Huang",
      "Dongyan Zhao"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.86": {
    "title": "Handling Cross- and Out-of-Domain Samples in Thai Word Segmentation",
    "volume": "findings",
    "abstract": "While word segmentation is a solved problem in many languages, it is still a challenge in continuous-script or low-resource languages. Like other NLP tasks, word segmentation is domain-dependent, which can be a challenge in low-resource languages like Thai and Urdu since there can be domains with insufficient data. This investigation proposes a new solution to adapt an existing domaingeneric model to a target domain, as well as a data augmentation technique to combat the low-resource problems. In addition to domain adaptation, we also propose a framework to handle out-of-domain inputs using an ensemble of domain-specific models called MultiDomain Ensemble (MDE). To assess the effectiveness of the proposed solutions, we conducted extensive experiments on domain adaptation and out-of-domain scenarios. Moreover, we also proposed a multiple task dataset for Thai text processing, including word segmentation. For domain adaptation, we compared our solution to the state-of-the-art Thai word segmentation (TWS) method and obtained improvements from 93.47% to 98.48% at the character level and 84.03% to 96.75% at the word level. For out-of-domain scenarios, our MDE method significantly outperformed the state-of-the-art TWS and multi-criteria methods. Furthermore, to demonstrate our method's generalizability, we also applied our MDE framework to other languages, namely Chinese, Japanese, and Urdu, and obtained improvements similar to Thai's",
    "checked": true,
    "id": "9c3efccaabab034634629364758d15518423386d",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Peerat Limkonchotiwat",
      "Wannaphong Phatthiyaphaibun",
      "Raheem Sarwar",
      "Ekapol Chuangsuwanich",
      "Sarana Nutanong"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.87": {
    "title": "Sensei: Self-Supervised Sensor Name Segmentation",
    "volume": "findings",
    "abstract": "Sensor names as alphanumeric strings typically encode their key contextual information such as their function or physical location. We focus here on sensors used in smart building applications. In these applications, sensor names are curated in a building vendorspecific manner using different structures and esoteric vocabularies. Tremendous manual effort is needed to annotate sensor nodes for each building or even to just segment these sensor names into meaningful chunks for intelligent operation of buildings. We propose here a fully automated self-supervised framework, Sensei, that can learn to segment sensor names without any human annotation. We employ a neural language model to capture the underlying structure in sensor names and then induce self-supervision based on information from the language model to build the segmentation model. Extensive experiments on five real-world buildings comprising thousands of sensors demonstrate the superiority of Sensei over baseline methods",
    "checked": true,
    "id": "e6f195cde91ab408f750525dfb3554ac2432b790",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaman Wu",
      "Dezhi Hong",
      "Rajesh Gupta",
      "Jingbo Shang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.88": {
    "title": "Frustratingly Simple Few-Shot Slot Tagging",
    "volume": "findings",
    "abstract": "We propose a simple and effective few-shot model for slot tagging. Recent work shows that it is promising to extend standard fewshot classification methods to sequence labeling with CRF-specific augmentations. Such methods show strengths in encoding slot name semantics and slot dependencies. However, we find these strengths can be obtained by a much simpler method, which casts slot tagging into machine reading comprehension (MRC). We fine-tune a standard BERT-based MRC model with a mixture of source domain and (few-shot) target domain data. Such simple method outperforms state-of-the-art methods by a large margin on the SNIPS dataset",
    "checked": true,
    "id": "d43f96523af43615950659561a8a9ccbccf812e1",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Jianqiang Ma",
      "Zeyu Yan",
      "Chang Li",
      "Yang Zhang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.89": {
    "title": "Medical Code Assignment with Gated Convolution and Note-Code Interaction",
    "volume": "findings",
    "abstract": "Medical code assignment from clinical text is a fundamental task in clinical information system management. As medical notes are typically lengthy and the medical coding system's code space is large, this task is a long-standing challenge. Recent work applies deep neural network models to encode the medical notes and assign medical codes to clinical documents. However, these methods are still ineffective as they do not fully encode and capture the lengthy and rich semantic information of medical notes nor explicitly exploit the interactions between the notes and codes. We propose a novel method, gated convolutional neural networks, and a note-code interaction (GatedCNN-NCI), for automatic medical code assignment to overcome these challenges. Our methods capture the rich semantic information of the lengthy clinical text for better representation by utilizing embedding injection and gated information propagation in the medical note encoding module. With a novel note-code interaction design and a graph message passing mechanism, we explicitly capture the underlying dependency between notes and codes, enabling effective code prediction. A weight sharing scheme is further designed to decrease the number of trainable parameters. Empirical experiments on real-world clinical datasets show that our proposed model outperforms state-of-the-art models in most cases, and our model size is on par with light-weighted baselines",
    "checked": true,
    "id": "6fe24352834ee2b77d15f7d9af7aaa1a896598bc",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Shaoxiong Ji",
      "Shirui Pan",
      "Pekka Marttinen"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.90": {
    "title": "Dynamic Semantic Graph Construction and Reasoning for Explainable Multi-hop Science Question Answering",
    "volume": "findings",
    "abstract": "Knowledge retrieval and reasoning are two key stages in multi-hop question answering (QA) at web scale. Existing approaches suffer from low confidence when retrieving evidence facts to fill the knowledge gap and lack transparent reasoning process. In this paper, we propose a new framework to exploit more valid facts while obtaining explainability for multi-hop QA by dynamically constructing a semantic graph and reasoning over it. We employ Abstract Meaning Representation (AMR) as semantic graph representation. Our framework contains three new ideas: (a) AMR-SG, an AMR-based Semantic Graph, constructed by candidate fact AMRs to uncover any hop relations among question, answer and multiple facts. (b) A novel path-based fact analytics approach exploiting AMR-SG to extract active facts from a large fact pool to answer questions. (c) A fact-level relation modeling leveraging graph convolution network (GCN) to guide the reasoning process. Results on two scientific multi-hop QA datasets show that we can surpass recent approaches including those using additional knowledge graphs while maintaining high explainability on OpenBookQA and achieve a new state-ofthe-art result on ARC-Challenge in a computationally practicable setting",
    "checked": true,
    "id": "8f894c51cae3f5e4067b41b139cf1e9ba5598a4a",
    "semantic_title": "",
    "citation_count": 19,
    "authors": [
      "Weiwen Xu",
      "Huihui Zhang",
      "Deng Cai",
      "Wai Lam"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.91": {
    "title": "Addressing Inquiries about History: An Efficient and Practical Framework for Evaluating Open-domain Chatbot Consistency",
    "volume": "findings",
    "abstract": "A good open-domain chatbot should avoid presenting contradictory responses about facts or opinions in a conversational session, known as its consistency capacity. However, evaluating the consistency capacity of a chatbot is still challenging. Employing human judges to interact with chatbots on purpose to check their capacities is costly and low-efficient, and difficult to get rid of subjective bias. In this paper, we propose the Addressing Inquiries about History (AIH), an efficient and practical framework for the consistency evaluation. At the conversation stage, AIH attempts to address appropriate inquiries about the dialogue history to induce the chatbot to redeclare the historical facts or opinions. We carry out the conversation between chatbots, which is more efficient than the human-bot interaction and can also alleviate the subjective bias. In this way, we manage to rapidly obtain a dialog session that contains responses with high contradiction possibilities. At the contradiction recognition stage, we can either employ human judges or a natural language inference (NLI) model to recognize whether the answers to the inquiries are contradictory with history. Finally, we are able to rank chatbots according to the contradiction statistics. Experiments on open-domain chatbots show that our approach can efficiently and reliably assess the consistency capacity of chatbots and achieve a high ranking correlation with the human evaluation. We release the framework and hope to help improve the consistency capacity of chatbots.1",
    "checked": true,
    "id": "f9ce79b7e238be2e5ca228672181169bb5bc3029",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Zekang Li",
      "Jinchao Zhang",
      "Zhengcong Fei",
      "Yang Feng",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.92": {
    "title": "Investigating the Reordering Capability in CTC-based Non-Autoregressive End-to-End Speech Translation",
    "volume": "findings",
    "abstract": "We study the possibilities of building a non-autoregressive speech-to-text translation model using connectionist temporal classification (CTC), and use CTC-based automatic speech recognition as an auxiliary task to improve the performance. CTC's success on translation is counter-intuitive due to its monotonicity assumption, so we analyze its reordering capability. Kendall's tau distance is introduced as the quantitative metric, and gradientbased visualization provides an intuitive way to take a closer look into the model. Our analysis shows that transformer encoders have the ability to change the word order and points out the future research direction that worth being explored more on non-autoregressive speech translation.1",
    "checked": true,
    "id": "4043c666017b4e843f5f3485ded1100962f4ed6d",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Shun-Po Chuang",
      "Yung-Sung Chuang",
      "Chih-Chiang Chang",
      "Hung-yi Lee"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.93": {
    "title": "Code Summarization with Structure-induced Transformer",
    "volume": "findings",
    "abstract": "Code summarization (CS) is becoming a promising area in recent language understanding, which aims to generate sensible human language automatically for programming language in the format of source code, serving in the most convenience of programmer developing. It is well known that programming languages are highly structured. Thus previous works attempt to apply structurebased traversal (SBT) or non-sequential models like Tree-LSTM and graph neural network (GNN) to learn structural program semantics. However, it is surprising that incorporating SBT into advanced encoder like Transformer instead of LSTM has been shown no performance gain, which lets GNN become the only rest means modeling such necessary structural clue in source code. To release such inconvenience, we propose structureinduced Transformer, which encodes sequential code inputs with multi-view structural clues in terms of a newly-proposed structureinduced self-attention mechanism. Extensive experiments show that our proposed structureinduced Transformer helps achieve new stateof-the-art results on benchmarks",
    "checked": true,
    "id": "77f9b54da444f49b2436712abe932627f16cbf95",
    "semantic_title": "",
    "citation_count": 25,
    "authors": [
      "Hongqiu Wu",
      "Hai Zhao",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.94": {
    "title": "Scheduled Dialog Policy Learning: An Automatic Curriculum Learning Framework for Task-oriented Dialog System",
    "volume": "findings",
    "abstract": "In reinforcement learning (RL) based taskoriented dialogue systems, users act as the environment and the agent learns the policy by interacting with users. However, due to the subjectivity of different users, the complexity of user-generated training conversations varies greatly, which leads to different difficulties for the agent to learn. Therefore, it is necessary for modeling dialogue complexity and make a reasonable learning schedule for efficiently training the agent. Towards that, we propose Scheduled Dialog Policy Learning, an automatic curriculum learning framework for jointing curriculum learning and policy optimization in the task-oriented dialog system. To our best knowledge, it is the first RL framework that improves dialogue policy learning by scheduling its learning process. Specifically, we introduce an automatic measurement to evaluate the dialogue complexity, and based on this automatic measurement, we train the dialog agent from easy dialogues to complex ones. Experiments demonstrate that our approach can be applied to the task-oriented dialogue policy learning and outperforms the previous state-of-the-art model, which increases 9.6% and 10.0% in the accuracy on the dialog success rate, respectively on the MultiWoz and Movie-Ticket Booking datasets",
    "checked": true,
    "id": "b7a7260270f1c596bd82ead3fa73d2ac2c4c59bf",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Sihong Liu",
      "Jinchao Zhang",
      "Keqing He",
      "Weiran Xu",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.95": {
    "title": "Do Explanations Help Users Detect Errors in Open-Domain QA? An Evaluation of Spoken vs. Visual Explanations",
    "volume": "findings",
    "abstract": "While research on explaining predictions of open-domain QA systems (ODQA) is gaining momentum, most works do not evaluate whether these explanations improve user trust. Furthermore, many users interact with ODQA using voice-assistants, yet prior works exclusively focus on visual displays, risking (as we also show) incorrectly extrapolating the effectiveness of explanations across modalities. To better understand the effectiveness of ODQA explanations strategies in the wild, we conduct user studies that measure whether explanations help users correctly decide when to accept or reject an ODQA system's answer. Unlike prior work, we control for explanation modality, i.e., whether they are communicated to users through a spoken or visual interface, and contrast effectiveness across modalities. We show that explanations derived from retrieved evidence can outperform strong baselines across modalities but the best explanation strategy varies with the modality. We show common failure cases of current explanations, emphasize end-to-end evaluation of explanations, and caution against evaluating them in proxy modalities that differ from deployment",
    "checked": true,
    "id": "ef6832885f9f2fb7fe9e14e4b54499e54508e88e",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Ana Valeria González",
      "Gagan Bansal",
      "Angela Fan",
      "Yashar Mehdad",
      "Robin Jia",
      "Srinivasan Iyer"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.96": {
    "title": "OntoEA: Ontology-guided Entity Alignment via Joint Knowledge Graph Embedding",
    "volume": "findings",
    "abstract": "Semantic embedding has been widely investigated for aligning knowledge graph (KG) entities. Current methods have explored and utilized the graph structure, the entity names, and attributes, but ignore the ontology (or ontological schema) which contains critical meta information such as classes and their membership relationships with entities. In this paper, we propose an ontology-guided entity alignment method named OntoEA, where both KGs and their ontologies are jointly embedded, and the class hierarchy and the class disjointness are utilized to avoid false mappings. Extensive experiments on seven public and industrial benchmarks have demonstrated the state-ofthe-art performance of OntoEA and the effectiveness of the ontologies",
    "checked": true,
    "id": "af051c87cecca64c2de4ad9110608f7579766653",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Yuejia Xiang",
      "Ziheng Zhang",
      "Jiaoyan Chen",
      "Xi Chen",
      "Zhenxi Lin",
      "Yefeng Zheng"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.97": {
    "title": "Learning Algebraic Recombination for Compositional Generalization",
    "volume": "findings",
    "abstract": "Neural sequence models exhibit limited compositional generalization ability in semantic parsing tasks. Compositional generalization requires algebraic recombination, i.e., dynamically recombining structured expressions in a recursive manner. However, most previous studies mainly concentrate on recombining lexical units, which is an important but not sufficient part of algebraic recombination. In this paper, we propose LEAR, an end-toend neural model to learn algebraic recombination for compositional generalization. The key insight is to model the semantic parsing task as a homomorphism between a latent syntactic algebra and a semantic algebra, thus encouraging algebraic recombination. Specifically, we learn two modules jointly: a Composer for producing latent syntax, and an Interpreter for assigning semantic operations. Experiments on two realistic and comprehensive compositional generalization benchmarks demonstrate the effectiveness of our model. The source code is publicly available at https://github.com/microsoft/ContextualSP",
    "checked": true,
    "id": "6d00b1024298e5b64ee873028385f7bb4396b05d",
    "semantic_title": "",
    "citation_count": 21,
    "authors": [
      "Chenyao Liu",
      "Shengnan An",
      "Zeqi Lin",
      "Qian Liu",
      "Bei Chen",
      "Jian-Guang Lou",
      "Lijie Wen",
      "Nanning Zheng",
      "Dongmei Zhang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.98": {
    "title": "Out of Order: How important is the sequential order of words in a sentence in Natural Language Understanding tasks?",
    "volume": "findings",
    "abstract": "Do state-of-the-art natural language understanding models care about word order? Not always! We found 75% to 90% of the correct predictions of BERT-based classifiers, trained on many GLUE tasks, remain constant after input words are randomly shuffled. Although BERT embeddings are famously contextual, the contribution of each individual word to classification is almost unchanged even after its surrounding words are shuffled. BERTbased models exploit superficial cues (e.g. the sentiment of keywords in sentiment analysis; or the word-wise similarity between sequencepair inputs in natural language inference) to make correct decisions when tokens are randomly shuffled. Encouraging models to capture word order information improves the performance on most GLUE tasks and SQuAD 2.0. Our work suggests that many GLUE tasks are not challenging machines to understand the meaning of a sentence",
    "checked": true,
    "id": "776a49616c84d52e8fff9911c561e3bac90910eb",
    "semantic_title": "",
    "citation_count": 72,
    "authors": [
      "Thang Pham",
      "Trung Bui",
      "Long Mai",
      "Anh Nguyen"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.99": {
    "title": "RevCore: Review-Augmented Conversational Recommendation",
    "volume": "findings",
    "abstract": "Existing conversational recommendation (CR) systems usually suffer from insufficient item information when conducted on short dialogue history and unfamiliar items. Incorporating external information (e.g., reviews) is a potential solution to alleviate this problem. Given that reviews often provide a rich and detailed user experience on different interests, they are potential ideal resources for providing high-quality recommendations within an informative conversation. In this paper, we design a novel end-to-end framework, namely, Review-augmented Conversational Recommender (RevCore), where reviews are seamlessly incorporated to enrich item information and assist in generating both coherent and informative responses. In detail, we extract sentiment-consistent reviews, perform review-enriched and entity-based recommendations for item suggestions, as well as use a review-attentive encoder-decoder for response generation. Experimental results demonstrate the superiority of our approach in yielding better performance on both recommendation and conversation responding.1",
    "checked": true,
    "id": "21ad14109f25210d55af3e2ea68d7c60462ddbe2",
    "semantic_title": "",
    "citation_count": 24,
    "authors": [
      "Yu Lu",
      "Junwei Bao",
      "Yan Song",
      "Zichen Ma",
      "Shuguang Cui",
      "Youzheng Wu",
      "Xiaodong He"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.100": {
    "title": "Awakening Latent Grounding from Pretrained Language Models for Semantic Parsing",
    "volume": "findings",
    "abstract": "Recent years pretrained language models (PLMs) hit a success on several downstream tasks, showing their power on modeling language. To better understand and leverage what PLMs have learned, several techniques have emerged to explore syntactic structures entailed by PLMs. However, few efforts have been made to explore grounding capabilities of PLMs, which are also essential. In this paper, we highlight the ability of PLMs to discover which token should be grounded to which concept, if combined with our proposed erasingthen-awakening approach. Empirical studies on four datasets demonstrate that our approach can awaken latent grounding which is understandable to human experts, even if it is not exposed to such labels during training. More importantly, our approach shows great potential to benefit downstream semantic parsing models. Taking text-to-SQL as a case study, we successfully couple our approach with two off-the-shelf parsers, obtaining an absolute improvement of up to 9.8%",
    "checked": true,
    "id": "0098123efc851b67137c1028f7bac8d8bffbc8fd",
    "semantic_title": "",
    "citation_count": 23,
    "authors": [
      "Qian Liu",
      "Dejian Yang",
      "Jiahui Zhang",
      "Jiaqi Guo",
      "Bin Zhou",
      "Jian-Guang Lou"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.101": {
    "title": "Enhancing Label Correlation Feedback in Multi-Label Text Classification via Multi-Task Learning",
    "volume": "findings",
    "abstract": "In multi-label text classification (MLTC), each given document is associated with a set of correlated labels. To capture label correlations, previous classifier-chain and sequenceto-sequence models transform MLTC to a sequence prediction task. However, they tend to suffer from label order dependency, label combination over-fitting and error propagation problems. To address these problems, we introduce a novel approach with multi-task learning to enhance label correlation feedback. We first utilize a joint embedding (JE) mechanism to obtain the text and label representation simultaneously. In MLTC task, a document-label cross attention (CA) mechanism is adopted to generate a more discriminative document representation. Furthermore, we propose two auxiliary label co-occurrence prediction tasks to enhance label correlation learning: 1) Pairwise Label Co-occurrence Prediction (PLCP), and 2) Conditional Label Co-occurrence Prediction (CLCP). Experimental results on AAPD and RCV1-V2 datasets show that our method outperforms competitive baselines by a large margin. We analyze low-frequency label performance, label dependency, label combination diversity and coverage speed to show the effectiveness of our proposed method on label correlation learning. Our code is available at https://github.com/EiraZhang/LACO",
    "checked": true,
    "id": "c8d6174672f6b139759bed1c0a7d87166fa503b7",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Ximing Zhang",
      "Qian-Wen Zhang",
      "Zhao Yan",
      "Ruifang Liu",
      "Yunbo Cao"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.102": {
    "title": "Fusing Context Into Knowledge Graph for Commonsense Question Answering",
    "volume": "findings",
    "abstract": "Commonsense question answering (QA) requires a model to grasp commonsense and factual knowledge to answer questions about world events. Many prior methods couple language modeling with knowledge graphs (KG). However, although a KG contains rich structural information, it lacks the context to provide a more precise understanding of the concepts. This creates a gap when fusing knowledge graphs into language modeling, especially when there is insufficient labeled data. Thus, we propose to employ external entity descriptions to provide contextual information for knowledge understanding. We retrieve descriptions of related concepts from Wiktionary and feed them as additional input to pretrained language models. The resulting model achieves state-of-the-art result in the CommonsenseQA dataset and the best result among non-generative models in OpenBookQA",
    "checked": true,
    "id": "7dab194e7a49213f2bb5bf694dfbaf24976730d9",
    "semantic_title": "",
    "citation_count": 32,
    "authors": [
      "Yichong Xu",
      "Chenguang Zhu",
      "Ruochen Xu",
      "Yang Liu",
      "Michael Zeng",
      "Xuedong Huang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.103": {
    "title": "Unsupervised Energy-based Adversarial Domain Adaptation for Cross-domain Text Classification",
    "volume": "findings",
    "abstract": "Transferring knowledge from a label-rich domain (source domain) to a label-scarce domain (target domain) for pervasive cross-domain Text Classification (TC) is a non-trivial task. To overcome this issue, we propose EADA, a novel unsupervised energy-based adversarial domain adaptation framework. First, a deep pre-trained language model (e.g. RoBERTa) is leveraged as a shared feature extractor that maps the text sequences from both source and target domains to a feature space. Since the source features maintain good feature discriminability because of the full supervised training, we design a method that encourages target features towards the source ones via adversarial learning. An autoencoder is designed as an energy function that focuses on reconstructing source feature embeddings, while the feature extractor aims to generate source-like target feature embeddings to deceive the autoencoder. In this manner, the target feature embeddings become domain-invariant and inherit great discriminability. Extensive experiments on multidomain sentiment classification (Amazon review dataset) and Yes/No question-answering classification (BoolQ and MARCO dataset) are conducted. The experimental results validate that EADA largely alleviates the domain discrepancy while maintaining excellent discriminability and achieves state-of-the-art cross-domain TC performance",
    "checked": true,
    "id": "d2caa1cbea3629ada3039089cbd69b5570da2aef",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Han Zou",
      "Jianfei Yang",
      "Xiaojian Wu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.104": {
    "title": "Survival text regression for time-to-event prediction in conversations",
    "volume": "findings",
    "abstract": "Time-to-event prediction tasks are common in conversation modelling, for applications such as predicting the length of a conversation or when a user will stop contributing to a platform. Despite the fact that it is natural to frame such predictions as regression tasks, recent work has modelled them as classification tasks, determining whether the time-to-event is greater than a pre-determined cut-off point. While this allows for the application of classification models which are well studied in NLP, it imposes a formulation that is contrived, as well as less informative. In this paper, we explore how to handle time-to-event forecasting in conversations as regression tasks. We focus on a family of regression techniques known as survival regression, which are commonly used in the context of healthcare and reliability engineering. We adapt these models to time-to-event prediction in conversations, using linguistic markers as features. On three datasets, we demonstrate that they outperform commonly considered text regression methods and comparable classification models",
    "checked": true,
    "id": "1afa3ab80abda57920b8d456a6513e6f01cc82e7",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christine De Kock",
      "Andreas Vlachos"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.105": {
    "title": "Unsupervised Knowledge Selection for Dialogue Generation",
    "volume": "findings",
    "abstract": "Knowledge selection is an important and challenging task which could provide the appropriate knowledge for informative dialogue generation. However, the needed gold knowledge label is difficult to collect in reality. In this paper, we study knowledge selection for dialogue generation in the unsupervised scenario and propose a novel Distilled Distant Supervision Loss (DDSL) to supervise knowledge selection when the gold knowledge label is unknown. Specifically, we first obtain an oracle knowledge label via distant supervision and then leverage knowledge distillation to alleviate the noisy labeling problem of distant supervision. Furthermore, we propose a pretraining-finetuning strategy to deal with the mismatch knowledge selection problem that models tend to select the mismatched knowledge for dialogue generation in the unsupervised setting and will cause the degeneration of knowledge-aware decoder. Experiments on two knowledge-grounded dialogue datasets show that our approach manages to select knowledge more accurately in the unsupervised setting and generates more informative responses, even outperforming many strong supervised baselines.1",
    "checked": true,
    "id": "e88ea5108f4bb9c596ee359994fcc94158c3b101",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Xiuyi Chen",
      "Feilong Chen",
      "Fandong Meng",
      "Peng Li",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.106": {
    "title": "Minimax and Neyman–Pearson Meta-Learning for Outlier Languages",
    "volume": "findings",
    "abstract": "Model-agnostic meta-learning (MAML) has been recently put forth as a strategy to learn resource-poor languages in a sample-efficient fashion. Nevertheless, the properties of these languages are often not well represented by those available during training. Hence, we argue that the i.i.d. assumption ingrained in MAML makes it ill-suited for cross-lingual NLP. In fact, under a decision-theoretic framework, MAML can be interpreted as minimising the expected risk across training languages (with a uniform prior), which is known as Bayes criterion. To increase its robustness to outlier languages, we create two variants of MAML based on alternative criteria: Minimax MAML reduces the maximum risk across languages, while Neyman–Pearson MAML constrains the risk in each language to a maximum threshold. Both criteria constitute fully differentiable two-player games. In light of this, we propose a new adaptive optimiser solving for a local approximation to their Nash equilibrium. We evaluate both model variants on two popular NLP tasks, part-of-speech tagging and question answering. We report gains for their average and minimum performance across low-resource languages in zeroand few-shot settings, compared to joint multisource transfer and vanilla MAML. The code for our experiments is available at https:// github.com/rahular/robust-maml",
    "checked": true,
    "id": "302a691914b1e000ba260f88e6859d1b0ae35557",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Edoardo Maria Ponti",
      "Rahul Aralikatte",
      "Disha Shrivastava",
      "Siva Reddy",
      "Anders Søgaard"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.107": {
    "title": "On-the-Fly Attention Modulation for Neural Generation",
    "volume": "findings",
    "abstract": "Despite considerable advancements with deep neural language models (LMs), neural text generation still suffers from degeneration: the generated text is repetitive, generic, selfcontradictory, and often lacks commonsense. Our analyses on sentence-level attention patterns in LMs reveal that neural degeneration may be associated with insufficient learning of task-specific characteristics by the attention mechanism. This finding motivates onthe-fly attention modulation1– a simple but effective method that enables the injection of priors into attention computation during inference. Automatic and human evaluation results on three text generation benchmarks demonstrate that attention modulation helps LMs generate text with enhanced fluency, creativity, and commonsense reasoning, in addition to significantly reduce sentence-level repetition",
    "checked": true,
    "id": "1949df8dc876e2e1919640cd03242a832b3bfcb2",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Yue Dong",
      "Chandra Bhagavatula",
      "Ximing Lu",
      "Jena D. Hwang",
      "Antoine Bosselut",
      "Jackie Chi Kit Cheung",
      "Yejin Choi"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.108": {
    "title": "Grammar-Constrained Neural Semantic Parsing with LR Parsers",
    "volume": "findings",
    "abstract": "Target meaning representations for semantic parsing tasks are often based on programming or query languages, such as SQL, and can be formalized by a context-free grammar. Assuming a priori knowledge of the target domain, such grammars can be exploited to enforce syntactical constraints when predicting logical forms. To that end, we assess how syntactical parsers can be integrated into modern encoder-decoder frameworks. Specifically, we implement an attentional SEQ2SEQ model that uses an LR parser to maintain syntactically valid sequences throughout the decoding procedure. Compared to other approaches to grammar-guided decoding that modify the underlying neural network architecture or attempt to derive full parse trees, our approach is conceptually simpler, adds less computational overhead during inference and integrates seamlessly with current SEQ2SEQ frameworks. We present preliminary evaluation results against a recurrent SEQ2SEQ baseline on GEOQUERY and ATIS and demonstrate improved performance while enforcing grammatical constraints",
    "checked": true,
    "id": "b670b00ecc6531201f944c90227257bb0b3574f8",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Artur Baranowski",
      "Nico Hochgeschwender"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.109": {
    "title": "Enhanced Metaphor Detection via Incorporation of External Knowledge Based on Linguistic Theories",
    "volume": "findings",
    "abstract": "Use of external knowledge is an important and effective method applied widely in metaphor detection. Although existing knowledge-based methods perform well, when leveraging external knowledge, they take little consideration on linguistic theories of metaphor detection. Based on Metaphor Identification Procedure (MIP) and Select Preference Violation (SPV), directly using examples and definitions of words from the Oxford Dictionary1, we propose two BERT-based models for metaphor detection: ExampleBERT and DefinitionBERT. Experimental results show that our methods achieve state-of-the-art performance on two established metaphor datasets. Furthermore, we show that our DefinitionBERT is highly interpretable",
    "checked": true,
    "id": "9f4d1924211b72e77257328ded3980b32786e75d",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Chang Su",
      "Kechun Wu",
      "Yijiang Chen"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.110": {
    "title": "Controlling Text Edition by Changing Answers of Specific Questions",
    "volume": "findings",
    "abstract": "In this paper, we introduce the new task of controllable text edition, in which we take as input a long text, a question, and a target answer, and the output is a minimally modified text, so that it fits the target answer. This task is very important in many situations, such as changing some conditions, consequences, or properties in a legal document, or changing some key information of an event in a news text. This is very challenging, as it is hard to obtain a parallel corpus for training, and we need to first find all text positions that should be changed and then decide how to change them. We constructed the new dataset WIKIBIOCTE for this task based on the existing dataset WIKIBIO (originally created for table-to-text generation). We use WIKIBIOCTE for training, and manually labeled a test set for testing. We also propose novel evaluation metrics and a novel method for solving the new task. Experimental results on the test set show that our proposed method is a good fit for this novel NLP task",
    "checked": true,
    "id": "72024eef4198f88130ba3b8823bdbda9168cc493",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Lei Sha",
      "Patrick Hohenecker",
      "Thomas Lukasiewicz"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.111": {
    "title": "Grammar-Based Patches Generation for Automated Program Repair",
    "volume": "findings",
    "abstract": "Automated program repair (APR) aims to find an automatic solution to program language bugs without human intervention, and it can potentially reduce debugging costs and improve software quality. Conventional approaches adopt learning-based methods such as sequence-to-sequence models for the patches generation. However, they tend to ignore the code structure information and suffer from grammar and syntax errors. To consider the grammar and syntax information, in this paper, we propose a grammar-based ruleto-rule model, which regards the repair process as the transformation of grammar rules, and leverages two encoders modeling both the original token sequence and the grammar rules, enhanced with a new tree-based self-attention. Besides, to guarantee grammar correctness, we employ a grammatically restricted inference method to generate each grammar rule in a legally constrained sub-search-space considering the generated previous rules. Experimental evaluations on a Java dataset demonstrate that the proposed approach significantly outperforms the state-of-the-art baselines in terms of generated code accuracy",
    "checked": true,
    "id": "c610aeef3a8da5150ff25d5e6bd8c2dd98dd4bee",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Yu Tang",
      "Long Zhou",
      "Ambrosio Blanco",
      "Shujie Liu",
      "Furu Wei",
      "Ming Zhou",
      "Muyun Yang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.112": {
    "title": "Manual Evaluation Matters: Reviewing Test Protocols of Distantly Supervised Relation Extraction",
    "volume": "findings",
    "abstract": "Distantly supervised (DS) relation extraction (RE) has attracted much attention in the past few years as it can utilize large-scale autolabeled data. However, its evaluation has long been a problem: previous works either take costly and inconsistent methods to manually examine a small sample of model predictions, or directly test models on auto-labeled data— which, by our check, produce as much as 53% wrong labels at the entity pair level in the popular NYT10 dataset. This problem has not only led to inaccurate evaluation, but also made it hard to understand where we are and what's left to improve in the research of DSRE. To evaluate DS-RE models more credibly, we build manually-annotated test sets for two DS-RE datasets, NYT10 and Wiki20, and thoroughly evaluate several competitive models, especially the latest pre-trained ones. The experimental results show that the manual evaluation can indicate very different conclusions from automatic ones, especially some unexpected observations, e.g., pre-trained models can achieve dominating performance while being more susceptible to false-positives compared with previous methods. We hope that both our manual test sets and observations can help advance future DS-RE research.1",
    "checked": true,
    "id": "1b5f62d474c2bdc21ec1c816f7c7353c0d735d63",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Tianyu Gao",
      "Xu Han",
      "Yuzhuo Bai",
      "Keyue Qiu",
      "Zhiyu Xie",
      "Yankai Lin",
      "Zhiyuan Liu",
      "Peng Li",
      "Maosong Sun",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.113": {
    "title": "GCRC: A New Challenging MRC Dataset from Gaokao Chinese for Explainable Evaluation",
    "volume": "findings",
    "abstract": "Recently, driven by numerous publicly available machine reading comprehension (MRC) datasets, MRC systems have made some progress. These datasets, however, have two major limitations: 1) the defined tasks are relatively simple, and 2) they do not provide explainable evaluation which is critical to objectively and comprehensively review the reasoning capabilities of current MRC systems. In this paper, we propose GCRC, a new dataset with challenging and high-quality multi-choice questions, collected from Gaokao Chinese (Chinese subject from the National College Entrance Examination of China). We have manually labelled three types of evidence to evaluate MRC systems' reasoning process: 1) sentence-level relevant supporting facts in an article required for answering a given question, 2) error reason of a distractor (i.e., an incorrect option) for explaining why a distractor should be eliminated, which is an important reasoning step for multi-choice questions, and 3) types of reasoning skills required for answering questions. Extensive experiments show that our proposed dataset is more challenging and very useful for identifying the limitations of existing MRC systems in an explainable way, facilitating researchers to develop novel machine learning and reasoning approaches to tackle this challenging research problem.1 *These authors contributed equally. Corresponding author Resources will be available through https:// github.com/SXUNLP/GCRC",
    "checked": true,
    "id": "5d1e996bb64a0d082c85010b9c78036db4da02e0",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Hongye Tan",
      "Xiaoyue Wang",
      "Yu Ji",
      "Ru Li",
      "Xiaoli Li",
      "Zhiwei Hu",
      "Yunxiao Zhao",
      "Xiaoqi Han"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.114": {
    "title": "Zero-shot Label-Aware Event Trigger and Argument Classification",
    "volume": "findings",
    "abstract": "Identifying events and mapping them to a pre-defined taxonomy of event types has long been an important NLP problem. Most previous work has relied heavily on labor-intensive, domain-specific, annotation, ignoring the semantic meaning of the event types' labels. Consequently, the learned models cannot effectively generalize to new label taxonomies and domains. We propose a zero-shot event extraction approach, which first identifies events with existing tools (e.g., SRL) and then maps them to a given taxonomy of event types in a zero-shot manner. Specifically, we leverage label representations induced by pre-trained language models, and map identified events to the target types via representation similarity. To semantically type the events' arguments, we further use the definition of the events (e.g., argument of type \"Victim\" appears as the argument of event of type \"Attack\") as global constraints to regularize the prediction. The proposed approach is shown to be very effective on the ACE-2005 dataset, which has 33 trigger and 22 argument types. Without using any annotation, we successfully map 83% of the triggers and 54% of the arguments to the semantic correct types, almost doubling the performance of previous zero-shot approaches1",
    "checked": true,
    "id": "fe137722e59afd9de04c012b0372a4d168bd4c71",
    "semantic_title": "",
    "citation_count": 24,
    "authors": [
      "Hongming Zhang",
      "Haoyu Wang",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.115": {
    "title": "Incorporating Global Information in Local Attention for Knowledge Representation Learning",
    "volume": "findings",
    "abstract": "Graph Attention Networks (GATs) have proven a promising model that takes advantage of localized attention mechanism to perform knowledge representation learning (KRL) on graph-structure data, e.g., Knowledge Graphs (KGs). While such approaches model entities' local pairwise importance, they lack the capability to model global importance relative to other entities of KGs. This causes such models to miss critical information in tasks where global information is also a significant component for the task, such as in knowledge representation learning. To address the issue, we allow the proper incorporation of global information into the GAT family of models through the use of scaled entity importance, which is calculated by an attention-based global random walk algorithm. In the context of KRL, incorporating global information boosts performance significantly. Experimental results on KG entity prediction against the state-of-thearts sufficiently demonstrate the effectiveness of our proposed model",
    "checked": true,
    "id": "114cd7727c71a1eb28b491e63082405d0af8fac5",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Yu Zhao",
      "Han Zhou",
      "Ruobing Xie",
      "Fuzhen Zhuang",
      "Qing Li",
      "Ji Liu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.116": {
    "title": "Exploiting Position Bias for Robust Aspect Sentiment Classification",
    "volume": "findings",
    "abstract": "Aspect sentiment classification (ASC) aims at determining sentiments expressed towards different aspects in a sentence. While state-ofthe-art ASC models have achieved remarkable performance, they are recently shown to suffer from the issue of robustness. Particularly in two common scenarios: when domains of test and training data are different (out-of-domain scenario) or test data is adversarially perturbed (adversarial scenario), ASC models may attend to irrelevant words and neglect opinion expressions that truly describe diverse aspects. To tackle the challenge, in this paper, we hypothesize that position bias (i.e., the words closer to a concerning aspect would carry a higher degree of importance) is crucial for building more robust ASC models by reducing the probability of mis-attending. Accordingly, we propose two mechanisms for capturing position bias, namely position-biased weight and position-biased dropout, which can be flexibly injected into existing models to enhance representations for classification. Experiments conducted on out-of-domain and adversarial datasets demonstrate that our proposed approaches largely improve the robustness and effectiveness of current models.1",
    "checked": true,
    "id": "70e22e27d0e533675e22ca976ca7b922b4a44a64",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Fang Ma",
      "Chen Zhang",
      "Dawei Song"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.117": {
    "title": "MRN: A Locally and Globally Mention-Based Reasoning Network for Document-Level Relation Extraction",
    "volume": "findings",
    "abstract": "Document-level relation extraction aims to detect the relations within one document, which is challenging since it requires complex reasoning using mentions, entities, local and global contexts. Few previous studies have distinguished local and global reasoning explicitly, which may be problematic because they play different roles in intraand inter-sentence relations. Moreover, the interactions between local and global contexts should be considered since they could help relation reasoning based on our observation. In this paper, we propose a novel mention-based reasoning (MRN) module based on explicitly and collaboratively local and global reasoning. Based on MRN, we design a co-predictor module to predict entity relations based on local and global entity and relation representations jointly. We evaluate our MRN model on three widelyused benchmark datasets, namely DocRED, CDR, and GDA. Experimental results show that our model outperforms previous state-ofthe-art models by a large margin",
    "checked": true,
    "id": "dc3864e841bf4d82a33b0ca303bf5fef256991d9",
    "semantic_title": "",
    "citation_count": 25,
    "authors": [
      "Jingye Li",
      "Kang Xu",
      "Fei Li",
      "Hao Fei",
      "Yafeng Ren",
      "Donghong Ji"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.118": {
    "title": "Adversary-Aware Rumor Detection",
    "volume": "findings",
    "abstract": "While social media becomes a primary source of news now, it also becomes more challenging for people to distinguish the rumors and non-rumors, which attracts malicious manipulation and may lead to public health harm or economic loss. Consequently, many rumor detection models have been proposed to automatically detect the rumors based on the contents and propagation path. However, most previous works are not aware of malicious attacks, e.g., framing. Therefore, we propose a novel rumor detection framework, Adversary-Aware Rumor Detection including Weighted-Edge Transformer-Graph Network and Position-aware Adversarial Response Generator, to improve the vulnerability of detection models. To the best of our knowledge, this is the first work that can generate the adversarial response with the consideration of the response position. Experimental results show that our model achieves the state-of-theart on various rumor detection tasks by the proposed Weighted-Edge Transformer-Graph Network and can maintain the performance under the adversarial response attack after the adversarial learning by Position-aware Adversarial Response Generator.1",
    "checked": true,
    "id": "35a3237dae11d0269c721aa1acd101c281e470a8",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Yun-Zhu Song",
      "Yi-Syuan Chen",
      "Yi-Ting Chang",
      "Shao-Yu Weng",
      "Hong-Han Shuai"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.119": {
    "title": "LICHEE: Improving Language Model Pre-training with Multi-grained Tokenization",
    "volume": "findings",
    "abstract": "Language model pre-training based on large corpora has achieved tremendous success in terms of constructing enriched contextual representations and has led to significant performance gains on a diverse range of Natural Language Understanding (NLU) tasks. Despite the success, most current pre-trained language models, such as BERT, are trained based on single-grained tokenization, usually with fine-grained characters or sub-words, making it hard for them to learn the precise meaning of coarse-grained words and phrases. In this paper, we propose a simple yet effective pretraining method named LICHEE to efficiently incorporate multi-grained information of input text. Our method can be applied to various pretrained language models and improve their representation capability. Extensive experiments conducted on CLUE and SuperGLUE demonstrate that our method achieves comprehensive improvements on a wide variety of NLU tasks in both Chinese and English with little extra inference cost incurred, and that our best ensemble model achieves the state-of-the-art performance on CLUE benchmark competition",
    "checked": true,
    "id": "27c39dd62635791a0ec3c0c81c2690e7a9bd62ad",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Weidong Guo",
      "Mingjun Zhao",
      "Lusheng Zhang",
      "Di Niu",
      "Jinwen Luo",
      "Zhenhua Liu",
      "Zhenyang Li",
      "Jianbo Tang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.120": {
    "title": "Detecting Hallucinated Content in Conditional Neural Sequence Generation",
    "volume": "findings",
    "abstract": "Neural sequence models can generate highly fluent sentences but recent studies have also shown that they are also prone to hallucinate additional content not supported by the input, which can cause a lack of trust in the model. To better assess the faithfulness of the machine outputs, we propose a new task to predict whether each token in the output sequence is hallucinated conditioned on the source input, and collect new manually annotated evaluation sets for this task. We also introduce a novel method for learning to model hallucination detection, based on pretrained language models fine tuned on synthetic data that includes automatically inserted hallucinations. Experiments on machine translation and abstract text summarization demonstrate the effectiveness of our proposed approach -- we obtain an average F1 of around 0.6 across all the benchmark datasets and achieve significant improvements in sentence-level hallucination scoring compared to baseline methods. We also release our annotated data and code for future research at this https URL",
    "checked": true,
    "id": "01ed852322e718d2c44c4debc8a64631070fa6df",
    "semantic_title": "",
    "citation_count": 72,
    "authors": [
      "Chunting Zhou",
      "Graham Neubig",
      "Jiatao Gu",
      "Mona Diab",
      "Francisco Guzmán",
      "Luke Zettlemoyer",
      "Marjan Ghazvininejad"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.121": {
    "title": "K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters",
    "volume": "findings",
    "abstract": "We study the problem of injecting knowledge into large pre-trained models like BERT and RoBERTa. Existing methods typically update the original parameters of pre-trained models when injecting knowledge. However, when multiple kinds of knowledge are injected, they may suffer from catastrophic forgetting. To address this, we propose K-Adapter, which remains the original parameters of the pre-trained model fixed and supports continual knowledge infusion. Taking RoBERTa as the pre-trained model, K-Adapter has a neural adapter for each kind of infused knowledge, like a plug-in connected to RoBERTa. There is no information flow between different adapters, thus different adapters are efficiently trained in a distributed way. We inject two kinds of knowledge, including factual knowledge obtained from automatically aligned text-triplets on Wikipedia and Wikidata, and linguistic knowledge obtained from dependency parsing. Results on three knowledge-driven tasks (total six datasets) including relation classification, entity typing and question answering demonstrate that each adapter improves the performance, and the combination of both adapters brings further improvements. Probing experiments further indicate that K-Adapter captures richer factual and commonsense knowledge than RoBERTa",
    "checked": true,
    "id": "4f03e69963b9649950ba29ae864a0de8c14f1f86",
    "semantic_title": "",
    "citation_count": 304,
    "authors": [
      "Ruize Wang",
      "Duyu Tang",
      "Nan Duan",
      "Zhongyu Wei",
      "Xuanjing Huang",
      "Jianshu Ji",
      "Guihong Cao",
      "Daxin Jiang",
      "Ming Zhou"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.122": {
    "title": "Global Attention Decoder for Chinese Spelling Error Correction",
    "volume": "findings",
    "abstract": "Recent progress has been made in using BERT framework for Chinese spelling error correction (CSC). However, most existing methods correct words based on local contextual information, without considering the influence of error words in sentences. Imposing attention on error contextual information could mislead and decrease the overall performance of CSC. To address this issue, we propose a Global Attention Decoder (GAD) approach for CSC. Specifically, the proposed method learns the global relationship of the potential correct input characters and the candidates of potential error characters. Rich global contextual information is obtained to alleviate the impact of the local error contextual information. In addition, a BERT with Confusion set guided Replacement Strategy (BERT CRS) is designed to narrow the gap between BERT and CSC. The candidates generated by BERT CRS covering the correct character with more than 99.9% probability. To demonstrate the effectiveness of our proposed framework, we test our method on three human-annotated datasets. The experimental results show that our approach outperforms all competitor models by a large margin of up to 6.2%, achieving state-of-the-art methods on all datasets",
    "checked": true,
    "id": "74512e17ba6e9617260bf9a65503ffa22bc66e3a",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Zhao Guo",
      "Yuan Ni",
      "Keqiang Wang",
      "Wei Zhu",
      "Guotong Xie"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.123": {
    "title": "Jointly Identifying Rhetoric and Implicit Emotions via Multi-Task Learning",
    "volume": "findings",
    "abstract": "Rhetorical implicit emotion identification is one of important and challenging tasks in natural language processing. We observe that each rhetoric may express certain evidence of semantic and syntactic patterns. Then, we design a gate mechanism based classification module to capture respective rhetorical representation and identify each rhetoric. Moreover, sentences carved with rhetoric tends to express emotions in subtle ways. We thus propose a new multi-task learning framework that can encode the categorical correlation between tasks to improve the performance of rhetoric and emotion identification problem. Experimental results validate the benefit of the proposed model over state-of-the-art baselines for rhetoric and emotion identification tasks. In addition, a new Chinese rhetorical implicit emotion dataset was constructed and will be released in this work",
    "checked": true,
    "id": "703f6adc2c80d3e9673ff90c64a41359ed99b4ab",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Xin Chen",
      "Zhen Hai",
      "Deyu Li",
      "Suge Wang",
      "Dian Wang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.124": {
    "title": "Exploring the Role of Context in Utterance-level Emotion, Act and Intent Classification in Conversations: An Empirical Study",
    "volume": "findings",
    "abstract": "the user inten-tion and background. In recent years, a number of context-aware approaches have been proposed for various utterance-level dialogue understanding tasks. In this paper, we explore and quantify the role of context for different aspects of a dialogue, namely emotion, dialogue act, and intent identiﬁcation, using state-of-the-art dialogue understanding methods as baselines. Speciﬁcally, we employ various perturbations to distort the context of a given utterance and study its impact on the different tasks and baselines. This provides us with insights into the fundamental context factors that have immediate implications on different aspects of a dialogue. Such insights may inspire more effective dialogue understanding models and provide support for future text generation approaches",
    "checked": true,
    "id": "84c018678e19d1508c5cd86f93ebdad62f0302a8",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Deepanway Ghosal",
      "Navonil Majumder",
      "Rada Mihalcea",
      "Soujanya Poria"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.125": {
    "title": "Encouraging Neural Machine Translation to Satisfy Terminology Constraints",
    "volume": "findings",
    "abstract": "We present a new approach to encourage neural machine translation to satisfy lexical constraints. Our method acts at the training step and thereby avoiding the introduction of any extra computational overhead at inference step. The proposed method combines three main ingredients. The first one consists in augmenting the training data to specify the constraints. Intuitively, this encourages the model to learn a copy behavior when it encounters constraint terms. Compared to previous work, we use a simplified augmentation strategy without source factors. The second ingredient is constraint token masking, which makes it even easier for the model to learn the copy behavior and generalize better. The third one, is a modification of the standard cross entropy loss to bias the model towards assigning high probabilities to constraint words. Empirical results show that our method improves upon related baselines in terms of both BLEU score and the percentage of generated constraint terms",
    "checked": true,
    "id": "648a3bd3647f062c0111799c5f262b4ac7db1391",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Melissa Ailem",
      "Jingshu Liu",
      "Raheel Qader"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.126": {
    "title": "BertGCN: Transductive Text Classification by Combining GNN and BERT",
    "volume": "findings",
    "abstract": "In this work, we propose BertGCN, a model that combines large scale pretraining and transductive learning for text classification. BertGCN constructs a heterogeneous graph over the dataset and represents documents as nodes using BERT representations. By jointly training the BERT and GCN modules within BertGCN, the proposed model is able to leverage the advantages of both worlds: large-scale pretraining which takes the advantage of the massive amount of raw data and transductive learning which jointly learns representations for both training data and unlabeled test data by propagating label influence through graph convolution. Experiments show that BertGCN achieves SOTA performances on a wide range of text classification datasets.1",
    "checked": true,
    "id": "d22b109eb5089179f8bd48ef47513533890f6bf9",
    "semantic_title": "",
    "citation_count": 89,
    "authors": [
      "Yuxiao Lin",
      "Yuxian Meng",
      "Xiaofei Sun",
      "Qinghong Han",
      "Kun Kuang",
      "Jiwei Li",
      "Fei Wu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.127": {
    "title": "Putting words into the system's mouth: A targeted attack on neural machine translation using monolingual data poisoning",
    "volume": "findings",
    "abstract": "Neural machine translation systems are known to be vulnerable to adversarial test inputs, however, as we show in this paper, these systems are also vulnerable to training attacks. Specifically, we propose a poisoning attack in which a malicious adversary inserts a small poisoned sample of monolingual text into the training set of a system trained using back-translation. This sample is designed to induce a specific, targeted translation behaviour, such as peddling misinformation. We present two methods for crafting poisoned examples, and show that only a tiny handful of instances, amounting to only 0.02% of the training set, is sufficient to enact a successful attack. We outline a defence method against said attacks, which partly ameliorates the problem. However, we stress that this is a blind-spot in modern NMT, demanding immediate attention",
    "checked": true,
    "id": "0d8f28b641adeb67d5f5bb6a50d50d530e088039",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Jun Wang",
      "Chang Xu",
      "Francisco Guzmán",
      "Ahmed El-Kishky",
      "Yuqing Tang",
      "Benjamin Rubinstein",
      "Trevor Cohn"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.128": {
    "title": "Semantic and Syntactic Enhanced Aspect Sentiment Triplet Extraction",
    "volume": "findings",
    "abstract": "Aspect Sentiment Triplet Extraction (ASTE) aims to extract triplets from sentences, where each triplet includes an entity, its associated sentiment, and the opinion span explaining the reason for the sentiment. Most existing research addresses this problem in a multi-stage pipeline manner, which neglects the mutual information between such three elements and has the problem of error propagation. In this paper, we propose a Semantic and Syntactic Enhanced aspect Sentiment triplet Extraction model (SE) to fully exploit the syntactic and semantic relationships between the triplet elements and jointly extract them. Specifically, we design a Graph-Sequence duel representation and modeling paradigm for the task of ASTE: we represent the semantic and syntactic relationships between word pairs in a sentence by graph and encode it by Graph Neural Networks (GNNs), as well as modeling the original sentence by LSTM to preserve the sequential information. Under this setting, we further apply a more efficient inference strategy for the extraction of triplets. Extensive evaluations on four benchmark datasets show that SE significantly outperforms existing approaches, which proves our SE's superiority and flexibility in an end-to-end fashion",
    "checked": true,
    "id": "0363e12da739e50bac2dc369be935c9b45d97512",
    "semantic_title": "",
    "citation_count": 24,
    "authors": [
      "Zhexue Chen",
      "Hong Huang",
      "Bang Liu",
      "Xuanhua Shi",
      "Hai Jin"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.129": {
    "title": "UserAdapter: Few-Shot User Learning in Sentiment Analysis",
    "volume": "findings",
    "abstract": "Adapting a model to a handful of personalized data is challenging, especially when it has gigantic parameters, such as a Transformerbased pretrained model. The standard way of fine-tuning all the parameters necessitates storing a huge model for each user. In this work, we introduce a lightweight approach dubbed UserAdapter, which clamps hundred millions of parameters of the Transformer model and optimizes a tiny user-specific vector. We take sentiment analysis as a test bed, and collect datasets of reviews from Yelp and IMDB respectively. Results show that, on both datasets, UserAdapter achieves better accuracy than the standard fine-tuned Transformerbased pre-trained model. More importantly, UserAdapter offers an efficient way to produce a personalized Transformer model with less than 0.5% parameters added for each user",
    "checked": true,
    "id": "331102c42a340e7bcd9a4b063b9b1204f30f665f",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Wanjun Zhong",
      "Duyu Tang",
      "Jiahai Wang",
      "Jian Yin",
      "Nan Duan"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.130": {
    "title": "PsyQA: A Chinese Dataset for Generating Long Counseling Text for Mental Health Support",
    "volume": "findings",
    "abstract": "Great research interests have been attracted to devise AI services that are able to provide mental health support. However, the lack of corpora is a main obstacle to this research, particularly in Chinese language. In this paper, we propose PsyQA, a Chinese dataset of psychological health support in the form of question and answer pair. PsyQA is crawled from a Chinese mental health service platform, and contains 22K questions and 56K long and wellstructured answers. Based on the psychological counseling theories, we annotate a portion of answer texts with typical strategies for providing support, and further present in-depth analysis of both lexical features and strategy patterns in the counseling answers. We also evaluate the performance of generating counseling answers with the generative pretrained models. Results show that utilizing strategies enhances the fluency and helpfulness of generated answers, but there is still a large space for future research",
    "checked": true,
    "id": "db20a10ef5641a0d0e60584e4cc8430a9763d437",
    "semantic_title": "",
    "citation_count": 17,
    "authors": [
      "Hao Sun",
      "Zhenru Lin",
      "Chujie Zheng",
      "Siyang Liu",
      "Minlie Huang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.131": {
    "title": "RiddleSense: Reasoning about Riddle Questions Featuring Linguistic Creativity and Commonsense Knowledge",
    "volume": "findings",
    "abstract": "Question: I have five fingers but I am not alive. What am I? Answer: a glove. Answering such a riddle-style question is a challenging cognitive process, in that it requires complex commonsense reasoning abilities, an understanding of figurative language, and counterfactual reasoning skills, which are all important abilities for advanced natural language understanding (NLU). However, there is currently no dataset aiming to test these abilities. In this paper, we present RIDDLESENSE1, a new multiple-choice question answering task, which comes with the first large dataset (5.7k examples) for answering riddlestyle commonsense questions. We systematically evaluate a wide range of models over the RIDDLESENSE challenge, and point out that there is a large gap between the bestsupervised model and human performance — suggesting intriguing future research in the direction of higher-order commonsense reasoning and linguistic creativity towards building advanced NLU systems",
    "checked": true,
    "id": "71fab1ce3c66998ba681ab378484be77690327a9",
    "semantic_title": "",
    "citation_count": 17,
    "authors": [
      "Bill Yuchen Lin",
      "Ziyi Wu",
      "Yichi Yang",
      "Dong-Ho Lee",
      "Xiang Ren"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.132": {
    "title": "Learning to Generate Questions by Learning to Recover Answer-containing Sentences",
    "volume": "findings",
    "abstract": "To train a question answering model based on machine reading comprehension (MRC), significant effort is required to prepare annotated training data composed of questions and their answers from contexts. Recent research has focused on synthetically generating a question from a given context and an annotated (or generated) answer by training an additional generative model to augment the training data. In light of this research direction, we propose a novel pre-training approach that learns to generate contextually rich questions, by recovering answer-containing sentences. We evaluate our method against existing ones in terms of the quality of generated questions, and fine-tuned MRC model accuracy after training on the data synthetically generated by our method. We consistently improve the question generation capability of existing models such as T5 and UniLM, and achieve state-ofthe-art results on MS MARCO and NewsQA, and comparable results to the state-of-the-art on SQuAD. Additionally, the data synthetically generated by our approach is beneficial for boosting up the downstream MRC accuracy across a wide range of datasets, such as SQuAD-v1.1, v2.0, KorQuAD and BioASQ, without any modification to the existing MRC models. Furthermore, our method shines especially when a limited amount of pre-training or downstream MRC data is given",
    "checked": true,
    "id": "622b17134bca8302305d5607cdeb6769b08b9fc1",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Seohyun Back",
      "Akhil Kedia",
      "Sai Chetan Chinthakindi",
      "Haejun Lee",
      "Jaegul Choo"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.133": {
    "title": "Learning Slice-Aware Representations with Mixture of Attentions",
    "volume": "findings",
    "abstract": "Real-world machine learning systems are achieving remarkable performance in terms of coarse-grained metrics like overall accuracy and F-1 score. However, model improvement and development often require fine-grained modeling on individual data subsets or slices, for instance, the data slices where the models have unsatisfactory results. In practice, it gives tangible values for developing such models that can pay extra attention to critical or interested slices while retaining the original overall performance. This work extends the recent slice-based learning (SBL) (Chen et al., 2019) with a mixture of attentions (MoA) to learn slice-aware dual attentive representations. We empirically show that the MoA approach outperforms the baseline method as well as the original SBL approach on monitored slices with two natural language understanding (NLU) tasks",
    "checked": true,
    "id": "48502b3988213b0d296c4b9d37577df8807b250d",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Cheng Wang",
      "Sungjin Lee",
      "Sunghyun Park",
      "Han Li",
      "Young-Bum Kim",
      "Ruhi Sarikaya"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.134": {
    "title": "Making Better Use of Bilingual Information for Cross-Lingual AMR Parsing",
    "volume": "findings",
    "abstract": "Abstract Meaning Representation (AMR) is a rooted, labeled, acyclic graph representing the semantics of natural language. As previous works show, although AMR is designed for English at first, it can also represent semantics in other languages. However, they find that concepts in their predicted AMR graphs are less specific. We argue that the misprediction of concepts is due to the high relevance between English tokens and AMR concepts. In this work, we introduce bilingual input, namely the translated texts as well as non-English texts, in order to enable the model to predict more accurate concepts. Besides, we also introduce an auxiliary task, requiring the decoder to predict the English sequences at the same time. The auxiliary task can help the decoder understand what exactly the corresponding English tokens are. Our proposed cross-lingual AMR parser surpasses previous state-of-the-art parser by 10.6 points on Smatch F1 score. The ablation study also demonstrates the efficacy of our proposed modules.Meaning Representation (AMR) is a rooted, labeled, acyclic graph representing the semantics of natural language. As previous works show, although AMR is designed for English at first, it can also represent semantics in other languages. However, they find that concepts in their predicted AMR graphs are less specific. We argue that the misprediction of concepts is due to the high relevance between English tokens and AMR concepts. In this work, we introduce bilingual input, namely the translated texts as well as non-English texts, in order to enable the model to predict more accurate concepts. Besides, we also introduce an auxiliary task, requiring the decoder to predict the English sequences at the same time. The auxiliary task can help the decoder understand what exactly the corresponding English tokens are. Our proposed cross-lingual AMR parser surpasses previous state-of-the-art parser by 10.6 points on Smatch F1 score. The ablation study also demonstrates the efficacy of our proposed modules",
    "checked": true,
    "id": "d986d3436ccdef0c1e450036dfc0ce47d6948f58",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Yitao Cai",
      "Zhe Lin",
      "Xiaojun Wan"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.135": {
    "title": "Pushing Paraphrase Away from Original Sentence: A Multi-Round Paraphrase Generation Approach",
    "volume": "findings",
    "abstract": "In recent years, neural paraphrase generation based on Seq2Seq has achieved superior performance, however, the generated paraphrase still has the problem of lack of diversity. In this paper, we focus on improving the diversity between the generated paraphrase and the original sentence, i.e., making generated paraphrase different from the original sentence as much as possible. We propose BTmPG (BackTranslation guided multi-round Paraphrase Generation), which leverages multi-round paraphrase generation to improve diversity and employs back-translation to preserve semantic information. We evaluate BTmPG on two benchmark datasets. Both automatic and human evaluation show BTmPG can improve the diversity of paraphrase while preserving the semantics of the original sentence",
    "checked": true,
    "id": "6133fa7026dbc659727b1554bc4ad167a3b1b315",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Zhe Lin",
      "Xiaojun Wan"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.136": {
    "title": "Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models",
    "volume": "findings",
    "abstract": "with Pretrained Language Models Junyi Li1,3, Tianyi Tang1, Wayne Xin Zhao1,3,5, Zhicheng Wei4, Nicholas Jing Yuan4 and Ji-Rong Wen1,2,3 1Gaoling School of Artificial Intelligence, Renmin University of China 2School of Information, Renmin University of China 3Beijing Key Laboratory of Big Data Management and Analysis Methods 4Huawei Cloud 5Beijing Academy of Artificial Intelligence, Beijing, 100084, China {lijunyi,jrwen,steven_tang}@ruc.edu.cn {batmanfly,nicholas.jing.yuan}@gmail.com weizhicheng1@huawei.com Abstract",
    "checked": true,
    "id": "5e1621967c6a85bfa2dc0277a09bd0d2d9789e47",
    "semantic_title": "",
    "citation_count": 19,
    "authors": [
      "Junyi Li",
      "Tianyi Tang",
      "Wayne Xin Zhao",
      "Zhicheng Wei",
      "Nicholas Jing Yuan",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.137": {
    "title": "Better Robustness by More Coverage: Adversarial and Mixup Data Augmentation for Robust Finetuning",
    "volume": "findings",
    "abstract": "Pretrained language models (PLMs) perform poorly under adversarial attacks. To improve the adversarial robustness, adversarial data augmentation (ADA) has been widely adopted to cover more search space of adversarial attacks by adding textual adversarial examples during training. However, the number of adversarial examples for text augmentation is still extremely insufficient due to the exponentially large attack search space. In this work, we propose a simple and effective method to cover a much larger proportion of the attack search space, called Adversarial and Mixup Data Augmentation (AMDA). Specifically, AMDA linearly interpolates the representations of pairs of training samples to form new virtual samples, which are more abundant and diverse than the discrete text adversarial examples in conventional ADA. Moreover, to fairly evaluate the robustness of different models, we adopt a challenging evaluation setup, which generates a new set of adversarial examples targeting each model. In text classification experiments of BERT and RoBERTa, AMDA achieves significant robustness gains under two strong adversarial attacks and alleviates the performance degradation of ADA on the clean data. Our code is available at: https://github.com/thunlp/MixADA",
    "checked": true,
    "id": "398e715ba5e0753e0146133daff181b5b440f970",
    "semantic_title": "",
    "citation_count": 27,
    "authors": [
      "Chenglei Si",
      "Zhengyan Zhang",
      "Fanchao Qi",
      "Zhiyuan Liu",
      "Yasheng Wang",
      "Qun Liu",
      "Maosong Sun"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.138": {
    "title": "NAST: A Non-Autoregressive Generator with Word Alignment for Unsupervised Text Style Transfer",
    "volume": "findings",
    "abstract": "Autoregressive models have been widely used in unsupervised text style transfer. Despite their success, these models still suffer from the content preservation problem that they usually ignore part of the source sentence and generate some irrelevant words with strong styles. In this paper, we propose a NonAutoregressive generator for unsupervised text Style Transfer (NAST), which alleviates the problem from two aspects. First, we observe that most words in the transferred sentence can be aligned with related words in the source sentence, so we explicitly model word alignments to suppress irrelevant words. Second, existing models trained with the cycle loss align sentences in two stylistic text spaces, which lacks fine-grained control at the word level. The proposed non-autoregressive generator focuses on the connections between aligned words, which learns the word-level transfer between styles. For experiments, we integrate the proposed generator into two base models and evaluate them on two style transfer tasks. The results show that NAST can significantly improve the overall performance and provide explainable word alignments. Moreover, the nonautoregressive generator achieves over 10x speedups at inference. Our codes are available at https://github.com/thu-coai/NAST",
    "checked": true,
    "id": "f747527e455c118cbadd169479323f38032c0475",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Fei Huang",
      "Zikai Chen",
      "Chen Henry Wu",
      "Qihan Guo",
      "Xiaoyan Zhu",
      "Minlie Huang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.139": {
    "title": "HyKnow: End-to-End Task-Oriented Dialog Modeling with Hybrid Knowledge Management",
    "volume": "findings",
    "abstract": "Task-oriented dialog (TOD) systems typically manage structured knowledge (e.g. ontologies and databases) to guide the goal-oriented conversations. However, they fall short of handling dialog turns grounded on unstructured knowledge (e.g. reviews and documents). In this paper, we formulate a task of modeling TOD grounded on both structured and unstructured knowledge. To address this task, we propose a TOD system with hybrid knowledge management, HyKnow. It extends the belief state to manage both structured and unstructured knowledge, and is the first end-to-end model that jointly optimizes dialog modeling grounded on these two kinds of knowledge. We conduct experiments on the modified version of MultiWOZ 2.1 dataset, where dialogs are grounded on hybrid knowledge. Experimental results show that HyKnow has strong end-to-end performance compared to existing TOD systems. It also outperforms the pipeline knowledge management schemes, with higher unstructured knowledge retrieval accuracy",
    "checked": true,
    "id": "8d3076c38f56df22052567f4783c670d8e860f09",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Silin Gao",
      "Ryuichi Takanobu",
      "Wei Peng",
      "Qun Liu",
      "Minlie Huang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.140": {
    "title": "Target-oriented Fine-tuning for Zero-Resource Named Entity Recognition",
    "volume": "findings",
    "abstract": "Zero-resource named entity recognition (NER) severely suffers from data scarcity in a specific domain or language. Most studies on zero-resource NER transfer knowledge from various data by fine-tuning on different auxiliary tasks. However, how to properly select training data and fine-tuning tasks is still an open problem. In this paper, we tackle the problem by transferring knowledge from three aspects, i.e., domain, language and task, and strengthening connections among them. Specifically, we propose four practical guidelines to guide knowledge transfer and task finetuning. Based on these guidelines, we design a target-oriented fine-tuning (TOF) framework to exploit various data from three aspects in a unified training manner. Experimental results on six benchmarks show that our method yields consistent improvements over baselines in both cross-domain and cross-lingual scenarios. Particularly, we achieve new state-of-theart performance on five benchmarks",
    "checked": true,
    "id": "b5ea9f36777b719c24ccee3b67dca361e9c93daa",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Ying Zhang",
      "Fandong Meng",
      "Yufeng Chen",
      "Jinan Xu",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.141": {
    "title": "BERT-Defense: A Probabilistic Model Based on BERT to Combat Cognitively Inspired Orthographic Adversarial Attacks",
    "volume": "findings",
    "abstract": "Adversarial attacks expose important blind spots of deep learning systems. While wordand sentence-level attack scenarios mostly deal with finding semantic paraphrases of the input that fool NLP models, character-level attacks typically insert typos into the input stream. It is commonly thought that these are easier to defend via spelling correction modules. In this work, we show that both a standard spellchecker and the approach of Pruthi et al. (2019), which trains to defend against insertions, deletions and swaps, perform poorly on the character-level benchmark recently proposed in Eger and Benz (2020) which includes more challenging attacks such as visual and phonetic perturbations and missing word segmentations. In contrast, we show that an untrained iterative approach which combines context-independent character-level information with context-dependent information from BERT's masked language modeling can perform on par with human crowd-workers from Amazon Mechanical Turk (AMT) supervised via 3-shot learning",
    "checked": true,
    "id": "a76c98c6814ce8de07707b81c18520af508b7184",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Yannik Keller",
      "Jan Mackensen",
      "Steffen Eger"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.142": {
    "title": "Event Detection as Graph Parsing",
    "volume": "findings",
    "abstract": "Event detection is a fundamental task in information extraction. Most previous approaches typically view event detection as a triggerbased classification problem, focusing on using syntactic dependency structure or external knowledge to boost the classification performance. To overcome the inherent issues with existing trigger classification based models, we propose a novel approach to event detection by formulating it as a graph parsing problem, which can explicitly model the multiple event correlations and naturally utilize the rich information conveyed by event type and subtype. Furthermore, to cope with data sparsity, we employ a pretrained sequence-tosequence (seq2seq) model to transduce an input sentence into an accurate event graph without the need for trigger words. Extensive experimental results on the public ACE2005 dataset show that, our approach outperforms all previous state-of-the-art models for event detection by a large margin, obtaining an improvement of 4.2% F1 score. The result is very encouraging since we achieve this with a conceptually simple seq2seq model; moreover, by extending the graph structure, this proposed architecture can be flexibly applied to more information extraction problems for sentences",
    "checked": true,
    "id": "f9c21c55e5a9e805be83b054a96d54303ddd2a16",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Jianye Xie",
      "Haotong Sun",
      "Junsheng Zhou",
      "Weiguang Qu",
      "Xinyu Dai"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.143": {
    "title": "Toward Fully Exploiting Heterogeneous Corpus:A Decoupled Named Entity Recognition Model with Two-stage Training",
    "volume": "findings",
    "abstract": "Named Entity Recognition (NER) is a fundamental and widely used task in natural language processing (NLP), which is generally trained on the human-annotated corpus. However, data annotation is costly and timeconsuming, which restricts its scale and further leads to the performance bottleneck of NER models. In reality, we can conveniently collect large-scale entity dictionaries and distantly supervised data. However, the collected dictionaries are lack of semantic context and the distantly supervised training instances contain large noise, which will bring uncertain effects to NER models when directly incorporated into the high-quality training set. To address the above issue, we propose a BERT-based decoupled NER model with two-stage training to appropriately take advantage of the heterogeneous corpus, including dictionaries, distantly supervised instances, and human-annotated instances. Our decoupled model consists of a Mention-BERT and a Context-BERT to respectively learn from the context-deficient dictionaries and noised distantly supervised instances at the pre-training stage. At the unifiedtraining stage, the two BERTs are trained together on human-annotated data to predict the correct labels for candidate regions. Empirical studies on three Chinese NER datasets demonstrate that our method achieves significant improvements against several baselines, establishing the new state-of-the-art performance",
    "checked": true,
    "id": "9b0c9749b8ef1d9e233a68a4270fb718a3d15d9a",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Yun Hu",
      "Yeshuang Zhu",
      "Jinchao Zhang",
      "Changwen Zheng",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.144": {
    "title": "Discriminative Reasoning for Document-level Relation Extraction",
    "volume": "findings",
    "abstract": "Document-level relation extraction (DocRE) models generally use graph networks to implicitly model the reasoning skill (i.e., pattern recognition, logical reasoning, coreference reasoning, etc.) related to the relation between one entity pair in a document. In this paper, we propose a novel discriminative reasoning framework to explicitly model the paths of these reasoning skills between each entity pair in this document. Thus, a discriminative reasoning network is designed to estimate the relation probability distribution of different reasoning paths based on the constructed graph and vectorized document contexts for each entity pair, thereby recognizing their relation. Experimental results show that our method outperforms the previous state-of-theart performance on the large-scale DocRE dataset. The code is publicly available at https://github.com/xwjim/DRN",
    "checked": true,
    "id": "b91157bd86212b565fa74229c9035db95560ae1a",
    "semantic_title": "",
    "citation_count": 22,
    "authors": [
      "Wang Xu",
      "Kehai Chen",
      "Tiejun Zhao"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.145": {
    "title": "Meta-Learning Adversarial Domain Adaptation Network for Few-Shot Text Classification",
    "volume": "findings",
    "abstract": "Meta-learning has emerged as a trending technique to tackle few-shot text classification and achieved state-of-the-art performance. However, existing solutions heavily rely on the exploitation of lexical features and their distributional signatures on training data, while neglecting to strengthen the model's ability to adapt to new tasks. In this paper, we propose a novel meta-learning framework integrated with an adversarial domain adaptation network, aiming to improve the adaptive ability of the model and generate high-quality text embedding for new classes. Extensive experiments are conducted on four benchmark datasets and our method demonstrates clear superiority over the state-of-the-art models in all the datasets. In particular, the accuracy of 1shot and 5-shot classification on the dataset of 20 Newsgroups is boosted from 52.1% to 59.6%, and from 68.3% to 77.8%, respectively1",
    "checked": true,
    "id": "34b709a6763cf1ed75216b5bc5f65ab0f0632ee5",
    "semantic_title": "",
    "citation_count": 18,
    "authors": [
      "Chengcheng Han",
      "Zeqiu Fan",
      "Dongxiang Zhang",
      "Minghui Qiu",
      "Ming Gao",
      "Aoying Zhou"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.146": {
    "title": "Documents Representation via Generalized Coupled Tensor Chain with the Rotation Group constraint",
    "volume": "findings",
    "abstract": "Continuous representations of linguistic structures are an important part of modern natural language processing systems. Despite the diversity, most of the existing log-multilinear embedding models are organized under vector operations. However, these operations can not precisely represent the compositionality of natural language due to a lack of order-preserving properties. In this work, we focus on one of the promising alternatives based on the embedding of documents and words in the rotation group through the generalization of the coupled tensor chain decomposition to the exponential family of the probability distributions. In this model, documents and words are represented as matrices, and n-grams representations are combined from word representations by matrix multiplication. The proposed model is optimized via noise-contrastive estimation. We show empirically that capturing word order and higher-order word interactions allows our model to achieve the best results in several document classification benchmarks",
    "checked": true,
    "id": "7bd5e6b44bd9aac23499944ac00e63f7d80b996a",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Igor Vorona",
      "Anh-Huy Phan",
      "Alexander Panchenko",
      "Andrzej Cichocki"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.147": {
    "title": "Improving Unsupervised Extractive Summarization with Facet-Aware Modeling",
    "volume": "findings",
    "abstract": "Unsupervised extractive summarization aims to extract salient sentences from documents without labeled corpus. Existing methods are mostly graph-based by computing sentence centrality. These methods usually tend to select sentences within the same facet, however, which often leads to the facet bias problem especially when the document has multiple facets (i.e. long-document and multidocuments). To address this problem, we proposed a novel facet-aware centrality-based ranking model. We let the model pay more attention to different facets by introducing a sentence-document weight. The weight is added to the sentence centrality score. We evaluate our method on a wide range of summarization tasks that include 8 representative benchmark datasets. Experimental results show that our method consistently outperforms strong baselines especially in longand multi-document scenarios and even performs comparably to some supervised models. Extensive analyses confirm that the performance gains come from alleviating the facet bias problem",
    "checked": true,
    "id": "21dabd3a989d578a32fb024732a2107e8da28581",
    "semantic_title": "",
    "citation_count": 22,
    "authors": [
      "Xinnian Liang",
      "Shuangzhi Wu",
      "Mu Li",
      "Zhoujun Li"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.148": {
    "title": "Improving Gradient-based Adversarial Training for Text Classification by Contrastive Learning and Auto-Encoder",
    "volume": "findings",
    "abstract": "Recent work has proposed several efficient approaches for generating gradient-based adversarial perturbations on embeddings and proved that the model's performance and robustness can be improved when they are trained with these contaminated embeddings. While they paid little attention to how to help the model to learn these adversarial samples more efficiently. In this work, we focus on enhancing the model's ability to defend gradient-based adversarial attack during the model's training process and propose two novel adversarial training approaches: (1) CARL narrows the original sample and its adversarial sample in the representation space while enlarging their distance from different labeled samples. (2) RAR forces the model to reconstruct the original sample from its adversarial representation. Experiments show that the proposed two approaches outperform strong baselines on various text classification datasets. Analysis experiments find that when using our approaches, the semantic representation of the input sentence won't be significantly affected by adversarial perturbations, and the model's performance drops less under adversarial attack. That is to say, our approaches can effectively improve the robustness of the model. Besides, RAR can also be used to generate text-form adversarial samples",
    "checked": true,
    "id": "9715e184e28f205fe15f2718ea873e674e66b23c",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Yao Qiu",
      "Jinchao Zhang",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.149": {
    "title": "Multi-Granularity Contrasting for Cross-Lingual Pre-Training",
    "volume": "findings",
    "abstract": "Cross-lingual pre-training aims at providing effective prior representations for the inputs from multiple languages. With the modeling of bidirectional contexts, recently prevalent language modeling approaches such as XLM achieve better performance than traditional methods based on embedding alignment, which strives to assign similar vector representations to semantic-equivalent units. However, such approaches like XLM capture cross-lingual information based solely on shared BPE vocabulary, resulting in the absence of fine-grained supervision induced by embedding alignment. Inheriting the advantages of the above two paradigms, this work presents a multi-granularity contrasting framework, namely MGC, to learn languageuniversal representations. While predicting the masked words based on bidirectional contexts, the proposal also encodes semantic equivalents from different languages into similar representations to introduce more finegrained and explicit cross-lingual information. Two effective contrasting strategies are further proposed, which can be built upon semantic units of multiple granularities covering words, span, and sentences. Extensive experiments demonstrate that our approach can achieve significant performance gains in various downstream tasks, including machine translation and cross-lingual language understanding",
    "checked": true,
    "id": "52e645112a845fc8286879a2ec00ee433ebc5900",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Shicheng Li",
      "Pengcheng Yang",
      "Fuli Luo",
      "Jun Xie"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.150": {
    "title": "A Comparison between Pre-training and Large-scale Back-translation for Neural Machine Translation",
    "volume": "findings",
    "abstract": "BERT has been studied as a promising technique to improve NMT. Given that BERT is based on the similar Transformer architecture to NMT and the current datasets for most MT tasks are rather large, how pre-training has managed to outperform standard Transformer NMT models is underestimated. We compare MT engines trained with pre-trained BERT and back-translation with incrementally larger amounts of data, implementing the two most widely-used monolingual paradigms. We analyze their strengths and weaknesses based on both standard automatic metrics and intrinsic test suites that comprise a large range of linguistic phenomena. Primarily, we find that 1) BERT has limited advantages compared with large-scale back-translation in accuracy and consistency on morphology and syntax; 2) BERT can boost the Transformer baseline in semantic and pragmatic tasks which involve intensive understanding; 3) pre-training on huge datasets may introduce inductive social bias thus affects translation fairness",
    "checked": true,
    "id": "86445cf8bb0712d21a4f07b752435702973f514b",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Dandan Huang",
      "Kun Wang",
      "Yue Zhang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.151": {
    "title": "Bi-Granularity Contrastive Learning for Post-Training in Few-Shot Scene",
    "volume": "findings",
    "abstract": "The major paradigm of applying a pre-trained language model to downstream tasks is to finetune it on labeled task data, which often suffers instability and low performance when the labeled examples are scarce. One way to alleviate this problem is to apply post-training on unlabeled task data before fine-tuning, adapting the pre-trained model to target domains by contrastive learning that considers either tokenlevel or sequence-level similarity. Inspired by the success of sequence masking, we argue that both token-level and sequence-level similarities can be captured with a pair of masked sequences. Therefore, we propose complementary random masking (CRM) to generate a pair of masked sequences from an input sequence for sequence-level contrastive learning and then develop contrastive masked language modeling (CMLM) for post-training to integrate both token-level and sequence-level contrastive learnings. Empirical results show that CMLM surpasses several recent post-training methods in few-shot settings without the need for data augmentation",
    "checked": true,
    "id": "1c49e8e25f52449ae6a986cad9d6a1f8fc7d2e71",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Ruikun Luo",
      "Guanhuan Huang",
      "Xiaojun Quan"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.152": {
    "title": "Fusing Label Embedding into BERT: An Efficient Improvement for Text Classification",
    "volume": "findings",
    "abstract": "With pre-trained models, such as BERT, gaining more and more attention, plenty of research has been done to further promote their capabilities, from enhancing the experimental procedures (Sun et al., 2019) to improving the mathematical principles. In this paper, we propose a concise method for improving BERT's performance in text classification by utilizing a label embedding technique while keeping almost the same computational cost. Experimental results on six text classification benchmark datasets demonstrate its effectiveness",
    "checked": true,
    "id": "ba0a55c06298eed1bc8dc2d855f064c834efc796",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Yijin Xiong",
      "Yukun Feng",
      "Hao Wu",
      "Hidetaka Kamigaito",
      "Manabu Okumura"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.153": {
    "title": "KACC: A Multi-task Benchmark for Knowledge Abstraction, Concretization and Completion",
    "volume": "findings",
    "abstract": "Knowledge graphs (KGs) contains an instance-level entity graph and an ontology-level concept graph. Recent studies reveal that jointly modeling of these two graphs could improve the understanding of each one. The completion processes on the concept graph and the entity graph can be further regarded as processes of knowledge abstraction and concretization. However, concept graphs in existing datasets are usually small and the links between concepts and entities are usually sparse, which cannot provide sufficient information for knowledge transfer between the two graphs. In this paper, we propose large-scale datasets extracted from Wikidata, which provide more size-balanced concept graphs and abundant cross-view links. Based on the datasets, we further propose a benchmark to test the ability of existing models on knowledge abstraction, concretization and completion (KACC). Our dataset is available at this https URL",
    "checked": true,
    "id": "28d794bc8b8cf579745146b3fd3362942787dd10",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Jie Zhou",
      "Shengding Hu",
      "Xin Lv",
      "Cheng Yang",
      "Zhiyuan Liu",
      "Wei Xu",
      "Jie Jiang",
      "Juanzi Li",
      "Maosong Sun"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.154": {
    "title": "A Query-Driven Topic Model",
    "volume": "findings",
    "abstract": "Topic modeling is an unsupervised method for revealing the hidden semantic structure of a corpus. It has been increasingly widely adopted as a tool in the social sciences, including political science, digital humanities and sociological research in general. One desirable property of topic models is to allow users to find topics describing a specific aspect of the corpus. A possible solution is to incorporate domain-specific knowledge into topic modeling, but this requires a specification from domain experts. We propose a novel query-driven topic model that allows users to specify a simple query in words or phrases and return query-related topics, thus avoiding tedious work from domain experts. Our proposed approach is particularly attractive when the user-specified query has a low occurrence in a text corpus, making it difficult for traditional topic models built on word cooccurrence patterns to identify relevant topics. Experimental results demonstrate the effectiveness of our model in comparison with both classical topic models and neural topic models",
    "checked": true,
    "id": "722058a002c8f2ee8b496073888095a37db5c83b",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Zheng Fang",
      "Yulan He",
      "Rob Procter"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.155": {
    "title": "How Reliable are Model Diagnostics?",
    "volume": "findings",
    "abstract": "In the pursuit of a deeper understanding of a model's behaviour, there is recent impetus for developing suites of probes aimed at diagnosing models beyond simple metrics like accuracy or BLEU. This paper takes a step back and asks an important and timely question: how reliable are these diagnostics in providing insight into models and training setups? We critically examine three recent diagnostic tests for pre-trained language models, and find that likelihood-based and representation-based model diagnostics are not yet as reliable as previously assumed. Based on our empirical findings, we also formulate recommendations for practitioners and researchers",
    "checked": true,
    "id": "7c799b7bd8c069c6feb7235345c97aa1f5330b84",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Vamsi Aribandi",
      "Yi Tay",
      "Donald Metzler"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.156": {
    "title": "Gaussian Process based Deep Dyna-Q approach for Dialogue Policy Learning",
    "volume": "findings",
    "abstract": "August 1–6, 2021. ©2021 Association for Computational Linguistics 1786 Gaussian Process based Deep Dyna-Q Approach for Dialogue Policy Learning Guanlin Wu 1,2,∗ Wenqi Fang 3,∗,† Ji Wang 1,∗ Jiang Cao 2 Weidong Bao 1 Yang Ping 2 Xiaomin Zhu 1 Zheng Wang 4 National University of Defense Technology, Changsha, China Academy of Military Science, Beijing, China Nanhu Laboratory, Jiaxing, China Shenzhen Institutes of Advanced Technology, CAS, Shenzhen, China {wuguanlin16,wangji,wdbao,xmzhu}@nudt.edu.cn ocean.py@163.com amscaojiang@126.com wqfang@nanhulab.ac.cn zheng.wang@siat.ac.cn Abstract",
    "checked": true,
    "id": "f896b12d743b471b83c5f2a012e1dbe4932320c7",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Guanlin Wu",
      "Wenqi Fang",
      "Ji Wang",
      "Jiang Cao",
      "Weidong Bao",
      "Yang Ping",
      "Xiaomin Zhu",
      "Zheng Wang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.157": {
    "title": "CiteWorth: Cite-Worthiness Detection for Improved Scientific Document Understanding",
    "volume": "findings",
    "abstract": "Scientiﬁc document understanding is challenging as the data is highly domain speciﬁc and diverse. However, datasets for tasks with scientiﬁc text require expensive manual annotation and tend to be small and limited to only one or a few ﬁelds. At the same time, scientiﬁc documents contain many potential training signals, such as citations, which can be used to build large labelled datasets. Given this, we present an in-depth study of cite-worthiness detection in English, where a sentence is labelled for whether or not it cites an external source. To accomplish this, we introduce C ITE W ORTH , a large, contextualized, rigorously cleaned labelled dataset for cite-worthiness detection built from a massive corpus of extracted plain-text scientiﬁc documents. We show that C ITE W ORTH is high-quality, challenging, and suitable for studying problems such as domain adaptation. Our best performing cite-worthiness detection model is a paragraph-level contextualized sentence labelling model based on Longformer, exhibiting a 5 F1 point improvement over SciBERT which considers only individual sentences. Finally, we demonstrate that language model ﬁne-tuning with cite-worthiness as a secondary task leads to improved performance on downstream scientiﬁc document understanding tasks",
    "checked": true,
    "id": "91ad9c6345723717b9cf77727482195834620060",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Dustin Wright",
      "Isabelle Augenstein"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.158": {
    "title": "Cross-Lingual Cross-Domain Nested Named Entity Evaluation on English Web Texts",
    "volume": "findings",
    "abstract": "Named Entity Recognition (NER) is a key Natural Language Processing task. However, most existing work on NER targets flat named entities (NEs) and ignores the recognition of nested structures, where entities can be enclosed within other NEs. Moreover, evaluation of Nested Named Entity Recognition (NNER) across domains remains challenging, mainly due to the limited availability of datasets. To address these gaps, we present EWT-NNER, a dataset covering five web domains annotated for nested named entities on top of the English Web Treebank (EWT). We present the corpus and an empirical evaluation, including transfer results from German and Danish. EWTNNER is annotated for four major entity types, including suffixes for derivational entity markers and partial named entities, spanning a total of 12 classes. We envision the public release of EWT-NNER to encourage further research on nested NER, particularly on cross-lingual cross-domain evaluation",
    "checked": true,
    "id": "3abdc83e17d1c93f04a6c4a60d0f8f0706269351",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Barbara Plank"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.159": {
    "title": "Counter-Argument Generation by Attacking Weak Premises",
    "volume": "findings",
    "abstract": "Text generation has received a lot of attention in computational argumentation research as of recently. A particularly challenging task is the generation of counter-arguments. So far, approaches primarily focus on rebutting a given conclusion, yet other ways to counter an argument exist. In this work, we go beyond previous research by exploring argument undermining, that is, countering an argument by attacking one of its premises. We hypothesize that identifying the argument's weak premises is key to effective countering. Accordingly, we propose a pipeline approach that first assesses the premises' strength and then generates a counter-argument undermining the weakest among them. On one hand, both manual and automatic evaluation underline the importance of identifying weak premises in counterargument generation. On the other hand, when considering correctness and content richness, human annotators favored our approach over state-of-the-art counter-argument baselines",
    "checked": true,
    "id": "fa3d808a6d65acddc27501be29cbbe120d7e693d",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Milad Alshomary",
      "Shahbaz Syed",
      "Arkajit Dhar",
      "Martin Potthast",
      "Henning Wachsmuth"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.160": {
    "title": "Alternated Training with Synthetic and Authentic Data for Neural Machine Translation",
    "volume": "findings",
    "abstract": "While synthetic bilingual corpora have demonstrated their effectiveness in low-resource neural machine translation (NMT), adding more synthetic data often deteriorates translation performance. In this work, we propose alternated training with synthetic and authentic data for NMT. The basic idea is to alternate synthetic and authentic corpora iteratively during training. Compared with previous work, we introduce authentic data as guidance to prevent the training of NMT models from being disturbed by noisy synthetic data. Experiments on Chinese-English and GermanEnglish translation tasks show that our approach improves the performance over several strong baselines. We visualize the BLEU landscape to further investigate the role of authentic and synthetic data during alternated training. From the visualization, we find that authentic data helps to direct the NMT model parameters towards points with higher BLEU scores and leads to consistent translation performance improvement",
    "checked": true,
    "id": "3ba0f3fb628569a4a6c0fcdcae0d7ff6542df618",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Rui Jiao",
      "Zonghan Yang",
      "Maosong Sun",
      "Yang Liu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.161": {
    "title": "Template-Based Named Entity Recognition Using BART",
    "volume": "findings",
    "abstract": "There is a recent interest in investigating fewshot NER, where the low-resource target domain has different label sets compared with a resource-rich source domain. Existing methods use a similarity-based metric. However, they cannot make full use of knowledge transfer in NER model parameters. To address the issue, we propose a template-based method for NER, treating NER as a language model ranking problem in a sequence-to-sequence framework, where original sentences and statement templates filled by candidate named entity span are regarded as the source sequence and the target sequence, respectively. For inference, the model is required to classify each candidate span based on the corresponding template scores. Our experiments demonstrate that the proposed method achieves 92.55% F1 score on the CoNLL03 (rich-resource task), and significantly better than fine-tuning BERT 10.88%, 15.34%, and 11.73% F1 score on the MIT Movie, the MIT Restaurant, and the ATIS (low-resource task), respectively",
    "checked": true,
    "id": "ce498651107588db67adcfbb5479bdb416f4de2f",
    "semantic_title": "",
    "citation_count": 127,
    "authors": [
      "Leyang Cui",
      "Yu Wu",
      "Jian Liu",
      "Sen Yang",
      "Yue Zhang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.162": {
    "title": "Does it Matter When I Think You Are Lying?\" Improving Deception Detection by Integrating Interlocutor's Judgements in Conversations",
    "volume": "findings",
    "abstract": "It is well known that human is not good at deception detection because of a natural inclination of truth-bias. However, during a conversation, when an interlocutor (interrogator) is being asked explicitly to assess whether his/her interacting partner (deceiver) is lying, this perceptual judgment depends highly on how the interrogator interprets the context of the conversation. While the deceptive behaviors can be difficult to model due to their heterogeneous manifestation, we hypothesize that this contextual information, i.e., whether the interlocutor trusts or distrusts what his/her partner is saying, provides an important condition in which the deceiver's deceptive behaviors are more consistently distinct. In this work, we propose a Judgmental-Enhanced Automatic Deception Detection Network (JEADDN) that explicitly considers interrogator's perceived truths-deceptions with three types of speechlanguage features (acoustic-prosodic, linguistic, and conversational temporal dynamics features) extracted during a conversation. We evaluate our framework on a large Mandarin Chinese Deception Dialog Database. The results show that the method significantly outperforms the current state-of-the-art approach without conditioning on the judgements of interrogators on this database. We further demonstrate that the behaviors of interrogators are important in detecting deception when the interrogators distrust the deceivers. Finally, with the late fusion of audio, text, and turntaking dynamics (TTD) features, we obtain promising results of 87.27% and 94.18% accuracy under the conditions that the interrogators trust and distrust the deceivers in deception detection which improves 7.27% and 13.57% than the model without considering the judgements of interlocutor respectively",
    "checked": true,
    "id": "9e8c54f4bd1ad434f3ebde280e242a18c14a53c3",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huang-Cheng Chou",
      "Woan-Shiuan Chien",
      "Da-Cheng Juan",
      "Chi-Chun Lee"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.163": {
    "title": "High-Quality Dialogue Diversification by Intermittent Short Extension Ensembles",
    "volume": "findings",
    "abstract": null,
    "checked": true,
    "id": "66cb883bfba8600740a043c8c119ef541ec07b73",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Zhiwen Tang",
      "Hrishikesh Kulkarni",
      "Grace Hui Yang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.164": {
    "title": "Structured Refinement for Sequential Labeling",
    "volume": "findings",
    "abstract": "Filtering target-irrelevant information through hierarchically refining hidden states has been demonstrated to be effective for obtaining informative representations. However, previous work simply relies on locally normalized attention without considering possible labels at other time steps, the capacity for modeling long-term dependency relations is thus limited. In this paper, we propose to extend previous work with globally normalized attention, e.g., structured attention, to leverage structural information for more effective representation refinement. We also propose two implementation tricks to accelerate CRF computation and an initialization trick for Chinese character embeddings to further improve performance. We provide extensive experimental results on various datasets to show the effectiveness and efficiency of our proposed method",
    "checked": true,
    "id": "94205c0e603b0359f8fc7b6db3e7bb0b6b33ca6c",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiran Wang",
      "Hiroyuki Shindo",
      "Yuji Matsumoto",
      "Taro Watanabe"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.165": {
    "title": "End-to-End Construction of NLP Knowledge Graph",
    "volume": "findings",
    "abstract": null,
    "checked": true,
    "id": "751e2acb19c2b4e4023795e4d8973471a79ab8d2",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Ishani Mondal",
      "Yufang Hou",
      "Charles Jochim"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.166": {
    "title": "Deciphering Implicit Hate: Evaluating Automated Detection Algorithms for Multimodal Hate",
    "volume": "findings",
    "abstract": "Accurate detection and classification of online hate is a difficult task. Implicit hate is particularly challenging as such content tends to have unusual syntax, polysemic words, and fewer markers of prejudice (e.g., slurs). This problem is heightened with multimodal content, such as memes (combinations of text and images), as they are often harder to decipher than unimodal content (e.g., text alone). This paper evaluates the role of semantic and multimodal context for detecting implicit and explicit hate. We show that both textand visualenrichment improves model performance, with the multimodal model (0.771) outperforming other models' F1 scores (0.544, 0.737, and 0.754). While the unimodal-text context-aware (transformer) model was the most accurate on the subtask of implicit hate detection, the multimodal model outperformed it overall because of a lower propensity towards false positives. We find that all models perform better on content with full annotator agreement and that multimodal models are best at classifying the content where annotators disagree. To conduct these investigations, we undertook highquality annotation of a sample of 5,000 multimodal entries. Tweets were annotated for primary category, modality, and strategy. We make this corpus, along with the codebook, code, and final model, freely available",
    "checked": true,
    "id": "69d45dfa4d5049671df9e9214264fbd57d99951e",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Austin Botelho",
      "Scott Hale",
      "Bertie Vidgen"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.167": {
    "title": "Studying the Evolution of Scientific Topics and their Relationships",
    "volume": "findings",
    "abstract": "We propose a study of the development of scientific topics through time, as well as the relations between them within the scientific field of computational linguistics and across subfields. We use topic modeling to analyze scientific texts published in the ACL Anthology, and introduce a categorization of topics in our field into 3 types: tasks, algorithms, and data. In order to understand how topics emerge, evolve, and gradually disappear over time, we analyze the evolution of these topics across time through several case studies. We further include in our analysis papers published in NeurIPS, and try to understand whether there was any influence between topics in this conference focused on neural methods and computational linguistics conferences, as well as measure the divergence over time between conferences in terms of the topics approached. We additionally look at the relationships between topics, categorizing them into types of competing or cooperating topics",
    "checked": true,
    "id": "f9f5830e5d292edc28a94dd4b6439f47ba6c038c",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Ana Sabina Uban",
      "Cornelia Caragea",
      "Liviu P. Dinu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.168": {
    "title": "End-to-End Self-Debiasing Framework for Robust NLU Training",
    "volume": "findings",
    "abstract": "Existing Natural Language Understanding (NLU) models have been shown to incorporate dataset biases leading to strong performance on in-distribution (ID) test sets but poor performance on out-of-distribution (OOD) ones. We introduce a simple yet effective debiasing framework whereby the shallow representations of the main model are used to derive a bias model and both models are trained simultaneously. We demonstrate on three well studied NLU tasks that despite its simplicity, our method leads to competitive OOD results. It significantly outperforms other debiasing approaches on two tasks, while still delivering high in-distribution performance",
    "checked": true,
    "id": "faf201efd2b860d1f93b091387f6c031bc31eb15",
    "semantic_title": "",
    "citation_count": 19,
    "authors": [
      "Abbas Ghaddar",
      "Phillippe Langlais",
      "Mehdi Rezagholizadeh",
      "Ahmad Rashid"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.169": {
    "title": "A Mixed-Method Design Approach for Empirically Based Selection of Unbiased Data Annotators",
    "volume": "findings",
    "abstract": "Implicit bias embedded in the annotated data is by far the greatest impediment in the effectual use of supervised machine learning models in tasks involving race, ethics, and geopolitical polarization. For societal good and demonstrable positive impact on wider society, it is paramount to carefully select data annotators and rigorously validate the annotation process. Current approaches to selecting annotators are not sufficiently grounded in scientific principles and are limited at the policy-guidance level, thereby rendering them unusable for machine learning practitioners. This work proposes a new approach based on the mixed-methods design that is functional, adaptable, and simpler to implement in selecting unbiased annotators for any machine learning problem. By demonstrating it on a realworld geopolitical problem, we also identified and ranked key inane profile characteristics towards an empirically-based selection of unbiased data annotators",
    "checked": true,
    "id": "e3fc48e01eb6e89a793b8f981b5836beb525fd99",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gautam Thakur",
      "Janna Caspersen",
      "Drahomira Herrmannova",
      "Bryan Eaton",
      "Jordan Burdette"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.170": {
    "title": "An Evaluation of Disentangled Representation Learning for Texts",
    "volume": "findings",
    "abstract": "Learning disentangled representations of texts, which encode information pertaining to different aspects of the text in separate representations, is an active area of research in NLP for controllable and interpretable text generation. These methods have, for the most part, been developed in the context of text style transfer, but are limited in their evaluation. In this work, we look at the motivation behind learning disentangled representations of content and style for texts and at the potential use-cases when compared to end-to-end methods. We then propose evaluation metrics that correspond to these use-cases. We conduct a systematic investigation of previously proposed loss functions for such models and we evaluate them on a highly-structured and synthetic natural language dataset that is well-suited for the task of disentangled representation learning, as well as two other parallel style transfer datasets. Our results demonstrate that current models still require considerable amounts of supervision in order to achieve good performance",
    "checked": true,
    "id": "43ca608abb757aaf31f0d43a719d7331fa3e4e00",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Krishnapriya Vishnubhotla",
      "Graeme Hirst",
      "Frank Rudzicz"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.171": {
    "title": "Injecting Knowledge Base Information into End-to-End Joint Entity and Relation Extraction and Coreference Resolution",
    "volume": "findings",
    "abstract": null,
    "checked": true,
    "id": "05561128b2d98753775de9cb12a84ac1fd03a794",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Severine Verlinden",
      "Klim Zaporojets",
      "Johannes Deleu",
      "Thomas Demeester",
      "Chris Develder"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.172": {
    "title": "Knowing More About Questions Can Help: Improving Calibration in Question Answering",
    "volume": "findings",
    "abstract": "We study calibration in question answering, estimating whether model correctly predicts answer for each question. Unlike prior work which mainly rely on the model's confidence score, our calibrator incorporates information about the input example (e.g., question and the evidence context). Together with data augmentation via back translation, our simple approach achieves 5-10% gains in calibration accuracy on reading comprehension benchmarks. Furthermore, we present the first calibration study in the open retrieval setting, comparing the calibration accuracy of retrieval-based span prediction models and answer generation models. Here again, our approach shows consistent gains over calibrators relying on the model confidence. Our simple and efficient calibrator can be easily adapted to many tasks and model architectures, showing robust gains in all settings.1",
    "checked": true,
    "id": "2c33f4aa30a4d8f708db79eef5c9e2a0ce84695b",
    "semantic_title": "",
    "citation_count": 35,
    "authors": [
      "Shujian Zhang",
      "Chengyue Gong",
      "Eunsol Choi"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.173": {
    "title": "Enhancing Metaphor Detection by Gloss-based Interpretations",
    "volume": "findings",
    "abstract": "This paper focuses on utilizing metaphor interpretation to enhance metaphor detection. Considering that existing approaches to metaphor interpretation are limited by ambiguous meanings of the metaphorical substitute words, this paper proposes a novel interpretation mechanism that utilizes glosses to interpret metaphorical words. Since there is no dataset annotated for both metaphor detection and metaphor interpretation, we enhance three datasets TroFi, VUA, and PSUCMC from the field of metaphor detection with gloss annotations. Accordingly, we develop a model for jointly conducting metaphor detection and gloss-based interpretation (named MDGIJoint for short). Experimental results demonstrate that MDGI-Joint outperforms state-ofthe-art models on all the three enhanced datasets and that gloss-based metaphor interpretation benefits metaphor detection.1",
    "checked": true,
    "id": "095c38aa15b2fad08daa567aab316145a331e76c",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Hai Wan",
      "Jinxia Lin",
      "Jianfeng Du",
      "Dawei Shen",
      "Manrong Zhang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.174": {
    "title": "Evaluating Word Embeddings with Categorical Modularity",
    "volume": "findings",
    "abstract": "We introduce categorical modularity, a novel low-resource intrinsic metric to evaluate word embedding quality. Categorical modularity is a graph modularity metric based on the k-nearest neighbor graph constructed with embedding vectors of words from a fixed set of semantic categories, in which the goal is to measure the proportion of words that have nearest neighbors within the same categories. We use a core set of 500 words belonging to 59 neurobiologically motivated semantic categories in 29 languages and analyze three word embedding models per language (FastText, MUSE, and subs2vec). We find moderate to strong positive correlations between categorical modularity and performance on the monolingual tasks of sentiment analysis and word similarity calculation and on the cross-lingual task of bilingual lexicon induction both to and from English. Overall, we suggest that categorical modularity provides non-trivial predictive information about downstream task performance, with breakdowns of correlations by model suggesting some meta-predictive properties about semantic information loss as well",
    "checked": true,
    "id": "304f93ebf38fcb7726e89157fadb5527ba16a1b7",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sílvia Casacuberta",
      "Karina Halevy",
      "Damián Blasi"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.175": {
    "title": "Attention-based Contextual Language Model Adaptation for Speech Recognition",
    "volume": "findings",
    "abstract": "Language modeling (LM) for automatic speech recognition (ASR) does not usually incorporate utterance level contextual information. For some domains like voice assistants, however, additional context, such as time at which an utterance was spoken, provides a rich input signal. We introduce an attention mechanism for training neural speech recognition language models on both text and nonlinguistic contextual data 1. When applied to a large de-identified dataset of utterances collected by a popular voice assistant platform, our method reduces perplexity by 7.0% relative over a standard LM that does not incorporate contextual information. When evaluated on utterances extracted from the long tail of the dataset, our method improves perplexity by 9.0% relative over a standard LM and by over 2.8% relative when compared to a state-of-theart model for contextual LM",
    "checked": true,
    "id": "7597deda4ff66e8aa068b7f4da993fd6c2e8b4c6",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Richard Diehl Martinez",
      "Scott Novotney",
      "Ivan Bulyko",
      "Ariya Rastrow",
      "Andreas Stolcke",
      "Ankur Gandhe"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.176": {
    "title": "Annotation and Evaluation of Coreference Resolution in Screenplays",
    "volume": "findings",
    "abstract": "Screenplays refer to characters using different names, pronouns, and nominal expressions. We need to resolve these mentions to the correct referent character for better story understanding and holistic research in computational narratology. Coreference resolution of character mentions in screenplays becomes challenging because of the large document lengths, unique structural features like scene headers, interleaving of action and speech passages, and reliance on the accompanying video. In this work, we first adapt widelyused annotation guidelines to address domainspecific issues in screenplays. We develop an automatic screenplay parser to extract the structural information and design coreference rules based upon the structure. Our model exploits these structural features and outperforms a benchmark coreference model on the screenplay coreference resolution task",
    "checked": true,
    "id": "8a740e79efe29be8154c91becc6277b699b1c766",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sabyasachee Baruah",
      "Sandeep Nallan Chakravarthula",
      "Shrikanth Narayanan"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.177": {
    "title": "Exploring Cross-Lingual Transfer Learning with Unsupervised Machine Translation",
    "volume": "findings",
    "abstract": "In Natural Language Understanding (NLU), to facilitate Cross-Lingual Transfer Learning (CLTL), especially CLTL between distant languages, we integrate CLTL with Machine Translation (MT), and thereby propose a novel CLTL model named Translation Aided Language Learner (TALL). TALL is constructed as a standard transformer, where the encoder is a pre-trained multilingual language model. The training of TALL includes an MT-oriented pre-training and an NLU-oriented fine-tuning. To make use of unannotated data, we implement the recently proposed Unsupervised Machine Translation (UMT) technique in the MToriented pre-training of TALL. The experimental results show that the application of UMT enables TALL to consistently achieve better CLTL performance than our baseline model, which is the pre-trained multilingual language model serving as the encoder of TALL, without using more annotated data, and the performance gain is relatively prominent in the case of distant languages",
    "checked": true,
    "id": "04a5ab102b5b4b0cbdc5dd5850a8fe925757e988",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Chao Wang",
      "Judith Gaspers",
      "Thi Ngoc Quynh Do",
      "Hui Jiang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.178": {
    "title": "Pipeline Signed Japanese Translation Focusing on a Post-positional Particle Complement and Conjugation in a Low-resource Setting",
    "volume": "findings",
    "abstract": "Because sign language is a visual language, the translation of it into spoken language is typically performed through an intermediate representation called gloss notation. In sign language, function words, such as particles and determiners, are not explicitly expressed, and there is little or no concept of morphological inflection in sign language. Therefore, gloss notation does not include such linguistic constructs. Because of these factors, we argue that sign language translation is effectively processed by taking advantage of the similarities and differences between sign language and its spoken counterpart. We thus propose a pipeline translation method that clearly focuses on the difference between spoken Japanese and signed Japanese written in gloss notation. Specifically, our method first uses statistical machine translation (SMT) to map glosses to corresponding spoken language words. We then use three transformer-based seq2seq models trained using a large out-ofdomain monolingual Japanese corpus to complement postpositional particles and estimate conjugations for the verbs, adjectives, and auxiliary verbs in the first translation. We apply the seq2seq models in sequence until the translation converges. Our experimental results show that the proposed method performs robustly on the low-resource corpus and is +4.4/+4.9 points above the SMT baseline for BLEU-3/4",
    "checked": true,
    "id": "ffa8f901372d67bd6e4b4f8c0f89f264fcdddce7",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ken Yano",
      "Akira Utsumi"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.179": {
    "title": "Language-Mediated, Object-Centric Representation Learning",
    "volume": "findings",
    "abstract": "We present Language-mediated, Objectcentric Representation Learning (LORL), a paradigm for learning disentangled, objectcentric scene representations from vision and language. LORL builds upon recent advances in unsupervised object discovery and segmentation, notably MONet and Slot Attention. While these algorithms learn an object-centric representation just by reconstructing the input image, LORL enables them to further learn to associate the learned representations to concepts, i.e., words for object categories, properties, and spatial relationships, from language input. These object-centric concepts derived from language facilitate the learning of object-centric representations. LORL can be integrated with various unsupervised object discovery algorithms that are language-agnostic. Experiments show that the integration of LORL consistently improves the performance of unsupervised object discovery methods on two datasets via the help of language. We also show that concepts learned by LORL, in conjunction with object discovery methods, aid downstream tasks such as referring expression comprehension",
    "checked": true,
    "id": "88f5743491bdf7368ec50a4340a629069dd64adb",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Ruocheng Wang",
      "Jiayuan Mao",
      "Samuel Gershman",
      "Jiajun Wu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.180": {
    "title": "Entheos: A Multimodal Dataset for Studying Enthusiasm",
    "volume": "findings",
    "abstract": "Enthusiasm plays an important role in engaging communication. It enables speakers to be distinguished and remembered, creating an emotional bond that inspires and motivates their addressees to act, listen, and coordinate (Bettencourt et al., 1983). Although people can easily identify enthusiasm, this is a rather difficult task for machines due to the lack of resources and models that can help them understand or generate enthusiastic behavior. We introduce Entheos, the first multimodal dataset for studying enthusiasm composed of video, audio, and text. We present several baseline models and an ablation study using different features, showing the importance of pitch, loudness, and discourse relation parsing in distinguishing enthusiastic communication",
    "checked": true,
    "id": "2d09646cb71e088d18d57a6915868a583a1c927d",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Carla Viegas",
      "Malihe Alikhani"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.181": {
    "title": "Are Rotten Apples Edible? Challenging Commonsense Inference Ability with Exceptions",
    "volume": "findings",
    "abstract": "Previous studies have argued that pre-trained language models encode commonsense relational knowledge (e.g. that apples are edible). However, simultaneous work has revealed that such models are often insensitive to context, even ignoring overt contextual cues such as negations. In this paper, we investigate whether masked language models (the BERT family) can move beyond naive associative biases (e.g., apple → edible) when the context warrants (e.g. ranking inedible higher when presented with the information that the apple is rotten). We introduce the WINOVENTI procedure, which adversarially exploits generic associations in masked language models to create model-specific Winograd-style entailment schemas. Using our constructed WINOVENTI challenges set of over 2, 000 schemas, we show that language models in the BERT family experience a steep drop in performance on prompts that require them to pick answers which require reasoning about context (e.g., from 89.8% to 18.4% for BERTLARGE). We present evidence that language models exhibit different associative biases, suggesting a need for future work in developing and analyzing frameworks similar to WINOVENTI that are tuned to model-specific weaknesses",
    "checked": true,
    "id": "fd074eaa6b2441347238df28462b5db39666c327",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Nam Do",
      "Ellie Pavlick"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.182": {
    "title": "GRICE: A Grammar-based Dataset for Recovering Implicature and Conversational rEasoning",
    "volume": "findings",
    "abstract": null,
    "checked": true,
    "id": "e764dee4e50db01d77976e8f313fc092fc0eba85",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Zilong Zheng",
      "Shuwen Qiu",
      "Lifeng Fan",
      "Yixin Zhu",
      "Song-Chun Zhu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.183": {
    "title": "RetroGAN: A Cyclic Post-Specialization System for Improving Out-of-Knowledge and Rare Word Representations",
    "volume": "findings",
    "abstract": "Retrofitting is a technique used to move word vectors closer together or further apart in their space to reflect their relationships in a Knowledge Base (KB). However, retrofitting only works on concepts that are present in that KB. RetroGAN uses a pair of Generative Adversarial Networks (GANs) to learn a oneto-one mapping between concepts and their retrofitted counterparts. It applies that mapping (post-specializes) to handle concepts that do not appear in the original KB in a manner similar to how some natural language systems handle out-of-vocabulary entries. We test our system on three word-similarity benchmarks and a downstream sentence simplification task, and achieve the state of the art (CARD-660). Altogether, our results demonstrate our system's effectiveness for out-of-knowledge and rare word generalization",
    "checked": true,
    "id": "644d4ffd908698e4fafa0b1320c185a3e2c94495",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Pedro Colon-Hernandez",
      "Yida Xin",
      "Henry Lieberman",
      "Catherine Havasi",
      "Cynthia Breazeal",
      "Peter Chin"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.184": {
    "title": "Fusion: Towards Automated ICD Coding via Feature Compression",
    "volume": "findings",
    "abstract": "ICD coding aims to automatically assign International Classification of Diseases (ICD) codes from unstructured clinical notes or discharge summaries, which saves human labor and reduces errors. Although several studies are proposed to solve this challenging task, none distinguishes the importance of different phrases with a word window. Intuitively, informative phrases should be more useful for the prediction. This paper proposes a feature compressed ICD coding model named Fusion to address this issue. In particular, we propose an attentive soft-pooling approach to compress the sparse and redundant word representations into informative and dense ones as local features. Besides, we use the key-query attention mechanism for modeling the inner relations among local features to generate the global features, which are further used to predict ICD codes. Experiments on two widely used datasets demonstrate that Fusion outperforms baselines. However, on the MIMIC-III Full dataset, we find that none of the state-ofthe-art approaches significantly perform better than others. Thus, automated ICD coding is still a challenging task",
    "checked": true,
    "id": "ae5294d6e32bf1af730101db30708cabc942f48f",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Junyu Luo",
      "Cao Xiao",
      "Lucas Glass",
      "Jimeng Sun",
      "Fenglong Ma"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.185": {
    "title": "Automatic Document Sketching: Generating Drafts from Analogous Texts",
    "volume": "findings",
    "abstract": "The advent of large pre-trained language models has made it possible to make high-quality predictions on how to add or change a sentence in a document. However, the high branching factor inherent to text generation impedes the ability of even the strongest language models to offer useful editing suggestions at a more global or document level. We introduce a new task, DOCUMENT SKETCHING, which involves generating entire draft documents for the writer to review and revise. These drafts are built from sets of documents that overlap in form – sharing large segments of potentially reusable text – while diverging in content. To support this task, we introduce a Wikipediabased dataset of analogous documents and investigate the application of weakly supervised methods, including use of a transformer-based mixture of experts, together with reinforcement learning. We report experiments using automated and human evaluation methods and discuss relative merits of these models",
    "checked": true,
    "id": "c86f268c3f37749595baad4f2474edf668645756",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Zeqiu Wu",
      "Michel Galley",
      "Chris Brockett",
      "Yizhe Zhang",
      "Bill Dolan"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.186": {
    "title": "Trade the Event: Corporate Events Detection for News-Based Event-Driven Trading",
    "volume": "findings",
    "abstract": "In this paper, we introduce an event-driven trading strategy that predicts stock movements by detecting corporate events from news articles. Unlike existing models that utilize textual features (e.g., bag-of-words) and sentiments to directly make stock predictions, we consider corporate events as the driving force behind stock movements and aim to profit from the temporary stock mispricing that may occur when corporate events take place. The core of the proposed strategy is a bi-level event detection model. The low-level event detector identifies events' existences from each token, while the high-level event detector incorporates the entire article's representation and the low-level detected results to discover events at the article-level. We also develop an elaborately-annotated dataset EDT for corporate event detection and news-based stock prediction benchmark. EDT includes 9721 news articles with token-level event labels as well as 303893 news articles with minute-level timestamps and comprehensive stock price labels. Experiments on EDT indicate that the proposed strategy outperforms all the baselines in winning rate, excess returns over the market, and the average return on each transaction. 1",
    "checked": true,
    "id": "803750b376603d1aa4b8512a18a865b0adbdc32c",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Zhihan Zhou",
      "Liqian Ma",
      "Han Liu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.187": {
    "title": "Language-based General Action Template for Reinforcement Learning Agents",
    "volume": "findings",
    "abstract": "Prior knowledge plays a critical role in decision-making, and humans preserve such knowledge in the form of natural language (NL). To emulate real-world decision-making, artificial agents should incorporate such generic knowledge into their decisionmaking framework through NL. However, since policy learning with NL-based action representation is intractable due to NL's combinatorial complexity, previous studies have limited agents' expressive power to only a specific environment, which sacrificed the generalization ability to other environments. This paper proposes a new environmentagnostic action framework, the languagebased general action template (L-GAT). We design action templates on the basis of general semantic schemes (FrameNet, VerbNet, and WordNet), facilitating the agent in finding a plausible action in a given state by using prior knowledge while covering broader types of actions in a general manner. Our experiment using 18 text-based games showed that our proposed L-GAT agent which uses the same actions across games, achieved a performance competitive with agents that rely on gamespecific actions. We have published the code at https://github.com/kohilin/lgat",
    "checked": true,
    "id": "182c36a784f768da6676a258813b94cc74a67be8",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Ryosuke Kohita",
      "Akifumi Wachi",
      "Daiki Kimura",
      "Subhajit Chaudhury",
      "Michiaki Tatsubori",
      "Asim Munawar"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.188": {
    "title": "MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers",
    "volume": "findings",
    "abstract": "We generalize deep self-attention distillation in MINILM (Wang et al., 2020) by only using self-attention relation distillation for taskagnostic compression of pretrained Transformers. In particular, we define multi-head selfattention relations as scaled dot-product between the pairs of query, key, and value vectors within each self-attention module. Then we employ the above relational knowledge to train the student model. Besides its simplicity and unified principle, more favorably, there is no restriction in terms of the number of student's attention heads, while most previous work has to guarantee the same head number between teacher and student. Moreover, the fine-grained self-attention relations tend to fully exploit the interaction knowledge learned by Transformer. In addition, we thoroughly examine the layer selection strategy for teacher models, rather than just relying on the last layer as in MINILM. We conduct extensive experiments on compressing both monolingual and multilingual pretrained models. Experimental results demonstrate that our models1 distilled from base-size and large-size teachers (BERT, RoBERTa and XLM-R) outperform the state-of-the-art",
    "checked": true,
    "id": "b5b006dc558cb7fbd532d67e989173b536e8ac80",
    "semantic_title": "",
    "citation_count": 85,
    "authors": [
      "Wenhui Wang",
      "Hangbo Bao",
      "Shaohan Huang",
      "Li Dong",
      "Furu Wei"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.189": {
    "title": "Attending via both Fine-tuning and Compressing",
    "volume": "findings",
    "abstract": "Though being a primary trend for enhancing interpretability of neural networks, attention mechanism's reliability and validity are still under debate. In this paper, we try to purify attention scores to obtain a more faithful explanation of downstream models. Specifically, we propose a framework consisting of a learner and a compressor, which performs finetuning and compressing iteratively to enhance the performance and interpretability of the attention mechanism. The learner focuses on learning better text representations to achieve good decisions by fine-tuning, while the compressor aims to perform compressions over the representations to retain the most useful clues for explanations with a Variational information bottleneck ATtention (VAT) mechanism. Extensive experiments on eight benchmark datasets show the great advantages of our proposed approach in terms of both performance and interpretability",
    "checked": true,
    "id": "fd375925c8fc8e932e6c8356b2c776cd81f69d37",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Jie Zhou",
      "Yuanbin Wu",
      "Qin Chen",
      "Xuanjing Huang",
      "Liang He"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.190": {
    "title": "Improving Event Causality Identification via Self-Supervised Representation Learning on External Causal Statement",
    "volume": "findings",
    "abstract": "Current models for event causality identification (ECI) mainly adopt a supervised framework, which heavily rely on labeled data for training. Unfortunately, the scale of current annotated datasets is relatively limited, which cannot provide sufficient support for models to capture useful indicators from causal statements, especially for handing those new, unseen cases. To alleviate this problem, we propose a novel approach, shortly named CauSeRL, which leverages external causal statements for event causality identification. First of all, we design a self-supervised framework to learn context-specific causal patterns from external causal statements. Then, we adopt a contrastive transfer strategy to incorporate the learned context-specific causal patterns into the target ECI model. Experimental results show that our method significantly outperforms previous methods on EventStoryLine and Causal-TimeBank (+2.0 and +3.4 points on F1 value respectively)",
    "checked": true,
    "id": "47ea802a1fed0f735a9a61a7c9b63062f02a5f5d",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Xinyu Zuo",
      "Pengfei Cao",
      "Yubo Chen",
      "Kang Liu",
      "Jun Zhao",
      "Weihua Peng",
      "Yuguang Chen"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.191": {
    "title": "PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval",
    "volume": "findings",
    "abstract": "Recently, dense passage retrieval has become a mainstream approach to finding relevant information in various natural language processing tasks. A number of studies have been devoted to improving the widely adopted dual-encoder architecture. However, most of the previous studies only consider query-centric similarity relation when learning the dual-encoder retriever. In order to capture more comprehensive similarity relations, we propose a novel approach that leverages both query-centric and PAssage-centric sImilarity Relations (called PAIR) for dense passage retrieval. To implement our approach, we make three major technical contributions by introducing formal formulations of the two kinds of similarity relations, generating high-quality pseudo labeled data via knowledge distillation, and designing an effective two-stage training procedure that incorporates passage-centric similarity relation constraint. Extensive experiments show that our approach significantly outperforms previous state-of-the-art models on both MSMARCO and Natural Questions datasets1",
    "checked": true,
    "id": "4097ae9aca6444fd7536bfbed1e62560521b70d3",
    "semantic_title": "",
    "citation_count": 48,
    "authors": [
      "Ruiyang Ren",
      "Shangwen Lv",
      "Yingqi Qu",
      "Jing Liu",
      "Wayne Xin Zhao",
      "QiaoQiao She",
      "Hua Wu",
      "Haifeng Wang",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.192": {
    "title": "Is Human Scoring the Best Criteria for Summary Evaluation?",
    "volume": "findings",
    "abstract": "Normally, summary quality measures are compared with quality scores produced by human annotators. A higher correlation with human scores is considered to be a fair indicator of a better measure. We discuss observations that cast doubt on this view. We attempt to show a possibility of an alternative indicator. Given a family of measures, we explore a criterion of selecting the best measure not relying on correlations with human scores. Our observations for the BLANC family of measures suggest that the criterion is universal across very different styles of summaries",
    "checked": true,
    "id": "0d8bf9ca6e58ab9f34bc9a24a82884073d0153ea",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Oleg Vasilyev",
      "John Bohannon"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.193": {
    "title": "Assessing Dialogue Systems with Distribution Distances",
    "volume": "findings",
    "abstract": "An important aspect of developing dialogue systems is how to evaluate and compare the performance of different systems. Existing automatic evaluation metrics are based on turnlevel quality evaluation and use average scores for system-level comparison. In this paper, we propose to measure the performance of a dialogue system by computing the distributionwise distance between its generated conversations and real-world conversations. Specifically, two distribution-wise metrics, FBD and PRD, are developed and evaluated. Experiments on several dialogue corpora show that our proposed metrics correlate better with human judgments than existing metrics",
    "checked": true,
    "id": "e3d8f27f124cdf495834798ffcc5507005ceab6d",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Jiannan Xiang",
      "Yahui Liu",
      "Deng Cai",
      "Huayang Li",
      "Defu Lian",
      "Lemao Liu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.194": {
    "title": "Neural Combinatory Constituency Parsing",
    "volume": "findings",
    "abstract": "We propose two fast neural combinatory models for constituency parsing: binary and multibranching. Our models decompose the bottomup parsing process into 1) classification of tags, labels, and binary orientations or chunks and 2) vector composition based on the computed orientations or chunks. These models have theoretical sub-quadratic complexity and empirical linear complexity. The binary model achieves an F1 score of 92.54 on Penn Treebank, speeding at 1327.2 sents/sec. Both the models with XLNet provide near state-of-theart accuracies for English. Syntactic branching tendency and headedness of a language are observed during the training and inference processes for Penn Treebank, Chinese Treebank, and Keyaki Treebank (Japanese)",
    "checked": true,
    "id": "37d3a46448e87d7abcb287472baf886c65783ee4",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Zhousi Chen",
      "Longtu Zhang",
      "Aizhan Imankulova",
      "Mamoru Komachi"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.195": {
    "title": "Learning Shared Semantic Space for Speech-to-Text Translation",
    "volume": "findings",
    "abstract": "Having numerous potential applications and great impact, end-to-end speech translation (ST) has long been treated as an independent task, failing to fully draw strength from the rapid advances of its sibling text machine translation (MT). With text and audio inputs represented differently, the modality gap has rendered MT data and its end-to-end models incompatible with their ST counterparts. In observation of this obstacle, we propose to bridge this representation gap with Chimera. By projecting audio and text features to a common semantic representation, Chimera unifies MT and ST tasks and boosts the performance on ST benchmarks, MuST-C and Augmented Librispeech, to a new state-of-the-art. Specifically, Chimera obtains 27.1 BLEU on MuST-C ENDE, improving the SOTA by a +1.9 BLEU margin. Further experimental analyses demonstrate that the shared semantic space indeed conveys common knowledge between these two tasks and thus paves a new way for augmenting training resources across modalities. 1",
    "checked": true,
    "id": "8b8f7c580bb94ace0676be7a5c424b27b1194913",
    "semantic_title": "",
    "citation_count": 35,
    "authors": [
      "Chi Han",
      "Mingxuan Wang",
      "Heng Ji",
      "Lei Li"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.196": {
    "title": "Empowering Language Understanding with Counterfactual Reasoning",
    "volume": "findings",
    "abstract": "Present language understanding methods have demonstrated extraordinary ability of recognizing patterns in texts via machine learning. However, existing methods indiscriminately use the recognized patterns in the testing phase that is inherently different from us humans who have counterfactual thinking, e.g., to scrutinize for the hard testing samples. Inspired by this, we propose a Counterfactual Reasoning Model, which mimics the counterfactual thinking by learning from few counterfactual samples. In particular, we devise a generation module to generate representative counterfactual samples for each factual sample, and a retrospective module to retrospect the model prediction by comparing the counterfactual and factual samples. Extensive experiments on sentiment analysis (SA) and natural language inference (NLI) validate the effectiveness of our method",
    "checked": true,
    "id": "f50abcae0477351e9d2814547158d348ebf59bcb",
    "semantic_title": "",
    "citation_count": 18,
    "authors": [
      "Fuli Feng",
      "Jizhi Zhang",
      "Xiangnan He",
      "Hanwang Zhang",
      "Tat-Seng Chua"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.197": {
    "title": "Knowledge-Empowered Representation Learning for Chinese Medical Reading Comprehension: Task, Model and Resources",
    "volume": "findings",
    "abstract": "Machine Reading Comprehension (MRC) aims to extract answers to questions given a passage. It has been widely studied recently, especially in open domains. However, few efforts have been made on closed-domain MRC, mainly due to the lack of large-scale training data. In this paper, we introduce a multi-target MRC task for the medical domain, whose goal is to predict answers to medical questions and the corresponding support sentences from medical information sources simultaneously, in order to ensure the high reliability of medical knowledge serving. A high-quality dataset is manually constructed for the purpose, named Multi-task Chinese Medical MRC dataset (CMedMRC), with detailed analysis conducted. We further propose the Chinese medical BERT model for the task (CMedBERT), which fuses medical knowledge into pre-trained language models by the dynamic fusion mechanism of heterogeneous features and the multi-task learning strategy. Experiments show that CMedBERT consistently outperforms strong baselines by fusing context-aware and knowledge-aware token representations",
    "checked": true,
    "id": "5f5929efc9bb806f0dfa1c8af38a5e49a40d922d",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Taolin Zhang",
      "Chengyu Wang",
      "Minghui Qiu",
      "Bite Yang",
      "Zerui Cai",
      "Xiaofeng He",
      "Jun Huang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.198": {
    "title": "Correcting Chinese Spelling Errors with Phonetic Pre-training",
    "volume": "findings",
    "abstract": "Chinese spelling correction (CSC) is an important yet challenging task. Existing state-ofthe-art methods either only use a pre-trained language model or incorporate phonological information as external knowledge. In this paper, we propose a novel end-to-end CSC model that integrates phonetic features into language model by leveraging the powerful pre-training and fine-tuning method. Instead of conventionally masking words with a special token in training language model, we replace words with phonetic features and their sound-alike words. We further propose an adaptive weighted objective to jointly train error detection and correction in a unified framework. Experimental results show that our model achieves significant improvements on SIGHAN datasets and outperforms the previous state-of-the-art methods",
    "checked": true,
    "id": "c70b9276d4e4a27f2218bc3e921e9fc3ffd18f14",
    "semantic_title": "",
    "citation_count": 29,
    "authors": [
      "Ruiqing Zhang",
      "Chao Pang",
      "Chuanqiang Zhang",
      "Shuohuan Wang",
      "Zhongjun He",
      "Yu Sun",
      "Hua Wu",
      "Haifeng Wang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.199": {
    "title": "Multi-Lingual Question Generation with Language Agnostic Language Model",
    "volume": "findings",
    "abstract": "Question generation is the task of generating coherent and relevant question given context paragraph. Recently, with the development of large-scale question answering datasets such as SQuAD, the English question generation has been rapidly developed. However, for other languages such as Chinese, the available training data is limited, which hinders the development of question generation in the corresponding language. To investigate the multi-lingual question generation, in this paper, we develop a language-agnostic language model, which learns the shared representation from several languages in a single architecture. We propose an adversarial training objective to encourage the model to learn both language-specific and language-independent information. We utilize abundant monolingual text to improve the multi-lingual question generation via pre-training. With the languageagnostic language model, we achieve significant improvement in multi-lingual question generation over five languages. In addition, we propose a large-scale Chinese question generation dataset containing more than 220k human-generated questions to benefit the multi-lingual question generation research",
    "checked": true,
    "id": "aaef74b96d40ba33a21f553117e7b962b1ce00ba",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Bingning Wang",
      "Ting Yao",
      "Weipeng Chen",
      "Jingfang Xu",
      "Xiaochuan Wang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.200": {
    "title": "Structure-Aware Pre-Training for Table-to-Text Generation",
    "volume": "findings",
    "abstract": "Table-to-text generation is a subtask of datato-text generation which aims to generate naltural language text based on input table. Pretraining techniques have achieved great success on table-to-text generation. However, the pre-trained models used in previous works are typically trained on free-form natural language text while the input of table-to-text task is structured table. In this paper, we propose STTP, a pre-trained model that is trained with tables and their contexts. The STTP model can understand the structured input table and generate fluent text. Experiments on two datasets show the efficacy of our model",
    "checked": true,
    "id": "70cd8680ef7fa1ac476a901ab94d16e310fb790e",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Xinyu Xing",
      "Xiaojun Wan"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.201": {
    "title": "On the Interplay Between Fine-tuning and Composition in Transformers",
    "volume": "findings",
    "abstract": "Pre-trained transformer language models have shown remarkable performance on a variety of NLP tasks. However, recent research has suggested that phrase-level representations in these models reflect heavy influences of lexical content, but lack evidence of sophisticated, compositional phrase information (Yu and Ettinger, 2020). Here we investigate the impact of fine-tuning on the capacity of contextualized embeddings to capture phrase meaning information beyond lexical content. Specifically, we fine-tune models on an adversarial paraphrase classification task with high lexical overlap, and on a sentiment classification task. After fine-tuning, we analyze phrasal representations in controlled settings following prior work. We find that fine-tuning largely fails to benefit compositionality in these representations, though training on sentiment yields a small, localized benefit for certain models. In follow-up analyses, we identify confounding cues in the paraphrase dataset that may explain the lack of composition benefits from that task, and we discuss potential factors underlying the localized benefits from sentiment training",
    "checked": true,
    "id": "52b1cf563d1368f72e82b91b0349a7012a746f4f",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Lang Yu",
      "Allyson Ettinger"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.202": {
    "title": "Lifelong Learning of Topics and Domain-Specific Word Embeddings",
    "volume": "findings",
    "abstract": "Lifelong topic models mainly focus on indomain text streams in which each chunk only contains documents from a single domain. To overcome data diversity of the in-domain corpus, most of the existing methods exploit the information from limited sources in a separate and heuristic manner. In this study, we develop a lifelong collaborative model (LCM) based on non-negative matrix factorization to accurately learn topics and domain-specific word embeddings. LCM particularly investigates: (1) developing a knowledge graph based on the semantic relationships among words in the lifelong learning process, so as to accumulate global context information discovered by topic models and local context information reflected by context word embeddings from previous domains, and (2) developing a subword graph based on byte pair encoding and pairwise word relationships to exploit subword information of words in the current in-domain corpus. To the best of our knowledge, we are the first to collaboratively learn topics and word embeddings via lifelong learning. Experiments on real-world in-domain text streams validate the effectiveness of our method",
    "checked": true,
    "id": "4e7891e7f3d9380614cb6b0152a7129ed5f4d1d3",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Xiaorui Qin",
      "Yuyin Lu",
      "Yufu Chen",
      "Yanghui Rao"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.203": {
    "title": "Leveraging Argumentation Knowledge Graph for Interactive Argument Pair Identification",
    "volume": "findings",
    "abstract": "Interactive argument pair identification is essential in the context of dialogical argumentation mining. Existing research treats it as a problem of sentence matching and largely relies on textual information to compute the similarities. However, the interaction of opinions usually involves the background of the topic and requires reasoning of knowledge, which is beyond textual information. In this paper, we propose to leverage external knowledge to enhance the identification of interactive argument pairs. We construct the argumentation knowledge graph from the discussion thread of the target topic in the online forum. The interaction between the original argument and the reply is then represented as the path of concepts in the knowledge graph. In practice, we utilize Graph Convolutional Network (GCN) to learn the concept representation in the knowledge graph and use a Transformerbased encoder to learn the representation of paths. Finally, an information alignment network is employed to capture the interaction of textual information of conceptual information (both entity-level and path-level). Experiment results indicate that our model achieves state-of-the-art performance in the benchmark dataset. Further analysis demonstrates the effectiveness of our model for enforcing knowledge reasoning through paths in the knowledge graph",
    "checked": true,
    "id": "63506de8d7f16369e8e73dadff1d0675ca89f5c1",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Jian Yuan",
      "Zhongyu Wei",
      "Donghua Zhao",
      "Qi Zhang",
      "Changjian Jiang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.204": {
    "title": "A Multi-Task Learning Framework for Multi-Target Stance Detection",
    "volume": "findings",
    "abstract": "Multi-target stance detection aims to identify the stance taken toward a pair of different targets from the same text, and typically, there are multiple target pairs per dataset. Existing works generally train one model for each target pair. However, they fail to learn target-specific representations and are prone to overfitting. In this paper, we propose a new training strategy under the multi-task learning setting by training one model on all target pairs, which helps the model learn more universal representations and alleviate overfitting. Moreover, in order to extract more accurate target-specific representations, we propose a multi-task learning network which can jointly train our model with a stance (dis)agreement detection task that is designed to identify agreement and disagreement between stances in paired texts. Experimental results demonstrate that our proposed model outperforms the best-performing baseline by 12.39% in macro-averaged F1-score. Our resources are publicly available on GitHub.1",
    "checked": true,
    "id": "167841e8d0a0a58ad7e92de8e9babdff057e4735",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Yingjie Li",
      "Cornelia Caragea"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.205": {
    "title": "Confidence-Aware Scheduled Sampling for Neural Machine Translation",
    "volume": "findings",
    "abstract": "Scheduled sampling is an effective method to alleviate the exposure bias problem of neural machine translation. It simulates the inference scene by randomly replacing groundtruth target input tokens with predicted ones during training. Despite its success, its critical schedule strategies are merely based on training steps, ignoring the real-time model competence, which limits its potential performance and convergence speed. To address this issue, we propose confidence-aware scheduled sampling. Specifically, we quantify realtime model competence by the confidence of model predictions, based on which we design fine-grained schedule strategies. In this way, the model is exactly exposed to predicted tokens for high-confidence positions and still ground-truth tokens for low-confidence positions. Moreover, we observe vanilla scheduled sampling suffers from degenerating into the original teacher forcing mode since most predicted tokens are the same as ground-truth tokens. Therefore, under the above confidenceaware strategy, we further expose more noisy tokens (e.g., wordy and incorrect word order) instead of predicted ones for high-confidence token positions. We evaluate our approach on the Transformer and conduct experiments on large-scale WMT 2014 English-German, WMT 2014 English-French, and WMT 2019 Chinese-English. Results show that our approach significantly outperforms the Transformer and vanilla scheduled sampling on both translation quality and convergence speed",
    "checked": true,
    "id": "938d5a00caa0a5951e1d6d6879b3f904b9741b13",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Yijin Liu",
      "Fandong Meng",
      "Yufeng Chen",
      "Jinan Xu",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.206": {
    "title": "MA-BERT: Learning Representation by Incorporating Multi-Attribute Knowledge in Transformers",
    "volume": "findings",
    "abstract": "Incorporating attribute information such as user and product features into deep neural networks has been shown to be useful in sentiment analysis. Previous works typically accomplished this in two ways: concatenating multiple attributes to word/text representation or treating them as a bias to adjust attention distribution. To leverage the advantages of both methods, this paper proposes a multi-attribute BERT (MA-BERT) to incorporate external attribute knowledge. The proposed method has two advantages. First, it applies multi-attribute transformer (MA-Transformer) encoders to incorporate multiple attributes into both input representation and attention distribution. Second, the MA-Transformer is implemented as a universal layer and stacked on a BERT-based model such that it can be initialized from a pre-trained checkpoint and fine-tuned for the downstream applications without extra pretraining costs. Experiments on three benchmark datasets show that the proposed method outperformed pre-trained BERT models and other methods incorporating external attribute knowledge",
    "checked": true,
    "id": "92492f73a2d2afac5ff18ad186fc83444c98991d",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "You Zhang",
      "Jin Wang",
      "Liang-Chih Yu",
      "Xuejie Zhang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.207": {
    "title": "A Closer Look into the Robustness of Neural Dependency Parsers Using Better Adversarial Examples",
    "volume": "findings",
    "abstract": "Previous work on adversarial attacks on dependency parsers has mostly focused on attack methods, as opposed to the quality of adversarial examples, which in previous work has been relatively low. To address this gap, we propose a method to generate high-quality adversarial examples with a higher number of candidate generators and stricter filters, and then verify their quality using automatic and human evaluations. We perform analysis with different parsing models and observe that: (i) injecting words not used in the training stage is an effective attack strategy; (ii) adversarial examples generated against a parser strongly depend on the parser model, the token embeddings, and even the specific instantiation of the model (i.e., a random seed). We use these insights to improve the robustness of English parsing models, relying on adversarial training and model ensembling.1",
    "checked": true,
    "id": "5dec0a3d01bb36074ae7dd1965ffe14b8f11f755",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Yuxuan Wang",
      "Wanxiang Che",
      "Ivan Titov",
      "Shay B. Cohen",
      "Zhilin Lei",
      "Ting Liu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.208": {
    "title": "P-Stance: A Large Dataset for Stance Detection in Political Domain",
    "volume": "findings",
    "abstract": "Stance detection determines whether the author of a text is in favor of, against or neutral to a specific target and provides valuable insights into important events such as presidential election. However, progress on stance detection has been hampered by the absence of large annotated datasets. In this paper, we present P-STANCE, a large stance detection dataset in the political domain, which contains 21,574 labeled tweets. We provide a detailed description of the newly created dataset and develop deep learning models on it. Our best model achieves a macro-average F1-score of 80.53%, which we improve further by using semi-supervised learning. Moreover, our PSTANCE dataset can facilitate research in the fields of cross-domain stance detection such as cross-target stance detection where a classifier is adapted from a different but related target. We publicly release our dataset and code.1",
    "checked": true,
    "id": "7c235d03b34794ae9d421778a74252c559f7f419",
    "semantic_title": "",
    "citation_count": 24,
    "authors": [
      "Yingjie Li",
      "Tiberiu Sosea",
      "Aditya Sawant",
      "Ajith Jayaraman Nair",
      "Diana Inkpen",
      "Cornelia Caragea"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.209": {
    "title": "WIND: Weighting Instances Differentially for Model-Agnostic Domain Adaptation",
    "volume": "findings",
    "abstract": "Domain Adaptation is a fundamental problem in machine learning and natural language processing. In this paper, we study the domain adaptation problem from the perspective of instance weighting. Conventional instance weighting approaches cannot learn the weights which make the model generalize well in target domain. To tackle this problem, inspired by meta-learning, we formulate the domain adaptation problem as a bi-level optimization problem, and propose a novel differentiable modelagnostic instance weighting algorithm. Our proposed approach can automatically learn the instance weights instead of using manually designed weighting metrics. To reduce the computational complexity, we adopt the secondorder approximation technique during training. Experimental results1 on three different NLP tasks (Sentiment Classification, Neural Machine Translation and Relation Extraction) illustrate the efficacy of our proposed method",
    "checked": true,
    "id": "a6842f8d70f599fdd74617632dc89de071f0a94b",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Xiang Chen",
      "Yue Cao",
      "Xiaojun Wan"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.210": {
    "title": "DocOIE: A Document-level Context-Aware Dataset for OpenIE",
    "volume": "findings",
    "abstract": "Open Information Extraction (OpenIE) aims to extract structured relational tuples (subject, relation, object) from sentences and plays critical roles for many downstream NLP applications. Existing solutions perform extraction at sentence level, without referring to any additional contextual information. In reality, however, a sentence typically exists as part of a document rather than standalone; we often need to access relevant contextual information around the sentence before we can accurately interpret it. As there is no documentlevel context-aware OpenIE dataset available, we manually annotate 800 sentences from 80 documents in two domains (Healthcare and Transportation) to form a DocOIE dataset for evaluation. In addition, we propose DocIE, a novel document-level context-aware OpenIE model. Our experimental results based on DocIE demonstrate that incorporating documentlevel context is helpful in improving OpenIE performance. Both DocOIE dataset and DocIE model are released for public 1",
    "checked": true,
    "id": "1249cce1cf2dd9c0f1d2daa261c0164d89ffec90",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Kuicai Dong",
      "Zhao Yilin",
      "Aixin Sun",
      "Jung-Jae Kim",
      "Xiaoli Li"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.211": {
    "title": "Event Extraction from Historical Texts: A New Dataset for Black Rebellions",
    "volume": "findings",
    "abstract": "Understanding historical events is necessary for the study of contemporary society, culture, and politics. In this work, we focus on the event extraction task (EE) to detect event trigger words and their arguments in a novel domain of historical texts. In particular, we introduce a new EE dataset for a corpus of nineteenth-century African American newspapers. Our goal is to study the discourse of slave and non-slave African diaspora rebellions published in the periodical press in this period. Our dataset features 5 entity types, 12 event types, and 6 argument roles that concern slavery and black movements between the eighteenth and nineteenth centuries. Historical newspapers present many challenges for existing EE systems, including the evolution of meanings of words and the extensive use of religious discourse in newspapers from this era. Our experiments with current state-ofthe-art EE systems and BERT models demonstrate their poor performance over historical texts and call for more robust research efforts in this area",
    "checked": true,
    "id": "b05576978f3ff0eb5e84c71b24eb7283c91c5544",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Viet Dac Lai",
      "Minh Van Nguyen",
      "Heidi Kaufman",
      "Thien Huu Nguyen"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.212": {
    "title": "Zero-shot Medical Entity Retrieval without Annotation: Learning From Rich Knowledge Graph Semantics",
    "volume": "findings",
    "abstract": "Medical entity retrieval is an integral component for understanding and communicating information across various health systems. Current approaches tend to work well on specific medical domains but generalize poorly to unseen sub-specialties. This is of increasing concern under a public health crisis as new medical conditions and drug treatments come to light frequently. Zero-shot retrieval is challenging due to the high degree of ambiguity and variability in medical corpora, making it difficult to build an accurate similarity measure between mentions and concepts. Medical knowledge graphs (KG), however, contain rich semantics including large numbers of synonyms as well as its curated graphical structures. To take advantage of this valuable information, we propose a suite of learning tasks designed for training efficient zero-shot entity retrieval models. Without requiring any human annotation, our knowledge graph enriched architecture significantly outperforms common zero-shot benchmarks including BM25 and Clinical BERT with 7% to 30% higher recall across multiple major medical ontologies, such as UMLS, SNOMED and ICD-10",
    "checked": true,
    "id": "b223ddf2189174ac2c0837d316c9b5636b4291d8",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Luyang Kong",
      "Christopher Winestock",
      "Parminder Bhatia"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.213": {
    "title": "CONDA: a CONtextual Dual-Annotated dataset for in-game toxicity understanding and detection",
    "volume": "findings",
    "abstract": "Traditional toxicity detection models have focused on the single utterance level without deeper understanding of context. We introduce CONDA, a new dataset for in-game toxic language detection enabling joint intent classification and slot filling analysis, which is the core task of Natural Language Understanding (NLU). The dataset consists of 45K utterances from 12K conversations from the chat logs of 1.9K completed Dota 2 matches. We propose a robust dual semantic-level toxicity framework, which handles utterance and token-level patterns, and rich contextual chatting history. Accompanying the dataset is a thorough in-game toxicity analysis, which provides comprehensive understanding of context at utterance, token, and dual levels. Inspired by NLU, we also apply its metrics to the toxicity detection tasks for assessing toxicity and game-specific aspects. We evaluate strong NLU models on CONDA, providing fine-grained results for different intent classes and slot classes. Furthermore, we examine the coverage of toxicity nature in our dataset by comparing it with other toxicity datasets.1",
    "checked": true,
    "id": "5bc32a8f9a6245ae7f0761d539074dc7623ff61e",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Henry Weld",
      "Guanghao Huang",
      "Jean Lee",
      "Tongshu Zhang",
      "Kunze Wang",
      "Xinghong Guo",
      "Siqu Long",
      "Josiah Poon",
      "Caren Han"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.214": {
    "title": "Adaptive Knowledge-Enhanced Bayesian Meta-Learning for Few-shot Event Detection",
    "volume": "findings",
    "abstract": "Event detection (ED) aims at detecting event trigger words in sentences and classifying them into specific event types. In real-world applications, ED typically does not have sufficient labelled data, thus can be formulated as a few-shot learning problem. To tackle the issue of low sample diversity in few-shot ED, we propose a novel knowledge-based fewshot event detection method which uses a definition-based encoder to introduce external event knowledge as the knowledge prior of event types. Furthermore, as external knowledge typically provides limited and imperfect coverage of event types, we introduce an adaptive knowledge-enhanced Bayesian metalearning method to dynamically adjust the knowledge prior of event types. Experiments show our method consistently and substantially outperforms a number of baselines by at least 15 absolute F1 points under the same fewshot settings",
    "checked": true,
    "id": "fc51941a6221441692ce93d5c709fae367a64fd9",
    "semantic_title": "",
    "citation_count": 23,
    "authors": [
      "Shirong Shen",
      "Tongtong Wu",
      "Guilin Qi",
      "Yuan-Fang Li",
      "Gholamreza Haffari",
      "Sheng Bi"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.215": {
    "title": "Stylized Story Generation with Style-Guided Planning",
    "volume": "findings",
    "abstract": "Current storytelling systems focus more on generating stories with coherent plots regardless of the narration style, which is important for controllable text generation. Therefore, we propose a new task, stylized story generation, namely generating stories with specified style given a leading context. To tackle the problem, we propose a novel generation model that first plans the stylized keywords and then generates the whole story with the guidance of the keywords. Besides, we propose two automatic metrics to evaluate the consistency between the generated story and the specified style. Experiments demonstrates that our model can controllably generate emotion-driven or event-driven stories based on the ROCStories dataset (Mostafazadeh et al., 2016). Our study presents insights for stylized story generation in further research",
    "checked": true,
    "id": "1513639b0079694855e87006545d55edd57e5e81",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Xiangzhe Kong",
      "Jialiang Huang",
      "Ziquan Tung",
      "Jian Guan",
      "Minlie Huang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.216": {
    "title": "Dynamic Connected Networks for Chinese Spelling Check",
    "volume": "findings",
    "abstract": "Chinese spelling check (CSC) is a task to detect and correct spelling errors in Chinese text. Most state-of-the-art works on the CSC task adopt a BERT-based non-autoregressive language model, which relies on the output independence assumption. The inappropriate independence assumption prevents BERTbased models from learning the dependencies among target tokens, resulting in an incoherent problem. To address the above issue, we propose a novel architecture named Dynamic Connected Networks (DCN), which generates the candidate Chinese characters via a Pinyin Enhanced Candidate Generator and then utilizes an attention-based network to model the dependencies between two adjacent Chinese characters. The experimental results show that our proposed method achieves a new state-ofthe-art performance on three human-annotated datasets",
    "checked": true,
    "id": "aec8c31d3b8cf37b106fb52d07b74fe8bb23c863",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Baoxin Wang",
      "Wanxiang Che",
      "Dayong Wu",
      "Shijin Wang",
      "Guoping Hu",
      "Ting Liu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.217": {
    "title": "A Multi-Level Attention Model for Evidence-Based Fact Checking",
    "volume": "findings",
    "abstract": "Evidence-based fact checking aims to verify the truthfulness of a claim against evidence extracted from textual sources. Learning a representation that effectively captures relations between a claim and evidence can be challenging. Recent state-of-the-art approaches have developed increasingly sophisticated models based on graph structures. We present a simple model that can be trained on sequence structures. Our model enables inter-sentence attentions at different levels and can benefit from joint training. Results on a large-scale dataset for Fact Extraction and VERification (FEVER) show that our model outperforms the graphbased approaches and yields 1.09% and 1.42% improvements in label accuracy and FEVER score, respectively, over the best published model.1",
    "checked": true,
    "id": "806dc5c64d3a65f89e0f26ff9f51bb029c6908b2",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Canasai Kruengkrai",
      "Junichi Yamagishi",
      "Xin Wang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.218": {
    "title": "RealTranS: End-to-End Simultaneous Speech Translation with Convolutional Weighted-Shrinking Transformer",
    "volume": "findings",
    "abstract": "End-to-end simultaneous speech translation (SST), which directly translates speech in one language into text in another language in realtime, is useful in many scenarios but has not been fully investigated. In this work, we propose RealTranS, an end-to-end model for SST. To bridge the modality gap between speech and text, RealTranS gradually downsamples the input speech with interleaved convolution and unidirectional Transformer layers for acoustic modeling, and then maps speech features into text space with a weighted-shrinking operation and a semantic encoder. Besides, to improve the model performance in simultaneous scenarios, we propose a blank penalty to enhance the shrinking quality and a Wait-K-Stride-N strategy to allow local reranking during decoding. Experiments on public and widely-used datasets show that RealTranS with the Wait-KStride-N strategy outperforms prior end-to-end models as well as cascaded models in diverse latency settings",
    "checked": true,
    "id": "69cb92f055d32de55327da24ac38e12db810f280",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Xingshan Zeng",
      "Liangyou Li",
      "Qun Liu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.219": {
    "title": "Training ELECTRA Augmented with Multi-word Selection",
    "volume": "findings",
    "abstract": "Pre-trained text encoders such as BERT and its variants have recently achieved state-of-the-art performances on many NLP tasks. While being effective, these pre-training methods typically demand massive computation resources. To accelerate pre-training, ELECTRA trains a discriminator that predicts whether each input token is replaced by a generator. However, this new task, as a binary classification, is less semantically informative. In this study, we present a new text encoder pre-training method that improves ELECTRA based on multi-task learning. Specifically, we train the discriminator to simultaneously detect replaced tokens and select original tokens from candidate sets. We further develop two techniques to effectively combine all pre-training tasks: (1) using attention-based networks for task-specific heads, and (2) sharing bottom layers of the generator and the discriminator. Extensive experiments on GLUE and SQuAD datasets demonstrate both the effectiveness and the efficiency of our proposed method",
    "checked": true,
    "id": "27794bca3b7327aff29e2593e8b989b6a5af678b",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Jiaming Shen",
      "Jialu Liu",
      "Tianqi Liu",
      "Cong Yu",
      "Jiawei Han"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.220": {
    "title": "REAM♯: An Enhancement Approach to Reference-based Evaluation Metrics for Open-domain Dialog Generation",
    "volume": "findings",
    "abstract": "The lack of reliable automatic evaluation metrics is a major impediment to the development of open-domain dialogue systems. Various reference-based metrics have been proposed to calculate a score between a predicted response and a small set of references. However, these metrics show unsatisfactory correlations with human judgments. For a reference-based metric, its reliability mainly depends on two factors: its ability to measure the similarity between the predicted response and the reference response, as well as the reliability of the given reference set. Yet, there are few discussions on the latter. Our work attempts to fill this vacancy. We first clarify an assumption on reference-based metrics that, if more high-quality references are added into the reference set, the reliability of the metric will increase. Next, we present REAM$\\sharp$: an enhancement approach to Reference-based EvAluation Metrics for open-domain dialogue systems. A prediction model is designed to estimate the reliability of the given reference set. We show how its predicted results can be helpful to augment the reference set, and thus improve the reliability of the metric. Experiments validate both the effectiveness of our prediction model and that the reliability of reference-based metrics improves with the augmented reference sets",
    "checked": true,
    "id": "27c765b660aba877425934cde30d7b3593d68324",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Jun Gao",
      "Wei Bi",
      "Ruifeng Xu",
      "Shuming Shi"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.221": {
    "title": "Relation Extraction with Type-aware Map Memories of Word Dependencies",
    "volume": "findings",
    "abstract": "Relation extraction is an important task in information extraction and retrieval that aims to extract relations among the given entities from running texts. To achieve a good performance for this task, previous studies have shown that a good modeling of the contextual information is required, where the dependency tree of the input sentence can be a beneficial source among different types of contextual information. However, most of these studies focus on the dependency connections between words with limited attention paid to exploiting dependency types. In addition, they often treat different dependency connections equally in modeling so that suffer from the noise (inaccurate dependency parses) in the auto-generated dependency tree. In this paper, we propose a neural approach for relation extraction, with type-aware map memories (TaMM) for encoding dependency types obtained from an off-theshelf dependency parser for the input sentence. Specifically, for each word in an entity, TaMM maps all associated words along with the dependencies among them to memory slots and then assigns a weight to each slot according to its contribution to relation extraction. Our approach not only leverages dependency connections and types between words, but also distinguishes reliable dependency information from noisy ones and appropriately model them. The effectiveness of our approach is demonstrated by the experiments on two English benchmark datasets, where our approach achieves state-ofthe-art performance on both datasets.1",
    "checked": true,
    "id": "cec94332ff4618b28d0adb6ecddd2ef7a87d3188",
    "semantic_title": "",
    "citation_count": 19,
    "authors": [
      "Guimin Chen",
      "Yuanhe Tian",
      "Yan Song",
      "Xiang Wan"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.222": {
    "title": "PLATO-2: Towards Building an Open-Domain Chatbot via Curriculum Learning",
    "volume": "findings",
    "abstract": "To build a high-quality open-domain chatbot, we introduce the effective training process of PLATO-2 via curriculum learning. There are two stages involved in the learning process. In the first stage, a coarse-grained generation model is trained to learn response generation under the simplified framework of one-to-one mapping. In the second stage, a fine-grained generation model and an evaluation model are further trained to learn diverse response generation and response coherence estimation, respectively. PLATO-2 was trained on both Chinese and English data, whose effectiveness and superiority are verified through comprehensive evaluations, achieving new state-of-the-art results",
    "checked": true,
    "id": "70af4173983eccc0beac29ed4602bf9db5568b92",
    "semantic_title": "",
    "citation_count": 96,
    "authors": [
      "Siqi Bao",
      "Huang He",
      "Fan Wang",
      "Hua Wu",
      "Haifeng Wang",
      "Wenquan Wu",
      "Zhen Guo",
      "Zhibin Liu",
      "Xinchao Xu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.223": {
    "title": "JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs",
    "volume": "findings",
    "abstract": "Existing pre-trained models for knowledgegraph-to-text (KG-to-text) generation simply fine-tune text-to-text pre-trained models such as BART or T5 on KG-to-text datasets, which largely ignore the graph structure during encoding and lack elaborate pre-training tasks to explicitly model graph-text alignments. To tackle these problems, we propose a graph-text joint representation learning model called JointGT. During encoding, we devise a structure-aware semantic aggregation module which is plugged into each Transformer layer to preserve the graph structure. Furthermore, we propose three new pre-training tasks to explicitly enhance the graph-text alignment including respective text / graph reconstruction, and graph-text alignment in the embedding space via Optimal Transport. Experiments show that JointGT obtains new stateof-the-art performance on various KG-to-text datasets1",
    "checked": true,
    "id": "b99c61f6957c1b04ec1376b74f82dd1e83559695",
    "semantic_title": "",
    "citation_count": 29,
    "authors": [
      "Pei Ke",
      "Haozhe Ji",
      "Yu Ran",
      "Xin Cui",
      "Liwei Wang",
      "Linfeng Song",
      "Xiaoyan Zhu",
      "Minlie Huang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.224": {
    "title": "AdaST: Dynamically Adapting Encoder States in the Decoder for End-to-End Speech-to-Text Translation",
    "volume": "findings",
    "abstract": "In end-to-end speech translation, acoustic representations learned by the encoder are usually fixed and static, from the perspective of the decoder, which is not desirable for dealing with the cross-modal and cross-lingual challenge in speech translation. In this paper, we show the benefits of varying acoustic states according to decoder hidden states and propose an adaptive speech-to-text translation model that is able to dynamically adapt acoustic states in the decoder. We concatenate the acoustic state and target word embedding sequence and feed the concatenated sequence into subsequent blocks in the decoder. In order to model the deep interaction between acoustic states and target hidden states, a speech-text mixed attention sublayer is introduced to replace the conventional cross-attention network. Experiment results on two widely-used datasets show that the proposed method significantly outperforms stateof-the-art neural speech translation models",
    "checked": true,
    "id": "2e9990d63103fce47e2017c74201daf3d7b59073",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Wuwei Huang",
      "Dexin Wang",
      "Deyi Xiong"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.225": {
    "title": "OKGIT: Open Knowledge Graph Link Prediction with Implicit Types",
    "volume": "findings",
    "abstract": "Open Knowledge Graphs (OpenKG) refer to a set of (head noun phrase, relation phrase, tail noun phrase) triples such as (tesla, return to, new york) extracted from a corpus using OpenIE tools. While OpenKGs are easy to bootstrap for a domain, they are very sparse and far from being directly usable in an end task. Therefore, the task of predicting new facts, i.e., link prediction, becomes an important step while using these graphs in downstream tasks such as text comprehension, question answering, and web search query recommendation. Learning embeddings for OpenKGs is one approach for link prediction that has received some attention lately. However, on careful examination, we found that current OpenKG link prediction algorithms often predict noun phrases (NPs) with incompatible types for given noun and relation phrases. We address this problem in this work and propose OKGIT that improves OpenKG link prediction using novel type compatibility score and type regularization. With extensive experiments on multiple datasets, we show that the proposed method achieves state-of-the-art performance while producing type compatible NPs in the link prediction task",
    "checked": true,
    "id": "865bfd8034785a084f0c7d651119b8504e939535",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      ". Chandrahas",
      "Partha Talukdar"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.226": {
    "title": "Multimodal Fusion with Co-Attention Networks for Fake News Detection",
    "volume": "findings",
    "abstract": "Fake news with textual and visual contents has a better story-telling ability than text-only contents, and can be spread quickly with social media. People can be easily deceived by such fake news, and traditional expert identification is labor-intensive. Therefore, automatic detection of multimodal fake news has become a new hot-spot issue. A shortcoming of existing approaches is their inability to fuse multimodality features effectively. They simply concatenate unimodal features without considering inter-modality relations. Inspired by the way people read news with image and text, we propose a novel Multimodal Co-Attention Networks (MCAN) to better fuse textual and visual features for fake news detection. Extensive experiments conducted on two realworld datasets demonstrate that MCAN can learn inter-dependencies among multimodal features and outperforms state-of-the-art methods",
    "checked": true,
    "id": "a852e1d3adae5179ca75abe1b93b4dcb410cc082",
    "semantic_title": "",
    "citation_count": 36,
    "authors": [
      "Yang Wu",
      "Pengwei Zhan",
      "Yunjian Zhang",
      "Liming Wang",
      "Zhen Xu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.227": {
    "title": "Joint Multi-Decoder Framework with Hierarchical Pointer Network for Frame Semantic Parsing",
    "volume": "findings",
    "abstract": "Current researches on frame semantic parsing include three subtasks, namely frame identification, argument identification and role classification. Most of previous systems process these subtasks independently and ignore their interactions. We introduce a novel architecture based on multi-decoder strategy to handle these subtasks together. The multi-decoder strategy strengthens the interactions. Moreover, we design a hierarchical pointer network for argument identification which reduces the computational complexity. To our best knowledge, it's the first practice to introduce the pointer network into frame semantic parsing. The experiments show improvement over state of the art models on FrameNet dataset",
    "checked": true,
    "id": "4db26cbd36396eee1ac8725bc2d3bfcd84382f7e",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Xudong Chen",
      "Ce Zheng",
      "Baobao Chang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.228": {
    "title": "H-FND: Hierarchical False-Negative Denoising for Distant Supervision Relation Extraction",
    "volume": "findings",
    "abstract": "Although distant supervision automatically generates training data for relation extraction, it also introduces false-positive (FP) and false-negative (FN) training instances to the generated datasets. Whereas both types of errors degrade the final model performance, previous work on distant supervision denoising focuses more on suppressing FP noise and less on resolving the FN problem. We here propose H-FND, a hierarchical false-negative denoising framework for robust distant supervision relation extraction, as an FN denoising solution. H-FND uses a hierarchical policy which first determines whether non-relation (NA) instances should be kept, discarded, or revised during the training process. For those learning instances which are to be revised, the policy further reassigns them appropriate relations, making them better training inputs. Experiments on SemEval-2010 and TACRED were conducted with controlled FN ratios that randomly turn the relations of training and validation instances into negatives to generate FN instances. In this setting, H-FND can revise FN instances correctly and maintains high F1 scores even when 50% of the instances have been turned into negatives. Experiment on NYT10 is further conducted to shows that H-FND is applicable in a realistic setting",
    "checked": true,
    "id": "ed6b28d509757e2c63b77830b1b8c315490868a2",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Jhih-wei Chen",
      "Tsu-Jui Fu",
      "Chen-Kang Lee",
      "Wei-Yun Ma"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.229": {
    "title": "GEM: A General Evaluation Benchmark for Multimodal Tasks",
    "volume": "findings",
    "abstract": "In this paper, we present GEM1 as a General Evaluation benchmark for Multimodal tasks. Different from existing datasets such as GLUE (Wang et al., 2018), SuperGLUE (Wang et al., 2019), XGLUE (Liang et al., 2020) and XTREME (Hu et al., 2020) that mainly focus on natural language tasks, GEM is a largescale vision-language benchmark, which consists of GEM-I for image-language tasks and GEM-V for video-language tasks. Comparing with existing multimodal datasets such as MSCOCO (Chen et al., 2015) and Flicker30K (Vinyals et al., 2015) for image-language tasks, YouCook2 (Zhou et al., 2018) and MSR-VTT (Xu et al., 2016) for video-language tasks, GEM is not only the largest vision-language dataset covering image-language tasks and video-language tasks at the same time, but also labeled in multiple languages. We also provide two baseline models for this benchmark. We will release the dataset, code and baseline models, aiming to advance the development of multilingual multimodal research",
    "checked": true,
    "id": "93316866fedaae835d5fe7f633f887e3571fb8b9",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Lin Su",
      "Nan Duan",
      "Edward Cui",
      "Lei Ji",
      "Chenfei Wu",
      "Huaishao Luo",
      "Yongfei Liu",
      "Ming Zhong",
      "Taroon Bharti",
      "Arun Sacheti"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.230": {
    "title": "Graph Relational Topic Model with Higher-order Graph Attention Auto-encoders",
    "volume": "findings",
    "abstract": "Learning low-dimensional representations of networked documents is a crucial task for documents linked in network structures. Relational Topic Models (RTMs) have shown their strengths in modeling both document contents and relations to discover the latent topic semantic representations. However, higher-order correlation structure information among documents is largely ignored in these methods. Therefore, we propose a novel graph relational topic model (GRTM) for document network, to fully explore and mix neighborhood information of documents on each order, based on the Higher-order Graph Attention Network (HGAT) with the log-normal prior in the graph attention. The proposed method can address the aforementioned issue via the information propagation among document-document based on the HGAT probabilistic encoder, to learn efficient networked document representations in the latent topic space, which can fully reflect document contents, along with document connections. Experiments on several real-world document network datasets show that, through fully exploring information in documents and document networks, our model achieves better performance on unsupervised representation learning and outperforms existing competitive methods in various downstream tasks",
    "checked": true,
    "id": "dc6b7903cfbfd7e53c1bc2a6df9a9b42d7cbc558",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Qianqian Xie",
      "Jimin Huang",
      "Pan Du",
      "Min Peng"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.231": {
    "title": "Paths to Relation Extraction through Semantic Structure",
    "volume": "findings",
    "abstract": "Syntactic and semantic structure directly reflect relations expressed by the text at hand and are thus very useful for the relation extraction (RE) task. Their symbolic nature allows increased interpretability for end-users and developers, which is particularly appealing in RE. Although they have been somewhat overshadowed recently by the use of end-to-end neural network models and contextualized word embeddings, we show that they may be leveraged as input for neural networks to positive effect. We present two methods for integrating broad-coverage semantic structure (specifically, UCCA) into supervised neural RE models, demonstrating benefits over the use of exclusively syntactic integrations. The first method involves reduction of UCCA into a bilexical structure, while the second leverages a novel technique for encoding semantic DAG structures. Our approach is general and can be used for integrating a wide range of graphbased semantic structures.1",
    "checked": true,
    "id": "d8ba4028a807db2b5c5e11f91ab9084bb5feb9ba",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Yellin",
      "Omri Abend"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.232": {
    "title": "Dynamic and Multi-Channel Graph Convolutional Networks for Aspect-Based Sentiment Analysis",
    "volume": "findings",
    "abstract": null,
    "checked": true,
    "id": "5041a39488c7f69305171737b347460691713044",
    "semantic_title": "",
    "citation_count": 17,
    "authors": [
      "Shiguan Pang",
      "Yun Xue",
      "Zehao Yan",
      "Weihao Huang",
      "Jinhui Feng"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.233": {
    "title": "Automatic Text Simplification for Social Good: Progress and Challenges",
    "volume": "findings",
    "abstract": "Since the late 1990s, automatic text simplification (ATS) was promoted as a natural language processing (NLP) task with great potential to make texts more accessible to people with various reading or cognitive disabilities, and enable their better social inclusion. Large multidisciplinary projects showed promising steps in that direction. Since 2010, the field started attracting more attention but at the cost of major shifts in system architecture, target audience, and evaluation strategies. Somewhere along the way, the focus has shifted from ATS for social good towards building complex endto-end neural architectures that are not aimed at any particular target population. This study presents the trajectory of ATS for social good, the main issues in current ATS trends, and the ways forward that could bring the field back to its initial goals",
    "checked": true,
    "id": "dfa3638197c63066006412c1e03753241e0b68f0",
    "semantic_title": "",
    "citation_count": 22,
    "authors": [
      "Sanja Stajner"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.234": {
    "title": "A Neural Edge-Editing Approach for Document-Level Relation Graph Extraction",
    "volume": "findings",
    "abstract": "In this paper, we propose a novel edge-editing approach to extract relation information from a document. We treat the relations in a document as a relation graph among entities in this approach. The relation graph is iteratively constructed by editing edges of an initial graph, which might be a graph extracted by another system or an empty graph. The way to edit edges is to classify them in a close-first manner using the document and temporallyconstructed graph information; each edge is represented with a document context information by a pretrained transformer model and a graph context information by a graph convolutional neural network model. We evaluate our approach on the task to extract material synthesis procedures from materials science texts. The experimental results show the effectiveness of our approach in editing the graphs initialized by our in-house rule-based system and empty graphs.1",
    "checked": true,
    "id": "b1f3d7e03d681470d55a3e9f367f9cea46fe9528",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Kohei Makino",
      "Makoto Miwa",
      "Yutaka Sasaki"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.235": {
    "title": "Dialogue-oriented Pre-training",
    "volume": "findings",
    "abstract": "Pre-trained language models (PrLM) has been shown powerful in enhancing a broad range of downstream tasks including various dialogue related ones. However, PrLMs are usually trained on general plain text with common language model (LM) training objectives, which cannot sufficiently capture dialogue exclusive features due to the limitation of such training setting, so that there is an immediate need to fill the gap between a specific dialogue task and the LM task. As it is unlikely to collect huge dialogue data for dialogueoriented pre-training, in this paper, we propose three strategies to simulate the conversation features on general plain text. Our proposed method differs from existing posttraining methods that it may yield a generalpurpose PrLM and does not individualize to any detailed task while keeping the capability of learning dialogue related features including speaker awareness, continuity and consistency. The resulted Dialog-PrLM is fine-tuned on three public multi-turn dialogue datasets and helps achieve significant and consistent improvement over the plain PrLMs",
    "checked": true,
    "id": "9cc73eb0431b13af7d031c9eea14d8c626b59e59",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Yi Xu",
      "Hai Zhao"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.236": {
    "title": "GrantRel: Grant Information Extraction via Joint Entity and Relation Extraction",
    "volume": "findings",
    "abstract": "August 1–6, 2021. ©2021 Association for Computational Linguistics 2674 GrantRel: Grant Information Extraction via Joint Entity and Relation Extraction Junyi Bian 1 8 , Li Huang 1 8 , Xiaodi Huang 2 , Hong Zhou 3, Shanfeng Zhu 4 5 6 7 8 1 School of Computer Science, Fudan University, Shanghai 200433, China 2 School of Computing and Mathematics, Charles Sturt University Albury, NSW 2640, Australia 3 Atypon Systems, LLC, UK 4 Institute of Science and Technology for Brain-Inspired Intelligence, Fudan University, China 5 Key Laboratory of Computational Neuroscience and Brain-Inspired Intelligence (Fudan University), Ministry of Education, Shanghai 200433, China 6 MOE Frontiers Center for Brain Science, Fudan University, Shanghai 200433, China 7 Zhangjiang Fudan International Innovation Center, Shanghai 200433, China 8 Shanghai Key Lab of Intelligent Information Processing, Fudan University, Shanghai 200433, China {zhusf, 20110240003}@fudan.edu.cn, hzhou@atypon.com",
    "checked": true,
    "id": "24f2e02437013fde3477621acfec48c53391853e",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Junyi Bian",
      "Li Huang",
      "Xiaodi Huang",
      "Hong Zhou",
      "Shanfeng Zhu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.237": {
    "title": "Enhancing Language Generation with Effective Checkpoints of Pre-trained Language Model",
    "volume": "findings",
    "abstract": "This work empirically explores effective exploiting of intermediate output from pretrained language models (PrLMs) for language generation tasks. For this purpose, we propose an improved method to integrate public checkpoints of PrLMs for the most convenience and perform extensive experiments on 6 different kinds of PrLMs, including BERT, ELECTRA, GPT2, Multi-lingual BERT, and XLM RoBERTa. Evaluation with automatic metrics shows that our approach significantly improves the generation quality on the generation tasks, up to 1.8 BLEU points for neural machine translation (Korean-to-English, Koreanto-Chinese) and 1.8 ROUGE points improvements for text summarization",
    "checked": true,
    "id": "5eec9616f58ac64f8510bff2e08ef4299b6c8df9",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeonghyeok Park",
      "Hai Zhao"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.238": {
    "title": "Making Flexible Use of Subtasks: A Multiplex Interaction Network for Unified Aspect-based Sentiment Analysis",
    "volume": "findings",
    "abstract": "Aspect Term Extraction (ATE), Opinion Term Extraction (OTE) and Aspect Sentiment Classification (ASC) are the essential building blocks of Aspect-based Sentiment Analysis (ABSA). They are typically treated as separate tasks and are individually studied by previous work. Recent studies intend to incorporate multiple sub-tasks into a unified framework, but suffer from the following major disadvantages: (1) ABSA models are extremely fragile when some sub-tasks are absent; (2) the interactive relations among subtasks are not adequate. To this end, we propose a multi-task learning approach named MIN (Multiplex Interaction Network) to make flexible use of sub-tasks for a unified ABSA. We divide the sub-tasks of ABSA into extractive sub-tasks and classification sub-tasks, and optimize these sub-tasks in a unified manner with multiplex interaction mechanisms. Specifically, we devise a pairwise attention to exploit bidirectional interactions between any arbitrary pair of extractive sub-tasks and a consistency-weighting to perform unidirectional interaction from an extractive sub-task to a classification sub-task. Since the proposed interaction mechanisms are task-agnostic, our model can also work well when some specific sub-tasks are absent. Extensive experiments on two widely used benchmarks with different numbers of sub-tasks demonstrate the superiority of the proposed model",
    "checked": true,
    "id": "fc1392ab293d3371e0166a4c42157a5e1431f1ae",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Guoxin Yu",
      "Xiang Ao",
      "Ling Luo",
      "Min Yang",
      "Xiaofei Sun",
      "Jiwei Li",
      "Qing He"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.239": {
    "title": "Continual Mixed-Language Pre-Training for Extremely Low-Resource Neural Machine Translation",
    "volume": "findings",
    "abstract": "The data scarcity in low-resource languages has become a bottleneck to building robust neural machine translation systems. Finetuning a multilingual pre-trained model (e.g., mBART (Liu et al., 2020a)) on the translation task is a good approach for low-resource languages; however, its performance will be greatly limited when there are unseen languages in the translation pairs. In this paper, we present a continual pre-training (CPT) framework on mBART to effectively adapt it to unseen languages. We first construct noisy mixed-language text from the monolingual corpus of the target language in the translation pair to cover both the source and target languages, and then, we continue pretraining mBART to reconstruct the original monolingual text. Results show that our method can consistently improve the finetuning performance upon the mBART baseline, as well as other strong baselines, across all tested low-resource translation pairs containing unseen languages. Furthermore, our approach also boosts the performance on translation pairs where both languages are seen in the original mBART's pre-training. The code is available at https://github.com/ zliucr/cpt-nmt",
    "checked": true,
    "id": "0c6b06a0498ad8156af499a3b9b2b612951b3504",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Zihan Liu",
      "Genta Indra Winata",
      "Pascale Fung"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.240": {
    "title": "Transformer-Exclusive Cross-Modal Representation for Vision and Language",
    "volume": "findings",
    "abstract": "Ever since the advent of deep learning, crossmodal representation learning has been dominated by the approaches involving convolutional neural networks for visual representation and recurrent neural networks for language representation. Transformer architecture, however, has rapidly taken over the recurrent neural networks in natural language processing tasks, and it has also been shown that vision tasks can be handled with transformer architecture, with compatible performance to convolutional neural networks. Such results naturally lead to speculation upon the possibility of tackling cross-modal representation for vision and language exclusively with transformer. This paper examines transformerexclusive cross-modal representation to explore such possibility, demonstrating its potentials as well as discussing its current limitations and its prospects",
    "checked": true,
    "id": "19bd1dd8e810051a51e24d5befe501719bb00fca",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew Shin",
      "Takuya Narihira"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.241": {
    "title": "Two Parents, One Child: Dual Transfer for Low-Resource Neural Machine Translation",
    "volume": "findings",
    "abstract": "Neural machine translation suffers when parallel data for training is scarce. Previous works have explored transfer learning to assist training in low-resource scenarios. However, they transfer either from high-resource parallel data, or from monolingual data. In this work, we propose a framework to transfer multiple sources of auxiliary data, including both high-resource parallel data and monolingual data of involved languages. Knowledge in those sources is respectively encoded in a high-resource translation model and pretrained language models, and dually transferred to the low-resource translation model by our approach. Extensive experiments show that our approach yields consistent improvements over strong competitors for multiple translation directions. Furthermore, our approach still exhibits benefit on top of back-translation, making it a useful addition to practitioners' toolbox",
    "checked": true,
    "id": "3584fcc11db19432812ce6c6704cf5b089c5343f",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Meng Zhang",
      "Liangyou Li",
      "Qun Liu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.242": {
    "title": "Contrastive Aligned Joint Learning for Multilingual Summarization",
    "volume": "findings",
    "abstract": "Multilingual text summarization requires the ability to understand documents in multiple languages and generate summaries in the corresponding language, which poses more challenges on current summarization systems. However, this problem has been rarely studied due to the lack of large-scale supervised summarization data in multiple languages. In this paper, we first provide a large-scale multilingual summarization corpus MLGSum consisting of 1.1 million articles and summaries in 12 different languages. Based on it, we develop a unified summarization model to understand the document and generate summaries in different languages. We use the contrastive learning strategy to train our multilingual summarization system (CALMS), which consists of two training objectives, contrastive sentence ranking (CSR) and sentence aligned substitution (SAS). The two training objectives are designed to share salient information extractive ability and align sentencelevel representation across different languages. Experimental results indicate that CALMS achieves significant improvement over monolingual models in all languages. We further transfer CALMS to other languages and find that it will also benefit similar languages. Our code and dataset are available at https://github.com/brxx122/CALMS",
    "checked": true,
    "id": "30a0717872ed6951db28c11ae270099accfc29d0",
    "semantic_title": "",
    "citation_count": 19,
    "authors": [
      "Danqing Wang",
      "Jiaze Chen",
      "Hao Zhou",
      "Xipeng Qiu",
      "Lei Li"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.243": {
    "title": "When Time Makes Sense: A Historically-Aware Approach to Targeted Sense Disambiguation",
    "volume": "findings",
    "abstract": "As languages evolve historically, making computational approaches sensitive to time can improve performance on specific tasks. In this work, we assess whether applying historical language models and time-aware methods help with determining the correct sense of polysemous words. We outline the task of time-sensitive Targeted Sense Disambiguation (TSD), which aims to detect instances of a sense or set of related senses in historical and time-stamped texts, and address two main goals: 1) we scrutinize the effect of applying historical language models on the performance of several TSD methods and 2) we assess different disambiguation methods that take into account the year in which a text was produced. We train historical BERT models on a corpus of nineteenth-century English books and draw on the Oxford English Dictionary (and its Historical Thesaurus) to create historically evolving sense representations. Our results show that using historical language models consistently improves performance whereas timesensitive disambiguation helps especially with older documents. ∗ Contributions of each author (in alphabetical order): Conceptualization: KB, BMcG, FN; Data curation: KB, GT; Formal Analysis: MCA; Funding acquisition: BMcG; Methodology: KB, MCA, KH, BMcG, FN; Project management: KB, BMcG, FN; Software: KB, MCA, KH, FN; Supervision: BMcG; Reproducibility: KB, MCA, FN; Writing: KB, MCA, BMcG, FN",
    "checked": true,
    "id": "5909f29aa91ec1d73e8daadada179fe4377cea0d",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Kaspar Beelen",
      "Federico Nanni",
      "Mariona Coll Ardanuy",
      "Kasra Hosseini",
      "Giorgia Tolfo",
      "Barbara McGillivray"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.244": {
    "title": "Understanding Feature Focus in Multitask Settings for Lexico-semantic Relation Identification",
    "volume": "findings",
    "abstract": "Discovering whether words are semantically related and identifying the specific semantic relation that holds between them is of crucial importance for automatic reasoning on text data. For that purpose, different methodologies have been proposed that either (1) tackle feature engineering, (2) fine-tune latent semantic spaces, or (3) take advantage of cognitive links between semantic relations in multitask settings. In this paper, we investigate how feature engineering and multitask architectures can be improved and consequently combined to identify lexico-semantic relations. Evaluation results over a set of gold-standard datasets show that (1) combinations of similar features are beneficial (feature sets), (2) asymmetric distributional features are a strong cue to discriminate asymmetric relations as well as they play an important role in multitask architectures, (3) shared-private models improve over binary and fully-shared classifiers as well as they correctly balance the focus on features between private and shared layers1",
    "checked": true,
    "id": "350c7a1877b90d825b84131985cdf44b7bbc59d7",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Houssam Akhmouch",
      "Gaël Dias",
      "Jose G. Moreno"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.245": {
    "title": "Don't Miss the Labels: Label-semantic Augmented Meta-Learner for Few-Shot Text Classification",
    "volume": "findings",
    "abstract": "Increasing studies leverage pre-trained language models and meta-learning frameworks to solve few-shot text classification problems. Most of the current studies focus on building a meta-learner from the information of input texts but ignore abundant semantic information beneath class labels. In this work, we show that class-label information can be utilized for extracting more discriminative feature representation of the input text from a pretrained language model like BERT, and can achieve a performance boost when the samples are scarce. Building on top of this discovery, we propose a framework called Labelsemantic augmented meta-learner (LaSAML) to make full use of label semantics. We systematically investigate various factors in this framework and show that it can be plugged into the existing few-shot text classification system. Through extensive experiments, we demonstrate that the few-shot text classification system upgraded by LaSAML can lead to significant performance improvement over its original counterparts",
    "checked": true,
    "id": "5be31bd3a8c4905b7ee2ffc7c8f477ba6365b5de",
    "semantic_title": "",
    "citation_count": 26,
    "authors": [
      "Qiaoyang Luo",
      "Lingqiao Liu",
      "Yuhao Lin",
      "Wei Zhang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.246": {
    "title": "Detecting Harmful Memes and Their Targets",
    "volume": "findings",
    "abstract": "Among the various modes of communication in social media, the use of Internet memes has emerged as a powerful means to convey political, psychological, and socio-cultural opinions. Although memes are typically humorous in nature, recent days have witnessed a proliferation of harmful memes targeted to abuse various social entities. As most harmful memes are highly satirical and abstruse without appropriate contexts, off-the-shelf multimodal models may not be adequate to understand their underlying semantics. In this work, we propose two novel problem formulations: detecting harmful memes and the social entities that these harmful memes target. To this end, we present HarMeme, the first benchmark dataset, containing 3,544 memes related to COVID-19. Each meme went through a rigorous two-stage annotation process. In the first stage, we labeled a meme as very harmful, partially harmful, or harmless;in the second stage, we further annotated the type of target(s) that each harmful meme points to: individual, organization, community, or society/general public/other. The evaluation results using ten unimodal and multimodal models highlight the importance of using multimodal signals for both tasks. We further discuss the limitations of these models and we argue that more research is needed to address these problems",
    "checked": true,
    "id": "24992c4b3cfd6f3dc0d4428846b36082ee781d96",
    "semantic_title": "",
    "citation_count": 36,
    "authors": [
      "Shraman Pramanick",
      "Dimitar Dimitrov",
      "Rituparna Mukherjee",
      "Shivam Sharma",
      "Md. Shad Akhtar",
      "Preslav Nakov",
      "Tanmoy Chakraborty"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.247": {
    "title": "Progressive Multi-Granularity Training for Non-Autoregressive Translation",
    "volume": "findings",
    "abstract": "Non-autoregressive translation (NAT) significantly accelerates the inference process via predicting the entire target sequence. However, recent studies show that NAT is weak at learning high-mode of knowledge such as one-to-many translations. We argue that modes can be divided into various granularities which can be learned from easy to hard. In this study, we empirically show that NAT models are prone to learn fine-grained lower-mode knowledge, such as words and phrases, compared with sentences. Based on this observation, we propose progressive multigranularity training for NAT. More specifically, to make the most of the training data, we break down the sentence-level examples into three types, i.e. words, phrases, sentences, and with the training goes, we progressively increase the granularities. Experiments on Romanian-English, English-German, Chinese-English and Japanese-English demonstrate that our approach improves the phrase translation accuracy and model reordering ability, therefore resulting in better translation quality against strong NAT baselines. Also, we show that more deterministic fine-grained knowledge can further enhance performance",
    "checked": true,
    "id": "9bc1b2f8ab82921e8f226c732f51d0ce1bd48822",
    "semantic_title": "",
    "citation_count": 21,
    "authors": [
      "Liang Ding",
      "Longyue Wang",
      "Xuebo Liu",
      "Derek F. Wong",
      "Dacheng Tao",
      "Zhaopeng Tu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.248": {
    "title": "ZmBART: An Unsupervised Cross-lingual Transfer Framework for Language Generation",
    "volume": "findings",
    "abstract": "Despite the recent advancement in NLP research, cross-lingual transfer for natural language generation is relatively understudied. In this work, we transfer supervision from high resource language (HRL) to multiple lowresource languages (LRLs) for natural language generation (NLG). We consider four NLG tasks (text summarization, question generation, news headline generation, and distractor generation) and three syntactically diverse languages, i.e., English, Hindi, and Japanese. We propose an unsupervised crosslingual language generation framework (called ZmBART) that does not use any parallel or pseudo-parallel/back-translated data. In this framework, we further pre-train mBART sequence-to-sequence denoising auto-encoder model with an auxiliary task using monolingual data of three languages. The objective function of the auxiliary task is close to the target tasks which enriches the multi-lingual latent representation of mBART and provides good initialization for target tasks. Then, this model is fine-tuned with task-specific supervised English data and directly evaluated with low-resource languages in the Zero-shot setting. To overcome catastrophic forgetting and spurious correlation issues, we applied freezing model component and data argumentation approaches respectively. This simple modeling approach gave us promising results. We experimented with few-shot training (with 1000 supervised data-points) which boosted the model performance further. We performed several ablations and cross-lingual transferability analysis to demonstrate the robustness of ZmBART",
    "checked": true,
    "id": "38d3657ee15f2612330eb5e036bbc38d9137f75a",
    "semantic_title": "",
    "citation_count": 17,
    "authors": [
      "Kaushal Kumar Maurya",
      "Maunendra Sankar Desarkar",
      "Yoshinobu Kano",
      "Kumari Deepshikha"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.249": {
    "title": "HacRED: A Large-Scale Relation Extraction Dataset Toward Hard Cases in Practical Applications",
    "volume": "findings",
    "abstract": "Relation extraction (RE) is an essential topic in natural language processing and has attracted extensive attention. Current RE approaches achieve fantastic results on common datasets, while they still struggle on practical applications. In this paper, we analyze the above performance gap, the underlying reason of which is that practical applications intrinsically have more hard cases. To make RE models more robust on such practical hard cases, we propose a case-oriented construction framework to build a Hard Case Relation Extraction Dataset (HacRED). The proposed HacRED consists of 65,225 relational facts annotated from 9,231 documents with sufficient and diverse hard cases. Notably, HacRED is one of the largest Chinese document-level RE datasets and achieves a high 96% F1 score on data quality. Furthermore, we apply the stateof-the-art RE models on this dataset and conduct a thorough evaluation. The results show that the performance of these models is far lower than humans, and RE applying on practical hard cases still requires further efforts. HacRED is publicly available at https://github. com/qiaojiim/HacRED",
    "checked": true,
    "id": "31fb3586cab89c1314bb9863d00222aeaa77d7d3",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Qiao Cheng",
      "Juntao Liu",
      "Xiaoye Qu",
      "Jin Zhao",
      "Jiaqing Liang",
      "Zhefeng Wang",
      "Baoxing Huai",
      "Nicholas Jing Yuan",
      "Yanghua Xiao"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.250": {
    "title": "Do Multilingual Neural Machine Translation Models Contain Language Pair Specific Attention Heads?",
    "volume": "findings",
    "abstract": "Recent studies on the analysis of the multilingual representations focus on identifying whether there is an emergence of languageindependent representations, or whether a multilingual model partitions its weights among different languages. While most of such work has been conducted in a \"black-box\" manner, this paper aims to analyze individual components of a multilingual neural translation (NMT) model. In particular, we look at the encoder self-attention and encoder-decoder attention heads (in a many-to-one NMT model) that are more specific to the translation of a certain language pair than others by (1) employing metrics that quantify some aspects of the attention weights such as \"variance\" or \"confidence\", and (2) systematically ranking the importance of attention heads with respect to translation quality. Experimental results show that surprisingly, the set of most important attention heads are very similar across the language pairs and that it is possible to remove nearly one-third of the less important heads without hurting the translation quality greatly",
    "checked": true,
    "id": "48558617dc71742c847d20be2ff0546df50e15ce",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Zae Myung Kim",
      "Laurent Besacier",
      "Vassilina Nikoulina",
      "Didier Schwab"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.251": {
    "title": "Learning Sequential and Structural Information for Source Code Summarization",
    "volume": "findings",
    "abstract": "We propose a model that learns both the sequential and the structural features of code for source code summarization. We adopt the abstract syntax tree (AST) and graph convolution to model the structural information and the Transformer to model the sequential information. We convert code snippets into ASTs and apply graph convolution to obtain structurally-encoded node representations. Then, the sequences of the graphconvolutioned AST nodes are processed by the Transformer layers. Since structurallyneighboring nodes will have similar representations in graph-convolutioned trees, the Transformer layers can effectively capture not only the sequential information but also the structural information such as sentences or blocks of source code. We show that our model outperforms the state-of-the-art for source code summarization by experiments and human evaluations",
    "checked": true,
    "id": "c58ad64a5774023884f5aef01106a419a8e40463",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "YunSeok Choi",
      "JinYeong Bak",
      "CheolWon Na",
      "Jee-Hyong Lee"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.252": {
    "title": "Energy-based Unknown Intent Detection with Data Manipulation",
    "volume": "findings",
    "abstract": "Unknown intent detection aims to identify the out-of-distribution (OOD) utterance whose intent has never appeared in the training set. In this paper, we propose using energy scores for this task as the energy score is theoretically aligned with the density of the input and can be derived from any classifier. However, highquality OOD utterances are required during the training stage in order to shape the energy gap between OOD and in-distribution (IND), and these utterances are difficult to collect in practice. To tackle this problem, we propose a data manipulation framework to Generate high-quality OOD utterances with importance weighTs (GOT). Experimental results show that the energy-based detector fine-tuned by GOT can achieve state-of-the-art results on two benchmark datasets",
    "checked": true,
    "id": "b089b54e77739993c804b46fd0310e353ebb4f1d",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Yawen Ouyang",
      "Jiasheng Ye",
      "Yu Chen",
      "Xinyu Dai",
      "Shujian Huang",
      "Jiajun Chen"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.253": {
    "title": "Automatic Rephrasing of Transcripts-based Action Items",
    "volume": "findings",
    "abstract": "The automated transcription of spoken language, and meetings, in particular, is becoming more widespread as automatic speech recognition systems are becoming more accurate. This trend has significantly accelerated since the outbreak of the COVID-19 pandemic, which led to a major increase in the number of online meetings. However, the transcription of spoken language has not received much attention from the NLP community compared to documents and other forms of written language. In this paper, we study a variation of the summarization problem over the transcription of spoken language: given a transcribed meeting, and an action item (i.e., a commitment or request to perform a task), our goal is to generate a coherent and self-contained rephrasing of the action item. To this end, we compiled a novel dataset of annotated meeting transcripts, including human rephrasing of action items. We use state-of-the-art supervised text generation techniques and establish a strong baseline based on BART and UniLM (two pretrained transformer models). Due to the nature of natural speech, language is often broken and incomplete and the task is shown to be harder than an analogous task over email data. Particularly, we show that the baseline models can be greatly improved once models are provided with additional information. We compare two approaches: one incorporating features extracted by coreference-resolution. Additional annotations are used to train an auxiliary model to detect the relevant context in the text. Based on the systematic human evaluation, our best models exhibit near-human-level rephrasing capability on a constrained subset of the problem. © 2021 Association for Computational Linguistics",
    "checked": true,
    "id": "9b840156b434bdb41c18f036644cdd86882f2254",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amir Cohen",
      "Amir Kantor",
      "Sagi Hilleli",
      "Eyal Kolman"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.254": {
    "title": "MergeDistill: Merging Language Models using Pre-trained Distillation",
    "volume": "findings",
    "abstract": null,
    "checked": true,
    "id": "7a6e5c8a566a9f6805dcb765594815cdc0de194b",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Simran Khanuja",
      "Melvin Johnson",
      "Partha Talukdar"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.255": {
    "title": "On Sparsifying Encoder Outputs in Sequence-to-Sequence Models",
    "volume": "findings",
    "abstract": "Sequence-to-sequence models usually transfer all encoder outputs to the decoder for generation. In this work, by contrast, we hypothesize that these encoder outputs can be compressed to shorten the sequence delivered for decoding. We take Transformer as the testbed and introduce a layer of stochastic gates in-between the encoder and the decoder. The gates are regularized using the expected value of the sparsity-inducing L0penalty, resulting in completely masking-out a subset of encoder outputs. In other words, via joint training, the L0DROP layer forces Transformer to route information through a subset of its encoder states. We investigate the effects of this sparsification on two machine translation and two summarization tasks. Experiments show that, depending on the task, around 40-70% of source encodings can be pruned without significantly compromising quality. The decrease of the output length endows L0DROP with the potential of improving decoding efficiency, where it yields a speedup of up to 1.65x on document summarization tasks against the standard Transformer. We analyze the L0DROP behaviour and observe that it exhibits systematic preferences for pruning certain word types, e.g., function words and punctuation get pruned most. Inspired by these observations, we explore the feasibility of specifying rule-based patterns that mask out encoder outputs based on information such as part-of-speech tags, word frequency and word position",
    "checked": true,
    "id": "c2f36f14419565a0fed3032b7f1d1811daf6702e",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Biao Zhang",
      "Ivan Titov",
      "Rico Sennrich"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.256": {
    "title": "FrameNet-assisted Noun Compound Interpretation",
    "volume": "findings",
    "abstract": "Given a noun compound (NC), we address the problem of predicting the appropriate semantic label linking the constituents of the NC. This problem is called Noun Compound Interpretation (NCI). We use FrameNet as a semantic label repository. For example, given the noun compound (board approval), we predict the frame (DENY OR GRANT PERMISSION, as per FrameNet) as appropriate and the semantic role of the modifier word (AUTHORITY) as the semantic label linking board and approval; the resulting label is DENY OR GRANT PERMISSION:AUTHORITY. Our semantic label repository is very large (≈ 11k labels) compared to the NC data available for training (approx 1900). Thus, learning in this case, especially for unseen semantic labels, is hard. We propose to solve this problem by predicting semantic labels in a continuous label embedding space, which is novel. This embedding space is created by learning label embeddings using the FrameNet data. The embeddings are then used to train two separate models – one for predicting Frames and the other for FEs. As the label embedding space captures the semantics of the labels, using these embeddings enables generalizing well on unseen labels, thus achieving zero-shot learning. Our preliminary investigations show that the proposed approach performs well for unseen labels, achieving 5% and 2% points improvements over baselines for the frame and FE prediction, respectively. The study shows the promise of the use of continuous space embeddings for noun compound interpretation and points to the need for further investigation",
    "checked": true,
    "id": "41c4495e3bb9ceaba4aa2e6e809d24d7ef6263d6",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Girishkumar Ponkiya",
      "Diptesh Kanojia",
      "Pushpak Bhattacharyya",
      "Girish Palshikar"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.257": {
    "title": "Hypernym Discovery via a Recurrent Mapping Model",
    "volume": "findings",
    "abstract": "Hypernym discovery aims to identify all possible hypernyms of a given term. The most recent hypernym discovery models exploit multiple mapping functions to project a term to different semantic spaces and then aggregate these embeddings to a general representation for further classification. We refer to this model as a parallel style model. In this work, we observe that there are hierarchical relations between a target terms' hypernyms. However, these hierarchical relations were not sufficiently considered in the previous parallel style model. To leverage the hierarchical relations, we propose a sequential style model that recurrently maps the query words to their hypernyms, starting from the most specific ones to the less specific ones. Empirical studies on SemEval-2018 Task 9 confirm the effectiveness of the presented model",
    "checked": true,
    "id": "c7919e1a2d5f92f5ce686288e42326645d1f6bde",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Yuhang Bai",
      "Richong Zhang",
      "Fanshuang Kong",
      "Junfan Chen",
      "Yongyi Mao"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.258": {
    "title": "Modeling the Influence of Verb Aspect on the Activation of Typical Event Locations with BERT",
    "volume": "findings",
    "abstract": "Prior studies on event knowledge in sentence comprehension have shown that the aspect of the main verb plays an important role in the processing of non-core semantic roles, such as locations: when the aspect of the main verb is imperfective, locations become more salient in the mental representation of the event and are easier for human comprehenders to process. In our study, we tested the popular language model BERT on two datasets derived from experimental studies to determine whether BERT's predictions of prototypical event locations were also influenced by aspect. We found that, although BERT efficiently modelled the typicality of locations, it did so independently of the verb aspect. Even when the transformer was forced to focus on the verb phrase by masking the context words in the sentence, the typicality predictions were still accurate; in addition, we found aspect to have a stronger influence on the scores, with locations in the imperfective setting being associated with lower surprisal values",
    "checked": true,
    "id": "d073db916b5c04c59f4e58e4951159693c54dd80",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Won Ik Cho",
      "Emmanuele Chersoni",
      "Yu-Yin Hsu",
      "Chu-Ren Huang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.259": {
    "title": "On the Interaction of Belief Bias and Explanations",
    "volume": "findings",
    "abstract": "A myriad of explainability methods have been proposed in recent years, but there is little consensus on how to evaluate them. While automatic metrics allow for quick benchmarking, it isn't clear how such metrics reflect human interaction with explanations. Human evaluation is of paramount importance, but previous protocols fail to account for belief biases affecting human performance, which may lead to misleading conclusions. We provide an overview of belief bias, its role in human evaluation, and ideas for NLP practitioners on how to account for it. For two experimental paradigms, we present a case study of gradientbased explainability introducing simple ways to account for humans' prior beliefs: models of varying quality and adversarial examples. We show that conclusions about the highest performing methods change when introducing such controls, pointing to the importance of accounting for belief bias in evaluation",
    "checked": true,
    "id": "c1826a38625760b32885cc62878a0396a1e6dbe0",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Ana Valeria González",
      "Anna Rogers",
      "Anders Søgaard"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.260": {
    "title": "Combining Static Word Embeddings and Contextual Representations for Bilingual Lexicon Induction",
    "volume": "findings",
    "abstract": "Bilingual Lexicon Induction (BLI) aims to map words in one language to their translations in another, and are typically through learning linear projections to align monolingual word representation spaces. Two classes of word representations have been explored for BLI: static word embeddings and contextual representations, but there is no studies to combine both. In this paper, we propose a simple yet effective mechanism to combine the static word embeddings and the contextual representations to utilize the advantages of both paradigms. We test the combination mechanism on various language pairs under the supervised and unsupervised BLI benchmark settings. Experiments show that our mechanism consistently improves performances over robust BLI baselines on all language pairs by averagely improving 3.2 points in the supervised setting, and 3.1 points in the unsupervised setting1",
    "checked": true,
    "id": "9d24d4304078f0be973220c63abcceb3ea1848c2",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Jinpeng Zhang",
      "Baijun Ji",
      "Nini Xiao",
      "Xiangyu Duan",
      "Min Zhang",
      "Yangbin Shi",
      "Weihua Luo"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.261": {
    "title": "Exploring Unsupervised Pretraining Objectives for Machine Translation",
    "volume": "findings",
    "abstract": "Unsupervised cross-lingual pretraining has achieved strong results in neural machine translation (NMT), by drastically reducing the need for large parallel data. Most approaches adapt masked-language modeling (MLM) to sequence-to-sequence architectures, by masking parts of the input and reconstructing them in the decoder. In this work, we systematically compare masking with alternative objectives that produce inputs resembling real (full) sentences, by reordering and replacing words based on their context. We pretrain models with different methods on English↔German, English↔Nepali and English↔Sinhala monolingual data, and evaluate them on NMT. In (semi-) supervised NMT, varying the pretraining objective leads to surprisingly small differences in the finetuned performance, whereas unsupervised NMT is much more sensitive to it. To understand these results, we thoroughly study the pretrained models using a series of probes and verify that they encode and use information in different ways. We conclude that finetuning on parallel data is mostly sensitive to few properties that are shared by most models, such as a strong decoder, in contrast to unsupervised NMT that also requires models with strong cross-lingual abilities",
    "checked": true,
    "id": "12d588cf21970e86f0be537c1d0123ea42093119",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Christos Baziotis",
      "Ivan Titov",
      "Alexandra Birch",
      "Barry Haddow"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.262": {
    "title": "Knowledge-Grounded Dialogue Generation with Term-level De-noising",
    "volume": "findings",
    "abstract": "Dialogue generation has been improved through injecting knowledge into generative models. However, addition of knowledge through simple selection of sentences or paragraphs is likely to introduce noise and diminish the effectiveness of the generative models. In this paper, we present a novel Knowledge Term Weighting Model (KTWM) that incorporates term-level de-noising of the selected knowledge. KTWM includes a module for generating Simulated Response Vectors (SRVs) and uses SRVs attention distributions with the knowledge embeddings to determine knowledge term weights. Our experiments demonstrate that KTWM, combined with various knowledge selection algorithms, consistently achieves statistically significant improvements over methods without term weighting when applied to two publicly available datasets Wizard of Wikipedia (Wiz) and Holl-E. The results are particularly improved for the Wiz test data with unseen topics, demonstrating the robustness of the KTWM noise-reduction approach",
    "checked": true,
    "id": "495235a416d35bbaf441e8c38b7cb20eb9a03848",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Wen Zheng",
      "Natasa Milic-Frayling",
      "Ke Zhou"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.263": {
    "title": "Inspecting the concept knowledge graph encoded by modern language models",
    "volume": "findings",
    "abstract": "The field of natural language understanding has experienced exponential progress in the last few years, with impressive results in several tasks. This success has motivated researchers to study the underlying knowledge encoded by these models. Despite this, attempts to understand their semantic capabilities have not been successful, often leading to non-conclusive, or contradictory conclusions among different works. Via a probing classifier, we extract the underlying knowledge graph of nine of the most influential language models of the last years, including word embeddings, text generators, and context encoders. This probe is based on concept relatedness, grounded on WordNet. Our results reveal that all the models encode this knowledge, but suffer from several inaccuracies. Furthermore, we show that the different architectures and training strategies lead to different model biases. We conduct a systematic evaluation to discover specific factors that explain why some concepts are challenging. We hope our insights will motivate the development of models that capture concepts more precisely",
    "checked": true,
    "id": "12b336abe8b9889bfd2b82ff790e53603a899cbe",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Carlos Aspillaga",
      "Marcelo Mendoza",
      "Alvaro Soto"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.264": {
    "title": "Language Tags Matter for Zero-Shot Neural Machine Translation",
    "volume": "findings",
    "abstract": "Multilingual Neural Machine Translation (MNMT) has aroused widespread interest due to its efficiency. An exciting advantage of MNMT models is that they could also translate between unsupervised (zero-shot) language directions. Language tag (LT) strategies are often adopted to indicate the translation directions in MNMT. In this paper, we demonstrate that the LTs are not only indicators for translation directions but also crucial to zero-shot translation qualities. Unfortunately, previous work tends to ignore the importance of LT strategies. We demonstrate that a proper LT strategy could enhance the consistency of semantic representations and alleviate the off-target issue in zero-shot directions. Experimental results show that by ignoring the source language tag (SLT) and adding the target language tag (TLT) to the encoder, the zero-shot translations could achieve a +8 BLEU score difference over other LT strategies in IWSLT17, Europarl, TED talks translation tasks",
    "checked": true,
    "id": "843db426afd8a43af077868f73dfa301c7f8be48",
    "semantic_title": "",
    "citation_count": 18,
    "authors": [
      "Liwei Wu",
      "Shanbo Cheng",
      "Mingxuan Wang",
      "Lei Li"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.265": {
    "title": "Latent Reasoning for Low-Resource Question Generation",
    "volume": "findings",
    "abstract": "Multi-hop question generation requires complex reasoning and coherent language realization. Learning a generation model for the problem requires extensive multi-hop question answering (QA) data, which are limited due to the manual collection effort. A two-phase strategy addresses the insufficiency of multihop QA data by first generating and then composing single-hop sub-questions. Learning this generating and then composing twophase model, however, requires manually labeled question decomposition data, which is labor intensive. To overcome this limitation, we propose a novel generative approach that optimizes the two-phase model without question decomposition data. We treat the unobserved sub-questions as latent variables and propose an objective that estimates the true sub-questions via variational inference. We further generalize the generative modeling to single-hop QA data. We hypothesize that each single-hop question is a sub-question of an unobserved multi-hop question, and propose an objective that generates single-hop questions by decomposing latent multi-hop questions. We show that the two objectives can be unified and both optimize the two-phase generation model. Experiments show that the proposed approach outperforms competitive baselines on HOTPOTQA, a benchmark multi-hop question answering dataset",
    "checked": true,
    "id": "fac6e5ab3eb90b3651757b9d7a7927ed9367ae5f",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Xinting Huang",
      "Jianzhong Qi",
      "Yu Sun",
      "Rui Zhang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.266": {
    "title": "Probing Pre-Trained Language Models for Disease Knowledge",
    "volume": "findings",
    "abstract": "Pre-trained language models such as ClinicalBERT have achieved impressive results on tasks such as medical Natural Language Inference. At first glance, this may suggest that these models are able to perform medical reasoning tasks, such as mapping symptoms to diseases. However, we find that standard benchmarks such as MedNLI contain relatively few examples that require such forms of reasoning. To better understand the medical reasoning capabilities of existing language models, in this paper we introduce DisKnE, a new benchmark for Disease Knowledge Evaluation. To construct this benchmark, we annotated each positive MedNLI example with the types of medical reasoning that are needed. We then created negative examples by corrupting these positive examples in an adversarial way. Furthermore, we define training-test splits per disease, ensuring that no knowledge about test diseases can be learned from the training data, and we canonicalize the formulation of the hypotheses to avoid the presence of artefacts. This leads to a number of binary classification problems, one for each type of reasoning and each disease. When analysing pre-trained models for the clinical/biomedical domain on the proposed benchmark, we find that their performance drops considerably",
    "checked": true,
    "id": "896c76bf99de9c1ba6fc3dfb695dfa66e73e1eb3",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Israa Alghanmi",
      "Luis Espinosa Anke",
      "Steven Schockaert"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.267": {
    "title": "AugVic: Exploiting BiText Vicinity for Low-Resource NMT",
    "volume": "findings",
    "abstract": "The success of Neural Machine Translation (NMT) largely depends on the availability of large bitext training corpora. Due to the lack of such large corpora in low-resource language pairs, NMT systems often exhibit poor performance. Extra relevant monolingual data often helps, but acquiring it could be quite expensive, especially for low-resource languages. Moreover, domain mismatch between bitext (train/test) and monolingual data might degrade the performance. To alleviate such issues, we propose AUGVIC, a novel data augmentation framework for low-resource NMT which exploits the vicinal samples of the given bitext without using any extra monolingual data explicitly. It can diversify the in-domain bitext data with finer level control. Through extensive experiments on four low-resource language pairs comprising data from different domains, we have shown that our method is comparable to the traditional back-translation that uses extra in-domain monolingual data. When we combine the synthetic parallel data generated from AUGVIC with the ones from the extra monolingual data, we achieve further improvements. We show that AUGVIC helps to attenuate the discrepancies between relevant and distant-domain monolingual data in traditional back-translation. To understand the contributions of different components of AUGVIC, we perform an in-depth framework analysis",
    "checked": true,
    "id": "ea0ab5238d7bfb62370789cccdb1cf856cb38aaa",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Tasnim Mohiuddin",
      "M Saiful Bari",
      "Shafiq Joty"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.268": {
    "title": "Provably Secure Generative Linguistic Steganography",
    "volume": "findings",
    "abstract": "Generative linguistic steganography mainly utilized language models and applied steganographic sampling (stegosampling) to generate high-security steganographic text (stegotext). However, previous methods generally lead to statistical differences between the conditional probability distributions of stegotext and natural text, which brings about security risks. In this paper, to further ensure security, we present a novel provably secure generative linguistic steganographic method ADG, which recursively embeds secret information by Adaptive Dynamic Grouping of tokens according to their probability given by an offthe-shelf language model. We not only prove the security of ADG mathematically, but also conduct extensive experiments on three public corpora to further verify its imperceptibility. The experimental results reveal that the proposed method is able to generate stegotext with nearly perfect security",
    "checked": true,
    "id": "8068a409f014b2f5c51f8579ce2f75d00b329515",
    "semantic_title": "",
    "citation_count": 18,
    "authors": [
      "Siyu Zhang",
      "Zhongliang Yang",
      "Jinshuai Yang",
      "Yongfeng Huang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.269": {
    "title": "Retrieval Enhanced Model for Commonsense Generation",
    "volume": "findings",
    "abstract": "Commonsense generation is a challenging task of generating a plausible sentence describing an everyday scenario using provided concepts. Its requirement of reasoning over commonsense knowledge and compositional generalization ability even puzzles strong pre-trained language generation models. We propose a novel framework using retrieval methods to enhance both the pre-training and fine-tuning for commonsense generation. We retrieve prototype sentence candidates by concept matching and use them as auxiliary input. For finetuning, we further boost its performance with a trainable sentence retriever. We demonstrate experimentally on the large-scale CommonGen benchmark that our approach achieves new state-of-the-art results.1",
    "checked": true,
    "id": "5c95de51ac81569fd515df1a91fe0a6617536fd9",
    "semantic_title": "",
    "citation_count": 24,
    "authors": [
      "Han Wang",
      "Yang Liu",
      "Chenguang Zhu",
      "Linjun Shou",
      "Ming Gong",
      "Yichong Xu",
      "Michael Zeng"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.270": {
    "title": "Decoupled Dialogue Modeling and Semantic Parsing for Multi-Turn Text-to-SQL",
    "volume": "findings",
    "abstract": "Recently, Text-to-SQL for multi-turn dialogue has attracted great interest. Here, the user input of the current turn is parsed into the corresponding SQL query of the appropriate database, given all previous dialogue history. Current approaches mostly employ end-to-end models and consequently face two challenges. First, dialogue history modeling and Text-toSQL parsing are implicitly combined, hence it is hard to carry out interpretable analysis and obtain targeted improvement. Second, SQL annotation of multi-turn dialogue is very expensive, leading to training data sparsity. In this paper, we propose a novel decoupled multi-turn Text-to-SQL framework, where an utterance rewrite model first explicitly solves completion of dialogue context, and then a single-turn Text-to-SQL parser follows. A dual learning approach is also proposed for the utterance rewrite model to address the data sparsity problem. Compared with end-to-end approaches, the proposed decoupled method can achieve excellent performance without any annotated in-domain data. With just a few annotated rewrite cases, the decoupled method outperforms the released state-of-the-art endto-end models on both SParC and CoSQL datasets",
    "checked": true,
    "id": "633489316d298b0316e2e33338cd120dfe379dd0",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Zhi Chen",
      "Lu Chen",
      "Hanqi Li",
      "Ruisheng Cao",
      "Da Ma",
      "Mengyue Wu",
      "Kai Yu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.271": {
    "title": "Adjacency List Oriented Relational Fact Extraction via Adaptive Multi-task Learning",
    "volume": "findings",
    "abstract": "Relational fact extraction aims to extract semantic triplets from unstructured text. In this work, we show that all of the relational fact extraction models can be organized according to a graph-oriented analytical perspective. An efficient model, aDjacency lIst oRiented rElational faCT (DIRECT), is proposed based on this analytical framework. To alleviate challenges of error propagation and sub-task loss equilibrium, DIRECT employs a novel adaptive multi-task learning strategy with dynamic sub-task loss balancing. Extensive experiments are conducted on two benchmark datasets, and results prove that the proposed model outperforms a series of state-of-the-art (SoTA) models for relational triplet extraction",
    "checked": true,
    "id": "1ba1977a1ac7775fbeb987ac9200156f480fbf06",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Fubang Zhao",
      "Zhuoren Jiang",
      "Yangyang Kang",
      "Changlong Sun",
      "Xiaozhong Liu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.272": {
    "title": "Self-Supervised Document Similarity Ranking via Contextualized Language Models and Hierarchical Inference",
    "volume": "findings",
    "abstract": "We present a novel model for the problem of ranking a collection of documents according to their semantic similarity to a source (query) document. While the problem of document-todocument similarity ranking has been studied, most modern methods are limited to relatively short documents or rely on the existence of \"ground-truth\" similarity labels. Yet, in most common real-world cases, similarity ranking is an unsupervised problem as similarity labels are unavailable. Moreover, an ideal model should not be restricted by documents' length. Hence, we introduce SDR, a self-supervised method for document similarity that can be applied to documents of arbitrary length. Importantly, SDR can be effectively applied to extremely long documents, exceeding the 4, 096 maximal token limit of Longformer. Extensive evaluations on large documents datasets show that SDR significantly outperforms its alternatives across all metrics. To accelerate future research on unlabeled long document similarity ranking, and as an additional contribution to the community, we herein publish two humanannotated test-sets of long documents similarity evaluation. The SDR code and datasets are publicly available 1",
    "checked": true,
    "id": "5397ef2da78aac248826b66156bed824d8aa03fb",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Dvir Ginzburg",
      "Itzik Malkiel",
      "Oren Barkan",
      "Avi Caciularu",
      "Noam Koenigstein"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.273": {
    "title": "How Good Is NLP? A Sober Look at NLP Tasks through the Lens of Social Impact",
    "volume": "findings",
    "abstract": "Recent years have seen many breakthroughs in natural language processing (NLP), transitioning it from a mostly theoretical field to one with many real-world applications. Noting the rising number of applications of other machine learning and AI techniques with pervasive societal impact, we anticipate the rising importance of developing NLP technologies for social good. Inspired by theories in moral philosophy and global priorities research, we aim to promote a guideline for social good in the context of NLP. We lay the foundations via the moral philosophy definition of social good, propose a framework to evaluate the direct and indirect real-world impact of NLP tasks, and adopt the methodology of global priorities research to identify priority causes for NLP research. Finally, we use our theoretical framework to provide some practical guidelines for future NLP research for social good.1",
    "checked": true,
    "id": "5c7b8f26ae23c457d85da9d0f19b28e588e9b9a7",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Zhijing Jin",
      "Geeticka Chauhan",
      "Brian Tse",
      "Mrinmaya Sachan",
      "Rada Mihalcea"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.274": {
    "title": "IgSEG: Image-guided Story Ending Generation",
    "volume": "findings",
    "abstract": "In this work, we propose a new task called Image-guided Story Ending Generation (IgSEG). Given a multi-sentence story plot and an ending-related image, IgSEG aims to generate a story ending that conforms to the contextual logic and the relevant visual concepts. In contrast to the story ending generation task, which generates open-ended endings, the major challenges of IgSEG are to comprehend the given context and image sufficiently, and mine the appropriate semantics from the image to make the generated story ending informative, reasonable, and coherent. To address the challenges, we propose a Multi-layer Graph convolution and Cascade-LSTM (MGCL) based model which mainly comprises of two collaborative modules: i) a multi-layer graph convolutional network to learn the dependency relations of sentences and the logical clue of the context; ii) a multiple context-image attention module to generate the story endings by gradually incorporating textual and visual semantic concepts. Our MGCL is thus capable of building logically consistent and semantically rich story endings. To evaluate the proposed model, we modify the existing VIST dataset to obtain the VIST-Ending dataset. Empirically, our MGCL outperforms all the strong baselines on both automatic and human evaluation",
    "checked": true,
    "id": "bd87fa465ec2bf05685af10b216eb3f3a2ee42b9",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Qingbao Huang",
      "Chuan Huang",
      "Linzhang Mo",
      "Jielong Wei",
      "Yi Cai",
      "Ho-fung Leung",
      "Qing Li"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.275": {
    "title": "Improve Query Focused Abstractive Summarization by Incorporating Answer Relevance",
    "volume": "findings",
    "abstract": "Query focused summarization (QFS) models aim to generate summaries from source documents that can answer the given query. Most previous work on QFS only considers the query relevance criterion when producing the summary. However, studying the effect of answer relevance in the summary generating process is also important. In this paper, we propose QFS-BART, a model that incorporates the explicit answer relevance of the source documents given the query via a question answering model, to generate coherent and answerrelated summaries. Furthermore, our model can take advantage of large pre-trained models which improve the summarization performance significantly. Empirical results on the Debatepedia dataset show that the proposed model achieves the new state-of-the-art performance.1",
    "checked": true,
    "id": "62e08a6e155fe0fe223d802cdf5bf49fefb6df2e",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Dan Su",
      "Tiezheng Yu",
      "Pascale Fung"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.276": {
    "title": "Learning a Reversible Embedding Mapping using Bi-Directional Manifold Alignment",
    "volume": "findings",
    "abstract": "We propose a Bi-Directional Manifold Alignment (BDMA) that learns a non-linear mapping between two manifolds by explicitly training it to be bijective. We demonstrate BDMA by training a model for a pair of languages rather than individual, directed source and target combinations, reducing the number of models by 50%. We show that models trained with BDMA in the \"forward\" (source to target) direction can successfully map words in the \"reverse\" (target to source) direction, yielding equivalent (or better) performance to standard unidirectional translation models where the source and target language is flipped. We also show how BDMA reduces the overall size of the model",
    "checked": true,
    "id": "fb0b53383dd2e25eeb66535b5ebaed3157acfee1",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Ashwinkumar Ganesan",
      "Francis Ferraro",
      "Tim Oates"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.277": {
    "title": "Probabilistic Graph Reasoning for Natural Proof Generation",
    "volume": "findings",
    "abstract": "In this paper, we investigate the problem of reasoning over natural language statements. Prior neural based approaches do not explicitly consider the inter-dependency among answers and their proofs. In this paper, we propose PROBR, a novel approach for joint answer prediction and proof generation. PROBR defines a joint probabilistic distribution over all possible proof graphs and answers via an induced graphical model. We then optimize the model using variational approximation on top of neural textual representation. Experiments on multiple datasets under diverse settings (fully supervised, few-shot and zero-shot evaluation) verify the effectiveness of PROBR, e.g., achieving 10%-30% improvement on QA accuracy in few/zero-shot evaluation. Our codes and models can be found at https://github.com/ changzhisun/PRobr/",
    "checked": true,
    "id": "cebaf1435bcee484b41cf0b97ba6fd86a36b38d0",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Changzhi Sun",
      "Xinbo Zhang",
      "Jiangjie Chen",
      "Chun Gan",
      "Yuanbin Wu",
      "Jiaze Chen",
      "Hao Zhou",
      "Lei Li"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.278": {
    "title": "Enhancing Zero-shot and Few-shot Stance Detection with Commonsense Knowledge Graph",
    "volume": "findings",
    "abstract": "In this paper, we consider a realistic scenario on stance detection with more application potential, i.e., zero-shot and few-shot stance detection, which identifies stances for a wide range of topics with no or very few training examples. Conventional data-driven approaches are not applicable to the above zero-shot and few-shot scenarios. For human beings, commonsense knowledge is a crucial element of understanding and reasoning. In the absence of annotated data and cryptic expression of users' stance, we believe that introducing commonsense relational knowledge as support for reasoning can further improve the generalization and reasoning ability of the model in the zero-shot and few-shot scenarios. Specifically, we introduce a commonsense knowledge enhanced model to exploit both the structurallevel and semantic-level information of the relational knowledge. Extensive experiments demonstrate that our model outperforms the state-of-the-art methods on zero-shot and fewshot stance detection task",
    "checked": true,
    "id": "a6c780c18ea2535427e478907873efa3042f32e2",
    "semantic_title": "",
    "citation_count": 31,
    "authors": [
      "Rui Liu",
      "Zheng Lin",
      "Yutong Tan",
      "Weiping Wang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.279": {
    "title": "Dialogue Graph Modeling for Conversational Machine Reading",
    "volume": "findings",
    "abstract": "Conversational Machine Reading (CMR) aims at answering questions in complicated interactive scenarios. Machine needs to answer questions through interactions with users based on given rule document, user scenario and dialogue history, and even initiatively asks questions for clarification if necessary. Namely, the answer to the task needs a machine in the response of either Yes, No, Irrelevant or to raise a follow-up question for further clarification. To effectively capture multiple objects in such a challenging task, graph modeling is supposed to be adopted, though it is surprising that this does not happen until this work proposes a dialogue graph modeling framework by incorporating two complementary graph models, i.e., explicit discourse graph and implicit discourse graph, which respectively capture explicit and implicit interactions hidden in the rule documents. The proposed model is evaluated on the ShARC benchmark and achieves new state-of-the-art by first exceeding the milestone accuracy score of 80%. The source code of our paper is available at https: //github.com/ozyyshr/DGM",
    "checked": true,
    "id": "114be5db62209a0d0682279f5a054a316f56697e",
    "semantic_title": "",
    "citation_count": 26,
    "authors": [
      "Siru Ouyang",
      "Zhuosheng Zhang",
      "Hai Zhao"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.280": {
    "title": "IndoCollex: A Testbed for Morphological Transformation of Indonesian Colloquial Words",
    "volume": "findings",
    "abstract": "Indonesian language is heavily riddled with colloquialism whether in written or spoken forms. In this paper, we identify a class of Indonesian colloquial words that have undergone morphological transformations from their standard forms, categorize their word formations, and propose a benchmark dataset of Indonesian Colloquial Lexicons (IndoCollex) consisting of informal words on Twitter expertly annotated with their standard forms and their word formation types/tags. We evaluate several models for character-level transduction to perform morphological word normalization on this testbed to understand their failure cases and provide baselines for future work. As IndoCollex catalogues word formation phenomena that are also present in the non-standard text of other languages, it can also provide an attractive testbed for methods tailored for cross-lingual word normalization and non-standard word formation",
    "checked": true,
    "id": "ceed352d2d5c0d29264f5c81328bfc92dee6ce5e",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Haryo Akbarianto Wibowo",
      "Made Nindyatama Nityasya",
      "Afra Feyza Akyürek",
      "Suci Fitriany",
      "Alham Fikri Aji",
      "Radityo Eko Prasojo",
      "Derry Tanti Wijaya"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.281": {
    "title": "Manifold Adversarial Augmentation for Neural Machine Translation",
    "volume": "findings",
    "abstract": "Improving the robustness of neural machine translation models on variations of input sentences is an active area of research. In this paper, we propose a simple data augmentation approach by sampling virtual sentences from the vicinity distributions in higher-level representations, constructed either from individual training samples via adversarial learning or pairs of training samples through mixup. By simplifying and extending previous work that operates at the token level, our method can construct virtual training samples in a broader space and achieve improved translation accuracy compared to the previous stateof-the-art. In addition, we present a simple variation of the mixup strategy to better utilize the pseudo training samples created from backtranslation, obtaining further improvement in performance",
    "checked": true,
    "id": "e327186b75e8359ea10c7258c2fdcf93c437e2a5",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Guandan Chen",
      "Kai Fan",
      "Kaibo Zhang",
      "Boxing Chen",
      "Zhongqiang Huang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.282": {
    "title": "Learning to Bridge Metric Spaces: Few-shot Joint Learning of Intent Detection and Slot Filling",
    "volume": "findings",
    "abstract": "In this paper, we investigate few-shot joint learning for dialogue language understanding. Most existing few-shot models learn a single task each time with only a few examples. However, dialogue language understanding contains two closely related tasks, i.e., intent detection and slot filling, and often benefits from jointly learning the two tasks. This calls for new few-shot learning techniques that are able to capture task relations from only a few examples and jointly learn multiple tasks. To achieve this, we propose a similarity-based few-shot learning scheme, named Contrastive Prototype Merging network (ConProm), that learns to bridge metric spaces of intent and slot on data-rich domains, and then adapt the bridged metric space to specific few-shot domain. Experiments on two public datasets, Snips and FewJoint, show that our model significantly outperforms the strong baselines in one and five shots settings",
    "checked": true,
    "id": "5ae2ef93ed93e4b11ed096c570448623f9a31f80",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Yutai Hou",
      "Yongkui Lai",
      "Cheng Chen",
      "Wanxiang Che",
      "Ting Liu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.283": {
    "title": "Insertion-based Tree Decoding",
    "volume": "findings",
    "abstract": "Sequences are typically decoded in a leftto-right fashion, requiring as many decoding steps as there are tokens in the sequence. Recently, several works have proposed nonautoregressive decoders that are sub-linear, allowing to decode a sequence using fewer decoding steps than the length of the sequence, and thus substantially speed up inference. In contrast, non-autoregressive decoding of trees is less well-analysed, even though trees are used in important applications like semantic parsing and code generation. In this work, we present a novel general-purpose partially autoregressive tree decoder that uses treebased insertion operations to generate trees in sub-linear time. We evaluate our approach on semantic parsing and compare it against strong baselines, including an insertion-based sequence decoder. The results demonstrate that the partially autoregressive tree decoder reaches competitive accuracies while clearly reducing the number of decoding steps",
    "checked": true,
    "id": "b8cba3c5880a8ad29259408ef67fbdc6b41d2027",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Denis Lukovnikov",
      "Asja Fischer"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.284": {
    "title": "Is the Lottery Fair? Evaluating Winning Tickets Across Demographics",
    "volume": "findings",
    "abstract": "Recent studies have suggested that weight pruning, e.g. using lottery ticket extraction techniques (Frankle and Carbin, 2018), comes at the risk of compromising the group fairness of machine learning models (Paganini, 2020; Hooker et al., 2020), but to the best of our knowledge, no one has empirically evaluated this hypothesis at scale in the context of natural language processing. We present experiments with two text classiﬁcation datasets annotated with demographic information: the Trustpilot Corpus (sentiment) and CivilComments (toxicity). We evaluate the fairness of lottery ticket extraction through layer-wise and global weight pruning across three lan-guages and two tasks. Our results suggest that there is a small increase in group disparity, which is most pronounced at high pruning rates and correlates with instability. The fairness of models trained with distributionally robust optimization objectives is sometimes less sensitive to pruning, but results are not consistent. The code for our experiments is available at https://github. com/vpetren/fairness_lottery",
    "checked": true,
    "id": "5b6ef4a959f40894101fc98305079bc3603ea265",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Victor Petrén Bach Hansen",
      "Anders Søgaard"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.285": {
    "title": "SSMix: Saliency-Based Span Mixup for Text Classification",
    "volume": "findings",
    "abstract": "Data augmentation with mixup has shown to be effective on various computer vision tasks. Despite its great success, there has been a hurdle to apply mixup to NLP tasks since text consists of discrete tokens with variable length. In this work, we propose SSMix, a novel mixup method where the operation is performed on input text rather than on hidden vectors like previous approaches. SSMix synthesizes a sentence while preserving the locality of two original texts by span-based mixing and keeping more tokens related to the prediction relying on saliency information. With extensive experiments, we empirically validate that our method outperforms hidden-level mixup methods on a wide range of text classification benchmarks, including textual entailment, sentiment classification, and questiontype classification. Our code is available at https://github.com/clovaai/ssmix",
    "checked": true,
    "id": "47ad38b92b843ef2521db9a650038d7d00e4e067",
    "semantic_title": "",
    "citation_count": 30,
    "authors": [
      "Soyoung Yoon",
      "Gyuwan Kim",
      "Kyumin Park"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.286": {
    "title": "Detecting Bot-Generated Text by Characterizing Linguistic Accommodation in Human-Bot Interactions",
    "volume": "findings",
    "abstract": "Language generation models' democratization benefits many domains, from answering health-related questions to enhancing education by providing AI-driven tutoring services. However, language generation models' democratization also makes it easier to generate human-like text at-scale for nefarious activities, from spreading misinformation to targeting specific groups with hate speech. Thus, it is essential to understand how people interact with bots and develop methods to detect bot-generated text. This paper shows that bot-generated text detection methods are more robust across datasets and models if we use information about how people respond to it rather than using the bot's text directly. We also analyze linguistic alignment, providing insight into differences between humanhuman and human-bot conversations",
    "checked": true,
    "id": "344e5e3492cb4ab3622be60bc3284368b3dbbb62",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Paras Bhatt",
      "Anthony Rios"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.287": {
    "title": "Defending Pre-trained Language Models from Adversarial Word Substitution Without Performance Sacrifice",
    "volume": "findings",
    "abstract": "Pre-trained contextualized language models (PrLMs) have led to strong performance gains in downstream natural language understanding tasks. However, PrLMs can still be easily fooled by adversarial word substitution, which is one of the most challenging textual adversarial attack methods. Existing defence approaches suffer from notable performance loss and complexities. Thus, this paper presents a compact and performance-preserved framework, Anomaly Detection with FrequencyAware Randomization (ADFAR). In detail, we design an auxiliary anomaly detection classifier and adopt a multi-task learning procedure, by which PrLMs are able to distinguish adversarial input samples. Then, in order to defend adversarial word substitution, a frequency-aware randomization process is applied to those recognized adversarial input samples. Empirical results show that ADFAR significantly outperforms those newly proposed defense methods over various tasks with much higher inference speed. Remarkably, ADFAR does not impair the overall performance of PrLMs. The code is available at https://github.com/LilyNLP/ADFAR",
    "checked": true,
    "id": "1c2e60e0e31d06c909ddffbb2387987b449aceb0",
    "semantic_title": "",
    "citation_count": 17,
    "authors": [
      "Rongzhou Bao",
      "Jiayi Wang",
      "Hai Zhao"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.288": {
    "title": "BERT-Proof Syntactic Structures: Investigating Errors in Discontinuous Constituency Parsing",
    "volume": "findings",
    "abstract": "The combined use of neural scoring systems and BERT fine-tuning has led to very high results in many natural language processing (NLP) tasks. These high results raise two important questions about the contribution and the limitations of pretrained-language models: (i) what are the remaining errors in the bestperforming systems? (ii) what are the types of test examples where pretrained language models help the most? In this paper, we investigate both questions for the task of English discontinuous constituency parsing on the Penn Treebank, for which recent models obtain close to 95 F1 score. To do so, we propose two methods for automatically analysing the errors of discontinuous parser. First, we annotate and release a test-suite focused on the syntactic phenomena responsible for discontinuities in the Penn Treebank, enabling us to obtain a per-phenomenon evaluation of a parser's output. Second, we extend the Berkeley Parser Analyser — a tool that classifies parsing errors according to predefined structural patterns —, to discontinuous trees. We apply both methods to characterize errors of a state-of-theart transition-based discontinuous parser, and to provide an overview of the contribution of BERT to this task",
    "checked": true,
    "id": "d8bf331ad447b93160067613f4283d59837d1a33",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Maximin Coavoux"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.289": {
    "title": "DoT: An efficient Double Transformer for NLP tasks with tables",
    "volume": "findings",
    "abstract": "Transformer-based approaches have been successfully used to obtain state-of-the-art accuracy on natural language processing (NLP) tasks with semi-structured tables. These model architectures are typically deep, resulting in slow training and inference, especially for long inputs. To improve efficiency while maintaining a high accuracy, we propose a new architecture, DoT , a double transformer model, that decomposes the problem into two sub-tasks: A shallow pruning transformer that selects the top-K tokens, followed by a deep task-specific transformer that takes as input those K tokens. Additionally, we modify the task-specific attention to incorporate the pruning scores. The two transformers are jointly trained by optimizing the task-specific loss. We run experiments on three benchmarks, including entailment and question-answering. We show that for a small drop of accuracy, DoT improves training and inference time by at least 50%. We also show that the pruning transformer effectively selects relevant tokens enabling the end-to-end model to maintain similar accuracy as slower baseline models. Finally, we analyse the pruning and give some insight into its impact on the task model",
    "checked": true,
    "id": "42ce2dd46dac1be5607d080ceb2a07128c7c95bb",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Syrine Krichene",
      "Thomas Müller",
      "Julian Eisenschlos"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.290": {
    "title": "Grammatical Error Correction as GAN-like Sequence Labeling",
    "volume": "findings",
    "abstract": "In Grammatical Error Correction (GEC), sequence labeling models enjoy fast inference compared to sequence-to-sequence models; however, inference in sequence labeling GEC models is an iterative process, as sentences are passed to the model for multiple rounds of correction, which exposes the model to sentences with progressively fewer errors at each round. Traditional GEC models learn from sentences with fixed error rates. Coupling this with the iterative correction process causes a mismatch between training and inference that affects final performance. In order to address this mismatch, we propose a GAN-like sequence labeling model, which consists of a grammatical error detector as a discriminator and a grammatical error labeler with Gumbel-Softmax sampling as a generator. By sampling from real error distributions, our errors are more genuine compared to traditional synthesized GEC errors, thus alleviating the aforementioned mismatch and allowing for better training. Our results on several evaluation benchmarks demonstrate that our proposed approach is effective and improves the previous state-of-the-art baseline",
    "checked": true,
    "id": "24467da89797924cc0fb3931184c17c25b472b37",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Kevin Parnow",
      "Zuchao Li",
      "Hai Zhao"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.291": {
    "title": "Neural Entity Recognition with Gazetteer based Fusion",
    "volume": "findings",
    "abstract": "Incorporating external knowledge into Named Entity Recognition (NER) systems has been widely studied in the generic domain. In this paper, we focus on clinical domain where only limited data is accessible and interpretability is important. Recent advancement in technology and the acceleration of clinical trials has resulted in the discovery of new drugs, procedures as well as medical conditions. These factors motivate towards building robust zeroshot NER systems which can quickly adapt to new medical terminology. We propose an auxiliary gazetteer model and fuse it with an NER system, which results in better robustness and interpretability across different clinical datasets. Our gazetteer based fusion model is data efficient, achieving +1.7 microF1 gains on the i2b2 dataset using 20% training data, and brings + 4.7 micro-F1 gains on novel entity mentions never presented during training. Moreover, our fusion model is able to quickly adapt to new mentions in gazetteers without re-training and the gains from the proposed fusion model are transferable to related datasets",
    "checked": true,
    "id": "79b7ab5c51f2d188eb5912137e6ffe1732639535",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Qing Sun",
      "Parminder Bhatia"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.292": {
    "title": "Hyperbolic Temporal Knowledge Graph Embeddings with Relational and Time Curvatures",
    "volume": "findings",
    "abstract": "Knowledge Graph (KG) completion has been excessively studied with a massive number of models proposed for the Link Prediction (LP) task. The main limitation of such models is their insensitivity to time. Indeed, the temporal aspect of stored facts is often ignored. To this end, more and more works consider time as a parameter to complete KGs. In this paper, we first demonstrate that, by simply increasing the number of negative samples, the recent ATTH model can achieve competitive or even better performance than the state-of-the-art on Temporal KGs (TKGs), albeit its nontemporality. We further propose HERCULES, a time-aware extension of ATTH model, which defines the curvature of a Riemannian manifold as the product of both relation and time. Our experiments show that both HERCULES and ATTH achieve competitive or new state-of-the-art performances on ICEWS04 and ICEWS05-15 datasets. Therefore, one should raise awareness when learning TKGs representations to identify whether time truly boosts performances",
    "checked": true,
    "id": "ca763f4dedeaa932ea2732ed090da3df4a148449",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Sebastien Montella",
      "Lina M. Rojas Barahona",
      "Johannes Heinecke"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.293": {
    "title": "Disfl-QA: A Benchmark Dataset for Understanding Disfluencies in Question Answering",
    "volume": "findings",
    "abstract": "Disfluencies is an under-studied topic in NLP, even though it is ubiquitous in human conversation. This is largely due to the lack of datasets containing disfluencies. In this paper, we present a new challenge question answering dataset, DISFL-QA, a derivative of SQUAD, where humans introduce contextual disfluencies in previously fluent questions. DISFL-QA contains a variety of challenging disfluencies that require a more comprehensive understanding of the text than what was necessary in prior datasets. Experiments show that the performance of existing state-of-the-art question answering models degrades significantly when tested on DISFLQA in a zero-shot setting. We show data augmentation methods partially recover the loss in performance and also demonstrate the efficacy of using gold data for fine-tuning. We argue that we need large-scale disfluency datasets in order for NLP models to be robust to them. The dataset is publicly available at: https://github.com/ google-research-datasets/disfl-qa",
    "checked": true,
    "id": "1662b96f9ad4a51a843dd6540ddc62e6d59fb03c",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Aditya Gupta",
      "Jiacheng Xu",
      "Shyam Upadhyay",
      "Diyi Yang",
      "Manaal Faruqui"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.294": {
    "title": "Does Robustness Improve Fairness? Approaching Fairness with Word Substitution Robustness Methods for Text Classification",
    "volume": "findings",
    "abstract": "Existing bias mitigation methods to reduce disparities in model outcomes across cohorts have focused on data augmentation, debiasing model embeddings, or adding fairness-based optimization objectives during training. Separately, certified word substitution robustness methods have been developed to decrease the impact of spurious features and synonym substitutions on model predictions. While their end goals are different, they both aim to encourage models to make the same prediction for certain changes in the input. In this paper, we investigate the utility of certified word substitution robustness methods to improve equality of odds and equality of opportunity on multiple text classification tasks. We observe that certified robustness methods improve fairness, and using both robustness and bias mitigation methods in training results in an improvement in both fronts",
    "checked": true,
    "id": "c5221a553d47de08ef507eaac019d933afd07eda",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Yada Pruksachatkun",
      "Satyapriya Krishna",
      "Jwala Dhamala",
      "Rahul Gupta",
      "Kai-Wei Chang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.295": {
    "title": "A Joint Model for Structure-based News Genre Classification with Application to Text Summarization",
    "volume": "findings",
    "abstract": "Journalists usually organize and present the contents of a news article following a welldefined structure. In this paper, we propose a novel joint model for structure-based news genre classification that simultaneously identifies one of four commonly used news structures (including Inverted Pyramid and three other structures) for a news article as well as recognizes a sequence of news elements within the article that define the corresponding news structure. Experiments show that the joint model consistently outperforms its variants that perform two tasks independently, which supports our motivation that preserving the two-way dependencies and constraints between a type of news structure and its sequence of news elements enables the model to better predict both of them. Although being not perfect, the system predicted news structure type and news elements have improved the performance of text summarization when incorporated into a recent neural network system",
    "checked": true,
    "id": "d7a3fee762555b7adfaec2d0ec157cc1fbded891",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Zeyu Dai",
      "Ruihong Huang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.296": {
    "title": "Representing Syntax and Composition with Geometric Transformations",
    "volume": "findings",
    "abstract": "The exploitation of syntactic graphs (SyGs) as a word's context has been shown to be beneficial for distributional semantic models (DSMs), both at the level of individual word representations and in deriving phrasal representations via composition. However, notwithstanding the potential performance benefit, the syntactically-aware DSMs proposed to date have huge numbers of parameters (compared to conventional DSMs) and suffer from data sparsity. Furthermore, the encoding of the SyG links (i.e., the syntactic relations) has been largely limited to linear maps. The knowledge graphs' literature, on the other hand, has proposed light-weight models employing different geometric transformations (GTs) to encode edges in a knowledge graph (KG). Our work explores the possibility of adopting this family of models to encode SyGs. Furthermore, we investigate which GT better encodes syntactic relations, so that these representations can be used to enhance phrase-level composition via syntactic contextualisation",
    "checked": true,
    "id": "e5d31a0b7650bf7e9e85f6c7c7bcf5d898df8d59",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Lorenzo Bertolini",
      "Julie Weeds",
      "David Weir",
      "Qiwei Peng"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.297": {
    "title": "Figurative Language in Recognizing Textual Entailment",
    "volume": "findings",
    "abstract": "We introduce a collection of recognizing textual entailment (RTE) datasets focused on figurative language. We leverage five existing datasets annotated for a variety of figurative language – simile, metaphor, and irony – and frame them into over 12,500 RTE examples.We evaluate how well state-of-the-art models trained on popular RTE datasets capture different aspects of figurative language. Our results and analyses indicate that these models might not sufficiently capture figurative language, struggling to perform pragmatic inference and reasoning about world knowledge. Ultimately, our datasets provide a challenging testbed for evaluating RTE models",
    "checked": true,
    "id": "bf10dec914620deba8c25c1aa37ad97a0fd437e0",
    "semantic_title": "",
    "citation_count": 18,
    "authors": [
      "Tuhin Chakrabarty",
      "Debanjan Ghosh",
      "Adam Poliak",
      "Smaranda Muresan"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.298": {
    "title": "To Point or Not to Point: Understanding How Abstractive Summarizers Paraphrase Text",
    "volume": "findings",
    "abstract": "Abstractive neural summarization models have seen great improvements in recent years, as shown by ROUGE scores of the generated summaries. But despite these improved metrics, there is limited understanding of the strategies different models employ, and how those strategies relate their understanding of language. To understand this better, we run several experiments to characterize how one popular abstractive model, the pointer-generator model of See et al. (2017), uses its explicit copy/generation switch to control its level of abstraction (generation) vs extraction (copying). On an extractive-biased dataset, the model utilizes syntactic boundaries to truncate sentences that are otherwise often copied verbatim. When we modify the copy/generation switch and force the model to generate, only simple paraphrasing abilities are revealed alongside factual inaccuracies and hallucinations. On an abstractivebiased dataset, the model copies infrequently but shows similarly limited abstractive abilities. In line with previous research, these results suggest that abstractive summarization models lack the semantic understanding necessary to generate paraphrases that are both abstractive and faithful to the source document.ive neural summarization models have seen great improvements in recent years, as shown by ROUGE scores of the generated summaries. But despite these improved metrics, there is limited understanding of the strategies different models employ, and how those strategies relate their understanding of language. To understand this better, we run several experiments to characterize how one popular abstractive model, the pointer-generator model of See et al. (2017), uses its explicit copy/generation switch to control its level of abstraction (generation) vs extraction (copying). On an extractive-biased dataset, the model utilizes syntactic boundaries to truncate sentences that are otherwise often copied verbatim. When we modify the copy/generation switch and force the model to generate, only simple paraphrasing abilities are revealed alongside factual inaccuracies and hallucinations. On an abstractivebiased dataset, the model copies infrequently but shows similarly limited abstractive abilities. In line with previous research, these results suggest that abstractive summarization models lack the semantic understanding necessary to generate paraphrases that are both abstractive and faithful to the source document",
    "checked": true,
    "id": "c513260095193c918419deb89c491aa19ab8491d",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Matt Wilber",
      "William Timkey",
      "Marten van Schijndel"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.299": {
    "title": "AgreeSum: Agreement-Oriented Multi-Document Summarization",
    "volume": "findings",
    "abstract": "We aim to renew interest in a particular multidocument summarization (MDS) task which we call AgreeSum: agreement-oriented multidocument summarization. Given a cluster of articles, the goal is to provide abstractive summaries that represent information common and faithful to all input articles. Given the lack of existing datasets, we create a dataset for AgreeSum, and provide annotations on article-summary entailment relations for a subset of the clusters in the dataset. We aim to create strong baselines for the task by applying the top-performing pretrained singledocument summarization model PEGASUS onto AgreeSum, leveraging both annotated clusters by supervised losses, and unannotated clusters by T5-based entailment-related and language-related losses. Compared to other baselines, both automatic evaluation and human evaluation show better article-summary and cluster-summary entailment in generated summaries. On a separate note, we hope that our article-summary entailment annotations contribute to the community's effort in improving abstractive summarization faithfulness",
    "checked": true,
    "id": "f87a921fe6984953634356b5cd895cc43dbdd2c9",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Richard Yuanzhe Pang",
      "Adam Lelkes",
      "Vinh Tran",
      "Cong Yu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.300": {
    "title": "BERT Busters: Outlier Dimensions that Disrupt Transformers",
    "volume": "findings",
    "abstract": "Multiple studies have shown that Transformers are remarkably robust to pruning. Contrary to this received wisdom, we demonstrate that pre-trained Transformer encoders are surprisingly fragile to the removal of a very small number of features in the layer outputs (<0.0001% of model weights). In case of BERT and other pre-trained encoder Transformers, the affected component is the scaling factors and biases in the LayerNorm. The outliers are high-magnitude normalization parameters that emerge early in pre-training and show up consistently in the same dimensional position throughout the model. We show that disabling them significantly degrades both the MLM loss and the downstream task performance. This effect is observed across several BERT-family models and other popular pre-trained Transformer architectures, including BART, XLNet and ELECTRA; we also show a similar effect in GPT-2",
    "checked": true,
    "id": "5a09edeb26f9f116f2c0503cd020f38fb943f79b",
    "semantic_title": "",
    "citation_count": 29,
    "authors": [
      "Olga Kovaleva",
      "Saurabh Kulshreshtha",
      "Anna Rogers",
      "Anna Rumshisky"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.301": {
    "title": "We will Reduce Taxes\" - Identifying Election Pledges with Language Models",
    "volume": "findings",
    "abstract": "In an election campaign, political parties pledge to implement various projects–should they be elected. But do they follow through? To track election pledges from parties' election manifestos, we need to distinguish between pledges and general statements. In this paper, we use election manifestos of Swedish and Indian political parties to learn neural models that distinguish actual pledges from generic political positions. Since pledges might vary by election year and party, we implement a Multi-Task Learning (MTL) setup, predicting election year and manifesto's party as auxiliary tasks. Pledges can also span several sentences, so we use hierarchical models that incorporate contextual information. Lastly, we evaluate the models in a Zero-Shot Learning (ZSL) framework across countries and languages. Our results indicate that year and party have predictive power even in ZSL, while context introduces some noise. We finally discuss the linguistic features of pledges",
    "checked": true,
    "id": "7eaad1f5b126839acb60b4bd2b91ca4b53d4aac0",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Tommaso Fornaciari",
      "Dirk Hovy",
      "Elin Naurin",
      "Julia Runeson",
      "Robert Thomson",
      "Pankaj Adhikari"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.302": {
    "title": "WeaQA: Weak Supervision via Captions for Visual Question Answering",
    "volume": "findings",
    "abstract": "Methodologies for training visual question answering (VQA) models assume the availability of datasets with human-annotated ImageQuestion-Answer (I-Q-A) triplets. This has led to heavy reliance on datasets and a lack of generalization to new types of questions and scenes. Linguistic priors along with biases and errors due to annotator subjectivity have been shown to percolate into VQA models trained on such samples. We study whether models can be trained without any human-annotated Q-A pairs, but only with images and their associated textual descriptions or captions. We present a method to train models with synthetic Q-A pairs generated procedurally from captions. Additionally, we demonstrate the efficacy of spatial-pyramid image patches as a simple but effective alternative to dense and costly object bounding box annotations used in existing VQA models. Our experiments on three VQA benchmarks demonstrate the efficacy of this weakly-supervised approach, especially on the VQA-CP challenge, which tests performance under changing linguistic priors",
    "checked": true,
    "id": "1a575075ba357723009a9a8905d5dccf9115ae6c",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Pratyay Banerjee",
      "Tejas Gokhale",
      "Yezhou Yang",
      "Chitta Baral"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.303": {
    "title": "How well do you know your summarization datasets?",
    "volume": "findings",
    "abstract": "State-of-the-art summarization systems are trained and evaluated on massive datasets scraped from the web. Despite their prevalence, we know very little about the underlying characteristics (data noise, summarization complexity, etc.) of these datasets, and how these affect system performance and the reliability of automatic metrics like ROUGE. In this study, we manually analyse 600 samples from three popular summarization datasets. Our study is driven by a six-class typology which captures different noise types (missing facts, entities) and degrees of summarization difficulty (extractive, abstractive). We follow with a thorough analysis of 27 state-of-the-art summarization models and 5 popular metrics, and report our key insights: (1) Datasets have distinct data quality and complexity distributions, which can be traced back to their collection process. (2) The performance of models and reliability of metrics is dependent on sample complexity. (3) Faithful summaries often receive low scores because of the poor diversity of references. We release the code, annotated data and model outputs.1",
    "checked": true,
    "id": "15bb07d0996ece844de8cae24d3dc15972e6841a",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Priyam Tejaswin",
      "Dhruv Naik",
      "Pengfei Liu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.304": {
    "title": "Multilingual Translation from Denoising Pre-Training",
    "volume": "findings",
    "abstract": "Recent work demonstrates the potential of training one model for multilingual machine translation. In parallel, denoising pretraining using unlabeled monolingual data as a starting point for finetuning bitext machine translation systems has demonstrated strong performance gains. However, little has been explored on the potential to combine denoising pretraining with multilingual machine translation in a single model. In this work, we fill this gap by studying how multilingual translation models can be created through multilingual finetuning. Fintuning multilingual model from a denoising pretrained model incorporates the benefits of large quantities of unlabeled monolingual data, which is particularly important for low resource languages where bitext is rare. Further, we create the ML50 benchmark to facilitate reproducible research by standardizing training and evaluation data. On ML50, we show that multilingual finetuning significantly improves over multilingual models trained from scratch and bilingual finetuning for translation into English. We also find that multilingual finetuning can significantly improve over multilingual models trained from scratch for zero-shot translation on non-English directions. Finally, we discuss that the pretraining and finetuning paradigm alone is not enough to address the challenges of multilingual models for to-Many directions performance",
    "checked": true,
    "id": "b88703f68f4abb9e69b86fb42dff85aa4a76fca4",
    "semantic_title": "",
    "citation_count": 72,
    "authors": [
      "Yuqing Tang",
      "Chau Tran",
      "Xian Li",
      "Peng-Jen Chen",
      "Naman Goyal",
      "Vishrav Chaudhary",
      "Jiatao Gu",
      "Angela Fan"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.305": {
    "title": "Annotations Matter: Leveraging Multi-task Learning to Parse UD and SUD",
    "volume": "findings",
    "abstract": "Using multiple treebanks to improve parsing performance has shown positive results. However, to what extent similar, yet competing annotation decisions play in parser behavior is unclear. We investigate this within a multi-task learning (MTL) dependency parser setup on two parallel treebanks, UD and SUD, which, while possessing similar annotation schemes, differ in specific linguistic annotation preferences. We perform a set of experiments with different MTL architectural choices, comparing performance across various input embeddings. We find languages tend to pattern in loose typological associations, but generally the performance within an MTL setting is lower than single model baseline parsers for each annotation scheme. The main contributing factor seems to be the competing syntactic annotation information shared between treebanks in an MTL setting, which is shown in experiments against differently annotated treebanks. This suggests that the impact of how the signal is encoded for annotations and its influence on possible negative transfer is more important than that of the input embeddings in an MTL setting",
    "checked": true,
    "id": "e7b1d326c5a0b0b7f2f886f3f0a58c2cce2e02e9",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Zeeshan Ali Sayyed",
      "Daniel Dakota"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.306": {
    "title": "Generating Informative Conclusions for Argumentative Texts",
    "volume": "findings",
    "abstract": "The purpose of an argumentative text is to support a certain conclusion. Yet, they are often omitted, expecting readers to infer them rather. While appropriate when reading an individual text, this rhetorical device limits accessibility when browsing many texts (e.g., on a search engine or on social media). In these scenarios, an explicit conclusion makes for a good candidate summary of an argumentative text. This is especially true if the conclusion is informative, emphasizing specific concepts from the text. With this paper we introduce the task of generating informative conclusions: First, WebisConcluGen-21 is compiled, a large-scale corpus of 136,996 samples of argumentative texts and their conclusions. Second, two paradigms for conclusion generation are investigated; one extractive, the other abstractive in nature. The latter exploits argumentative knowledge that augment the data via control codes and finetuning the BART model on several subsets of the corpus. Third, insights are provided into the suitability of our corpus for the task, the differences between the two generation paradigms, the trade-off between informativeness and conciseness, and the impact of encoding argumentative knowledge. The corpus, code, and the trained models are publicly available.1",
    "checked": true,
    "id": "4b905b2577962362764d263460034513f417c49e",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Shahbaz Syed",
      "Khalid Al Khatib",
      "Milad Alshomary",
      "Henning Wachsmuth",
      "Martin Potthast"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.307": {
    "title": "Substructure Substitution: Structured Data Augmentation for NLP",
    "volume": "findings",
    "abstract": "We study a family of data augmentation methods, substructure substitution (SUB), that generalizes prior methods. SUB generates new examples by substituting substructures (e.g., subtrees or subsequences) with others having the same label. This idea can be applied to many structured NLP tasks such as part-of-speech tagging and parsing. For more general tasks (e.g., text classification) which do not have explicitly annotated substructures, we present variations of SUB based on text spans or parse trees, introducing structureaware data augmentation methods to general NLP tasks. For most cases, training with a dataset augmented by SUB achieves better performance than training with the original training set. Further experiments show that SUB has more consistent performance than other investigated augmentation methods, across different tasks and sizes of the seed dataset.1",
    "checked": true,
    "id": "d6357b1c61611f744acbae69484acd7f21c89dff",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Haoyue Shi",
      "Karen Livescu",
      "Kevin Gimpel"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.308": {
    "title": "Towards Protecting Vital Healthcare Programs by Extracting Actionable Knowledge from Policy",
    "volume": "findings",
    "abstract": "In challenging economic times, obtaining value for money by ensuring financial integrity and fairer distribution of services are among the top priorities for social and health-care systems globally. However, healthcare billing policies are complex and identifying non-compliance is often narrow-scope, manual and expensive. Maintaining 'integrity' is a challenge ensuring that scarce resources get to those in need and are not lost to fraud and waste. Our approach fuses recent advances in dependency parsing with a policy ontology to convert the content of regulatory healthcare policy into human-friendly policy rules, that are amenable to machineexecution, with human oversight. We describe the ontology-guided transformation of textual patterns into a semantically-meaningful knowledge graph of rules, outline our experiments and evaluate results against policy rules obtained from professional investigators. The aim is to make a policy-compliance 'landscape' visible to healthcare programs helping them identify Fraud, Waste or Abuse",
    "checked": true,
    "id": "d7adb567dc176fd7db8e820caf6635482128b9d0",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vanessa Lopez",
      "Nagesh Yadav",
      "Gabriele Picco",
      "Inge Vejsbjerg",
      "Eoin Carrol",
      "Seamus Brady",
      "Marco Luca Sbodio",
      "Lam Thanh Hoang",
      "Miao Wei",
      "John Segrave"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.309": {
    "title": "Not Far Away, Not So Close: Sample Efficient Nearest Neighbour Data Augmentation via MiniMax",
    "volume": "findings",
    "abstract": "In Natural Language Processing (NLP), finding data augmentation techniques that can produce high-quality human-interpretable examples has always been challenging. Recently, leveraging kNN such that augmented examples are retrieved from large repositories of unlabelled sentences has made a step toward interpretable augmentation. Inspired by this paradigm, we introduce MiniMax-kNN, a sample efficient data augmentation strategy tailored for Knowledge Distillation (KD). We exploit a semi-supervised approach based on KD to train a model on augmented data. In contrast to existing kNN augmentation techniques that blindly incorporate all samples, our method dynamically selects a subset of augmented samples that maximizes KL-divergence between the teacher and student models. This step aims to extract the most efficient samples to ensure our augmented data covers regions in the input space with maximum loss value. We evaluated our technique on several text classification tasks and demonstrated that MiniMaxkNN consistently outperforms strong baselines. Our results show that MiniMax-kNN requires fewer augmented examples and less computation to achieve superior performance over the state-of-the-art kNN-based augmentation techniques",
    "checked": true,
    "id": "be4bc115836e0a1cdef82e26a586c9fe26a0d8a5",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Ehsan Kamalloo",
      "Mehdi Rezagholizadeh",
      "Peyman Passban",
      "Ali Ghodsi"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.310": {
    "title": "It's All in the Heads: Using Attention Heads as a Baseline for Cross-Lingual Transfer in Commonsense Reasoning",
    "volume": "findings",
    "abstract": "Commonsense reasoning is one of the key problems in natural language processing, but the relative scarcity of labeled data holds back the progress for languages other than English. Pretrained cross-lingual models are a source of powerful language-agnostic representations, yet their inherent reasoning capabilities are still actively studied. In this work, we design a simple approach to commonsense reasoning which trains a linear classiﬁer with weights of multi-head attention as features. To evaluate this approach, we create a multilingual Winograd Schema corpus by processing several datasets from prior work within a standardized pipeline and measure cross-lingual generalization ability in terms of out-of-sample performance. The method performs competitively with recent supervised and unsupervised approaches for commonsense reasoning, even when applied to other languages in a zero-shot manner. Also, we demonstrate that most of the performance is given by the same small subset of attention heads for all studied languages, which provides evidence of universal reasoning capabilities in multilingual encoders",
    "checked": true,
    "id": "64902a5077ee68011cd467398dbb66511e8e891a",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Alexey Tikhonov",
      "Max Ryabinin"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.311": {
    "title": "Biomedical Interpretable Entity Representations",
    "volume": "findings",
    "abstract": "Pre-trained language models induce dense entity representations that offer strong performance on entity-centric NLP tasks, but such representations are not immediately interpretable. This can be a barrier to model uptake in important domains such as biomedicine. There has been recent work on general interpretable representation learning (Onoe and Durrett, 2020), but these domain-agnostic representations do not readily transfer to the important domain of biomedicine. In this paper, we create a new entity type system and training set from a large corpus of biomedical texts by mapping entities to concepts in a medical ontology, and from these to Wikipedia pages whose categories are our types. From this mapping we derive Biomedical Interpretable Entity Representations (BIERs), in which dimensions correspond to fine-grained entity types, and values are predicted probabilities that a given entity is of the corresponding type. We propose a novel method that exploits BIER's final sparse and intermediate dense representations to facilitate model and entity type debugging. We show that BIERs achieve strong performance in biomedical tasks including named entity disambiguation and entity label classification, and we provide error analysis to highlight the utility of their interpretability, particularly in low-supervision settings. Finally, we provide our induced 68K biomedical type system, the corresponding 37 million triples of derived data used to train BIER models and our best performing model",
    "checked": true,
    "id": "bfa915760eec185cf7cac0eff56d3b434da91a79",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Diego Garcia-Olano",
      "Yasumasa Onoe",
      "Ioana Baldini",
      "Joydeep Ghosh",
      "Byron Wallace",
      "Kush Varshney"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.312": {
    "title": "Learning Robust Latent Representations for Controllable Speech Synthesis",
    "volume": "findings",
    "abstract": "State-of-the-art Variational Auto-Encoders (VAEs) for learning disentangled latent representations give impressive results in discovering features like pitch, pause duration, and accent in speech data, leading to highly controllable text-to-speech (TTS) synthesis. However, these LSTM-based VAEs fail to learn latent clusters of speaker attributes when trained on limited or noisy datasets. Further, different latent variables are found to encode the same features, limiting the control and expressiveness during speech synthesis. To resolve these issues, we propose REMMI (Reordered transformer Encoder with Minimal Mutual Information) where we minimize the mutual information between different latent variables and devise a modified Transformer architecture with layer reordering to learn controllable latent representations in speech data. We show that REMMI reduces the cluster overlap of speaker attributes by at least 30% over LSTM-VAE",
    "checked": true,
    "id": "c83110f84af2cecbd83ef4c49e729c706a6436b2",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Shakti Kumar",
      "Jithin Pradeep",
      "Hussain Zaidi"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.313": {
    "title": "How to Split: the Effect of Word Segmentation on Gender Bias in Speech Translation",
    "volume": "findings",
    "abstract": "Having recognized gender bias as a major issue affecting current translation technologies, researchers have primarily attempted to mitigate it by working on the data front. However, whether algorithmic aspects concur to exacerbate unwanted outputs remains so far under-investigated. In this work, we bring the analysis on gender bias in automatic translation onto a seemingly neutral yet critical component: word segmentation. Can segmenting methods influence the ability to translate gender? Do certain segmentation approaches penalize the representation of feminine linguistic markings? We address these questions by comparing 5 existing segmentation strategies on the target side of speech translation systems. Our results on two language pairs (EnglishItalian/French) show that state-of-the-art subword splitting (BPE) comes at the cost of higher gender bias. In light of this finding, we propose a combined approach that preserves BPE overall translation quality, while leveraging the higher ability of character-based segmentation to properly translate gender. Bias Statement.1 We study the effect of segmentation methods on the ability of speech translation (ST) systems to translate masculine and feminine forms referring to human entities. In this area, structural linguistic properties interact with the perception and representation of individuals (Gygax et al., 2019; Corbett, 2013; Stahlberg et al., 2007). Thus, we believe they are relevant gender expressions, used to communicate about the self and others, and by which the sociocultural and political reality of gender is negotiated (Hellinger and Motschenbacher, 2015). †The authors contributed equally. As suggested by (Blodgett et al., 2020) and required for other venues (Hardmeier et al., 2021), we formulate our bias statement. Accordingly, we consider a model that systematically and disproportionately favours masculine over feminine forms to be biased, as it fails to properly recognize women. From a technical perspective, such behaviour deteriorates models' performance. Most importantly, however, from a humancentered view, real-world harms are at stake (Crawford, 2017), as translation technologies are unequally beneficial across gender groups and reduce feminine visibility, thus contributing to misrepresent an already socially disadvantaged group. This work is motivated by the intent to shed light on whether issues in the generation of feminine forms are also a by-product of current algorithms and techniques. In our view, architectural improvements of ST systems should also account for the trade-offs between overall translation quality and gender representation: our proposal of a model that combines two segmentation techniques is a step",
    "checked": true,
    "id": "996f0d401acd11e95ce5586010e7e4e18f5c3bb9",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Marco Gaido",
      "Beatrice Savoldi",
      "Luisa Bentivogli",
      "Matteo Negri",
      "Marco Turchi"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.314": {
    "title": "On the Ethical Limits of Natural Language Processing on Legal Text",
    "volume": "findings",
    "abstract": "Natural language processing (NLP) methods for analyzing legal text offer legal scholars and practitioners a range of tools allowing to empirically analyze law on a large scale. However, researchers seem to struggle when it comes to identifying ethical limits to using NLP systems for acquiring genuine insights both about the law and the systems' predictive capacity. In this paper we set out a number of ways in which to think systematically about such issues. We place emphasis on three crucial normative parameters which have, to the best of our knowledge, been underestimated by current debates: (a) the importance of academic freedom, (b) the existence of a wide diversity of legal and ethical norms domestically but even more so internationally and (c) the threat of moralism in research related to computational law. For each of these three parameters we provide specific recommendations for the legal NLP community. Our discussion is structured around the study of a real-life scenario that has prompted recent debate in the legal NLP research community",
    "checked": true,
    "id": "1c2c7ca6436ebb3097c17cd14bd374a319ae4f8c",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Dimitrios Tsarapatsanis",
      "Nikolaos Aletras"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.315": {
    "title": "An Exploratory Analysis of the Relation between Offensive Language and Mental Health",
    "volume": "findings",
    "abstract": "In this paper, we analyze the interplay between the use of offensive language and mental health. We acquired publicly available datasets created for offensive language identification and depression detection and we train computational models to compare the use of offensive language in social media posts written by groups of individuals with and without self-reported depression diagnosis. We also look at samples written by groups of individuals whose posts show signs of depression according to recent related studies. Our analysis indicates that offensive language is more frequently used in the samples written by individuals with self-reported depression as well as individuals showing signs of depression. The results discussed here open new avenues in research in politeness/offensiveness and mental health",
    "checked": true,
    "id": "c301f1a644fc40552ff3cfae63b6afed3ba0779a",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Ana-Maria Bucur",
      "Marcos Zampieri",
      "Liviu P. Dinu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.316": {
    "title": "Transforming Term Extraction: Transformer-Based Approaches to Multilingual Term Extraction Across Domains",
    "volume": "findings",
    "abstract": "Automated Term Extraction (ATE), even though well-investigated, continues to be a challenging task. Approaches conventionally extract terms on corpus or document level and the benefits of neural models still remain underexplored with very few exceptions. We introduce three transformer-based term extraction models operating on sentence level: a language model for token classification, one for sequence classification, and an innovative use of Neural Machine Translation (NMT), which learns to reduce sentences to terms. All three models are trained and tested on the dataset of the ATE challenge TermEval 2020 in English, French, and Dutch across four specialized domains. The two best performing approaches are also evaluated on the ACL RD-TEC 2.0 dataset. Our models outperform previous baselines, one of which is BERT-based, by a substantial margin, with the token-classifier language model performing best",
    "checked": true,
    "id": "066f6096e566f0317737a69d3e7d60cb259930d3",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Christian Lang",
      "Lennart Wachowiak",
      "Barbara Heinisch",
      "Dagmar Gromann"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.317": {
    "title": "ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language",
    "volume": "findings",
    "abstract": "Transformers have been shown to emulate logical deduction over natural language theories (logical rules expressed in natural language), reliably assigning true/false labels to candidate implications. However, their ability to generate implications of a theory has not yet been demonstrated, and methods for reconstructing proofs of answers are imperfect. In this work we show that a generative model, called ProofWriter, can reliably generate both implications of a theory and the natural language proofs that support them. In particular, iterating a 1-step implication generator results in proofs that are highly reliable, and represent actual model decisions (rather than post-hoc rationalizations). On the RuleTaker dataset, the accuracy of ProofWriter's proofs exceed previous methods by +9% absolute, and in a way that generalizes to proof depths unseen in training and on out-of-domain problems. We also show that generative techniques can perform a type of abduction with high precision: Given a theory and an unprovable conclusion, identify a missing fact that allows the conclusion to be proved, along with a proof. These results significantly improve the viability of neural methods for systematically reasoning over natural language.1",
    "checked": true,
    "id": "87c45a908537ffe1d2ab71a5d609bd7b4efa4fe1",
    "semantic_title": "",
    "citation_count": 95,
    "authors": [
      "Oyvind Tafjord",
      "Bhavana Dalvi",
      "Peter Clark"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.318": {
    "title": "Probing Image-Language Transformers for Verb Understanding",
    "volume": "findings",
    "abstract": "Multimodal image–language transformers have achieved impressive results on a variety of tasks that rely on fine-tuning (e.g., visual question answering and image retrieval). We are interested in shedding light on the quality of their pretrained representations – in particular, if these models can distinguish different types of verbs or if they rely solely on nouns in a given sentence. To do so, we collect a dataset of image–sentence pairs (in English) consisting of 421 verbs that are either visual or commonly found in the pretraining data (i.e., the Conceptual Captions dataset). We use this dataset to evaluate pretrained image–language transformers and find that they fail more in situations that require verb understanding compared to other parts of speech. We also investigate what category of verbs are particularly challenging. 1 Evaluating Verb Understanding The success of image–language models in realworld applications relies on their ability to relate different aspects of language (such as verbs or objects) to images, which we refer to as multimodal understanding. For example, an image-retrieval model needs to distinguish between \"eating an apple\" and \"cutting an apple\" and a captioning model must accurately describe the actions in a scene. Previous work shows that image–language benchmarks do not always fully measure such multimodal understanding: object retrieval models fail to account for linguistic structure (Akula et al., 2020), visual question answering (VQA) models overly rely on language priors (Goyal et al., 2017; Agrawal et al., 2018), and captioning metrics do not always measure if captions \"hallucinate\" objects in an image (Rohrbach et al., 2018). Inspired by this, prior work introduced tasks to specifically examine whether models can relate objects to images (Shekhar et al., 2017) or classify frequent interactions associated with objects (Chao et al., 2015). However, both these datasets are limited to the 80 objects in the MSCOCO detection challenge (Lin et al., 2014). To address this gap, we design a benchmark focused on verbs called SVO-Probes for examining subject, verb, object triplets; more specifically, we collect a set of image–sentence pairs (in English) where each pair is annotated with whether the sentence corresponds to the image or not. As shown in Fig. 1, for a given sentence, in addition to a positive image that matches the sentence, our dataset includes controlled negative images that do not correspond to specific aspects of the sentence (i.e., subject, verb, and object). These controlled examples enable us to probe models for their understanding of verbs as well as subjects and objects. Our dataset consists of 421 verbs and includes over 48, 000 image–sentence pairs. We use our benchmark to evaluate the recent family of multimodal (image–language) transformers that have shown impressive results on benchmarks like VQA and image retrieval (Lu et al., 2019; Chen et al., 2020; Tan and Bansal, 2019; Li et al., 2020b,a; Huang et al., 2020). Our goal is to investigate if the good performance of these models is due to learned representations that successfully relate different aspects of language to images. More specifically, we evaluate a few architectural variations of these models in a zero-shot way by using the pretrained models to classify if image–sentence pairs from SVO-Probes match. Our results show that the performance of all evaluated models is worst on verbs, with subjects being easier than verbs but harder than objects. We find that this observation does not depend on the frequency of test examples in pretraining data. Moreover, it is considerably harder for all models to correctly classify image–sentence pairs that do not ar X iv :2 10 6. 09 14 1v 1 [ cs .C L ] 1 6 Ju n 20 21 A woman jogs on the beach. A man is jumping into the sea. A person sings at a concert. A man jumping into a river. A animal lays in the grass. Children cross the street. child, cross, street lady, cross, street animal, lay, grass woman, lay, grass person, sing, concert person, dance, concert man, jump, river man, kayak, river man, jump, sea man, jump, mountain woman, jog, beach woman, jog, forest Pos Neg",
    "checked": true,
    "id": "5c09c7b9d749e7a1f90573b0cfd53606f1038d73",
    "semantic_title": "",
    "citation_count": 40,
    "authors": [
      "Lisa Anne Hendricks",
      "Aida Nematzadeh"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.319": {
    "title": "Implications of Using Internet Sting Corpora to Approximate Underage Victims",
    "volume": "findings",
    "abstract": "Law enforcement officers (LEOs) and the justice system employ NLP models for classifying and triaging child exploitation cases due to the textual communications between predators and victims. The usefulness of these systems depend on the quality of data that can be used for training. Data in the domain are scarce, sensitive, and emotionally taxing for annotators. NLP researchers approximate victimization conversations using transcripts from internet stings performed by either vigilantes or LEOs, with an implicit assumption that vigilante or LEO conversations represent the victimization process. Psychology research, however, states that underage victim chats differ from internet stings in goal and modus operandi. We present a methodology and observations from annotating a corpus of victim, vigilante, and LEO conversations with convicted predators with the goal of comparing these chats. The corpus is annotated for stages and tactics of the victimization process described within psychology research. As predicted by psychological research, we found significant differences in the three classes of chats that are usually not taken into account in chat classification",
    "checked": true,
    "id": "90453b0e6f035d741449bc82abbc7ce5cbf915d1",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tatiana Ringenberg",
      "Kathryn Seigfried-Spellar",
      "Julia Rayz"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.320": {
    "title": "Detecting Domain Polarity-Changes of Words in a Sentiment Lexicon",
    "volume": "findings",
    "abstract": "Sentiment lexicons are instrumental for sentiment analysis. One can use a set of sentiment words provided in a sentiment lexicon and a lexicon-based classifier to perform sentiment classification. One major issue with this approach is that many sentiment words are domain dependent. That is, they may be positive in some domains but negative in some others. We refer to this problem as domain polarity-changes of words. Detecting such words and correcting their sentiment for an application domain is very important. In this paper, we propose a graph-based technique to tackle this problem. Experimental results show its effectiveness on multiple real-world datasets",
    "checked": true,
    "id": "02d22338bd95b829dcaafdf6766034a1f029b447",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Shuai Wang",
      "Guangyi Lv",
      "Sahisnu Mazumder",
      "Bing Liu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.321": {
    "title": "Analyzing Online Political Advertisements",
    "volume": "findings",
    "abstract": "Online political advertising is a central aspect of modern election campaigning for influencing public opinion. Computational analysis of political ads is of utmost importance in political science to understand characteristics of digital campaigning. It is also important in computational linguistics to study features of political discourse and communication on a large scale. In this work, we present the first computational study on online political ads with the aim to (1) infer the political ideology of an ad sponsor; and (2) identify whether the sponsor is an official political party or a thirdparty organization. We develop two new large datasets for the two tasks consisting of ads from the U.S.. Evaluation results show that our approach that combines textual and visual information from pre-trained neural models outperforms a state-of-the-art method for generic commercial ad classification. Finally, we provide an in-depth analysis of the limitations of our best performing models and a linguistic analysis to study the characteristics of political ads discourse.1",
    "checked": true,
    "id": "33316e3cba3cf40859648b542187337e4f6043af",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Danae Sánchez Villegas",
      "Saeid Mokaram",
      "Nikolaos Aletras"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.322": {
    "title": "Do Language Models Perform Generalizable Commonsense Inference?",
    "volume": "findings",
    "abstract": "Inspired by evidence that pretrained language models (LMs) encode commonsense knowledge, recent work has applied LMs to automatically populate commonsense knowledge graphs (CKGs). However, there is a lack of understanding on their generalization to multiple CKGs, unseen relations, and novel entities. This paper analyzes the ability of LMs to perform generalizable commonsense inference, in terms of knowledge capacity, transferability, and induction. Our experiments with these three aspects show that: (1) LMs can adapt to different schemas defined by multiple CKGs but fail to reuse the knowledge to generalize to new relations. (2) Adapted LMs generalize well to unseen subjects, but less so on novel objects. Future work should investigate how to improve the transferability and induction of commonsense mining from LMs.1",
    "checked": true,
    "id": "572b9183d3eaf45a31c9308f20e420c5f922588e",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Peifeng Wang",
      "Filip Ilievski",
      "Muhao Chen",
      "Xiang Ren"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.323": {
    "title": "Probing Multi-modal Machine Translation with Pre-trained Language Model",
    "volume": "findings",
    "abstract": "Multi-modal machine translation (MMT) aimed at using images to help disambiguate the target during translation and improving robustness, but some recent works showed that the contribution of visual features is either negligible or incremental. In this paper, we show that incorporating pre-trained (vision) language model (VLP) on the source side can improve the multi-modal translation quality significantly. Motivated by BERT, VLP aims to learn better cross-modal representations that improve target sequence generation. We simply adapt BERT to a cross-modal domain for the vision language pre-training, and the downstream multi-modal machine translation can substantially benefit from the pre-training. We also introduce an attention based modality loss to promote the image-text alignment in the latent semantic space. Ablation study verifies that it is effective in further improving the translation quality. Our experiments on the widely used Multi-30K dataset show increased BLEU score up to 6.2 points compared with the text-only model, achieving the state-of-the-art results with a large margin in the semi-unconstrained scenario and indicating a possible direction to rejuvenate the multi-modal machine translation",
    "checked": true,
    "id": "c0731ec306b3182834a55f85b712c9d6e1538529",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Kong Yawei",
      "Kai Fan"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.324": {
    "title": "The interplay between language similarity and script on a novel multi-layer Algerian dialect corpus",
    "volume": "findings",
    "abstract": "Recent years have seen a rise in interest for cross-lingual transfer between languages with similar typology, and between languages of various scripts. However, the interplay between language similarity and difference in script on cross-lingual transfer is a less studied problem. We explore this interplay on cross-lingual transfer for two supervised tasks, namely part-of-speech tagging and sentiment analysis. We introduce a newly annotated corpus of Algerian user-generated comments comprising parallel annotations of Algerian written in Latin, Arabic, and code-switched scripts, as well as annotations for sentiment and topic categories. We perform baseline experiments by fine-tuning multi-lingual language models. We further explore the effect of script vs. language similarity in cross-lingual transfer by fine-tuning multi-lingual models on languages which are a) typologically distinct, but use the same script, b) typologically similar, but use a distinct script, or c) are typologically similar and use the same script. We find there is a delicate relationship between script and typology for part-of-speech, while sentiment analysis is less sensitive",
    "checked": true,
    "id": "188f8a19dbfd2884ea818db822a17866e978ce75",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Samia Touileb",
      "Jeremy Barnes"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.325": {
    "title": "Few-Shot Upsampling for Protest Size Detection",
    "volume": "findings",
    "abstract": "We propose a new task and dataset for a common problem in social science research: \"upsampling\" coarse document labels to finegrained labels or spans. We pose the problem in a question answering format, with the answers providing the fine-grained labels. We provide a benchmark dataset and baselines on a socially impactful task: identifying the exact crowd size at protests and demonstrations in the United States given only orderof-magnitude information about protest attendance, a very small sample of fine-grained examples, and English-language news text. We evaluate several baseline models, including zero-shot results from rule-based and questionanswering models, few-shot models fine-tuned on a small set of documents, and weakly supervised models using a larger set of coarselylabeled documents. We find that our rulebased model initially outperforms a zero-shot pre-trained transformer language model but that further fine-tuning on a very small subset of 25 examples substantially improves out-ofsample performance. We also demonstrate a method for fine-tuning the transformer span on only the coarse labels that performs similarly to our rule-based approach. This work will contribute to social scientists' ability to generate data to understand the causes and successes of collective action",
    "checked": true,
    "id": "31129ea14f1143c7d37767927db702b85299745b",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Andrew Halterman",
      "Benjamin J. Radford"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.326": {
    "title": "Modeling the Unigram Distribution",
    "volume": "findings",
    "abstract": "The unigram distribution is the non-contextual probability of finding a specific word form in a corpus. While of central importance to the study of language, it is commonly approximated by each word's sample frequency in the corpus. This approach, being highly dependent on sample size, assigns zero probability to any out-of-vocabulary (oov) word form. As a result, it produces negatively biased probabilities for any oov word form, while positively biased probabilities to in-corpus words. In this work, we argue in favor of properly modeling the unigram distribution—claiming it should be a central task in natural language processing. With this in mind, we present a novel model for estimating it in a language (a neuralization of Goldwater et al.'s (2011) model) and show it produces much better estimates across a diverse set of 7 languages than the naı̈ve use of neural character-level language models",
    "checked": true,
    "id": "c4ff1df07755699b76c9f6a7891b6c67a15a320f",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Irene Nikkarinen",
      "Tiago Pimentel",
      "Damián Blasi",
      "Ryan Cotterell"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.327": {
    "title": "On the Lack of Robust Interpretability of Neural Text Classifiers",
    "volume": "findings",
    "abstract": "With the ever-increasing complexity of neural language models, practitioners have turned to methods for understanding the predictions of these models. One of the most welladopted approaches for model interpretability is feature-based interpretability, i.e., ranking the features in terms of their impact on model predictions. Several prior studies have focused on assessing the fidelity of feature-based interpretability methods, i.e., measuring the impact of dropping the top-ranked features on the model output. However, relatively little work has been conducted on quantifying the robustness of interpretations. In this work, we assess the robustness of interpretations of neural text classifiers, specifically, those based on pretrained Transformer encoders, using two randomization tests. The first compares the interpretations of two models that are identical except for their initializations. The second measures whether the interpretations differ between a model with trained parameters and a model with random parameters. Both tests show surprising deviations from expected behavior, raising questions about the extent of insights that practitioners may draw from interpretations",
    "checked": true,
    "id": "03360b29206604818c28afca3b70debb24fa372b",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Muhammad Bilal Zafar",
      "Michele Donini",
      "Dylan Slack",
      "Cedric Archambeau",
      "Sanjiv Das",
      "Krishnaram Kenthapadi"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.328": {
    "title": "Multimodal Graph-based Transformer Framework for Biomedical Relation Extraction",
    "volume": "findings",
    "abstract": "The recent advancement of pre-trained Transformer models has propelled the development of effective text mining models across various biomedical tasks. However, these models are primarily learned on the textual data and often lack the domain knowledge of the entities to capture the context beyond the sentence. In this study, we introduced a novel framework that enables the model to learn multi-omnics biological information about entities (proteins) with the help of additional multi-modal cues like molecular structure. Towards this, rather developing modality-specific architectures, we devise a generalized and optimized graph based multi-modal learning mechanism that utilizes the GraphBERT model to encode the textual and molecular structure information and exploit the underlying features of various modalities to enable the end-to-end learning. We evaluated our proposed method on ProteinProtein Interaction task from the biomedical corpus, where our proposed generalized approach is observed to be benefited by the additional domain-specific modality",
    "checked": true,
    "id": "71671dde0ea5e62a491062986b64db1f96b71295",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Sriram Pingali",
      "Shweta Yadav",
      "Pratik Dutta",
      "Sriparna Saha"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.329": {
    "title": "Summary Grounded Conversation Generation",
    "volume": "findings",
    "abstract": "Many conversation datasets have been constructed in the recent years using crowdsourcing. However, the data collection process can be time consuming and presents many challenges to ensure data quality. Since language generation has improved immensely in recent years with the advancement of pretrained language models, we investigate how such models can be utilized to generate entire conversations, given only a summary of a conversation as the input. We explore three approaches to generate summary grounded conversations, and evaluate the generated conversations using automatic measures and human judgements. We also show that the accuracy of conversation summarization can be improved by augmenting a conversation summarization dataset with generated conversations",
    "checked": true,
    "id": "225cbcb6ea433208ec1cf9eb2d55a14fa917dceb",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Chulaka Gunasekara",
      "Guy Feigenblat",
      "Benjamin Sznajder",
      "Sachindra Joshi",
      "David Konopnicki"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.330": {
    "title": "A Non-Autoregressive Edit-Based Approach to Controllable Text Simplification",
    "volume": "findings",
    "abstract": "We introduce a new approach for the task of Controllable Text Simplification, where systems rewrite a complex English sentence so that it can be understood by readers at different grade levels in the US K-12 system. It uses a non-autoregressive model to iteratively edit an input sequence and incorporates lexical complexity information seamlessly into the refinement process to generate simplifications that better match the desired output complexity than strong autoregressive baselines. Analysis shows that our model's local edit operations are combined to achieve more complex simplification operations such as content deletion and paraphrasing, as well as sentence splitting",
    "checked": true,
    "id": "749dc7adadc015e78e4c92a373e859fb4922e092",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Sweta Agrawal",
      "Weijia Xu",
      "Marine Carpuat"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.331": {
    "title": "Investigating Transfer Learning in Multilingual Pre-trained Language Models through Chinese Natural Language Inference",
    "volume": "findings",
    "abstract": "Multilingual transformers (XLM, mT5) have been shown to have remarkable transfer skills in zero-shot settings. Most transfer studies, however, rely on automatically translated resources (XNLI, XQuAD), making it hard to discern the particular linguistic knowledge that is being transferred, and the role of expert annotated monolingual datasets when developing task-specific models. We investigate the cross-lingual transfer abilities of XLM-R for Chinese and English natural language inference (NLI), with a focus on the recent largescale Chinese dataset OCNLI. To better understand linguistic transfer, we created 4 categories of challenge and adversarial tasks (totaling 17 new datasets1) for Chinese that build on several well-known resources for English (e.g., HANS, NLI stress-tests). We find that cross-lingual models trained on English NLI do transfer well across our Chinese tasks (e.g., in 3/4 of our challenge categories, they perform as well/better than the best monolingual models, even on 3/5 uniquely Chinese linguistic phenomena such as idioms, pro drop). These results, however, come with important caveats: cross-lingual models often perform best when trained on a mixture of English and high-quality monolingual NLI data (OCNLI), and are often hindered by automatically translated resources (XNLI-zh). For many phenomena, all models continue to struggle, highlighting the need for our new diagnostics to help benchmark Chinese and cross-lingual models",
    "checked": true,
    "id": "1cde1aa4f7bcebc47b35518cec452893ea6b824c",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Hai Hu",
      "He Zhou",
      "Zuoyu Tian",
      "Yiwen Zhang",
      "Yina Patterson",
      "Yanting Li",
      "Yixin Nie",
      "Kyle Richardson"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.332": {
    "title": "Using surprisal and fMRI to map the neural bases of broad and local contextual prediction during natural language comprehension",
    "volume": "findings",
    "abstract": "Context guides comprehenders' expectations during language processing, and informationtheoretic surprisal is commonly used as an index of cognitive processing effort. However, prior work using surprisal has considered only within-sentence context, using n-grams, neural language models, or syntactic structure as conditioning context. In this paper, we extend the surprisal approach to use broader topical context, investigating the influence of local and topical context on processing via an analysis of fMRI time courses collected during naturalistic listening. Lexical surprisal calculated from ngram and LSTM language models is used to capture effects of local context; to capture the effects of broader context a new metric based on topic models, topical surprisal, is introduced. We identify distinct patterns of neural activation for lexical surprisal and topical surprisal. These differing neuro-anatomical correlates suggest that local and broad contextual cues during sentence processing recruit different brain regions and that those regions of the language network functionally contribute to processing different dimensions of contextual information during comprehension. More generally, our approach adds to a growing literature using methods from computational linguistics to operationalize and test hypotheses about neuro-cognitive mechanisms in sentence processing",
    "checked": true,
    "id": "9f8f19ebfe3b760967445ed23ad144f90faf4316",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shohini Bhattasali",
      "Philip Resnik"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.333": {
    "title": "Assessing the Syntactic Capabilities of Transformer-based Multilingual Language Models",
    "volume": "findings",
    "abstract": "Multilingual Transformer-based language models, usually pretrained on more than 100 languages, have been shown to achieve outstanding results in a wide range of crosslingual transfer tasks. However, it remains unknown whether the optimization for different languages conditions the capacity of the models to generalize over syntactic structures, and how languages with syntactic phenomena of different complexity are affected. In this work, we explore the syntactic generalization capabilities of the monolingual and multilingual versions of BERT and RoBERTa. More specifically, we evaluate the syntactic generalization potential of the models on English and Spanish tests, comparing the syntactic abilities of monolingual and multilingual models on the same language (English), and of multilingual models on two different languages (English and Spanish). For English, we use the available SyntaxGym test suite; for Spanish, we introduce SyntaxGymES, a novel ensemble of targeted syntactic tests in Spanish, designed to evaluate the syntactic generalization capabilities of language models through the SyntaxGym online platform",
    "checked": true,
    "id": "ab8ea4bf7b58edea54edbd179c48bbc24b8aeadf",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Laura Pérez-Mayos",
      "Alba Táboas García",
      "Simon Mille",
      "Leo Wanner"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.334": {
    "title": "Are Larger Pretrained Language Models Uniformly Better? Comparing Performance at the Instance Level",
    "volume": "findings",
    "abstract": "Larger language models have higher accuracy on average, but are they better on every single instance (datapoint)? Some work suggests larger models have higher out-ofdistribution robustness, while other work suggests they have lower accuracy on rare subgroups. To understand these differences, we investigate these models at the level of individual instances. However, one major challenge is that individual predictions are highly sensitive to noise in the randomness in training. We develop statistically rigorous methods to address this, and after accounting for pretraining and finetuning noise, we find that our BERT-LARGE is worse than BERT-MINI on at least 1−4% of instances across MNLI, SST-2, and QQP, compared to the overall accuracy improvement of 2−10%. We also find that finetuning noise increases with model size, and that instance-level accuracy has momentum: improvement from BERT-MINI to BERT-MEDIUM correlates with improvement from BERT-MEDIUM to BERT-LARGE . Our findings suggest that instance-level predictions provide a rich source of information; we therefore recommend that researchers supplement model weights with model predictions",
    "checked": true,
    "id": "6a8cb4fb5a20c7e5733a9bd50cd5feaad6c11360",
    "semantic_title": "",
    "citation_count": 25,
    "authors": [
      "Ruiqi Zhong",
      "Dhruba Ghosh",
      "Dan Klein",
      "Jacob Steinhardt"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.335": {
    "title": "Named Entity Recognition through Deep Representation Learning and Weak Supervision",
    "volume": "findings",
    "abstract": "Weakly supervised methods estimate the labels for a dataset using the predictions of several noisy supervision sources. Many machine learning practitioners have begun using weak supervision to more quickly and cheaply annotate data compared to traditional manual labeling. In this paper, we focus on the specific problem of weakly supervised named entity recognition (NER) and propose an endto-end model to learn optimal assignments of latent NER tags using observed tokens and weak labels provided by labeling functions. To capture the sequential dependencies between the latent and observed variables, we propose a sequential graphical model where the components are approximated using neural networks. State-of-the-art contextual embeddings are used to further discriminate the quality of noisy weak labels in various contexts. Results of experiments on four public weakly supervised named entity recognition datasets show a significant improvement in F1 score over recent approaches",
    "checked": true,
    "id": "8797767dc66f7d0acb50b01276a3e3b378c79b0c",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Jerrod Parker",
      "Shi Yu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.336": {
    "title": "Explaining NLP Models via Minimal Contrastive Editing (MiCE)",
    "volume": "findings",
    "abstract": "Humans have been shown to give contrastive explanations, which explain why an observed event happened rather than some other counterfactual event (the contrast case). Despite the influential role that contrastivity plays in how humans explain, this property is largely missing from current methods for explaining NLP models. We present MINIMAL CONTRASTIVE EDITING (MICE), a method for producing contrastive explanations of model predictions in the form of edits to inputs that change model outputs to the contrast case. Our experiments across three tasks—binary sentiment classification, topic classification, and multiple-choice question answering—show that MICE is able to produce edits that are not only contrastive, but also minimal and fluent, consistent with human contrastive edits. We demonstrate how MICE edits can be used for two use cases in NLP system development—debugging incorrect model outputs and uncovering dataset artifacts—and thereby illustrate that producing contrastive explanations is a promising research direction for model interpretability",
    "checked": true,
    "id": "84f52e131e31a47bd0d9f40a2f9fbc770024b9c9",
    "semantic_title": "",
    "citation_count": 65,
    "authors": [
      "Alexis Ross",
      "Ana Marasović",
      "Matthew Peters"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.337": {
    "title": "Differential Privacy for Text Analytics via Natural Text Sanitization",
    "volume": "findings",
    "abstract": "Texts convey sophisticated knowledge. However, texts also convey sensitive information. Despite the success of general-purpose language models and domain-specific mechanisms with differential privacy (DP), existing text sanitization mechanisms still provide low utility, as cursed by the high-dimensional text representation. The companion issue of utilizing sanitized texts for downstream analytics is also under-explored. This paper takes a direct approach to text sanitization. Our insight is to consider both sensitivity and similarity via our new local DP notion. The sanitized texts also contribute to our sanitization-aware pretraining and fine-tuning, enabling privacypreserving natural language processing over the BERT language model with promising utility. Surprisingly, the high utility does not boost up the success rate of inference attacks",
    "checked": true,
    "id": "ff9d04fc15a2c52d982b5b7daa787a373ed7f899",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Xiang Yue",
      "Minxin Du",
      "Tianhao Wang",
      "Yaliang Li",
      "Huan Sun",
      "Sherman S. M. Chow"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.338": {
    "title": "Synthesizing Adversarial Negative Responses for Robust Response Ranking and Evaluation",
    "volume": "findings",
    "abstract": "Open-domain neural dialogue models have achieved high performance in response ranking and evaluation tasks. These tasks are formulated as a binary classification of responses given in a dialogue context, and models generally learn to make predictions based on context-response content similarity. However, over-reliance on content similarity makes the models less sensitive to the presence of inconsistencies, incorrect time expressions and other factors important for response appropriateness and coherence. We propose approaches for automatically creating adversarial negative training data to help ranking and evaluation models learn features beyond content similarity. We propose mask-and-fill and keyword-guided approaches that generate negative examples for training more robust dialogue systems. These generated adversarial responses have high content similarity with the contexts but are either incoherent, inappropriate or not fluent. Our approaches are fully data-driven and can be easily incorporated in existing models and datasets. Experiments on classification, ranking and evaluation tasks across multiple datasets demonstrate that our approaches outperform strong baselines in providing informative negative examples for training dialogue systems.1",
    "checked": true,
    "id": "d08a6a41e2b16928a1dc93b259bffbe37dae021d",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Prakhar Gupta",
      "Yulia Tsvetkov",
      "Jeffrey Bigham"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.339": {
    "title": "Leveraging Abstract Meaning Representation for Knowledge Base Question Answering",
    "volume": "findings",
    "abstract": "Knowledge base question answering (KBQA) is an important task in Natural Language Processing. Existing approaches face significant challenges including complex question understanding, necessity for reasoning, and lack of large end-to-end training datasets. In this work, we propose Neuro-Symbolic Question Answering (NSQA), a modular KBQA system, that leverages (1) Abstract Meaning Representation (AMR) parses for task-independent question understanding; (2) a simple yet effective graph transformation approach to convert AMR parses into candidate logical queries that are aligned to the KB; (3) a pipeline-based approach which integrates multiple, reusable modules that are trained specifically for their individual tasks (semantic parser, entity and relationship linkers, and neuro-symbolic reasoner) and do not require end-to-end training data. NSQA achieves state-of-the-art performance on two prominent KBQA datasets based on DBpedia (QALD-9 and LC-QuAD 1.0). Furthermore, our analysis emphasizes that AMR is a powerful tool for KBQA systems",
    "checked": true,
    "id": "6b1c351c7969e70daf13b4af83ac256c45041074",
    "semantic_title": "",
    "citation_count": 54,
    "authors": [
      "Pavan Kapanipathi",
      "Ibrahim Abdelaziz",
      "Srinivas Ravishankar",
      "Salim Roukos",
      "Alexander Gray",
      "Ramón Fernandez Astudillo",
      "Maria Chang",
      "Cristina Cornelio",
      "Saswati Dana",
      "Achille Fokoue",
      "Dinesh Garg",
      "Alfio Gliozzo",
      "Sairam Gurajada",
      "Hima Karanam",
      "Naweed Khan",
      "Dinesh Khandelwal",
      "Young-Suk Lee",
      "Yunyao Li",
      "Francois Luus",
      "Ndivhuwo Makondo",
      "Nandana Mihindukulasooriya",
      "Tahira Naseem",
      "Sumit Neelam",
      "Lucian Popa",
      "Revanth Gangi Reddy",
      "Ryan Riegel",
      "Gaetano Rossiello",
      "Udit Sharma",
      "G P Shrivatsa Bhargav",
      "Mo Yu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.340": {
    "title": "On the Gap between Adoption and Understanding in NLP",
    "volume": "findings",
    "abstract": "There are some issues with current research trends in NLP that can hamper the free development of scientific research. We identify five of particular concern: 1) the early adoption of methods without sufficient understanding or analysis; 2) the preference for computational methods regardless of risks associated with their limitations; 3) the resulting bias in the papers we publish; 4) the impossibility of re-running some experiments due to their cost; 5) the dangers of unexplainable methods. If these issues are not addressed, we risk a loss of reproducibility, reputability, and subsequently public trust in our field. In this position paper, we outline each of these points and suggest ways forward",
    "checked": true,
    "id": "67a2a1c14d795f00ef97026e9bf8320c53537ba0",
    "semantic_title": "",
    "citation_count": 18,
    "authors": [
      "Federico Bianchi",
      "Dirk Hovy"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.341": {
    "title": "Learning Disentangled Latent Topics for Twitter Rumour Veracity Classification",
    "volume": "findings",
    "abstract": "With the rapid growth of social media in the past decade, the news are no longer controlled by just a few mainstream sources. Users themselves create large numbers of potentially fictitious rumours, necessitating automated veracity classification systems. Here we present a novel approach towards automatically classifying rumours circulating on Twitter with respect to their veracity. We use a model built on Variational Autoencoder which disentangles the informational content of a tweet from the manner in which the information is written. This is achieved by obtaining latent topic vectors in an adversarial learning setting using the auxiliary task of stance classification. The latent vectors learnt in this way are used to predict rumour veracity, obtaining state-of-the-art accuracy scores on the PHEME dataset.1",
    "checked": true,
    "id": "6260975a9a50ab68f136ab79f4a912e253aa2680",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "John Dougrez-Lewis",
      "Maria Liakata",
      "Elena Kochkina",
      "Yulan He"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.342": {
    "title": "Perceptual Models of Machine-Edited Text",
    "volume": "findings",
    "abstract": "We introduce a novel dataset of human judgments of machine-edited text and initial models of those perceptions. Six machine-editing methods ranging from character swapping to variational autoencoders are applied to collections of English-language social media text and scientific abstracts. The edits are judged in context for detectability and the extent to which they preserve the meaning of the original. Automated measures of semantic similarity and fluency are evaluated individually and combined to produce composite models of human perception. Both meaning preservation and detectability are predicted within 6% of the upper bound of human consensus labeling",
    "checked": true,
    "id": "a4e50201369c70f25e82d005ca4fe754086dc9a9",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elizabeth Merkhofer",
      "Monica-Ann Mendoza",
      "Rebecca Marvin",
      "John Henderson"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.343": {
    "title": "Scaling Within Document Coreference to Long Texts",
    "volume": "findings",
    "abstract": "State of the art end-to-end coreference resolution models use expensive span representations and antecedent prediction mechanisms. These approaches are expensive both in terms of their memory requirements as well as compute time, and are particularly ill-suited for long documents. In this paper, we propose an approximation to end-to-end models which scales gracefully to documents of any length. Replacing span representations with token representations, we reduce the time/memory complexity via token windows and nearest neighbor sparsification methods for more efficient antecedent prediction. We show our approach's resulting reduction of training and inference time compared to state-of-the-art methods with only a minimal loss in accuracy",
    "checked": true,
    "id": "903703495a11d17811581fb41a291a6c3539adc8",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Raghuveer Thirukovalluru",
      "Nicholas Monath",
      "Kumar Shridhar",
      "Manzil Zaheer",
      "Mrinmaya Sachan",
      "Andrew McCallum"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.344": {
    "title": "LEWIS: Levenshtein Editing for Unsupervised Text Style Transfer",
    "volume": "findings",
    "abstract": "Many types of text style transfer can be achieved with only small, precise edits (e.g. sentiment transfer from I had a terrible time... to I had a great time...). We propose a coarse-to-fine editor for style transfer that transforms text using Levenshtein edit operations (e.g. insert, replace, delete). Unlike prior single-span edit methods, our method concurrently edits multiple spans in the source text. To train without parallel style text pairs (e.g. pairs of +/sentiment statements), we propose an unsupervised data synthesis procedure. We first convert text to style-agnostic templates using style classifier attention (e.g. I had a SLOT time...), then fill in slots in these templates using fine-tuned pretrained language models. Our method outperforms existing generation and editing style transfer methods on sentiment (YELP, AMAZON) and politeness (POLITE) transfer. In particular, multi-span editing achieves higher performance and more diverse output than single-span editing. Moreover, compared to previous methods on unsupervised data synthesis, our method results in higher quality parallel style pairs and improves model performance.1",
    "checked": true,
    "id": "fd90d2d2853c5b550eab7db203db9f4e7e5a2aaa",
    "semantic_title": "",
    "citation_count": 37,
    "authors": [
      "Machel Reid",
      "Victor Zhong"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.345": {
    "title": "Constructing Flow Graphs from Procedural Cybersecurity Texts",
    "volume": "findings",
    "abstract": "Following procedural texts written in natural languages is challenging. We must read the whole text to identify the relevant information or identify the instruction-flow to complete a task, which is prone to failures. If such texts are structured, we can readily visualize instruction-flows, reason or infer a particular step, or even build automated systems to help novice agents achieve a goal. However, this structure recovery task is a challenge because of such texts' diverse nature. This paper proposes to identify relevant information from such texts and generate information flows between sentences. We built a large annotated procedural text dataset (CTFW) in the cybersecurity domain (3154 documents). This dataset contains valuable instructions regarding software vulnerability analysis experiences. We performed extensive experiments on CTFW with our LM-GNN model variants in multiple settings. To show the generalizability of both this task and our method, we also experimented with procedural texts from two other domains (Maintenance Manual and Cooking), which are substantially different from cybersecurity. Our experiments show that Graph Convolution Network with BERT sentence embeddings outperforms BERT in all three domains",
    "checked": true,
    "id": "4d243265ce959caf07e5f882fc703c34b75b86f2",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Kuntal Kumar Pal",
      "Kazuaki Kashihara",
      "Pratyay Banerjee",
      "Swaroop Mishra",
      "Ruoyu Wang",
      "Chitta Baral"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.346": {
    "title": "Cluster-Former: Clustering-based Sparse Transformer for Question Answering",
    "volume": "findings",
    "abstract": "Transformer has become ubiquitous in the deep learning field. One of the key ingredients that destined its success is the self-attention mechanism, which allows fully-connected contextual encoding over input tokens. However, despite its effectiveness in modeling short sequences, self-attention suffers when handling inputs with extreme long-range dependencies, as its complexity grows quadratically w.r.t. the sequence length. Therefore, long sequences are often encoded by Transformer in chunks using a sliding window. In this paper, we propose Cluster-Former, a novel clusteringbased sparse Transformer to perform attention across chunked sequences. The proposed framework is pivoted on two unique types of Transformer layer: Sliding-Window Layer and Cluster-Former Layer, which encode local sequence information and global context jointly and iteratively. This new design allows information integration beyond local windows, which is especially beneficial for question answering (QA) tasks that rely on long-range dependencies. Experiments show that ClusterFormer achieves state-of-the-art performance on several major QA benchmarks",
    "checked": true,
    "id": "60e292b7cef1507cb779385080cfaeac8849a78e",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Shuohang Wang",
      "Luowei Zhou",
      "Zhe Gan",
      "Yen-Chun Chen",
      "Yuwei Fang",
      "Siqi Sun",
      "Yu Cheng",
      "Jingjing Liu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.347": {
    "title": "Minimally-Supervised Morphological Segmentation using Adaptor Grammars with Linguistic Priors",
    "volume": "findings",
    "abstract": "With the increasing interest in low-resource languages, unsupervised morphological segmentation has become an active area of research, where approaches based on Adaptor Grammars achieve state-of-the-art results. We demonstrate the power of harnessing linguistic knowledge as priors within Adaptor Grammars in a minimally-supervised learning fashion. We introduce two types of priors: 1) grammar definition, where we design language-specific grammars; and 2) linguistprovided affixes, collected by an expert in the language and seeded into the grammars. We use Japanese and Georgian as respective case studies for the two types of priors and introduce new datasets for these languages, with gold morphological segmentation for evaluation. We show that the use of priors results in error reductions of 8.9 % and 34.2 %, respectively, over the equivalent state-of-the-art unsupervised system",
    "checked": true,
    "id": "9e542485a262ca6380d0df43a0a80aea6d1fd675",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Ramy Eskander",
      "Cass Lowry",
      "Sujay Khandagale",
      "Francesca Callejas",
      "Judith Klavans",
      "Maria Polinsky",
      "Smaranda Muresan"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.348": {
    "title": "Multi-Task Learning and Adapted Knowledge Models for Emotion-Cause Extraction",
    "volume": "findings",
    "abstract": "Detecting what emotions are expressed in text is a well-studied problem in natural language processing. However, research on finer grained emotion analysis such as what causes an emotion is still in its infancy. We present solutions that tackle both emotion recognition and emotion cause detection in a joint fashion. Considering that common-sense knowledge plays an important role in understanding implicitly expressed emotions and the reasons for those emotions, we propose novel methods that combine common-sense knowledge via adapted knowledge models with multi-task learning to perform joint emotion classification and emotion cause tagging. We show performance improvement on both tasks when including common-sense reasoning and a multitask framework. We provide a thorough analysis to gain insights into model performance",
    "checked": true,
    "id": "241abdd1a8f2e98b0b0dbb04df54a4795d799ffe",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Elsbeth Turcan",
      "Shuai Wang",
      "Rishita Anubhai",
      "Kasturi Bhattacharjee",
      "Yaser Al-Onaizan",
      "Smaranda Muresan"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.349": {
    "title": "The Utility and Interplay of Gazetteers and Entity Segmentation for Named Entity Recognition in English",
    "volume": "findings",
    "abstract": "Recent papers have introduced methods to incorporate gazetteer features and entity segmentation techniques in neural named entity recognition models. These papers rely on different resources and include features not related to the use of gazetteers, rendering impossible the comparison of the relative effectiveness of the approaches. Here, we provide a comprehensive overview of methods for incorporating gazetteers and for entity segmentation. We evaluate representative methods from each in similar settings for a fair comparison and identify the ones that are consistently better across datasets and input representations. We further show that gazetteers improve entity segmentation and not just entity typing. Hence, we explore their utility in recognizing long entities, a problem for which entity segmentation techniques were developed. Our work explains the mechanisms via which gazetteers improve the performance of neural NER models",
    "checked": true,
    "id": "7fdb0c6d5cbb4bbee78a306554e5937b67a3e491",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Oshin Agarwal",
      "Ani Nenkova"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.350": {
    "title": "On the Cost-Effectiveness of Stacking of Neural and Non-Neural Methods for Text Classification: Scenarios and Performance Prediction",
    "volume": "findings",
    "abstract": "Nowadays, neural networks algorithms, such as those based on Attention and Transformers, have excelled on Automatic Text Classification (ATC). However, such enhanced performance comes at high computational costs. Stacking of simpler classifiers that exploit algorithmic and representational complementarity has also been shown to produce superior performance in ATC, enjoying high effectiveness and potentially lower computational costs than complex neural networks. In this master's thesis, we present the first and largest comparative study to exploit the cost-effectiveness of Stacking in ATC, consisting of Transformers and non-neural algorithms. In particular, we are interested in answering the following research question: Is it possible to obtain an effective ensemble with significantly less computational cost than the best learning model for a given dataset? Besides answering that question, another main contribution of this thesis is the proposal of a low-cost oracle-based method that can predict the best ensemble in each scenario using only a fraction of the training data",
    "checked": true,
    "id": "7af9d490ebe4fef40ba125e67056beee825b3b1c",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Christian Gomes",
      "Marcos Goncalves",
      "Leonardo Rocha",
      "Sergio Canuto"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.351": {
    "title": "Unsupervised Domain Adaptation for Event Detection using Domain-specific Adapters",
    "volume": "findings",
    "abstract": "Due to the multi-dimensional variation of textual data, detection of event triggers from new domains can become a lot more challenging. This prompts a need to research on domain adaptation methods for event detection task, especially for the most practical unsupervised setting. Recently, large transformer-based language models, e.g. BERT, have become essential to achieve top performance for event detection. However, their unwieldy nature also prevents effective adaptation across domains. To this end, this work proposes a Domain-specific Adapter-based Adaptation (DAA) framework to improve the adaptability of BERT-based models for event detection across domains. By explicitly representing data from different domains with separate adapter modules in each layer of BERT, DAA introduces a novel joint representation learning mechanism and a Wasserstein distance-based technique for data selection in adversarial learning to substantially boost the performance on target domains. Extensive experiments and analysis over different datasets (i.e., LitBank, TimeBank, and ACE-05) demonstrate the effectiveness of our approach",
    "checked": true,
    "id": "4bedcae34d4526ea9fb7220eb6a90218673e2288",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Nghia Ngo Trung",
      "Duy Phung",
      "Thien Huu Nguyen"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.352": {
    "title": "Predicting in-hospital mortality by combining clinical notes with time-series data",
    "volume": "findings",
    "abstract": "In intensive care units (ICUs), patient health is monitored through (1) continuous vital signals from various medical devices, and (2) clinical notes consisting of opinions and summaries from doctors which are recorded in electronic health records (EHR). It is difficult to jointly model these two sources of information because clinical notes, unlike vital signals, are collected at irregular intervals and their contents are relatively unstructured. In this paper, we present a model that combines both sources of information about ICU patients to make accurate in-hospital mortality predictions. We apply a fine-tuned BERT model to each of the patient's clinical notes. The resulting embeddings are then combined to obtain the overall embedding for the entire text part of the data. This is then combined with the output of an LSTM model that encodes patients' vital signals. Our model improves upon the state of the art for mortality prediction, attaining an AUC score of 0.9, compared to the previous 0.87, setting a new standard for mortality prediction on the MIMIC III benchmark.1",
    "checked": true,
    "id": "4d7a50f6cfd8f27ebd4d5201fad6c5ef42c33733",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Iman Deznabi",
      "Mohit Iyyer",
      "Madalina Fiterau"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.353": {
    "title": "Sequence Models for Computational Etymology of Borrowings",
    "volume": "findings",
    "abstract": "We computationally model the processes of word borrowing from a donor word to an incorporated word, and vice versa, by answering two questions: (1) what does a word look like incorporated into another language, and in the opposite direction (2) where did a word come from? We employ neural sequence models, focusing on six specific borrowing relations: calques, partial calques, semantic loans, phono-semantic matches, transliterations, and generic borrowings. We experiment with several model variants, including LSTM encoderdecoders, copy attention, and Transformers. In both directions, we find that an LSTM model can beat strong baselines, with the quantity of data strongly influencing model performance",
    "checked": true,
    "id": "7569d4b5ab8f609dadb7a1f4c4839327b6b36de7",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Winston Wu",
      "Kevin Duh",
      "David Yarowsky"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.354": {
    "title": "Learning Contextualized Knowledge Structures for Commonsense Reasoning",
    "volume": "findings",
    "abstract": "Recently, neural-symbolic architectures have achieved success on commonsense reasoning through effectively encoding relational structures retrieved from external knowledge graphs (KGs) and obtained state-of-the-art results in tasks such as (commonsense) question answering and natural language inference. However, these methods rely on quality and contextualized knowledge structures (i.e., fact triples) that are retrieved at the pre-processing stage but overlook challenges caused by incompleteness of a KG, limited expressiveness of its relations, and retrieved facts irrelevant to the reasoning context. In this paper, we present a novel neural-symbolic model, named Hybrid Graph Network (HGN), which jointly generates feature representations for new triples (as a complement to existing edges in the KG), determines the relevance of the triples to the reasoning context, and learns graph module parameters for encoding the relational information. Our model learns a compact graph structure (comprising both extracted and generated edges) through filtering edges that are unhelpful to the reasoning process. We show marked improvement on three commonsense reasoning benchmarks and demonstrate the superiority of the learned graph structures with user studies",
    "checked": true,
    "id": "b393a7a481345e8a5567d8fe56209a1c1fe88d04",
    "semantic_title": "",
    "citation_count": 24,
    "authors": [
      "Jun Yan",
      "Mrigank Raman",
      "Aaron Chan",
      "Tianyu Zhang",
      "Ryan Rossi",
      "Handong Zhao",
      "Sungchul Kim",
      "Nedim Lipka",
      "Xiang Ren"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.355": {
    "title": "Analyzing Stereotypes in Generative Text Inference Tasks",
    "volume": "findings",
    "abstract": "Stereotypes are inferences drawn about people based on their demographic attributes, which may result in harms to users when a system is deployed. In generative language-inference tasks, given a premise, a model produces plausible hypotheses that follow either logically (natural language inference) or commonsensically (commonsense inference). Such tasks are therefore a fruitful setting in which to explore the degree to which NLP systems encode stereotypes. In our work, we study how stereotypes manifest when the potential targets of stereotypes are situated in real-life, neutral contexts. We collect human judgments on the presence of stereotypes in generated inferences, and compare how perceptions of stereotypes vary due to annotator positionality",
    "checked": true,
    "id": "54db327cd53fe043449c9f242d3fc34c593a70ef",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Anna Sotnikova",
      "Yang Trista Cao",
      "Hal Daumé III",
      "Rachel Rudinger"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.356": {
    "title": "HySPA: Hybrid Span Generation for Scalable Text-to-Graph Extraction",
    "volume": "findings",
    "abstract": "Text-to-Graph extraction aims to automatically extract information graphs consisting of mentions and types from natural language texts. Existing approaches, such as table filling and pairwise scoring, have shown impressive performance on various information extraction tasks, but they are difficult to scale to datasets with longer input texts because of their secondorder space/time complexities with respect to the input length. In this work, we propose a Hybrid SPan GenerAtor (HySPA) that invertibly maps the information graph to an alternating sequence of nodes and edge types, and directly generates such sequences via a hybrid span decoder which can decode both the spans and the types recurrently in linear time and space complexities. Extensive experiments on the ACE05 dataset show that our approach also significantly outperforms state-ofthe-art on the joint entity and relation extraction task.1",
    "checked": true,
    "id": "77ee8a42b007ce1fa22718647298f496ef0ac499",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Liliang Ren",
      "Chenkai Sun",
      "Heng Ji",
      "Julia Hockenmaier"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.357": {
    "title": "Improving Automated Evaluation of Open Domain Dialog via Diverse Reference Augmentation",
    "volume": "findings",
    "abstract": "Multiple different responses are often plausible for a given open domain dialog context. Prior work has shown the importance of having multiple valid reference responses for meaningful and robust automated evaluations. In such cases, common practice has been to collect more human written references. However, such collection can be expensive, time consuming, and not easily scalable. Instead, we propose a novel technique for automatically expanding a human generated reference to a set of candidate references. We fetch plausible references from knowledge sources, and adapt them so that they are more fluent in context of the dialog instance in question. More specifically, we use (1) a commonsense knowledge base to elicit a large number of plausible reactions given the dialog history (2) relevant instances retrieved from dialog corpus, using similar past as well as future contexts. We demonstrate that our automatically expanded reference sets lead to large improvements in correlations of automated metrics with human ratings of system outputs for DailyDialog dataset. 1",
    "checked": true,
    "id": "43946aa78c80b6d7e6ffff837bdf4cff85f6a935",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Varun Gangal",
      "Harsh Jhamtani",
      "Eduard Hovy",
      "Taylor Berg-Kirkpatrick"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.358": {
    "title": "Who Blames or Endorses Whom? Entity-to-Entity Directed Sentiment Extraction in News Text",
    "volume": "findings",
    "abstract": "Understanding who blames or supports whom in news text is a critical research question in computational social science. Traditional methods and datasets for sentiment analysis are, however, not suitable for the domain of political text as they do not consider the direction of sentiments expressed between entities. In this paper, we propose a novel NLP task of identifying directed sentiment relationship between political entities from a given news document, which we call directed sentiment extraction. From a million-scale news corpus, we construct a dataset of news sentences where sentiment relations of political entities are manually annotated. We present a simple but effective approach for utilizing a pretrained transformer, which infers the target class by predicting multiple question-answering tasks and combining the outcomes. We demonstrate the utility of our proposed method for social science research questions by analyzing positive and negative opinions between political entities in two major events: 2016 U.S. presidential election and COVID-19. The newly proposed problem, data, and method will facilitate future studies on interdisciplinary NLP methods and applications. © 2021 Association for Computational Linguistics",
    "checked": true,
    "id": "c2d33e97c811cfb0b4aa8956cdcaeab828c7f356",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Kunwoo Park",
      "Zhufeng Pan",
      "Jungseock Joo"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.359": {
    "title": "New Dataset and Strong Baselines for the Grammatical Error Correction of Russian",
    "volume": "findings",
    "abstract": "Motivated by recent advancements in grammatical error correction in English and existing issues in the field, we describe a new resource, an annotated learner corpus of Russian, extracted from the Lang-8 language learning website. This new dataset is benchmarked against two grammatical error correction models that use state-of-the-art neural architectures. Results are provided on the newlycreated corpus and are compared against performance on another, existing resource. We also evaluate the contribution of the Lang-8 training data to the grammatical error correction of Russian and perform type-based analysis of the models. The expert annotations are available for research purposes",
    "checked": true,
    "id": "663d5e73ee992e6e39029de386a11103935c4084",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Viet Anh Trinh",
      "Alla Rozovskaya"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.360": {
    "title": "A Formidable Ability: Detecting Adjectival Extremeness with DSMs",
    "volume": "findings",
    "abstract": "While distributional semantic models (DSMs) can successfully capture the similarity structure within a semantic domain, less is known about their ability to represent abstract semantic properties that hold across domains. Such properties can form the basis for abstract semantic classes that are a crucial aspect of human semantic knowledge. For example, the abstract class of extreme adjectives (such as brilliant and freezing) spans a wide range of domains (here, INTELLIGENCE and TEMPERATURE). Using a model that compares query items to an aggregate DSM representation of a set of extreme adjectives, we show that novel adjectives can be classified accurately, supporting the insight that a cross-domain property like extremeness can be captured in a word's DSM representation. We then use the extremeness classifier to model the emergence of intensifier meaning in adverbs, demonstrating, in a separate task, the effectiveness of detecting this abstract semantic property. 1 Distributional Models and Abstract Semantic Classes Distributional semantic models (DSMs) are widely used as representations of word-level semantics. However, open questions remain as to precisely which aspects of human semantic knowledge DSMs effectively capture (e.g., Baroni et al., 2014; Hollis and Westbury, 2016; Schnabel et al., 2015; Utsumi, 2020). For example, popular DSMs such as word2vec and GloVe have been shown to predict human ratings of semantic features of objects (Rubinstein et al., 2015; Grand et al., 2018). However, performance is variable across features and object categories (Grand et al., 2018), and in particular, is better for taxonomic properties ('is an animal', 'is a weapon') than for general attributive properties ('is yellow', 'is dangerous') (Rubinstein et al., 2015). While people may or may not have semantic categories such as \"all yellow things\", abstract semantic classes are an important part of human linguistic knowledge that should be captured in a computational system. Note that by abstract we mean the schematic properties of word meaning, rather than the content-related classes;1 such properties abstract over commonalities of meaning that may cross traditional semantic domains. Consider, e.g., a semantic verb class such as changeof-state (Levin, 1993; Kipper et al., 2008), with members such as melt (the TEMPERATURE domain) and quicken (SPEED), or relational adjectives (Boleda et al., 2012), including, e.g., Chinese (NATIONALITY) or pulmonary (BODY-PART). Much work shows the ability of DSMs to match human knowledge of semantic properties within a domain (e.g., Baroni et al., 2014; Pereira et al., 2016; An et al., 2018; Grand et al., 2018), but there is little work, to our knowledge, on whether the similarity structure of a DSM is sensitive to commonalities of abstract properties that hold across a variety of semantic domains.2 Research on vector-based representations of analogy suggests that DSMs may be limited in their ability to represent crossdomain word relations: Rogers et al. (2017) show that cross-domain analogical relations like hypernymy (e.g., turtle:reptile::salmon:fish) are significantly harder to solve than within-domain ones (e.g., Paris:France::Ottawa:Canada). Lu et al. (2019) make significant progress towards representing such relations, showing that a DSM can form the basis for detecting the cross-domain word The same distinction between schematic and content is applied in Paradis (2001); Cruse and Togia (1996). It is also worth noting explicitly that our use of the term 'abstract' in this sense is not to be interpreted as 'not concrete'. DSMs may, e.g., encode concreteness and valence/arousal/dominance (e.g., Hollis and Westbury, 2016; Hollis et al., 2017), but the former can be viewed as a taxonomic property, and the latter as within the EMOTION domain",
    "checked": true,
    "id": "1da5615b2cbc6a4afe8f4a3469c94ba38d69c4d9",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Farhan Samir",
      "Barend Beekhuizen",
      "Suzanne Stevenson"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.361": {
    "title": "Effective Attention Sheds Light On Interpretability",
    "volume": "findings",
    "abstract": "An attention matrix of a transformer selfattention sublayer can provably be decomposed into two components and only one of them (effective attention) contributes to the model output. This leads us to ask whether visualizing effective attention gives different conclusions than interpretation of standard attention. Using a subset of the GLUE tasks and BERT, we carry out an analysis to compare the two attention matrices, and show that their interpretations differ. Effective attention is less associated with the features related to the language modeling pretraining such as the separator token, and it has more potential to illustrate linguistic features captured by the model for solving the end-task. Given the found differences, we recommend using effective attention for studying a transformer's behavior since it is more pertinent to the model output by design",
    "checked": true,
    "id": "3fa01ebe92d8bab53b2756beeecfe6faa9e573bb",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Kaiser Sun",
      "Ana Marasović"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.362": {
    "title": "Compositionality of Complex Graphemes in the Undeciphered Proto-Elamite Script using Image and Text Embedding Models",
    "volume": "findings",
    "abstract": "We introduce a language modeling architecture which operates over sequences of images, or over multimodal sequences of images with associated labels. We use this architecture alongside other embedding models to investigate a category of signs called complex graphemes (CGs) in the undeciphered protoElamite script. We argue that CGs have meanings which are at least partly compositional, and we discover novel rules governing the construction of CGs. We find that a language model over sign images produces more interpretable results than a model over text or over sign images and text, which suggests that the names given to signs may be obscuring signals in the corpus. Our results reveal previously unknown regularities in proto-Elamite sign use that can inform future decipherment efforts, and our image-aware language model provides a novel way to abstract away from biases introduced by human annotators",
    "checked": true,
    "id": "60b59b92739a24de4b4255628da63ac5534e0051",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Logan Born",
      "Kathryn Kelley",
      "M. Willis Monroe",
      "Anoop Sarkar"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.363": {
    "title": "On the Distribution, Sparsity, and Inference-time Quantization of Attention Values in Transformers",
    "volume": "findings",
    "abstract": "How much information do NLP tasks really need from a transformer's attention mechanism at application-time (inference)? From recent work, we know that there is sparsity in transformers and that the floating-points within its computation can be discretized to fewer values with minimal loss to task accuracies. However, this requires retraining or even creating entirely new models, both of which can be expensive and carbon-emitting. Focused on optimizations that do not require training, we systematically study the full range of typical attention values necessary. This informs the design of an inference-time quantization technique using both pruning and logscaled mapping which produces only a few (e.g. 2) unique values. Over the tasks of question answering and sentiment analysis, we find nearly 80% of attention values can be pruned to zeros with minimal (< 1.0%) relative loss in accuracy. We use this pruning technique in conjunction with quantizing the attention values to only a 3-bit format, without retraining, resulting in only a 0.8% accuracy reduction on question answering with fine-tuned RoBERTa",
    "checked": true,
    "id": "daab62304d0b1beeddad06846eaadce9c7610d9d",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Tianchu Ji",
      "Shraddhan Jain",
      "Michael Ferdman",
      "Peter Milder",
      "H. Andrew Schwartz",
      "Niranjan Balasubramanian"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.364": {
    "title": "Ethical-Advice Taker: Do Language Models Understand Natural Language Interventions?",
    "volume": "findings",
    "abstract": "Is it possible to use natural language to intervene in a model's behavior and alter its prediction in a desired way? We investigate the effectiveness of natural language interventions for reading-comprehension systems, studying this in the context of social stereotypes. Specifically, we propose a new language understanding task, Linguistic Ethical Interventions (LEI), where the goal is to amend a questionanswering (QA) model's unethical behavior by communicating context-specific principles of ethics and equity to it. To this end, we build upon recent methods for quantifying a system's social stereotypes, augmenting them with different kinds of ethical interventions and the desired model behavior under such interventions. Our zero-shot evaluation finds that even today's powerful neural language models are extremely poor ethical-advice takers, that is, they respond surprisingly little to ethical interventions even though these interventions are stated as simple sentences. Fewshot learning improves model behavior but remains far from the desired outcome, especially when evaluated for various types of generalization. Our new task thus poses a novel language understanding challenge for the community.1",
    "checked": true,
    "id": "6f9fc51102cf49bff4f4e2b336739a45f8389c80",
    "semantic_title": "",
    "citation_count": 17,
    "authors": [
      "Jieyu Zhao",
      "Daniel Khashabi",
      "Tushar Khot",
      "Ashish Sabharwal",
      "Kai-Wei Chang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.365": {
    "title": "Unsupervised Label Refinement Improves Dataless Text Classification",
    "volume": "findings",
    "abstract": "Dataless text classification is capable of classifying documents into previously unseen labels by assigning a score to any document paired with a label description. While promising, it crucially relies on accurate descriptions of the label set for each downstream task. This reliance causes dataless classifiers to be highly sensitive to the choice of label descriptions and hinders the broader application of dataless classification in practice. In this paper, we ask the following question: how can we improve dataless text classification using the inputs of the downstream task dataset? Our primary solution is a clustering based approach. Given a dataless classifier, our approach refines its set of predictions using k-means clustering. We demonstrate the broad applicability of our approach by improving the performance of two widely used classifier architectures, one that encodes text-category pairs with two independent encoders and one with a single joint encoder. Experiments show that our approach consistently improves dataless classification across different datasets and makes the classifier more robust to the choice of label descriptions",
    "checked": true,
    "id": "8ed62fb352098f3f943e2c1b08bb9998589de03d",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Zewei Chu",
      "Karl Stratos",
      "Kevin Gimpel"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.366": {
    "title": "Prompting Contrastive Explanations for Commonsense Reasoning Tasks",
    "volume": "findings",
    "abstract": "Many commonsense reasoning NLP tasks involve choosing between one or more possible answers to a question or prompt based on knowledge that is often implicit. Large pretrained language models (PLMs) can achieve near-human performance on such tasks, while providing little human-interpretable evidence of the underlying reasoning they use. In this work, we show how to use these same models to generate such evidence: inspired by the contrastive nature of human explanations, we use PLMs to complete explanation prompts which contrast alternatives according to the key attribute(s) required to justify the correct answer (for example, peanuts ts e t are usually salty while raisins i i r s s i i are sweet). Conditioning model decisions on these explanations improves performance on two commonsense reasoning benchmarks, as compared to previous non-contrastive alternatives. These explanations are also judged by humans to be more relevant for solving the task, and facilitate a novel method to evaluate explanation faithfulness",
    "checked": true,
    "id": "7747ecbc26b1688e6cad1a6ce83914efa2a3c04c",
    "semantic_title": "",
    "citation_count": 35,
    "authors": [
      "Bhargavi Paranjape",
      "Julian Michael",
      "Marjan Ghazvininejad",
      "Hannaneh Hajishirzi",
      "Luke Zettlemoyer"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.367": {
    "title": "SMS Spam Detection Through Skip-gram Embeddings and Shallow Networks",
    "volume": "findings",
    "abstract": "The drastic decrease in mobile SMS costs turned phone users more prone to spam messages, usually with unwanted marketing or questionable content. As such, researchers have proposed different methods for detecting SMS spam messages. This paper presents a technique for embedding SMS messages into vector spaces that is suitable for spam detection. The proposed approach relies on mining patterns that are relevant for distinguishing spam from legitimate messages. A subset of those patterns is used to construct a function that maps text messages into a multidimensional vector space. The extracted patterns are represented as skip-grams of token attributes, where a skip-gram can be seen as a generalization of the n-gram model that allows a distance greater than one between matched tokens in the text. We evaluate the proposed approach using the generated vectors for spam classification on the UCI Spam Collection dataset. The experiments showed that our method combined with shallow networks reached accuracy that is competitive with state-of-the-art approaches",
    "checked": true,
    "id": "69910e6f11930353a2c0296e46631d41a2124b80",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Gustavo Sousa",
      "Daniel Carlos Guimarães Pedronette",
      "João Paulo Papa",
      "Ivan Rizzo Guilherme"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.368": {
    "title": "Hierarchical Task Learning from Language Instructions with Unified Transformers and Self-Monitoring",
    "volume": "findings",
    "abstract": "Despite recent progress, learning new tasks through language instructions remains an extremely challenging problem. On the ALFRED benchmark for task learning, the published state-of-the-art system only achieves a task success rate of less than 10% in an unseen environment, compared to the human performance of over 90%. To address this issue, this paper takes a closer look at task learning. In a departure from a widely applied end-toend architecture, we decomposed task learning into three sub-problems: sub-goal planning, scene navigation, and object manipulation; and developed a model HiTUT1 (stands for Hierarchical Tasks via Unified Transformers) that addresses each sub-problem in a unified manner to learn a hierarchical task structure. On the ALFRED benchmark, HiTUT has achieved the best performance with a remarkably higher generalization ability. In the unseen environment, HiTUT achieves over 160% performance gain in success rate compared to the previous state of the art. The explicit representation of task structures also enables an in-depth understanding of the nature of the problem and the ability of the agent, which provides insight for future benchmark development and evaluation",
    "checked": true,
    "id": "f41e6c832c9e0d5360b66ee7681d3b1ffd2d9c3d",
    "semantic_title": "",
    "citation_count": 37,
    "authors": [
      "Yichi Zhang",
      "Joyce Chai"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.369": {
    "title": "Marked Attribute Bias in Natural Language Inference",
    "volume": "findings",
    "abstract": "Reporting and providing test sets for harmful bias in NLP applications is essential for building a robust understanding of the current problem. We present a new observation of gender bias in a downstream NLP application: marked attribute bias in natural language inference. Bias in downstream applications can stem from training data, word embeddings, or be amplified by the model in use. However, focusing on biased word embeddings is potentially the most impactful first step due to their universal nature. Here we seek to understand how the intrinsic properties of word embeddings contribute to this observed marked attribute effect, and whether current post-processing methods address the bias successfully. An investigation of the current debiasing landscape reveals two open problems: none of the current debiased embeddings mitigate the marked attribute error, and none of the intrinsic bias measures are predictive of the marked attribute effect. By noticing that a new type of intrinsic bias measure correlates meaningfully with the marked attribute effect, we propose a new postprocessing debiasing scheme for static word embeddings. The proposed method applied to existing embeddings achieves new best results on the marked attribute bias test set. See https://github.com/hillary-dawkins/MAB",
    "checked": true,
    "id": "6df96339baab731d79974eedcf85e4401ea9e5fe",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Hillary Dawkins"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.370": {
    "title": "VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding",
    "volume": "findings",
    "abstract": "We present a simplified, task-agnostic multimodal pre-training approach that can accept either video or text input, or both for a variety of end tasks. Existing pre-training are task-specific by adopting either a single crossmodal encoder that requires both modalities, limiting their use for retrieval-style end tasks or more complex multitask learning with two unimodal encoders, limiting early cross-modal fusion. We instead introduce new pretraining masking schemes that better mix across modalities (e.g. by forcing masks for text to predict the closest video embeddings) while also maintaining separability (e.g. unimodal predictions are sometimes required, without using all the input). Experimental results show strong performance across a wider range of tasks than any previous methods, often outperforming task-specific pre-training1",
    "checked": true,
    "id": "18f37f62d2bf3c2e34e2bde78545b47e92d7b72d",
    "semantic_title": "",
    "citation_count": 52,
    "authors": [
      "Hu Xu",
      "Gargi Ghosh",
      "Po-Yao Huang",
      "Prahal Arora",
      "Masoumeh Aminzadeh",
      "Christoph Feichtenhofer",
      "Florian Metze",
      "Luke Zettlemoyer"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.371": {
    "title": "Corpus-Level Evaluation for Event QA: The IndiaPoliceEvents Corpus Covering the 2002 Gujarat Violence",
    "volume": "findings",
    "abstract": "Automated event extraction in social science applications often requires corpus-level evaluations: for example, aggregating text predictions across metadata and unbiased estimates of recall. We combine corpus-level evaluation requirements with a real-world, social science setting and introduce the INDIAPOLICEEVENTS corpus—all 21,391 sentences from 1,257 English-language Times of India articles about events in the state of Gujarat during March 2002. Our trained annotators read and label every document for mentions of police activity events, allowing for unbiased recall evaluations. In contrast to other datasets with structured event representations, we gather annotations by posing natural questions, and evaluate off-the-shelf models for three different tasks: sentence classification, document ranking, and temporal aggregation of target events. We present baseline results from zero-shot BERT-based models fine-tuned on natural language inference and passage retrieval tasks. Our novel corpus-level evaluations and annotation approach can guide creation of similar social-science-oriented resources in the future",
    "checked": true,
    "id": "5e1669dd9b568f2a8a0804c2eea241006568b594",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Andrew Halterman",
      "Katherine Keith",
      "Sheikh Sarwar",
      "Brendan O’Connor"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.372": {
    "title": "Memory-Efficient Differentiable Transformer Architecture Search",
    "volume": "findings",
    "abstract": "Differentiable architecture search (DARTS) is successfully applied in many vision tasks. However, directly using DARTS for Transformers is memory-intensive, which renders the search process infeasible. To this end, we propose a multi-split reversible network and combine it with DARTS. Specifically, we devise a backpropagation-with-reconstruction algorithm so that we only need to store the last layer's outputs. By relieving the memory burden for DARTS, it allows us to search with larger hidden size and more candidate operations. We evaluate the searched architecture on three sequence-to-sequence datasets, i.e., WMT'14 English-German, WMT'14 EnglishFrench, and WMT'14 English-Czech. Experimental results show that our network consistently outperforms standard Transformers across the tasks. Moreover, our method compares favorably with big-size Evolved Transformers, reducing search computation by an order of magnitude",
    "checked": true,
    "id": "1a57318be32b740aef1d9b2070db6c0cc565ab0a",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Yuekai Zhao",
      "Li Dong",
      "Yelong Shen",
      "Zhihua Zhang",
      "Furu Wei",
      "Weizhu Chen"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.373": {
    "title": "On the Copying Behaviors of Pre-Training for Neural Machine Translation",
    "volume": "findings",
    "abstract": "Previous studies have shown that initializing neural machine translation (NMT) models with the pre-trained language models (LM) can speed up the model training and boost the model performance. In this work, we identify a critical side-effect of pre-training for NMT, which is due to the discrepancy between the training objectives of LM-based pre-training and NMT. Since the LM objective learns to reconstruct a few source tokens and copy most of them, the pre-training initialization would affect the copying behaviors of NMT models. We provide a quantitative analysis of copying behaviors by introducing a metric called copying ratio, which empirically shows that pre-training based NMT models have a larger copying ratio than the standard one. In response to this problem, we propose a simple and effective method named copying penalty to control the copying behaviors in decoding. Extensive experiments on both indomain and out-of-domain benchmarks show that the copying penalty method consistently improves translation performance by controlling copying behaviors for pre-training based NMT models. Source code is freely available at https://github.com/SunbowLiu/ CopyingPenalty",
    "checked": true,
    "id": "816ea6211b032b83ab67b9e65d351864359bb506",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Xuebo Liu",
      "Longyue Wang",
      "Derek F. Wong",
      "Liang Ding",
      "Lidia S. Chao",
      "Shuming Shi",
      "Zhaopeng Tu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.374": {
    "title": "Answer Generation for Retrieval-based Question Answering Systems",
    "volume": "findings",
    "abstract": "Recent advancements in transformer-based models have greatly improved the ability of Question Answering (QA) systems to provide correct answers; in particular, answer sentence selection (AS2) models, core components of retrieval-based systems, have achieved impressive results. While generally effective, these models fail to provide a satisfying answer when all retrieved candidates are of poor quality, even if they contain correct information. In AS2, models are trained to select the best answer sentence among a set of candidates retrieved for a given question. In this work, we propose to generate answers from a set of AS2 top candidates. Rather than selecting the best candidate, we train a sequence to sequence transformer model to generate an answer from a candidate set. Our tests on three English AS2 datasets show improvement up to 32 absolute points in accuracy over the state of the art",
    "checked": true,
    "id": "5b2a13b85ef9d3ad61639c3b400e447f3217ec98",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Chao-Chun Hsu",
      "Eric Lind",
      "Luca Soldaini",
      "Alessandro Moschitti"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.375": {
    "title": "Grounding ‘Grounding' in NLP",
    "volume": "findings",
    "abstract": "The NLP community has seen substantial recent interest in grounding to facilitate interaction between language technologies and the world. However, as a community, we use the term broadly to reference any linking of text to data or non-textual modality. In contrast, Cognitive Science more formally defines \"grounding\" as the process of establishing what mutual information is required for successful communication between two interlocutors – a definition which might implicitly capture the NLP usage but differs in intent and scope. We investigate the gap between these definitions and seek answers to the following questions: (1) What aspects of grounding are missing from NLP tasks? Here we present the dimensions of coordination, purviews and constraints. (2) How is the term \"grounding\" used in the current research? We study the trends in datasets, domains, and tasks introduced in recent NLP conferences. And finally, (3) How to advance our current definition to bridge the gap with Cognitive Science? We present ways to both create new tasks or repurpose existing ones to make advancements towards achieving a more complete sense of grounding. github.com/khyathiraghavi/Grounding-Grounding",
    "checked": true,
    "id": "2550fafc0cbd8bbf7aadd864ac569596d33db038",
    "semantic_title": "",
    "citation_count": 25,
    "authors": [
      "Khyathi Raghavi Chandu",
      "Yonatan Bisk",
      "Alan W Black"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.376": {
    "title": "Federated Chinese Word Segmentation with Global Character Associations",
    "volume": "findings",
    "abstract": "Chinese word segmentation (CWS) is a fundamental task for Chinese information processing, which always suffers from out-ofvocabulary word issues, especially when it is tested on data from different sources. Although one possible solution is to use more training data, in real applications, these data are stored at different locations and thus are invisible and isolated among each other owing to the privacy or legal issues (e.g., clinical reports from different hospitals). To address this issue and benefit from extra data, we propose a neural model for CWS with federated learning (FL) adopted to help CWS deal with data isolation, where a mechanism of global character associations is proposed to enhance FL to learn from different data sources. Experimental results on a simulated environment with five nodes confirm the effectiveness of our approach, where our approach outperforms different baselines including some well-designed FL frameworks.1",
    "checked": true,
    "id": "219d18c17c756fd33cc3a2b1fd661405777a3384",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Yuanhe Tian",
      "Guimin Chen",
      "Han Qin",
      "Yan Song"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.377": {
    "title": "PSED: A Dataset for Selecting Emphasis in Presentation Slides",
    "volume": "findings",
    "abstract": "Emphasizing words in presentation slides allows viewers to direct their gaze to focal points without reading the entire slide, retaining their attention on the speaker. Despite many studies on automatic slide generation, few have addressed helping authors choose which words to emphasize. Motivated by this, we study the problem of choosing candidates for emphasis by introducing a new dataset containing presentation slides with a wide variety of topics. We evaluated a range of state-of-the-art models on this novel dataset by organizing a shared task and inviting multiple researchers to model emphasis in slides",
    "checked": true,
    "id": "16e65d305b5561fd104dae8d2a9fdff8f1b5e7b5",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Amirreza Shirani",
      "Giai Tran",
      "Hieu Trinh",
      "Franck Dernoncourt",
      "Nedim Lipka",
      "Jose Echevarria",
      "Thamar Solorio",
      "Paul Asente"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.378": {
    "title": "MLMLM: Link Prediction with Mean Likelihood Masked Language Model",
    "volume": "findings",
    "abstract": "Knowledge Bases (KBs) are easy to query, verifiable, and interpretable. They however scale with man-hours and high-quality data. Masked Language Models (MLMs), such as BERT, scale with computing power as well as unstructured raw text data. The knowledge contained within those models is however not directly interpretable. We propose to perform link prediction with MLMs to address both the KBs scalability issues and the MLMs interpretability issues. To do that we introduce MLMLM, Mean Likelihood Masked Language Model, an approach comparing the mean likelihood of generating the different entities to perform link prediction in a tractable manner. We obtain State of the Art (SotA) results on the WN18RR dataset and the best non-entity-embedding based results on the FB15k-237 dataset. We also obtain convincing results on link prediction on previously unseen entities, making MLMLM a suitable approach to introducing new entities to a KB",
    "checked": true,
    "id": "fe41501e55d3bb83f9cfc33aac725dbd942b6419",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Louis Clouatre",
      "Philippe Trempe",
      "Amal Zouaq",
      "Sarath Chandar"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.379": {
    "title": "Modulating Language Models with Emotions",
    "volume": "findings",
    "abstract": "Generating context-aware language that embodies diverse emotions is an important step towards building empathetic NLP systems. In this paper, we propose a formulation of modulated layer normalization—a technique inspired by computer vision—that allows us to use large-scale language models for emotional response generation. In automatic and human evaluation on the MojiTalk dataset, our proposed modulated layer normalization method outperforms prior baseline methods while maintaining diversity, fluency, and coherence. Our method also obtains competitive performance even when using only 10% of the available training data",
    "checked": true,
    "id": "ac97f277002a9b514e8d667df8367cb979f8b86f",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Ruibo Liu",
      "Jason Wei",
      "Chenyan Jia",
      "Soroush Vosoughi"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.380": {
    "title": "Effective Batching for Recurrent Neural Network Grammars",
    "volume": "findings",
    "abstract": "As a language model that integrates traditional symbolic operations and flexible neural representations, recurrent neural network grammars (RNNGs) have attracted great attention from both scientific and engineering perspectives. However, RNNGs are known to be harder to scale due to the difficulty of batched training. In this paper, we propose effective batching for RNNGs, where every operation is computed in parallel with tensors across multiple sentences. Our PyTorch implementation effectively employs a GPU and achieves x6 speedup compared to the existing C++ DyNet implementation with model-independent auto-batching. Moreover, our batched RNNG also accelerates inference and achieves x20-150 speedup for beam search depending on beam sizes. Finally, we evaluate syntactic generalization performance of the scaled RNNG against the LSTM baseline, based on the large training data of 100M tokens from English Wikipedia and the broad-coverage targeted syntactic evaluation benchmark.1",
    "checked": true,
    "id": "b93600b4fd9b2676d27db6179457a319d659f30d",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Hiroshi Noji",
      "Yohei Oseki"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.381": {
    "title": "Verb Sense Clustering using Contextualized Word Representations for Semantic Frame Induction",
    "volume": "findings",
    "abstract": "Contextualized word representations have proven useful for various natural language processing tasks. However, it remains unclear to what extent these representations can cover hand-coded semantic information such as semantic frames, which specify the semantic role of the arguments associated with a predicate. In this paper, we focus on verbs that evoke different frames depending on the context, and we investigate how well contextualized word representations can recognize the difference of frames that the same verb evokes. We also explore which types of representation are suitable for semantic frame induction. In our experiments, we compare seven different contextualized word representations for two English frame-semantic resources, FrameNet and PropBank. We demonstrate that several contextualized word representations, especially BERT and its variants, are considerably informative for semantic frame induction. Furthermore, we examine the extent to which the contextualized representation of a verb can estimate the number of frames that the verb can evoke",
    "checked": true,
    "id": "c7051ff470194c96a66f45d0a2b3fdf9159fb0fe",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Kosuke Yamada",
      "Ryohei Sasano",
      "Koichi Takeda"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.382": {
    "title": "Benchmarking Neural Topic Models: An Empirical Study",
    "volume": "findings",
    "abstract": "Neural topic modeling approach has been attracting much attention recently as it is able to leverage the advantages of both neural networks and probabilistic topic models. Previous works have proposed several models that are based on this framework and obtained impressive experimental results compared to traditional probabilistic models. However, the reported result is not consistent across the works, making them hard for gaining a rigorous assessment of these approaches. This work aims to address this issue by offering an extensive empirical evaluation of typical neural topic models in different aspects using large, diverse datasets as well as a thorough set of metrics. Precisely, we examine the performance of these models in three tasks, namely uncovering cohesive topics, modeling the input documents, and representing them for downstream classification. Our results show that while the neural topic models are better in the first and the third tasks, the traditional probabilistic models are still a strong baseline and are better in the second task in many cases. These findings give us more insights for choosing offthe-shelf topic modeling toolboxes in different contexts, as well as for designing more comprehensive evaluation for neural topic models",
    "checked": true,
    "id": "b830fba9dd228f63aaecfb040461acb1df6bb64c",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Thanh-Nam Doan",
      "Tuan-Anh Hoang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.383": {
    "title": "Enhancing Chinese Word Segmentation via Pseudo Labels for Practicability",
    "volume": "findings",
    "abstract": "Pre-trained language models (e.g., BERT) significantly alleviate two traditional challenging problems for Chinese word segmentation (CWS): segmentation ambiguity and out-ofvocabulary (OOV) words. However, such improvements are usually achieved on traditional benchmark datasets and not close to an important goal of CWS: practicability (i.e., low complexity as a standalone task and high beneficiality to downstream tasks). To make a trade-off between traditional evaluation and practicability for CWS, we propose a semisupervised neural method via pseudo labels. The neural method consists of a teacher model and a student model, which distills knowledge from unlabeled data to the student model so as to improve both in-domain and out-ofdomain CWS. Experiments show that our proposed method can not only keep the practicability of the lightweight student model but also improve the performance of segmentation effectively. We also evaluate a range of heterogeneous neural architectures of CWS on downstream Chinese NLP tasks. Results of further experiments demonstrate that our proposed segmenter is reliable and practical as a pre-processing step of the downstream NLP tasks at the minimum cost.1",
    "checked": true,
    "id": "0d00c94ed73f0e8f9ecc683eb6f8e2af7d127ee2",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Kaiyu Huang",
      "Junpeng Liu",
      "Degen Huang",
      "Deyi Xiong",
      "Zhuang Liu",
      "Jinsong Su"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.384": {
    "title": "Analysis of Tree-Structured Architectures for Code Generation",
    "volume": "findings",
    "abstract": "Code generation is the task of generating code snippets from input user specifications in natural language. Leveraging the linguisticallymotivated hierarchical structure of the input can benefit code generation, especially since the specifications are complex sentences containing multiple variables and operations over various data structures. Moreover, recent advances in Transformer architectures have led to improved performance with tree-to-tree style generation for other seq2seq tasks e.g., machine translation. Hence, we present an empirical analysis of the significance of input parse trees for code generation. We run textto-tree, linearized tree-to-tree, and structured tree-to-tree models, using constituency-based parse trees as input, where the target is Abstract Syntax Tree (AST) of the code. We evaluate our models on the Python-based code generation dataset CoNaLa and a semantic parsing dataset ATIS. We find that constituency trees encoded using a structure-aware model improve performance for both datasets. We also provide an analysis of those aspects of the input parse trees which are most impactful. For instance, we find that structure-aware encodings are better at modelling inputs with multiple variables and capturing long-range dependencies for code generation.1",
    "checked": true,
    "id": "e13d317fe0178a8b8b67f4af995e7fac12c35014",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Samip Dahal",
      "Adyasha Maharana",
      "Mohit Bansal"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.385": {
    "title": "How Does Distilled Data Complexity Impact the Quality and Confidence of Non-Autoregressive Machine Translation?",
    "volume": "findings",
    "abstract": "While non-autoregressive (NAR) models are showing great promise for machine translation (MT), their use is limited by their dependence on knowledge distillation from autoregressive models. To address this issue, we seek to understand why distillation is so effective. Prior work suggests that distilled training data is less complex than manual translations. Based on experiments with the Levenshtein Transformer and the Mask-Predict NAR models on the WMT14 German-English task, this paper shows that different types of complexity have different impacts: while reducing lexical diversity and decreasing reordering complexity both help NAR learn better alignment between source and target, and thus improve translation quality, lexical diversity is the main reason why distillation increases model confidence, which affects the calibration of different NAR models differently",
    "checked": true,
    "id": "d076d0639df7c9729c2ab8fd3361efc1af4077ad",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Weijia Xu",
      "Shuming Ma",
      "Dongdong Zhang",
      "Marine Carpuat"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.386": {
    "title": "Leveraging Topic Relatedness for Argument Persuasion",
    "volume": "findings",
    "abstract": "Argumentation exposes individuals to conflicting viewpoints and can help them make more informed decisions based on the pros and cons of a particular issue. While recent studies of argumentation in Natural Language Processing have mainly focused on understanding the effect of various factors of persuasion (i.e. the source, audience, and language style), the impact of exploiting the relationships among controversial topics when predicting argument persuasiveness remains under-explored. In this paper, we model the relatedness among controversial topics utilizing an embedding-based method based on individuals' stances on the topics. We then leverage these topic embedding features and incorporate topic semantics features extracted from the arguments along with the previously studied factors of persuasion. We show that incorporating both types of topic relatedness features explicitly leads to significant improvement in predicting persuasiveness and also helps enhance generalization to rare topics, in a few-shot setting",
    "checked": true,
    "id": "ff8214ffa8ee898fc9dc85933fb108e53e3e7a49",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Xinran Zhao",
      "Esin Durmus",
      "Hongming Zhang",
      "Claire Cardie"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.387": {
    "title": "One Teacher is Enough? Pre-trained Language Model Distillation from Multiple Teachers",
    "volume": "findings",
    "abstract": "Pre-trained language models (PLMs) achieve great success in NLP. However, their huge model sizes hinder their applications in many practical systems. Knowledge distillation is a popular technique to compress PLMs, which learns a small student model from a large teacher PLM. However, the knowledge learned from a single teacher may be limited and even biased, resulting in low-quality student model. In this paper, we propose a multi-teacher knowledge distillation framework named MTBERT for pre-trained language model compression, which can train high-quality student model from multiple teacher PLMs. In MTBERT we design a multi-teacher co-finetuning method to jointly finetune multiple teacher PLMs in downstream tasks with shared pooling and prediction layers to align their output space for better collaborative teaching. In addition, we propose a multi-teacher hidden loss and a multi-teacher distillation loss to transfer the useful knowledge in both hidden states and soft labels from multiple teacher PLMs to the student model. Experiments on three benchmark datasets validate the effectiveness of MTBERT in compressing PLMs",
    "checked": true,
    "id": "066529517e46417825624f1416e200d15a6e3b64",
    "semantic_title": "",
    "citation_count": 25,
    "authors": [
      "Chuhan Wu",
      "Fangzhao Wu",
      "Yongfeng Huang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.388": {
    "title": "Logic-Consistency Text Generation from Semantic Parses",
    "volume": "findings",
    "abstract": "Text generation from semantic parses is to generate textual descriptions for formal representation inputs such as logic forms and SQL queries. This is challenging due to two reasons: (1) the complex and intensive inner logic with the data scarcity constraint, (2) the lack of automatic evaluation metrics for logic consistency. To address these two challenges, this paper first proposes SNOWBALL, a framework for logic consistent text generation from semantic parses that employs an iterative training procedure by recursively augmenting the training set with quality control. Second, we propose a novel automatic metric, BLEC, for evaluating the logical consistency between the semantic parses and generated texts. The experimental results on two benchmark datasets, Logic2Text and Spider, demonstrate the SNOWBALL framework enhances the logic consistency on both BLEC and human evaluation. Furthermore, our statistical analysis reveals that BLEC is more logically consistent with human evaluation than general-purpose automatic metrics including BLEU, ROUGE and, BLEURT. Our data and code are available at https://github.com/ Ciaranshu/relogic",
    "checked": true,
    "id": "8ede9ccbaad8780304d47482b9e404d3de88e658",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Chang Shu",
      "Yusen Zhang",
      "Xiangyu Dong",
      "Peng Shi",
      "Tao Yu",
      "Rui Zhang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.389": {
    "title": "Inducing Semantic Roles Without Syntax",
    "volume": "findings",
    "abstract": "Semantic roles are a key component of linguistic predicate-argument structure, but developing ontologies of these roles requires significant expertise and manual effort. Methods exist for automatically inducing semantic roles using syntactic representations, but syntax can also be difficult to define, annotate, and predict. We show it is possible to automatically induce semantic roles from QA-SRL, a scalable and ontology-free semantic annotation scheme that uses question-answer pairs to represent predicate-argument structure. By associating arguments with distributions over QASRL questions and clustering them in a mixture model, our method outperforms all previous models as well as a new state-of-the-art baseline over gold syntax. We show that our method works because QA-SRL acts as surrogate syntax, capturing non-overt arguments and syntactic alternations, which are central motivators for the use of semantic role labeling systems.1",
    "checked": true,
    "id": "cdf4a757298add156fc181c4b1c649f8bb75f80c",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Julian Michael",
      "Luke Zettlemoyer"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.390": {
    "title": "Plot and Rework: Modeling Storylines for Visual Storytelling",
    "volume": "findings",
    "abstract": "Writing a coherent and engaging story is not easy. Creative writers use their knowledge and worldview to put disjointed elements together to form a coherent storyline, and work and rework iteratively toward perfection. Automated visual storytelling (VIST) models, however, make poor use of external knowledge and iterative generation when attempting to create stories. This paper introduces PR-VIST, a framework that represents the input image sequence as a story graph in which it finds the best path to form a storyline. PR-VIST then takes this path and learns to generate the final story via an iterative training process. This framework produces stories that are superior in terms of diversity, coherence, and humanness, per both automatic and human evaluations. An ablation study shows that both plotting and reworking contribute to the model's superiority",
    "checked": true,
    "id": "2800db93276eadcf43d1485957c25a594f1f9c7b",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Chi-yang Hsu",
      "Yun-Wei Chu",
      "Ting-Hao Huang",
      "Lun-Wei Ku"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.391": {
    "title": "Disentangled Code Representation Learning for Multiple Programming Languages",
    "volume": "findings",
    "abstract": "Developing effective distributed representations of source code is fundamental yet challenging for many software engineering tasks such as code clone detection, code search, code translation and transformation. However, current code embedding approaches that represent the semantic and syntax of code in a mixed way are less interpretable and the resulting embedding can not be easily generalized across programming languages. In this paper, we propose a disentangled code representation learning approach to separate the semantic from the syntax of source code under a multi-programming-language setting, obtaining better interpretability and generalizability. Specially, we design three losses dedicated to the characteristics of source code to enforce the disentanglement effectively. We conduct comprehensive experiments on a real-world dataset composed of programming exercises implemented by multiple solutions that are semantically identical but grammatically distinguished. The experimental results validate the superiority of our proposed disentangled code representation, compared to several baselines, across three types of downstream tasks, i.e., code clone detection, code translation, and code-to-code search",
    "checked": true,
    "id": "87ce2a72c2fc0fc751cb23c4cf115a54b3b06896",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Jingfeng Zhang",
      "Haiwen Hong",
      "Yin Zhang",
      "Yao Wan",
      "Ye Liu",
      "Yulei Sui"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.392": {
    "title": "Exploring Self-Identified Counseling Expertise in Online Support Forums",
    "volume": "findings",
    "abstract": "A growing number of people engage in online health forums, making it important to understand the quality of the advice they receive. In this paper, we explore the role of expertise in responses provided to help-seeking posts regarding mental health. We study the differences between (1) interactions with peers; and (2) interactions with self-identified mental health professionals. First, we show that a classifier can distinguish between these two groups, indicating that their language use does in fact differ. To understand this difference, we perform several analyses addressing engagement aspects, including whether their comments engage the support-seeker further as well as linguistic aspects, such as dominant language and linguistic style matching. Our work contributes toward the developing efforts of understanding how health experts engage with health informationand support-seekers in social networks. More broadly, it is a step toward a deeper understanding of the styles of interactions that cultivate supportive engagement in online communities",
    "checked": true,
    "id": "c0e191909f4fef84aec41cfbe3ec826a9454ea58",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Allison Lahnala",
      "Yuntian Zhao",
      "Charles Welch",
      "Jonathan K. Kummerfeld",
      "Lawrence C An",
      "Kenneth Resnicow",
      "Rada Mihalcea",
      "Verónica Pérez-Rosas"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.393": {
    "title": "An Investigation of Suitability of Pre-Trained Language Models for Dialogue Generation – Avoiding Discrepancies",
    "volume": "findings",
    "abstract": "Pre-trained language models have been widely used in response generation for open-domain dialogue. These approaches are built within 4 frameworks: Transformer-ED, TransformerDec, Transformer-MLM and Transformer-AR. In this study, we experimentally compare them using both large and small-scale data. This reveals that decoder-only architecture is better than stacked encoder-decoder, and both leftto-right and bi-directional attention have their own advantages. We further define two concepts of model discrepancy, which provides a new explanation to the model performance. As discrepancies may hinder performance, we propose two solutions to reduce them, which successfully improve the model performance",
    "checked": true,
    "id": "7dd25287048fbc8753f412f30d30509e87308071",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Yan Zeng",
      "Jian-Yun Nie"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.394": {
    "title": "Learning to Sample Replacements for ELECTRA Pre-Training",
    "volume": "findings",
    "abstract": "ELECTRA (Clark et al., 2020a) pretrains a discriminator to detect replaced tokens, where the replacements are sampled from a generator trained with masked language modeling. Despite the compelling performance, ELECTRA suffers from the following two issues. First, there is no direct feedback loop from discriminator to generator, which renders replacement sampling inefficient. Second, the generator's prediction tends to be over-confident along with training, making replacements biased to correct tokens. In this paper, we propose two methods to improve replacement sampling for ELECTRA pre-training. Specifically, we augment sampling with a hardness prediction mechanism, so that the generator can encourage the discriminator to learn what it has not acquired. We also prove that the efficient sampling reduces the training variance of the discriminator. Moreover, we propose to use a focal loss for the generator in order to relieve oversampling correct tokens as replacements. Experimental results show that our method improves ELECTRA pre-training on various downstream tasks. Our code and pre-trained models will be released at: https: //github.com/YRdddream/electra-hp",
    "checked": true,
    "id": "077108a733f9b505437d404bf44d85a5858a434f",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Yaru Hao",
      "Li Dong",
      "Hangbo Bao",
      "Ke Xu",
      "Furu Wei"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.395": {
    "title": "Reordering Examples Helps during Priming-based Few-Shot Learning",
    "volume": "findings",
    "abstract": "The ability to learn from limited data, or fewshot learning, is a desirable and often critical requirement for NLP systems. While many existing methods do poorly at learning from a handful of examples, large pretrained language models have recently been shown to be efficient few-shot learners. One approach to few-shot learning, which does not require finetuning of model parameters, is to augment the language model's input with priming text which is typically constructed using task specific descriptions and examples. In this work, we further explore priming-based few-shot learning, with focus on using examples as prompts. We show that presenting examples in the right order is key for generalization. We introduce PERO (Prompting with Examples in the Right Order), where we formulate few-shot learning as search over the set of permutations of the training examples. We show that PERO can learn to generalize efficiently using as few as 10 examples, in contrast to existing approaches. While the newline token is a natural choice for separating the examples in the prompt, we show that learning a new separator token can potentially provide further gains in performance. We demonstrate the effectiveness of the proposed method on the tasks of sentiment classification, natural language inference and fact retrieval. Finally, we analyze the learned prompts to reveal novel insights, including the idea that two training examples in the right order alone can provide competitive performance for sentiment classification and natural language inference",
    "checked": true,
    "id": "355b66a65aee97822eb7404183ee72b18cb648de",
    "semantic_title": "",
    "citation_count": 32,
    "authors": [
      "Sawan Kumar",
      "Partha Talukdar"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.396": {
    "title": "Constrained Labeled Data Generation for Low-Resource Named Entity Recognition",
    "volume": "findings",
    "abstract": "Named Entity Recognition (NER) in lowresource languages has been a long-standing challenge in NLP. Recent work has shown great progress in two directions: developing cross-lingual features/models to transfer knowledge to low-resource languages, and translating source-language training data into low-resource target-language training data by projecting annotations with cheap resources. We focus on the second direction in this study. Existing methods suffer from the low quality of the resulting annotated data in the target language; for example, they cannot handle word order and lexical ambiguity well. To handle these limitations we propose a novel approach that uses the projected annotation to generate pseudo supervised data with a transformer language model and a constrained beam search. This allows us to generate more diverse, higher quality, as well as higher quantities of annotated data in the target language. Experiments demonstrate that, when combining our method with available cross-lingual features, it achieves state-of-the-art or competitive performance on NER in a low-resource setting, especially for languages that are distant from our source language, English. 1",
    "checked": true,
    "id": "ceda73e32bdd8f4e62e503a748f1752e3fba094f",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Ruohao Guo",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.397": {
    "title": "He is very intelligent, she is very beautiful? On Mitigating Social Biases in Language Modelling and Generation",
    "volume": "findings",
    "abstract": "Social biases with respect to demographics (e.g., gender, age, race) in datasets are often encoded in the large pre-trained language models trained on them. Prior works have largely focused on mitigating biases in context-free representations, with recent shift to contextual ones. While this is useful for several word and sentence-level classification tasks, mitigating biases in only the representations may not suffice to use these models for language generation tasks, such as auto-completion, summarization, or dialogue generation. In this paper, we propose an approach to mitigate social biases in BERT, a large pre-trained contextual language model, and show its effectiveness in fill-in-the-blank sentence completion and summarization tasks. In addition to mitigating biases in BERT, which in general acts as an encoder, we propose lexical co-occurrence-based bias penalization in the decoder units in generation frameworks, and show bias mitigation in summarization. Finally, our approach results in better debiasing of BERT-based representations compared to post training bias mitigation, thus illustrating the efficacy of our approach to not just mitigate biases in representations, but also generate text with reduced biases",
    "checked": true,
    "id": "ea667d3f5df2954c7365b8d1218889e2fc514829",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Aparna Garimella",
      "Akhash Amarnath",
      "Kiran Kumar",
      "Akash Pramod Yalla",
      "Anandhavelu N",
      "Niyati Chhaya",
      "Balaji Vasan Srinivasan"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.398": {
    "title": "Task-adaptive Pre-training of Language Models with Word Embedding Regularization",
    "volume": "findings",
    "abstract": "Pre-trained language models (PTLMs) acquire domain-independent linguistic knowledge through pre-training with massive textual resources. Additional pre-training is effective in adapting PTLMs to domains that are not well covered by the pre-training corpora. Here, we focus on the static word embeddings of PTLMs for domain adaptation to teach PTLMs domain-specific meanings of words. We propose a novel fine-tuning process: task-adaptive pre-training with word embedding regularization (TAPTER). TAPTER runs additional pretraining by making the static word embeddings of a PTLM close to the word embeddings obtained in the target domain with fastText. TAPTER requires no additional corpus except for the training data of the downstream task. We confirmed that TAPTER improves the performance of the standard fine-tuning and the task-adaptive pre-training on BioASQ (question answering in the biomedical domain) and on SQuAD (the Wikipedia domain) when their pre-training corpora were not dominated by indomain data",
    "checked": true,
    "id": "4c1a845d34ad84b50f5dea1c749d5fa7df697847",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Kosuke Nishida",
      "Kyosuke Nishida",
      "Sen Yoshida"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.399": {
    "title": "Do Grammatical Error Correction Models Realize Grammatical Generalization?",
    "volume": "findings",
    "abstract": "There has been an increased interest in data generation approaches to grammatical error correction (GEC) using pseudo data. However, these approaches suffer from several issues that make them inconvenient for realworld deployment including a demand for large amounts of training data. On the other hand, some errors based on grammatical rules may not necessarily require a large amount of data if GEC models can realize grammatical generalization. This study explores to what extent GEC models generalize grammatical knowledge required for correcting errors. We introduce an analysis method using synthetic and real GEC datasets with controlled vocabularies to evaluate whether models can generalize to unseen errors. We found that a current standard Transformer-based GEC model fails to realize grammatical generalization even in simple settings with limited vocabulary and syntax, suggesting that it lacks the generalization ability required to correct errors from provided training examples",
    "checked": true,
    "id": "7ac2f6e5a25eaf6a1a59b0c96ff7cdbfbc17b535",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Masato Mita",
      "Hitomi Yanaka"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.400": {
    "title": "Domain-Aware Dependency Parsing for Questions",
    "volume": "findings",
    "abstract": "Parsing natural language questions in specific domains is crucial to a wide range of applications from question-answering to dialog systems. Pre-trained parsers are usually trained on corpora dominated by non-questions, and thus perform poorly on domain-specific questions. Retraining parsers with domain-specific questions labeled with syntactic parse trees is expensive, as these annotations require linguistic expertise. In this paper, we propose an automatic labeled domain question generation framework by leveraging domain knowledge and seed domain questions. We evaluate our approach in two domains, and release the generated question datasets. Our experimental results demonstrate that auto-generated labeled questions indeed lead to significant (4.9% − 9%) increase in the accuracy of state-of-the-art (SoTA) parsers on domain questions",
    "checked": true,
    "id": "17972be63441aff7ff2dc6e941d6ca7c935bb339",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aparna Garimella",
      "Laura Chiticariu",
      "Yunyao Li"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.401": {
    "title": "Using Social and Linguistic Information to Adapt Pretrained Representations for Political Perspective Identification",
    "volume": "findings",
    "abstract": "Understanding the political perspective shaping the way events are discussed in the media is increasingly important due to the dramatic change in news distribution. With the advance in text classification models, the performance of political perspective detection is also improving rapidly. However, current deep learning based text models often require a large amount of supervised data for training, which can be very expensive to obtain for this task. Meanwhile, models pre-trained on the general source and task (e.g. BERT) lack the ability to focus on bias-related text span. In this paper, we propose a novel framework that pretrains the text model using signals from the rich social and linguistic context that is readily available, including entity mentions, news sharing, and frame indicators. The pre-trained models benefit from tasks related to bias detection and therefore are easier to train with the bias labels. We demonstrate the effectiveness of our proposed framework by experiments on two news bias datasets. The models with pre-training achieve significant improvement in performance and are capable of identifying the text span for bias better",
    "checked": true,
    "id": "56441f4f3beb3276350d811c67307e28d5f61b43",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Chang Li",
      "Dan Goldwasser"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.402": {
    "title": "Enhancing Dialogue-based Relation Extraction by Speaker and Trigger Words Prediction",
    "volume": "findings",
    "abstract": "Identifying relations from dialogues is more challenging than traditional sentence-level relation extraction (RE), since the difficulties of speaker information representation and the long-range semantic reasoning. Despite the successful efforts, existing methods do not fully consider the particularity of dialogues, making them difficult to truly understand the semantics between conversational arguments. In this paper, we propose two beneficial tasks, speaker prediction and trigger words prediction, to enhance the extraction of dialoguebased relations. Specifically, speaker prediction captures the characteristics of speakerrelated entities, and the trigger words prediction provides supportive contexts for relations between arguments. Extensive experiments on the DialogRE dataset show noticeable improvements compared to the baseline models, which achieves a new state-of-the-art performance with a 65.5% of F1 score and a 60.5% of F1c score, respectively",
    "checked": true,
    "id": "ece13dbeb8094ba580701469952de38c841b9894",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Tianyang Zhao",
      "Zhao Yan",
      "Yunbo Cao",
      "Zhoujun Li"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.403": {
    "title": "Modeling Event-Pair Relations in External Knowledge Graphs for Script Reasoning",
    "volume": "findings",
    "abstract": "Script reasoning infers subsequent events from a given event chain, which involves the ability to understand relations between events. A human-labeled script reasoning dataset is usually of small size with limited event relations, which highlights the necessity to leverage external eventuality knowledge graphs (KG) consisting of numerous triple facts to describe the inferential relation between events. Existing methods adopt a retrieval and integration paradigm to focus merely on the graph triples that have event overlap with a script, but ignore much more supportive triples in the KG with similar inferential patterns, leading to underexploiting. To fully exploit the KG, we propose a knowledge model to learn the inferential relations between events from the whole eventuality KG and then support downstream models by directly capturing the relation between events in a script. We further present a neural script adapter to extend the knowledge model for inferring the associated relations between an event chain and a subsequent event candidate. We evaluate the proposed approach on a popular multi-choice narrative cloze task for script reasoning and achieve new state-ofthe-art accuracy, compared with baselines either incorporating external KG or not",
    "checked": true,
    "id": "c5a810bdec24cf170aa32bada901695e55be71f6",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Yucheng Zhou",
      "Xiubo Geng",
      "Tao Shen",
      "Jian Pei",
      "Wenqiang Zhang",
      "Daxin Jiang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.404": {
    "title": "PROST: Physical Reasoning about Objects through Space and Time",
    "volume": "findings",
    "abstract": "We present a new probing dataset named PROST: Physical Reasoning about Objects Through Space and Time. This dataset contains 18,736 multiple-choice questions made from 14 manually curated templates, covering 10 physical reasoning concepts. All questions are designed to probe both causal and masked language models in a zero-shot setting. We conduct an extensive analysis which demonstrates that state-of-the-art pretrained models are inadequate at physical reasoning: they are influenced by the order in which answer options are presented to them, they struggle when the superlative in a question is inverted (e.g., most ↔ least), and increasing the amount of pretraining data and parameters only yields minimal improvements. These results provide support for the hypothesis that current pretrained models' ability to reason about physical interactions is inherently limited by a lack of real world experience. By highlighting these limitations, we hope to motivate the development of models with a human-like understanding of the physical world",
    "checked": true,
    "id": "5aab57cc0530560d82c74c055f664280619d7e81",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Stéphane Aroca-Ouellette",
      "Cory Paik",
      "Alessandro Roncone",
      "Katharina Kann"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.405": {
    "title": "Revisiting the Evaluation of End-to-end Event Extraction",
    "volume": "findings",
    "abstract": "Event extraction (EE) aims to harvest event instances from plain text, where each instance is composed of a group of event arguments with specific event roles. Existing end-to-end EE research usually adopts the role-averaged evaluation that produces evaluation measures by averaging evaluation statistics of each event role. However, although this averaged metric can indicate the model performance to some extent, we find that such metric can be pretty misleading to downstream applications that utilize an event instance as a whole, where one wrongly identified event argument can substantially alter the whole meaning of an event instance. To mitigate this gap and provide a more complete understanding of performance, we propose two new evaluation metrics that also consider an event instance as a whole and explicitly penalize wrongly identified event arguments. Moreover, to support diverse preferences of evaluation metrics motivated by different scenarios, we propose a new training paradigm based on reinforcement learning for a typical end-to-end EE model, i.e., Doc2EDAG. Our extensive experiments show that the new training improves the initial one by a large margin (about 10%) under new metrics. Nevertheless, the current performance is still far from satisfactory, and optimizing towards these new metrics calls for more future research",
    "checked": true,
    "id": "7dcb0b16b059588376d57b4df7b31fb002784cc0",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Shun Zheng",
      "Wei Cao",
      "Wei Xu",
      "Jiang Bian"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.406": {
    "title": "Direct Simultaneous Speech-to-Text Translation Assisted by Synchronized Streaming ASR",
    "volume": "findings",
    "abstract": "Simultaneous speech-to-text translation is widely useful in many scenarios. The conventional cascaded approach uses a pipeline of streaming ASR followed by simultaneous MT, but suffers from error propagation and extra latency. To alleviate these issues, recent efforts attempt to directly translate the source speech into target text simultaneously, but this is much harder due to the combination of two separate tasks. We instead propose a new paradigm with the advantages of both cascaded and endto-end approaches. The key idea is to use two separate, but synchronized, decoders on streaming ASR and direct speech-to-text translation (ST), respectively, and the intermediate results of ASR guide the decoding policy of (but is not fed as input to) ST. During training time, we use multitask learning to jointly learn these two tasks with a shared encoder. En-toDe and En-to-Es experiments on the MuSTC dataset demonstrate that our proposed technique achieves substantially better translation quality at similar levels of latency",
    "checked": true,
    "id": "2ebba8d3acd0fec329edfda24ef8dac96ee9c6a4",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Junkun Chen",
      "Mingbo Ma",
      "Renjie Zheng",
      "Liang Huang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.407": {
    "title": "HIT - A Hierarchically Fused Deep Attention Network for Robust Code-mixed Language Representation",
    "volume": "findings",
    "abstract": "Understanding linguistics and morphology of resource-scarce code-mixed texts remains a key challenge in text processing. Although word embedding comes in handy to support downstream tasks for low-resource languages, there are plenty of scopes in improving the quality of language representation particularly for code-mixed languages. In this paper, we propose HIT, a robust representation learning method for code-mixed texts. HIT is a hierarchical transformer-based framework that captures the semantic relationship among words and hierarchically learns the sentencelevel semantics using a fused attention mechanism. HIT incorporates two attention modules, a multi-headed self-attention and an outer product attention module, and computes their weighted sum to obtain the attention weights. Our evaluation of HIT on one European (Spanish) and five Indic (Hindi, Bengali, Tamil, Telugu, and Malayalam) languages across four NLP tasks on eleven datasets suggests significant performance improvement against various state-of-the-art systems. We further show the adaptability of learned representation across tasks in a transfer learning setup (with and without fine-tuning)",
    "checked": true,
    "id": "940824fb28d4c92382e481e8250100925ef83c87",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Ayan Sengupta",
      "Sourabh Kumar Bhattacharjee",
      "Tanmoy Chakraborty",
      "Md. Shad Akhtar"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.408": {
    "title": "Semi-Supervised Data Programming with Subset Selection",
    "volume": "findings",
    "abstract": "The paradigm of data programming, which uses weak supervision in the form of rules/labelling functions, and semi-supervised learning, which augments small amounts of labelled data with a large unlabelled dataset, have shown great promise in several text classification scenarios. In this work, we argue that by not using any labelled data, data programming based approaches can yield sub-optimal performances, particularly when the labelling functions are noisy. The first contribution of this work is an introduction of a framework, SPEAR which is a semi-supervised data programming paradigm that learns a joint model that effectively uses the rules/labelling functions along with semi-supervised loss functions on the feature space. Next, we also study SPEAR-SS which additionally does subset selection on top of the joint semi-supervised data programming objective and selects a set of examples that can be used as the labelled set by SPEAR. The goal of SPEAR-SS is to ensure that the labelled data can complement the labelling functions, thereby benefiting from both data-programming as well as appropriately selected data for human labelling. We demonstrate that by effectively combining semi-supervision, data-programming, and subset selection paradigms, we significantly outperform the current state-of-the-art on seven publicly available datasets. 1",
    "checked": true,
    "id": "21532eb5ff79a18d9075d729dc5b5df40c8729d8",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Ayush Maheshwari",
      "Oishik Chatterjee",
      "Krishnateja Killamsetty",
      "Ganesh Ramakrishnan",
      "Rishabh Iyer"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.409": {
    "title": "Fingerprinting Fine-tuned Language Models in the Wild",
    "volume": "findings",
    "abstract": "There are concerns that the ability of language models (LMs) to generate high quality synthetic text can be misused to launch spam, disinformation, or propaganda. Therefore, the research community is actively working on developing approaches to detect whether a given text is organic or synthetic. While this is a useful first step, it is important to be able to further fingerprint the author LM to attribute its origin. Prior work on fingerprinting LMs is limited to attributing synthetic text generated by a handful (usually< 10) of pre-trained LMs. However, LMs such as GPT2 are commonly fine-tuned in a myriad of ways (e.g., on a domain-specific text corpus) before being used to generate synthetic text. It is challenging to fingerprinting fine-tuned LMs because the universe of fine-tuned LMs is much larger in realistic scenarios. To address this challenge, we study the problem of large-scale fingerprinting of fine-tuned LMs in the wild. Using a real-world dataset of synthetic text generated by 108 different fine-tuned LMs, we conduct comprehensive experiments to demonstrate the limitations of existing fingerprinting approaches. Our results show that fine-tuning itself is the most effective in attributing the synthetic text generated by fine-tuned LMs",
    "checked": true,
    "id": "a66ab2feae147f4ec59203019d51525ab5f0a92b",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Nirav Diwan",
      "Tanmoy Chakraborty",
      "Zubair Shafiq"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.410": {
    "title": "Analyzing Code Embeddings for Coding Clinical Narratives",
    "volume": "findings",
    "abstract": "Medical professionals review clinical narratives to assign medical codes as per the International Classification of Diseases (ICD) for billing and care management. This manual process is inefficient and error-prone as it involves a nuanced one-to-many mapping. Recent works on automated ICD coding learn mappings between low-dimensional representations of the reports and the codes. While they propose novel neural networks for encoding varied types of information about the codes, it is unclear as to what information in the medical codes is helpful for performance improvement and why. Here, we compare different ways to represent, or embed, the codes based on their textual, structural and statistical characteristics, using a single deep learning baseline model in quantitative evaluations on discharge reports from the MIMIC-III Intensive Care Unit database. We also qualitatively analyse the nature of the cases that benefit most from the code embeddings and demonstrate that code embeddings are important for predicting ambiguous and oblique codes",
    "checked": true,
    "id": "c7c09cd0013b7f9fff085d22b910dfea56f30ee4",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Shi",
      "Jiewen Wu",
      "Xiwen Yang",
      "Nancy Chen",
      "Ivan Ho Mien",
      "Jung-Jae Kim",
      "Pavitra Krishnaswamy"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.411": {
    "title": "Automatic Construction of Sememe Knowledge Bases via Dictionaries",
    "volume": "findings",
    "abstract": "A sememe is defined as the minimum semantic unit in linguistics. Sememe knowledge bases (SKBs), which comprise words annotated with sememes, enable sememes to be applied to natural language processing. So far a large body of research has showcased the unique advantages and effectiveness of SKBs in various tasks. However, most languages have no SKBs, and manual construction of SKBs is time-consuming and labor-intensive. To tackle this challenge, we propose a simple and fully automatic method of building an SKB via an existing dictionary. We use this method to build an English SKB and a French SKB, and conduct comprehensive evaluations from both intrinsic and extrinsic perspectives. Experimental results demonstrate that the automatically built English SKB is even superior to HowNet, the most widely used SKB that takes decades to build manually. And both the English and French SKBs can bring obvious performance enhancement in multiple downstream tasks. All the code and data of this paper (except the copyrighted dictionaries) can be obtained at https://github.com/ thunlp/DictSKB",
    "checked": true,
    "id": "eea82bf4389ac91e2ee66fcf550a611f621c3eb9",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Fanchao Qi",
      "Yangyi Chen",
      "Fengyu Wang",
      "Zhiyuan Liu",
      "Xiao Chen",
      "Maosong Sun"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.412": {
    "title": "Rule-Aware Reinforcement Learning for Knowledge Graph Reasoning",
    "volume": "findings",
    "abstract": "Multi-hop reasoning is an effective and explainable approach to predicting missing facts in Knowledge Graphs (KGs). It usually adopts the Reinforcement Learning (RL) framework and searches over the KG to find an evidential path. However, due to the large exploration space, the RL-based model struggles with the serious sparse reward problem and needs to make a lot of trials. Moreover, its exploration can be biased towards spurious paths that coincidentally lead to correct answers. To solve both problems, we propose a simple but effective RL-based method called RARL (RuleAware RL). It injects high quality symbolic rules into the model's reasoning process and employs partially random beam search, which can not only increase the probability of paths getting rewards, but also alleviate the impact of spurious paths. Experimental results show that it outperforms existing multi-hop methods in terms of Hit@1 and MRR",
    "checked": true,
    "id": "fe7382db243694c67c667cf2ec80072577d2372b",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Zhongni Hou",
      "Xiaolong Jin",
      "Zixuan Li",
      "Long Bai"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.413": {
    "title": "XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages",
    "volume": "findings",
    "abstract": "Contemporary works on abstractive text summarization have focused primarily on highresource languages like English, mostly due to the limited availability of datasets for low/midresource ones. In this work, we present XLSum, a comprehensive and diverse dataset comprising 1 million professionally annotated article-summary pairs from BBC, extracted using a set of carefully designed heuristics. The dataset covers 44 languages ranging from low to high-resource, for many of which no public dataset is currently available. XL-Sum is highly abstractive, concise, and of high quality, as indicated by human and intrinsic evaluation. We fine-tune mT5, a state-of-theart pretrained multilingual model, with XLSum and experiment on multilingual and lowresource summarization tasks. XL-Sum induces competitive results compared to the ones obtained using similar monolingual datasets: we show higher than 11 ROUGE-2 scores on 10 languages we benchmark on, with some of them exceeding 15, as obtained by multilingual training. Additionally, training on low-resource languages individually also provides competitive performance. To the best of our knowledge, XL-Sum is the largest abstractive summarization dataset in terms of the number of samples collected from a single source and the number of languages covered. We are releasing our dataset and models to encourage future research on multilingual abstractive summarization. The resources can be found at https://github. com/csebuetnlp/xl-sum",
    "checked": true,
    "id": "ecf5618b513aa5c4d5bf62ca251923a188251117",
    "semantic_title": "",
    "citation_count": 103,
    "authors": [
      "Tahmid Hasan",
      "Abhik Bhattacharjee",
      "Md. Saiful Islam",
      "Kazi Mubasshir",
      "Yuan-Fang Li",
      "Yong-Bin Kang",
      "M. Sohel Rahman",
      "Rifat Shahriyar"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.414": {
    "title": "Use of Formal Ethical Reviews in NLP Literature: Historical Trends and Current Practices",
    "volume": "findings",
    "abstract": "Ethical aspects of research in language technologies have received much attention recently. It is a standard practice to get a study involving human subjects reviewed and approved by a professional ethics committee/board of the institution. How commonly do we see mention of ethical approvals in NLP research? What types of research or aspects of studies are usually subject to such reviews? With the rising concerns and discourse around the ethics of NLP, do we also observe a rise in formal ethical reviews of NLP studies? And, if so, would this imply that there is a heightened awareness of ethical issues that was previously lacking? We aim to address these questions by conducting a detailed quantitative and qualitative analysis of the ACL Anthology, as well as comparing the trends in our field to those of other related disciplines, such as cognitive science, machine learning, data mining, and systems",
    "checked": true,
    "id": "f29d5cb8f405903fc8af7a5d7ab4bf7d65796e95",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Sebastin Santy",
      "Anku Rani",
      "Monojit Choudhury"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.415": {
    "title": "As Easy as 1, 2, 3: Behavioural Testing of NMT Systems for Numerical Translation",
    "volume": "findings",
    "abstract": "Mistranslated numbers have the potential to cause serious effects, such as financial loss or medical misinformation. In this work we develop comprehensive assessments of the robustness of neural machine translation systems to numerical text via behavioural testing. We explore a variety of numerical translation capabilities a system is expected to exhibit and design effective test examples to expose system underperformance. We find that numerical mistranslation is a general issue: major commercial systems and state-of-the-art research models fail on many of our test examples, for highand low-resource languages. Our tests reveal novel errors that have not previously been reported in NMT systems, to the best of our knowledge. Lastly, we discuss strategies to mitigate numerical mistranslation",
    "checked": true,
    "id": "9afec11bc43451b920b390359a68a559dd380da7",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Jun Wang",
      "Chang Xu",
      "Francisco Guzmán",
      "Ahmed El-Kishky",
      "Benjamin Rubinstein",
      "Trevor Cohn"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.416": {
    "title": "Investigating Memorization of Conspiracy Theories in Text Generation",
    "volume": "findings",
    "abstract": "The adoption of natural language generation (NLG) models can leave individuals vulnerable to the generation of harmful information memorized by the models, such as conspiracy theories. While previous studies examine conspiracy theories in the context of social media, they have not evaluated their presence in the new space of generative language models. In this work, we investigate the capability of language models to generate conspiracy theory text. Specifically, we aim to answer: can we test pretrained generative language models for the memorization and elicitation of conspiracy theories without access to the model's training data? We highlight the difficulties of this task and discuss it in the context of memorization, generalization, and hallucination. Utilizing a new dataset consisting of conspiracy theory topics and machine-generated conspiracy theories helps us discover that many conspiracy theories are deeply rooted in the pretrained language models. Our experiments demonstrate a relationship between model parameters such as size and temperature and their propensity to generate conspiracy theory text. These results indicate the need for a more thorough review of NLG applications before release and an indepth discussion of the drawbacks of memorization in generative language models",
    "checked": true,
    "id": "5ef0554efe188ef7ab7f89a67bc9c1e3b31eeffb",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Sharon Levy",
      "Michael Saxon",
      "William Yang Wang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.417": {
    "title": "A Text-Centered Shared-Private Framework via Cross-Modal Prediction for Multimodal Sentiment Analysis",
    "volume": "findings",
    "abstract": "Multimodal fusion is a core problem for multimodal sentiment analysis. Previous works usually treat all three modal features equally and implicitly explore the interactions between different modalities. In this paper, we break this kind of methods in two ways. Firstly, we observe that textual modality plays the most important role in multimodal sentiment analysis, and this can be seen from the previous works. Secondly, we observe that comparing to the textual modality, the other two kinds of nontextual modalities (visual and acoustic) can provide two kinds of semantics, shared and private semantics. The shared semantics from the other two modalities can obviously enhance the textual semantics and make the sentiment analysis model more robust, and the private semantics can be complementary to the textual semantics and meanwhile provide different views to improve the performance of sentiment analysis together with the shared semantics. Motivated by these two observations, we propose a text-centered shared-private framework (TCSP) for multimodal fusion, which consists of the cross-modal prediction and sentiment regression parts. Experiments on the MOSEI and MOSI datasets demonstrate the effectiveness of our shared-private framework, which outperforms all baselines. Furthermore, our approach provides a new way to utilize the unlabeled data for multimodal sentiment analysis",
    "checked": true,
    "id": "1698871ecc3e9cfac5994e7a80693c0b6cceefd0",
    "semantic_title": "",
    "citation_count": 23,
    "authors": [
      "Yang Wu",
      "Zijie Lin",
      "Yanyan Zhao",
      "Bing Qin",
      "Li-Nan Zhu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.418": {
    "title": "What Would a Teacher Do? Predicting Future Talk Moves",
    "volume": "findings",
    "abstract": "Recent advances in natural language processing (NLP) have the ability to transform how classroom learning takes place. Combined with the increasing integration of technology in today's classrooms, NLP systems leveraging question answering and dialog processing techniques can serve as private tutors or participants in classroom discussions to increase student engagement and learning. To progress towards this goal, we use the classroom discourse framework of academically productive talk (APT) to learn strategies that make for the best learning experience. In this paper, we introduce a new task, called future talk move prediction (FTMP): it consists of predicting the next talk move – an utterance strategy from APT – given a conversation history with its corresponding talk moves. We further introduce a neural network model for this task, which outperforms multiple baselines by a large margin. Finally, we compare our model's performance on FTMP to human performance and show several similarities between the two",
    "checked": true,
    "id": "7e63226e228ffb8cbb6e3c55ae5b83efb845c4b8",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Ananya Ganesh",
      "Martha Palmer",
      "Katharina Kann"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.419": {
    "title": "BioGen: Generating Biography Summary under Table Guidance on Wikipedia",
    "volume": "findings",
    "abstract": "Capturing the salient information from an input article has been a long-standing challenge for summarization. On Wikipedia, most of the wiki pages about people contain a factual table that lists the basic properties of the people. Illuminatingly, a factual table can be regarded as a natural summary of the key information in the corresponding article. Thus, in this paper we propose the task of tableguided abstractive biography summarization, which utilizes factual tables to capture important information and then generate a summary of a biography. We first introduce the TaGS (Table-Guided Summarization) dataset1, the first large-scale biography summarization dataset with tables. Next, we report some statistics about this dataset to validate the quality of the dataset. We also benchmark several commonly used summarization methods on TaGS and hope this will inspire more exciting methods",
    "checked": true,
    "id": "0bab7c91be83f836ee3f76154277246676babe50",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shen Gao",
      "Xiuying Chen",
      "Chang Liu",
      "Dongyan Zhao",
      "Rui Yan"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.420": {
    "title": "Multilingual Simultaneous Neural Machine Translation",
    "volume": "findings",
    "abstract": "Simultaneous machine translation (SIMT) involves translating source utterances to the target language in real-time before the speaker utterance completes. This paper proposes the multilingual approach to SIMT, where a single model simultaneously translates between multiple language-pairs. This not only results in more efficiency in terms of the number of models and parameters (hence simpler deployment), but may also lead to higher performing models by capturing commonalities among the languages. We further explore simple and effective multilingual architectures based on two strong recently proposed SIMT models. Our results on translating from two Germanic languages (German, Dutch) and three Romance languages (French, Italian, Romanian) into English show (i) the single multilingual model is on-par or better than individual models, and (ii) multilingual SIMT models trained based on language families are on-par or better than the universal model trained for all languages.1",
    "checked": true,
    "id": "3eb8cb91a91819ec4d68d1fa61ef12cd43b4492f",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Philip Arthur",
      "Dongwon Ryu",
      "Gholamreza Haffari"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.421": {
    "title": "Cross-Domain Review Generation for Aspect-Based Sentiment Analysis",
    "volume": "findings",
    "abstract": "Supervised learning methods have proven to be effective for Aspect-Based Sentiment Analysis (ABSA). However, the lack of finegrained labeled data hinders their effectiveness in many domains. To address this issue, unsupervised domain adaptation methods are desired to transfer knowledge from a labeled source domain to any unlabeled target domain. In this paper, we propose a new domain adaptation paradigm called cross-domain review generation (CDRG), which aims to generate target-domain reviews with fine-grained annotation based on the source-domain labeled reviews. To achieve this goal, we propose a two-step approach as a concrete realization of CDRG. It first converts a sourcedomain review to a domain-independent review by masking its source-specific attributes, and then converts the domain-independent review to a target-domain review with a masked language model pre-trained in the target domain. We further propose two ways to leverage the generated target-domain reviews for two cross-domain ABSA tasks. Extensive experiments demonstrate the superiority of our CDRG-based approaches over the state-of-theart domain adaptation methods",
    "checked": true,
    "id": "cede7dce7b826d6442350a16e3bbcec9da923bfa",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Jianfei Yu",
      "Chenggong Gong",
      "Rui Xia"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.422": {
    "title": "On the Language Coverage Bias for Neural Machine Translation",
    "volume": "findings",
    "abstract": "Language coverage bias, which indicates the content-dependent differences between sentence pairs originating from the source and target languages, is important for neural machine translation (NMT) because the target-original training data is not well exploited in current practice. By carefully designing experiments, we provide comprehensive analyses of the language coverage bias in the training data, and find that using only the source-original data achieves comparable performance with using full training data. Based on these observations, we further propose two simple and effective approaches to alleviate the language coverage bias problem through explicitly distinguishing between the sourceand target-original training data, which consistently improve the performance over strong baselines on six WMT20 translation tasks. Complementary to the translationese effect, language coverage bias provides another explanation for the performance drop caused by back-translation (Marie et al., 2020). We also apply our approach to both backand forward-translation and find that mitigating the language coverage bias can improve the performance of both the two representative data augmentation methods and their tagged variants (Caswell et al., 2019)",
    "checked": true,
    "id": "0f192e9c7a1e3fdc6e051fc502f74b04c53bb3a3",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Shuo Wang",
      "Zhaopeng Tu",
      "Zhixing Tan",
      "Shuming Shi",
      "Maosong Sun",
      "Yang Liu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.423": {
    "title": "Named Entity Recognition via Noise Aware Training Mechanism with Data Filter",
    "volume": "findings",
    "abstract": "Named entity recognition (NER) is a fundamental task in natural language processing, these is a long held belief that datasets benefit the model. However, not all the data help with generalization, and some samples may contain ambiguous entities or noisy labels. The existing methods can not distinguish hard samples from noisy samples well, and becomes particularly challenging in the case of overfitting. This paper proposes a new method called Noise-Aware-with-Filter (NAF) to solve the issues from two sides. From the perspective of the data, we design a Logit-MaximumDifference (LMD) mechanism, which maximizes the diversity between different samples to help the model identify noisy samples. From the perspective of the model, we design an Incomplete-Trust (In-trust) loss function, which boosts LCRF with a robust DistrustCross-Entropy(DCE) term. Our proposed Intrust can effectively alleviate the overfitting caused by previous loss function. Experiments on six real-world Chinese and English NER datasets show that NAF outperforms the previous methods, and which obtained the state-ofthe-art(SOTA) results on the CoNLL2003 and CoNLL++ datasets",
    "checked": true,
    "id": "6110581bd5650ebdf777f008b10e1db6a6e03bc0",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Xiusheng Huang",
      "Yubo Chen",
      "Shun Wu",
      "Jun Zhao",
      "Yuantao Xie",
      "Weijian Sun"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.424": {
    "title": "A Multi-Task Approach for Improving Biomedical Named Entity Recognition by Incorporating Multi-Granularity information",
    "volume": "findings",
    "abstract": "Neural biomedical named entity recognition (BioNER) methods usually require a large amount of annotated data, while the annotated BioNER datasets are often difficult to obtain and small in scale due to the limitations of privacy, ethics and high degree of specialization. To alleviate the lack of training samples, unlike conventional methods that only use token-level information, this paper proposes a method that simultaneously utilize the latent multi-granularity information in the dataset. Concretely, the proposed model is based on a multi-task approach, which leverages different training objectives by introducing auxiliary tasks, i.e. binary classification, multi-class and multi-token classification. Experimental results over three BioNER datasets show that the proposed model produces better performance over the BioBERT baseline and can get more than 3% improvements of F1score in low-resource scenarios. Finally, we released our code at https://github.com/ zgzjdx/MT-BioNER",
    "checked": true,
    "id": "d888ac9e9ac54dacb5fa8b6ce29ee6ecf64d9710",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Yiqi Tong",
      "Yidong Chen",
      "Xiaodong Shi"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.425": {
    "title": "EBERT: Efficient BERT Inference with Dynamic Structured Pruning",
    "volume": "findings",
    "abstract": "Pruning has been demonstrated as an effective way of reducing computational complexity for deep networks, especially CNNs for computer vision tasks. In this paper, we investigate the opportunity to accelerate the inference of large-scale pre-trained language model via pruning. We propose EBERT, a dynamic structured pruning algorithm for efficient BERT inference. Unlike previous methods that randomly prune the model weights for static inference, EBERT dynamically determines and prunes the unimportant heads in multi-head self-attention layers and the unimportant structured computations in feed-forward network for each input sample at run-time. Experimental results show that our proposed EBERT outperforms other state-of-the-art methods on different tasks",
    "checked": true,
    "id": "cbde5598c1a78285adfcfd77fb3636f5498987a0",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Zejian Liu",
      "Fanrong Li",
      "Gang Li",
      "Jian Cheng"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.426": {
    "title": "Strong and Light Baseline Models for Fact-Checking Joint Inference",
    "volume": "findings",
    "abstract": "How to combine several pieces of evidence to verify a claim is an interesting semantic task. Very complex methods have been proposed, combining different evidence vectors using an evidence interaction graph. In this paper, we show that in case of inference based on transformer models, two effective approaches use either (i) a simple application of max pooling over the Transformer evidence vectors; or (ii) computing a weighted sum of the evidence vectors. Our experiments on the FEVER claim verification task show that the methods above achieve the state of the art, constituting strong baseline for much more computationally complex methods",
    "checked": true,
    "id": "4460d4f468e266ef69b8572c5f7704b2e55fe73b",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Kateryna Tymoshenko",
      "Alessandro Moschitti"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.427": {
    "title": "Sketch and Refine: Towards Faithful and Informative Table-to-Text Generation",
    "volume": "findings",
    "abstract": "Table-to-text generation refers to generating a descriptive text from a key-value table. Traditional autoregressive methods, though can generate text with high fluency, suffer from low coverage and poor faithfulness problems. To mitigate these problems, we propose a novel Skeleton-based two-stage method that combines both Autoregressive and NonAutoregressive generation (SANA). Our approach includes: (1) skeleton generation with an autoregressive pointer network to select key tokens from the source table; (2) edit-based non-autoregressive generation model to produce texts via iterative insertion and deletion operations. By integrating hard constraints from the skeleton, the non-autoregressive model improves the generation's coverage over the source table and thus enhances its faithfulness. We conduct experiments on both the WikiPerson and WikiBio datasets. Experimental results demonstrate that our method outperforms the previous state-of-the-art methods in both automatic and human evaluation, especially on coverage and faithfulness. In particular, we achieve PARENT-T recall of 99.47 in WikiPerson, improving over the existing best results by more than 10 points",
    "checked": true,
    "id": "c4d0df5f7a700f8c999bb2fa5a7a9df0b2d2dd97",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Peng Wang",
      "Junyang Lin",
      "An Yang",
      "Chang Zhou",
      "Yichang Zhang",
      "Jingren Zhou",
      "Hongxia Yang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.428": {
    "title": "TILGAN: Transformer-based Implicit Latent GAN for Diverse and Coherent Text Generation",
    "volume": "findings",
    "abstract": "Conventional autoregressive models have achieved great success in text generation but suffer from the exposure bias problem in that token sequences in the training and in the generation stages are mismatched. While generative adversarial networks (GANs) can remedy this problem, existing implementations of GANs directly on discrete outputs tend to be unstable and lack diversity. In this work, we propose TILGAN, a Transformerbased Implicit Latent GAN, which combines a Transformer autoencoder and GAN in the latent space with a novel design and distribution matching based on the Kullback-Leibler (KL) divergence. Specifically, to improve local and global coherence, we explicitly introduce a multi-scale discriminator to capture the semantic information at varying scales among the sequence of hidden representations encoded by Transformer. Moreover, the decoder is enhanced by an additional KL loss to be consistent with the latent-generator. Experimental results on three benchmark datasets demonstrate the validity and effectiveness of our model, by obtaining significant improvements and a better quality-diversity trade-off in automatic and human evaluation for both unconditional and conditional generation tasks.1",
    "checked": true,
    "id": "c61459f2ddf053a2768105c00a020ec4a81b1091",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Shizhe Diao",
      "Xinwei Shen",
      "Kashun Shum",
      "Yan Song",
      "Tong Zhang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.429": {
    "title": "John praised Mary because _he_? Implicit Causality Bias and Its Interaction with Explicit Cues in LMs",
    "volume": "findings",
    "abstract": "Some interpersonal verbs can implicitly attribute causality to either their subject or their object and are therefore said to carry an implicit causality (IC) bias. Through this bias, causal links can be inferred from a narrative, aiding language comprehension. We investigate whether pre-trained language models (PLMs) encode IC bias and use it at inference time. We find that to be the case, albeit to different degrees, for three distinct PLM architectures. However, causes do not always need to be implicit—when a cause is explicitly stated in a subordinate clause, an incongruent IC bias associated with the verb in the main clause leads to a delay in human processing. We hypothesize that the temporary challenge humans face in integrating the two contradicting signals, one from the lexical semantics of the verb, one from the sentence-level semantics, would be reflected in higher error rates for models on tasks dependent on causal links. The results of our study lend support to this hypothesis, suggesting that PLMs tend to prioritize lexical patterns over higher-order signals",
    "checked": true,
    "id": "38cfcd4681cf85ab507ec0586c753182a4c8eecb",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Yova Kementchedjhieva",
      "Mark Anderson",
      "Anders Søgaard"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.430": {
    "title": "Do It Once: An Embarrassingly Simple Joint Matching Approach to Response Selection",
    "volume": "findings",
    "abstract": "Existing matching models for response selection adopt the independent matching (IM) approach. To complete a prediction, they have to perform N independent matches, where N is the number of response options. In this paper, we explore a joint matching (JM) approach which performs matching only once regardless of the number of options. The JM approach does not change the structure of matching component but only modifies its input and output format. It also enables a cheap but effective data augmentation method. Extensive experiments on the MuTual dataset demonstrate that, even with the simplest formulation, JM outperforms IM approach by a large margin and reduces training time by over half",
    "checked": true,
    "id": "c3d4f9f721a2b1164f043d7ca2db10daaeb19e68",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Linhao Zhang",
      "Dehong Ma",
      "Sujian Li",
      "Houfeng Wang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.431": {
    "title": "Climbing the Tower of Treebanks: Improving Low-Resource Dependency Parsing via Hierarchical Source Selection",
    "volume": "findings",
    "abstract": "Recent work on multilingual dependency parsing focused on developing highly multilingual parsers that can be applied to a wide range of low-resource languages. In this work, we substantially outperform such \"one model to rule them all\" approach with a heuristic selection of languages and treebanks on which to train the parser for a specific target language. Our approach, dubbed TOWER, first hierarchically clusters all Universal Dependencies languages based on their mutual syntactic similarity computed from human-coded URIEL vectors. For each low-resource target language, we then climb this language hierarchy starting from the leaf node of that language and heuristically choose the hierarchy level at which to collect training treebanks. This treebank selection heuristic is based on: (i) the aggregate size of all treebanks subsumed by the hierarchy level and (ii) the similarity of the languages in the training sample with the target language. For languages without development treebanks, we additionally use (ii) for model selection (i.e., early stopping) in order to prevent overfitting to development treebanks of closest languages. Our TOWER approach shows substantial gains for low-resource languages over two state-ofthe-art multilingual parsers, with more than 20 LAS point gains for some of those languages. Parsing models and code available at: https: //github.com/codogogo/towerparse",
    "checked": true,
    "id": "d7fe1ddd5c2cf02d640d16a70a418562eac5963a",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Goran Glavaš",
      "Ivan Vulić"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.432": {
    "title": "Enhancing the Open-Domain Dialogue Evaluation in Latent Space",
    "volume": "findings",
    "abstract": "The notorious one-to-many nature of opendomain dialogues poses huge challenges for automatic evaluation methods. Recent studies attempt to mitigate this issue by considering the similarity of the generated response with the conversational context and design discriminative models to learn from multiple positive responses. Despite the promising results, they can not be applied to general scenarios where training data with multiple responses is unavailable. To this end, in this paper, we propose a self-supervised setting to obtain a smooth latent space that can both capture discourse-level context information and implicitly model more references in latent space. Specifically, we present EMS, an Enhanced dialogue evaluation Metric in latent Space. Experimental results on two real-world dialogue datasets confirm the superiority of our method for open-domain dialogue evaluation, where both Pearson and Spearman correlations with human judgments outperform all baselines",
    "checked": true,
    "id": "9588603547d14c81beca87f6de399334b8d3645d",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Zhangming Chan",
      "Lemao Liu",
      "Juntao Li",
      "Haisong Zhang",
      "Dongyan Zhao",
      "Shuming Shi",
      "Rui Yan"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.433": {
    "title": "Adapting Monolingual Models: Data can be Scarce when Language Similarity is High",
    "volume": "findings",
    "abstract": "For many (minority) languages, the resources needed to train large models are not available. We investigate the performance of zero-shot transfer learning with as little data as possible, and the influence of language similarity in this process. We retrain the lexical layers of four BERT-based models using data from two low-resource target language varieties, while the Transformer layers are independently finetuned on a POS-tagging task in the model's source language. By combining the new lexical layers and fine-tuned Transformer layers, we achieve high task performance for both target languages. With high language similarity, 10MB of data appears sufficient to achieve substantial monolingual transfer performance. Monolingual BERT-based models generally achieve higher downstream task performance after retraining the lexical layer than multilingual BERT, even when the target language is included in the multilingual model",
    "checked": true,
    "id": "15e653d727fe90e627a328e0ebe9d47cb93cc5f1",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Wietse de Vries",
      "Martijn Bartelds",
      "Malvina Nissim",
      "Martijn Wieling"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.434": {
    "title": "BatchMixup: Improving Training by Interpolating Hidden States of the Entire Mini-batch",
    "volume": "findings",
    "abstract": "Usually, we train a neural system on a sequence of mini-batches of labeled instances. Each mini-batch is composed of k samples, and each sample will learn a representation vector. MIXUP implicitly generates synthetic samples through linearly interpolating inputs and their corresponding labels of random sample pairs in the same mini-batch. This means that MIXUP only generates new points on the edges connecting every two original points in the representation space. We observed that the new points by the standard MIXUP cover pretty limited regions in the entire space of the mini-batch. In this work, we propose BATCHMIXUP—improving the model learning by interpolating hidden states of the entire mini-batch. BATCHMIXUP can generate new points scattered throughout the space corresponding to the mini-batch. In experiments, BATCHMIXUP shows superior performance than competitive baselines in improving the performance of NLP tasks while using different ratios of training data",
    "checked": true,
    "id": "bcf9b9238aef3c69f10d5a05136feb71f9a3de79",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Wenpeng Yin",
      "Huan Wang",
      "Jin Qu",
      "Caiming Xiong"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.435": {
    "title": "DocNLI: A Large-scale Dataset for Document-level Natural Language Inference",
    "volume": "findings",
    "abstract": "Natural language inference (NLI) is formulated as a unified framework for solving various NLP problems such as relation extraction, question answering, summarization, etc. It has been studied intensively in the past few years thanks to the availability of large-scale labeled datasets. However, most existing studies focus on merely sentence-level inference, which limits the scope of NLI's application in downstream NLP problems. This work presents DOCNLI — a newly-constructed large-scale dataset for document-level NLI. DOCNLI is transformed from a broad range of NLP problems and covers multiple genres of text. The premises always stay in the document granularity, whereas the hypotheses vary in length from single sentences to passages with hundreds of words. Additionally, DOCNLI has pretty limited artifacts1 which unfortunately widely exist in some popular sentence-level NLI datasets. Our experiments demonstrate that, even without fine-tuning, a model pretrained on DOCNLI shows promising performance on popular sentence-level benchmarks, and generalizes well to out-of-domain NLP tasks that rely on inference at document granularity. Task-specific fine-tuning can bring further improvements. Data, code and pretrained models can be found at https://github. com/salesforce/DocNLI",
    "checked": true,
    "id": "42f8a3da7021bc725fa14fdb63fa9c7c9fc934f6",
    "semantic_title": "",
    "citation_count": 47,
    "authors": [
      "Wenpeng Yin",
      "Dragomir Radev",
      "Caiming Xiong"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.436": {
    "title": "Rule Augmented Unsupervised Constituency Parsing",
    "volume": "findings",
    "abstract": "Recently, unsupervised parsing of syntactic trees has gained considerable attention. A prototypical approach to such unsupervised parsing employs reinforcement learning and auto-encoders. However, no mechanism ensures that the learnt model leverages the wellunderstood language grammar. We propose an approach that utilizes very generic linguistic knowledge of the language present in the form of syntactic rules, thus inducing better syntactic structures. We introduce a novel formulation that takes advantage of the syntactic grammar rules and is independent of the base system. We achieve new state-of-the-art results on two benchmarks datasets, MNLI and WSJ.1",
    "checked": true,
    "id": "9dbc8ba1bca2336ab4d07530882a7ca8b78cb925",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Atul Sahay",
      "Anshul Nasery",
      "Ayush Maheshwari",
      "Ganesh Ramakrishnan",
      "Rishabh Iyer"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.437": {
    "title": "Are Multilingual Models the Best Choice for Moderately Under-resourced Languages? A Comprehensive Assessment for Catalan",
    "volume": "findings",
    "abstract": "Multilingual language models have been a crucial breakthrough as they considerably reduce the need of data for under-resourced languages. Nevertheless, the superiority of language-specific models has already been proven for languages having access to large amounts of data. In this work, we focus on Catalan with the aim to explore to what extent a medium-sized monolingual language model is competitive with state-of-the-art large multilingual models. For this, we: (1) build a clean, high-quality textual Catalan corpus (CaText), the largest to date (but only a fraction of the usual size of the previous work in monolingual language models), (2) train a Transformerbased language model for Catalan (BERTa), and (3) devise a thorough evaluation in a diversity of settings, comprising a complete array of downstream tasks, namely, Part of Speech Tagging, Named Entity Recognition and Classification, Text Classification, Question Answering, and Semantic Textual Similarity, with most of the corresponding datasets being created ex novo. The result is a new benchmark, the Catalan Language Understanding Benchmark (CLUB), which we publish as an open resource, together with the clean textual corpus, the language model, and the cleaning pipeline. Using state-of-the-art multilingual models and a monolingual model trained only on Wikipedia as baselines, we consistently observe the superiority of our model across tasks and settings",
    "checked": true,
    "id": "da7f2f8e7c4e68e995c8eeda324b8cfaeeb27f10",
    "semantic_title": "",
    "citation_count": 22,
    "authors": [
      "Jordi Armengol-Estapé",
      "Casimiro Pio Carrino",
      "Carlos Rodriguez-Penagos",
      "Ona de Gibert Bonet",
      "Carme Armentano-Oller",
      "Aitor Gonzalez-Agirre",
      "Maite Melero",
      "Marta Villegas"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.438": {
    "title": "How transfer learning impacts linguistic knowledge in deep NLP models?",
    "volume": "findings",
    "abstract": "Transfer learning from pre-trained neural language models towards downstream tasks has been a predominant theme in NLP recently. Several researchers have shown that deep NLP models learn non-trivial amount of linguistic knowledge, captured at different layers of the model. We investigate how fine-tuning towards downstream NLP tasks impacts the learned linguistic knowledge. We carry out a study across popular pre-trained models BERT, RoBERTa and XLNet using layer and neuronlevel diagnostic classifiers. We found that for some GLUE tasks, the network relies on the core linguistic information and preserve it deeper in the network, while for others it forgets. Linguistic information is distributed in the pre-trained language models but becomes localized to the lower layers post-fine-tuning, reserving higher layers for the task specific knowledge. The pattern varies across architectures, with BERT retaining linguistic information relatively deeper in the network compared to RoBERTa and XLNet, where it is predominantly delegated to the lower layers",
    "checked": true,
    "id": "40b3bebc595ca091b4ee654e12272ad9201c04dc",
    "semantic_title": "",
    "citation_count": 31,
    "authors": [
      "Nadir Durrani",
      "Hassan Sajjad",
      "Fahim Dalvi"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.439": {
    "title": "Language Models Use Monotonicity to Assess NPI Licensing",
    "volume": "findings",
    "abstract": "We investigate the semantic knowledge of language models (LMs), focusing on (1) whether these LMs create categories of linguistic environments based on their semantic monotonicity properties, and (2) whether these categories play a similar role in LMs as in human language understanding, using negative polarity item licensing as a case study. We introduce a series of experiments consisting of probing with diagnostic classifiers (DCs), linguistic acceptability tasks, as well as a novel DC ranking method that tightly connects the probing results to the inner workings of the LM. By applying our experimental pipeline to LMs trained on various filtered corpora, we are able to gain stronger insights into the semantic generalizations that are acquired by these models",
    "checked": true,
    "id": "2d3e4e5690b992d3099caa1606d4310a0d632868",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Jaap Jumelet",
      "Milica Denic",
      "Jakub Szymanik",
      "Dieuwke Hupkes",
      "Shane Steinert-Threlkeld"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.440": {
    "title": "Slot Transferability for Cross-domain Slot Filling",
    "volume": "findings",
    "abstract": "Cross-domain slot filling focuses on using labeled data from source domains to train a slot filling model for target domains. It is of great significance for transferring a dialogue system into new domains. Most of the existing work focused on building a cross-domain transfer model. From the perspective of slots themselves, this paper proposes a model-agnostic Slot Transferability Measure (STM) for evaluating the transferability from a source slot to a target slot, specifically, the degree that labeled data of the source slot is helpful to train the slot filling model for the target slot. We also give a STM-based method for a model to select helpful source slots and their labeled data for a given target slot. Experimental results on multiple existing models and datasets show that our method significantly outperforms state-ofthe-art baselines in cross-domain slot filling. The code is available at https://github. com/luhengtong/STM-for-cdsf.git",
    "checked": true,
    "id": "47bd2e1e3a56a2226649e7b912f5bcccdc978087",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Hengtong Lu",
      "Zhuoxin Han",
      "Caixia Yuan",
      "Xiaojie Wang",
      "Shuyu Lei",
      "Huixing Jiang",
      "Wei Wu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.441": {
    "title": "Word Graph Guided Summarization for Radiology Findings",
    "volume": "findings",
    "abstract": "Radiology reports play a critical role in communicating medical findings to physicians. In each report, the impression section summarizes essential radiology findings. In clinical practice, writing impression is highly demanded yet time-consuming and prone to errors for radiologists. Therefore, automatic impression generation has emerged as an attractive research direction to facilitate such clinical practice. Existing studies mainly focused on introducing salient word information to the general text summarization framework to guide the selection of the key content in radiology findings. However, for this task, a model needs not only capture the important words in findings but also accurately describe their relations so as to generate highquality impressions. In this paper, we propose a novel method for automatic impression generation, where a word graph is constructed from the findings to record the critical words and their relations, then a Word Graph guided Summarization model (WGSUM) is designed to generate impressions with the help of the word graph. Experimental results on two datasets, OPENI and MIMIC-CXR, confirm the validity and effectiveness of our proposed approach, where the state-of-the-art results are achieved on both datasets. Further experiments are also conducted to analyze the impact of different graph designs to the performance of our method.1",
    "checked": true,
    "id": "008721e4f9cb9b2d3242bc31af48db6fb3f8727d",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Jinpeng Hu",
      "Jianling Li",
      "Zhihong Chen",
      "Yaling Shen",
      "Yan Song",
      "Xiang Wan",
      "Tsung-Hui Chang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.442": {
    "title": "Generalized Supervised Attention for Text Generation",
    "volume": "findings",
    "abstract": "The attention-based encoder-decoder framework is widely used in many natural language generation tasks. The attention mechanism builds alignments between target words and source items that facilitate text generation. Previous work proposes supervised attention that uses human knowledge to guide the attention mechanism to learn better alignments. However, well-designed supervision built from ideal alignments can be costly or even infeasible. In this paper, we build a Generalized Supervised Attention method (GSA) based on quasi alignments, which specify candidate sets of alignments and are much easier to obtain than ideal alignments. We design a Summation Cross-Entropy (SCE) loss and a Supervised Multiple Attention (SMA) structure to accommodate quasi alignments. Experiments on three text generation tasks demonstrate that GSA improves generation performance and is robust against errors in attention supervision",
    "checked": true,
    "id": "44e34bddc5f0bf794b40271f958159970ff973ef",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Yixian Liu",
      "Liwen Zhang",
      "Xinyu Zhang",
      "Yong Jiang",
      "Yue Zhang",
      "Kewei Tu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.443": {
    "title": "Uncertainty Aware Review Hallucination for Science Article Classification",
    "volume": "findings",
    "abstract": "The high subjectivity and costs inherent in peer reviewing have recently motivated the preliminary design of machine learning-based acceptance decision methods. However, such approaches are limited in that they: a) do not explore the usage of both the reviewer and area chair recommendations, b) do not explicitly model subjectivity on a per submission basis, and c) are not applicable in realistic settings, by assuming that review texts are available at test time, when these are exactly the inputs that should be considered to be missing in this application. We propose to utilise methods that model the aleatory uncertainty of the submissions, while also exploring different loss importance interpolations between area chair and reviewers' recommendations. We also propose a modality hallucination approach to impute review representations at test time, providing the first realistic evaluation framework for this challenging task",
    "checked": true,
    "id": "c69a5ebbca886a0fa288ec9c60b9e31ee21edbd9",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Korbinian Friedl",
      "Georgios Rizos",
      "Lukas Stappen",
      "Madina Hasan",
      "Lucia Specia",
      "Thomas Hain",
      "Björn Schuller"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.444": {
    "title": "Automatically Select Emotion for Response via Personality-affected Emotion Transition",
    "volume": "findings",
    "abstract": "To provide consistent emotional interaction with users, dialog systems should be capable to automatically select appropriate emotions for responses like humans. However, most existing works focus on rendering specified emotions in responses or empathetically respond to the emotion of users, yet the individual difference in emotion expression is overlooked. This may lead to inconsistent emotional expressions and disinterest users. To tackle this issue, we propose to equip the dialog system with personality and enable it to automatically select emotions in responses by simulating the emotion transition of humans in conversation. In detail, the emotion of the dialog system is transitioned from its preceding emotion in context. The transition is triggered by the preceding dialog context and affected by the specified personality trait. To achieve this, we first model the emotion transition in the dialog system as the variation between the preceding emotion and the response emotion in the Valence-Arousal-Dominance (VAD) emotion space. Then, we design neural networks to encode the preceding dialog context and the specified personality traits to compose the variation. Finally, the emotion for response is selected from the sum of the preceding emotion and the variation. We construct a dialog dataset with emotion and personality labels and conduct emotion prediction tasks for evaluation. Experimental results validate the effectiveness of the personality-affected emotion transition.1",
    "checked": true,
    "id": "34f9d7ffc2e76038c4af316c4776a2dbbeec564c",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Zhiyuan Wen",
      "Jiannong Cao",
      "Ruosong Yang",
      "Shuaiqi Liu",
      "Jiaxing Shen"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.445": {
    "title": "Highlight-Transformer: Leveraging Key Phrase Aware Attention to Improve Abstractive Multi-Document Summarization",
    "volume": "findings",
    "abstract": "Abstractive multi-document summarization aims to generate a comprehensive summary covering salient content from multiple input documents. Compared with previous RNNbased models, the Transformer-based models employ the self-attention mechanism to capture the dependencies in input documents and can generate better summaries. Existing works have not considered key phrases in determining attention weights of self-attention. Consequently, some of the tokens within key phrases only receive small attention weights. It can affect completely encoding key phrases that convey the salient ideas of input documents. In this paper, we introduce the HighlightTransformer, a model with the highlighting mechanism in the encoder to assign greater attention weights for the tokens within key phrases. We propose two structures of highlighting attention for each head and the multihead highlighting attention. The experimental results on the Multi-News dataset show that our proposed model significantly outperforms the competitive baseline models.ive multi-document summarization aims to generate a comprehensive summary covering salient content from multiple input documents. Compared with previous RNNbased models, the Transformer-based models employ the self-attention mechanism to capture the dependencies in input documents and can generate better summaries. Existing works have not considered key phrases in determining attention weights of self-attention. Consequently, some of the tokens within key phrases only receive small attention weights. It can affect completely encoding key phrases that convey the salient ideas of input documents. In this paper, we introduce the HighlightTransformer, a model with the highlighting mechanism in the encoder to assign greater attention weights for the tokens within key phrases. We propose two structures of highlighting attention for each head and the multihead highlighting attention. The experimental results on the Multi-News dataset show that our proposed model significantly outperforms the competitive baseline models",
    "checked": true,
    "id": "e885c810fbe23bd730eb80631c488556c9fd1d85",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Shuaiqi Liu",
      "Jiannong Cao",
      "Ruosong Yang",
      "Zhiyuan Wen"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.446": {
    "title": "Phrase-Level Action Reinforcement Learning for Neural Dialog Response Generation",
    "volume": "findings",
    "abstract": "Defining a sophisticated action space for a dialog agent is essential for efficient training with reinforcement learning (RL). Recent work introduces discrete latent variables to use as an action space; however, a limitation is that a global vector can contain entangled information such as dialog act, sentence structure, and content. This sacrifices the flexibility of the response generation. In this paper, we propose phrase-level action reinforcement learning (PHRASERL), which allows the model to flexibly alter the sentence structure and content with the sequential action selection. Our model first learns to generate useful phrases during the supervised pre-training, and then further trained to form a response by rearranging the phrases with reinforcement learning. Experiments on the MultiWOZ dataset show that our model achieves competitive results with state-of-the-art models on automatic evaluation metrics, indicating that our phrase-level action space has improved flexibility and is effective for solving task-oriented dialogs",
    "checked": true,
    "id": "d0babb55df48d544f4765a29fba7fd6c854d03ba",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Takato Yamazaki",
      "Akiko Aizawa"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.447": {
    "title": "Automatic Speech Recognition in Sanskrit: A New Speech Corpus and Modelling Insights",
    "volume": "findings",
    "abstract": "Automatic speech recognition (ASR) in Sanskrit is interesting, owing to the various linguistic peculiarities present in the language. The Sanskrit language is lexically productive, undergoes euphonic assimilation of phones at the word boundaries and exhibits variations in spelling conventions and in pronunciations. In this work, we propose the first large scale study of automatic speech recognition (ASR) in Sanskrit, with an emphasis on the impact of unit selection in Sanskrit ASR. In this work, we release a 78 hour ASR dataset for Sanskrit, which faithfully captures several of the linguistic characteristics expressed by the language. We investigate the role of different acoustic model and language model units in ASR systems for Sanskrit. We also propose a newmodelling unit, inspired by the syllable level unit selection, that captures character sequences from one vowel in the word to the next vowel. We also highlight the importance of choosing graphemic representations for Sanskrit and show the impact of this choice on word error rates (WER). Finally, we extend these insights from Sanskrit ASR for building ASR systems in two other Indic languages, Gujarati and Telugu. For both these languages, our experimental results show that the use of phonetic based graphemic representations in ASR results in performance improvements as compared to ASR systems that use native scripts.1",
    "checked": true,
    "id": "5fd1269f7258d33c06022553c1c983c9e9218ab4",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Devaraja Adiga",
      "Rishabh Kumar",
      "Amrith Krishna",
      "Preethi Jyothi",
      "Ganesh Ramakrishnan",
      "Pawan Goyal"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.448": {
    "title": "Constraint based Knowledge Base Distillation in End-to-End Task Oriented Dialogs",
    "volume": "findings",
    "abstract": "End-to-End task-oriented dialogue systems generate responses based on dialog history and an accompanying knowledge base (KB). Inferring those KB entities that are most relevant for an utterance is crucial for response generation. Existing state of the art scales to large KBs by softly filtering over irrelevant KB information. In this paper, we propose a novel filtering technique that consists of (1) a pairwise similarity based filter that identifies relevant information by respecting the n-ary structure in a KB record. and, (2) an auxiliary loss that helps in separating contextually unrelated KB information. We also propose a new metric – multiset entity F1 which fixes a correctness issue in the existing entity F1 metric. Experimental results on three publicly available task-oriented dialog datasets show that our proposed approach outperforms existing state-ofthe-art models",
    "checked": true,
    "id": "0201812947eff2f6ff08ccd33fcab781738e0305",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Dinesh Raghu",
      "Atishya Jain",
      "Mausam",
      "Sachindra Joshi"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.449": {
    "title": "DialogSum: A Real-Life Scenario Dialogue Summarization Dataset",
    "volume": "findings",
    "abstract": "Proposal of large-scale datasets has facilitated research on deep neural models for news summarization. Deep learning can also be potentially useful for spoken dialogue summarization, which can benefit a range of reallife scenarios including customer service management and medication tracking. To this end, we propose DIALOGSUM, a large-scale labeled dialogue summarization dataset. We conduct empirical analysis on DIALOGSUM using state-of-the-art neural summarizers. Experimental results show unique challenges in dialogue summarization, such as spoken terms, special discourse structures, coreferences and ellipsis, pragmatics and social common sense, which require specific representation learning technologies to better deal with",
    "checked": true,
    "id": "0934d7cac5a86b02fc49852334051bde540b34bd",
    "semantic_title": "",
    "citation_count": 74,
    "authors": [
      "Yulong Chen",
      "Yang Liu",
      "Liang Chen",
      "Yue Zhang"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.450": {
    "title": "What Did You Refer to? Evaluating Co-References in Dialogue",
    "volume": "findings",
    "abstract": "Existing neural end-to-end dialogue models have limitations on exactly interpreting the linguistic structures, such as ellipsis, anaphor and co-reference, etc., in dialogue history context. Therefore, it is hard to determine whether the dialogue models truly understand a dialogue or not, only depending on the coherence evaluation of their generated responses. To address these issues, in this paper, we proposed to directly measure the capability of dialogue models on understanding the entity-oriented structures via question answering and construct a new benchmark dataset, DEQA, including large-scale English and Chinese humanhuman dialogues. Experiments carried on representative dialogue models show that these models all face challenges on the proposed dialogue understanding task. The DEQA dataset will release for research use",
    "checked": true,
    "id": "44b9391d660b319e9814ece1b3d9ac6f3a98e5f1",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Wei-Nan Zhang",
      "Yue Zhang",
      "Hanlin Tang",
      "Zhengyu Zhao",
      "Caihai Zhu",
      "Ting Liu"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.451": {
    "title": "Beyond Metadata: What Paper Authors Say About Corpora They Use",
    "volume": "findings",
    "abstract": "The growing ecosystem of data sharing in science has put dataset search into the focus. To make data sharing and reuse more feasible, new retrieval tools and services are being de-veloped. Currently, dataset retrieval relies al-most exclusively on metadata provided by the publishers. To extend this knowledge source our work studies the task of \"dataset review mining\" in scientiﬁc publications. For the ﬁeld of Natural Language Processing we collect metadata about datasets from established resources such as the ELRA and LDC catalogs, and then extract review statements about the datasets from ACL Anthology Corpus publications, compiling the Webis-Dataset-Reviews-21 corpus. By analyzing the reviews we identify different categories of what paper authors write about data. To the best of our knowledge, this is the ﬁrst analysis of this kind in the ﬁeld of Natural Language Processing, albeit similar analyses have been carried out in the social and medical sciences. Our corpus and the underly-ing code are shared alongside this paper. 1,2,3",
    "checked": true,
    "id": "8f45b9abb6444495f10bd7123ee706aee0f4526d",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikolay Kolyada",
      "Martin Potthast",
      "Benno Stein"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.452": {
    "title": "Knowledge Distillation for Quality Estimation",
    "volume": "findings",
    "abstract": "Quality Estimation (QE) is the task of automatically predicting Machine Translation quality in the absence of reference translations, making it applicable in real-time settings, such as translating online social media conversations. Recent success in QE stems from the use of multilingual pre-trained representations, where very large models lead to impressive results. However, the inference time, disk and memory requirements of such models do not allow for wide usage in the real world. Models trained on distilled pre-trained representations remain prohibitively large for many usage scenarios. We instead propose to directly transfer knowledge from a strong QE teacher model to a much smaller model with a different, shallower architecture. We show that this approach, in combination with data augmentation, leads to light-weight QE models that perform competitively with distilled pre-trained representations with 8x fewer parameters",
    "checked": true,
    "id": "0f4197cb978525b300a764e7e62000c89ef0a23a",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Amit Gajbhiye",
      "Marina Fomicheva",
      "Fernando Alva-Manchego",
      "Frédéric Blain",
      "Abiola Obamuyide",
      "Nikolaos Aletras",
      "Lucia Specia"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.453": {
    "title": "Cross-document Coreference Resolution over Predicted Mentions",
    "volume": "findings",
    "abstract": "Coreference resolution has been mostly investigated within a single document scope, showing impressive progress in recent years based on end-to-end models. However, the more challenging task of cross-document (CD) coreference resolution remained relatively under-explored, with the few recent models applied only to gold mentions. Here, we introduce the first end-to-end model for CD coreference resolution from raw text, which extends the prominent model for withindocument coreference to the CD setting. Our model achieves competitive results for event and entity coreference resolution on gold mentions. More importantly, we set first baseline results, on the standard ECB+ dataset, for CD coreference resolution over predicted mentions. Further, our model is simpler and more efficient than recent CD coreference resolution systems, while not using any external resources.1",
    "checked": true,
    "id": "3e009dabee818e761d580c94d4b960aa8c763bd9",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Arie Cattan",
      "Alon Eirew",
      "Gabriel Stanovsky",
      "Mandar Joshi",
      "Ido Dagan"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.454": {
    "title": "Controllable Abstractive Dialogue Summarization with Sketch Supervision",
    "volume": "findings",
    "abstract": "In this paper, we aim to improve abstractive dialogue summarization quality and, at the same time, enable granularity control. Our model has two primary components and stages: 1) a two-stage generation strategy that generates a preliminary summary sketch serving as the basis for the final summary. This summary sketch provides a weakly supervised signal in the form of pseudo-labeled interrogative pronoun categories and key phrases extracted using a constituency parser. 2) A simple strategy to control the granularity of the final summary, in that our model can automatically determine or control the number of generated summary sentences for a given dialogue by predicting and highlighting different text spans from the source text. Our model achieves state-of-theart performance on the largest dialogue summarization corpus SAMSum, with as high as 50.79 in ROUGE-L score. In addition, we conduct a case study and show competitive human evaluation results and controllability to humanannotated summaries",
    "checked": true,
    "id": "95085501ff72c296b2df3f12969e0f8a57c224fd",
    "semantic_title": "",
    "citation_count": 25,
    "authors": [
      "Chien-Sheng Wu",
      "Linqing Liu",
      "Wenhao Liu",
      "Pontus Stenetorp",
      "Caiming Xiong"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.455": {
    "title": "Elaborative Simplification: Content Addition and Explanation Generation in Text Simplification",
    "volume": "findings",
    "abstract": "Much of modern day text simplification research focuses on sentence-level simplification, transforming original, more complex sentences to simplified versions. However, adding content can often be useful when difficult concepts and reasoning need to be explained. In this work, we present the first data-driven study of content addition in document simplification, which we call elaborative simplification. We introduce a new annotated dataset of 1.3K instances of elaborative simplification and analyze how entities, ideas, and concepts are elaborated through the lens of contextual specificity. We establish baselines for elaboration generation using large scale pre-trained language models, and illustrate that considering contextual specificity during generation can improve performance. Our results illustrate the complexities of elaborative simplification, suggesting many interesting directions for future work",
    "checked": true,
    "id": "0c7ff554c98435b398c0f8687c8bd6e7dbbfbf0d",
    "semantic_title": "",
    "citation_count": 17,
    "authors": [
      "Neha Srikanth",
      "Junyi Jessy Li"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.456": {
    "title": "Could you give me a hint ? Generating inference graphs for defeasible reasoning",
    "volume": "findings",
    "abstract": "Defeasible reasoning is a mode of reasoning where conclusions can be overturned by taking into account new evidence. A commonly used method in cognitive science and logic literature is to handcraft argumentation supporting inference graphs. While humans find inference graphs very useful for reasoning, constructing them at scale is difficult. In this paper, we automatically generate such inference graphs through transfer learning from a related NLP task that shares the kind of reasoning that inference graphs support. Through automated metrics and human evaluation, we find that our method generates meaningful graphs for the defeasible inference task. Human accuracy on this task improves by 20% by consulting the generated graphs. Our findings open up exciting new research avenues for cases where machine reasoning can help human reasoning.1",
    "checked": true,
    "id": "b9057dce43181a30aa3e0435c8ffc4c0b6f8f127",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Aman Madaan",
      "Dheeraj Rajagopal",
      "Niket Tandon",
      "Yiming Yang",
      "Eduard Hovy"
    ]
  },
  "https://aclanthology.org/2021.findings-acl.457": {
    "title": "Characterizing Social Spambots by their Human Traits",
    "volume": "findings",
    "abstract": "Social spambots, an emerging class of spammers attempting to emulate people, are difficult for both human annotators and classic bot detection techniques to reliably distinguish from genuine accounts. We examine this human emulation through studying the human characteristics (personality, gender, age, emotions) exhibited by social spambots' language, hypothesizing the values for these attributes will be unhuman-like (e.g. unusually high or low). We found our hypothesis mostly disconfirmed — individually, social bots exhibit very human-like attributes. However, a striking pattern emerged when consider the full distributions of these estimated human attributes: social bots were extremely similar and average in their expressed personality, demographics, and emotion (in contrast with traditional bots which we found to exhibit more variance and extreme values than genuine accounts). We thus consider how well social bots can be identified only using the 17 variables of these human attributes and ended up with a new state of the art in social spambot detection (e.g. F1 = .946). Further, simulating the situation of not knowing the bots a priori, we found that even an unsupervised clustering using the same 17 attributes could yield nearly as accurate of social bot identification (F1 = 0.925)",
    "checked": true,
    "id": "59583454cef87dfee40ddb4db1ae67b277a5bacb",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Salvatore Giorgi",
      "Lyle Ungar",
      "H. Andrew Schwartz"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.1": {
    "title": "Investigation on Data Adaptation Techniques for Neural Named Entity Recognition",
    "volume": "student",
    "abstract": "Data processing is an important step in various natural language processing tasks. As the commonly used datasets in named entity recognition contain only a limited number of samples, it is important to obtain additional labeled data in an efficient and reliable manner. A common practice is to utilize large monolingual unlabeled corpora. Another popular technique is to create synthetic data from the original labeled data (data augmentation). In this work, we investigate the impact of these two methods on the performance of three different named entity recognition tasks",
    "checked": true,
    "id": "1e446c33455568e4d4660d360f10761351a6df6b",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Evgeniia Tokarchuk",
      "David Thulke",
      "Weiyue Wang",
      "Christian Dugast",
      "Hermann Ney"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.2": {
    "title": "Stage-wise Fine-tuning for Graph-to-Text Generation",
    "volume": "student",
    "abstract": "Graph-to-text generation has benefited from pre-trained language models (PLMs) in achieving better performance than structured graph encoders. However, they fail to fully utilize the structure information of the input graph. In this paper, we aim to further improve the performance of the pre-trained language model by proposing a structured graph-to-text model with a two-step fine-tuning mechanism which first fine-tunes model on Wikipedia before adapting to the graph-to-text generation. In addition to using the traditional token and position embeddings to encode the knowledge graph (KG), we propose a novel tree-level embedding method to capture the inter-dependency structures of the input graph. This new approach has significantly improved the performance of all text generation metrics for the English WebNLG 2017 dataset",
    "checked": true,
    "id": "97107a9b2d60a52ccfc53b6c2ae2f786927dcc7c",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Qingyun Wang",
      "Semih Yavuz",
      "Xi Victoria Lin",
      "Heng Ji",
      "Nazneen Rajani"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.3": {
    "title": "Transformer-Based Direct Hidden Markov Model for Machine Translation",
    "volume": "student",
    "abstract": "The neural hidden Markov model has been proposed as an alternative to attention mechanism in machine translation with recurrent neural networks. However, since the introduction of the transformer models, its performance has been surpassed. This work proposes to introduce the concept of the hidden Markov model to the transformer architecture, which outperforms the transformer baseline. Interestingly, we find that the zero-order model already provides promising performance, giving it an edge compared to a model with first-order dependency, which performs similarly but is significantly slower in training and decoding",
    "checked": true,
    "id": "20a3d791febed1b727e11dce72b017960148f05d",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Weiyue Wang",
      "Zijian Yang",
      "Yingbo Gao",
      "Hermann Ney"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.4": {
    "title": "AutoRC: Improving BERT Based Relation Classification Models via Architecture Search",
    "volume": "student",
    "abstract": "Although BERT based relation classification (RC) models have achieved significant improvements over the traditional deep learning models, it seems that no consensus can be reached on what is the optimal architecture, since there are many design choices available. In this work, we design a comprehensive search space for BERT based RC models and employ a modified version of efficient neural architecture search (ENAS) method to automatically discover the design choices mentioned above. Experiments on eight benchmark RC tasks show that our method is efficient and effective in finding better architectures than the baseline BERT based RC models. Ablation study demonstrates the necessity of our search space design and the effectiveness of our search method. We also show that our framework can also apply to other entity related tasks like coreference resolution and span based named entity recognition (NER)",
    "checked": true,
    "id": "44928ee327335883eacce30d3baeb7dfdaa1fed2",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Wei Zhu"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.5": {
    "title": "How Low is Too Low? A Computational Perspective on Extremely Low-Resource Languages",
    "volume": "student",
    "abstract": "Despite the recent advancements of attention-based deep learning architectures across a majority of Natural Language Processing tasks, their application remains limited in a low-resource setting because of a lack of pre-trained models for such languages. In this study, we make the first attempt to investigate the challenges of adapting these techniques to an extremely low-resource language – Sumerian cuneiform – one of the world’s oldest written languages attested from at least the beginning of the 3rd millennium BC. Specifically, we introduce the first cross-lingual information extraction pipeline for Sumerian, which includes part-of-speech tagging, named entity recognition, and machine translation. We introduce InterpretLR, an interpretability toolkit for low-resource NLP and use it alongside human evaluations to gauge the trained models. Notably, all our techniques and most components of our pipeline can be generalised to any low-resource language. We publicly release all our implementations including a novel data set with domain-specific pre-processing to promote further research in this domain",
    "checked": true,
    "id": "fe53ea09903af7e0a32a29c2c9578ad3350749d7",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Rachit Bansal",
      "Himanshu Choudhary",
      "Ravneet Punia",
      "Niko Schenk",
      "Émilie Pagé-Perron",
      "Jacob Dahl"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.6": {
    "title": "On the Relationship between Zipf's Law of Abbreviation and Interfering Noise in Emergent Languages",
    "volume": "student",
    "abstract": "This paper studies whether emergent languages in a signaling game follow Zipf’s law of abbreviation (ZLA), especially when the communication ability of agents is limited because of interfering noises. ZLA is a well-known tendency in human languages where the more frequently a word is used, the shorter it will be. Surprisingly, previous work demonstrated that emergent languages do not obey ZLA at all when neural agents play a signaling game. It also reported that a ZLA-like tendency appeared by adding an explicit penalty on word lengths, which can be considered some external factors in reality such as articulatory effort. We hypothesize, on the other hand, that there might be not only such external factors but also some internal factors related to cognitive abilities. We assume that it could be simulated by modeling the effect of noises on the agents’ environment. In our experimental setup, the hidden states of the LSTM-based speaker and listener were added with Gaussian noise, while the channel was subject to discrete random replacement. Our results suggest that noise on a speaker is one of the factors for ZLA or at least causes emergent languages to approach ZLA, while noise on a listener and a channel is not",
    "checked": true,
    "id": "d7c2369e65865eedfa612283a26924d90b440fc1",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Ryo Ueda",
      "Koki Washio"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.7": {
    "title": "Long Document Summarization in a Low Resource Setting using Pretrained Language Models",
    "volume": "student",
    "abstract": "Abstractive summarization is the task of compressing a long document into a coherent short document while retaining salient information. Modern abstractive summarization methods are based on deep neural networks which often require large training datasets. Since collecting summarization datasets is an expensive and time-consuming task, practical industrial settings are usually low-resource. In this paper, we study a challenging low-resource setting of summarizing long legal briefs with an average source document length of 4268 words and only 120 available (document, summary) pairs. To account for data scarcity, we used a modern pre-trained abstractive summarizer BART, which only achieves 17.9 ROUGE-L as it struggles with long documents. We thus attempt to compress these long documents by identifying salient sentences in the source which best ground the summary, using a novel algorithm based on GPT-2 language model perplexity scores, that operates within the low resource regime. On feeding the compressed documents to BART, we observe a 6.0 ROUGE-L improvement. Our method also beats several competitive salience detection baselines. Furthermore, the identified salient sentences tend to agree with independent human labeling by domain experts",
    "checked": true,
    "id": "8ff620f704a4151fd7abba1db792463fbd32bfe5",
    "semantic_title": "",
    "citation_count": 18,
    "authors": [
      "Ahsaas Bajaj",
      "Pavitra Dangati",
      "Kalpesh Krishna",
      "Pradhiksha Ashok Kumar",
      "Rheeya Uppaal",
      "Bradford Windsor",
      "Eliot Brenner",
      "Dominic Dotterrer",
      "Rajarshi Das",
      "Andrew McCallum"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.8": {
    "title": "Attending Self-Attention: A Case Study of Visually Grounded Supervision in Vision-and-Language Transformers",
    "volume": "student",
    "abstract": "The impressive performances of pre-trained visually grounded language models have motivated a growing body of research investigating what has been learned during the pre-training. As a lot of these models are based on Transformers, several studies on the attention mechanisms used by the models to learn to associate phrases with their visual grounding in the image have been conducted. In this work, we investigate how supervising attention directly to learn visual grounding can affect the behavior of such models. We compare three different methods on attention supervision and their impact on the performances of a state-of-the-art visually grounded language model on two popular vision-and-language tasks",
    "checked": true,
    "id": "aab3c69ce6dab0b912e4a05e65c8ef0316c25ce0",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jules Samaran",
      "Noa Garcia",
      "Mayu Otani",
      "Chenhui Chu",
      "Yuta Nakashima"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.9": {
    "title": "Video-guided Machine Translation with Spatial Hierarchical Attention Network",
    "volume": "student",
    "abstract": "Video-guided machine translation, as one type of multimodal machine translations, aims to engage video contents as auxiliary information to address the word sense ambiguity problem in machine translation. Previous studies only use features from pretrained action detection models as motion representations of the video to solve the verb sense ambiguity, leaving the noun sense ambiguity a problem. To address this problem, we propose a video-guided machine translation system by using both spatial and motion representations in videos. For spatial features, we propose a hierarchical attention network to model the spatial information from object-level to video-level. Experiments on the VATEX dataset show that our system achieves 35.86 BLEU-4 score, which is 0.51 score higher than the single model of the SOTA method",
    "checked": true,
    "id": "8a3cb6d0a5be98c01bba7830cd9042a664bb7a5d",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Weiqi Gu",
      "Haiyue Song",
      "Chenhui Chu",
      "Sadao Kurohashi"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.10": {
    "title": "Stylistic approaches to predicting Reddit popularity in diglossia",
    "volume": "student",
    "abstract": "Past work investigating what makes a Reddit post popular has indicated that style is a far better predictor than content, where posts conforming to a subreddit’s community style are better received. However, what about a diglossia, when there are two community styles? In Singapore, the basilect (‘Singlish’) co-exists with an acrolect (standard English), each with contrasting advantages of community identity and prestige respectively. In this paper, I apply stylistic approaches to predicting Reddit post scores in a diglossia. Using data from the Singaporean and British subreddits, I show that while the acrolect’s prestige attracts more upvotes, the most popular posts also draw on Singlish vocabulary to appeal to the community identity",
    "checked": true,
    "id": "db22b7d1cc4e38e0413a4aabdfe5755facda65e9",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huikai Chua"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.11": {
    "title": "I've Seen Things You People Wouldn't Believe\": Hallucinating Entities in GuessWhat?!",
    "volume": "student",
    "abstract": "Natural language generation systems have witnessed important progress in the last years, but they are shown to generate tokens that are unrelated to the source input. This problem affects computational models in many NLP tasks, and it is particularly unpleasant in multimodal systems. In this work, we assess the rate of object hallucination in multimodal conversational agents playing the GuessWhat?! referential game. Better visual processing has been shown to mitigate this issue in image captioning; hence, we adapt to the GuessWhat?! task the best visual processing models at disposal, and propose two new models to play the Questioner agent. We show that the new models generate few hallucinations compared to other renowned models available in the literature. Moreover, their hallucinations are less severe (affect task-accuracy less) and are more human-like. We also analyse where hallucinations tend to occur more often through the dialogue: hallucinations are less frequent in earlier turns, cause a cascade hallucination effect, and are often preceded by negative answers, which have been shown to be harder to ground",
    "checked": true,
    "id": "5cac47b57b9ba4d92e5579b3f53f6eda88678b86",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Alberto Testoni",
      "Raffaella Bernardi"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.12": {
    "title": "How do different factors Impact the Inter-language Similarity? A Case Study on Indian languages",
    "volume": "student",
    "abstract": "India is one of the most linguistically diverse nations of the world and is culturally very rich. Most of these languages are somewhat similar to each other on account of sharing a common ancestry or being in contact for a long period of time. Nowadays, researchers are constantly putting efforts in utilizing the language relatedness to improve the performance of various NLP systems such as cross lingual semantic search, machine translation, sentiment analysis systems, etc. So in this paper, we performed an extensive case study on similarity involving languages of the Indian subcontinent. Language similarity prediction is defined as the task of measuring how similar the two languages are on the basis of their lexical, morphological and syntactic features. In this study, we concentrate only on the approach to calculate lexical similarity between Indian languages by looking at various factors such as size and type of corpus, similarity algorithms, subword segmentation, etc. The main takeaways from our work are: (i) Relative order of the language similarities largely remain the same, regardless of the factors mentioned above, (ii) Similarity within the same language family is higher, (iii) Languages share more lexical features at the subword level",
    "checked": true,
    "id": "a829889d277bd42d990e079433cec64bc8d3ff1c",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sourav Kumar",
      "Salil Aggarwal",
      "Dipti Misra Sharma",
      "Radhika Mamidi"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.13": {
    "title": "COVID-19 and Misinformation: A Large-Scale Lexical Analysis on Twitter",
    "volume": "student",
    "abstract": "Social media is often used by individuals and organisations as a platform to spread misinformation. With the recent coronavirus pandemic we have seen a surge of misinformation on Twitter, posing a danger to public health. In this paper, we compile a large COVID-19 Twitter misinformation corpus and perform an analysis to discover patterns with respect to vocabulary usage. Among others, our analysis reveals that the variety of topics and vocabulary usage are considerably more limited and negative in tweets related to misinformation than in randomly extracted tweets. In addition to our qualitative analysis, our experimental results show that a simple linear model based only on lexical features is effective in identifying misinformation-related tweets (with accuracy over 80%), providing evidence to the fact that the vocabulary used in misinformation largely differs from generic tweets",
    "checked": true,
    "id": "0293686b2df1fde97e102b5a48c51e9286bd5504",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Dimosthenis Antypas",
      "Jose Camacho-Collados",
      "Alun Preece",
      "David Rogers"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.14": {
    "title": "Situation-Based Multiparticipant Chat Summarization: a Concept, an Exploration-Annotation Tool and an Example Collection",
    "volume": "student",
    "abstract": "Currently, text chatting is one of the primary means of communication. However, modern text chat still in general does not offer any navigation or even full-featured search, although the high volumes of messages demand it. In order to mitigate these inconveniences, we formulate the problem of situation-based summarization and propose a special data annotation tool intended for developing training and gold-standard data. A situation is a subset of messages revolving around a single event in both temporal and contextual senses: e.g, a group of friends arranging a meeting in chat, agreeing on date, time, and place. Situations can be extracted via information retrieval, natural language processing, and machine learning techniques. Since the task is novel, neither training nor gold-standard datasets for it have been created yet. In this paper, we present the formulation of the situation-based summarization problem. Next, we describe Chat Corpora Annotator (CCA): the first annotation system designed specifically for exploring and annotating chat log data. We also introduce a custom query language for semi-automatic situation extraction. Finally, we present the first gold-standard dataset for situation-based summarization. The software source code and the dataset are publicly available",
    "checked": true,
    "id": "f151e4c63ead5c9cea344d753cec36ed804241b2",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Anna Smirnova",
      "Evgeniy Slobodkin",
      "George Chernishev"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.15": {
    "title": "Modeling Text using the Continuous Space Topic Model with Pre-Trained Word Embeddings",
    "volume": "student",
    "abstract": "In this study, we propose a model that extends the continuous space topic model (CSTM), which flexibly controls word probability in a document, using pre-trained word embeddings. To develop the proposed model, we pre-train word embeddings, which capture the semantics of words and plug them into the CSTM. Intrinsic experimental results show that the proposed model exhibits a superior performance over the CSTM in terms of perplexity and convergence speed. Furthermore, extrinsic experimental results show that the proposed model is useful for a document classification task when compared with the baseline model. We qualitatively show that the latent coordinates obtained by training the proposed model are better than those of the baseline model",
    "checked": true,
    "id": "022be579841554c84d09a09a9af67a1314ad8a8f",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Seiichi Inoue",
      "Taichi Aida",
      "Mamoru Komachi",
      "Manabu Asai"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.16": {
    "title": "Semantics of the Unwritten: The Effect of End of Paragraph and Sequence Tokens on Text Generation with GPT2",
    "volume": "student",
    "abstract": "The semantics of a text is manifested not only by what is read but also by what is not read. In this article, we will study how those implicit “not read” information such as end-of-paragraph () and end-of-sequence () affect the quality of text generation. Specifically, we find that the pre-trained language model GPT2 can generate better continuations by learning to generate the in the fine-tuning stage. Experimental results on English story generation show that can lead to higher BLEU scores and lower perplexity. We also conduct experiments on a self-collected Chinese essay dataset with Chinese-GPT2, a character level LM without and during pre-training. Experimental results show that the Chinese GPT2 can generate better essay endings with",
    "checked": true,
    "id": "188c4b9a13bf98e0c8818d43baf75b1930342912",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "He Bai",
      "Peng Shi",
      "Jimmy Lin",
      "Luchen Tan",
      "Kun Xiong",
      "Wen Gao",
      "Jie Liu",
      "Ming Li"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.17": {
    "title": "Data Augmentation with Unsupervised Machine Translation Improves the Structural Similarity of Cross-lingual Word Embeddings",
    "volume": "student",
    "abstract": "Unsupervised cross-lingual word embedding(CLWE) methods learn a linear transformation matrix that maps two monolingual embedding spaces that are separately trained with monolingual corpora. This method relies on the assumption that the two embedding spaces are structurally similar, which does not necessarily hold true in general. In this paper, we argue that using a pseudo-parallel corpus generated by an unsupervised machine translation model facilitates the structural similarity of the two embedding spaces and improves the quality of CLWEs in the unsupervised mapping method. We show that our approach outperforms other alternative approaches given the same amount of data, and, through detailed analysis, we show that data augmentation with the pseudo data from unsupervised machine translation is especially effective for mapping-based CLWEs because (1) the pseudo data makes the source and target corpora (partially) parallel; (2) the pseudo data contains information on the original language that helps to learn similar embedding spaces between the source and target languages",
    "checked": true,
    "id": "0f296ee7d1e0a786f1224aeea09daedabcbc69d5",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Sosuke Nishikawa",
      "Ryokan Ri",
      "Yoshimasa Tsuruoka"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.18": {
    "title": "Joint Detection and Coreference Resolution of Entities and Events with Document-level Context Aggregation",
    "volume": "student",
    "abstract": "Constructing knowledge graphs from unstructured text is an important task that is relevant to many domains. Most previous work focuses on extracting information from sentences or paragraphs, due to the difficulty of analyzing longer contexts. In this paper we propose a new jointly trained model that can be used for various information extraction tasks at the document level. The tasks performed by this system are entity and event identification, typing, and coreference resolution. In order to improve entity and event typing, we utilize context-aware representations aggregated from the detected mentions of the corresponding entities and events across the entire document. By extending our system to document-level, we can improve our results by incorporating cross-sentence dependencies and additional contextual information that might not be available at the sentence level, which allows for more globally optimized predictions. We evaluate our system on documents from the ACE05-E+ dataset and find significant improvement over the sentence-level SOTA on entity and event trigger identification and classification",
    "checked": true,
    "id": "91e3921aea5563f036c3fd0b5470bd3a3656c756",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Samuel Kriman",
      "Heng Ji"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.19": {
    "title": "Hold on honey, men at work\": A semi-supervised approach to detecting sexism in sitcoms",
    "volume": "student",
    "abstract": "Television shows play an important role inpropagating societal norms. Owing to the popularity of the situational comedy (sitcom) genre, it contributes significantly to the over-all development of society. In an effort to analyze the content of television shows belong-ing to this genre, we present a dataset of dialogue turns from popular sitcoms annotated for the presence of sexist remarks. We train a text classification model to detect sexism using domain adaptive learning. We apply the model to our dataset to analyze the evolution of sexist content over the years. We propose a domain-specific semi-supervised architecture for the aforementioned detection of sexism.Through extensive experiments, we show that our model often yields better classification performance over generic deep learn-ing based sentence classification that does not employ domain-specific training. We find that while sexism decreases over time on average,the proportion of sexist dialogue for the most sexist sitcom actually increases. A quantitative analysis along with a detailed error analysis presents the case for our proposed methodology",
    "checked": true,
    "id": "503e089a4413900a7d930a9b8d43b29796dedb1e",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Smriti Singh",
      "Tanvi Anand",
      "Arijit Ghosh Chowdhury",
      "Zeerak Waseem"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.20": {
    "title": "Observing the Learning Curve of NMT Systems With Regard to Linguistic Phenomena",
    "volume": "student",
    "abstract": "In this paper we present our observations and evaluations by observing the linguistic performance of the system on several steps on the training process of various English-to-German Neural Machine Translation models. The linguistic performance is measured through a semi-automatic process using a test suite. Among several linguistic observations, we find that the translation quality of some linguistic categories decreased within the recorded iterations. Additionally, we notice some drops of the translation quality of certain categories when using a larger corpus",
    "checked": true,
    "id": "8dc279bcd385bde9f2950583153ddfc5e20cd631",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Patrick Stadler",
      "Vivien Macketanz",
      "Eleftherios Avramidis"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.21": {
    "title": "Improving the Robustness of QA Models to Challenge Sets with Variational Question-Answer Pair Generation",
    "volume": "student",
    "abstract": "Question answering (QA) models for reading comprehension have achieved human-level accuracy on in-distribution test sets. However, they have been demonstrated to lack robustness to challenge sets, whose distribution is different from that of training sets. Existing data augmentation methods mitigate this problem by simply augmenting training sets with synthetic examples sampled from the same distribution as the challenge sets. However, these methods assume that the distribution of a challenge set is known a priori, making them less applicable to unseen challenge sets. In this study, we focus on question-answer pair generation (QAG) to mitigate this problem. While most existing QAG methods aim to improve the quality of synthetic examples, we conjecture that diversity-promoting QAG can mitigate the sparsity of training sets and lead to better robustness. We present a variational QAG model that generates multiple diverse QA pairs from a paragraph. Our experiments show that our method can improve the accuracy of 12 challenge sets, as well as the in-distribution accuracy",
    "checked": true,
    "id": "5f1a436277476cbcac5eaa7c0b925b051ef05ff7",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Kazutoshi Shinoda",
      "Saku Sugawara",
      "Akiko Aizawa"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.22": {
    "title": "Tools Impact on the Quality of Annotations for Chat Untangling",
    "volume": "student",
    "abstract": "The quality of the annotated data directly influences in the success of supervised NLP models. However, creating annotated datasets is often time-consuming and expensive. Although the annotation tool takes an important role, we know little about how it influences annotation quality. We compare the quality of annotations for the task of chat-untangling made by non-experts annotators using two different tools. The first is SLATE, an existing command-line based tool, and the second is Parlay, a new tool we developed that integrates mouse interaction and visual links. Our experimental results indicate that, while both tools perform similarly in terms of annotation quality, Parlay offers a significantly better user experience",
    "checked": true,
    "id": "f69a66f9832c7c7aae05bf5fed4e133c53688915",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Jhonny Cerezo",
      "Felipe Bravo-Marquez",
      "Alexandre Henri Bergel"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.23": {
    "title": "How Many Layers and Why? An Analysis of the Model Depth in Transformers",
    "volume": "student",
    "abstract": "In this study, we investigate the role of the multiple layers in deep transformer models. We design a variant of Albert that dynamically adapts the number of layers for each token of the input. The key specificity of Albert is that weights are tied across layers. Therefore, the stack of encoder layers iteratively repeats the application of the same transformation function on the input. We interpret the repetition of this application as an iterative process where the token contextualized representations are progressively refined. We analyze this process at the token level during pre-training, fine-tuning, and inference. We show that tokens do not require the same amount of iterations and that difficult or crucial tokens for the task are subject to more iterations",
    "checked": true,
    "id": "a010fe2f9404a951c3a9f50cba2006a551690917",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Antoine Simoulin",
      "Benoit Crabbé"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.24": {
    "title": "Edit Distance Based Curriculum Learning for Paraphrase Generation",
    "volume": "student",
    "abstract": "Curriculum learning has improved the quality of neural machine translation, where only source-side features are considered in the metrics to determine the difficulty of translation. In this study, we apply curriculum learning to paraphrase generation for the first time. Different from machine translation, paraphrase generation allows a certain level of discrepancy in semantics between source and target, which results in diverse transformations from lexical substitution to reordering of clauses. Hence, the difficulty of transformations requires considering both source and target contexts. Experiments on formality transfer using GYAFC showed that our curriculum learning with edit distance improves the quality of paraphrase generation. Additionally, the proposed method improves the quality of difficult samples, which was not possible for previous methods",
    "checked": true,
    "id": "6799e796fb26d081253027feb5ec33ae6664ee23",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Sora Kadotani",
      "Tomoyuki Kajiwara",
      "Yuki Arase",
      "Makoto Onizuka"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.25": {
    "title": "Changing the Basis of Contextual Representations with Explicit Semantics",
    "volume": "student",
    "abstract": "The application of transformer-based contextual representations has became a de facto solution for solving complex NLP tasks. Despite their successes, such representations are arguably opaque as their latent dimensions are not directly interpretable. To alleviate this limitation of contextual representations, we devise such an algorithm where the output representation expresses human-interpretable information of each dimension. We achieve this by constructing a transformation matrix based on the semantic content of the embedding space and predefined semantic categories using Hellinger distance. We evaluate our inferred representations on supersense prediction task. Our experiments reveal that the interpretable nature of transformed contextual representations makes it possible to accurately predict the supersense category of a word by simply looking for its transformed coordinate with the largest coefficient. We quantify the effects of our proposed transformation when applied over traditional dense contextual embeddings. We additionally investigate and report consistent improvements for the integration of sparse contextual word representations into our proposed algorithm",
    "checked": true,
    "id": "016dc75cb6666eb3e9394b68741690af57956e97",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tamás Ficsor",
      "Gábor Berend"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.26": {
    "title": "Personal Bias in Prediction of Emotions Elicited by Textual Opinions",
    "volume": "student",
    "abstract": "Analysis of emotions elicited by opinions, comments, or articles commonly exploits annotated corpora, in which the labels assigned to documents average the views of all annotators, or represent a majority decision. The models trained on such data are effective at identifying the general views of the population. However, their usefulness for predicting the emotions evoked by the textual content in a particular individual is limited. In this paper, we present a study performed on a dataset containing 7,000 opinions, each annotated by about 50 people with two dimensions: valence, arousal, and with intensity of eight emotions from Plutchik’s model. Our study showed that individual responses often significantly differed from the mean. Therefore, we proposed a novel measure to estimate this effect – Personal Emotional Bias (PEB). We also developed a new BERT-based transformer architecture to predict emotions from an individual human perspective. We found PEB a major factor for improving the quality of personalized reasoning. Both the method and measure may boost the quality of content recommendation systems and personalized solutions that protect users from hate speech or unwanted content, which are highly subjective in nature",
    "checked": true,
    "id": "1e33044678e4883f16f4b02e9ddcb875340485ec",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Piotr Milkowski",
      "Marcin Gruza",
      "Kamil Kanclerz",
      "Przemyslaw Kazienko",
      "Damian Grimling",
      "Jan Kocon"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.27": {
    "title": "MVP-BERT: Multi-Vocab Pre-training for Chinese BERT",
    "volume": "student",
    "abstract": "Despite the development of pre-trained language models (PLMs) significantly raise the performances of various Chinese natural language processing (NLP) tasks, the vocabulary (vocab) for these Chinese PLMs remains to be the one provided by Google Chinese BERT (CITATION), which is based on Chinese characters (chars). Second, the masked language model pre-training is based on a single vocab, limiting its downstream task performances. In this work, we first experimentally demonstrate that building a vocab via Chinese word segmentation (CWS) guided sub-word tokenization (SGT) can improve the performances of Chinese PLMs. Then we propose two versions of multi-vocab pre-training (MVP), Hi-MVP and AL-MVP, to improve the models’ expressiveness. Experiments show that: (a) MVP training strategies improve PLMs’ downstream performances, especially it can improve the PLM’s performances on span-level tasks; (b) our AL-MVP outperforms the recent AMBERT (CITATION) after large-scale pre-training, and it is more robust against adversarial attacks",
    "checked": true,
    "id": "4690eb050572a279f94560b6bbdccaae577b45f5",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Wei Zhu"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.28": {
    "title": "CMTA: COVID-19 Misinformation Multilingual Analysis on Twitter",
    "volume": "student",
    "abstract": "The internet has actually come to be an essential resource of health knowledge for individuals around the world in the present situation of the coronavirus condition pandemic(COVID-19). During pandemic situations, myths, sensationalism, rumours and misinformation, generated intentionally or unintentionally, spread rapidly through social networks. Twitter is one of these popular social networks people use to share COVID-19 related news, information, and thoughts that reflect their perception and opinion about the pandemic. Evaluation of tweets for recognizing misinformation can create beneficial understanding to review the top quality and also the readability of online information concerning the COVID-19. This paper presents a multilingual COVID-19 related tweet analysis method, CMTA, that uses BERT, a deep learning model for multilingual tweet misinformation detection and classification. CMTA extracts features from multilingual textual data, which is then categorized into specific information classes. Classification is done by a Dense-CNN model trained on tweets manually annotated into information classes (i.e., ‘false’, ‘partly false’, ‘misleading’). The paper presents an analysis of multilingual tweets from February to June, showing the distribution type of information spread across different languages. To access the performance of the CMTA multilingual model, we performed a comparative analysis of 8 monolingual model and CMTA for the misinformation detection task. The results show that our proposed CMTA model has surpassed various monolingual models which consolidated the fact that through transfer learning a multilingual framework could be developed",
    "checked": true,
    "id": "5090628f2f034d0df837ea533bf25ac721c2041d",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Raj Pranesh",
      "Mehrdad Farokhenajd",
      "Ambesh Shekhar",
      "Genoveva Vargas-Solar"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.29": {
    "title": "Predicting pragmatic discourse features in the language of adults with autism spectrum disorder",
    "volume": "student",
    "abstract": "Individuals with autism spectrum disorder (ASD) experience difficulties in social aspects of communication, but the linguistic characteristics associated with deficits in discourse and pragmatic expression are often difficult to precisely identify and quantify. We are currently collecting a corpus of transcribed natural conversations produced in an experimental setting in which participants with and without ASD complete a number of collaborative tasks with their neurotypical peers. Using this dyadic conversational data, we investigate three pragmatic features – politeness, uncertainty, and informativeness – and present a dataset of utterances annotated for each of these features on a three-point scale. We then introduce ongoing work in developing and training neural models to automatically predict these features, with the goal of identifying the same between-groups differences that are observed using manual annotations. We find the best performing model for all three features is a feed-forward neural network trained with BERT embeddings. Our models yield higher accuracy than ones used in previous approaches for deriving these features, with F1 exceeding 0.82 for all three pragmatic features",
    "checked": true,
    "id": "9b0d5db75ead8014cd4bca6d1bbd2ae551081fb4",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christine Yang",
      "Duanchen Liu",
      "Qingyun Yang",
      "Zoey Liu",
      "Emily Prud’hommeaux"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.30": {
    "title": "SumPubMed: Summarization Dataset of PubMed Scientific Articles",
    "volume": "student",
    "abstract": "Most earlier work on text summarization is carried out on news article datasets. The summary in these datasets is naturally located at the beginning of the text. Hence, a model can spuriously utilize this correlation for summary generation instead of truly learning to summarize. To address this issue, we constructed a new dataset, SumPubMed , using scientific articles from the PubMed archive. We conducted a human analysis of summary coverage, redundancy, readability, coherence, and informativeness on SumPubMed . SumPubMed is challenging because (a) the summary is distributed throughout the text (not-localized on top), and (b) it contains rare domain-specific scientific terms. We observe that seq2seq models that adequately summarize news articles struggle to summarize SumPubMed . Thus, SumPubMed opens new avenues for the future improvement of models as well as the development of new evaluation metrics",
    "checked": true,
    "id": "8e1d376229b0bc6896eb50896761854a63b596ab",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Vivek Gupta",
      "Prerna Bharti",
      "Pegah Nokhiz",
      "Harish Karnick"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.31": {
    "title": "A Case Study of Analysis of Construals in Language on Social Media Surrounding a Crisis Event",
    "volume": "student",
    "abstract": "The events that took place at the Unite the Right rally held in Charlottesville, Virginia on August 11-12, 2017 caused intense reaction on social media from users across the political spectrum. We present a novel application of psycholinguistics - specifically, construal level theory - to analyze the language on social media around this event of social import through topic models. We find that including psycholinguistic measures of concreteness as covariates in topic models can lead to informed analysis of the language surrounding an event of political import",
    "checked": true,
    "id": "347918ae216667b1d1b4d4075eda616ab25afb9c",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lolo Aboufoul",
      "Khyati Mahajan",
      "Tiffany Gallicano",
      "Sara Levens",
      "Samira Shaikh"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.32": {
    "title": "Cross-lingual Evidence Improves Monolingual Fake News Detection",
    "volume": "student",
    "abstract": "Misleading information spreads on the Internet at an incredible speed, which can lead to irreparable consequences in some cases. Therefore, it is becoming essential to develop fake news detection technologies. While substantial work has been done in this direction, one of the limitations of the current approaches is that these models are focused only on one language and do not use multilingual information. In this work, we propose a new technique based on cross-lingual evidence (CE) that can be used for fake news detection and improve existing approaches. The hypothesis of the usage of cross-lingual evidence as a feature for fake news detection is confirmed, firstly, by manual experiment based on a set of known true and fake news. Besides, we compared our fake news classification system based on the proposed feature with several strong baselines on two multi-domain datasets of general-topic news and one newly fake COVID-19 news dataset showing that combining cross-lingual evidence with strong baselines such as RoBERTa yields significant improvements in fake news detection",
    "checked": true,
    "id": "485530d40fc69d967a7be739522c616891fc0b74",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Daryna Dementieva",
      "Alexander Panchenko"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.33": {
    "title": "Neural Machine Translation with Synchronous Latent Phrase Structure",
    "volume": "student",
    "abstract": "It is reported that grammatical information is useful for machine translation (MT) task. However, the annotation of grammatical information requires the highly human resources. Furthermore, it is not trivial to adapt grammatical information to MT since grammatical annotation usually adapts tokenization standards which might not be suitable to capture the relation of two languages, and the use of sub-word tokenization, e.g., Byte-Pair-Encoding, to alleviate out-of-vocabulary problem might not be compatible with those annotations. In this work, we propose two methods to explicitly incorporate grammatical information without supervising annotation; first, latent phrase structure is induced in an unsupervised fashion from a multi-head attention mechanism; second, the induced phrase structures in encoder and decoder are synchronized so that they are compatible with each other using constraints during training. We demonstrate that our approach produces better performance and explainability in two tasks, translation and alignment tasks without extra resources. Although we could not obtain the high quality phrase structure in constituency parsing when evaluated monolingually, we find that the induced phrase structures enhance the explainability of translation through the synchronization constraint",
    "checked": true,
    "id": "8547edec7c87ab01972762656ee6294dcbf03e52",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Shintaro Harada",
      "Taro Watanabe"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.34": {
    "title": "Zero Pronouns Identification based on Span prediction",
    "volume": "student",
    "abstract": "The presence of zero-pronoun (ZP) greatly affects the downstream tasks of NLP in pro-drop languages such as Japanese and Chinese. To tackle the problem, the previous works identified ZPs as sequence labeling on the word sequence or the linearlized tree nodes of the input. We propose a novel approach to ZP identification by casting it as a query-based argument span prediction task. Given a predicate as a query, our model predicts the omission with ZP. In the experiments, our model surpassed the sequence labeling baseline",
    "checked": true,
    "id": "adfc71c6d5090f3f8eb4396d99449d43532db8a8",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sei Iwata",
      "Taro Watanabe",
      "Masaaki Nagata"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.35": {
    "title": "On the differences between BERT and MT encoder spaces and how to address them in translation tasks",
    "volume": "student",
    "abstract": "Various studies show that pretrained language models such as BERT cannot straightforwardly replace encoders in neural machine translation despite their enormous success in other tasks. This is even more astonishing considering the similarities between the architectures. This paper sheds some light on the embedding spaces they create, using average cosine similarity, contextuality metrics and measures for representational similarity for comparison, revealing that BERT and NMT encoder representations look significantly different from one another. In order to address this issue, we propose a supervised transformation from one into the other using explicit alignment and fine-tuning. Our results demonstrate the need for such a transformation to improve the applicability of BERT in MT",
    "checked": true,
    "id": "4ca1d88e196612d6a4bd6d18cdfd508ebc6edcc3",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Raúl Vázquez",
      "Hande Celikkanat",
      "Mathias Creutz",
      "Jörg Tiedemann"
    ]
  },
  "https://aclanthology.org/2021.acl-srw.36": {
    "title": "Synchronous Syntactic Attention for Transformer Neural Machine Translation",
    "volume": "student",
    "abstract": "This paper proposes a novel attention mechanism for Transformer Neural Machine Translation, “Synchronous Syntactic Attention,” inspired by synchronous dependency grammars. The mechanism synchronizes source-side and target-side syntactic self-attentions by minimizing the difference between target-side self-attentions and the source-side self-attentions mapped by the encoder-decoder attention matrix. The experiments show that the proposed method improves the translation performance on WMT14 En-De, WMT16 En-Ro, and ASPEC Ja-En (up to +0.38 points in BLEU)",
    "checked": true,
    "id": "bfd5b3ed81567f072f8516ef4c11111ea3adf423",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Hiroyuki Deguchi",
      "Akihiro Tamura",
      "Takashi Ninomiya"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.1": {
    "title": "TexSmart: A System for Enhanced Natural Language Understanding",
    "volume": "demo",
    "abstract": "This paper introduces TexSmart, a text understanding system that supports fine-grained named entity recognition (NER) and enhanced semantic analysis functionalities. Compared to most previous publicly available text understanding systems and tools, TexSmart holds some unique features. First, the NER function of TexSmart supports over 1,000 entity types, while most other public tools typically support several to (at most) dozens of entity types. Second, TexSmart introduces new semantic analysis functions like semantic expansion and deep semantic representation, that are absent in most previous systems. Third, a spectrum of algorithms (from very fast algorithms to those that are relatively slow but more accurate) are implemented for one function in TexSmart, to fulfill the requirements of different academic and industrial applications. The adoption of unsupervised or weakly-supervised algorithms is especially emphasized, with the goal of easily updating our models to include fresh data with less human annotation efforts",
    "checked": true,
    "id": "39369c2e5d6d6b403c444e72948ccf0dfdf59489",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Lemao Liu",
      "Haisong Zhang",
      "Haiyun Jiang",
      "Yangming Li",
      "Enbo Zhao",
      "Kun Xu",
      "Linfeng Song",
      "Suncong Zheng",
      "Botong Zhou",
      "Dick Zhu",
      "Xiao Feng",
      "Tao Chen",
      "Tao Yang",
      "Dong Yu",
      "Feng Zhang",
      "ZhanHui Kang",
      "Shuming Shi"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.2": {
    "title": "IntelliCAT: Intelligent Machine Translation Post-Editing with Quality Estimation and Translation Suggestion",
    "volume": "demo",
    "abstract": "We present IntelliCAT, an interactive translation interface with neural models that streamline the post-editing process on machine translation output. We leverage two quality estimation (QE) models at different granularities: sentence-level QE, to predict the quality of each machine-translated sentence, and word-level QE, to locate the parts of the machine-translated sentence that need correction. Additionally, we introduce a novel translation suggestion model conditioned on both the left and right contexts, providing alternatives for specific words or phrases for correction. Finally, with word alignments, IntelliCAT automatically preserves the original document’s styles in the translated document. The experimental results show that post-editing based on the proposed QE and translation suggestions can significantly improve translation quality. Furthermore, a user study reveals that three features provided in IntelliCAT significantly accelerate the post-editing task, achieving a 52.9% speedup in translation time compared to translating from scratch. The interface is publicly available at https://intellicat.beringlab.com/",
    "checked": true,
    "id": "e94ae0f0640d3ca3e00c359edb5684bf3f54121a",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Dongjun Lee",
      "Junhyeong Ahn",
      "Heesoo Park",
      "Jaemin Jo"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.3": {
    "title": "The Classical Language Toolkit: An NLP Framework for Pre-Modern Languages",
    "volume": "demo",
    "abstract": "This paper announces version 1.0 of the Classical Language Toolkit (CLTK), an NLP framework for pre-modern languages. The vast majority of NLP, its algorithms and software, is created with assumptions particular to living languages, thus neglecting certain important characteristics of largely non-spoken historical languages. Further, scholars of pre-modern languages often have different goals than those of living-language researchers. To fill this void, the CLTK adapts ideas from several leading NLP frameworks to create a novel software architecture that satisfies the unique needs of pre-modern languages and their researchers. Its centerpiece is a modular processing pipeline that balances the competing demands of algorithmic diversity with pre-configured defaults. The CLTK currently provides pipelines, including models, for almost 20 languages",
    "checked": true,
    "id": "4bec66ec7b21cc8ef2f7dd4cfc79d0fa96e5d417",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Kyle P. Johnson",
      "Patrick J. Burns",
      "John Stewart",
      "Todd Cook",
      "Clément Besnier",
      "William J. B. Mattingly"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.4": {
    "title": "TextBox: A Unified, Modularized, and Extensible Framework for Text Generation",
    "volume": "demo",
    "abstract": "In this paper, we release an open-source library, called TextBox, to provide a unified, modularized, and extensible text generation framework. TextBox aims to support a broad set of text generation tasks and models. In our library, we implement 21 text generation models on 9 benchmark datasets, covering the categories of VAE, GAN, and pretrained language models. Meanwhile, our library maintains sufficient modularity and extensibility by properly decomposing the model architecture, inference, and learning process into highly reusable modules, which allows users to easily incorporate new models into our framework. The above features make TextBox especially suitable for researchers and practitioners to quickly reproduce baseline models and develop new models. TextBox is implemented based on PyTorch, and released under Apache License 2.0 at the link https://github.com/RUCAIBox/TextBox",
    "checked": true,
    "id": "7d1cd398a9a035ccc2cba2cf79ea79943a758889",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Junyi Li",
      "Tianyi Tang",
      "Gaole He",
      "Jinhao Jiang",
      "Xiaoxuan Hu",
      "Puzhao Xie",
      "Zhipeng Chen",
      "Zhuohao Yu",
      "Wayne Xin Zhao",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.5": {
    "title": "Inside ASCENT: Exploring a Deep Commonsense Knowledge Base and its Usage in Question Answering",
    "volume": "demo",
    "abstract": "ASCENT is a fully automated methodology for extracting and consolidating commonsense assertions from web contents (Nguyen et al., 2021). It advances traditional triple-based commonsense knowledge representation by capturing semantic facets like locations and purposes, and composite concepts, i.e., subgroups and related aspects of subjects. In this demo, we present a web portal that allows users to understand its construction process, explore its content, and observe its impact in the use case of question answering. The demo website (https://ascent.mpi-inf.mpg.de) and an introductory video (https://youtu.be/qMkJXqu_Yd4) are both available online",
    "checked": true,
    "id": "489ffd70cb2afd550ab809bc90f5a766eb07aa80",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Tuan-Phong Nguyen",
      "Simon Razniewski",
      "Gerhard Weikum"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.6": {
    "title": "SciConceptMiner: A system for large-scale scientific concept discovery",
    "volume": "demo",
    "abstract": "Scientific knowledge is evolving at an unprecedented rate of speed, with new concepts constantly being introduced from millions of academic articles published every month. In this paper, we introduce a self-supervised end-to-end system, SciConceptMiner, for the automatic capture of emerging scientific concepts from both independent knowledge sources (semi-structured data) and academic publications (unstructured documents). First, we adopt a BERT-based sequence labeling model to predict candidate concept phrases with self-supervision data. Then, we incorporate rich Web content for synonym detection and concept selection via a web search API. This two-stage approach achieves highly accurate (94.7%) concept identification with more than 740K scientific concepts. These concepts are deployed in the Microsoft Academic production system and are the backbone for its semantic search capability",
    "checked": true,
    "id": "ca608192fc9d0c013915248c867656916b1059e8",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Zhihong Shen",
      "Chieh-Han Wu",
      "Li Ma",
      "Chien-Pang Chen",
      "Kuansan Wang"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.7": {
    "title": "NeurST: Neural Speech Translation Toolkit",
    "volume": "demo",
    "abstract": "NeurST is an open-source toolkit for neural speech translation. The toolkit mainly focuses on end-to-end speech translation, which is easy to use, modify, and extend to advanced speech translation research and products. NeurST aims at facilitating the speech translation research for NLP researchers and building reliable benchmarks for this field. It provides step-by-step recipes for feature extraction, data preprocessing, distributed training, and evaluation. In this paper, we will introduce the framework design of NeurST and show experimental results for different benchmark datasets, which can be regarded as reliable baselines for future research. The toolkit is publicly available at https://github.com/bytedance/neurst and we will continuously update the performance of with other counterparts and studies at https://st-benchmark.github.io/",
    "checked": true,
    "id": "34fbca00589c77fca70db4962beb43a9094c49ef",
    "semantic_title": "",
    "citation_count": 22,
    "authors": [
      "Chengqi Zhao",
      "Mingxuan Wang",
      "Qianqian Dong",
      "Rong Ye",
      "Lei Li"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.8": {
    "title": "ParCourE: A Parallel Corpus Explorer for a Massively Multilingual Corpus",
    "volume": "demo",
    "abstract": "With more than 7000 languages worldwide, multilingual natural language processing (NLP) is essential both from an academic and commercial perspective. Researching typological properties of languages is fundamental for progress in multilingual NLP. Examples include assessing language similarity for effective transfer learning, injecting inductive biases into machine learning models or creating resources such as dictionaries and inflection tables. We provide ParCourE, an online tool that allows to browse a word-aligned parallel corpus, covering 1334 languages. We give evidence that this is useful for typological research. ParCourE can be set up for any parallel corpus and can thus be used for typological research on other corpora as well as for exploring their quality and properties",
    "checked": true,
    "id": "59c6400221188cec53a676f980a0dc89120ee119",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Ayyoob ImaniGooghari",
      "Masoud Jalili Sabet",
      "Philipp Dufter",
      "Michael Cysou",
      "Hinrich Schütze"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.9": {
    "title": "MT-Telescope: An interactive platform for contrastive evaluation of MT systems",
    "volume": "demo",
    "abstract": "We present MT-Telescope, a visualization platform designed to facilitate comparative analysis of the output quality of two Machine Translation (MT) systems. While automated MT evaluation metrics are commonly used to evaluate MT systems at a corpus-level, our platform supports fine-grained segment-level analysis and interactive visualisations that expose the fundamental differences in the performance of the compared systems. MT-Telescope also supports dynamic corpus filtering to enable focused analysis on specific phenomena such as; translation of named entities, handling of terminology, and the impact of input segment length on translation quality. Furthermore, the platform provides a bootstrapped t-test for statistical significance as a means of evaluating the rigor of the resulting system ranking. MT-Telescope is open source, written in Python, and is built around a user friendly and dynamic web interface. Complementing other existing tools, our platform is designed to facilitate and promote the broader adoption of more rigorous analysis practices in the evaluation of MT quality",
    "checked": true,
    "id": "46dac3c30478f76140d11969452784821b012a87",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Ricardo Rei",
      "Ana C Farinha",
      "Craig Stewart",
      "Luisa Coheur",
      "Alon Lavie"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.10": {
    "title": "Supporting Complaints Investigation for Nursing and Midwifery Regulatory Agencies",
    "volume": "demo",
    "abstract": "Health professional regulators aim to protect the health and well-being of patients and the public by setting standards for scrutinising and overseeing the training and conduct of health and care professionals. A major task of such regulators is the investigation of complaints against practitioners. However, processing a complaint often lasts several months and is particularly costly. Hence, we worked with international regulators from different countries (the UK, US and Australia), to develop the first decision support tool that aims to help such regulators process complaints more efficiently. Our system uses state-of-the-art machine learning and natural language processing techniques to process complaints and predict their risk level. Our tool also provides additional useful information including explanations, to help the regulatory staff interpret the prediction results, and similar past cases as well as non-compliance to regulations, to support the decision making",
    "checked": true,
    "id": "e8761be78e9222495c63e54ae386b7ea023c72be",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Piyawat Lertvittayakumjorn",
      "Ivan Petej",
      "Yang Gao",
      "Yamuna Krishnamurthy",
      "Anna Van Der Gaag",
      "Robert Jago",
      "Kostas Stathis"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.11": {
    "title": "CogIE: An Information Extraction Toolkit for Bridging Texts and CogNet",
    "volume": "demo",
    "abstract": "CogNet is a knowledge base that integrates three types of knowledge: linguistic knowledge, world knowledge and commonsense knowledge. In this paper, we propose an information extraction toolkit, called CogIE, which is a bridge connecting raw texts and CogNet. CogIE has three features: versatile, knowledge-grounded and extensible. First, CogIE is a versatile toolkit with a rich set of functional modules, including named entity recognition, entity typing, entity linking, relation extraction, event extraction and frame-semantic parsing. Second, as a knowledge-grounded toolkit, CogIE can ground the extracted facts to CogNet and leverage different types of knowledge to enrich extracted results. Third, for extensibility, owing to the design of three-tier architecture, CogIE is not only a plug-and-play toolkit for developers but also an extensible programming framework for researchers. We release an open-access online system to visually extract information from texts. Source code, datasets and pre-trained models are publicly available at GitHub, with a short instruction video",
    "checked": true,
    "id": "d74aaa18d851e16a2f6e0284eea584bf8f52b27f",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Zhuoran Jin",
      "Yubo Chen",
      "Dianbo Sui",
      "Chenhao Wang",
      "Zhipeng Xue",
      "Jun Zhao"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.12": {
    "title": "fastHan: A BERT-based Multi-Task Toolkit for Chinese NLP",
    "volume": "demo",
    "abstract": "We present fastHan, an open-source toolkit for four basic tasks in Chinese natural language processing: Chinese word segmentation (CWS), Part-of-Speech (POS) tagging, named entity recognition (NER), and dependency parsing. The backbone of fastHan is a multi-task model based on a pruned BERT, which uses the first 8 layers in BERT. We also provide a 4-layer base model compressed from the 8-layer model. The joint-model is trained and evaluated on 13 corpora of four tasks, yielding near state-of-the-art (SOTA) performance in dependency parsing and NER, achieving SOTA performance in CWS and POS. Besides, fastHan’s transferability is also strong, performing much better than popular segmentation tools on a non-training corpus. To better meet the need of practical application, we allow users to use their own labeled data to further fine-tune fastHan. In addition to its small size and excellent performance, fastHan is user-friendly. Implemented as a python package, fastHan isolates users from the internal technical details and is convenient to use. The project is released on Github",
    "checked": true,
    "id": "6d2263ddcbaf988aebc6930f741a3121918190e2",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Zhichao Geng",
      "Hang Yan",
      "Xipeng Qiu",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.13": {
    "title": "Erase and Rewind: Manual Correction of NLP Output through a Web Interface",
    "volume": "demo",
    "abstract": "In this paper, we present Tintful, an NLP annotation software that can be used both to manually annotate texts and to fix mistakes in NLP pipelines, such as Stanford CoreNLP. Using a paradigm similar to wiki-like systems, a user who notices some wrong annotation can easily fix it and submit the resulting (and right) entry back to the tool developers. Moreover, Tintful can be used to easily annotate data from scratch. The input documents do not need to be in a particular format: starting from the plain text, the sentences are first annotated with CoreNLP, then the user can edit the annotations and submit everything back through a user-friendly interface",
    "checked": true,
    "id": "35133d6931d1710313b183c755bdcae6df95c4ed",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Valentino Frasnelli",
      "Lorenzo Bocchi",
      "Alessio Palmero Aprosio"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.14": {
    "title": "ESRA: Explainable Scientific Research Assistant",
    "volume": "demo",
    "abstract": "We introduce Explainable Scientific Research Assistant (ESRA), a literature discovery platform that augments search results with relevant details and explanations, aiding users in understanding more about their queries and the returned papers beyond existing literature search systems. Enabled by a knowledge graph we extracted from abstracts of 23k papers on the arXiv’s cs.CL category, ESRA provides three main features: explanation (for why a paper is returned to the user), list of facts (that are relevant to the query), and graph visualization (drawing connections between the query and each paper with surrounding related entities). The experimental results with humans involved show that ESRA can accelerate the users’ search process with paper explanations and helps them better explore the landscape of the topics of interest by exploiting the underlying knowledge graph. We provide the ESRA web application at http://esra.cp.eng.chula.ac.th/",
    "checked": true,
    "id": "bf37e581f9d2447bf732b8502c8aff5141a88f06",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Pollawat Hongwimol",
      "Peeranuth Kehasukcharoen",
      "Pasit Laohawarutchai",
      "Piyawat Lertvittayakumjorn",
      "Aik Beng Ng",
      "Zhangsheng Lai",
      "Timothy Liu",
      "Peerapon Vateekul"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.15": {
    "title": "Trafilatura: A Web Scraping Library and Command-Line Tool for Text Discovery and Extraction",
    "volume": "demo",
    "abstract": "An essential operation in web corpus construction consists in retaining the desired content while discarding the rest. Another challenge finding one’s way through websites. This article introduces a text discovery and extraction tool published under open-source license. Its installation and use is straightforward, notably from Python and on the command-line. The software allows for main text, comments and metadata extraction, while also providing building blocks for web crawling tasks. A comparative evaluation on real-world data also shows its interest as well as the performance of other available solutions. The contributions of this paper are threefold: it references the software, features a benchmark, and provides a meaningful baseline for similar tasks. The tool performs significantly better than other open-source solutions in this evaluation and in external benchmarks",
    "checked": true,
    "id": "48a3a41dab9d9e13d193b9b3d6fa7fe9df261cb0",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Adrien Barbaresi"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.16": {
    "title": "Dodrio: Exploring Transformer Models with Interactive Visualization",
    "volume": "demo",
    "abstract": "Why do large pre-trained transformer-based models perform so well across a wide variety of NLP tasks? Recent research suggests the key may lie in multi-headed attention mechanism’s ability to learn and represent linguistic information. Understanding how these models represent both syntactic and semantic knowledge is vital to investigate why they succeed and fail, what they have learned, and how they can improve. We present Dodrio, an open-source interactive visualization tool to help NLP researchers and practitioners analyze attention mechanisms in transformer-based models with linguistic knowledge. Dodrio tightly integrates an overview that summarizes the roles of different attention heads, and detailed views that help users compare attention weights with the syntactic structure and semantic information in the input text. To facilitate the visual comparison of attention weights and linguistic knowledge, Dodrio applies different graph visualization techniques to represent attention weights scalable to longer input text. Case studies highlight how Dodrio provides insights into understanding the attention mechanism in transformer-based models. Dodrio is available at https://poloclub.github.io/dodrio/",
    "checked": true,
    "id": "c96dbf796fdaf16e03896d38b24d25942c1a857e",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Zijie J. Wang",
      "Robert Turko",
      "Duen Horng Chau"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.17": {
    "title": "REM: Efficient Semi-Automated Real-Time Moderation of Online Forums",
    "volume": "demo",
    "abstract": "This paper presents REM, a novel tool for the semi-automated real-time moderation of large scale online forums. The growing demand for online participation and the increasing number of user comments raise challenges in filtering out harmful and undesirable content from public debates in online forums. Since a manual moderation does not scale well and pure automated approaches often lack the required level of accuracy, we suggest a semi-automated moderation approach. Our approach maximizes the efficiency of manual efforts by targeting only those comments for which human intervention is needed, e.g. due to high classification uncertainty. Our tool offers a rich visual interactive environment enabling the exploration of online debates. We conduct a preliminary evaluation experiment to demonstrate the suitability of our approach and publicly release the source code of REM",
    "checked": true,
    "id": "fc6c921a246e92cee300cb3f00711635b784cc9e",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Jakob Smedegaard Andersen",
      "Olaf Zukunft",
      "Walid Maalej"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.18": {
    "title": "SummVis: Interactive Visual Analysis of Models, Data, and Evaluation for Text Summarization",
    "volume": "demo",
    "abstract": "Novel neural architectures, training strategies, and the availability of large-scale corpora haven been the driving force behind recent progress in abstractive text summarization. However, due to the black-box nature of neural models, uninformative evaluation metrics, and scarce tooling for model and data analysis the true performance and failure modes of summarization models remain largely unknown. To address this limitation, we introduce SummVis, an open-source tool for visualizing abstractive summaries that enables fine-grained analysis of the models, data, and evaluation metrics associated with text summarization. Through its lexical and semantic visualizations, the tools offers an easy entry point for in-depth model prediction exploration across important dimensions such as factual consistency or abstractiveness. The tool together with several pre-computed model outputs is available at https://summvis.com",
    "checked": true,
    "id": "7139a82d9cce629b6782c7c67202797da035a01d",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Jesse Vig",
      "Wojciech Kryscinski",
      "Karan Goel",
      "Nazneen Rajani"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.19": {
    "title": "A Graphical Interface for Curating Schemas",
    "volume": "demo",
    "abstract": "Much past work has focused on extracting information like events, entities, and relations from documents. Very little work has focused on analyzing these results for better model understanding. In this paper, we introduce a curation interface that takes an Information Extraction (IE) system’s output in a pre-defined format and generates a graphical representation of its elements. The interface supports editing while curating schemas for complex events like Improvised Explosive Device (IED) based scenarios. We identify various schemas that either have linear event chains or contain parallel events with complicated temporal ordering. We iteratively update an induced schema to uniquely identify events specific to it, add optional events around them, and prune unnecessary events. The resulting schemas are improved and enriched versions of the machine-induced versions",
    "checked": true,
    "id": "e95e0caa59f006777f633860f105cf139e5d8611",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Piyush Mishra",
      "Akanksha Malhotra",
      "Susan Windisch Brown",
      "Martha Palmer",
      "Ghazaleh Kazeminejad"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.20": {
    "title": "TEXTOIR: An Integrated and Visualized Platform for Text Open Intent Recognition",
    "volume": "demo",
    "abstract": "TEXTOIR is the first integrated and visualized platform for text open intent recognition. It is composed of two main modules: open intent detection and open intent discovery. Each module integrates most of the state-of-the-art algorithms and benchmark intent datasets. It also contains an overall framework connecting the two modules in a pipeline scheme. In addition, this platform has visualized tools for data and model management, training, evaluation and analysis of the performance from different aspects. TEXTOIR provides useful toolkits and convenient visualized interfaces for each sub-module, and designs a framework to implement a complete process to both identify known intents and discover open intents",
    "checked": true,
    "id": "9569703963cbaa618b7a3b59e32a036d76c706ab",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Hanlei Zhang",
      "Xiaoteng Li",
      "Hua Xu",
      "Panpan Zhang",
      "Kang Zhao",
      "Kai Gao"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.21": {
    "title": "KuiLeiXi: a Chinese Open-Ended Text Adventure Game",
    "volume": "demo",
    "abstract": "There is a long history of research related to automated story generation, dating back as far as the 1970s. Recently, the rapid development of pre-trained language models has spurred great progresses in this field. Equipped with GPT-2 and the latest GPT-3, AI Dungeon has been seen as a famous example of the powerful text generation capabilities of large-scale pre-trained language models, and a possibility for future games. However, as a game, AI Dungeon lacks incentives to players and relies entirely on players to explore on their own. This makes players’ enthusiasm decline rapidly. In this paper, we present an open-ended text adventure game in Chinese, named as KuiLeiXi. In KuiLeiXi, players need to interact with the AI until the pre-determined plot goals are reached. By introducing the plot goals, players have a stronger incentive to explore ways to reach plot goals, while the AI’s abilities are not abused to generate harmful contents. This limited freedom allows this game to be integrated as a part of a romance simulation mobile game, Yu Jian Love. Since KuiLeiXi was launched, it has received a lot of positive feedbacks from more than 100,000 players. A demo video is available at https://youtu.be/DyYZhxMRrkk",
    "checked": true,
    "id": "19003e5f5623c9d778aeaa580a946cae9b473841",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Yadong Xi",
      "Xiaoxi Mao",
      "Le Li",
      "Lei Lin",
      "Yanjiang Chen",
      "Shuhan Yang",
      "Xuhan Chen",
      "Kailun Tao",
      "Zhi Li",
      "Gongzheng Li",
      "Lin Jiang",
      "Siyan Liu",
      "Zeng Zhao",
      "Minlie Huang",
      "Changjie Fan",
      "Zhipeng Hu"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.22": {
    "title": "CRSLab: An Open-Source Toolkit for Building Conversational Recommender System",
    "volume": "demo",
    "abstract": "In recent years, conversational recommender systems (CRSs) have drawn a wide attention in the research community, which focus on providing high-quality recommendations to users via natural language conversations. However, due to diverse scenarios and data formats, existing studies on CRSs lack unified and standardized implementation or comparison. To tackle this challenge, we release an open-source toolkit CRSLab, which provides a unified and extensible framework with highly-decoupled modules to develop CRSs. Based on this framework, we collect 6 commonly used human-annotated CRS datasets and implement 19 models that include advanced techniques such as graph neural networks and pre-training models. Besides, our toolkit provides a series of automatic evaluation protocols and a human-machine interaction interface to evaluate and compare different CRS methods. The project and documents are released at https://github.com/RUCAIBox/CRSLab",
    "checked": true,
    "id": "f5ec998b44beb4bd743204e77f564a62e9df46b2",
    "semantic_title": "",
    "citation_count": 29,
    "authors": [
      "Kun Zhou",
      "Xiaolei Wang",
      "Yuanhang Zhou",
      "Chenzhan Shang",
      "Yuan Cheng",
      "Wayne Xin Zhao",
      "Yaliang Li",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.23": {
    "title": "Does My Representation Capture X? Probe-Ably",
    "volume": "demo",
    "abstract": "Probing (or diagnostic classification) has become a popular strategy for investigating whether a given set of intermediate features is present in the representations of neural models. Naive probing studies may have misleading results, but various recent works have suggested more reliable methodologies that compensate for the possible pitfalls of probing. However, these best practices are numerous and fast-evolving. To simplify the process of running a set of probing experiments in line with suggested methodologies, we introduce Probe-Ably: an extendable probing framework which supports and automates the application of probing methods to the user’s inputs",
    "checked": true,
    "id": "491d193b32dfdf4604bf48e494fa1fda96c60cfe",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Deborah Ferreira",
      "Julia Rozanova",
      "Mokanarangan Thayaparan",
      "Marco Valentino",
      "André Freitas"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.24": {
    "title": "CLTR: An End-to-End, Transformer-Based System for Cell-Level Table Retrieval and Table Question Answering",
    "volume": "demo",
    "abstract": "We present the first end-to-end, transformer-based table question answering (QA) system that takes natural language questions and massive table corpora as inputs to retrieve the most relevant tables and locate the correct table cells to answer the question. Our system, CLTR, extends the current state-of-the-art QA over tables model to build an end-to-end table QA architecture. This system has successfully tackled many real-world table QA problems with a simple, unified pipeline. Our proposed system can also generate a heatmap of candidate columns and rows over complex tables and allow users to quickly identify the correct cells to answer questions. In addition, we introduce two new open domain benchmarks, E2E_WTQ and E2E_GNQ, consisting of 2,005 natural language questions over 76,242 tables. The benchmarks are designed to validate CLTR as well as accommodate future table retrieval and end-to-end table QA research and experiments. Our experiments demonstrate that our system is the current state-of-the-art model on the table retrieval task and produces promising results for end-to-end table QA",
    "checked": true,
    "id": "016c171611237518dd8f167f9195c984cfb482c4",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Feifei Pan",
      "Mustafa Canim",
      "Michael Glass",
      "Alfio Gliozzo",
      "Peter Fox"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.25": {
    "title": "Neural Extractive Search",
    "volume": "demo",
    "abstract": "Domain experts often need to extract structured information from large corpora. We advocate for a search paradigm called “extractive search”, in which a search query is enriched with capture-slots, to allow for such rapid extraction. Such an extractive search system can be built around syntactic structures, resulting in high-precision, low-recall results. We show how the recall can be improved using neural retrieval and alignment. The goals of this paper are to concisely introduce the extractive-search paradigm; and to demonstrate a prototype neural retrieval system for extractive search and its benefits and potential. Our prototype is available at https://spike.neural-sim.apps.allenai.org/ and a video demonstration is available at https://vimeo.com/559586687",
    "checked": true,
    "id": "7006eb1ed218921745a3a598d0bc809f2bf4d7b6",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Shauli Ravfogel",
      "Hillel Taub-Tabib",
      "Yoav Goldberg"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.26": {
    "title": "FastSeq: Make Sequence Generation Faster",
    "volume": "demo",
    "abstract": "Transformer-based models have made tremendous impacts in natural language generation. However the inference speed is a bottleneck due to large model size and intensive computing involved in auto-regressive decoding process. We develop FastSeq framework to accelerate sequence generation without accuracy loss. The proposed optimization techniques include an attention cache optimization, an efficient algorithm for detecting repeated n-grams, and an asynchronous generation pipeline with parallel I/O. These optimizations are general enough to be applicable to Transformer-based models (e.g., T5, GPT2, and UniLM). Our benchmark results on a set of widely used and diverse models demonstrate 4-9x inference speed gain. Additionally, FastSeq is easy to use with a simple one-line code change. The source code is available at https://github.com/microsoft/fastseq",
    "checked": true,
    "id": "2384c92bbde47f5dbc8d8f175aa67e0f95c413d4",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Yu Yan",
      "Fei Hu",
      "Jiusheng Chen",
      "Nikhil Bhendawade",
      "Ting Ye",
      "Yeyun Gong",
      "Nan Duan",
      "Desheng Cui",
      "Bingyu Chi",
      "Ruofei Zhang"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.27": {
    "title": "LOA: Logical Optimal Actions for Text-based Interaction Games",
    "volume": "demo",
    "abstract": "We present Logical Optimal Actions (LOA), an action decision architecture of reinforcement learning applications with a neuro-symbolic framework which is a combination of neural network and symbolic knowledge acquisition approach for natural language interaction games. The demonstration for LOA experiments consists of a web-based interactive platform for text-based games and visualization for acquired knowledge for improving interpretability for trained rules. This demonstration also provides a comparison module with other neuro-symbolic approaches as well as non-symbolic state-of-the-art agent models on the same text-based games. Our LOA also provides open-sourced implementation in Python for the reinforcement learning environment to facilitate an experiment for studying neuro-symbolic agents. Demo site: https://ibm.biz/acl21-loa, Code: https://github.com/ibm/loa",
    "checked": true,
    "id": "9ca19acce35fd440cb9ffa504907f36a2e176bbc",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Daiki Kimura",
      "Subhajit Chaudhury",
      "Masaki Ono",
      "Michiaki Tatsubori",
      "Don Joven Agravante",
      "Asim Munawar",
      "Akifumi Wachi",
      "Ryosuke Kohita",
      "Alexander Gray"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.28": {
    "title": "ProphetNet-X: Large-Scale Pre-training Models for English, Chinese, Multi-lingual, Dialog, and Code Generation",
    "volume": "demo",
    "abstract": "Now, the pre-training technique is ubiquitous in natural language processing field. ProphetNet is a pre-training based natural language generation method which shows powerful performance on English text summarization and question generation tasks. In this paper, we extend ProphetNet into other domains and languages, and present the ProphetNet family pre-training models, named ProphetNet-X, where X can be English, Chinese, Multi-lingual, and so on. We pre-train a cross-lingual generation model ProphetNet-Multi, a Chinese generation model ProphetNet-Zh, two open-domain dialog generation models ProphetNet-Dialog-En and ProphetNet-Dialog-Zh. And also, we provide a PLG (Programming Language Generation) model ProphetNet-Code to show the generation performance besides NLG (Natural Language Generation) tasks. In our experiments, ProphetNet-X models achieve new state-of-the-art performance on 10 benchmarks. All the models of ProphetNet-X share the same model structure, which allows users to easily switch between different models. We make the code and models publicly available, and we will keep updating more pre-training models and finetuning scripts",
    "checked": true,
    "id": "26e3d58181724f9ef77973ff0f65bac06e499fec",
    "semantic_title": "",
    "citation_count": 40,
    "authors": [
      "Weizhen Qi",
      "Yeyun Gong",
      "Yu Yan",
      "Can Xu",
      "Bolun Yao",
      "Bartuer Zhou",
      "Biao Cheng",
      "Daxin Jiang",
      "Jiusheng Chen",
      "Ruofei Zhang",
      "Houqiang Li",
      "Nan Duan"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.29": {
    "title": "IFlyEA: A Chinese Essay Assessment System with Automated Rating, Review Generation, and Recommendation",
    "volume": "demo",
    "abstract": "Automated Essay Assessment (AEA) aims to judge students’ writing proficiency in an automatic way. This paper presents a Chinese AEA system IFlyEssayAssess (IFlyEA), targeting on evaluating essays written by native Chinese students from primary and junior schools. IFlyEA provides multi-level and multi-dimension analytical modules for essay assessment. It has state-of-the-art grammar level analysis techniques, and also integrates components for rhetoric and discourse level analysis, which are important for evaluating native speakers’ writing ability, but still challenging and less studied in previous work. Based on the comprehensive analysis, IFlyEA provides application services for essay scoring, review generation, recommendation, and explainable analytical visualization. These services can benefit both teachers and students during the process of writing teaching and learning",
    "checked": true,
    "id": "1bd035b551175ac8bbaf6c443de5aa668d5d91a0",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Jiefu Gong",
      "Xiao Hu",
      "Wei Song",
      "Ruiji Fu",
      "Zhichao Sheng",
      "Bo Zhu",
      "Shijin Wang",
      "Ting Liu"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.30": {
    "title": "Ecco: An Open Source Library for the Explainability of Transformer Language Models",
    "volume": "demo",
    "abstract": "Our understanding of why Transformer-based NLP models have been achieving their recent success lags behind our ability to continue scaling these models. To increase the transparency of Transformer-based language models, we present Ecco – an open-source library for the explainability of Transformer-based NLP models. Ecco provides a set of tools to capture, analyze, visualize, and interactively explore the inner mechanics of these models. This includes (1) gradient-based feature attribution for natural language generation (2) hidden states and their evolution between model layers (3) convenient access and examination tools for neuron activations in the under-explored Feed-Forward Neural Network sublayer of Transformer layers. (4) convenient examination of activation vectors via canonical correlation analysis (CCA), non-negative matrix factorization (NMF), and probing classifiers. We find that syntactic information can be retrieved from BERT’s FFNN representations in levels comparable to those in hidden state representations. More curiously, we find that the model builds up syntactic information in its hidden states even when intermediate FFNNs indicate diminished levels of syntactic information. Ecco is available at https://www.eccox.io/",
    "checked": true,
    "id": "ac34c70ee85b048ad97328713c790f389656e4eb",
    "semantic_title": "",
    "citation_count": 24,
    "authors": [
      "J Alammar"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.31": {
    "title": "PAWLS: PDF Annotation With Labels and Structure",
    "volume": "demo",
    "abstract": "Adobe’s Portable Document Format (PDF) is a popular way of distributing view-only documents with a rich visual markup. This presents a challenge to NLP practitioners who wish to use the information contained within PDF documents for training models or data analysis, because annotating these documents is difficult. In this paper, we present PDF Annotation with Labels and Structure (PAWLS), a new annotation tool designed specifically for the PDF document format. PAWLS is particularly suited for mixed-mode annotation and scenarios in which annotators require extended context to annotate accurately. PAWLS supports span-based textual annotation, N-ary relations and freeform, non-textual bounding boxes, all of which can be exported in convenient formats for training multi-modal machine learning models. A read-only PAWLS server is available at https://pawls.apps.allenai.org/, and the source code is available at https://github.com/allenai/pawls",
    "checked": true,
    "id": "d4f95365a0c1aec74333f50a996c62bfad4a8478",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Mark Neumann",
      "Zejiang Shen",
      "Sam Skjonsberg"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.32": {
    "title": "TweeNLP: A Twitter Exploration Portal for Natural Language Processing",
    "volume": "demo",
    "abstract": "We present TweeNLP, a one-stop portal that organizes Twitter’s natural language processing (NLP) data and builds a visualization and exploration platform. It curates 19,395 tweets (as of April 2021) from various NLP conferences and general NLP discussions. It supports multiple features such as TweetExplorer to explore tweets by topics, visualize insights from Twitter activity throughout the organization cycle of conferences, discover popular research papers and researchers. It also builds a timeline of conference and workshop submission deadlines. We envision TweeNLP to function as a collective memory unit for the NLP community by integrating the tweets pertaining to research papers with the NLPExplorer scientific literature search engine. The current system is hosted at http://nlpexplorer.org/twitter/CFP",
    "checked": true,
    "id": "591db6a777fb744046715eed9997b889114d3c64",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Viraj Shah",
      "Shruti Singh",
      "Mayank Singh"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.33": {
    "title": "ChrEnTranslate: Cherokee-English Machine Translation Demo with Quality Estimation and Corrective Feedback",
    "volume": "demo",
    "abstract": "We introduce ChrEnTranslate, an online machine translation demonstration system for translation between English and an endangered language Cherokee. It supports both statistical and neural translation models as well as provides quality estimation to inform users of reliability, two user feedback interfaces for experts and common users respectively, example inputs to collect human translations for monolingual data, word alignment visualization, and relevant terms from the Cherokee English dictionary. The quantitative evaluation demonstrates that our backbone translation models achieve state-of-the-art translation performance and our quality estimation well correlates with both BLEU and human judgment. By analyzing 216 pieces of expert feedback, we find that NMT is preferable because it copies less than SMT, and, in general, current models can translate fragments of the source sentence but make major mistakes. When we add these 216 expert-corrected parallel texts into the training set and retrain models, equal or slightly better performance is observed, which demonstrates indicates the potential of human-in-the-loop learning",
    "checked": true,
    "id": "bb53946c7da617a05bbeef47fff74012db27ee78",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Shiyue Zhang",
      "Benjamin Frey",
      "Mohit Bansal"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.34": {
    "title": "ExplainaBoard: An Explainable Leaderboard for NLP",
    "volume": "demo",
    "abstract": "With the rapid development of NLP research, leaderboards have emerged as one tool to track the performance of various systems on various NLP tasks. They are effective in this goal to some extent, but generally present a rather simplistic one-dimensional view of the submitted systems, communicated only through holistic accuracy numbers. In this paper, we present a new conceptualization and implementation of NLP evaluation: the ExplainaBoard, which in addition to inheriting the functionality of the standard leaderboard, also allows researchers to (i) diagnose strengths and weaknesses of a single system (e.g. what is the best-performing system bad at?) (ii) interpret relationships between multiple systems. (e.g. where does system A outperform system B? What if we combine systems A, B and C?) and (iii) examine prediction results closely (e.g. what are common errors made by multiple systems or in what contexts do particular errors occur?). So far, ExplainaBoard covers more than 400 systems, 50 datasets, 40 languages, and 12 tasks. We not only released an online platform at the website but also make our evaluation tool an API with MIT Licence at Github and PyPi that allows users to conveniently assess their models offline. We additionally release all output files from systems that we have run or collected to motivate “output-driven” research in the future",
    "checked": true,
    "id": "1cf2e9e198feef3893da2800a7949f6880ddc084",
    "semantic_title": "",
    "citation_count": 39,
    "authors": [
      "Pengfei Liu",
      "Jinlan Fu",
      "Yang Xiao",
      "Weizhe Yuan",
      "Shuaichen Chang",
      "Junqi Dai",
      "Yixin Liu",
      "Zihuiwen Ye",
      "Graham Neubig"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.35": {
    "title": "Exploring Word Usage Change with Continuously Evolving Embeddings",
    "volume": "demo",
    "abstract": "The usage of individual words can change over time, for example, when words experience a semantic shift. As text datasets generally comprise documents that were collected over a longer period of time, examining word usage changes in a corpus can often reveal interesting patterns. In this paper, we introduce a simple and intuitive way to track word usage changes via continuously evolving embeddings, computed as a weighted running average of transformer-based contextualized embeddings. We demonstrate our approach on a corpus of recent New York Times article snippets and provide code for an easy to use web app to conveniently explore semantic shifts with interactive plots",
    "checked": true,
    "id": "127ee5675b3d36151a840c4b55b2e5946c9bd8d7",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Franziska Horn"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.36": {
    "title": "TURING: an Accurate and Interpretable Multi-Hypothesis Cross-Domain Natural Language Database Interface",
    "volume": "demo",
    "abstract": "A natural language database interface (NLDB) can democratize data-driven insights for non-technical users. However, existing Text-to-SQL semantic parsers cannot achieve high enough accuracy in the cross-database setting to allow good usability in practice. This work presents TURING, a NLDB system toward bridging this gap. The cross-domain semantic parser of TURING with our novel value prediction method achieves 75.1% execution accuracy, and 78.3% top-5 beam execution accuracy on the Spider validation set (Yu et al., 2018b). To benefit from the higher beam accuracy, we design an interactive system where the SQL hypotheses in the beam are explained step-by-step in natural language, with their differences highlighted. The user can then compare and judge the hypotheses to select which one reflects their intention if any. The English explanations of SQL queries in TURING are produced by our high-precision natural language generation system based on synchronous grammars",
    "checked": true,
    "id": "4f05aba9dea39063c77f3f186aab4547dde2993e",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Peng Xu",
      "Wenjie Zi",
      "Hamidreza Shahidi",
      "Ákos Kádár",
      "Keyi Tang",
      "Wei Yang",
      "Jawad Ateeq",
      "Harsh Barot",
      "Meidan Alon",
      "Yanshuai Cao"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.37": {
    "title": "Many-to-English Machine Translation Tools, Data, and Pretrained Models",
    "volume": "demo",
    "abstract": "While there are more than 7000 languages in the world, most translation research efforts have targeted a few high resource languages. Commercial translation systems support only one hundred languages or fewer, and do not make these models available for transfer to low resource languages. In this work, we present useful tools for machine translation research: MTData, NLCodec and RTG. We demonstrate their usefulness by creating a multilingual neural machine translation model capable of translating from 500 source languages to English. We make this multilingual model readily downloadable and usable as a service, or as a parent model for transfer-learning to even lower-resource languages",
    "checked": true,
    "id": "a49b0997efaec2db61109a6deed1512672c3cd0c",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Thamme Gowda",
      "Zhao Zhang",
      "Chris Mattmann",
      "Jonathan May"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.38": {
    "title": "LEGOEval: An Open-Source Toolkit for Dialogue System Evaluation via Crowdsourcing",
    "volume": "demo",
    "abstract": "We present LEGOEval, an open-source toolkit that enables researchers to easily evaluate dialogue systems in a few lines of code using the online crowdsource platform, Amazon Mechanical Turk. Compared to existing toolkits, LEGOEval features a flexible task design by providing a Python API that maps to commonly used React.js interface components. Researchers can personalize their evaluation procedures easily with our built-in pages as if playing with LEGO blocks. Thus, LEGOEval provides a fast, consistent method for reproducing human evaluation results. Besides the flexible task design, LEGOEval also offers an easy API to review collected data",
    "checked": true,
    "id": "a1fb0ebf04a929cf7748bb2b6b09f391dc63b9dc",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Yu Li",
      "Josh Arnold",
      "Feifan Yan",
      "Weiyan Shi",
      "Zhou Yu"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.39": {
    "title": "ReTraCk: A Flexible and Efficient Framework for Knowledge Base Question Answering",
    "volume": "demo",
    "abstract": "We present Retriever-Transducer-Checker (ReTraCk), a neural semantic parsing framework for large scale knowledge base question answering (KBQA). ReTraCk is designed as a modular framework to maintain high flexibility. It includes a retriever to retrieve relevant KB items efficiently, a transducer to generate logical form with syntax correctness guarantees and a checker to improve transduction procedure. ReTraCk is ranked at top1 overall performance on the GrailQA leaderboard and obtains highly competitive performance on the typical WebQuestionsSP benchmark. Our system can interact with users timely, demonstrating the efficiency of the proposed framework",
    "checked": true,
    "id": "ebc64974e9e0021984a0158b3c04b60327730a88",
    "semantic_title": "",
    "citation_count": 25,
    "authors": [
      "Shuang Chen",
      "Qian Liu",
      "Zhiwei Yu",
      "Chin-Yew Lin",
      "Jian-Guang Lou",
      "Feng Jiang"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.40": {
    "title": "skweak: Weak Supervision Made Easy for NLP",
    "volume": "demo",
    "abstract": "We present skweak, a versatile, Python-based software toolkit enabling NLP developers to apply weak supervision to a wide range of NLP tasks. Weak supervision is an emerging machine learning paradigm based on a simple idea: instead of labelling data points by hand, we use labelling functions derived from domain knowledge to automatically obtain annotations for a given dataset. The resulting labels are then aggregated with a generative model that estimates the accuracy (and possible confusions) of each labelling function. The skweak toolkit makes it easy to implement a large spectrum of labelling functions (such as heuristics, gazetteers, neural models or linguistic constraints) on text data, apply them on a corpus, and aggregate their results in a fully unsupervised fashion. skweak is especially designed to facilitate the use of weak supervision for NLP tasks such as text classification and sequence labelling. We illustrate the use of skweak for NER and sentiment analysis. skweak is released under an open-source license and is available at https://github.com/NorskRegnesentral/skweak",
    "checked": true,
    "id": "1a64ccfa07381895b278b9e85173d2825f3331c3",
    "semantic_title": "",
    "citation_count": 18,
    "authors": [
      "Pierre Lison",
      "Jeremy Barnes",
      "Aliaksandr Hubin"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.41": {
    "title": "TextFlint: Unified Multilingual Robustness Evaluation Toolkit for Natural Language Processing",
    "volume": "demo",
    "abstract": "TextFlint is a multilingual robustness evaluation toolkit for NLP tasks that incorporates universal text transformation, task-specific transformation, adversarial attack, subpopulation, and their combinations to provide comprehensive robustness analyses. This enables practitioners to automatically evaluate their models from various aspects or to customize their evaluations as desired with just a few lines of code. TextFlint also generates complete analytical reports as well as targeted augmented data to address the shortcomings of the model in terms of its robustness. To guarantee acceptability, all the text transformations are linguistically based and all the transformed data selected (up to 100,000 texts) scored highly under human evaluation. To validate the utility, we performed large-scale empirical evaluations (over 67,000) on state-of-the-art deep learning models, classic supervised methods, and real-world systems. The toolkit is already available at https://github.com/textflint with all the evaluation results demonstrated at textflint.io",
    "checked": true,
    "id": "37c4ec8d4fcaaf7f2c3f01a2fb30e9e41d8865a2",
    "semantic_title": "",
    "citation_count": 57,
    "authors": [
      "Xiao Wang",
      "Qin Liu",
      "Tao Gui",
      "Qi Zhang",
      "Yicheng Zou",
      "Xin Zhou",
      "Jiacheng Ye",
      "Yongxin Zhang",
      "Rui Zheng",
      "Zexiong Pang",
      "Qinzhuo Wu",
      "Zhengyan Li",
      "Chong Zhang",
      "Ruotian Ma",
      "Zichu Fei",
      "Ruijian Cai",
      "Jun Zhao",
      "Xingwu Hu",
      "Zhiheng Yan",
      "Yiding Tan",
      "Yuan Hu",
      "Qiyuan Bian",
      "Zhihua Liu",
      "Shan Qin",
      "Bolin Zhu",
      "Xiaoyu Xing",
      "Jinlan Fu",
      "Yue Zhang",
      "Minlong Peng",
      "Xiaoqing Zheng",
      "Yaqian Zhou",
      "Zhongyu Wei",
      "Xipeng Qiu",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.42": {
    "title": "Stretch-VST: Getting Flexible With Visual Stories",
    "volume": "demo",
    "abstract": "In visual storytelling, a short story is generated based on a given image sequence. Despite years of work, most visual storytelling models remain limited in terms of the generated stories’ fixed length: most models produce stories with exactly five sentences because five-sentence stories dominate the training data. The fix-length stories carry limited details and provide ambiguous textual information to the readers. Therefore, we propose to “stretch” the stories, which create the potential to present in-depth visual details. This paper presents Stretch-VST, a visual storytelling framework that enables the generation of prolonged stories by adding appropriate knowledge, which is selected by the proposed scoring function. We propose a length-controlled Transformer to generate long stories. This model introduces novel positional encoding methods to maintain story quality with lengthy inputs. Experiments confirm that long stories are generated without deteriorating the quality. The human evaluation further shows that Stretch-VST can provide better focus and detail when stories are prolonged compared to state of the art. We create a webpage to demonstrate our prolonged capability",
    "checked": true,
    "id": "84e5815125b8c5b81a5e6554e1044b057020cf89",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Chi-yang Hsu",
      "Yun-Wei Chu",
      "Tsai-Lun Yang",
      "Ting-Hao Huang",
      "Lun-Wei Ku"
    ]
  },
  "https://aclanthology.org/2021.acl-demo.43": {
    "title": "OpenAttack: An Open-source Textual Adversarial Attack Toolkit",
    "volume": "demo",
    "abstract": "Textual adversarial attacking has received wide and increasing attention in recent years. Various attack models have been proposed, which are enormously distinct and implemented with different programming frameworks and settings. These facts hinder quick utilization and fair comparison of attack models. In this paper, we present an open-source textual adversarial attack toolkit named OpenAttack to solve these issues. Compared with existing other textual adversarial attack toolkits, OpenAttack has its unique strengths in support for all attack types, multilinguality, and parallel processing. Currently, OpenAttack includes 15 typical attack models that cover all attack types. Its highly inclusive modular design not only supports quick utilization of existing attack models, but also enables great flexibility and extensibility. OpenAttack has broad uses including comparing and evaluating attack models, measuring robustness of a model, assisting in developing new attack models, and adversarial training. Source code and documentation can be obtained at https://github.com/thunlp/OpenAttack",
    "checked": true,
    "id": "95968b89040146cb015827aee8ff6f77d67bbaf1",
    "semantic_title": "",
    "citation_count": 60,
    "authors": [
      "Guoyang Zeng",
      "Fanchao Qi",
      "Qianrui Zhou",
      "Tingji Zhang",
      "Zixian Ma",
      "Bairu Hou",
      "Yuan Zang",
      "Zhiyuan Liu",
      "Maosong Sun"
    ]
  },
  "https://aclanthology.org/2021.acl-tutorials.1": {
    "title": "Advances in Debating Technologies: Building AI That Can Debate Humans",
    "volume": "tutorial",
    "abstract": "The tutorial focuses on Debating Technologies, a sub-field of computational argumentation defined as “computational technologies developed directly to enhance, support, and engage with human debating” (Gurevych et al., 2016). A recent milestone in this field is Project Debater, which was revealed in 2019 as the first AI system that can debate human experts on complex topics. Project Debater is the third in the series of IBM Research AI’s grand challenges, following Deep Blue and Watson. It has been developed for over six years by a large team of researchers and engineers, and its live demonstration in February 2019 received massive media attention. This research effort has resulted in more than 50 scientific papers to date, and many datasets freely available for research purposes. We discuss the scientific challenges that arise when building such a system, including argument mining, argument quality assessment, stance classification, principled argument detection, narrative generation, and rebutting a human opponent. Many of the underlying capabilities of Project Debater have been made freely available for academic research, and the tutorial will include a detailed explanation of how to use and leverage these tools. In addition to discussing individual components, the tutorial also provides a holistic view of a debating system. Such a view is largely missing in the academic literature, where each paper typically addresses a specific problem in isolation. We present a complete pipeline of a debating system, and discuss the information flow and the interaction between the various components. Finally, we discuss practical applications and future challenges of debating technologies",
    "checked": true,
    "id": "2af8a3517e26bf2ca77fbfeddad28d9403bca4c1",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Roy Bar-Haim",
      "Liat Ein-Dor",
      "Matan Orbach",
      "Elad Venezian",
      "Noam Slonim"
    ]
  },
  "https://aclanthology.org/2021.acl-tutorials.2": {
    "title": "Event-Centric Natural Language Processing",
    "volume": "tutorial",
    "abstract": "This tutorial targets researchers and practitioners who are interested in AI technologies that help machines understand natural language text, particularly real-world events described in the text. These include methods to extract the internal structures of an event regarding its protagonist(s), participant(s) and properties, as well as external structures concerning memberships, temporal and causal relations of multiple events. This tutorial will provide audience with a systematic introduction of (i) knowledge representations of events, (ii) various methods for automated extraction, conceptualization and prediction of events and their relations, (iii) induction of event processes and properties, and (iv) a wide range of NLU and commonsense understanding tasks that benefit from aforementioned techniques. We will conclude the tutorial by outlining emerging research problems in this area",
    "checked": true,
    "id": "49294af4478741e296815b4c49f491eaf2db9f94",
    "semantic_title": "",
    "citation_count": 18,
    "authors": [
      "Muhao Chen",
      "Hongming Zhang",
      "Qiang Ning",
      "Manling Li",
      "Heng Ji",
      "Kathleen McKeown",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/2021.acl-tutorials.3": {
    "title": "Meta Learning and Its Applications to Natural Language Processing",
    "volume": "tutorial",
    "abstract": "Deep learning based natural language processing (NLP) has become the mainstream of research in recent years and significantly outperforms conventional methods. However, deep learning models are notorious for being data and computation hungry. These downsides limit the application of such models from deployment to different domains, languages, countries, or styles, since collecting in-genre data and model training from scratch are costly. The long-tail nature of human language makes challenges even more significant. Meta-learning, or ‘Learning to Learn’, aims to learn better learning algorithms, including better parameter initialization, optimization strategy, network architecture, distance metrics, and beyond. Meta-learning has been shown to allow faster fine-tuning, converge to better performance, and achieve amazing results for few-shot learning in many applications. Meta-learning is one of the most important new techniques in machine learning in recent years. There is a related tutorial in ICML 2019 and a related course at Stanford, but most of the example applications given in these materials are about image processing. It is believed that meta-learning has great potential to be applied in NLP, and some works have been proposed with notable achievements in several relevant problems, e.g., relation extraction, machine translation, and dialogue generation and state tracking. However, it does not catch the same level of attention as in the image processing community. In the tutorial, we will first introduce Meta-learning approaches and the theory behind them, and then review the works of applying this technology to NLP problems. This tutorial intends to facilitate researchers in the NLP community to understand this new technology better and promote more research studies using this new technology",
    "checked": true,
    "id": "3a4840ecfddc5e938e715379462af6fdbaa5e9b6",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Hung-yi Lee",
      "Ngoc Thang Vu",
      "Shang-Wen Li"
    ]
  },
  "https://aclanthology.org/2021.acl-tutorials.4": {
    "title": "Pre-training Methods for Neural Machine Translation",
    "volume": "tutorial",
    "abstract": "This tutorial provides a comprehensive guide to make the most of pre-training for neural machine translation. Firstly, we will briefly introduce the background of NMT, pre-training methodology, and point out the main challenges when applying pre-training for NMT. Then we will focus on analysing the role of pre-training in enhancing the performance of NMT, how to design a better pre-training model for executing specific NMT tasks and how to better integrate the pre-trained model into NMT system. In each part, we will provide examples, discuss training techniques and analyse what is transferred when applying pre-training",
    "checked": true,
    "id": "d922481e0d1d069a9d9a5748451f60868db3d64a",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingxuan Wang",
      "Lei Li"
    ]
  },
  "https://aclanthology.org/2021.acl-tutorials.5": {
    "title": "Prosody: Models, Methods, and Applications",
    "volume": "tutorial",
    "abstract": "Prosody is essential in human interaction, enabling people to show interest, establish rapport, efficiently convey nuances of attitude or intent, and so on. Some applications that exploit prosodic knowledge have recently shown superhuman performance, and in many respects our ability to effectively model prosody is rapidly advancing. This tutorial will overview the computational modeling of prosody, including recent advances and diverse actual and potential applications",
    "checked": true,
    "id": "f58c3c4c334a0218eb6ddd9dded29d0274ee4c73",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nigel Ward",
      "Gina-Anne Levow"
    ]
  },
  "https://aclanthology.org/2021.acl-tutorials.6": {
    "title": "Recognizing Multimodal Entailment",
    "volume": "tutorial",
    "abstract": "How information is created, shared and consumed has changed rapidly in recent decades, in part thanks to new social platforms and technologies on the web. With ever-larger amounts of unstructured and limited labels, organizing and reconciling information from different sources and modalities is a central challenge in machine learning. This cutting-edge tutorial aims to introduce the multimodal entailment task, which can be useful for detecting semantic alignments when a single modality alone does not suffice for a whole content understanding. Starting with a brief overview of natural language processing, computer vision, structured data and neural graph learning, we lay the foundations for the multimodal sections to follow. We then discuss recent multimodal learning literature covering visual, audio and language streams, and explore case studies focusing on tasks which require fine-grained understanding of visual and linguistic semantics question answering, veracity and hatred classification. Finally, we introduce a new dataset for recognizing multimodal entailment, exploring it in a hands-on collaborative section. Overall, this tutorial gives an overview of multimodal learning, introduces a multimodal entailment dataset, and encourages future research in the topic",
    "checked": true,
    "id": "21f83719c9df64f79bac4ac1c81b7ea40fd9034e",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Cesar Ilharco",
      "Afsaneh Shirazi",
      "Arjun Gopalan",
      "Arsha Nagrani",
      "Blaz Bratanic",
      "Chris Bregler",
      "Christina Funk",
      "Felipe Ferreira",
      "Gabriel Barcik",
      "Gabriel Ilharco",
      "Georg Osang",
      "Jannis Bulian",
      "Jared Frank",
      "Lucas Smaira",
      "Qin Cao",
      "Ricardo Marino",
      "Roma Patel",
      "Thomas Leung",
      "Vaiva Imbrasaite"
    ]
  },
  "https://aclanthology.org/2021.bppf-1.1": {
    "title": "Benchmarking: Past, Present and Future",
    "volume": "workshop",
    "abstract": "Where have we been, and where are we going? It is easier to talk about the past than the future. These days, benchmarks evolve more bottom up (such as papers with code). There used to be more top-down leadership from government (and industry, in the case of systems, with benchmarks such as SPEC). Going forward, there may be more top-down leadership from organizations like MLPerf and/or influencers like David Ferrucci, who was responsible for IBM’s success with Jeopardy, and has recently written a paper suggesting how the community should think about benchmarking for machine comprehension. Tasks such as reading comprehension become even more interesting as we move beyond English. Multilinguality introduces many challenges, and even more opportunities",
    "checked": true,
    "id": "651883aae54e22bac13aa23776923ebce6086c4b",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Kenneth Church",
      "Mark Liberman",
      "Valia Kordoni"
    ]
  },
  "https://aclanthology.org/2021.bppf-1.2": {
    "title": "Guideline Bias in Wizard-of-Oz Dialogues",
    "volume": "workshop",
    "abstract": "NLP models struggle with generalization due to sampling and annotator bias. This paper focuses on a different kind of bias that has received very little attention: guideline bias, i.e., the bias introduced by how our annotator guidelines are formulated. We examine two recently introduced dialogue datasets, CCPE-M and Taskmaster-1, both collected by trained assistants in a Wizard-of-Oz set-up. For CCPE-M, we show how a simple lexical bias for the word like in the guidelines biases the data collection. This bias, in effect, leads to poor performance on data without this bias: a preference elicitation architecture based on BERT suffers a 5.3% absolute drop in performance, when like is replaced with a synonymous phrase, and a 13.2% drop in performance when evaluated on out-of-sample data. For Taskmaster-1, we show how the order in which instructions are resented, biases the data collection",
    "checked": true,
    "id": "c77e874b18852738acbccf17f9e585a78973d924",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Victor Petrén Bach Hansen",
      "Anders Søgaard"
    ]
  },
  "https://aclanthology.org/2021.bppf-1.3": {
    "title": "We Need to Consider Disagreement in Evaluation",
    "volume": "workshop",
    "abstract": "Evaluation is of paramount importance in data-driven research fields such as Natural Language Processing (NLP) and Computer Vision (CV). Current evaluation practice largely hinges on the existence of a single “ground truth” against which we can meaningfully compare the prediction of a model. However, this comparison is flawed for two reasons. 1) In many cases, more than one answer is correct. 2) Even where there is a single answer, disagreement among annotators is ubiquitous, making it difficult to decide on a gold standard. We argue that the current methods of adjudication, agreement, and evaluation need serious reconsideration. Some researchers now propose to minimize disagreement and to fix datasets. We argue that this is a gross oversimplification, and likely to conceal the underlying complexity. Instead, we suggest that we need to better capture the sources of disagreement to improve today’s evaluation practice. We discuss three sources of disagreement: from the annotator, the data, and the context, and show how this affects even seemingly objective tasks. Datasets with multiple annotations are becoming more common, as are methods to integrate disagreement into modeling. The logical next step is to extend this to evaluation",
    "checked": true,
    "id": "e98cae0fe2e4c4a7577233f14459cf05497bad3d",
    "semantic_title": "",
    "citation_count": 44,
    "authors": [
      "Valerio Basile",
      "Michael Fell",
      "Tommaso Fornaciari",
      "Dirk Hovy",
      "Silviu Paun",
      "Barbara Plank",
      "Massimo Poesio",
      "Alexandra Uma"
    ]
  },
  "https://aclanthology.org/2021.bppf-1.4": {
    "title": "How Might We Create Better Benchmarks for Speech Recognition?",
    "volume": "workshop",
    "abstract": "The applications of automatic speech recognition (ASR) systems are proliferating, in part due to recent significant quality improvements. However, as recent work indicates, even state-of-the-art speech recognition systems – some which deliver impressive benchmark results, struggle to generalize across use cases. We review relevant work, and, hoping to inform future benchmark development, outline a taxonomy of speech recognition use cases, proposed for the next generation of ASR benchmarks. We also survey work on metrics, in addition to the de facto standard Word Error Rate (WER) metric, and we introduce a versatile framework designed to describe interactions between linguistic variation and ASR performance metrics",
    "checked": true,
    "id": "d18908ea5ba46e244f2894339b393a5895b23c08",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Alëna Aksënova",
      "Daan van Esch",
      "James Flynn",
      "Pavel Golik"
    ]
  },
  "https://aclanthology.org/2021.case-1.1": {
    "title": "Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2021): Workshop and Shared Task Report",
    "volume": "workshop",
    "abstract": "This workshop is the fourth issue of a series of workshops on automatic extraction of socio-political events from news, organized by the Emerging Market Welfare Project, with the support of the Joint Research Centre of the European Commission and with contributions from many other prominent scholars in this field. The purpose of this series of workshops is to foster research and development of reliable, valid, robust, and practical solutions for automatically detecting descriptions of socio-political events, such as protests, riots, wars and armed conflicts, in text streams. This year workshop contributors make use of the state-of-the-art NLP technologies, such as Deep Learning, Word Embeddings and Transformers and cover a wide range of topics from text classification to news bias detection. Around 40 teams have registered and 15 teams contributed to three tasks that are i) multilingual protest news detection detection, ii) fine-grained classification of socio-political events, and iii) discovering Black Lives Matter protest events. The workshop also highlights two keynote and four invited talks about various aspects of creating event data sets and multi- and cross-lingual machine learning in few- and zero-shot settings",
    "checked": true,
    "id": "8a9fd5b7f37af3f36941e527b1d0761816fd1bef",
    "semantic_title": "",
    "citation_count": 25,
    "authors": [
      "Ali Hürriyetoğlu",
      "Hristo Tanev",
      "Vanni Zavarella",
      "Jakub Piskorski",
      "Reyyan Yeniterzi",
      "Osman Mutlu",
      "Deniz Yuret",
      "Aline Villavicencio"
    ]
  },
  "https://aclanthology.org/2021.case-1.2": {
    "title": "Keynote Abstract: Events on a Global Scale: Towards Language-Agnostic Event Extraction",
    "volume": "workshop",
    "abstract": "Event extraction is a challenging and exciting task in the world of machine learning & natural language processing. The breadth of events of possible interest, the speed at which surrounding socio-political event contexts evolve, and the complexities involved in generating representative annotated data all contribute to this challenge. One particular dimension of difficulty is the intrinsically global nature of events: many downstream use cases for event extraction involve reporting not just in a few major languages but in a much broader context. The languages of interest for even a fixed task may still shift from day to day, e.g. when a disease emerges in an unexpected location. Early approaches to multi-lingual event extraction (e.g. ACE) relied wholly on supervised data provided in each language of interest. Later approaches leveraged the success of machine translation to side-step the issue, simply translating foreign-language content to English and deploying English models on the result (often leaving some significant portion of the original content behind). Most recently, however, the community has begun to shown significant progress applying zero-shot transfer techniques to the problem, developing models using supervised English data but decoding in a foreign language without translation, typically using embedding spaces specifically designed to capture multi-lingual semantic content. In this talk I will discuss multiple dimensions of these promising new approaches and the linguistic representations that underlie them. I will compare them with approaches based on machine translation (as well as with models trained using in-language training data, where available), and discuss their strengths and weaknesses in different contexts, including the amount of English/foreign bitext available and the nature of the target event ontology. I will also discuss possible future directions with an eye to improving the quality of event extraction no matter its source around the globe",
    "checked": true,
    "id": "578e91ecb9a1e75eb6b477ca2511a6338dba333f",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Elizabeth Boschee"
    ]
  },
  "https://aclanthology.org/2021.case-1.3": {
    "title": "Keynote Abstract: Machine Learning in Conflict Studies: Reflections on Ethics, Collaboration, and Ongoing Challenges",
    "volume": "workshop",
    "abstract": "Advances in machine learning are nothing short of revolutionary in their potential to analyze massive amounts of data and in doing so, create new knowledge bases. But there is a responsibility in wielding the power to analyze these data since the public attributes a high degree of confidence to results which are based on big datasets. In this keynote, I will first address our ethical imperative as scholars to “get it right.” This imperative relates not only to model precision but also to the quality of the underlying data, and to whether the models inadvertently reproduce or obscure political biases in the source material. In considering the ethical imperative to get it right, it is also important to define what is “right”: what is considered an acceptable threshold for classification success needs to be understood in light of the project’s objectives. I then reflect on the different topics and data which are sourced in this field. Much of the existing research has focused on identifying conflict events (e.g. battles), but scholars are also increasingly turning to ML approaches to address other facets of the conflict environment. Conflict event extraction has long been a challenge for the natural language processing (NLP) community because it requires sophisticated methods for defining event ontologies, creating language resources, and developing algorithmic approaches. NLP machine-learning tools are ill-adapted to the complex, often messy, and diverse data generated during conflicts. Relative to other types of NLP text corpora, conflicts tend to generate less textual data, and texts are generated non-systematically. Conflict-related texts are often lexically idiosyncratic and tend to be written differently across actors, periods, and conflicts. Event definition and adjudication present tough challenges in the context of conflict corpora. Topics which rely on other types of data may be better-suited to NLP and machine learning methods. For example, Twitter and other social media data lend themselves well to studying hate speech, public opinion, social polarization, or discursive aspects of conflictual environments. Likewise, government-produced policy documents have typically been analyzed with historical, qualitative methods but their standardized formats and quantity suggest that ML methods can provide new traction. ML approaches may also allow scholars to exploit local sources and multi-language sources to a greater degree than has been possible. Many challenges remain, and these are best addressed in collaborative projects which build on interdisciplinary expertise. Classification projects need to be anchored in the theoretical interests of scholars of political violence if the data they produce are to be put to analytical use. There are few ontologies for classification that adequately reflect conflict researchers’ interests, which highlights the need for conceptual as well as technical development",
    "checked": true,
    "id": "71dae484071ef59eec4c92bda6aa03bd07cdd203",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Kristine Eck"
    ]
  },
  "https://aclanthology.org/2021.case-1.4": {
    "title": "PROTEST-ER: Retraining BERT for Protest Event Extraction",
    "volume": "workshop",
    "abstract": "We analyze the effect of further retraining BERT with different domain specific data as an unsupervised domain adaptation strategy for event extraction. Portability of event extraction models is particularly challenging, with large performance drops affecting data on the same text genres (e.g., news). We present PROTEST-ER, a retrained BERT model for protest event extraction. PROTEST-ER outperforms a corresponding generic BERT on out-of-domain data of 8.1 points. Our best performing models reach 51.91-46.39 F1 across both domains",
    "checked": true,
    "id": "2caf7df62e38ed6bebad395379d0e596f0d25b4e",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Tommaso Caselli",
      "Osman Mutlu",
      "Angelo Basile",
      "Ali Hürriyetoğlu"
    ]
  },
  "https://aclanthology.org/2021.case-1.5": {
    "title": "ArgFuse: A Weakly-Supervised Framework for Document-Level Event Argument Aggregation",
    "volume": "workshop",
    "abstract": "Most of the existing information extraction frameworks (Wadden et al., 2019; Veysehet al., 2020) focus on sentence-level tasks and are hardly able to capture the consolidated information from a given document. In our endeavour to generate precise document-level information frames from lengthy textual records, we introduce the task of Information Aggregation or Argument Aggregation. More specifically, our aim is to filter irrelevant and redundant argument mentions that were extracted at a sentence level and render a document level information frame. Majority of the existing works have been observed to resolve related tasks of document-level event argument extraction (Yang et al., 2018; Zheng et al., 2019) and salient entity identification (Jain et al., 2020) using supervised techniques. To remove dependency from large amounts of labelled data, we explore the task of information aggregation using weakly supervised techniques. In particular, we present an extractive algorithm with multiple sieves which adopts active learning strategies to work efficiently in low-resource settings. For this task, we have annotated our own test dataset comprising of 131 document information frames and have released the code and dataset to further research prospects in this new domain. To the best of our knowledge, we are the first to establish baseline results for this task in English. Our data and code are publicly available at https://github.com/DebanjanaKar/ArgFuse",
    "checked": true,
    "id": "427d52b3d5f5d1c4c1ec8fa436c254fd3a91a579",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Debanjana Kar",
      "Sudeshna Sarkar",
      "Pawan Goyal"
    ]
  },
  "https://aclanthology.org/2021.case-1.6": {
    "title": "Modality and Negation in Event Extraction",
    "volume": "workshop",
    "abstract": "Language provides speakers with a rich system of modality for expressing thoughts about events, without being committed to their actual occurrence. Modality is commonly used in the political news domain, where both actual and possible courses of events are discussed. NLP systems struggle with these semantic phenomena, often incorrectly extracting events which did not happen, which can lead to issues in downstream applications. We present an open-domain, lexicon-based event extraction system that captures various types of modality. This information is valuable for Question Answering, Knowledge Graph construction and Fact-checking tasks, and our evaluation shows that the system is sufficiently strong to be used in downstream applications",
    "checked": true,
    "id": "6f4e7ae892fb8ad98cf43753bb0b719c05ed5005",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Sander Bijl de Vroe",
      "Liane Guillou",
      "Miloš Stanojević",
      "Nick McKenna",
      "Mark Steedman"
    ]
  },
  "https://aclanthology.org/2021.case-1.7": {
    "title": "Characterizing News Portrayal of Civil Unrest in Hong Kong, 1998–2020",
    "volume": "workshop",
    "abstract": "We apply statistical techniques from natural language processing to a collection of Western and Hong Kong–based English-language newspaper articles spanning the years 1998–2020, studying the difference and evolution of its portrayal. We observe that both content and attitudes differ between Western and Hong Kong–based sources. ANOVA on keyword frequencies reveals that Hong Kong–based papers discuss protests and democracy less often. Topic modeling detects salient aspects of protests and shows that Hong Kong–based papers made fewer references to police violence during the Anti–Extradition Law Amendment Bill Movement. Diachronic shifts in word embedding neighborhoods reveal a shift in the characterization of salient keywords once the Movement emerged. Together, these raise questions about the existence of anodyne reporting from Hong Kong–based media. Likewise, they illustrate the importance of sample selection for protest event analysis",
    "checked": true,
    "id": "e624c6a333b23910244797040ccc4a4d01e1ccf7",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "James Scharf",
      "Arya D. McCarthy",
      "Giovanna Maria Dora Dore"
    ]
  },
  "https://aclanthology.org/2021.case-1.8": {
    "title": "Regressing Location on Text for Probabilistic Geocoding",
    "volume": "workshop",
    "abstract": "Text data are an important source of detailed information about social and political events. Automated systems parse large volumes of text data to infer or extract structured information that describes actors, actions, dates, times, and locations. One of these sub-tasks is geocoding: predicting the geographic coordinates associated with events or locations described by a given text. I present an end-to-end probabilistic model for geocoding text data. Additionally, I collect a novel data set for evaluating the performance of geocoding systems. I compare the model-based solution, called ELECTRo-map, to the current state-of-the-art open source system for geocoding texts for event data. Finally, I discuss the benefits of end-to-end model-based geocoding, including principled uncertainty estimation and the ability of these models to leverage contextual information",
    "checked": true,
    "id": "022122f91cbdfd0c8d5c484fe2e05cea1a69c57a",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Benjamin J. Radford"
    ]
  },
  "https://aclanthology.org/2021.case-1.9": {
    "title": "Extracting Events from Industrial Incident Reports",
    "volume": "workshop",
    "abstract": "Incidents in industries have huge social and political impact and minimizing the consequent damage has been a high priority. However, automated analysis of repositories of incident reports has remained a challenge. In this paper, we focus on automatically extracting events from incident reports. Due to absence of event annotated datasets for industrial incidents we employ a transfer learning based approach which is shown to outperform several baselines. We further provide detailed analysis regarding effect of increase in pre-training data and provide explainability of why pre-training improves the performance",
    "checked": true,
    "id": "f5c5b49f95513d0f55e4653c5c998a2c4e09dbec",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Nitin Ramrakhiyani",
      "Swapnil Hingmire",
      "Sangameshwar Patil",
      "Alok Kumar",
      "Girish Palshikar"
    ]
  },
  "https://aclanthology.org/2021.case-1.10": {
    "title": "Automatic Fake News Detection in Political Platforms - A Transformer-based Approach",
    "volume": "workshop",
    "abstract": "The dynamics and influence of fake news on Twitter during the 2020 US presidential election remains to be clarified. Here, we use a dataset related to 2020 U.S Election that consists of news articles and tweets on those articles. Therefore, it is extremely important to stop the spread of fake news before it reaches a mass level, which is a big challenge. We propose a novel fake news detection framework that can address this challenge. Our proposed framework exploits the information from news articles and social contexts to detect fake news. The proposed model is based on a Transformer architecture, which can learn useful representations from fake news data and predicts the probability of a news as being fake or real. Experimental results on real-world data show that our model can detect fake news with higher accuracy and much earlier, compared to the baselines",
    "checked": true,
    "id": "e5fb9e16e315c81cdc24c817397a786ece738df4",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Shaina Raza"
    ]
  },
  "https://aclanthology.org/2021.case-1.11": {
    "title": "Multilingual Protest News Detection - Shared Task 1, CASE 2021",
    "volume": "workshop",
    "abstract": "Benchmarking state-of-the-art text classification and information extraction systems in multilingual, cross-lingual, few-shot, and zero-shot settings for socio-political event information collection is achieved in the scope of the shared task Socio-political and Crisis Events Detection at the workshop CASE @ ACL-IJCNLP 2021. Socio-political event data is utilized for national and international policy- and decision-making. Therefore, the reliability and validity of these datasets are of the utmost importance. We split the shared task into three parts to address the three aspects of data collection (Task 1), fine-grained semantic classification (Task 2), and evaluation (Task 3). Task 1, which is the focus of this report, is on multilingual protest news detection and comprises four subtasks that are document classification (subtask 1), sentence classification (subtask 2), event sentence coreference identification (subtask 3), and event extraction (subtask 4). All subtasks had English, Portuguese, and Spanish for both training and evaluation data. Data in Hindi language was available only for the evaluation of subtask 1. The majority of the submissions, which are 238 in total, are created using multi- and cross-lingual approaches. Best scores are above 77.27 F1-macro for subtask 1, above 85.32 F1-macro for subtask 2, above 84.23 CoNLL 2012 average score for subtask 3, and above 66.20 F1-macro for subtask 4 in all evaluation settings. The performance of the best system for subtask 4 is above 66.20 F1 for all available languages. Although there is still a significant room for improvement in cross-lingual and zero-shot settings, the best submissions for each evaluation scenario yield remarkable results. Monolingual models outperformed the multilingual models in a few evaluation scenarios",
    "checked": true,
    "id": "766e30612f93e2e2b1f517fbb0f6fa4c7e953b71",
    "semantic_title": "",
    "citation_count": 35,
    "authors": [
      "Ali Hürriyetoğlu",
      "Osman Mutlu",
      "Erdem Yörük",
      "Farhana Ferdousi Liza",
      "Ritesh Kumar",
      "Shyam Ratan"
    ]
  },
  "https://aclanthology.org/2021.case-1.12": {
    "title": "Shared Task 1 System Description : Exploring different approaches for multilingual tasks",
    "volume": "workshop",
    "abstract": "The aim of the CASE 2021 Shared Task 1 was to detect and classify socio-political and crisis event information at document, sentence, cross-sentence, and token levels in a multilingual setting, with each of these subtasks being evaluated separately in each test language. Our submission contained entries in all of the subtasks, and the scores obtained validated our research finding : That the multilingual element of the tasks should be embraced, so that modeling and training regimes use the multilingual nature of the tasks to their mutual benefit, rather than trying to tackle the different languages separately",
    "checked": true,
    "id": "c9e467c6c983abc41567bd0a5f6a7eb2b35f320e",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Sureshkumar Vivek Kalyan",
      "Tan Paul",
      "Tan Shaun",
      "Martin Andrews"
    ]
  },
  "https://aclanthology.org/2021.case-1.13": {
    "title": "IIITT at CASE 2021 Task 1: Leveraging Pretrained Language Models for Multilingual Protest Detection",
    "volume": "workshop",
    "abstract": "In a world abounding in constant protests resulting from events like a global pandemic, climate change, religious or political conflicts, there has always been a need to detect events/protests before getting amplified by news media or social media. This paper demonstrates our work on the sentence classification subtask of multilingual protest detection in CASE@ACL-IJCNLP 2021. We approached this task by employing various multilingual pre-trained transformer models to classify if any sentence contains information about an event that has transpired or not. We performed soft voting over the models, achieving the best results among the models, accomplishing a macro F1-Score of 0.8291, 0.7578, and 0.7951 in English, Spanish, and Portuguese, respectively",
    "checked": true,
    "id": "d36d6abf8f9f1e80124d8a72dc5203802a6fdb26",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Pawan Kalyan",
      "Duddukunta Reddy",
      "Adeep Hande",
      "Ruba Priyadharshini",
      "Ratnasingam Sakuntharaj",
      "Bharathi Raja Chakravarthi"
    ]
  },
  "https://aclanthology.org/2021.case-1.14": {
    "title": "NUS-IDS at CASE 2021 Task 1: Improving Multilingual Event Sentence Coreference Identification With Linguistic Information",
    "volume": "workshop",
    "abstract": "Event Sentence Coreference Identification (ESCI) aims to cluster event sentences that refer to the same event together for information extraction. We describe our ESCI solution developed for the ACL-CASE 2021 shared tasks on the detection and classification of socio-political and crisis event information in a multilingual setting. For a given article, our proposed pipeline comprises of an accurate sentence pair classifier that identifies coreferent sentence pairs and subsequently uses these predicted probabilities to cluster sentences into groups. Sentence pair representations are constructed from fine-tuned BERT embeddings plus POS embeddings fed through a BiLSTM model, and combined with linguistic-based lexical and semantic similarities between sentences. Our best models ranked 2nd, 1st and 2nd and obtained CoNLL F1 scores of 81.20%, 93.03%, 83.15% for the English, Portuguese and Spanish test sets respectively in the ACL-CASE 2021 competition",
    "checked": true,
    "id": "e99dcd48b3bd92389b5676e37fdb2aa666105012",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Fiona Anting Tan",
      "Sujatha Das Gollapalli",
      "See-Kiong Ng"
    ]
  },
  "https://aclanthology.org/2021.case-1.15": {
    "title": "FKIE_itf_2021 at CASE 2021 Task 1: Using Small Densely Fully Connected Neural Nets for Event Detection and Clustering",
    "volume": "workshop",
    "abstract": "In this paper we present multiple approaches for event detection on document and sentence level, as well as a technique for event sentence co-reference resolution. The advantage of our co-reference resolution approach, which handles the task as a clustering problem, is that we use a single neural net to solve the task, which stands in contrast to other clustering algorithms that often are build on more complex models. This means that we can set our focus on the optimization of a single neural network instead of having to optimize numerous different parameters. We use small densely connected neural networks and pre-trained multilingual transformer embeddings in all subtasks. We use either document or sentence embeddings, depending on the task, and refrain from using word embeddings, so that the implementation of complicated network structures and unfolding of RNNs, which can deal with input of different sizes, is not necessary. We achieved an average macro F1 of 0.65 in subtask 1 (i.e., document level classification), and a macro F1 of 0.70 in subtask 2 (i.e., sentence level classification). For the co-reference resolution subtask, we achieved an average CoNLL-2012 score across all languages of 0.83",
    "checked": true,
    "id": "a2579a0dde6ec2c2934f8738cf4b08cfde26d696",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Nils Becker",
      "Theresa Krumbiegel"
    ]
  },
  "https://aclanthology.org/2021.case-1.16": {
    "title": "DAAI at CASE 2021 Task 1: Transformer-based Multilingual Socio-political and Crisis Event Detection",
    "volume": "workshop",
    "abstract": "Automatic socio-political and crisis event detection has been a challenge for natural language processing as well as social and political science communities, due to the diversity and nuance in such events and high accuracy requirements. In this paper, we propose an approach which can handle both document and cross-sentence level event detection in a multilingual setting using pretrained transformer models. Our approach became the winning solution in document level predictions and secured the 3rd place in cross-sentence level predictions for the English language. We could also achieve competitive results for other languages to prove the effectiveness and universality of our approach",
    "checked": true,
    "id": "5f73efd838d217c10c349745c9d14a0e799f8dad",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Hansi Hettiarachchi",
      "Mariam Adedoyin-Olowe",
      "Jagdev Bhogal",
      "Mohamed Medhat Gaber"
    ]
  },
  "https://aclanthology.org/2021.case-1.17": {
    "title": "SU-NLP at CASE 2021 Task 1: Protest News Detection for English",
    "volume": "workshop",
    "abstract": "This paper summarizes our group’s efforts in the multilingual protest news detection shared task, which is organized as a part of the Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE) Workshop. We participated in all four subtasks in English. Especially in the identification of event containing sentences task, our proposed ensemble approach using RoBERTa and multichannel CNN-LexStem model yields higher performance. Similarly in the event extraction task, our transformer-LSTM-CRF architecture outperforms regular transformers significantly",
    "checked": true,
    "id": "3f56e432bb52f3e196f297e5daeb375ada573031",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Furkan Çelik",
      "Tuğberk Dalkılıç",
      "Fatih Beyhan",
      "Reyyan Yeniterzi"
    ]
  },
  "https://aclanthology.org/2021.case-1.18": {
    "title": "IBM MNLP IE at CASE 2021 Task 1: Multigranular and Multilingual Event Detection on Protest News",
    "volume": "workshop",
    "abstract": "In this paper, we present the event detection models and systems we have developed for Multilingual Protest News Detection - Shared Task 1 at CASE 2021. The shared task has 4 subtasks which cover event detection at different granularity levels (from document level to token level) and across multiple languages (English, Hindi, Portuguese and Spanish). To handle data from multiple languages, we use a multilingual transformer-based language model (XLM-R) as the input text encoder. We apply a variety of techniques and build several transformer-based models that perform consistently well across all the subtasks and languages. Our systems achieve an average F_1 score of 81.2. Out of thirteen subtask-language tracks, our submissions rank 1st in nine and 2nd in four tracks",
    "checked": true,
    "id": "8a56a6cb53b8cd11df61a96f4643847f09de475e",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Parul Awasthy",
      "Jian Ni",
      "Ken Barker",
      "Radu Florian"
    ]
  },
  "https://aclanthology.org/2021.case-1.19": {
    "title": "ALEM at CASE 2021 Task 1: Multilingual Text Classification on News Articles",
    "volume": "workshop",
    "abstract": "We participated CASE shared task in ACL-IJCNLP 2021. This paper is a summary of our experiments and ideas about this shared task. For each subtask we shared our approach, successful and failed methods and our thoughts about them. We submit our results once for every subtask, except for subtask3, in task submission system and present scores based on our validation set formed from given training samples in this paper. Techniques and models we mentioned includes BERT, Multilingual BERT, oversampling, undersampling, data augmentation and their implications with each other. Most of the experiments we came up with were not completed, as time did not permit, but we share them here as we plan to do them as suggested in the future work part of document",
    "checked": true,
    "id": "0aa0e71233676c512d82cae0e1e5c6c35b446f1f",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Alaeddin Gürel",
      "Emre Emin"
    ]
  },
  "https://aclanthology.org/2021.case-1.20": {
    "title": "Team \"NoConflict\" at CASE 2021 Task 1: Pretraining for Sentence-Level Protest Event Detection",
    "volume": "workshop",
    "abstract": "An ever-increasing amount of text, in the form of social media posts and news articles, gives rise to new challenges and opportunities for the automatic extraction of socio-political events. In this paper, we present our submission to the Shared Tasks on Socio-Political and Crisis Events Detection, Task 1, Multilingual Protest News Detection, Subtask 2, Event Sentence Classification, of CASE @ ACL-IJCNLP 2021. In our submission, we utilize the RoBERTa model with additional pretraining, and achieve the best F1 score of 0.8532 in event sentence classification in English and the second-best F1 score of 0.8700 in Portuguese via simple translation. We analyze the failure cases of our model. We also conduct an ablation study to show the effect of choosing the right pretrained language model, adding additional training data and data augmentation",
    "checked": true,
    "id": "a591de4c03a8b25fbd47dd2b1dcd9778a86537b3",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Tiancheng Hu",
      "Niklas Stoehr"
    ]
  },
  "https://aclanthology.org/2021.case-1.21": {
    "title": "AMU-EURANOVA at CASE 2021 Task 1: Assessing the stability of multilingual BERT",
    "volume": "workshop",
    "abstract": "This paper explains our participation in task 1 of the CASE 2021 shared task. This task is about multilingual event extraction from news. We focused on sub-task 4, event information extraction. This sub-task has a small training dataset and we fine-tuned a multilingual BERT to solve this sub-task. We studied the instability problem on the dataset and tried to mitigate it",
    "checked": true,
    "id": "2d1a058afb5ff5029d0f9a0637bbabbbf359eabc",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Léo Bouscarrat",
      "Antoine Bonnefoy",
      "Cécile Capponi",
      "Carlos Ramisch"
    ]
  },
  "https://aclanthology.org/2021.case-1.22": {
    "title": "Team \"DaDeFrNi\" at CASE 2021 Task 1: Document and Sentence Classification for Protest Event Detection",
    "volume": "workshop",
    "abstract": "This paper accompanies our top-performing submission to the CASE 2021 shared task, which is hosted at the workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text. Subtasks 1 and 2 of Task 1 concern the classification of newspaper articles and sentences into “conflict” versus “not conflict”-related in four different languages. Our model performs competitively in both subtasks (up to 0.8662 macro F1), obtaining the highest score of all contributions for subtask 1 on Hindi articles (0.7877 macro F1). We describe all experiments conducted with the XLM-RoBERTa (XLM-R) model and report results obtained in each binary classification task. We propose supplementing the original training data with additional data on political conflict events. In addition, we provide an analysis of unigram probability estimates and geospatial references contained within the original training corpus",
    "checked": true,
    "id": "70911655f8488fdc595d26aa350c0fb7d5edf7c5",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Francesco Re",
      "Daniel Vegh",
      "Dennis Atzenhofer",
      "Niklas Stoehr"
    ]
  },
  "https://aclanthology.org/2021.case-1.23": {
    "title": "Fine-grained Event Classification in News-like Text Snippets - Shared Task 2, CASE 2021",
    "volume": "workshop",
    "abstract": "This paper describes the Shared Task on Fine-grained Event Classification in News-like Text Snippets. The Shared Task is divided into three sub-tasks: (a) classification of text snippets reporting socio-political events (25 classes) for which vast amount of training data exists, although exhibiting different structure and style vis-a-vis test data, (b) enhancement to a generalized zero-shot learning problem, where 3 additional event types were introduced in advance, but without any training data (‘unseen’ classes), and (c) further extension, which introduced 2 additional event types, announced shortly prior to the evaluation phase. The reported Shared Task focuses on classification of events in English texts and is organized as part of the Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2021), co-located with the ACL-IJCNLP 2021 Conference. Four teams participated in the task. Best performing systems for the three aforementioned sub-tasks achieved 83.9%, 79.7% and 77.1% weighted F1 scores respectively",
    "checked": true,
    "id": "fd8f89e592d06af710b8350349769dbd8979f49f",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Jacek Haneczok",
      "Guillaume Jacquet",
      "Jakub Piskorski",
      "Nicolas Stefanovitch"
    ]
  },
  "https://aclanthology.org/2021.case-1.24": {
    "title": "IBM MNLP IE at CASE 2021 Task 2: NLI Reranking for Zero-Shot Text Classification",
    "volume": "workshop",
    "abstract": "Supervised models can achieve very high accuracy for fine-grained text classification. In practice, however, training data may be abundant for some types but scarce or even non-existent for others. We propose a hybrid architecture that uses as much labeled data as available for fine-tuning classification models, while also allowing for types with little (few-shot) or no (zero-shot) labeled data. In particular, we pair a supervised text classification model with a Natural Language Inference (NLI) reranking model. The NLI reranker uses a textual representation of target types that allows it to score the strength with which a type is implied by a text, without requiring training data for the types. Experiments show that the NLI model is very sensitive to the choice of textual representation, but can be effective for classifying unseen types. It can also improve classification accuracy for the known types of an already highly accurate supervised model",
    "checked": true,
    "id": "5826d8f3fb3ccd79cfaf292bb0ddc5eebc13a580",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Ken Barker",
      "Parul Awasthy",
      "Jian Ni",
      "Radu Florian"
    ]
  },
  "https://aclanthology.org/2021.case-1.25": {
    "title": "CASE 2021 Task 2: Zero-Shot Classification of Fine-Grained Sociopolitical Events with Transformer Models",
    "volume": "workshop",
    "abstract": "We introduce a method for the classification of texts into fine-grained categories of sociopolitical events. This particular method is responsive to all three Subtasks of Task 2, Fine-Grained Classification of Socio-Political Events, introduced at the CASE workshop of ACL-IJCNLP 2021. We frame Task 2 as textual entailment: given an input text and a candidate event class (“query”), the model predicts whether the text describes an event of the given type. The model is able to correctly classify in-sample event types with an average F1-score of 0.74 but struggles with some out-of-sample event types. Despite this, the model shows promise for the zero-shot identification of certain sociopolitical events by achieving an F1-score of 0.52 on one wholly out-of-sample event class",
    "checked": true,
    "id": "7927f61242887c5bf7539db20144ca7114cd0fdd",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Benjamin J. Radford"
    ]
  },
  "https://aclanthology.org/2021.case-1.26": {
    "title": "CASE 2021 Task 2 Socio-political Fine-grained Event Classification using Fine-tuned RoBERTa Document Embeddings",
    "volume": "workshop",
    "abstract": "We present our submission to Task 2 of the Socio-political and Crisis Events Detection Shared Task at the CASE @ ACL-IJCNLP 2021 workshop. The task at hand aims at the fine-grained classification of socio-political events. Our best model was a fine-tuned RoBERTa transformer model using document embeddings. The corpus consisted of a balanced selection of sub-events extracted from the ACLED event dataset. We achieved a macro F-score of 0.923 and a micro F-score of 0.932 during our preliminary experiments on a held-out test set. The same model also performed best on the shared task test data (weighted F-score = 0.83). To analyze the results we calculated the topic compactness of the commonly misclassified events and conducted an error analysis",
    "checked": true,
    "id": "18883f797d25d5c1cb52f7256b84aacc33b6831e",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Samantha Kent",
      "Theresa Krumbiegel"
    ]
  },
  "https://aclanthology.org/2021.case-1.27": {
    "title": "Discovering Black Lives Matter Events in the United States: Shared Task 3, CASE 2021",
    "volume": "workshop",
    "abstract": "Evaluating the state-of-the-art event detection systems on determining spatio-temporal distribution of the events on the ground is performed unfrequently. But, the ability to both (1) extract events “in the wild” from text and (2) properly evaluate event detection systems has potential to support a wide variety of tasks such as monitoring the activity of socio-political movements, examining media coverage and public support of these movements, and informing policy decisions. Therefore, we study performance of the best event detection systems on detecting Black Lives Matter (BLM) events from tweets and news articles. The murder of George Floyd, an unarmed Black man, at the hands of police officers received global attention throughout the second half of 2020. Protests against police violence emerged worldwide and the BLM movement, which was once mostly regulated to the United States, was now seeing activity globally. This shared task asks participants to identify BLM related events from large unstructured data sources, using systems pretrained to extract socio-political events from text. We evaluate several metrics, accessing each system’s ability to identify protest events both temporally and spatially. Results show that identifying daily protest counts is an easier task than classifying spatial and temporal protest trends simultaneously, with maximum performance of 0.745 and 0.210 (Pearson r), respectively. Additionally, all baselines and participant systems suffered from low recall, with a maximum recall of 5.08",
    "checked": true,
    "id": "6c3fa2310ea8476042bc7a005d8c509bc3a5643c",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Salvatore Giorgi",
      "Vanni Zavarella",
      "Hristo Tanev",
      "Nicolas Stefanovitch",
      "Sy Hwang",
      "Hansi Hettiarachchi",
      "Tharindu Ranasinghe",
      "Vivek Kalyan",
      "Paul Tan",
      "Shaun Tan",
      "Martin Andrews",
      "Tiancheng Hu",
      "Niklas Stoehr",
      "Francesco Ignazio Re",
      "Daniel Vegh",
      "Dennis Atzenhofer",
      "Brenda Curtis",
      "Ali Hürriyetoğlu"
    ]
  },
  "https://aclanthology.org/2021.dialdoc-1.1": {
    "title": "DialDoc 2021 Shared Task: Goal-Oriented Document-grounded Dialogue Modeling",
    "volume": "workshop",
    "abstract": "We present the results of Shared Task at Workshop DialDoc 2021 that is focused on document-grounded dialogue and conversational question answering. The primary goal of this Shared Task is to build goal-oriented information-seeking conversation systems that can identify the most relevant knowledge in the associated document for generating agent responses in natural language. It includes two subtasks on predicting agent responses: the first subtask is to predict the grounding text span in the given document for next agent response; the second subtask is to generate agent response in natural language given the context. Many submissions outperform baseline significantly. For the first task, the best-performing system achieved 67.1 Exact Match and 76.3 F1. For the second subtask, the best system achieved 41.1 SacreBLEU and highest rank by human evaluation",
    "checked": true,
    "id": "8a4967022792fa0113388aef757b9a2c54dcc3a0",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Song Feng"
    ]
  },
  "https://aclanthology.org/2021.dialdoc-1.2": {
    "title": "SeqDialN: Sequential Visual Dialog Network in Joint Visual-Linguistic Representation Space",
    "volume": "workshop",
    "abstract": "The key challenge of the visual dialog task is how to fuse features from multimodal sources and extract relevant information from dialog history to answer the current query. In this work, we formulate a visual dialog as an information flow in which each piece of information is encoded with the joint visual-linguistic representation of a single dialog round. Based on this formulation, we consider the visual dialog task as a sequence problem consisting of ordered visual-linguistic vectors.For featurization, we use a Dense SymmetricCo-Attention network (Nguyen and Okatani,2018) as a lightweight vison-language joint representation generator to fuse multimodal features (i.e., image and text), yielding better computation and data efficiencies. For inference, we propose two Sequential Dialog Networks (SeqDialN): the first uses LSTM(Hochreiter and Schmidhuber,1997) for information propagation (IP) and the second uses a modified Transformer (Vaswani et al.,2017) for multi-step reasoning (MR). Our architecture separates the complexity of multimodal feature fusion from that of inference, which allows simpler design of the inference engine. On VisDial v1.0 test-std dataset, our best single generative SeqDialN achieves 62.54% NDCG and 48.63% MRR; our ensemble generative SeqDialN achieves 63.78% NDCG and 49.98% MRR, which set a new state-of-the-art generative visual dialog model. We fine-tune discriminative SeqDialN with dense annotations and boost the performance up to 72.41% NDCG and 55.11% MRR. In this work, we discuss the extensive experiments we have conducted to demonstrate the effectiveness of our model components. We also provide visualization for the reasoning process from the relevant conversation rounds and discuss our fine-tuning methods. The code is available at https://github.com/xiaoxiaoheimei/SeqDialN",
    "checked": true,
    "id": "6244ee78af37fc875d4d2d75833b9b1fdebc5fd3",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Liu Yang",
      "Fanqi Meng",
      "Xiao Liu",
      "Ming-Kuang Daniel Wu",
      "Vicent Ying",
      "James Xu"
    ]
  },
  "https://aclanthology.org/2021.dialdoc-1.3": {
    "title": "A Template-guided Hybrid Pointer Network for Knowledge-based Task-oriented Dialogue Systems",
    "volume": "workshop",
    "abstract": "Most existing neural network based task-oriented dialog systems follow encoder-decoder paradigm, where the decoder purely depends on the source texts to generate a sequence of words, usually suffering from instability and poor readability. Inspired by the traditional template-based generation approaches, we propose a template-guided hybrid pointer network for knowledge-based task-oriented dialog systems, which retrieves several potentially relevant answers from a pre-constructed domain-specific conversational repository as guidance answers, and incorporates the guidance answers into both the encoding and decoding processes. Specifically, we design a memory pointer network model with a gating mechanism to fully exploit the semantic correlation between the retrieved answers and the ground-truth response. We evaluate our model on four widely used task-oriented datasets, including one simulated and three manually created datasets. The experimental results demonstrate that the proposed model achieves significantly better performance than the state-of-the-art methods over different automatic evaluation metrics",
    "checked": true,
    "id": "45e07c02e3247567c64948563179d988c80d0804",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Dingmin Wang",
      "Ziyao Chen",
      "Wanwei He",
      "Li Zhong",
      "Yunzhe Tao",
      "Min Yang"
    ]
  },
  "https://aclanthology.org/2021.dialdoc-1.4": {
    "title": "Automatic Learning Assistant in Telugu",
    "volume": "workshop",
    "abstract": "This paper presents a learning assistant that tests one’s knowledge and gives feedback that helps a person learn at a faster pace. A learning assistant (based on automated question generation) has extensive uses in education, information websites, self-assessment, FAQs, testing ML agents, research, etc. Multiple researchers, and companies have worked on Virtual Assistance, but majorly in English. We built our learning assistant for Telugu language to help with teaching in the mother tongue, which is the most efficient way of learning. Our system is built primarily based on Question Generation in Telugu. Many experiments were conducted on Question Generation in English in multiple ways. We have built the first hybrid machine learning and rule-based solution in Telugu, which proves efficient for short stories or short passages in children’s books. Our work covers the fundamental question forms with question types: adjective, yes/no, adverb, verb, when, where, whose, quotative, and quantitative (how many/how much). We constructed rules for question generation using Part of Speech (POS) tags and Universal Dependency (UD) tags along with linguistic information of the surrounding relevant context of the word. We used keyword matching, multilingual sentence embedding to evaluate the answer. Our system is primarily built on question generation in Telugu, and is also capable of evaluating the user’s answers to the generated questions",
    "checked": true,
    "id": "7e98efedeedc6c1eab1a7c7962b9a2d9de4dcf98",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meghana Bommadi",
      "Shreya Terupally",
      "Radhika Mamidi"
    ]
  },
  "https://aclanthology.org/2021.dialdoc-1.5": {
    "title": "Combining Open Domain Question Answering with a Task-Oriented Dialog System",
    "volume": "workshop",
    "abstract": "We apply the modular dialog system framework to combine open-domain question answering with a task-oriented dialog system. This meta dialog system can answer questions from Wikipedia and at the same time act as a personal assistant. The aim of this system is to combine the strength of an open-domain question answering system with the conversational power of task-oriented dialog systems. After explaining the technical details of the system, we combined a new dataset out of standard datasets to evaluate the system. We further introduce an evaluation method for this system. Using this method, we compare the performance of the non-modular system with the performance of the modular system and show that the modular dialog system framework is very suitable for this combination of conversational agents and that the performance of each agent decreases only marginally through the modular setting",
    "checked": true,
    "id": "1f7ed24751fe59f270054bf91707e0b36ecb2d1f",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Jan Nehring",
      "Nils Feldhus",
      "Harleen Kaur",
      "Akhyar Ahmed"
    ]
  },
  "https://aclanthology.org/2021.dialdoc-1.6": {
    "title": "CAiRE in DialDoc21: Data Augmentation for Information Seeking Dialogue System",
    "volume": "workshop",
    "abstract": "Information-seeking dialogue systems, including knowledge identification and response generation, aim to respond to users with fluent, coherent, and informative responses based on users’ needs, which. To tackle this challenge, we utilize data augmentation methods and several training techniques with the pre-trained language models to learn a general pattern of the task and thus achieve promising performance. In DialDoc21 competition, our system achieved 74.95 F1 score and 60.74 Exact Match score in subtask 1, and 37.72 SacreBLEU score in subtask 2. Empirical analysis is provided to explain the effectiveness of our approaches",
    "checked": true,
    "id": "1d32e792d54bac1f87e9857b7d55f01ed0ab4065",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Yan Xu",
      "Etsuko Ishii",
      "Genta Indra Winata",
      "Zhaojiang Lin",
      "Andrea Madotto",
      "Zihan Liu",
      "Peng Xu",
      "Pascale Fung"
    ]
  },
  "https://aclanthology.org/2021.dialdoc-1.7": {
    "title": "Technical Report on Shared Task in DialDoc21",
    "volume": "workshop",
    "abstract": "We participate in the DialDoc Shared Task sub-task 1 (Knowledge Identification). The task requires identifying the grounding knowledge in form of a document span for the next dialogue turn. We employ two well-known pre-trained language models (RoBERTa and ELECTRA) to identify candidate document spans and propose a metric-based ensemble method for span selection. Our methods include data augmentation, model pre-training/fine-tuning, post-processing, and ensemble. On the submission page, we rank 2nd based on the average of normalized F1 and EM scores used for the final evaluation. Specifically, we rank 2nd on EM and 3rd on F1",
    "checked": true,
    "id": "fe76877921a3112e02202f6646547138138f2971",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Jiapeng Li",
      "Mingda Li",
      "Longxuan Ma",
      "Wei-Nan Zhang",
      "Ting Liu"
    ]
  },
  "https://aclanthology.org/2021.dialdoc-1.8": {
    "title": "Cascaded Span Extraction and Response Generation for Document-Grounded Dialog",
    "volume": "workshop",
    "abstract": "This paper summarizes our entries to both subtasks of the first DialDoc shared task which focuses on the agent response prediction task in goal-oriented document-grounded dialogs. The task is split into two subtasks: predicting a span in a document that grounds an agent turn and generating an agent response based on a dialog and grounding document. In the first subtask, we restrict the set of valid spans to the ones defined in the dataset, use a biaffine classifier to model spans, and finally use an ensemble of different models. For the second sub-task, we use a cascaded model which grounds the response prediction on the predicted span instead of the full document. With these approaches, we obtain significant improvements in both subtasks compared to the baseline",
    "checked": true,
    "id": "4058cd333b2426e897fa22ae1d0c16da54295081",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Nico Daheim",
      "David Thulke",
      "Christian Dugast",
      "Hermann Ney"
    ]
  },
  "https://aclanthology.org/2021.dialdoc-1.9": {
    "title": "Ensemble ALBERT and RoBERTa for Span Prediction in Question Answering",
    "volume": "workshop",
    "abstract": "Retrieving relevant answers from heterogeneous data formats, for given for questions, is a challenging problem. The process of pinpointing relevant information suitable to answer a question is further compounded in large document collections containing documents of substantial length. This paper presents the models designed as part of our submission to the DialDoc21 Shared Task (Document-grounded Dialogue and Conversational Question Answering) for span prediction in question answering. The proposed models leverage the superior predictive power of pretrained transformer models like RoBERTa, ALBERT and ELECTRA, to identify the most relevant information in an associated passage for the next agent turn. To further enhance the performance, the models were fine-tuned on different span selection based question answering datasets like SQuAD2.0 and Natural Questions (NQ) corpus. We also explored ensemble techniques for combining multiple models to achieve enhanced performance for the task. Our team SB_NITK ranked 6th on the leaderboard for the Knowledge Identification task, and our best ensemble model achieved an Exact score of 58.58 and an F1 score of 73.39",
    "checked": true,
    "id": "873f58e26ff8987b2a77e3e89e3419213011059c",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Sony Bachina",
      "Spandana Balumuri",
      "Sowmya Kamath S"
    ]
  },
  "https://aclanthology.org/2021.dialdoc-1.10": {
    "title": "WeaSuL: Weakly Supervised Dialogue Policy Learning: Reward Estimation for Multi-turn Dialogue",
    "volume": "workshop",
    "abstract": "An intelligent dialogue system in a multi-turn setting should not only generate the responses which are of good quality, but it should also generate the responses which can lead to long-term success of the dialogue. Although, the current approaches improved the response quality, but they over-look the training signals present in the dialogue data. We can leverage these signals to generate the weakly supervised training data for learning dialog policy and reward estimator, and make the policy take actions (generates responses) which can foresee the future direction for a successful (rewarding) conversation. We simulate the dialogue between an agent and a user (modelled similar to an agent with supervised learning objective) to interact with each other. The agent uses dynamic blocking to generate ranked diverse responses and exploration-exploitation to select among the Top-K responses. Each simulated state-action pair is evaluated (works as a weak annotation) with three quality modules: Semantic Relevant, Semantic Coherence and Consistent Flow. Empirical studies with two benchmarks indicate that our model can significantly out-perform the response quality and lead to a successful conversation on both automatic evaluation and human judgment",
    "checked": true,
    "id": "bab1b89c4b69b9661037114d45af68d26d3cdd70",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Anant Khandelwal"
    ]
  },
  "https://aclanthology.org/2021.dialdoc-1.11": {
    "title": "Summary-Oriented Question Generation for Informational Queries",
    "volume": "workshop",
    "abstract": "Users frequently ask simple factoid questions for question answering (QA) systems, attenuating the impact of myriad recent works that support more complex questions. Prompting users with automatically generated suggested questions (SQs) can improve user understanding of QA system capabilities and thus facilitate more effective use. We aim to produce self-explanatory questions that focus on main document topics and are answerable with variable length passages as appropriate. We satisfy these requirements by using a BERT-based Pointer-Generator Network trained on the Natural Questions (NQ) dataset. Our model shows SOTA performance of SQ generation on the NQ dataset (20.1 BLEU-4). We further apply our model on out-of-domain news articles, evaluating with a QA system due to the lack of gold questions and demonstrate that our model produces better SQs for news articles – with further confirmation via a human evaluation",
    "checked": true,
    "id": "cbff47217d9b016a7060e2713a97f12b922918f4",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Xusen Yin",
      "Li Zhou",
      "Kevin Small",
      "Jonathan May"
    ]
  },
  "https://aclanthology.org/2021.dialdoc-1.12": {
    "title": "Document-Grounded Goal-Oriented Dialogue Systems on Pre-Trained Language Model with Diverse Input Representation",
    "volume": "workshop",
    "abstract": "Document-grounded goal-oriented dialog system understands users’ utterances, and generates proper responses by using information obtained from documents. The Dialdoc21 shared task consists of two subtasks; subtask1, finding text spans associated with users’ utterances from documents, and subtask2, generating responses based on information obtained from subtask1. In this paper, we propose two models (i.e., a knowledge span prediction model and a response generation model) for the subtask1 and the subtask2. In the subtask1, dialogue act losses are used with RoBERTa, and title embeddings are added to input representation of RoBERTa. In the subtask2, various special tokens and embeddings are added to input representation of BART’s encoder. Then, we propose a method to assign different difficulty scores to leverage curriculum learning. In the subtask1, our span prediction model achieved F1-scores of 74.81 (ranked at top 7) and 73.41 (ranked at top 5) in test-dev phase and test phase, respectively. In the subtask2, our response generation model achieved sacreBLEUs of 37.50 (ranked at top 3) and 41.06 (ranked at top 1) in in test-dev phase and test phase, respectively",
    "checked": true,
    "id": "defffa480d4a22dee2c05eba8d888ddcca2889fc",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Boeun Kim",
      "Dohaeng Lee",
      "Sihyung Kim",
      "Yejin Lee",
      "Jin-Xia Huang",
      "Oh-Woog Kwon",
      "Harksoo Kim"
    ]
  },
  "https://aclanthology.org/2021.dialdoc-1.13": {
    "title": "Team JARS: DialDoc Subtask 1 - Improved Knowledge Identification with Supervised Out-of-Domain Pretraining",
    "volume": "workshop",
    "abstract": "In this paper, we discuss our submission for DialDoc subtask 1. The subtask requires systems to extract knowledge from FAQ-type documents vital to reply to a user’s query in a conversational setting. We experiment with pretraining a BERT-based question-answering model on different QA datasets from MRQA, as well as conversational QA datasets like CoQA and QuAC. Our results show that models pretrained on CoQA and QuAC perform better than their counterparts that are pretrained on MRQA datasets. Our results also indicate that adding more pretraining data does not necessarily result in improved performance. Our final model, which is an ensemble of AlBERT-XL pretrained on CoQA and QuAC independently, with the chosen answer having the highest average probability score, achieves an F1-Score of 70.9% on the official test-set",
    "checked": true,
    "id": "b709ea41d67bb86f10b58c1f1012322b2fd1e3d2",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Sopan Khosla",
      "Justin Lovelace",
      "Ritam Dutt",
      "Adithya Pratapa"
    ]
  },
  "https://aclanthology.org/2021.dialdoc-1.14": {
    "title": "Building Goal-oriented Document-grounded Dialogue Systems",
    "volume": "workshop",
    "abstract": "In this paper, we describe our systems for solving the two Doc2Dial shared task: knowledge identification and response generation. We proposed several pre-processing and post-processing methods, and we experimented with data augmentation by pre-training the models on other relevant datasets. Our best model for knowledge identification outperformed the baseline by 10.5+ f1-score on the test-dev split, and our best model for response generation outperformed the baseline by 11+ Sacrebleu score on the test-dev split",
    "checked": true,
    "id": "8ff73241a805c99f4e14600a224e0b2bedc7aab2",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Xi Chen",
      "Faner Lin",
      "Yeju Zhou",
      "Kaixin Ma",
      "Jonathan Francis",
      "Eric Nyberg",
      "Alessandro Oltramari"
    ]
  },
  "https://aclanthology.org/2021.dialdoc-1.15": {
    "title": "Agenda Pushing in Email to Thwart Phishing",
    "volume": "workshop",
    "abstract": "In this work, we draw parallels between automatically responding to emails for combating social-engineering attacks and document-grounded response generation and lay out the blueprint of our approach. Phishing emails are longer than dialogue utterances and often contain multiple intents. Hence, we need to make decisions similar to those for document-grounded responses in deciding what parts of long text to use and how to address each intent to generate a knowledgeable multi-component response that pushes scammers towards agendas that aid in attribution and linking attacks. We propose , a hybrid system that uses customizable probabilistic finite state transducers to orchestrate pushing agendas coupled with neural dialogue systems that generate responses to unexpected prompts, as a promising solution to this end. We emphasize the need for this system by highlighting each component’s strengths and weaknesses and show how they complement each other",
    "checked": true,
    "id": "81f38f044b8d7b5904b1fc7efd921dfdcabc6abc",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyundong Cho",
      "Genevieve Bartlett",
      "Marjorie Freedman"
    ]
  },
  "https://aclanthology.org/2021.dialdoc-1.16": {
    "title": "Can I Be of Further Assistance? Using Unstructured Knowledge Access to Improve Task-oriented Conversational Modeling",
    "volume": "workshop",
    "abstract": "Most prior work on task-oriented dialogue systems are restricted to limited coverage of domain APIs. However, users oftentimes have requests that are out of the scope of these APIs. This work focuses on responding to these beyond-API-coverage user turns by incorporating external, unstructured knowledge sources. Our approach works in a pipelined manner with knowledge-seeking turn detection, knowledge selection, and response generation in sequence. We introduce novel data augmentation methods for the first two steps and demonstrate that the use of information extracted from dialogue context improves the knowledge selection and end-to-end performances. Through experiments, we achieve state-of-the-art performance for both automatic and human evaluation metrics on the DSTC9 Track 1 benchmark dataset, validating the effectiveness of our contributions",
    "checked": true,
    "id": "34a6f05b9f8bb8222fa7d02cf5f0ad544ffcfaf7",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Di Jin",
      "Seokhwan Kim",
      "Dilek Hakkani-Tur"
    ]
  },
  "https://aclanthology.org/2021.ecnlp-1.1": {
    "title": "BERT Goes Shopping: Comparing Distributional Models for Product Representations",
    "volume": "workshop",
    "abstract": "Word embeddings (e.g., word2vec) have been applied successfully to eCommerce products through prod2vec. Inspired by the recent performance improvements on several NLP tasks brought by contextualized embeddings, we propose to transfer BERT-like architectures to eCommerce: our model - Prod2BERT - is trained to generate representations of products through masked session modeling. Through extensive experiments over multiple shops, different tasks, and a range of design choices, we systematically compare the accuracy of Prod2BERT and prod2vec embeddings: while Prod2BERT is found to be superior in several scenarios, we highlight the importance of resources and hyperparameters in the best performing models. Finally, we provide guidelines to practitioners for training embeddings under a variety of computational and data constraints",
    "checked": true,
    "id": "98cadf04e1f4a65552debc0376471f7370f2e8da",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Federico Bianchi",
      "Bingqing Yu",
      "Jacopo Tagliabue"
    ]
  },
  "https://aclanthology.org/2021.ecnlp-1.2": {
    "title": "Attribute Value Generation from Product Title using Language Models",
    "volume": "workshop",
    "abstract": "Identifying the value of product attribute is essential for many e-commerce functions such as product search and product recommendations. Therefore, identifying attribute values from unstructured product descriptions is a critical undertaking for any e-commerce retailer. What makes this problem challenging is the diversity of product types and their attributes and values. Existing methods have typically employed multiple types of machine learning models, each of which handles specific product types or attribute classes. This has limited their scalability and generalization for large scale real world e-commerce applications. Previous approaches for this task have formulated the attribute value extraction as a Named Entity Recognition (NER) task or a Question Answering (QA) task. In this paper we have presented a generative approach to the attribute value extraction problem using language models. We leverage the large-scale pretraining of the GPT-2 and the T5 text-to-text transformer to create fine-tuned models that can effectively perform this task. We show that a single general model is very effective for this task over a broad set of product attribute values with the open world assumption. Our approach achieves state-of-the-art performance for different attribute classes, which has previously required a diverse set of models",
    "checked": true,
    "id": "c9e58cdcd05ab00f48953238a159dc8b1943c047",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Kalyani Roy",
      "Pawan Goyal",
      "Manish Pandey"
    ]
  },
  "https://aclanthology.org/2021.ecnlp-1.3": {
    "title": "ASR Adaptation for E-commerce Chatbots using Cross-Utterance Context and Multi-Task Language Modeling",
    "volume": "workshop",
    "abstract": "Automatic Speech Recognition (ASR) robustness toward slot entities are critical in e-commerce voice assistants that involve monetary transactions and purchases. Along with effective domain adaptation, it is intuitive that cross utterance contextual cues play an important role in disambiguating domain specific content words from speech. In this paper, we investigate various techniques to improve contextualization, content word robustness and domain adaptation of a Transformer-XL neural language model (NLM) to rescore ASR N-best hypotheses. To improve contextualization, we utilize turn level dialogue acts along with cross utterance context carry over. Additionally, to adapt our domain-general NLM towards e-commerce on-the-fly, we use embeddings derived from a finetuned masked LM on in-domain data. Finally, to improve robustness towards in-domain content words, we propose a multi-task model that can jointly perform content word detection and language modeling tasks. Compared to a non-contextual LSTM LM baseline, our best performing NLM rescorer results in a content WER reduction of 19.2% on e-commerce audio test set and a slot labeling F1 improvement of 6.4%",
    "checked": true,
    "id": "692ad068725747b80a7f4a2d5f9f826447031c90",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Ashish Shenoy",
      "Sravan Bodapati",
      "Katrin Kirchhoff"
    ]
  },
  "https://aclanthology.org/2021.ecnlp-1.4": {
    "title": "Turn-Level User Satisfaction Estimation in E-commerce Customer Service",
    "volume": "workshop",
    "abstract": "User satisfaction estimation in the dialogue-based customer service is critical not only for helping developers find the system defects, but also making it possible to get timely human intervention for dissatisfied customers. In this paper, we investigate the problem of user satisfaction estimation in E-commerce customer service. In order to apply the estimator to online services for timely human intervention, we need to estimate the satisfaction score at each turn. However, in actual scenario we can only collect the satisfaction labels for the whole dialogue sessions via user feedback. To this end, we formalize the turn-level satisfaction estimation as a reinforcement learning problem, in which the model can be optimized with only session-level satisfaction labels. We conduct experiments on the dataset collected from a commercial customer service system, and compare our model with the supervised learning models. Extensive experiments show that the proposed method outperforms all the baseline models",
    "checked": true,
    "id": "60c978e8a0929ec10f0177677fa44f2c30ec941f",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Runze Liang",
      "Ryuichi Takanobu",
      "Feng-Lin Li",
      "Ji Zhang",
      "Haiqing Chen",
      "Minlie Huang"
    ]
  },
  "https://aclanthology.org/2021.ecnlp-1.5": {
    "title": "Keyword Augmentation via Generative Methods",
    "volume": "workshop",
    "abstract": "Keyword augmentation is a fundamental problem for sponsored search modeling and business. Machine generated keywords can be recommended to advertisers for better campaign discoverability as well as used as features for sourcing and ranking models. Generating high-quality keywords is difficult, especially for cold campaigns with limited or even no historical logs; and the industry trend of including multiple products in a single ad campaign is making the problem more challenging. In this paper, we propose a keyword augmentation method based on generative seq2seq model and trie-based search mechanism, which is able to generate high-quality keywords for any products or product lists. We conduct human annotations, offline analysis, and online experiments to evaluate the performance of our method against benchmarks in terms of augmented keyword quality as well as lifted ad exposure. The experiment results demonstrate that our method is able to generate more valid keywords which can serve as an efficient addition to advertiser selected keywords",
    "checked": true,
    "id": "4320e6cfe1c4b5b20e50c7563319db3a7637b6b8",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Shi",
      "Zhibiao Rao",
      "Yongning Wu",
      "Zuohua Zhang",
      "Chu Wang"
    ]
  },
  "https://aclanthology.org/2021.ecnlp-1.6": {
    "title": "Personalized Entity Resolution with Dynamic Heterogeneous KnowledgeGraph Representations",
    "volume": "workshop",
    "abstract": "The growing popularity of Virtual Assistants poses new challenges for Entity Resolution, the task of linking mentions in text to their referent entities in a knowledge base. Specifically, in the shopping domain, customers tend to mention the entities implicitly (e.g., “organic milk”) rather than use the entity names explicitly, leading to a large number of candidate products. Meanwhile, for the same query, different customers may expect different results. For example, with “add milk to my cart”, a customer may refer to a certain product from his/her favorite brand, while some customers may want to re-order products they regularly purchase. Moreover, new customers may lack persistent shopping history, which requires us to enrich the connections between customers through products and their attributes. To address these issues, we propose a new framework that leverages personalized features to improve the accuracy of product ranking. We first build a cross-source heterogeneous knowledge graph from customer purchase history and product knowledge graph to jointly learn customer and product embeddings. After that, we incorporate product, customer, and history representations into a neural reranking model to predict which candidate is most likely to be purchased by a specific customer. Experiment results show that our model substantially improves the accuracy of the top ranked candidates by 24.6% compared to the state-of-the-art product search model",
    "checked": true,
    "id": "d18ebaf39cd27aa65c1bcf3748f1824d668709b5",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Ying Lin",
      "Han Wang",
      "Jiangning Chen",
      "Tong Wang",
      "Yue Liu",
      "Heng Ji",
      "Yang Liu",
      "Premkumar Natarajan"
    ]
  },
  "https://aclanthology.org/2021.ecnlp-1.7": {
    "title": "A Semi-supervised Multi-task Learning Approach to Classify Customer Contact Intents",
    "volume": "workshop",
    "abstract": "In the area of customer support, understanding customers’ intents is a crucial step. Machine learning plays a vital role in this type of intent classification. In reality, it is typical to collect confirmation from customer support representatives (CSRs) regarding the intent prediction, though it can unnecessarily incur prohibitive cost to ask CSRs to assign existing or new intents to the mis-classified cases. Apart from the confirmed cases with and without intent labels, there can be a number of cases with no human curation. This data composition (Positives + Unlabeled + multiclass Negatives) creates unique challenges for model development. In response to that, we propose a semi-supervised multi-task learning paradigm. In this manuscript, we share our experience in building text-based intent classification models for a customer support service on an E-commerce website. We improve the performance significantly by evolving the model from multiclass classification to semi-supervised multi-task learning by leveraging the negative cases, domain- and task-adaptively pretrained ALBERT on customer contact texts, and a number of un-curated data with no labels. In the evaluation, the final model boosts the average AUC ROC by almost 20 points compared to the baseline finetuned multiclass classification ALBERT model",
    "checked": true,
    "id": "da1f51bd129809c644d1e067d3183c6948b2d63f",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Li Dong",
      "Matthew C. Spencer",
      "Amir Biagi"
    ]
  },
  "https://aclanthology.org/2021.ecnlp-1.8": {
    "title": "Are you calling for the vaporizer you ordered?\" Combining Search and Prediction to Identify Orders in Contact Centers",
    "volume": "workshop",
    "abstract": "With the growing footprint of ecommerce worldwide, the role of contact center is becoming increasingly crucial for customer satisfaction. To effectively handle scale and manage operational cost, automation through chat-bots and voice-bots are getting rapidly adopted. With customers having multiple, often long list of active orders - the first task of a voice-bot is to identify which one they are calling about. Towards solving this problem which we refer to as order identification, we propose a two-staged real-time technique by combining search and prediction in a sequential manner. In the first stage, analogous to retrieval-based question-answering, a fuzzy search technique uses customized textual similarity measures on noisy transcripts of calls to retrieve the order of interest. The coverage of fuzzy search is limited by no or limited response from customers to voice prompts. Hence, in the second stage, a predictive solution that predict the most likely order a customer is calling about based on certain features of orders is introduced. We compare with multiple relevant techniques based on word embeddings as well as ecommerce product search to show that the proposed approach provides the best performance with 64% coverage and 87% accuracy on a large real-life data-set. A system based on the proposed technique is also deployed in production for a fraction of calls landing in the contact center of a large ecommerce provider; providing real evidence of operational benefits as well as increased customer delight",
    "checked": true,
    "id": "a554c173617459905a1eae1d18d76eebb9e205a2",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Abinaya K",
      "Shourya Roy"
    ]
  },
  "https://aclanthology.org/2021.ecnlp-1.9": {
    "title": "Identifying Hijacked Reviews",
    "volume": "workshop",
    "abstract": "Fake reviews and review manipulation are growing problems on online marketplaces globally. Review Hijacking is a new review manipulation tactic in which unethical sellers “hijack” an existing product page (usually one with many positive reviews), then update the product details like title, photo, and description with those of an entirely different product. With the earlier reviews still attached, the new item appears well-reviewed. So far, little knowledge about hijacked reviews has resulted in little academic research and an absence of labeled data. Hence, this paper proposes a three-part study: (i) we propose a framework to generate synthetically labeled data for review hijacking by swapping products and reviews; (ii) then, we evaluate the potential of both a Siamese LSTM network and BERT sequence pair classifier to distinguish legitimate reviews from hijacked ones using this data; and (iii) we then deploy the best performing model on a collection of 31K products (with 6.5 M reviews) in the original data, where we find 100s of previously unknown examples of review hijacking",
    "checked": true,
    "id": "fc8fd2708e61746ad6a5666b7507176aa3649d8c",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Monika Daryani",
      "James Caverlee"
    ]
  },
  "https://aclanthology.org/2021.ecnlp-1.10": {
    "title": "Learning Cross-Task Attribute - Attribute Similarity for Multi-task Attribute-Value Extraction",
    "volume": "workshop",
    "abstract": "Automatic extraction of product attribute-value pairs from unstructured text like product descriptions is an important problem for e-commerce companies. The attribute schema typically varies from one category of products (which will be referred as vertical) to another. This leads to extreme annotation efforts for training of supervised deep sequence labeling models such as LSTM-CRF, and consequently not enough labeled data for some vertical-attribute pairs. In this work, we propose a technique for alleviating this problem by using annotated data from related verticals in a multi-task learning framework. Our approach relies on availability of similar attributes (labels) in another related vertical. Our model jointly learns the similarity between attributes of the two verticals along with the model parameters for the sequence tagging model. The main advantage of our approach is that it does not need any prior annotation of attribute similarity. Our system has been tested with datasets of size more than 10000 from a large e-commerce company in India. We perform detailed experiments to show that our method indeed increases the macro-F1 scores for attribute value extraction in general, and for labels with low training data in particular. We also report top labels from other verticals that contribute towards learning of particular labels",
    "checked": true,
    "id": "1088832ac7d16a9042d2ad03cb7d8991b9f18ff9",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Mayank Jain",
      "Sourangshu Bhattacharya",
      "Harshit Jain",
      "Karimulla Shaik",
      "Muthusamy Chelliah"
    ]
  },
  "https://aclanthology.org/2021.ecnlp-1.11": {
    "title": "Unsupervised Class-Specific Abstractive Summarization of Customer Reviews",
    "volume": "workshop",
    "abstract": "Large-scale unsupervised abstractive summarization is sorely needed to automatically scan millions of customer reviews in today’s fast-paced e-commerce landscape. We address a key challenge in unsupervised abstractive summarization – reducing generic and uninformative content and producing useful information that relates to specific product aspects. To do so, we propose to model reviews in the context of some topical classes of interest. In particular, for any arbitrary set of topical classes of interest, the proposed model can learn to generate a set of class-specific summaries from multiple reviews of each product without ground-truth summaries, and the only required signal is class probabilities or class label for each review. The model combines a generative variational autoencoder, with an integrated class-correlation gating mechanism and a hierarchical structure capturing dependence among products, reviews and classes. Human evaluation shows that generated summaries are highly relevant, fluent, and representative. Evaluation using a reference dataset shows that our model outperforms state-of-the-art abstractive and extractive baselines",
    "checked": true,
    "id": "b6e62f1c9e50a6624304d48094810a05d00e3175",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Thi Nhat Anh Nguyen",
      "Mingwei Shen",
      "Karen Hovsepian"
    ]
  },
  "https://aclanthology.org/2021.ecnlp-1.12": {
    "title": "Scalable Approach for Normalizing E-commerce Text Attributes (SANTA)",
    "volume": "workshop",
    "abstract": "In this paper, we present SANTA, a scalable framework to automatically normalize E-commerce attribute values (e.g. “Win 10 Pro”) to a fixed set of pre-defined canonical values (e.g. “Windows 10”). Earlier works on attribute normalization focused on fuzzy string matching (also referred as syntactic matching in this paper). In this work, we first perform an extensive study of nine syntactic matching algorithms and establish that ‘cosine’ similarity leads to best results, showing 2.7% improvement over commonly used Jaccard index. Next, we show that string similarity alone is not sufficient for attribute normalization as many surface forms require going beyond syntactic matching (e.g. “720p” and “HD” are synonyms). While semantic techniques like unsupervised embeddings (e.g. word2vec/fastText) have shown good results in word similarity tasks, we observed that they perform poorly to distinguish between close canonical forms, as these close forms often occur in similar contexts. We propose to learn token embeddings using a twin network with triplet loss. We propose an embedding learning task leveraging raw attribute values and product titles to learn these embeddings in a self-supervised fashion. We show that providing supervision using our proposed task improves over both syntactic and unsupervised embeddings based techniques for attribute normalization. Experiments on a real-world dataset of 50 attributes show that the embeddings trained using our proposed approach obtain 2.3% improvement over best string similarity and 19.3% improvement over best unsupervised embeddings",
    "checked": true,
    "id": "678ee0571346b27692bd82889d1d38fc7250ab64",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Ravi Shankar Mishra",
      "Kartik Mehta",
      "Nikhil Rasiwasia"
    ]
  },
  "https://aclanthology.org/2021.ecnlp-1.13": {
    "title": "Multimodal Item Categorization Fully Based on Transformer",
    "volume": "workshop",
    "abstract": "The Transformer has proven to be a powerful feature extraction method and has gained widespread adoption in natural language processing (NLP). In this paper we propose a multimodal item categorization (MIC) system solely based on the Transformer for both text and image processing. On a multimodal product data set collected from a Japanese e-commerce giant, we tested a new image classification model based on the Transformer and investigated different ways of fusing bi-modal information. Our experimental results on real industry data showed that the Transformer-based image classifier has performance on par with ResNet-based classifiers and is four times faster to train. Furthermore, a cross-modal attention layer was found to be critical for the MIC system to achieve performance gains over text-only and image-only models",
    "checked": true,
    "id": "e08c23bd4918040213533a0ab3534a524a87b32b",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Lei Chen",
      "Houwei Chou",
      "Yandi Xia",
      "Hirokazu Miyake"
    ]
  },
  "https://aclanthology.org/2021.ecnlp-1.14": {
    "title": "Textual Representations for Crosslingual Information Retrieval",
    "volume": "workshop",
    "abstract": "In this paper, we explored different levels of textual representations for cross-lingual information retrieval. Beyond the traditional token level representation, we adopted the subword and character level representations for information retrieval that had shown to improve neural machine translation by reducing the out-of-vocabulary issues in machine translation. We found that crosslingual information retrieval performance can be improved by combining search results from subwords and token level representation.Additionally, we improved the search performance by combining and re-ranking the result sets from the different text representations for German, French and Japanese",
    "checked": true,
    "id": "a609db40216a4071f9f739766c6691fa46fb8072",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Hang Zhang",
      "Liling Tan"
    ]
  },
  "https://aclanthology.org/2021.ecnlp-1.15": {
    "title": "Detect Profane Language in Streaming Services to Protect Young Audiences",
    "volume": "workshop",
    "abstract": "With the rapid growth of online video streaming, recent years have seen increasing concerns about profane language in their content. Detecting profane language in streaming services is challenging due to the long sentences appeared in a video. While recent research on handling long sentences has focused on developing deep learning modeling techniques, little work has focused on techniques on improving data pipelines. In this work, we develop a data collection pipeline to address long sequence of texts and integrate this pipeline with a multi-head self-attention model. With this pipeline, our experiments show the self-attention model offers 12.5% relative accuracy improvement over state-of-the-art distilBERT model on profane language detection while requiring only 3% of parameters. This research designs a better system for informing users of profane language in video streaming services",
    "checked": true,
    "id": "f0c7852cf4fd7b8831bb3919074c32180b8cec75",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Jingxiang Chen",
      "Kai Wei",
      "Xiang Hao"
    ]
  },
  "https://aclanthology.org/2021.ecnlp-1.16": {
    "title": "Exploring Inspiration Sets in a Data Programming Pipeline for Product Moderation",
    "volume": "workshop",
    "abstract": "We carry out a case study on the use of data programming to create data to train classifiers used for product moderation on a large e-commerce platform. Data programming is a recently-introduced technique that uses human-defined rules to generate training data sets without tedious item-by-item hand labeling. Our study investigates methods for allowing product moderators to quickly modify the rules given their knowledge of the domain and, especially, of textual item descriptions. Our results show promise that moderators can use this approach to steer the training data, making possible fast and close control of classifiers that detect policy violations",
    "checked": true,
    "id": "c371a3443d9aa8bb3333e9aa149ff96f6e933024",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Justine Winkler",
      "Simon Brugman",
      "Bas van Berkel",
      "Martha Larson"
    ]
  },
  "https://aclanthology.org/2021.ecnlp-1.17": {
    "title": "Enhancing Aspect Extraction for Hindi",
    "volume": "workshop",
    "abstract": "Aspect extraction is not a well-explored topic in Hindi, with only one corpus having been developed for the task. In this paper, we discuss the merits of the existing corpus in terms of quality, size, sparsity, and performance in aspect extraction tasks using established models. To provide a better baseline corpus for aspect extraction, we translate the SemEval 2014 aspect-based sentiment analysis dataset and annotate the aspects in that data. We provide rigorous guidelines and a replicable methodology for this task. We quantitatively evaluate the translations and annotations using inter-annotator agreement scores. We also evaluate our dataset using state-of-the-art neural aspect extraction models in both monolingual and multilingual settings and show that the models perform far better on our corpus than on the existing Hindi dataset. With this, we establish our corpus as the gold-standard aspect extraction dataset in Hindi",
    "checked": true,
    "id": "3b3df20afa189f599c82baf3bc7a1ef4ae78d428",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Arghya Bhattacharya",
      "Alok Debnath",
      "Manish Shrivastava"
    ]
  },
  "https://aclanthology.org/2021.ecnlp-1.18": {
    "title": "Combining semantic search and twin product classification for recognition of purchasable items in voice shopping",
    "volume": "workshop",
    "abstract": "The accuracy of an online shopping system via voice commands is particularly important and may have a great impact on customer trust. This paper focuses on the problem of detecting if an utterance contains actual and purchasable products, thus referring to a shopping-related intent in a typical Spoken Language Understanding architecture consist- ing of an intent classifier and a slot detec- tor. Searching through billions of products to check if a detected slot is a purchasable item is prohibitively expensive. To overcome this problem, we present a framework that (1) uses a retrieval module that returns the most rele- vant products with respect to the detected slot, and (2) combines it with a twin network that decides if the detected slot is indeed a pur- chasable item or not. Through various exper- iments, we show that this architecture outper- forms a typical slot detector approach, with a gain of +81% in accuracy and +41% in F1 score",
    "checked": true,
    "id": "bbc85ba0a8d9ec9c4e9a83b87ffc3b0a6a2c9636",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dieu-Thu Le",
      "Verena Weber",
      "Melanie Bradford"
    ]
  },
  "https://aclanthology.org/2021.ecnlp-1.19": {
    "title": "Improving Factual Consistency of Abstractive Summarization on Customer Feedback",
    "volume": "workshop",
    "abstract": "E-commerce stores collect customer feedback to let sellers learn about customer concerns and enhance customer order experience. Because customer feedback often contains redundant information, a concise summary of the feedback can be generated to help sellers better understand the issues causing customer dissatisfaction. Previous state-of-the-art abstractive text summarization models make two major types of factual errors when producing summaries from customer feedback, which are wrong entity detection (WED) and incorrect product-defect description (IPD). In this work, we introduce a set of methods to enhance the factual consistency of abstractive summarization on customer feedback. We augment the training data with artificially corrupted summaries, and use them as counterparts of the target summaries. We add a contrastive loss term into the training objective so that the model learns to avoid certain factual errors. Evaluation results show that a large portion of WED and IPD errors are alleviated for BART and T5. Furthermore, our approaches do not depend on the structure of the summarization model and thus are generalizable to any abstractive summarization systems",
    "checked": true,
    "id": "95d4ed9bbb42d6065e99d7857085fa76361c52e9",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Yang Liu",
      "Yifei Sun",
      "Vincent Gao"
    ]
  },
  "https://aclanthology.org/2021.ecnlp-1.20": {
    "title": "SupportNet: Neural Networks for Summary Generation and Key Segment Extraction from Technical Support Tickets",
    "volume": "workshop",
    "abstract": "We improve customer experience and gain their trust when their issues are resolved rapidly with less friction. Existing work has focused on reducing the overall case resolution time by binning a case into predefined categories and routing it to the desired support engineer. However, the actions taken by the engineer during case analysis and resolution are altogether ignored, even though it forms the bulk of the case resolution time. In this work, we propose two systems that enable support engineers to resolve cases faster. The first, a guidance extraction model, mines historical cases and provides technical guidance phrases to the support engineers. The phrases can then be used to educate the customer or to obtain critical information needed to resolve the case and thus minimize the number of correspondences between the engineer and customer. The second, a summarization model, creates an abstractive summary of the case to provide better context to the support engineer. Through quantitative evaluation we obtain an F1 score of 0.64 on the guidance extraction model and a BertScore (F1) of 0.55 on the summarization model",
    "checked": true,
    "id": "092e239f8270f54fde62c1fd0689adfa876dcd15",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Vinayshekhar Bannihatti Kumar",
      "Mohan Yarramsetty",
      "Sharon Sun",
      "Anukul Goel"
    ]
  },
  "https://aclanthology.org/2021.ecnlp-1.21": {
    "title": "Product Review Translation: Parallel Corpus Creation and Robustness towards User-generated Noisy Text",
    "volume": "workshop",
    "abstract": "Reviews written by the users for a particular product or service play an influencing role for the customers to make an informative decision. Although online e-commerce portals have immensely impacted our lives, available contents predominantly are in English language- often limiting its widespread usage. There is an exponential growth in the number of e-commerce users who are not proficient in English. Hence, there is a necessity to make these services available in non-English languages, especially in a multilingual country like India. This can be achieved by an in-domain robust machine translation (MT) system. However, the reviews written by the users pose unique challenges to MT, such as misspelled words, ungrammatical constructions, presence of colloquial terms, lack of resources such as in-domain parallel corpus etc. We address the above challenges by presenting an English–Hindi review domain parallel corpus. We train an English–to–Hindi neural machine translation (NMT) system to translate the product reviews available on e-commerce websites. By training the Transformer based NMT model over the generated data, we achieve a score of 33.26 BLEU points for English–to–Hindi translation. In order to make our NMT model robust enough to handle the noisy tokens in the reviews, we integrate a character based language model to generate word vectors and map the noisy tokens with their correct forms. Experiments on four language pairs, viz. English-Hindi, English-German, English-French, and English-Czech show the BLUE scores of 35.09, 28.91, 34.68 and 14.52 which are the improvements of 1.61, 1.05, 1.63 and 1.94, respectively, over the baseline",
    "checked": true,
    "id": "8e9e4bd50a3d98975d1d365b18808c960347a614",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Kamal Kumar Gupta",
      "Soumya Chennabasavaraj",
      "Nikesh Garera",
      "Asif Ekbal"
    ]
  },
  "https://aclanthology.org/2021.gebnlp-1.1": {
    "title": "gENder-IT: An Annotated English-Italian Parallel Challenge Set for Cross-Linguistic Natural Gender Phenomena",
    "volume": "workshop",
    "abstract": "Languages differ in terms of the absence or presence of gender features, the number of gender classes and whether and where gender features are explicitly marked. These cross-linguistic differences can lead to ambiguities that are difficult to resolve, especially for sentence-level MT systems. The identification of ambiguity and its subsequent resolution is a challenging task for which currently there aren’t any specific resources or challenge sets available. In this paper, we introduce gENder-IT, an English–Italian challenge set focusing on the resolution of natural gender phenomena by providing word-level gender tags on the English source side and multiple gender alternative translations, where needed, on the Italian target side",
    "checked": true,
    "id": "581c294947f9e0d352e4016f754fcd4c50b7a21d",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Eva Vanmassenhove",
      "Johanna Monti"
    ]
  },
  "https://aclanthology.org/2021.gebnlp-1.2": {
    "title": "Gender Bias Hidden Behind Chinese Word Embeddings: The Case of Chinese Adjectives",
    "volume": "workshop",
    "abstract": "Gender bias in word embeddings gradually becomes a vivid research field in recent years. Most studies in this field aim at measurement and debiasing methods with English as the target language. This paper investigates gender bias in static word embeddings from a unique perspective, Chinese adjectives. By training word representations with different models, the gender bias behind the vectors of adjectives is assessed. Through a comparison between the produced results and a human scored data set, we demonstrate how gender bias encoded in word embeddings differentiates from people’s attitudes",
    "checked": true,
    "id": "41a53afaf6d616438ba866f8df28130d13d6881f",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Meichun Jiao",
      "Ziyang Luo"
    ]
  },
  "https://aclanthology.org/2021.gebnlp-1.3": {
    "title": "Evaluating Gender Bias in Hindi-English Machine Translation",
    "volume": "workshop",
    "abstract": "With language models being deployed increasingly in the real world, it is essential to address the issue of the fairness of their outputs. The word embedding representations of these language models often implicitly draw unwanted associations that form a social bias within the model. The nature of gendered languages like Hindi, poses an additional problem to the quantification and mitigation of bias, owing to the change in the form of the words in the sentence, based on the gender of the subject. Additionally, there is sparse work done in the realm of measuring and debiasing systems for Indic languages. In our work, we attempt to evaluate and quantify the gender bias within a Hindi-English machine translation system. We implement a modified version of the existing TGBI metric based on the grammatical considerations for Hindi. We also compare and contrast the resulting bias measurements across multiple metrics for pre-trained embeddings and the ones learned by our machine translation model",
    "checked": true,
    "id": "2ff522a22d744938bf5150a022904166d4dd45f8",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Krithika Ramesh",
      "Gauri Gupta",
      "Sanjay Singh"
    ]
  },
  "https://aclanthology.org/2021.gebnlp-1.4": {
    "title": "Alexa, Google, Siri: What are Your Pronouns? Gender and Anthropomorphism in the Design and Perception of Conversational Assistants",
    "volume": "workshop",
    "abstract": "Technology companies have produced varied responses to concerns about the effects of the design of their conversational AI systems. Some have claimed that their voice assistants are in fact not gendered or human-like—despite design features suggesting the contrary. We compare these claims to user perceptions by analysing the pronouns they use when referring to AI assistants. We also examine systems’ responses and the extent to which they generate output which is gendered and anthropomorphic. We find that, while some companies appear to be addressing the ethical concerns raised, in some cases, their claims do not seem to hold true. In particular, our results show that system outputs are ambiguous as to the humanness of the systems, and that users tend to personify and gender them as a result",
    "checked": true,
    "id": "ce18007d195d75a30710e510eb1b4e19ebf31c97",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Gavin Abercrombie",
      "Amanda Cercas Curry",
      "Mugdha Pandya",
      "Verena Rieser"
    ]
  },
  "https://aclanthology.org/2021.gebnlp-1.5": {
    "title": "Gender Bias in Text: Origin, Taxonomy, and Implications",
    "volume": "workshop",
    "abstract": "Gender inequality represents a considerable loss of human potential and perpetuates a culture of violence, higher gender wage gaps, and a lack of representation of women in higher and leadership positions. Applications powered by Artificial Intelligence (AI) are increasingly being used in the real world to provide critical decisions about who is going to be hired, granted a loan, admitted to college, etc. However, the main pillars of AI, Natural Language Processing (NLP) and Machine Learning (ML) have been shown to reflect and even amplify gender biases and stereotypes, which are mainly inherited from historical training data. In an effort to facilitate the identification and mitigation of gender bias in English text, we develop a comprehensive taxonomy that relies on the following gender bias types: Generic Pronouns, Sexism, Occupational Bias, Exclusionary Bias, and Semantics. We also provide a bottom-up overview of gender bias, from its societal origin to its spillover onto language. Finally, we link the societal implications of gender bias to their corresponding type(s) in the proposed taxonomy. The underlying motivation of our work is to help enable the technical community to identify and mitigate relevant biases from training corpora for improved fairness in NLP systems",
    "checked": true,
    "id": "058e1d5faa5499c35a5e651dead828785bccfd03",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Jad Doughman",
      "Wael Khreich",
      "Maya El Gharib",
      "Maha Wiss",
      "Zahraa Berjawi"
    ]
  },
  "https://aclanthology.org/2021.gebnlp-1.6": {
    "title": "Sexism in the Judiciary: The Importance of Bias Definition in NLP and In Our Courts",
    "volume": "workshop",
    "abstract": "We analyze 6.7 million case law documents to determine the presence of gender bias within our judicial system. We find that current bias detection methods in NLP are insufficient to determine gender bias in our case law database and propose an alternative approach. We show that existing algorithms’ inconsistent results are consequences of prior research’s inconsistent definitions of biases themselves. Bias detection algorithms rely on groups of words to represent bias (e.g., ‘salary,’ ‘job,’ and ‘boss’ to represent employment as a potentially biased theme against women in text). However, the methods to build these groups of words have several weaknesses, primarily that the word lists are based on the researchers’ own intuitions. We suggest two new methods of automating the creation of word lists to represent biases. We find that our methods outperform current NLP bias detection methods. Our research improves the capabilities of NLP technology to detect bias and highlights gender biases present in influential case law. In order to test our NLP bias detection method’s performance, we regress our results of bias in case law against U.S census data of women’s participation in the workforce in the last 100 years",
    "checked": true,
    "id": "7e069b99a73efac0b063d068528de10007998fd4",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Noa Baker Gillis"
    ]
  },
  "https://aclanthology.org/2021.gebnlp-1.7": {
    "title": "Towards Equal Gender Representation in the Annotations of Toxic Language Detection",
    "volume": "workshop",
    "abstract": "Classifiers tend to propagate biases present in the data on which they are trained. Hence, it is important to understand how the demographic identities of the annotators of comments affect the fairness of the resulting model. In this paper, we focus on the differences in the ways men and women annotate comments for toxicity, investigating how these differences result in models that amplify the opinions of male annotators. We find that the BERT model associates toxic comments containing offensive words with male annotators, causing the model to predict 67.7% of toxic comments as having been annotated by men. We show that this disparity between gender predictions can be mitigated by removing offensive words and highly toxic comments from the training data. We then apply the learned associations between gender and language to toxic language classifiers, finding that models trained exclusively on female-annotated data perform 1.8% better than those trained solely on male-annotated data, and that training models on data after removing all offensive words reduces bias in the model by 55.5% while increasing the sensitivity by 0.4%",
    "checked": true,
    "id": "4c9c5411037637dab5c4aab4a2f5cdc276218268",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Elizabeth Excell",
      "Noura Al Moubayed"
    ]
  },
  "https://aclanthology.org/2021.gebnlp-1.8": {
    "title": "Using Gender- and Polarity-Informed Models to Investigate Bias",
    "volume": "workshop",
    "abstract": "In this work we explore the effect of incorporating demographic metadata in a text classifier trained on top of a pre-trained transformer language model. More specifically, we add information about the gender of critics and book authors when classifying the polarity of book reviews, and the polarity of the reviews when classifying the genders of authors and critics. We use an existing data set of Norwegian book reviews with ratings by professional critics, which has also been augmented with gender information, and train a document-level sentiment classifier on top of a recently released Norwegian BERT-model. We show that gender-informed models obtain substantially higher accuracy, and that polarity-informed models obtain higher accuracy when classifying the genders of book authors. For this particular data set, we take this result as a confirmation of the gender bias in the underlying label distribution, but in other settings we believe a similar approach can be used for mitigating bias in the model",
    "checked": true,
    "id": "ea8b8a4077febdaffae02a3ab18001b4ee682f3a",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Samia Touileb",
      "Lilja Øvrelid",
      "Erik Velldal"
    ]
  },
  "https://aclanthology.org/2021.gebnlp-1.9": {
    "title": "Assessing Gender Bias in Wikipedia: Inequalities in Article Titles",
    "volume": "workshop",
    "abstract": "Potential gender biases existing in Wikipedia’s content can contribute to biased behaviors in a variety of downstream NLP systems. Yet, efforts in understanding what inequalities in portraying women and men occur in Wikipedia focused so far only on *biographies*, leaving open the question of how often such harmful patterns occur in other topics. In this paper, we investigate gender-related asymmetries in Wikipedia titles from *all domains*. We assess that for only half of gender-related articles, i.e., articles with words such as *women* or *male* in their titles, symmetrical counterparts describing the same concept for the other gender (and clearly stating it in their titles) exist. Among the remaining imbalanced cases, the vast majority of articles concern sports- and social-related issues. We provide insights on how such asymmetries can influence other Wikipedia components and propose steps towards reducing the frequency of observed patterns",
    "checked": true,
    "id": "02e6d95e8a8c936a834e20fff92f2681b4605905",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Agnieszka Falenska",
      "Özlem Çetinoğlu"
    ]
  },
  "https://aclanthology.org/2021.gebnlp-1.10": {
    "title": "Investigating the Impact of Gender Representation in ASR Training Data: a Case Study on Librispeech",
    "volume": "workshop",
    "abstract": "In this paper we question the impact of gender representation in training data on the performance of an end-to-end ASR system. We create an experiment based on the Librispeech corpus and build 3 different training corpora varying only the proportion of data produced by each gender category. We observe that if our system is overall robust to the gender balance or imbalance in training data, it is nonetheless dependant of the adequacy between the individuals present in the training and testing sets",
    "checked": true,
    "id": "01a5759e868ba563e254a996fb22dd23829cd282",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Mahault Garnerin",
      "Solange Rossato",
      "Laurent Besacier"
    ]
  },
  "https://aclanthology.org/2021.gebnlp-1.11": {
    "title": "Generating Gender Augmented Data for NLP",
    "volume": "workshop",
    "abstract": "Gender bias is a frequent occurrence in NLP-based applications, especially pronounced in gender-inflected languages. Bias can appear through associations of certain adjectives and animate nouns with the natural gender of referents, but also due to unbalanced grammatical gender frequencies of inflected words. This type of bias becomes more evident in generating conversational utterances where gender is not specified within the sentence, because most current NLP applications still work on a sentence-level context. As a step towards more inclusive NLP, this paper proposes an automatic and generalisable re-writing approach for short conversational sentences. The rewriting method can be applied to sentences that, without extra-sentential context, have multiple equivalent alternatives in terms of gender. The method can be applied both for creating gender balanced outputs as well as for creating gender balanced training data. The proposed approach is based on a neural machine translation system trained to ‘translate’ from one gender alternative to another. Both the automatic and manual analysis of the approach show promising results with respect to the automatic generation of gender alternatives for conversational sentences in Spanish",
    "checked": true,
    "id": "2dfd336a2beb6af1f7facb5ad9ad0444467ae5fe",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Nishtha Jain",
      "Maja Popović",
      "Declan Groves",
      "Eva Vanmassenhove"
    ]
  },
  "https://aclanthology.org/2021.gebnlp-1.12": {
    "title": "Second Order WinoBias (SoWinoBias) Test Set for Latent Gender Bias Detection in Coreference Resolution",
    "volume": "workshop",
    "abstract": "We observe an instance of gender-induced bias in a downstream application, despite the absence of explicit gender words in the test cases. We provide a test set, SoWinoBias, for the purpose of measuring such latent gender bias in coreference resolution systems. We evaluate the performance of current debiasing methods on the SoWinoBias test set, especially in reference to the method’s design and altered embedding space properties. See https://github.com/hillary-dawkins/SoWinoBias",
    "checked": true,
    "id": "7328ee1811b8a5fca1df70f84e1f82deb0def285",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hillary Dawkins"
    ]
  },
  "https://aclanthology.org/2021.gem-1.1": {
    "title": "Flesch-Kincaid is Not a Text Simplification Evaluation Metric",
    "volume": "workshop",
    "abstract": "Sentence-level text simplification is currently evaluated using both automated metrics and human evaluation. For automatic evaluation, a combination of metrics is usually employed to evaluate different aspects of the simplification. Flesch-Kincaid Grade Level (FKGL) is one metric that has been regularly used to measure the readability of system output. In this paper, we argue that FKGL should not be used to evaluate text simplification systems. We provide experimental analyses on recent system output showing that the FKGL score can easily be manipulated to improve the score dramatically with only minor impact on other automated metrics (BLEU and SARI). Instead of using FKGL, we suggest that the component statistics, along with others, be used for posthoc analysis to understand system behavior",
    "checked": true,
    "id": "ffc45648378d5816e46ca339d27d76a483d49aed",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Teerapaun Tanprasert",
      "David Kauchak"
    ]
  },
  "https://aclanthology.org/2021.gem-1.2": {
    "title": "Human Perception in Natural Language Generation",
    "volume": "workshop",
    "abstract": "We ask subjects whether they perceive as human-produced a bunch of texts, some of which are actually human-written, while others are automatically generated. We use this data to fine-tune a GPT-2 model to push it to generate more human-like texts, and observe that this fine-tuned model produces texts that are indeed perceived more human-like than the original model. Contextually, we show that our automatic evaluation strategy well correlates with human judgements. We also run a linguistic analysis to unveil the characteristics of human- vs machine-perceived language",
    "checked": true,
    "id": "0f6ba1ee40b4867269b82135a293bc630b3b20ec",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Lorenzo De Mattei",
      "Huiyuan Lai",
      "Felice Dell’Orletta",
      "Malvina Nissim"
    ]
  },
  "https://aclanthology.org/2021.gem-1.3": {
    "title": "Semantic Similarity Based Evaluation for Abstractive News Summarization",
    "volume": "workshop",
    "abstract": "ROUGE is a widely used evaluation metric in text summarization. However, it is not suitable for the evaluation of abstractive summarization systems as it relies on lexical overlap between the gold standard and the generated summaries. This limitation becomes more apparent for agglutinative languages with very large vocabularies and high type/token ratios. In this paper, we present semantic similarity models for Turkish and apply them as evaluation metrics for an abstractive summarization task. To achieve this, we translated the English STSb dataset into Turkish and presented the first semantic textual similarity dataset for Turkish as well. We showed that our best similarity models have better alignment with average human judgments compared to ROUGE in both Pearson and Spearman correlations",
    "checked": true,
    "id": "7b3f9d6343aeb5659b2d6c39e8a4004b00b0695e",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Figen Beken Fikri",
      "Kemal Oflazer",
      "Berrin Yanikoglu"
    ]
  },
  "https://aclanthology.org/2021.gem-1.4": {
    "title": "Shades of BLEU, Flavours of Success: The Case of MultiWOZ",
    "volume": "workshop",
    "abstract": "The MultiWOZ dataset (Budzianowski et al.,2018) is frequently used for benchmarkingcontext-to-response abilities of task-orienteddialogue systems. In this work, we identifyinconsistencies in data preprocessing and re-porting of three corpus-based metrics used onthis dataset, i.e., BLEU score and Inform &Success rates. We point out a few problemsof the MultiWOZ benchmark such as unsat-isfactory preprocessing, insufficient or under-specified evaluation metrics, or rigid database.We re-evaluate 7 end-to-end and 6 policy opti-mization models in as-fair-as-possible setups,and we show that their reported scores cannotbe directly compared. To facilitate compari-son of future systems, we release our stand-alone standardized evaluation scripts. We alsogive basic recommendations for corpus-basedbenchmarking in future works",
    "checked": true,
    "id": "d586ece0db87c7ae8dfcb87d18f6a26d0695ec09",
    "semantic_title": "",
    "citation_count": 26,
    "authors": [
      "Tomáš Nekvinda",
      "Ondřej Dušek"
    ]
  },
  "https://aclanthology.org/2021.gem-1.5": {
    "title": "Personalized Response Generation with Tensor Factorization",
    "volume": "workshop",
    "abstract": "Personalized response generation is essential for more human-like conversations. However, how to model user personalization information with no explicit user persona descriptions or demographics still remains under-investigated. To tackle the data sparsity problem and the huge number of users, we utilize tensor factorization to model users’ personalization information with their posting histories. Specifically, we introduce the personalized response embedding for all question-user pairs and form them into a three-mode tensor, decomposed by Tucker decomposition. The personalized response embedding is fed to either the decoder of an LSTM-based Seq2Seq model or a transformer language model to help generate more personalized responses. To evaluate how personalized the generated responses are, we further propose a novel ranking-based metric called Per-Hits@k which measures how likely are the generated responses come from the corresponding users. Results on a large-scale conversation dataset show that our proposed tensor factorization based models generate more personalized and higher quality responses compared to baselines",
    "checked": true,
    "id": "3119576d650785955efab54cb57f9f66438ca862",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenghui Wang",
      "Lingxiao Luo",
      "Diyi Yang"
    ]
  },
  "https://aclanthology.org/2021.gem-1.6": {
    "title": "A Review of Human Evaluation for Style Transfer",
    "volume": "workshop",
    "abstract": "This paper reviews and summarizes human evaluation practices described in 97 style transfer papers with respect to three main evaluation aspects: style transfer, meaning preservation, and fluency. In principle, evaluations by human raters should be the most reliable. However, in style transfer papers, we find that protocols for human evaluations are often underspecified and not standardized, which hampers the reproducibility of research in this field and progress toward better human and automatic evaluation methods",
    "checked": true,
    "id": "0cbbc608256636bd344c940e90f9a4b004ab27d5",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Eleftheria Briakou",
      "Sweta Agrawal",
      "Ke Zhang",
      "Joel Tetreault",
      "Marine Carpuat"
    ]
  },
  "https://aclanthology.org/2021.gem-1.7": {
    "title": "GOT: Testing for Originality in Natural Language Generation",
    "volume": "workshop",
    "abstract": "We propose an approach to automatically test for originality in generation tasks where no standard automatic measures exist. Our proposal addresses original uses of language, not necessarily original ideas. We provide an algorithm for our approach and a run-time analysis. The algorithm, which finds all of the original fragments in a ground-truth corpus and can reveal whether a generated fragment copies an original without attribution, has a run-time complexity of theta(nlogn) where n is the number of sentences in the ground truth",
    "checked": true,
    "id": "d5512e5ad1df77d81fd9de1193857850a8f4d142",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Jennifer Brooks",
      "Abdou Youssef"
    ]
  },
  "https://aclanthology.org/2021.gem-1.8": {
    "title": "Evaluating Text Generation from Discourse Representation Structures",
    "volume": "workshop",
    "abstract": "We present an end-to-end neural approach to generate English sentences from formal meaning representations, Discourse Representation Structures (DRSs). We use a rather standard bi-LSTM sequence-to-sequence model, work with a linearized DRS input representation, and evaluate character-level and word-level decoders. We obtain very encouraging results in terms of reference-based automatic metrics such as BLEU. But because such metrics only evaluate the surface level of generated output, we develop a new metric, ROSE, that targets specific semantic phenomena. We do this with five DRS generation challenge sets focusing on tense, grammatical number, polarity, named entities and quantities. The aim of these challenge sets is to assess the neural generator’s systematicity and generalization to unseen inputs",
    "checked": true,
    "id": "439246df6d82bf95d2006bc027cdc1ef3c604f74",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Chunliu Wang",
      "Rik van Noord",
      "Arianna Bisazza",
      "Johan Bos"
    ]
  },
  "https://aclanthology.org/2021.gem-1.9": {
    "title": "Human Evaluation of Creative NLG Systems: An Interdisciplinary Survey on Recent Papers",
    "volume": "workshop",
    "abstract": "We survey human evaluation in papers presenting work on creative natural language generation that have been published in INLG 2020 and ICCC 2020. The most typical human evaluation method is a scaled survey, typically on a 5 point scale, while many other less common methods exist. The most commonly evaluated parameters are meaning, syntactic correctness, novelty, relevance and emotional value, among many others. Our guidelines for future evaluation include clearly defining the goal of the generative system, asking questions as concrete as possible, testing the evaluation setup, using multiple different evaluation setups, reporting the entire evaluation process and potential biases clearly, and finally analyzing the evaluation results in a more profound way than merely reporting the most typical statistics",
    "checked": true,
    "id": "dfb4c9396e99a4eb7abfe3798e83ec56b4ccdb9c",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Mika Hämäläinen",
      "Khalid Alnajjar"
    ]
  },
  "https://aclanthology.org/2021.gem-1.10": {
    "title": "The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics",
    "volume": "workshop",
    "abstract": "We introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, datasets, and human evaluation standards. Due to this moving target, new models often still evaluate on divergent anglo-centric corpora with well-established, but flawed, metrics. This disconnect makes it challenging to identify the limitations of current models and opportunities for progress. Addressing this limitation, GEM provides an environment in which models can easily be applied to a wide set of tasks and in which evaluation strategies can be tested. Regular updates to the benchmark will help NLG research become more multilingual and evolve the challenge alongside models. This paper serves as the description of the data for the 2021 shared task at the associated GEM Workshop",
    "checked": true,
    "id": "824cd8db8a68732db04f4d8b7139eb4475e59ff2",
    "semantic_title": "",
    "citation_count": 170,
    "authors": [
      "Sebastian Gehrmann",
      "Tosin Adewumi",
      "Karmanya Aggarwal",
      "Pawan Sasanka Ammanamanchi",
      "Anuoluwapo Aremu",
      "Antoine Bosselut",
      "Khyathi Raghavi Chandu",
      "Miruna-Adriana Clinciu",
      "Dipanjan Das",
      "Kaustubh Dhole",
      "Wanyu Du",
      "Esin Durmus",
      "Ondřej Dušek",
      "Chris Chinenye Emezue",
      "Varun Gangal",
      "Cristina Garbacea",
      "Tatsunori Hashimoto",
      "Yufang Hou",
      "Yacine Jernite",
      "Harsh Jhamtani",
      "Yangfeng Ji",
      "Shailza Jolly",
      "Mihir Kale",
      "Dhruv Kumar",
      "Faisal Ladhak",
      "Aman Madaan",
      "Mounica Maddela",
      "Khyati Mahajan",
      "Saad Mahamood",
      "Bodhisattwa Prasad Majumder",
      "Pedro Henrique Martins",
      "Angelina McMillan-Major",
      "Simon Mille",
      "Emiel van Miltenburg",
      "Moin Nadeem",
      "Shashi Narayan",
      "Vitaly Nikolaev",
      "Andre Niyongabo Rubungo",
      "Salomey Osei",
      "Ankur Parikh",
      "Laura Perez-Beltrachini",
      "Niranjan Ramesh Rao",
      "Vikas Raunak",
      "Juan Diego Rodriguez",
      "Sashank Santhanam",
      "João Sedoc",
      "Thibault Sellam",
      "Samira Shaikh",
      "Anastasia Shimorina",
      "Marco Antonio Sobrevilla Cabezudo",
      "Hendrik Strobelt",
      "Nishant Subramani",
      "Wei Xu",
      "Diyi Yang",
      "Akhila Yerukola",
      "Jiawei Zhou"
    ]
  },
  "https://aclanthology.org/2021.gem-1.11": {
    "title": "Reusable Templates and Guides For Documenting Datasets and Models for Natural Language Processing and Generation: A Case Study of the HuggingFace and GEM Data and Model Cards",
    "volume": "workshop",
    "abstract": "Developing documentation guidelines and easy-to-use templates for datasets and models is a challenging task, especially given the variety of backgrounds, skills, and incentives of the people involved in the building of natural language processing (NLP) tools. Nevertheless, the adoption of standard documentation practices across the field of NLP promotes more accessible and detailed descriptions of NLP datasets and models, while supporting researchers and developers in reflecting on their work. To help with the standardization of documentation, we present two case studies of efforts that aim to develop reusable documentation templates – the HuggingFace data card, a general purpose card for datasets in NLP, and the GEM benchmark data and model cards with a focus on natural language generation. We describe our process for developing these templates, including the identification of relevant stakeholder groups, the definition of a set of guiding principles, the use of existing templates as our foundation, and iterative revisions based on feedback",
    "checked": true,
    "id": "cb0de2de79533d4faada3d745f43702eb89d1a60",
    "semantic_title": "",
    "citation_count": 20,
    "authors": [
      "Angelina McMillan-Major",
      "Salomey Osei",
      "Juan Diego Rodriguez",
      "Pawan Sasanka Ammanamanchi",
      "Sebastian Gehrmann",
      "Yacine Jernite"
    ]
  },
  "https://aclanthology.org/2021.gem-1.12": {
    "title": "Structure-to-Text Generation with Self-Training, Acceptability Classifiers and Context-Conditioning for the GEM Shared Task",
    "volume": "workshop",
    "abstract": "We explore the use of self-training and acceptability classifiers with pre-trained models for natural language generation in structure-to-text settings using three GEM datasets (E2E, WebNLG-en, Schema-Guided Dialog). With the Schema-Guided Dialog dataset, we also experiment with including multiple turns of context in the input. We find that self-training with reconstruction matching along with acceptability classifier filtering can improve semantic correctness, though gains are limited in the full-data setting. With context-conditioning, we find that including multiple turns in the context encourages the model to align with the user’s word and phrasing choices as well as to generate more self-consistent responses. In future versions of the GEM challenge, we encourage the inclusion of few-shot tracks to encourage research on data efficiency",
    "checked": true,
    "id": "f83618f13fce0e71e9127784f6ecc261dbdbf089",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Shreyan Bakshi",
      "Soumya Batra",
      "Peyman Heidari",
      "Ankit Arun",
      "Shashank Jain",
      "Michael White"
    ]
  },
  "https://aclanthology.org/2021.gem-1.13": {
    "title": "NUIG-DSI's submission to The GEM Benchmark 2021",
    "volume": "workshop",
    "abstract": "This paper describes the submission by NUIG-DSI to the GEM benchmark 2021. We participate in the modeling shared task where we submit outputs on four datasets for data-to-text generation, namely, DART, WebNLG (en), E2E and CommonGen. We follow an approach similar to the one described in the GEM benchmark paper where we use the pre-trained T5-base model for our submission. We train this model on additional monolingual data where we experiment with different masking strategies specifically focused on masking entities, predicates and concepts as well as a random masking strategy for pre-training. In our results we find that random masking performs the best in terms of automatic evaluation metrics, though the results are not statistically significantly different compared to other masking strategies",
    "checked": true,
    "id": "c889463d4d8a11d132c33c52447291e5385d85ab",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nivranshu Pasricha",
      "Mihael Arcan",
      "Paul Buitelaar"
    ]
  },
  "https://aclanthology.org/2021.gem-1.14": {
    "title": "SimpleNER Sentence Simplification System for GEM 2021",
    "volume": "workshop",
    "abstract": "This paper describes SimpleNER, a model developed for the sentence simplification task at GEM-2021. Our system is a monolingual Seq2Seq Transformer architecture that uses control tokens pre-pended to the data, allowing the model to shape the generated simplifications according to user desired attributes. Additionally, we show that NER-tagging the training data before use helps stabilize the effect of the control tokens and significantly improves the overall performance of the system. We also employ pretrained embeddings to reduce data sparsity and allow the model to produce more generalizable outputs",
    "checked": true,
    "id": "475ae14bee20d9db8abda962e2856f5a31da6726",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "K V Aditya Srivatsa",
      "Monil Gokani",
      "Manish Shrivastava"
    ]
  },
  "https://aclanthology.org/2021.gem-1.15": {
    "title": "System Description for the CommonGen task with the POINTER model",
    "volume": "workshop",
    "abstract": "In a current experiment we were testing CommonGen dataset for structure-to-text task from GEM living benchmark with the constraint based POINTER model. POINTER represents a hybrid architecture, combining insertion-based and transformer paradigms, predicting the token and the insertion position at the same time. The text is therefore generated gradually in a parallel non-autoregressive manner, given the set of keywords. The pretrained model was fine-tuned on a training split of the CommonGen dataset and the generation result was compared to the validation and challenge splits. The received metrics outputs, which measure lexical equivalence, semantic similarity and diversity, are discussed in details in a present system description",
    "checked": true,
    "id": "3df732b977c2025e98d1e323059c75cf53a62b20",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Anna Shvets"
    ]
  },
  "https://aclanthology.org/2021.gem-1.16": {
    "title": "Decoding Methods for Neural Narrative Generation",
    "volume": "workshop",
    "abstract": "Narrative generation is an open-ended NLP task in which a model generates a story given a prompt. The task is similar to neural response generation for chatbots; however, innovations in response generation are often not applied to narrative generation, despite the similarity between these tasks. We aim to bridge this gap by applying and evaluating advances in decoding methods for neural response generation to neural narrative generation. In particular, we employ GPT-2 and perform ablations across nucleus sampling thresholds and diverse decoding hyperparameters—specifically, maximum mutual information—analyzing results over multiple criteria with automatic and human evaluation. We find that (1) nucleus sampling is generally best with thresholds between 0.7 and 0.9; (2) a maximum mutual information objective can improve the quality of generated stories; and (3) established automatic metrics do not correlate well with human judgments of narrative quality on any qualitative metric",
    "checked": true,
    "id": "3c23a892605e55f260f647234eb6b5108c84ab84",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Alexandra DeLucia",
      "Aaron Mueller",
      "Xiang Lisa Li",
      "João Sedoc"
    ]
  },
  "https://aclanthology.org/2021.internlp-1.1": {
    "title": "HILDIF: Interactive Debugging of NLI Models Using Influence Functions",
    "volume": "workshop",
    "abstract": "Biases and artifacts in training data can cause unwelcome behavior in text classifiers (such as shallow pattern matching), leading to lack of generalizability. One solution to this problem is to include users in the loop and leverage their feedback to improve models. We propose a novel explanatory debugging pipeline called HILDIF, enabling humans to improve deep text classifiers using influence functions as an explanation method. We experiment on the Natural Language Inference (NLI) task, showing that HILDIF can effectively alleviate artifact problems in fine-tuned BERT models and result in increased model generalizability",
    "checked": true,
    "id": "d84183f96462f2cf3023f7a6121b3f7ae274d208",
    "semantic_title": "",
    "citation_count": 15,
    "authors": [
      "Hugo Zylberajch",
      "Piyawat Lertvittayakumjorn",
      "Francesca Toni"
    ]
  },
  "https://aclanthology.org/2021.internlp-1.2": {
    "title": "Apple Core-dination: Linguistic Feedback and Learning in a Speech-to-Action Shared World Game",
    "volume": "workshop",
    "abstract": "We investigate the question of how adaptive feedback from a virtual agent impacts the linguistic input of the user in a shared world game environment. To do so, we carry out an exploratory pilot study to observe how individualized linguistic feedback affects the user’s speech input. We introduce a speech-controlled game, Apple Core-dination, in which an agent learns complex tasks using a base knowledge of simple actions. The agent is equipped with a learning mechanism for mapping new commands to sequences of simple actions, as well as the ability to incorporate user input into written responses. The agent repeatedly shares its internal knowledge state by responding to what it knows and does not know about language meaning and the shared environment. Our paper focuses on the linguistic feedback loop in order to analyze the nature of user input. Feedback from the agent is provided in the form of visual movement and written linguistic responses. Particular attention is given to incorporating user input into agent responses and updating the speech-to-action mappings based on commands provided by the user. Through our pilot study, we analyze task success and compare the lexical features of user input. Results show variation in input length and lexical variety across users, suggesting a correlation between the two that can be studied further",
    "checked": true,
    "id": "58e58b633a23df6f6c832f31f2b709c84caf8473",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Susann Boy",
      "AriaRay Brown",
      "Morgan Wixted"
    ]
  },
  "https://aclanthology.org/2021.internlp-1.3": {
    "title": "SHAPELURN: An Interactive Language Learning Game with Logical Inference",
    "volume": "workshop",
    "abstract": "We investigate if a model can learn natural language with minimal linguistic input through interaction. Addressing this question, we design and implement an interactive language learning game that learns logical semantic representations compositionally. Our game allows us to explore the benefits of logical inference for natural language learning. Evaluation shows that the model can accurately narrow down potential logical representations for words over the course of the game, suggesting that our model is able to learn lexical mappings from scratch successfully",
    "checked": true,
    "id": "67f3d4addcfb9066ec436934f8d48ac58fa2b479",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Katharina Stein",
      "Leonie Harter",
      "Luisa Geiger"
    ]
  },
  "https://aclanthology.org/2021.internlp-1.4": {
    "title": "A Proposal: Interactively Learning to Summarise Timelines by Reinforcement Learning",
    "volume": "workshop",
    "abstract": "Timeline Summarisation (TLS) aims to generate a concise, time-ordered list of events described in sources such as news articles. However, current systems do not provide an adequate way to adapt to new domains nor to focus on the aspects of interest to a particular user. Therefore, we propose a method for interactively learning abstractive TLS using Reinforcement Learning (RL). We define a compound reward function and use RL to fine-tune an abstractive Multi-document Summarisation (MDS) model, which avoids the need to train using reference summaries. One of the sub-reward functions will be learned interactively from user feedback to ensure the consistency between users’ demands and the generated timeline. The other sub-reward functions contribute to topical coherence and linguistic fluency. We plan experiments to evaluate whether our approach could generate accurate and precise timelines tailored for each user",
    "checked": true,
    "id": "47f6c7a8219a61f87e4fcb3e8ecd0a468168b22b",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Yuxuan Ye",
      "Edwin Simpson"
    ]
  },
  "https://aclanthology.org/2021.internlp-1.5": {
    "title": "Dynamic Facet Selection by Maximizing Graded Relevance",
    "volume": "workshop",
    "abstract": "Dynamic faceted search (DFS), an interactive query refinement technique, is a form of Human–computer information retrieval (HCIR) approach. It allows users to narrow down search results through facets, where the facets-documents mapping is determined at runtime based on the context of user query instead of pre-indexing the facets statically. In this paper, we propose a new unsupervised approach for dynamic facet generation, namely optimistic facets, which attempts to generate the best possible subset of facets, hence maximizing expected Discounted Cumulative Gain (DCG), a measure of ranking quality that uses a graded relevance scale. We also release code to generate a new evaluation dataset. Through empirical results on two datasets, we show that the proposed DFS approach considerably improves the document ranking in the search results",
    "checked": true,
    "id": "b1780b6a5a2ab91e40a3f46c9321a1eae9fe0249",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Glass",
      "Md Faisal Mahbub Chowdhury",
      "Yu Deng",
      "Ruchi Mahindru",
      "Nicolas Rodolfo Fauceglia",
      "Alfio Gliozzo",
      "Nandana Mihindukulasooriya"
    ]
  },
  "https://aclanthology.org/2021.internlp-1.6": {
    "title": "Active Curriculum Learning",
    "volume": "workshop",
    "abstract": "This paper investigates and reveals the relationship between two closely related machine learning disciplines, namely Active Learning (AL) and Curriculum Learning (CL), from the lens of several novel curricula. This paper also introduces Active Curriculum Learning (ACL) which improves AL by combining AL with CL to benefit from the dynamic nature of the AL informativeness concept as well as the human insights used in the design of the curriculum heuristics. Comparison of the performance of ACL and AL on two public datasets for the Named Entity Recognition (NER) task shows the effectiveness of combining AL and CL using our proposed framework",
    "checked": true,
    "id": "09b6d09fe977f8b6ad502248b7b2f44b4e03a165",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Borna Jafarpour",
      "Dawn Sepehr",
      "Nick Pogrebnyakov"
    ]
  },
  "https://aclanthology.org/2021.internlp-1.7": {
    "title": "Tackling Fake News Detection by Interactively Learning Representations using Graph Neural Networks",
    "volume": "workshop",
    "abstract": "Easy access, variety of content, and fast widespread interactions are some of the reasons that have made social media increasingly popular in today’s society. However, this has also enabled the widespread propagation of fake news, text that is published with an intent to spread misinformation and sway beliefs. Detecting fake news is important to prevent misinformation and maintain a healthy society. While prior works have tackled this problem by building supervised learning systems, automatedly modeling the social media landscape that enables the spread of fake news is challenging. On the contrary, having humans fact check all news is not scalable. Thus, in this paper, we propose to approach this problem interactively, where human insight can be continually combined with an automated system, enabling better social media representation quality. Our experiments show performance improvements in this setting",
    "checked": true,
    "id": "9570bf97cecef8d43b2233d7dfdc75433d468baa",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Nikhil Mehta",
      "Dan Goldwasser"
    ]
  },
  "https://aclanthology.org/2021.iwpt-1.1": {
    "title": "Generic Oracles for Structured Prediction",
    "volume": "workshop",
    "abstract": "When learned without exploration, local models for structured prediction tasks are subject to exposure bias and cannot be trained without detailed guidance. Active Imitation Learning (AIL), also known in NLP as Dynamic Oracle Learning, is a general technique for working around these issues by allowing the exploration of different outputs at training time. AIL requires oracle feedback: an oracle is any algorithm which can, given a partial candidate solution and gold annotation, find the correct (minimum loss) next output to produce. This paper describes a general finite state technique for deriving oracles. The technique describe is also efficient and will greatly expand the tasks for which AIL can be used",
    "checked": true,
    "id": "87ebc7003096f78f131c9b5c6e0c660a541475a3",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christoph Teichmann",
      "Antoine Venant"
    ]
  },
  "https://aclanthology.org/2021.iwpt-1.2": {
    "title": "Proof Net Structure for Neural Lambek Categorial Parsing",
    "volume": "workshop",
    "abstract": "In this paper, we present the first statistical parser for Lambek categorial grammar (LCG), a grammatical formalism for which the graphical proof method known as *proof nets* is applicable. Our parser incorporates proof net structure and constraints into a system based on self-attention networks via novel model elements. Our experiments on an English LCG corpus show that incorporating term graph structure is helpful to the model, improving both parsing accuracy and coverage. Moreover, we derive novel loss functions by expressing proof net constraints as differentiable functions of our model output, enabling us to train our parser without ground-truth derivations",
    "checked": true,
    "id": "fd184b37746eb264a7f58479bacb601a65a6ce71",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Bhargava",
      "Gerald Penn"
    ]
  },
  "https://aclanthology.org/2021.iwpt-1.3": {
    "title": "The Reading Machine: A Versatile Framework for Studying Incremental Parsing Strategies",
    "volume": "workshop",
    "abstract": "The Reading Machine, is a parsing framework that takes as input raw text and performs six standard nlp tasks: tokenization, pos tagging, morphological analysis, lemmatization, dependency parsing and sentence segmentation. It is built upon Transition Based Parsing, and allows to implement a large number of parsing configurations, among which a fully incremental one. Three case studies are presented to highlight the versatility of the framework. The first one explores whether an incremental parser is able to take into account top-down dependencies (i.e. the influence of high level decisions on low level ones), the second compares the performances of an incremental and a pipe-line architecture and the third quantifies the impact of the right context on the predictions made by an incremental parser",
    "checked": true,
    "id": "474c609c5172e7564fc2abeeba82fffca98f3688",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Franck Dary",
      "Alexis Nasr"
    ]
  },
  "https://aclanthology.org/2021.iwpt-1.4": {
    "title": "Semi-Automatic Construction of Text-to-SQL Data for Domain Transfer",
    "volume": "workshop",
    "abstract": "Strong and affordable in-domain data is a desirable asset when transferring trained semantic parsers to novel domains. As previous methods for semi-automatically constructing such data cannot handle the complexity of realistic SQL queries, we propose to construct SQL queries via context-dependent sampling, and introduce the concept of topic. Along with our SQL query construction method, we propose a novel pipeline of semi-automatic Text-to-SQL dataset construction that covers the broad space of SQL queries. We show that the created dataset is comparable with expert annotation along multiple dimensions, and is capable of improving domain transfer performance for SOTA semantic parsers",
    "checked": true,
    "id": "4e405f320f4d1b887a9534739926b62e28cb8709",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyi Li",
      "Sujian Li",
      "Mark Steedman"
    ]
  },
  "https://aclanthology.org/2021.iwpt-1.5": {
    "title": "Levi Graph AMR Parser using Heterogeneous Attention",
    "volume": "workshop",
    "abstract": "Coupled with biaffine decoders, transformers have been effectively adapted to text-to-graph transduction and achieved state-of-the-art performance on AMR parsing. Many prior works, however, rely on the biaffine decoder for either or both arc and label predictions although most features used by the decoder may be learned by the transformer already. This paper presents a novel approach to AMR parsing by combining heterogeneous data (tokens, concepts, labels) as one input to a transformer to learn attention, and use only attention matrices from the transformer to predict all elements in AMR graphs (concepts, arcs, labels). Although our models use significantly fewer parameters than the previous state-of-the-art graph parser, they show similar or better accuracy on AMR 2.0 and 3.0",
    "checked": true,
    "id": "4c68e9dad9911fc59f68505d680e3aad80368d1e",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Han He",
      "Jinho D. Choi"
    ]
  },
  "https://aclanthology.org/2021.iwpt-1.6": {
    "title": "Translate, then Parse! A Strong Baseline for Cross-Lingual AMR Parsing",
    "volume": "workshop",
    "abstract": "In cross-lingual Abstract Meaning Representation (AMR) parsing, researchers develop models that project sentences from various languages onto their AMRs to capture their essential semantic structures: given a sentence in any language, we aim to capture its core semantic content through concepts connected by manifold types of semantic relations. Methods typically leverage large silver training data to learn a single model that is able to project non-English sentences to AMRs. However, we find that a simple baseline tends to be overlooked: translating the sentences to English and projecting their AMR with a monolingual AMR parser (translate+parse,T+P). In this paper, we revisit this simple two-step base-line, and enhance it with a strong NMT system and a strong AMR parser. Our experiments show that T+P outperforms a recent state-of-the-art system across all tested languages: German, Italian, Spanish and Mandarin with +14.6, +12.6, +14.3 and +16.0 Smatch points",
    "checked": true,
    "id": "b1eacf4f14961a63a01497ac8566aabcc0c30c1c",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Sarah Uhrig",
      "Yoalli Garcia",
      "Juri Opitz",
      "Anette Frank"
    ]
  },
  "https://aclanthology.org/2021.iwpt-1.7": {
    "title": "Great Service! Fine-grained Parsing of Implicit Arguments",
    "volume": "workshop",
    "abstract": "Broad-coverage meaning representations in NLP mostly focus on explicitly expressed content. More importantly, the scarcity of datasets annotating diverse implicit roles limits empirical studies into their linguistic nuances. For example, in the web review “Great service!”, the provider and consumer are implicit arguments of different types. We examine an annotated corpus of fine-grained implicit arguments (Cui and Hershcovich, 2020) by carefully re-annotating it, resolving several inconsistencies. Subsequently, we present the first transition-based neural parser that can handle implicit arguments dynamically, and experiment with two different transition systems on the improved dataset. We find that certain types of implicit arguments are more difficult to parse than others and that the simpler system is more accurate in recovering implicit arguments, despite having a lower overall parsing score, attesting current reasoning limitations of NLP models. This work will facilitate a better understanding of implicit and underspecified language, by incorporating it holistically into meaning representations",
    "checked": true,
    "id": "25ff4f65eab707afca0f9e10115aea61aff3725f",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Ruixiang Cui",
      "Daniel Hershcovich"
    ]
  },
  "https://aclanthology.org/2021.iwpt-1.8": {
    "title": "A Falta de Pan, Buenas Son Tortas: The Efficacy of Predicted UPOS Tags for Low Resource UD Parsing",
    "volume": "workshop",
    "abstract": "We evaluate the efficacy of predicted UPOS tags as input features for dependency parsers in lower resource settings to evaluate how treebank size affects the impact tagging accuracy has on parsing performance. We do this for real low resource universal dependency treebanks, artificially low resource data with varying treebank sizes, and for very small treebanks with varying amounts of augmented data. We find that predicted UPOS tags are somewhat helpful for low resource treebanks, especially when fewer fully-annotated trees are available. We also find that this positive impact diminishes as the amount of data increases",
    "checked": true,
    "id": "e7262b5fd26749945fa1d7f2cf2ff5158957f531",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Mark Anderson",
      "Mathieu Dehouck",
      "Carlos Gómez-Rodríguez"
    ]
  },
  "https://aclanthology.org/2021.iwpt-1.9": {
    "title": "Multilingual Dependency Parsing for Low-Resource African Languages: Case Studies on Bambara, Wolof, and Yoruba",
    "volume": "workshop",
    "abstract": "This paper describes a methodology for syntactic knowledge transfer between high-resource languages to extremely low-resource languages. The methodology consists in leveraging multilingual BERT self-attention model pretrained on large datasets to develop a multilingual multi-task model that can predict Universal Dependencies annotations for three African low-resource languages. The UD annotations include universal part-of-speech, morphological features, lemmas, and dependency trees. In our experiments, we used multilingual word embeddings and a total of 11 Universal Dependencies treebanks drawn from three high-resource languages (English, French, Norwegian) and three low-resource languages (Bambara, Wolof and Yoruba). We developed various models to test specific language combinations involving contemporary contact languages or genetically related languages. The results of the experiments show that multilingual models that involve high-resource languages and low-resource languages with contemporary contact between each other can provide better results than combinations that only include unrelated languages. As far genetic relationships are concerned, we could not draw any conclusion regarding the impact of language combinations involving the selected low-resource languages, namely Wolof and Yoruba",
    "checked": true,
    "id": "359442cc56eb8bcc81f07c4aa6f621ec1e20f4ca",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheikh M. Bamba Dione"
    ]
  },
  "https://aclanthology.org/2021.iwpt-1.10": {
    "title": "Bidirectional Domain Adaptation Using Weighted Multi-Task Learning",
    "volume": "workshop",
    "abstract": "Domain adaption in syntactic parsing is still a significant challenge. We address the issue of data imbalance between the in-domain and out-of-domain treebank typically used for the problem. We define domain adaptation as a Multi-task learning (MTL) problem, which allows us to train two parsers, one for each do-main. Our results show that the MTL approach is beneficial for the smaller treebank. For the larger treebank, we need to use loss weighting in order to avoid a decrease in performance be-low the single task. In order to determine towhat degree the data imbalance between two domains and the domain differences affect results, we also carry out an experiment with two imbalanced in-domain treebanks and show that loss weighting also improves performance in an in-domain setting. Given loss weighting in MTL, we can improve results for both parsers",
    "checked": true,
    "id": "6de4f0a5a1071c018248d2921706f68db4610522",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Daniel Dakota",
      "Zeeshan Ali Sayyed",
      "Sandra Kübler"
    ]
  },
  "https://aclanthology.org/2021.iwpt-1.11": {
    "title": "Strength in Numbers: Averaging and Clustering Effects in Mixture of Experts for Graph-Based Dependency Parsing",
    "volume": "workshop",
    "abstract": "We review two features of mixture of experts (MoE) models which we call averaging and clustering effects in the context of graph-based dependency parsers learned in a supervised probabilistic framework. Averaging corresponds to the ensemble combination of parsers and is responsible for variance reduction which helps stabilizing and improving parsing accuracy. Clustering describes the capacity of MoE models to give more credit to experts believed to be more accurate given an input. Although promising, this is difficult to achieve, especially without additional data. We design an experimental set-up to study the impact of these effects. Whereas averaging is always beneficial, clustering requires good initialization and stabilization techniques, but its advantages over mere averaging seem to eventually vanish when enough experts are present. As a by product, we show how this leads to state-of-the-art results on the PTB and the CoNLL09 Chinese treebank, with low variance across experiments",
    "checked": true,
    "id": "b70f685ae967c45754a19e2c321575729eddde33",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Xudong Zhang",
      "Joseph Le Roux",
      "Thierry Charnois"
    ]
  },
  "https://aclanthology.org/2021.iwpt-1.12": {
    "title": "A Modest Pareto Optimisation Analysis of Dependency Parsers in 2021",
    "volume": "workshop",
    "abstract": "We evaluate three leading dependency parser systems from different paradigms on a small yet diverse subset of languages in terms of their accuracy-efficiency Pareto front. As we are interested in efficiency, we evaluate core parsers without pretrained language models (as these are typically huge networks and would constitute most of the compute time) or other augmentations that can be transversally applied to any of them. Biaffine parsing emerges as a well-balanced default choice, with sequence-labelling parsing being preferable if inference speed (but not training energy cost) is the priority",
    "checked": true,
    "id": "2bcc1148b7d19a827300f1d4d5a575d1bc61eb86",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Mark Anderson",
      "Carlos Gómez-Rodríguez"
    ]
  },
  "https://aclanthology.org/2021.iwpt-1.13": {
    "title": "Applying Occam's Razor to Transformer-Based Dependency Parsing: What Works, What Doesn't, and What is Really Necessary",
    "volume": "workshop",
    "abstract": "The introduction of pre-trained transformer-based contextualized word embeddings has led to considerable improvements in the accuracy of graph-based parsers for frameworks such as Universal Dependencies (UD). However, previous works differ in various dimensions, including their choice of pre-trained language models and whether they use LSTM layers. With the aims of disentangling the effects of these choices and identifying a simple yet widely applicable architecture, we introduce STEPS, a new modular graph-based dependency parser. Using STEPS, we perform a series of analyses on the UD corpora of a diverse set of languages. We find that the choice of pre-trained embeddings has by far the greatest impact on parser performance and identify XLM-R as a robust choice across the languages in our study. Adding LSTM layers provides no benefits when using transformer-based embeddings. A multi-task training setup outputting additional UD features may contort results. Taking these insights together, we propose a simple but widely applicable parser architecture and configuration, achieving new state-of-the-art results (in terms of LAS) for 10 out of 12 diverse languages",
    "checked": true,
    "id": "1a7ca0620507645ce9f4baa1f5a7c42705d3ed74",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Stefan Grünewald",
      "Annemarie Friedrich",
      "Jonas Kuhn"
    ]
  },
  "https://aclanthology.org/2021.iwpt-1.14": {
    "title": "Incorporating Compositionality and Morphology into End-to-End Models",
    "volume": "workshop",
    "abstract": "Many neural end-to-end systems today do not rely on syntactic parse trees, as much of the information that parse trees provide is encoded in the parameters of pretrained models. Lessons learned from parsing technologies and from taking a multilingual perspective, however, are still relevant even for end-to-end models. This talk will describe work that relies on compositionality in semantic parsing and in reading comprehension requiring numerical reasoning. We’ll then describe a new dataset that requires advances in multilingual modeling, and some approaches designed to better model morphology than off-the-shelf subword models that make some progress on these challenges",
    "checked": true,
    "id": "e5355f2292bbb998463fa42aa78b58949ff3136a",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emily Pitler"
    ]
  },
  "https://aclanthology.org/2021.iwpt-1.15": {
    "title": "From Raw Text to Enhanced Universal Dependencies: The Parsing Shared Task at IWPT 2021",
    "volume": "workshop",
    "abstract": "We describe the second IWPT task on end-to-end parsing from raw text to Enhanced Universal Dependencies. We provide details about the evaluation metrics and the datasets used for training and evaluation. We compare the approaches taken by participating teams and discuss the results of the shared task, also in comparison with the first edition of this task",
    "checked": true,
    "id": "7226d14e6dea73dfad521256248ec2b19ae66ad8",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Gosse Bouma",
      "Djamé Seddah",
      "Daniel Zeman"
    ]
  },
  "https://aclanthology.org/2021.iwpt-1.16": {
    "title": "COMBO: A New Module for EUD Parsing",
    "volume": "workshop",
    "abstract": "We introduce the COMBO-based approach for EUD parsing and its implementation, which took part in the IWPT 2021 EUD shared task. The goal of this task is to parse raw texts in 17 languages into Enhanced Universal Dependencies (EUD). The proposed approach uses COMBO to predict UD trees and EUD graphs. These structures are then merged into the final EUD graphs. Some EUD edge labels are extended with case information using a single language-independent expansion rule. In the official evaluation, the solution ranked fourth, achieving an average ELAS of 83.79%. The source code is available at https://gitlab.clarin-pl.eu/syntactic-tools/combo",
    "checked": true,
    "id": "30d5deddf20f3ffeb172394e4d46861d39e1e749",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Mateusz Klimaszewski",
      "Alina Wróblewska"
    ]
  },
  "https://aclanthology.org/2021.iwpt-1.17": {
    "title": "Splitting EUD Graphs into Trees: A Quick and Clatty Approach",
    "volume": "workshop",
    "abstract": "We present the system submission from the FASTPARSE team for the EUD Shared Task at IWPT 2021. We engaged in the task last year by focusing on efficiency. This year we have focused on experimenting with new ideas on a limited time budget. Our system is based on splitting the EUD graph into several trees, based on linguistic criteria. We predict these trees using a sequence-labelling parser and combine them into an EUD graph. The results were relatively poor, although not a total disaster and could probably be improved with some polishing of the system’s rough edges",
    "checked": true,
    "id": "c61af63dd76574dd8e52c13a6a5edf0844368036",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Mark Anderson",
      "Carlos Gómez-Rodríguez"
    ]
  },
  "https://aclanthology.org/2021.iwpt-1.18": {
    "title": "Graph Rewriting for Enhanced Universal Dependencies",
    "volume": "workshop",
    "abstract": "This paper describes a system proposed for the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies (EUD). We propose a Graph Rewriting based system for computing Enhanced Universal Dependencies, given the Basic Universal Dependencies (UD)",
    "checked": true,
    "id": "d57f7f571eabbb4319ed60af47496b7ac1c546b5",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Bruno Guillaume",
      "Guy Perrier"
    ]
  },
  "https://aclanthology.org/2021.iwpt-1.19": {
    "title": "Biaffine Dependency and Semantic Graph Parsing for EnhancedUniversal Dependencies",
    "volume": "workshop",
    "abstract": "This paper presents the system used in our submission to the IWPT 2021 Shared Task. This year the official evaluation metrics was ELAS, therefore dependency parsing might have been avoided as well as other pipeline stages like POS tagging and lemmatization. We nevertheless chose to deploy a combination of a dependency parser and a graph parser. The dependency parser is a biaffine parser, that uses transformers for representing input sentences, with no other feature. The graph parser is a semantic parser that exploits a similar architecture except for using a sigmoid crossentropy loss function to return multiple values for the predicted arcs. The final output is obtained by merging the output of the two parsers. The dependency parser achieves top or close to top LAS performance with respect to other systems that report results on such metrics, except on low resource languages (Tamil, Estonian, Latvian)",
    "checked": true,
    "id": "dcbbb67eac0b2e4bd0a30fb75c6e0c379e3c5c9a",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Giuseppe Attardi",
      "Daniele Sartiano",
      "Maria Simi"
    ]
  },
  "https://aclanthology.org/2021.iwpt-1.20": {
    "title": "Enhanced Universal Dependency Parsing with Automated Concatenation of Embeddings",
    "volume": "workshop",
    "abstract": "This paper describe the system used in our submission to the IWPT 2021 Shared Task. Our system is a graph-based parser with the technique of Automated Concatenation of Embeddings (ACE). Because recent work found that better word representations can be obtained by concatenating different types of embeddings, we use ACE to automatically find the better concatenation of embeddings for the task of enhanced universal dependencies. According to official results averaged on 17 languages, our system rank 2nd over 9 teams",
    "checked": true,
    "id": "2d169d698a20659242f6f875d966527f2e2417ad",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Xinyu Wang",
      "Zixia Jia",
      "Yong Jiang",
      "Kewei Tu"
    ]
  },
  "https://aclanthology.org/2021.iwpt-1.21": {
    "title": "RobertNLP at the IWPT 2021 Shared Task: Simple Enhanced UD Parsing for 17 Languages",
    "volume": "workshop",
    "abstract": "This paper presents our multilingual dependency parsing system as used in the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies. Our system consists of an unfactorized biaffine classifier that operates directly on fine-tuned XLM-R embeddings and generates enhanced UD graphs by predicting the best dependency label (or absence of a dependency) for each pair of tokens. To avoid sparsity issues resulting from lexicalized dependency labels, we replace lexical items in relations with placeholders at training and prediction time, later retrieving them from the parse via a hybrid rule-based/machine-learning system. In addition, we utilize model ensembling at prediction time. Our system achieves high parsing accuracy on the blind test data, ranking 3rd out of 9 with an average ELAS F1 score of 86.97",
    "checked": true,
    "id": "033199f7d31951b6bbfe502b2291fddd3e445d88",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Stefan Grünewald",
      "Frederik Tobias Oertel",
      "Annemarie Friedrich"
    ]
  },
  "https://aclanthology.org/2021.iwpt-1.22": {
    "title": "The DCU-EPFL Enhanced Dependency Parser at the IWPT 2021 Shared Task",
    "volume": "workshop",
    "abstract": "We describe the DCU-EPFL submission to the IWPT 2021 Parsing Shared Task: From Raw Text to Enhanced Universal Dependencies. The task involves parsing Enhanced UD graphs, which are an extension of the basic dependency trees designed to be more facilitative towards representing semantic structure. Evaluation is carried out on 29 treebanks in 17 languages and participants are required to parse the data from each language starting from raw strings. Our approach uses the Stanza pipeline to preprocess the text files, XLM-RoBERTa to obtain contextualized token representations, and an edge-scoring and labeling model to predict the enhanced graph. Finally, we run a postprocessing script to ensure all of our outputs are valid Enhanced UD graphs. Our system places 6th out of 9 participants with a coarse Enhanced Labeled Attachment Score (ELAS) of 83.57. We carry out additional post-deadline experiments which include using Trankit for pre-processing, XLM-RoBERTa LARGE, treebank concatenation, and multitask learning between a basic and an enhanced dependency parser. All of these modifications improve our initial score and our final system has a coarse ELAS of 88.04",
    "checked": true,
    "id": "965e2c78db7885960e65d001d65c314603a69b1c",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "James Barry",
      "Alireza Mohammadshahi",
      "Joachim Wagner",
      "Jennifer Foster",
      "James Henderson"
    ]
  },
  "https://aclanthology.org/2021.iwpt-1.23": {
    "title": "TGIF: Tree-Graph Integrated-Format Parser for Enhanced UD with Two-Stage Generic- to Individual-Language Finetuning",
    "volume": "workshop",
    "abstract": "We present our contribution to the IWPT 2021 shared task on parsing into enhanced Universal Dependencies. Our main system component is a hybrid tree-graph parser that integrates (a) predictions of spanning trees for the enhanced graphs with (b) additional graph edges not present in the spanning trees. We also adopt a finetuning strategy where we first train a language-generic parser on the concatenation of data from all available languages, and then, in a second step, finetune on each individual language separately. Additionally, we develop our own complete set of pre-processing modules relevant to the shared task, including tokenization, sentence segmentation, and multiword token expansion, based on pre-trained XLM-R models and our own pre-training of character-level language models. Our submission reaches a macro-average ELAS of 89.24 on the test set. It ranks top among all teams, with a margin of more than 2 absolute ELAS over the next best-performing submission, and best score on 16 out of 17 languages",
    "checked": true,
    "id": "c98291229d6069f6dabbe1a6f9853881694ea06e",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Tianze Shi",
      "Lillian Lee"
    ]
  },
  "https://aclanthology.org/2021.iwpt-1.24": {
    "title": "End-to-end mBERT based Seq2seq Enhanced Dependency Parser with Linguistic Typology knowledge",
    "volume": "workshop",
    "abstract": "We describe the NUIG solution for IWPT 2021 Shared Task of Enhanced Dependency (ED) parsing in multiple languages. For this shared task, we propose and evaluate an End-to-end Seq2seq mBERT-based ED parser which predicts the ED-parse tree of a given input sentence as a relative head-position tag-sequence. Our proposed model is a multitasking neural-network which performs five key tasks simultaneously namely UPOS tagging, UFeat tagging, Lemmatization, Dependency-parsing and ED-parsing. Furthermore we utilise the linguistic typology available in the WALS database to improve the ability of our proposed end-to-end parser to transfer across languages. Results show that our proposed Seq2seq ED-parser performs on par with state-of-the-art ED-parser despite having a much simpler de- sign",
    "checked": true,
    "id": "83a7cae43ef2e49052866068d07f846be9bda3b2",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Chinmay Choudhary",
      "Colm O’riordan"
    ]
  },
  "https://aclanthology.org/2021.iwslt-1.1": {
    "title": "FINDINGS OF THE IWSLT 2021 EVALUATION CAMPAIGN",
    "volume": "workshop",
    "abstract": "The evaluation campaign of the International Conference on Spoken Language Translation (IWSLT 2021) featured this year four shared tasks: (i) Simultaneous speech translation, (ii) Offline speech translation, (iii) Multilingual speech translation, (iv) Low-resource speech translation. A total of 22 teams participated in at least one of the tasks. This paper describes each shared task, data and evaluation metrics, and reports results of the received submissions",
    "checked": true,
    "id": "0a53f5ef9769b2f08ceee72e23312974a7cdf7e7",
    "semantic_title": "",
    "citation_count": 48,
    "authors": [
      "Antonios Anastasopoulos",
      "Ondřej Bojar",
      "Jacob Bremerman",
      "Roldano Cattoni",
      "Maha Elbayad",
      "Marcello Federico",
      "Xutai Ma",
      "Satoshi Nakamura",
      "Matteo Negri",
      "Jan Niehues",
      "Juan Pino",
      "Elizabeth Salesky",
      "Sebastian Stüker",
      "Katsuhito Sudoh",
      "Marco Turchi",
      "Alexander Waibel",
      "Changhan Wang",
      "Matthew Wiesner"
    ]
  },
  "https://aclanthology.org/2021.iwslt-1.2": {
    "title": "The USTC-NELSLIP Systems for Simultaneous Speech Translation Task at IWSLT 2021",
    "volume": "workshop",
    "abstract": "This paper describes USTC-NELSLIP’s submissions to the IWSLT2021 Simultaneous Speech Translation task. We proposed a novel simultaneous translation model, Cross-Attention Augmented Transducer (CAAT), which extends conventional RNN-T to sequence-to-sequence tasks without monotonic constraints, e.g., simultaneous translation. Experiments on speech-to-text (S2T) and text-to-text (T2T) simultaneous translation tasks shows CAAT achieves better quality-latency trade-offs compared to wait-k, one of the previous state-of-the-art approaches. Based on CAAT architecture and data augmentation, we build S2T and T2T simultaneous translation systems in this evaluation campaign. Compared to last year’s optimal systems, our S2T simultaneous translation system improves by an average of 11.3 BLEU for all latency regimes, and our T2T simultaneous translation system improves by an average of 4.6 BLEU",
    "checked": true,
    "id": "cde1a9f35a1427ae83e3dc4203e775d3da611d8b",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Dan Liu",
      "Mengge Du",
      "Xiaoxi Li",
      "Yuchen Hu",
      "Lirong Dai"
    ]
  },
  "https://aclanthology.org/2021.iwslt-1.3": {
    "title": "NAIST English-to-Japanese Simultaneous Translation System for IWSLT 2021 Simultaneous Text-to-text Task",
    "volume": "workshop",
    "abstract": "This paper describes NAIST’s system for the English-to-Japanese Simultaneous Text-to-text Translation Task in IWSLT 2021 Evaluation Campaign. Our primary submission is based on wait-k neural machine translation with sequence-level knowledge distillation to encourage literal translation",
    "checked": true,
    "id": "c9bfc4ba3b3472beba67cc65d5a9746fcf73f243",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Ryo Fukuda",
      "Yui Oka",
      "Yasumasa Kano",
      "Yuki Yano",
      "Yuka Ko",
      "Hirotaka Tokuyama",
      "Kosuke Doi",
      "Sakriani Sakti",
      "Katsuhito Sudoh",
      "Satoshi Nakamura"
    ]
  },
  "https://aclanthology.org/2021.iwslt-1.4": {
    "title": "The University of Edinburgh's Submission to the IWSLT21 Simultaneous Translation Task",
    "volume": "workshop",
    "abstract": "We describe our submission to the IWSLT 2021 shared task on simultaneous text-to-text English-German translation. Our system is based on the re-translation approach where the agent re-translates the whole source prefix each time it receives a new source token. This approach has the advantage of being able to use a standard neural machine translation (NMT) inference engine with beam search, however, there is a risk that incompatibility between successive re-translations will degrade the output. To improve the quality of the translations, we experiment with various approaches: we use a fixed size wait at the beginning of the sentence, we use a language model score to detect translatable units, and we apply dynamic masking to determine when the translation is unstable. We find that a combination of dynamic masking and language model score obtains the best latency-quality trade-off",
    "checked": true,
    "id": "c132c51b5de421cdaab1776d6ad427fcf7aa6f9b",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Sukanta Sen",
      "Ulrich Germann",
      "Barry Haddow"
    ]
  },
  "https://aclanthology.org/2021.iwslt-1.5": {
    "title": "Without Further Ado: Direct and Simultaneous Speech Translation by AppTek in 2021",
    "volume": "workshop",
    "abstract": "This paper describes the offline and simultaneous speech translation systems developed at AppTek for IWSLT 2021. Our offline ST submission includes the direct end-to-end system and the so-called posterior tight integrated model, which is akin to the cascade system but is trained in an end-to-end fashion, where all the cascaded modules are end-to-end models themselves. For simultaneous ST, we combine hybrid automatic speech recognition with a machine translation approach whose translation policy decisions are learned from statistical word alignments. Compared to last year, we improve general quality and provide a wider range of quality/latency trade-offs, both due to a data augmentation method making the MT model robust to varying chunk sizes. Finally, we present a method for ASR output segmentation into sentences that introduces a minimal additional delay",
    "checked": true,
    "id": "2dc296f20373dd0da15cdded346361af6e2e2702",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Parnia Bahar",
      "Patrick Wilken",
      "Mattia A. Di Gangi",
      "Evgeny Matusov"
    ]
  },
  "https://aclanthology.org/2021.iwslt-1.6": {
    "title": "The Volctrans Neural Speech Translation System for IWSLT 2021",
    "volume": "workshop",
    "abstract": "This paper describes the systems submitted to IWSLT 2021 by the Volctrans team. We participate in the offline speech translation and text-to-text simultaneous translation tracks. For offline speech translation, our best end-to-end model achieves 7.9 BLEU improvements over the benchmark on the MuST-C test set and is even approaching the results of a strong cascade solution. For text-to-text simultaneous translation, we explore the best practice to optimize the wait-k model. As a result, our final submitted systems exceed the benchmark at around 7 BLEU on the same latency regime. We release our code and model to facilitate both future research works and industrial applications",
    "checked": true,
    "id": "5583f5495653457017599d59f8d0922826c6f2b8",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Chengqi Zhao",
      "Zhicheng Liu",
      "Jian Tong",
      "Tao Wang",
      "Mingxuan Wang",
      "Rong Ye",
      "Qianqian Dong",
      "Jun Cao",
      "Lei Li"
    ]
  },
  "https://aclanthology.org/2021.iwslt-1.7": {
    "title": "THE IWSLT 2021 BUT SPEECH TRANSLATION SYSTEMS",
    "volume": "workshop",
    "abstract": "The paper describes BUT’s English to German offline speech translation (ST) systems developed for IWSLT2021. They are based on jointly trained Automatic Speech Recognition-Machine Translation models. Their performances is evaluated on MustC-Common test set. In this work, we study their efficiency from the perspective of having a large amount of separate ASR training data and MT training data, and a smaller amount of speech-translation training data. Large amounts of ASR and MT training data are utilized for pre-training the ASR and MT models. Speech-translation data is used to jointly optimize ASR-MT models by defining an end-to-end differentiable path from speech to translations. For this purpose, we use the internal continuous representations from the ASR-decoder as the input to MT module. We show that speech translation can be further improved by training the ASR-decoder jointly with the MT-module using large amount of text-only MT training data. We also show significant improvements by training an ASR module capable of generating punctuated text, rather than leaving the punctuation task to the MT module",
    "checked": true,
    "id": "05b3a2b3b554b9be617a506fc377c0502f9e28fd",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "hari Krishna Vydana",
      "Martin Karafiat",
      "Lukas Burget",
      "Jan Černocký"
    ]
  },
  "https://aclanthology.org/2021.iwslt-1.8": {
    "title": "Dealing with training and test segmentation mismatch: FBK@IWSLT2021",
    "volume": "workshop",
    "abstract": "This paper describes FBK’s system submission to the IWSLT 2021 Offline Speech Translation task. We participated with a direct model, which is a Transformer-based architecture trained to translate English speech audio data into German texts. The training pipeline is characterized by knowledge distillation and a two-step fine-tuning procedure. Both knowledge distillation and the first fine-tuning step are carried out on manually segmented real and synthetic data, the latter being generated with an MT system trained on the available corpora. Differently, the second fine-tuning step is carried out on a random segmentation of the MuST-C v2 En-De dataset. Its main goal is to reduce the performance drops occurring when a speech translation model trained on manually segmented data (i.e. an ideal, sentence-like segmentation) is evaluated on automatically segmented audio (i.e. actual, more realistic testing conditions). For the same purpose, a custom hybrid segmentation procedure that accounts for both audio content (pauses) and for the length of the produced segments is applied to the test data before passing them to the system. At inference time, we compared this procedure with a baseline segmentation method based on Voice Activity Detection (VAD). Our results indicate the effectiveness of the proposed hybrid approach, shown by a reduction of the gap with manual segmentation from 8.3 to 1.4 BLEU points",
    "checked": true,
    "id": "963f6ea170ee86b53459407208ca17007b8cb7cb",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Sara Papi",
      "Marco Gaido",
      "Matteo Negri",
      "Marco Turchi"
    ]
  },
  "https://aclanthology.org/2021.iwslt-1.9": {
    "title": "The NiuTrans End-to-End Speech Translation System for IWSLT 2021 Offline Task",
    "volume": "workshop",
    "abstract": "This paper describes the submission of the NiuTrans end-to-end speech translation system for the IWSLT 2021 offline task, which translates from the English audio to German text directly without intermediate transcription. We use the Transformer-based model architecture and enhance it by Conformer, relative position encoding, and stacked acoustic and textual encoding. To augment the training data, the English transcriptions are translated to German translations. Finally, we employ ensemble decoding to integrate the predictions from several models trained with the different datasets. Combining these techniques, we achieve 33.84 BLEU points on the MuST-C En-De test set, which shows the enormous potential of the end-to-end model",
    "checked": true,
    "id": "8dfa41ada085940f3e4cb2af3eb7ba6fe1082a41",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Chen Xu",
      "Xiaoqian Liu",
      "Xiaowen Liu",
      "Tiger Wang",
      "Canan Huang",
      "Tong Xiao",
      "Jingbo Zhu"
    ]
  },
  "https://aclanthology.org/2021.iwslt-1.10": {
    "title": "ESPnet-ST IWSLT 2021 Offline Speech Translation System",
    "volume": "workshop",
    "abstract": "This paper describes the ESPnet-ST group’s IWSLT 2021 submission in the offline speech translation track. This year we made various efforts on training data, architecture, and audio segmentation. On the data side, we investigated sequence-level knowledge distillation (SeqKD) for end-to-end (E2E) speech translation. Specifically, we used multi-referenced SeqKD from multiple teachers trained on different amounts of bitext. On the architecture side, we adopted the Conformer encoder and the Multi-Decoder architecture, which equips dedicated decoders for speech recognition and translation tasks in a unified encoder-decoder model and enables search in both source and target language spaces during inference. We also significantly improved audio segmentation by using the pyannote.audio toolkit and merging multiple short segments for long context modeling. Experimental evaluations showed that each of them contributed to large improvements in translation performance. Our best E2E system combined all the above techniques with model ensembling and achieved 31.4 BLEU on the 2-ref of tst2021 and 21.2 BLEU and 19.3 BLEU on the two single references of tst2021",
    "checked": true,
    "id": "9195186cf44876d0d1d03b87756c464b760a7f4e",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Hirofumi Inaguma",
      "Brian Yan",
      "Siddharth Dalmia",
      "Pengcheng Guo",
      "Jiatong Shi",
      "Kevin Duh",
      "Shinji Watanabe"
    ]
  },
  "https://aclanthology.org/2021.iwslt-1.11": {
    "title": "End-to-End Speech Translation with Pre-trained Models and Adapters: UPC at IWSLT 2021",
    "volume": "workshop",
    "abstract": "This paper describes the submission to the IWSLT 2021 offline speech translation task by the UPC Machine Translation group. The task consists of building a system capable of translating English audio recordings extracted from TED talks into German text. Submitted systems can be either cascade or end-to-end and use a custom or given segmentation. Our submission is an end-to-end speech translation system, which combines pre-trained models (Wav2Vec 2.0 and mBART) with coupling modules between the encoder and decoder, and uses an efficient fine-tuning technique, which trains only 20% of its total parameters. We show that adding an Adapter to the system and pre-training it, can increase the convergence speed and the final result, with which we achieve a BLEU score of 27.3 on the MuST-C test set. Our final model is an ensemble that obtains 28.22 BLEU score on the same set. Our submission also uses a custom segmentation algorithm that employs pre-trained Wav2Vec 2.0 for identifying periods of untranscribable text and can bring improvements of 2.5 to 3 BLEU score on the IWSLT 2019 test set, as compared to the result with the given segmentation",
    "checked": true,
    "id": "5e5e6c3317814423158054285b81c911a8382b88",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Gerard I. Gállego",
      "Ioannis Tsiamas",
      "Carlos Escolano",
      "José A. R. Fonollosa",
      "Marta R. Costa-jussà"
    ]
  },
  "https://aclanthology.org/2021.iwslt-1.12": {
    "title": "VUS at IWSLT 2021: A Finetuned Pipeline for Offline Speech Translation",
    "volume": "workshop",
    "abstract": "In this technical report, we describe the fine-tuned ASR-MT pipeline used for the IWSLT shared task. We remove less useful speech samples by checking WER with an ASR model, and further train a wav2vec and Transformers-based ASR module based on the filtered data. In addition, we cleanse the errata that can interfere with the machine translation process and use it for Transformer-based MT module training. Finally, in the actual inference phase, we use a sentence boundary detection model trained with constrained data to properly merge fragment ASR outputs into full sentences. The merged sentences are post-processed using part of speech. The final result is yielded by the trained MT module. The performance using the dev set displays BLEU 20.37, and this model records the performance of BLEU 20.9 with the test set",
    "checked": true,
    "id": "6ba79f383dca7c5143e0171943aa738ed4e46b03",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Yong Rae Jo",
      "Youngki Moon",
      "Minji Jung",
      "Jungyoon Choi",
      "Jihyung Moon",
      "Won Ik Cho"
    ]
  },
  "https://aclanthology.org/2021.iwslt-1.13": {
    "title": "KIT's IWSLT 2021 Offline Speech Translation System",
    "volume": "workshop",
    "abstract": "This paper describes KIT’submission to the IWSLT 2021 Offline Speech Translation Task. We describe a system in both cascaded condition and end-to-end condition. In the cascaded condition, we investigated different end-to-end architectures for the speech recognition module. For the text segmentation module, we trained a small transformer-based model on high-quality monolingual data. For the translation module, our last year’s neural machine translation model was reused. In the end-to-end condition, we improved our Speech Relative Transformer architecture to reach or even surpass the result of the cascade system",
    "checked": true,
    "id": "7ed66f3c5f897ed89515bb0d2d35d5c57dcf2cb7",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Tuan Nam Nguyen",
      "Thai Son Nguyen",
      "Christian Huber",
      "Ngoc-Quan Pham",
      "Thanh-Le Ha",
      "Felix Schneider",
      "Sebastian Stüker"
    ]
  },
  "https://aclanthology.org/2021.iwslt-1.14": {
    "title": "FST: the FAIR Speech Translation System for the IWSLT21 Multilingual Shared Task",
    "volume": "workshop",
    "abstract": "In this paper, we describe our end-to-end multilingual speech translation system submitted to the IWSLT 2021 evaluation campaign on the Multilingual Speech Translation shared task. Our system is built by leveraging transfer learning across modalities, tasks and languages. First, we leverage general-purpose multilingual modules pretrained with large amounts of unlabelled and labelled data. We further enable knowledge transfer from the text task to the speech task by training two tasks jointly. Finally, our multilingual model is finetuned on speech translation task-specific data to achieve the best translation results. Experimental results show our system outperforms the reported systems, including both end-to-end and cascaded based approaches, by a large margin. In some translation directions, our speech translation results evaluated on the public Multilingual TEDx test set are even comparable with the ones from a strong text-to-text translation system, which uses the oracle speech transcripts as input",
    "checked": true,
    "id": "aaacdfcd2af6296181b416e7a43bcde7e0376394",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Yun Tang",
      "Hongyu Gong",
      "Xian Li",
      "Changhan Wang",
      "Juan Pino",
      "Holger Schwenk",
      "Naman Goyal"
    ]
  },
  "https://aclanthology.org/2021.iwslt-1.15": {
    "title": "Maastricht University's Multilingual Speech Translation System for IWSLT 2021",
    "volume": "workshop",
    "abstract": "This paper describes Maastricht University’s participation in the IWSLT 2021 multilingual speech translation track. The task in this track is to build multilingual speech translation systems in supervised and zero-shot directions. Our primary system is an end-to-end model that performs both speech transcription and translation. We observe that the joint training for the two tasks is complementary especially when the speech translation data is scarce. On the source and target side, we use data augmentation and pseudo-labels respectively to improve the performance of our systems. We also introduce an ensembling technique that consistently improves the quality of transcriptions and translations. The experiments show that the end-to-end system is competitive with its cascaded counterpart especially in zero-shot conditions",
    "checked": true,
    "id": "279a70239627cdda75e49e58d37711e45d042a54",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Danni Liu",
      "Jan Niehues"
    ]
  },
  "https://aclanthology.org/2021.iwslt-1.16": {
    "title": "ZJU's IWSLT 2021 Speech Translation System",
    "volume": "workshop",
    "abstract": "In this paper, we describe Zhejiang University’s submission to the IWSLT2021 Multilingual Speech Translation Task. This task focuses on speech translation (ST) research across many non-English source languages. Participants can decide whether to work on constrained systems or unconstrained systems which can using external data. We create both cascaded and end-to-end speech translation constrained systems, using the provided data only. In the cascaded approach, we combine Conformer-based automatic speech recognition (ASR) with the Transformer-based neural machine translation (NMT). Our end-to-end direct speech translation systems use ASR pretrained encoder and multi-task decoders. The submitted systems are ensembled by different cascaded models",
    "checked": true,
    "id": "659b10f3c5b58d88fd4d242e14ffca131367d426",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Linlin Zhang"
    ]
  },
  "https://aclanthology.org/2021.iwslt-1.17": {
    "title": "Multilingual Speech Translation with Unified Transformer: Huawei Noah's Ark Lab at IWSLT 2021",
    "volume": "workshop",
    "abstract": "This paper describes the system submitted to the IWSLT 2021 Multilingual Speech Translation (MultiST) task from Huawei Noah’s Ark Lab. We use a unified transformer architecture for our MultiST model, so that the data from different modalities (i.e., speech and text) and different tasks (i.e., Speech Recognition, Machine Translation, and Speech Translation) can be exploited to enhance the model’s ability. Specifically, speech and text inputs are firstly fed to different feature extractors to extract acoustic and textual features, respectively. Then, these features are processed by a shared encoder–decoder architecture. We apply several training techniques to improve the performance, including multi-task learning, task-level curriculum learning, data augmentation, etc. Our final system achieves significantly better results than bilingual baselines on supervised language pairs and yields reasonable results on zero-shot language pairs",
    "checked": true,
    "id": "33ea05476e9c1ff1f5ecb96d740b8b16fe80e744",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Xingshan Zeng",
      "Liangyou Li",
      "Qun Liu"
    ]
  },
  "https://aclanthology.org/2021.iwslt-1.18": {
    "title": "Multilingual Speech Translation KIT @ IWSLT2021",
    "volume": "workshop",
    "abstract": "This paper contains the description for the submission of Karlsruhe Institute of Technology (KIT) for the multilingual TEDx translation task in the IWSLT 2021 evaluation campaign. Our main approach is to develop both cascade and end-to-end systems and eventually combine them together to achieve the best possible results for this extremely low-resource setting. The report also confirms certain consistent architectural improvement added to the Transformer architecture, for all tasks: translation, transcription and speech translation",
    "checked": true,
    "id": "3071e5ca83f2c4fe4a4c62fd48a8abd3342a0dc1",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Ngoc-Quan Pham",
      "Tuan Nam Nguyen",
      "Thanh-Le Ha",
      "Sebastian Stüker",
      "Alexander Waibel",
      "Dan He"
    ]
  },
  "https://aclanthology.org/2021.iwslt-1.19": {
    "title": "Edinburgh's End-to-End Multilingual Speech Translation System for IWSLT 2021",
    "volume": "workshop",
    "abstract": "This paper describes Edinburgh’s submissions to the IWSLT2021 multilingual speech translation (ST) task. We aim at improving multilingual translation and zero-shot performance in the constrained setting (without using any extra training data) through methods that encourage transfer learning and larger capacity modeling with advanced neural components. We build our end-to-end multilingual ST model based on Transformer, integrating techniques including adaptive speech feature selection, language-specific modeling, multi-task learning, deep and big Transformer, sparsified linear attention and root mean square layer normalization. We adopt data augmentation using machine translation models for ST which converts the zero-shot problem into a zero-resource one. Experimental results show that these methods deliver substantial improvements, surpassing the official baseline by > 15 average BLEU and outperforming our cascading system by > 2 average BLEU. Our final submission achieves competitive performance (runner up)",
    "checked": true,
    "id": "2f8f7aac687886705c05bdf82e23036a1cc121c8",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Biao Zhang",
      "Rico Sennrich"
    ]
  },
  "https://aclanthology.org/2021.iwslt-1.20": {
    "title": "ON-TRAC' systems for the IWSLT 2021 low-resource speech translation and multilingual speech translation shared tasks",
    "volume": "workshop",
    "abstract": "This paper describes the ON-TRAC Consortium translation systems developed for two challenge tracks featured in the Evaluation Campaign of IWSLT 2021, low-resource speech translation and multilingual speech translation. The ON-TRAC Consortium is composed of researchers from three French academic laboratories and an industrial partner: LIA (Avignon Université), LIG (Université Grenoble Alpes), LIUM (Le Mans Université), and researchers from Airbus. A pipeline approach was explored for the low-resource speech translation task, using a hybrid HMM/TDNN automatic speech recognition system fed by wav2vec features, coupled to an NMT system. For the multilingual speech translation task, we investigated the us of a dual-decoder Transformer that jointly transcribes and translates an input speech. This model was trained in order to translate from multiple source languages to multiple target ones",
    "checked": true,
    "id": "f247435c430f4fad32803f9e523c5a15ad07b141",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Hang Le",
      "Florentin Barbier",
      "Ha Nguyen",
      "Natalia Tomashenko",
      "Salima Mdhaffar",
      "Souhir Gabiche Gahbiche",
      "Benjamin Lecouteux",
      "Didier Schwab",
      "Yannick Estève"
    ]
  },
  "https://aclanthology.org/2021.iwslt-1.21": {
    "title": "IMS' Systems for the IWSLT 2021 Low-Resource Speech Translation Task",
    "volume": "workshop",
    "abstract": "This paper describes the submission to the IWSLT 2021 Low-Resource Speech Translation Shared Task by IMS team. We utilize state-of-the-art models combined with several data augmentation, multi-task and transfer learning approaches for the automatic speech recognition (ASR) and machine translation (MT) steps of our cascaded system. Moreover, we also explore the feasibility of a full end-to-end speech translation (ST) model in the case of very constrained amount of ground truth labeled data. Our best system achieves the best performance among all submitted systems for Congolese Swahili to English and French with BLEU scores 7.7 and 13.7 respectively, and the second best result for Coastal Swahili to English with BLEU score 14.9",
    "checked": true,
    "id": "7ce795787841dd029397622d7753ec0d3c9e98ce",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Pavel Denisov",
      "Manuel Mager",
      "Ngoc Thang Vu"
    ]
  },
  "https://aclanthology.org/2021.iwslt-1.22": {
    "title": "The USYD-JD Speech Translation System for IWSLT2021",
    "volume": "workshop",
    "abstract": "This paper describes the University of Sydney & JD’s joint submission of the IWSLT 2021 low resource speech translation task. We participated in the Swahili->English direction and got the best scareBLEU (25.3) score among all the participants. Our constrained system is based on a pipeline framework, i.e. ASR and NMT. We trained our models with the officially provided ASR and MT datasets. The ASR system is based on the open-sourced tool Kaldi and this work mainly explores how to make the most of the NMT models. To reduce the punctuation errors generated by the ASR model, we employ our previous work SlotRefine to train a punctuation correction model. To achieve better translation performance, we explored the most recent effective strategies, including back translation, knowledge distillation, multi-feature reranking, and transductive finetuning. For model structure, we tried auto-regressive and non-autoregressive models, respectively. In addition, we proposed two novel pre-train approaches, i.e. de-noising training and bidirectional training to fully exploit the data. Extensive experiments show that adding the above techniques consistently improves the BLEU scores, and the final submission system outperforms the baseline (Transformer ensemble model trained with the original parallel data) by approximately 10.8 BLEU score, achieving the SOTA performance",
    "checked": true,
    "id": "6773573fe5e28b6d439c1cdf88d33166c00dc702",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Liang Ding",
      "Dacheng Tao"
    ]
  },
  "https://aclanthology.org/2021.iwslt-1.23": {
    "title": "mixSeq: A Simple Data Augmentation Methodfor Neural Machine Translation",
    "volume": "workshop",
    "abstract": "Data augmentation, which refers to manipulating the inputs (e.g., adding random noise,masking specific parts) to enlarge the dataset,has been widely adopted in machine learning. Most data augmentation techniques operate on a single input, which limits the diversity of the training corpus. In this paper, we propose a simple yet effective data augmentation technique for neural machine translation, mixSeq, which operates on multiple inputs and their corresponding targets. Specifically, we randomly select two input sequences,concatenate them together as a longer input aswell as their corresponding target sequencesas an enlarged target, and train models on theaugmented dataset. Experiments on nine machine translation tasks demonstrate that such asimple method boosts the baselines by a non-trivial margin. Our method can be further combined with single input based data augmentation methods to obtain further improvements",
    "checked": true,
    "id": "dba443fef956452fba9754b0be1ed8312e1b467f",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Xueqing Wu",
      "Yingce Xia",
      "Jinhua Zhu",
      "Lijun Wu",
      "Shufang Xie",
      "Yang Fan",
      "Tao Qin"
    ]
  },
  "https://aclanthology.org/2021.iwslt-1.24": {
    "title": "On Knowledge Distillation for Translating Erroneous Speech Transcriptions",
    "volume": "workshop",
    "abstract": "Recent studies argue that knowledge distillation is promising for speech translation (ST) using end-to-end models. In this work, we investigate the effect of knowledge distillation with a cascade ST using automatic speech recognition (ASR) and machine translation (MT) models. We distill knowledge from a teacher model based on human transcripts to a student model based on erroneous transcriptions. Our experimental results demonstrated that knowledge distillation is beneficial for a cascade ST. Further investigation that combined knowledge distillation and fine-tuning revealed that the combination consistently improved two language pairs: English-Italian and Spanish-English",
    "checked": true,
    "id": "59cd48f53961607d57f2282d21172b285251d8a4",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryo Fukuda",
      "Katsuhito Sudoh",
      "Satoshi Nakamura"
    ]
  },
  "https://aclanthology.org/2021.iwslt-1.25": {
    "title": "Self-Guided Curriculum Learning for Neural Machine Translation",
    "volume": "workshop",
    "abstract": "In supervised learning, a well-trained model should be able to recover ground truth accurately, i.e. the predicted labels are expected to resemble the ground truth labels as much as possible. Inspired by this, we formulate a difficulty criterion based on the recovery degrees of training examples. Motivated by the intuition that after skimming through the training corpus, the neural machine translation (NMT) model “knows” how to schedule a suitable curriculum according to learning difficulty, we propose a self-guided curriculum learning strategy that encourages the NMT model to learn from easy to hard on the basis of recovery degrees. Specifically, we adopt sentence-level BLEU score as the proxy of recovery degree. Experimental results on translation benchmarks including WMT14 English-German and WMT17 Chinese-English demonstrate that our proposed method considerably improves the recovery degree, thus consistently improving the translation performance",
    "checked": true,
    "id": "c8ec022c3af2d7ed317c6c52274b9ed3089701fa",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Lei Zhou",
      "Liang Ding",
      "Kevin Duh",
      "Shinji Watanabe",
      "Ryohei Sasano",
      "Koichi Takeda"
    ]
  },
  "https://aclanthology.org/2021.iwslt-1.26": {
    "title": "Between Flexibility and Consistency: Joint Generation of Captions and Subtitles",
    "volume": "workshop",
    "abstract": "Speech translation (ST) has lately received growing interest for the generation of subtitles without the need for an intermediate source language transcription and timing (i.e. captions). However, the joint generation of source captions and target subtitles does not only bring potential output quality advantages when the two decoding processes inform each other, but it is also often required in multilingual scenarios. In this work, we focus on ST models which generate consistent captions-subtitles in terms of structure and lexical content. We further introduce new metrics for evaluating subtitling consistency. Our findings show that joint decoding leads to increased performance and consistency between the generated captions and subtitles while still allowing for sufficient flexibility to produce subtitles conforming to language-specific needs and norms",
    "checked": true,
    "id": "68678bd17b60ccbc58dba2ab3decc1868eaaf6fe",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Alina Karakanta",
      "Marco Gaido",
      "Matteo Negri",
      "Marco Turchi"
    ]
  },
  "https://aclanthology.org/2021.iwslt-1.27": {
    "title": "Large-Scale English-Japanese Simultaneous Interpretation Corpus: Construction and Analyses with Sentence-Aligned Data",
    "volume": "workshop",
    "abstract": "This paper describes the construction of a new large-scale English-Japanese Simultaneous Interpretation (SI) corpus and presents the results of its analysis. A portion of the corpus contains SI data from three interpreters with different amounts of experience. Some of the SI data were manually aligned with the source speeches at the sentence level. Their latency, quality, and word order aspects were compared among the SI data themselves as well as against offline translations. The results showed that (1) interpreters with more experience controlled the latency and quality better, and (2) large latency hurt the SI quality",
    "checked": true,
    "id": "addc599a8a27bfcfc02f43ffa21543ae31a8e314",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Kosuke Doi",
      "Katsuhito Sudoh",
      "Satoshi Nakamura"
    ]
  },
  "https://aclanthology.org/2021.iwslt-1.28": {
    "title": "Inverted Projection for Robust Speech Translation",
    "volume": "workshop",
    "abstract": "Traditional translation systems trained on written documents perform well for text-based translation but not as well for speech-based applications. We aim to adapt translation models to speech by introducing actual lexical errors from ASR and segmentation errors from automatic punctuation into our translation training data. We introduce an inverted projection approach that projects automatically detected system segments onto human transcripts and then re-segments the gold translations to align with the projected human transcripts. We demonstrate that this overcomes the train-test mismatch present in other training approaches. The new projection approach achieves gains of over 1 BLEU point over a baseline that is exposed to the human transcripts and segmentations, and these gains hold for both IWSLT data and YouTube data",
    "checked": true,
    "id": "8b98a36d28fcdd6c8a2b3427b1c66e90fc0dd96b",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dirk Padfield",
      "Colin Cherry"
    ]
  },
  "https://aclanthology.org/2021.iwslt-1.29": {
    "title": "Towards the evaluation of automatic simultaneous speech translation from a communicative perspective",
    "volume": "workshop",
    "abstract": "In recent years, automatic speech-to-speech and speech-to-text translation has gained momentum thanks to advances in artificial intelligence, especially in the domains of speech recognition and machine translation. The quality of such applications is commonly tested with automatic metrics, such as BLEU, primarily with the goal of assessing improvements of releases or in the context of evaluation campaigns. However, little is known about how the output of such systems is perceived by end users or how they compare to human performances in similar communicative tasks. In this paper, we present the results of an experiment aimed at evaluating the quality of a real-time speech translation engine by comparing it to the performance of professional simultaneous interpreters. To do so, we adopt a framework developed for the assessment of human interpreters and use it to perform a manual evaluation on both human and machine performances. In our sample, we found better performance for the human interpreters in terms of intelligibility, while the machine performs slightly better in terms of informativeness. The limitations of the study and the possible enhancements of the chosen framework are discussed. Despite its intrinsic limitations, the use of this framework represents a first step towards a user-centric and communication-oriented methodology for evaluating real-time automatic speech translation",
    "checked": true,
    "id": "681552646f709a72ec0d9168330909657f647b39",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Claudio Fantinuoli",
      "Bianca Prandi"
    ]
  },
  "https://aclanthology.org/2021.iwslt-1.30": {
    "title": "Tag Assisted Neural Machine Translation of Film Subtitles",
    "volume": "workshop",
    "abstract": "We implemented a neural machine translation system that uses automatic sequence tagging to improve the quality of translation. Instead of operating on unannotated sentence pairs, our system uses pre-trained tagging systems to add linguistic features to source and target sentences. Our proposed neural architecture learns a combined embedding of tokens and tags in the encoder, and simultaneous token and tag prediction in the decoder. Compared to a baseline with unannotated training, this architecture increased the BLEU score of German to English film subtitle translation outputs by 1.61 points using named entity tags; however, the BLEU score decreased by 0.38 points using part-of-speech tags. This demonstrates that certain token-level tag outputs from off-the-shelf tagging systems can improve the output of neural translation systems using our combined embedding and simultaneous decoding extensions",
    "checked": true,
    "id": "e81b9afa2656af374a4369b54ee0de81fcd7188e",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Aren Siekmeier",
      "WonKee Lee",
      "Hongseok Kwon",
      "Jong-Hyeok Lee"
    ]
  },
  "https://aclanthology.org/2021.iwslt-1.31": {
    "title": "A Statistical Extension of Byte-Pair Encoding",
    "volume": "workshop",
    "abstract": "Sub-word segmentation is currently a standard tool for training neural machine translation (MT) systems and other NLP tasks. The goal is to split words (both in the source and target languages) into smaller units which then constitute the input and output vocabularies of the MT system. The aim of reducing the size of the input and output vocabularies is to increase the generalization capabilities of the translation model, enabling the system to translate and generate infrequent and new (unseen) words at inference time by combining previously seen sub-word units. Ideally, we would expect the created units to have some linguistic meaning, so that words are created in a compositional way. However, the most popular word-splitting method, Byte-Pair Encoding (BPE), which originates from the data compression literature, does not include explicit criteria to favor linguistic splittings nor to find the optimal sub-word granularity for the given training data. In this paper, we propose a statistically motivated extension of the BPE algorithm and an effective convergence criterion that avoids the costly experimentation cycle needed to select the best sub-word vocabulary size. Experimental results with morphologically rich languages show that our model achieves nearly-optimal BLEU scores and produces morphologically better word segmentations, which allows to outperform BPE’s generalization in the translation of sentences containing new words, as shown via human evaluation",
    "checked": true,
    "id": "1c59de25af45cef20d846ec7454251e8237d45d1",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "David Vilar",
      "Marcello Federico"
    ]
  },
  "https://aclanthology.org/2021.iwslt-1.32": {
    "title": "Integrated Training for Sequence-to-Sequence Models Using Non-Autoregressive Transformer",
    "volume": "workshop",
    "abstract": "Complex natural language applications such as speech translation or pivot translation traditionally rely on cascaded models. However,cascaded models are known to be prone to error propagation and model discrepancy problems. Furthermore, there is no possibility of using end-to-end training data in conventional cascaded systems, meaning that the training data most suited for the task cannot be used.Previous studies suggested several approaches for integrated end-to-end training to overcome those problems, however they mostly rely on(synthetic or natural) three-way data. We propose a cascaded model based on the non-autoregressive Transformer that enables end-to-end training without the need for an explicit intermediate representation. This new architecture (i) avoids unnecessary early decisions that can cause errors which are then propagated throughout the cascaded models and (ii) utilizes the end-to-end training data directly. We conduct an evaluation on two pivot-based machine translation tasks, namely French→German and German→Czech. Our experimental results show that the proposed architecture yields an improvement of more than 2 BLEU for French→German over the cascaded baseline",
    "checked": true,
    "id": "000d465229ff00058d15a372c8e9249ee8ec8220",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Evgeniia Tokarchuk",
      "Jan Rosendahl",
      "Weiyue Wang",
      "Pavel Petrushkov",
      "Tomer Lancewicki",
      "Shahram Khadivi",
      "Hermann Ney"
    ]
  },
  "https://aclanthology.org/2021.iwslt-1.33": {
    "title": "Data Augmentation by Concatenation for Low-Resource Translation: A Mystery and a Solution",
    "volume": "workshop",
    "abstract": "In this paper, we investigate the driving factors behind concatenation, a simple but effective data augmentation method for low-resource neural machine translation. Our experiments suggest that discourse context is unlikely the cause for concatenation improving BLEU by about +1 across four language pairs. Instead, we demonstrate that the improvement comes from three other factors unrelated to discourse: context diversity, length diversity, and (to a lesser extent) position shifting",
    "checked": true,
    "id": "bd1e84abd521b88d152f19fccbe876ce426ea96f",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Toan Q. Nguyen",
      "Kenton Murray",
      "David Chiang"
    ]
  },
  "https://aclanthology.org/2021.lchange-1.1": {
    "title": "Time-Aware Ancient Chinese Text Translation and Inference",
    "volume": "workshop",
    "abstract": "In this paper, we aim to address the challenges surrounding the translation of ancient Chinese text: (1) The linguistic gap due to the difference in eras results in translations that are poor in quality, and (2) most translations are missing the contextual information that is often very crucial to understanding the text. To this end, we improve upon past translation techniques by proposing the following: We reframe the task as a multi-label prediction task where the model predicts both the translation and its particular era. We observe that this helps to bridge the linguistic gap as chronological context is also used as auxiliary information. We validate our framework on a parallel corpus annotated with chronology information and show experimentally its efficacy in producing quality translation outputs. We release both the code and the data for future research",
    "checked": true,
    "id": "1f274b4036eb99e93945f266c227a359e5e25569",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Ernie Chang",
      "Yow-Ting Shiue",
      "Hui-Syuan Yeh",
      "Vera Demberg"
    ]
  },
  "https://aclanthology.org/2021.lchange-1.2": {
    "title": "Three-part diachronic semantic change dataset for Russian",
    "volume": "workshop",
    "abstract": "We present a manually annotated lexical semantic change dataset for Russian: RuShiftEval. Its novelty is ensured by a single set of target words annotated for their diachronic semantic shifts across three time periods, while the previous work either used only two time periods, or different sets of target words. The paper describes the composition and annotation procedure for the dataset. In addition, it is shown how the ternary nature of RuShiftEval allows to trace specific diachronic trajectories: ‘changed at a particular time period and stable afterwards’ or ‘was changing throughout all time periods’. Based on the analysis of the submissions to the recent shared task on semantic change detection for Russian, we argue that correctly identifying such trajectories can be an interesting sub-task itself",
    "checked": true,
    "id": "8be3632a872b6784d5bb3bf0843463afc8df4838",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Andrey Kutuzov",
      "Lidia Pivovarova"
    ]
  },
  "https://aclanthology.org/2021.lchange-1.3": {
    "title": "The Corpora They Are a-Changing: a Case Study in Italian Newspapers",
    "volume": "workshop",
    "abstract": "The use of automatic methods for the study of lexical semantic change (LSC) has led to the creation of evaluation benchmarks. Benchmark datasets, however, are intimately tied to the corpus used for their creation questioning their reliability as well as the robustness of automatic methods. This contribution investigates these aspects showing the impact of unforeseen social and cultural dimensions. We also identify a set of additional issues (OCR quality, named entities) that impact the performance of the automatic methods, especially when used to discover LSC",
    "checked": true,
    "id": "b8211b01b53689f20273172f0ee58d54e6f25f09",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pierpaolo Basile",
      "Annalina Caputo",
      "Tommaso Caselli",
      "Pierluigi Cassotti",
      "Rossella Varvara"
    ]
  },
  "https://aclanthology.org/2021.lchange-1.4": {
    "title": "Linguistic change and historical periodization of Old Literary Finnish",
    "volume": "workshop",
    "abstract": "In this study, we have normalized and lemmatized an Old Literary Finnish corpus using a lemmatization model trained on texts from Agricola. We analyse the error types that occur and appear in different decades, and use word error rate (WER) and different error types as a proxy for measuring linguistic innovation and change. We show that the proposed approach works, and the errors are connected to accumulating changes and innovations, which also results in a continuous decrease in the accuracy of the model. The described error types also guide further work in improving these models, and document the currently observed issues. We also have trained word embeddings for four centuries of lemmatized Old Literary Finnish, which are available on Zenodo",
    "checked": true,
    "id": "a0808352153afbd1eced0da562edd6f0ddb525bd",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Niko Partanen",
      "Khalid Alnajjar",
      "Mika Hämäläinen",
      "Jack Rueter"
    ]
  },
  "https://aclanthology.org/2021.lchange-1.5": {
    "title": "A diachronic evaluation of gender asymmetry in euphemism",
    "volume": "workshop",
    "abstract": "The use of euphemisms is a known driver of language change. It has been proposed that women use euphemisms more than men. Although there have been several studies investigating gender differences in language, the claim about euphemism usage has not been tested comprehensively through time. If women do use euphemisms more, this could mean that women also lead the formation of new euphemisms and language change over time. Using four large diachronic text corpora of English, we evaluate the claim that women use euphemisms more than men through a quantitative analysis. We assembled a list of 106 euphemism-taboo pairs to analyze their relative use through time by each gender in the corpora. Contrary to the existing belief, our results show that women do not use euphemisms with a higher proportion than men. We repeated the analysis using different subsets of the euphemism-taboo pairs list and found that our result was robust. Our study indicates that in a broad range of settings involving both speech and writing, and with varying degrees of formality, women do not use or form euphemisms more than men",
    "checked": true,
    "id": "b6fbb1a76cfe92ec3fe5f8c973ad448465f5e5eb",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Anna Kapron-King",
      "Yang Xu"
    ]
  },
  "https://aclanthology.org/2021.lchange-1.6": {
    "title": "The GLAUx corpus: methodological issues in designing a long-term, diverse, multi-layered corpus of Ancient Greek",
    "volume": "workshop",
    "abstract": "This paper describes the GLAUx project (“the Greek Language Automated”), an ongoing effort to develop a large long-term diachronic corpus of Greek, covering sixteen centuries of literary and non-literary material annotated with NLP methods. After providing an overview of related corpus projects and discussing the general architecture of the corpus, it zooms in on a number of larger methodological issues in the design of historical corpora. These include the encoding of textual variants, handling extralinguistic variation and annotating linguistic ambiguity. Finally, the long- and short-term perspectives of this project are discussed",
    "checked": true,
    "id": "5f7bc31e83e8eb386b7073a1dbedb090d78e2b36",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Alek Keersmaekers"
    ]
  },
  "https://aclanthology.org/2021.lchange-1.7": {
    "title": "Bhāṣācitra: Visualising the dialect geography of South Asia",
    "volume": "workshop",
    "abstract": "We present Bhāṣācitra, a dialect mapping system for South Asia built on a database of linguistic studies of languages of the region annotated for topic and location data. We analyse language coverage and look towards applications to typology by visualising example datasets. The application is not only meant to be useful for feature mapping, but also serves as a new kind of interactive bibliography for linguists of South Asian languages",
    "checked": true,
    "id": "4e1e196fd082f1e001c3a779b74893c24e1bfb30",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Aryaman Arora",
      "Adam Farris",
      "Gopalakrishnan R",
      "Samopriya Basu"
    ]
  },
  "https://aclanthology.org/2021.lchange-1.8": {
    "title": "Modeling the Evolution of Word Senses with Force-Directed Layouts of Co-occurrence Networks",
    "volume": "workshop",
    "abstract": "Languages evolve over time and the meaning of words can shift. Furthermore, individual words can have multiple senses. However, existing language models often only reflect one word sense per word and do not reflect semantic changes over time. While there are language models that can either model semantic change of words or multiple word senses, none of them cover both aspects simultaneously. We propose a novel force-directed graph layout algorithm to draw a network of frequently co-occurring words. In this way, we are able to use the drawn graph to visualize the evolution of word senses. In addition, we hope that jointly modeling semantic change and multiple senses of words results in improvements for the individual tasks",
    "checked": true,
    "id": "a8bc9f074d1a97535a54a426a8f617ef980a6149",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Reke",
      "Robert Schwanhold",
      "Ralf Krestel"
    ]
  },
  "https://aclanthology.org/2021.lchange-1.9": {
    "title": "Tracking Semantic Change in Cognate Sets for English and Romance Languages",
    "volume": "workshop",
    "abstract": "Semantic divergence in related languages is a key concern of historical linguistics. We cross-linguistically investigate the semantic divergence of cognate pairs in English and Romance languages, by means of word embeddings. To this end, we introduce a new curated dataset of cognates in all pairs of those languages. We describe the types of errors that occurred during the automated cognate identification process and manually correct them. Additionally, we label the English cognates according to their etymology, separating them into two groups: old borrowings and recent borrowings. On this curated dataset, we analyse word properties such as frequency and polysemy, and the distribution of similarity scores between cognate sets in different languages. We automatically identify different clusters of English cognates, setting a new direction of research in cognates, borrowings and possibly false friends analysis in related languages",
    "checked": true,
    "id": "a2b4ae0c24a81c50647bb09a9813358919151d11",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Ana Sabina Uban",
      "Alina Maria Cristea",
      "Anca Dinu",
      "Liviu P. Dinu",
      "Simona Georgescu",
      "Laurentiu Zoicas"
    ]
  },
  "https://aclanthology.org/2021.metanlp-1.1": {
    "title": "Meta-Reinforcement Learning for Mastering Multiple Skills and Generalizing across Environments in Text-based Games",
    "volume": "workshop",
    "abstract": "Text-based games can be used to develop task-oriented text agents for accomplishing tasks with high-level language instructions, which has potential applications in domains such as human-robot interaction. Given a text instruction, reinforcement learning is commonly used to train agents to complete the intended task owing to its convenience of learning policies automatically. However, because of the large space of combinatorial text actions, learning a policy network that generates an action word by word with reinforcement learning is challenging. Recent research works show that imitation learning provides an effective way of training a generation-based policy network. However, trained agents with imitation learning are hard to master a wide spectrum of task types or skills, and it is also difficult for them to generalize to new environments. In this paper, we propose a meta reinforcement learning based method to train text agents through learning-to-explore. In particular, the text agent first explores the environment to gather task-specific information and then adapts the execution policy for solving the task with this information. On the publicly available testbed ALFWorld, we conducted a comparison study with imitation learning and show the superiority of our method",
    "checked": true,
    "id": "ff8ae25ec5c1306619478a96d465751ff52ed29c",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Zhenjie Zhao",
      "Mingfei Sun",
      "Xiaojuan Ma"
    ]
  },
  "https://aclanthology.org/2021.metanlp-1.2": {
    "title": "Soft Layer Selection with Meta-Learning for Zero-Shot Cross-Lingual Transfer",
    "volume": "workshop",
    "abstract": "Multilingual pre-trained contextual embedding models (Devlin et al., 2019) have achieved impressive performance on zero-shot cross-lingual transfer tasks. Finding the most effective fine-tuning strategy to fine-tune these models on high-resource languages so that it transfers well to the zero-shot languages is a non-trivial task. In this paper, we propose a novel meta-optimizer to soft-select which layers of the pre-trained model to freeze during fine-tuning. We train the meta-optimizer by simulating the zero-shot transfer scenario. Results on cross-lingual natural language inference show that our approach improves over the simple fine-tuning baseline and X-MAML (Nooralahzadeh et al., 2020)",
    "checked": true,
    "id": "0027287f577e70b029203daaf9728e8f5fc33570",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Weijia Xu",
      "Batool Haider",
      "Jason Krone",
      "Saab Mansour"
    ]
  },
  "https://aclanthology.org/2021.metanlp-1.3": {
    "title": "Zero-Shot Compositional Concept Learning",
    "volume": "workshop",
    "abstract": "In this paper, we study the problem of recognizing compositional attribute-object concepts within the zero-shot learning (ZSL) framework. We propose an episode-based cross-attention (EpiCA) network which combines merits of cross-attention mechanism and episode-based training strategy to recognize novel compositional concepts. Firstly, EpiCA bases on cross-attention to correlate conceptvisual information and utilizes the gated pooling layer to build contextualized representations for both images and concepts. The updated representations are used for a more indepth multi-modal relevance calculation for concept recognition. Secondly, a two-phase episode training strategy, especially the ransductive phase, is adopted to utilize unlabeled test examples to alleviate the low-resource learning problem. Experiments on two widelyused zero-shot compositional learning (ZSCL) benchmarks have demonstrated the effectiveness of the model compared with recent approaches on both conventional and generalized ZSCL settings",
    "checked": true,
    "id": "10b809531cdd20b05274adcffcd4aa927f1fe54c",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Guangyue Xu",
      "Parisa Kordjamshidi",
      "Joyce Chai"
    ]
  },
  "https://aclanthology.org/2021.metanlp-1.4": {
    "title": "Multi-Pair Text Style Transfer for Unbalanced Data via Task-Adaptive Meta-Learning",
    "volume": "workshop",
    "abstract": "Text-style transfer aims to convert text given in one domain into another by paraphrasing the sentence or substituting the keywords without altering the content. By necessity, state-of-the-art methods have evolved to accommodate nonparallel training data, as it is frequently the case there are multiple data sources of unequal size, with a mixture of labeled and unlabeled sentences. Moreover, the inherent style defined within each source might be distinct. A generic bidirectional (e.g., formal ⇔ informal) style transfer regardless of different groups may not generalize well to different applications. In this work, we developed a task adaptive meta-learning framework that can simultaneously perform a multi-pair text-style transfer using a single model. The proposed method can adaptively balance the difference of meta-knowledge across multiple tasks. Results show that our method leads to better quantitative performance as well as coherent style variations. Common challenges of unbalanced data and mismatched domains are handled well by this method",
    "checked": true,
    "id": "92fb9e98468a5e7c0890e284ce6ff435094a2fc2",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Xing Han",
      "Jessica Lundin"
    ]
  },
  "https://aclanthology.org/2021.metanlp-1.5": {
    "title": "On the cross-lingual transferability of multilingual prototypical models across NLU tasks",
    "volume": "workshop",
    "abstract": "Supervised deep learning-based approaches have been applied to task-oriented dialog and have proven to be effective for limited domain and language applications when a sufficient number of training examples are available. In practice, these approaches suffer from the drawbacks of domain-driven design and under-resourced languages. Domain and language models are supposed to grow and change as the problem space evolves. On one hand, research on transfer learning has demonstrated the cross-lingual ability of multilingual Transformers-based models to learn semantically rich representations. On the other, in addition to the above approaches, meta-learning have enabled the development of task and language learning algorithms capable of far generalization. Through this context, this article proposes to investigate the cross-lingual transferability of using synergistically few-shot learning with prototypical neural networks and multilingual Transformers-based models. Experiments in natural language understanding tasks on MultiATIS++ corpus shows that our approach substantially improves the observed transfer learning performances between the low and the high resource languages. More generally our approach confirms that the meaningful latent space learned in a given language can be can be generalized to unseen and under-resourced ones using meta-learning",
    "checked": true,
    "id": "30e3c4a3e38e42c515b9dcc7c4ac19f7db430a17",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Oralie Cattan",
      "Sophie Rosset",
      "Christophe Servan"
    ]
  },
  "https://aclanthology.org/2021.metanlp-1.6": {
    "title": "Meta-Learning for Few-Shot Named Entity Recognition",
    "volume": "workshop",
    "abstract": "Meta-learning has recently been proposed to learn models and algorithms that can generalize from a handful of examples. However, applications to structured prediction and textual tasks pose challenges for meta-learning algorithms. In this paper, we apply two meta-learning algorithms, Prototypical Networks and Reptile, to few-shot Named Entity Recognition (NER), including a method for incorporating language model pre-training and Conditional Random Fields (CRF). We propose a task generation scheme for converting classical NER datasets into the few-shot setting, for both training and evaluation. Using three public datasets, we show these meta-learning algorithms outperform a reasonable fine-tuned BERT baseline. In addition, we propose a novel combination of Prototypical Networks and Reptile",
    "checked": true,
    "id": "04320b48470a2965f72a31b80fcf09047c55cd38",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Cyprien de Lichy",
      "Hadrien Glaude",
      "William Campbell"
    ]
  },
  "https://aclanthology.org/2021.metanlp-1.7": {
    "title": "Multi-accent Speech Separation with One Shot Learning",
    "volume": "workshop",
    "abstract": "Speech separation is a problem in the field of speech processing that has been studied in full swing recently. However, there has not been much work studying a multi-accent speech separation scenario. Unseen speakers with new accents and noise aroused the domain mismatch problem which cannot be easily solved by conventional joint training methods. Thus, we applied MAML and FOMAML to tackle this problem and obtained higher average Si-SNRi values than joint training on almost all the unseen accents. This proved that these two methods do have the ability to generate well-trained parameters for adapting to speech mixtures of new speakers and accents. Furthermore, we found out that FOMAML obtains similar performance compared to MAML while saving a lot of time",
    "checked": true,
    "id": "b50d9d93ca198fd48dc7c1fdc59c2812feb8353b",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Kuan Po Huang",
      "Yuan-Kuei Wu",
      "Hung-yi Lee"
    ]
  },
  "https://aclanthology.org/2021.metanlp-1.8": {
    "title": "Semi-supervised Meta-learning for Cross-domain Few-shot Intent Classification",
    "volume": "workshop",
    "abstract": "Meta learning aims to optimize the model’s capability to generalize to new tasks and domains. Lacking a data-efficient way to create meta training tasks has prevented the application of meta-learning to the real-world few shot learning scenarios. Recent studies have proposed unsupervised approaches to create meta-training tasks from unlabeled data for free, e.g., the SMLMT method (Bansal et al., 2020a) constructs unsupervised multi-class classification tasks from the unlabeled text by randomly masking words in the sentence and let the meta learner choose which word to fill in the blank. This study proposes a semi-supervised meta-learning approach that incorporates both the representation power of large pre-trained language models and the generalization capability of prototypical networks enhanced by SMLMT. The semi-supervised meta training approach avoids overfitting prototypical networks on a small number of labeled training examples and quickly learns cross-domain task-specific representation only from a few supporting examples. By incorporating SMLMT with prototypical networks, the meta learner generalizes better to unseen domains and gains higher accuracy on out-of-scope examples without the heavy lifting of pre-training. We observe significant improvement in few-shot generalization after training only a few epochs on the intent classification tasks evaluated in a multi-domain setting",
    "checked": true,
    "id": "4302276c6fe5e01ddde123e97ffdc64bf8c33b81",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Yue Li",
      "Jiong Zhang"
    ]
  },
  "https://aclanthology.org/2021.metanlp-1.9": {
    "title": "Meta-learning for Classifying Previously Unseen Data Source into Previously Unseen Emotional Categories",
    "volume": "workshop",
    "abstract": "In this paper, we place ourselves in a classification scenario in which the target classes and data type are not accessible during training. We use a meta-learning approach to determine whether or not meta-trained information from common social network data with fine-grained emotion labels can achieve competitive performance on messages labeled with different emotion categories. We leverage few-shot learning to match with the classification scenario and consider metric learning based meta-learning by setting up Prototypical Networks with a Transformer encoder, trained in an episodic fashion. This approach proves to be effective for capturing meta-information from a source emotional tag set to predict previously unseen emotional tags. Even though shifting the data type triggers an expected performance drop, our meta-learning approach achieves decent results when compared to the fully supervised one",
    "checked": true,
    "id": "924c3d9bd1525c08e640e922e9fe910e44163723",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaël Guibon",
      "Matthieu Labeau",
      "Hélène Flamein",
      "Luce Lefeuvre",
      "Chloé Clavel"
    ]
  },
  "https://aclanthology.org/2021.mwe-1.1": {
    "title": "A Long Hard Look at MWEs in the Age of Language Models",
    "volume": "workshop",
    "abstract": "In recent years, language models (LMs) have become almost synonymous with NLP. Pre-trained to “read” a large text corpus, such models are useful as both a representation layer as well as a source of world knowledge. But how well do they represent MWEs? This talk will discuss various problems in representing MWEs, and the extent to which LMs address them: • Do LMs capture the implicit relationship between constituents in compositional MWEs (from baby oil through parsley cake to cheeseburger stabbing)? • Do LMs recognize when words are used nonliterally in non-compositional MWEs (e.g. do they know whether there are fleas in the flea market)? • Do LMs know idioms, and can they infer the meaning of new idioms from the context as humans often do?",
    "checked": true,
    "id": "72b441b9953ba148892d4f100afda77b251bd863",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Vered Shwartz"
    ]
  },
  "https://aclanthology.org/2021.mwe-1.2": {
    "title": "Where Do Aspectual Variants of Light Verb Constructions Belong?",
    "volume": "workshop",
    "abstract": "Expressions with an aspectual variant of a light verb, e.g. ‘take on debt’ vs. ‘have debt’, are frequent in texts but often difficult to classify between verbal idioms, light verb constructions or compositional phrases. We investigate the properties of such expressions with a disputed membership and propose a selection of features that determine more satisfactory boundaries between the three categories in this zone, assigning the expressions to one of them",
    "checked": true,
    "id": "df90d59a0dea2c4002232802741042243c3454a8",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Aggeliki Fotopoulou",
      "Eric Laporte",
      "Takuya Nakamura"
    ]
  },
  "https://aclanthology.org/2021.mwe-1.3": {
    "title": "Data-driven Identification of Idioms in Song Lyrics",
    "volume": "workshop",
    "abstract": "The automatic recognition of idioms poses a challenging problem for NLP applications. Whereas native speakers can intuitively handle multiword expressions whose compositional meanings are hard to trace back to individual word semantics, there is still ample scope for improvement regarding computational approaches. We assume that idiomatic constructions can be characterized by gradual intensities of semantic non-compositionality, formal fixedness, and unusual usage context, and introduce a number of measures for these characteristics, comprising count-based and predictive collocation measures together with measures of context (un)similarity. We evaluate our approach on a manually labelled gold standard, derived from a corpus of German pop lyrics. To this end, we apply a Random Forest classifier to analyze the individual contribution of features for automatically detecting idioms, and study the trade-off between recall and precision. Finally, we evaluate the classifier on an independent dataset of idioms extracted from a list of Wikipedia idioms, achieving state-of-the art accuracy",
    "checked": true,
    "id": "e37e9a4e7274c9f7cadbec08fc58d86679009b23",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Miriam Amin",
      "Peter Fankhauser",
      "Marc Kupietz",
      "Roman Schneider"
    ]
  },
  "https://aclanthology.org/2021.mwe-1.4": {
    "title": "Contextualized Embeddings Encode Monolingual and Cross-lingual Knowledge of Idiomaticity",
    "volume": "workshop",
    "abstract": "Potentially idiomatic expressions (PIEs) are ambiguous between non-compositional idiomatic interpretations and transparent literal interpretations. For example, “hit the road” can have an idiomatic meaning corresponding to ‘start a journey’ or have a literal interpretation. In this paper we propose a supervised model based on contextualized embeddings for predicting whether usages of PIEs are idiomatic or literal. We consider monolingual experiments for English and Russian, and show that the proposed model outperforms previous approaches, including in the case that the model is tested on instances of PIE types that were not observed during training. We then consider cross-lingual experiments in which the model is trained on PIE instances in one language, English or Russian, and tested on the other language. We find that the model outperforms baselines in this setting. These findings suggest that contextualized embeddings are able to learn representations that encode knowledge of idiomaticity that is not restricted to specific expressions, nor to a specific language",
    "checked": true,
    "id": "c3fb860db2a9e5297e9d98cd0af37cb589dfeaa3",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Samin Fakharian",
      "Paul Cook"
    ]
  },
  "https://aclanthology.org/2021.mwe-1.5": {
    "title": "PIE: A Parallel Idiomatic Expression Corpus for Idiomatic Sentence Generation and Paraphrasing",
    "volume": "workshop",
    "abstract": "Idiomatic expressions (IE) play an important role in natural language, and have long been a “pain in the neck” for NLP systems. Despite this, text generation tasks related to IEs remain largely under-explored. In this paper, we propose two new tasks of idiomatic sentence generation and paraphrasing to fill this research gap. We introduce a curated dataset of 823 IEs, and a parallel corpus with sentences containing them and the same sentences where the IEs were replaced by their literal paraphrases as the primary resource for our tasks. We benchmark existing deep learning models, which have state-of-the-art performance on related tasks using automated and manual evaluation with our dataset to inspire further research on our proposed tasks. By establishing baseline models, we pave the way for more comprehensive and accurate modeling of IEs, both for generation and paraphrasing",
    "checked": true,
    "id": "789bf456ed9f36bb5a6b82ff23570d74c73f1889",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Jianing Zhou",
      "Hongyu Gong",
      "Suma Bhat"
    ]
  },
  "https://aclanthology.org/2021.mwe-1.6": {
    "title": "Lexical Semantic Recognition",
    "volume": "workshop",
    "abstract": "In lexical semantics, full-sentence segmentation and segment labeling of various phenomena are generally treated separately, despite their interdependence. We hypothesize that a unified lexical semantic recognition task is an effective way to encapsulate previously disparate styles of annotation, including multiword expression identification / classification and supersense tagging. Using the STREUSLE corpus, we train a neural CRF sequence tagger and evaluate its performance along various axes of annotation. As the label set generalizes that of previous tasks (PARSEME, DiMSUM), we additionally evaluate how well the model generalizes to those test sets, finding that it approaches or surpasses existing models despite training only on STREUSLE. Our work also establishes baseline models and evaluation metrics for integrated and accurate modeling of lexical semantics, facilitating future work in this area",
    "checked": true,
    "id": "a66fd96f3ea9473a5e7ff30eb5c876850492e7e1",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Nelson F. Liu",
      "Daniel Hershcovich",
      "Michael Kranzlein",
      "Nathan Schneider"
    ]
  },
  "https://aclanthology.org/2021.mwe-1.7": {
    "title": "Finding BERT's Idiomatic Key",
    "volume": "workshop",
    "abstract": "Sentence embeddings encode information relating to the usage of idioms in a sentence. This paper reports a set of experiments that combine a probing methodology with input masking to analyse where in a sentence this idiomatic information is taken from, and what form it takes. Our results indicate that BERT’s idiomatic key is primarily found within an idiomatic expression, but also draws on information from the surrounding context. Also, BERT can distinguish between the disruption in a sentence caused by words missing and the incongruity caused by idiomatic usage",
    "checked": true,
    "id": "cc3da99747cb3f63a57142eebd50f5b03b837c68",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Vasudevan Nedumpozhimana",
      "John Kelleher"
    ]
  },
  "https://aclanthology.org/2021.mwe-1.8": {
    "title": "Light Verb Constructions and Their Families - A Corpus Study on German ‘stehen unter'-LVCs",
    "volume": "workshop",
    "abstract": "The paper reports on a corpus study of German light verb constructions (LVCs). LVCs come in families which exemplify systematic interpretation patterns. The paper’s aim is to account for the properties determining these patterns on the basis of a corpus study on German LVCs of the type ‘stehen unter’ NP’ (‘stand under NP’)",
    "checked": true,
    "id": "e7a04218d8bfe2a6b87467f11d4868b707cd502f",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Jens Fleischhauer"
    ]
  },
  "https://aclanthology.org/2021.nlp4posimpact-1.1": {
    "title": "Restatement and Question Generation for Counsellor Chatbot",
    "volume": "workshop",
    "abstract": "Amidst rising mental health needs in society, virtual agents are increasingly deployed in counselling. In order to give pertinent advice, counsellors must first gain an understanding of the issues at hand by eliciting sharing from the counsellee. It is thus important for the counsellor chatbot to encourage the user to open up and talk. One way to sustain the conversation flow is to acknowledge the counsellee’s key points by restating them, or probing them further with questions. This paper applies models from two closely related NLP tasks — summarization and question generation — to restatement and question generation in the counselling context. We conducted experiments on a manually annotated dataset of Cantonese post-reply pairs on topics related to loneliness, academic anxiety and test anxiety. We obtained the best performance in both restatement and question generation by fine-tuning BertSum, a state-of-the-art summarization model, with the in-domain manual dataset augmented with a large-scale, automatically mined open-domain dataset",
    "checked": true,
    "id": "100e0f3dcd319266b2772f0841dad388b45cce3f",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "John Lee",
      "Baikun Liang",
      "Haley Fong"
    ]
  },
  "https://aclanthology.org/2021.nlp4posimpact-1.2": {
    "title": "The Climate Change Debate and Natural Language Processing",
    "volume": "workshop",
    "abstract": "The debate around climate change (CC)—its extent, its causes, and the necessary responses—is intense and of global importance. Yet, in the natural language processing (NLP) community, this domain has so far received little attention. In contrast, it is of enormous prominence in various social science disciplines, and some of that work follows the ”text-as-data” paradigm, seeking to employ quantitative methods for analyzing large amounts of CC-related text. Other research is qualitative in nature and studies details, nuances, actors, and motivations within CC discourses. Coming from both NLP and Political Science, and reviewing key works in both disciplines, we discuss how social science approaches to CC debates can inform advances in text-mining/NLP, and how, in return, NLP can support policy-makers and activists in making sense of large-scale and complex CC discourses across multiple genres, channels, topics, and communities. This is paramount for their ability to make rapid and meaningful impact on the discourse, and for shaping the necessary policy change",
    "checked": true,
    "id": "1d37460baded22f488085e82985419178679dce0",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Manfred Stede",
      "Ronny Patz"
    ]
  },
  "https://aclanthology.org/2021.nlp4posimpact-1.3": {
    "title": "Cartography of Natural Language Processing for Social Good (NLP4SG): Searching for Definitions, Statistics and White Spots",
    "volume": "workshop",
    "abstract": "The range of works that can be considered as developing NLP for social good (NLP4SG) is enormous. While many of them target the identification of hate speech or fake news, there are others that address, e.g., text simplification to alleviate consequences of dyslexia, or coaching strategies to fight depression. However, so far, there is no clear picture of what areas are targeted by NLP4SG, who are the actors, which are the main scenarios and what are the topics that have been left aside. In order to obtain a clearer view in this respect, we first propose a working definition of NLP4SG and identify some primary aspects that are crucial for NLP4SG, including, e.g., areas, ethics, privacy and bias. Then, we draw upon a corpus of around 50,000 articles downloaded from the ACL Anthology. Based on a list of keywords retrieved from the literature and revised in view of the task, we select from this corpus articles that can be considered to be on NLP4SG according to our definition and analyze them in terms of trends along the time line, etc. The result is a map of the current NLP4SG research and insights concerning the white spots on this map",
    "checked": true,
    "id": "4975c64466149c72f31489fadbbbff4e85d7b3f3",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Paula Fortuna",
      "Laura Pérez-Mayos",
      "Ahmed AbuRa’ed",
      "Juan Soler-Company",
      "Leo Wanner"
    ]
  },
  "https://aclanthology.org/2021.nlp4posimpact-1.4": {
    "title": "Guiding Principles for Participatory Design-inspired Natural Language Processing",
    "volume": "workshop",
    "abstract": "We introduce 9 guiding principles to integrate Participatory Design (PD) methods in the development of Natural Language Processing (NLP) systems. The adoption of PD methods by NLP will help to alleviate issues concerning the development of more democratic, fairer, less-biased technologies to process natural language data. This short paper is the outcome of an ongoing dialogue between designers and NLP experts and adopts a non-standard format following previous work by Traum (2000); Bender (2013); Abzianidze and Bos (2019). Every section is a guiding principle. While principles 1–3 illustrate assumptions and methods that inform community-based PD practices, we used two fictional design scenarios (Encinas and Blythe, 2018), which build on top of situations familiar to the authors, to elicit the identification of the other 6. Principles 4–6 describes the impact of PD methods on the design of NLP systems, targeting two critical aspects: data collection & annotation, and the deployment & evaluation. Finally, principles 7–9 guide a new reflexivity of the NLP research with respect to its context, actors and participants, and aims. We hope this guide will offer inspiration and a road-map to develop a new generation of PD-inspired NLP",
    "checked": true,
    "id": "022b80b663c51563a1c6772c12ada3c79f5d798d",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Tommaso Caselli",
      "Roberto Cibin",
      "Costanza Conforti",
      "Enrique Encinas",
      "Maurizio Teli"
    ]
  },
  "https://aclanthology.org/2021.nlp4posimpact-1.5": {
    "title": "Theano: A Greek-speaking conversational agent for COVID-19",
    "volume": "workshop",
    "abstract": "Conversational Agents (CAs) can be a proxy for disseminating information and providing support to the public, especially in times of crisis. CAs can scale to reach larger numbers of end-users than human operators, while they can offer information interactively and engagingly. In this work, we present Theano, a Greek-speaking virtual assistant for COVID-19. Theano presents users with COVID-19 statistics and facts and informs users about the best health practices as well as the latest COVID-19 related guidelines. Additionally, Theano provides support to end-users by helping them self-assess their symptoms and redirecting them to first-line health workers. The relevant, localized information that Theano provides, makes it a valuable tool for combating COVID-19 in Greece. Theano has already conversed with different users in more than 170 different conversations through a web interface as a chatbot and over the phone as a voice bot",
    "checked": true,
    "id": "d4eb2ca9694f34d63abe6d27bd2d958992431017",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Nikoletta Ventoura",
      "Kosmas Palios",
      "Yannis Vasilakis",
      "Georgios Paraskevopoulos",
      "Nassos Katsamanis",
      "Vassilis Katsouros"
    ]
  },
  "https://aclanthology.org/2021.nlp4posimpact-1.6": {
    "title": "Are we human, or are we users? The role of natural language processing in human-centric news recommenders that nudge users to diverse content",
    "volume": "workshop",
    "abstract": "In this position paper, we present a research agenda and ideas for facilitating exposure to diverse viewpoints in news recommendation. Recommending news from diverse viewpoints is important to prevent potential filter bubble effects in news consumption, and stimulate a healthy democratic debate.To account for the complexity that is inherent to humans as citizens in a democracy, we anticipate (among others) individual-level differences in acceptance of diversity. We connect this idea to techniques in Natural Language Processing, where distributional language models would allow us to place different users and news articles in a multidimensional space based on semantic content, where diversity is operationalized as distance and variance. In this way, we can model individual “latitudes of diversity” for different users, and thus personalize viewpoint diversity in support of a healthy public debate. In addition, we identify technical, ethical and conceptual issues related to our presented ideas. Our investigation describes how NLP can play a central role in diversifying news recommendations",
    "checked": true,
    "id": "9995132dda17b36e5513c8e98d58ff992d0ba79a",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Myrthe Reuver",
      "Nicolas Mattis",
      "Marijn Sax",
      "Suzan Verberne",
      "Nava Tintarev",
      "Natali Helberger",
      "Judith Moeller",
      "Sanne Vrijenhoek",
      "Antske Fokkens",
      "Wouter van Atteveldt"
    ]
  },
  "https://aclanthology.org/2021.nlp4posimpact-1.7": {
    "title": "Automatic Sentence Simplification in Low Resource Settings for Urdu",
    "volume": "workshop",
    "abstract": "To build automated simplification systems, corpora of complex sentences and their simplified versions is the first step to understand sentence complexity and enable the development of automatic text simplification systems. We present a lexical and syntactically simplified Urdu simplification corpus with a detailed analysis of the various simplification operations and human evaluation of corpus quality. We further analyze our corpora using text readability measures and present a comparison of the original, lexical simplified and syntactically simplified corpora. In addition, we compare our corpus with other existing simplification corpora by building simplification systems and evaluating these systems using BLEU and SARI scores. Our system achieves the highest BLEU score and comparable SARI score in comparison to other systems. We release our simplification corpora for the benefit of the research community",
    "checked": true,
    "id": "d6a25d8726c5484bb224a3350528aae9fcaae65f",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Yusra Anees",
      "Sadaf Abdul Rauf"
    ]
  },
  "https://aclanthology.org/2021.nlp4posimpact-1.8": {
    "title": "Challenges for Information Extraction from Dialogue in Criminal Law",
    "volume": "workshop",
    "abstract": "Information extraction and question answering have the potential to introduce a new paradigm for how machine learning is applied to criminal law. Existing approaches generally use tabular data for predictive metrics. An alternative approach is needed for matters of equitable justice, where individuals are judged on a case-by-case basis, in a process involving verbal or written discussion and interpretation of case factors. Such discussions are individualized, but they nonetheless rely on underlying facts. Information extraction can play an important role in surfacing these facts, which are still important to understand. We analyze unsupervised, weakly supervised, and pre-trained models’ ability to extract such factual information from the free-form dialogue of California parole hearings. With a few exceptions, most F1 scores are below 0.85. We use this opportunity to highlight some opportunities for further research for information extraction and question answering. We encourage new developments in NLP to enable analysis and review of legal cases to be done in a post-hoc, not predictive, manner",
    "checked": true,
    "id": "03c046041bc509f2cc9671ee71a78642275b77c3",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Jenny Hong",
      "Catalin Voss",
      "Christopher Manning"
    ]
  },
  "https://aclanthology.org/2021.nlp4posimpact-1.9": {
    "title": "Detecting Hashtag Hijacking for Hashtag Activism",
    "volume": "workshop",
    "abstract": "Social media has changed the way we engage in social activities. On Twitter, users can participate in social movements using hashtags such as #MeToo; this is known as hashtag activism. However, while these hashtags can help reshape social norms, they can also be used maliciously by spammers or troll communities for other purposes, such as signal boosting unrelated content, making a dent in a movement, or sharing hate speech. We present a Tweet-level hashtag hijacking detection framework focusing on hashtag activism. Our weakly-supervised framework uses bootstrapping to update itself as new Tweets are posted. Our experiments show that the system adapts to new topics in a social movement, as well as new hijacking strategies, maintaining strong performance over time",
    "checked": true,
    "id": "203bdaca3986b51f8d011422c04ff1489e425ce5",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Pooneh Mousavi",
      "Jessica Ouyang"
    ]
  },
  "https://aclanthology.org/2021.nlp4posimpact-1.10": {
    "title": "NLP for Consumer Protection: Battling Illegal Clauses in German Terms and Conditions in Online Shopping",
    "volume": "workshop",
    "abstract": "Online shopping is an ever more important part of the global consumer economy, not just in times of a pandemic. When we place an order online as consumers, we regularly agree to the so-called “Terms and Conditions” (T&C), a contract unilaterally drafted by the seller. Often, consumers do not read these contracts and unwittingly agree to unfavourable and often void terms. Government and non-government organisations (NGOs) for consumer protection battle such terms on behalf of consumers, who often hesitate to take on legal actions themselves. However, the growing number of online shops and a lack of funding makes it increasingly difficult for such organisations to monitor the market effectively. This paper describes how Natural Language Processing (NLP) can be applied to support consumer advocates in their efforts to protect consumers. Together with two NGOs from Germany, we developed an NLP-based application that legally assesses clauses in T&C from German online shops under the European Union’s (EU) jurisdiction. We report that we could achieve an accuracy of 0.9 in the detection of void clauses by fine-tuning a pre-trained German BERT model. The approach is currently used by two NGOs and has already helped to challenge void clauses in T&C",
    "checked": true,
    "id": "9e1616dcabf4d04d14d642fcb7963c461cf13d41",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Daniel Braun",
      "Florian Matthes"
    ]
  },
  "https://aclanthology.org/2021.nlp4posimpact-1.11": {
    "title": "A Research Framework for Understanding Education-Occupation Alignment with NLP Techniques",
    "volume": "workshop",
    "abstract": "Understanding the gaps between job requirements and university curricula is crucial for improving student success and institutional effectiveness in higher education. In this context, natural language processing (NLP) can be leveraged to generate granular insights into where the gaps are and how they change. This paper proposes a three-dimensional research framework that combines NLP techniques with economic and educational research to quantify the alignment between course syllabi and job postings. We elaborate on key technical details of the framework and further discuss its potential positive impacts on practice, including unveiling the inequalities in and long-term consequences of education-occupation alignment to inform policymakers, and fostering information systems to support students, institutions and employers in the school-to-work pipeline",
    "checked": true,
    "id": "40141f0933b5111b089049e226dc8d969b0a7fca",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Renzhe Yu",
      "Subhro Das",
      "Sairam Gurajada",
      "Kush Varshney",
      "Hari Raghavan",
      "Carlos Lastra-Anadon"
    ]
  },
  "https://aclanthology.org/2021.nlp4posimpact-1.12": {
    "title": "Dialogue Act Classification for Augmentative and Alternative Communication",
    "volume": "workshop",
    "abstract": "Augmentative and Alternative Communication (AAC) devices and applications are intended to make it easier for individuals with complex communication needs to participate in conversations. However, these devices have low adoption and retention rates. We review prior work with text recommendation systems that have not been successful in mitigating these problems. To address these gaps, we propose applying Dialogue Act classification to AAC conversations. We evaluated the performance of a state of the art model on a limited AAC dataset that was trained on both AAC and non-AAC datasets. The one trained on AAC (accuracy = 38.6%) achieved better performance than that trained on a non-AAC corpus (accuracy = 34.1%). These results reflect the need to incorporate representative datasets in later experiments. We discuss the need to collect more labeled AAC datasets and propose areas of future work",
    "checked": true,
    "id": "41ebff09aff17c37efdab8c1d7051cbf150970f8",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "E. Margaret Perkoff"
    ]
  },
  "https://aclanthology.org/2021.nlp4posimpact-1.13": {
    "title": "Improving Policing with Natural Language Processing",
    "volume": "workshop",
    "abstract": "This article explores the potential for Natural Language Processing (NLP) to enable a more effective, prevention focused and less confrontational policing model that has hitherto been too resource consuming to implement at scale. Problem-Oriented Policing (POP) is a potential replacement, at least in part, for traditional policing which adopts a reactive approach, relying heavily on the criminal justice system. By contrast, POP seeks to prevent crime by manipulating the underlying conditions that allow crimes to be committed. Identifying these underlying conditions requires a detailed understanding of crime events - tacit knowledge that is often held by police officers but which can be challenging to derive from structured police data. One potential source of insight exists in unstructured free text data commonly collected by police for the purposes of investigation or administration. Yet police agencies do not typically have the skills or resources to analyse these data at scale. In this article we argue that NLP offers the potential to unlock these unstructured data and by doing so allow police to implement more POP initiatives. However we caution that using NLP models without adequate knowledge may either allow or perpetuate bias within the data potentially leading to unfavourable outcomes",
    "checked": true,
    "id": "d393f2a793930a6e38321340185756860f43c62c",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Anthony Dixon",
      "Daniel Birks"
    ]
  },
  "https://aclanthology.org/2021.nlp4posimpact-1.14": {
    "title": "Empathy and Hope: Resource Transfer to Model Inter-country Social Media Dynamics",
    "volume": "workshop",
    "abstract": "The ongoing COVID-19 pandemic resulted in significant ramifications for international relations ranging from travel restrictions, global ceasefires, and international vaccine production and sharing agreements. Amidst a wave of infections in India that resulted in a systemic breakdown of healthcare infrastructure, a social welfare organization based in Pakistan offered to procure medical-grade oxygen to assist India - a nation which was involved in four wars with Pakistan in the past few decades. In this paper, we focus on Pakistani Twitter users’ response to the ongoing healthcare crisis in India. While #IndiaNeedsOxygen and #PakistanStandsWithIndia featured among the top-trending hashtags in Pakistan, divisive hashtags such as #EndiaSaySorryToKashmir simultaneously started trending. Against the backdrop of a contentious history including four wars, divisive content of this nature, especially when a country is facing an unprecedented healthcare crisis, fuels further deterioration of relations. In this paper, we define a new task of detecting supportive content and demonstrate that existing NLP for social impact tools can be effectively harnessed for such tasks within a quick turnaround time. We also release the first publicly available data set at the intersection of geopolitical relations and a raging pandemic in the context of India and Pakistan",
    "checked": true,
    "id": "e511b338559a1df846059068ce7cc64c7066be4c",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Clay H. Yoo",
      "Shriphani Palakodety",
      "Rupak Sarkar",
      "Ashiqur KhudaBukhsh"
    ]
  },
  "https://aclanthology.org/2021.nlp4posimpact-1.15": {
    "title": "A Speech-enabled Fixed-phrase Translator for Healthcare Accessibility",
    "volume": "workshop",
    "abstract": "In this overview article we describe an application designed to enable communication between health practitioners and patients who do not share a common language, in situations where professional interpreters are not available. Built on the principle of a fixed phrase translator, the application implements different natural language processing (NLP) technologies, such as speech recognition, neural machine translation and text-to-speech to improve usability. Its design allows easy portability to new domains and integration of different types of output for multiple target audiences. Even though BabelDr is far from solving the problem of miscommunication between patients and doctors, it is a clear example of NLP in a real world application designed to help minority groups to communicate in a medical context. It also gives some insights into the relevant criteria for the development of such an application",
    "checked": true,
    "id": "934dbfbb33cbec11fc825db56ac85a48fc52158f",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Pierrette Bouillon",
      "Johanna Gerlach",
      "Jonathan Mutal",
      "Nikos Tsourakis",
      "Hervé Spechbach"
    ]
  },
  "https://aclanthology.org/2021.nlp4posimpact-1.16": {
    "title": "A Grounded Well-being Conversational Agent with Multiple Interaction Modes: Preliminary Results",
    "volume": "workshop",
    "abstract": "Technologies for enhancing well-being, healthcare vigilance and monitoring are on the rise. However, despite patient interest, such technologies suffer from low adoption. One hypothesis for this limited adoption is loss of human interaction that is central to doctor-patient encounters. In this paper we seek to address this limitation via a conversational agent that adopts one aspect of in-person doctor-patient interactions: A human avatar to facilitate medical grounded question answering. This is akin to the in-person scenario where the doctor may point to the human body or the patient may point to their own body to express their conditions. Additionally, our agent has multiple interaction modes, that may give more options for the patient to use the agent, not just for medical question answering, but also to engage in conversations about general topics and current events. Both the avatar, and the multiple interaction modes could help improve adherence. We present a high level overview of the design of our agent, Marie Bot Wellbeing. We also report implementation details of our early prototype , and present preliminary results",
    "checked": true,
    "id": "8d9a678c56b9085de65024aa2f6b406ccad97390",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinxin Yan",
      "Ndapa Nakashole"
    ]
  },
  "https://aclanthology.org/2021.nlp4prog-1.1": {
    "title": "Code to Comment Translation: A Comparative Study on Model Effectiveness & Errors",
    "volume": "workshop",
    "abstract": "Automated source code summarization is a popular software engineering research topic wherein machine translation models are employed to “translate” code snippets into relevant natural language descriptions. Most evaluations of such models are conducted using automatic reference-based metrics. However, given the relatively large semantic gap between programming languages and natural language, we argue that this line of research would benefit from a qualitative investigation into the various error modes of current state-of-the-art models. Therefore, in this work, we perform both a quantitative and qualitative comparison of three recently proposed source code summarization models. In our quantitative evaluation, we compare the models based on the smoothed BLEU-4, METEOR, and ROUGE-L machine translation metrics, and in our qualitative evaluation, we perform a manual open-coding of the most common errors committed by the models when compared to ground truth captions. Our investigation reveals new insights into the relationship between metric-based performance and model prediction errors grounded in an error taxonomy that can be used to drive future research efforts",
    "checked": true,
    "id": "2825d62b90ef59ed027f2ca22d185aae382aa73b",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Junayed Mahmud",
      "Fahim Faisal",
      "Raihan Islam Arnob",
      "Antonios Anastasopoulos",
      "Kevin Moran"
    ]
  },
  "https://aclanthology.org/2021.nlp4prog-1.2": {
    "title": "ConTest: A Unit Test Completion Benchmark featuring Context",
    "volume": "workshop",
    "abstract": "We introduce CONTEST, a benchmark for NLP-based unit test completion, the task of predicting a test’s assert statements given its setup and focal method, i.e. the method to be tested. ConTest is large-scale (with 365k datapoints). Besides the test code and tested code, it also features context code called by either. We found context to be crucial for accurately predicting assertions. We also introduce baselines based on transformer encoder-decoders, and study the effects of including syntactic information and context. Overall, our models achieve a BLEU score of 38.2, while only generating unparsable code in 1.92% of cases",
    "checked": true,
    "id": "fa53372b56bdc5a63e31096c41831657d31f4cd5",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Johannes Villmow",
      "Jonas Depoix",
      "Adrian Ulges"
    ]
  },
  "https://aclanthology.org/2021.nlp4prog-1.3": {
    "title": "CommitBERT: Commit Message Generation Using Pre-Trained Programming Language Model",
    "volume": "workshop",
    "abstract": "Commit message is a document that summarizes source code changes in natural language. A good commit message clearly shows the source code changes, so this enhances collaboration between developers. Therefore, our work is to develop a model that automatically writes the commit message. To this end, we release 345K datasets consisting of code modification and commit messages in six programming languages (Python, PHP, Go, Java, JavaScript, and Ruby). Similar to the neural machine translation (NMT) model, using our dataset, we feed the code modification to the encoder input and the commit message to the decoder input and measure the result of the generated commit message with BLEU-4. Also, we propose the following two training methods to improve the result of generating the commit message: (1) A method of preprocessing the input to feed the code modification to the encoder input. (2) A method that uses an initial weight suitable for the code domain to reduce the gap in contextual representation between programming language (PL) and natural language (NL)",
    "checked": true,
    "id": "0eb11495a21c0635c84cb8ce3764624ffa5d8214",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Tae Hwan Jung"
    ]
  },
  "https://aclanthology.org/2021.nlp4prog-1.4": {
    "title": "Time-Efficient Code Completion Model for the R Programming Language",
    "volume": "workshop",
    "abstract": "In this paper we present a deep learning code completion model for the R language. We introduce several techniques to utilize language modeling based architecture in the code completion task. With these techniques, the model requires low resources, but still achieves high quality. We also present an evaluation dataset for the R language completion task. Our dataset contains multiple autocompletion usage contexts that provides robust validation results. The dataset is publicly available",
    "checked": true,
    "id": "c2061cab5772512a8c3af36c1c6de5a82f213a20",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Artem Popov",
      "Dmitrii Orekhov",
      "Denis Litvinov",
      "Nikolay Korolev",
      "Gleb Morgachev"
    ]
  },
  "https://aclanthology.org/2021.nlp4prog-1.5": {
    "title": "CoTexT: Multi-task Learning with Code-Text Transformer",
    "volume": "workshop",
    "abstract": "We present CoTexT, a pre-trained, transformer-based encoder-decoder model that learns the representative context between natural language (NL) and programming language (PL). Using self-supervision, CoTexT is pre-trained on large programming language corpora to learn a general understanding of language and code. CoTexT supports downstream NL-PL tasks such as code summarizing/documentation, code generation, defect detection, and code debugging. We train CoTexT on different combinations of available PL corpus including both “bimodal” and “unimodal” data. Here, bimodal data is the combination of text and corresponding code snippets, whereas unimodal data is merely code snippets. We first evaluate CoTexT with multi-task learning: we perform Code Summarization on 6 different programming languages and Code Refinement on both small and medium size featured in the CodeXGLUE dataset. We further conduct extensive experiments to investigate CoTexT on other tasks within the CodeXGlue dataset, including Code Generation and Defect Detection. We consistently achieve SOTA results in these tasks, demonstrating the versatility of our models",
    "checked": true,
    "id": "0b077c9577f4297dcf3da835e253d21965bbc6e0",
    "semantic_title": "",
    "citation_count": 63,
    "authors": [
      "Long Phan",
      "Hieu Tran",
      "Daniel Le",
      "Hieu Nguyen",
      "James Annibal",
      "Alec Peltekian",
      "Yanfang Ye"
    ]
  },
  "https://aclanthology.org/2021.nlp4prog-1.6": {
    "title": "DIRECT : A Transformer-based Model for Decompiled Identifier Renaming",
    "volume": "workshop",
    "abstract": "Decompiling binary executables to high-level code is an important step in reverse engineering scenarios, such as malware analysis and legacy code maintenance. However, the generated high-level code is difficult to understand since the original variable names are lost. In this paper, we leverage transformer models to reconstruct the original variable names from decompiled code. Inherent differences between code and natural language present certain challenges in applying conventional transformer-based architectures to variable name recovery. We propose DIRECT, a novel transformer-based architecture customized specifically for the task at hand. We evaluate our model on a dataset of decompiled functions and find that DIRECT outperforms the previous state-of-the-art model by up to 20%. We also present ablation studies evaluating the impact of each of our modifications. We make the source code of DIRECT available to encourage reproducible research",
    "checked": true,
    "id": "44853693985c33c2c9edce796338b15483595ae4",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Vikram Nitin",
      "Anthony Saieva",
      "Baishakhi Ray",
      "Gail Kaiser"
    ]
  },
  "https://aclanthology.org/2021.nlp4prog-1.7": {
    "title": "Shellcode_IA32: A Dataset for Automatic Shellcode Generation",
    "volume": "workshop",
    "abstract": "We take the first step to address the task of automatically generating shellcodes, i.e., small pieces of code used as a payload in the exploitation of a software vulnerability, starting from natural language comments. We assemble and release a novel dataset (Shellcode_IA32), consisting of challenging but common assembly instructions with their natural language descriptions. We experiment with standard methods in neural machine translation (NMT) to establish baseline performance levels on this task",
    "checked": true,
    "id": "b15fa9e57fb791899154a0f6c321eb703f1c0b09",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Pietro Liguori",
      "Erfan Al-Hossami",
      "Domenico Cotroneo",
      "Roberto Natella",
      "Bojan Cukic",
      "Samira Shaikh"
    ]
  },
  "https://aclanthology.org/2021.nlp4prog-1.8": {
    "title": "Reading StackOverflow Encourages Cheating: Adding Question Text Improves Extractive Code Generation",
    "volume": "workshop",
    "abstract": "Answering a programming question with only its title is difficult as salient contextual information is left out. To address this, we present a corpus of over 40,000 StackOverflow question texts to be used in conjunction with the corresponding intents from the CoNaLa dataset (Yin et al., 2018). Using both the intent and the question body, we use BART to establish a baseline BLEU score of 34.35 for this new task. We then find further improvements of 2.8% by combining the mined CoNaLa data with the labeled data to achieve a 35.32 BLEU score. We then evaluate the prior state-of-the-art CoNaLa models with this additional data. We find that our proposed method of using the body and mined data beats that of the previous state-of-the-art by a 71.96% BLEU score. Finally, we perform ablations that prove that BART is an unsupervised multimodal learner and examine its extractive behavior",
    "checked": true,
    "id": "cc9ad384ec0d0176ce05865d3866b44d2519bd68",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Gabriel Orlanski",
      "Alex Gittens"
    ]
  },
  "https://aclanthology.org/2021.nlp4prog-1.9": {
    "title": "Text-to-SQL in the Wild: A Naturally-Occurring Dataset Based on Stack Exchange Data",
    "volume": "workshop",
    "abstract": "Most available semantic parsing datasets, comprising of pairs of natural utterances and logical forms, were collected solely for the purpose of training and evaluation of natural language understanding systems. As a result, they do not contain any of the richness and variety of natural-occurring utterances, where humans ask about data they need or are curious about. In this work, we release SEDE, a dataset with 12,023 pairs of utterances and SQL queries collected from real usage on the Stack Exchange website. We show that these pairs contain a variety of real-world challenges which were rarely reflected so far in any other semantic parsing dataset, propose an evaluation metric based on comparison of partial query clauses that is more suitable for real-world queries, and conduct experiments with strong baselines, showing a large gap between the performance on SEDE compared to other common datasets",
    "checked": true,
    "id": "67e8e2d3b276c339588b9551e6b20cd62ebdda7c",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Moshe Hazoom",
      "Vibhor Malik",
      "Ben Bogin"
    ]
  },
  "https://aclanthology.org/2021.nlp4prog-1.10": {
    "title": "Bag-of-Words Baselines for Semantic Code Search",
    "volume": "workshop",
    "abstract": "The task of semantic code search is to retrieve code snippets from a source code corpus based on an information need expressed in natural language. The semantic gap between natural language and programming languages has for long been regarded as one of the most significant obstacles to the effectiveness of keyword-based information retrieval (IR) methods. It is a common assumption that “traditional” bag-of-words IR methods are poorly suited for semantic code search: our work empirically investigates this assumption. Specifically, we examine the effectiveness of two traditional IR methods, namely BM25 and RM3, on the CodeSearchNet Corpus, which consists of natural language queries paired with relevant code snippets. We find that the two keyword-based methods outperform several pre-BERT neural models. We also compare several code-specific data pre-processing strategies and find that specialized tokenization improves effectiveness",
    "checked": true,
    "id": "cbbcc7d518c626bbce1e859c6179d1faa2d2fd46",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Zhang",
      "Ji Xin",
      "Andrew Yates",
      "Jimmy Lin"
    ]
  },
  "https://aclanthology.org/2021.repl4nlp-1.1": {
    "title": "Improving Cross-lingual Text Classification with Zero-shot Instance-Weighting",
    "volume": "workshop",
    "abstract": "Cross-lingual text classification (CLTC) is a challenging task made even harder still due to the lack of labeled data in low-resource languages. In this paper, we propose zero-shot instance-weighting, a general model-agnostic zero-shot learning framework for improving CLTC by leveraging source instance weighting. It adds a module on top of pre-trained language models for similarity computation of instance weights, thus aligning each source instance to the target language. During training, the framework utilizes gradient descent that is weighted by instance weights to update parameters. We evaluate this framework over seven target languages on three fundamental tasks and show its effectiveness and extensibility, by improving on F1 score up to 4% in single-source transfer and 8% in multi-source transfer. To the best of our knowledge, our method is the first to apply instance weighting in zero-shot CLTC. It is simple yet effective and easily extensible into multi-source transfer",
    "checked": true,
    "id": "66978698693c0bfaa0859ca400fdce3acd4c7f7b",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Irene Li",
      "Prithviraj Sen",
      "Huaiyu Zhu",
      "Yunyao Li",
      "Dragomir Radev"
    ]
  },
  "https://aclanthology.org/2021.repl4nlp-1.2": {
    "title": "Probing Multilingual Language Models for Discourse",
    "volume": "workshop",
    "abstract": "Pre-trained multilingual language models have become an important building block in multilingual Natural Language Processing. In the present paper, we investigate a range of such models to find out how well they transfer discourse-level knowledge across languages. This is done with a systematic evaluation on a broader set of discourse-level tasks than has been previously been assembled. We find that the XLM-RoBERTa family of models consistently show the best performance, by simultaneously being good monolingual models and degrading relatively little in a zero-shot setting. Our results also indicate that model distillation may hurt the ability of cross-lingual transfer of sentence representations, while language dissimilarity at most has a modest effect. We hope that our test suite, covering 5 tasks with a total of 22 languages in 10 distinct families, will serve as a useful evaluation platform for multilingual performance at and beyond the sentence level",
    "checked": true,
    "id": "c4a24f0a480b15254b84b1f5620a6c1fc34f4c72",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Murathan Kurfalı",
      "Robert Östling"
    ]
  },
  "https://aclanthology.org/2021.repl4nlp-1.3": {
    "title": "Comprehension Based Question Answering using Bloom's Taxonomy",
    "volume": "workshop",
    "abstract": "Current pre-trained language models have lots of knowledge, but a more limited ability to use that knowledge. Bloom’s Taxonomy helps educators teach children how to use knowledge by categorizing comprehension skills, so we use it to analyze and improve the comprehension skills of large pre-trained language models. Our experiments focus on zero-shot question answering, using the taxonomy to provide proximal context that helps the model answer questions by being relevant to those questions. We show targeting context in this manner improves performance across 4 popular common sense question answer datasets",
    "checked": true,
    "id": "a767c29092698b55c1e1c1d1635d53e598e12fd5",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Pritish Sahu",
      "Michael Cogswell",
      "Ajay Divakaran",
      "Sara Rutherford-Quach"
    ]
  },
  "https://aclanthology.org/2021.repl4nlp-1.4": {
    "title": "Larger-Scale Transformers for Multilingual Masked Language Modeling",
    "volume": "workshop",
    "abstract": "Recent work has demonstrated the effectiveness of cross-lingual language model pretraining for cross-lingual understanding. In this study, we present the results of two larger multilingual masked language models, with 3.5B and 10.7B parameters. Our two new models dubbed and outperform XLM-R by 1.8% and 2.4% average accuracy on XNLI. Our model also outperforms the RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages. This suggests larger capacity models for language understanding may obtain strong performance on high-resource languages while greatly improving low-resource languages. We make our code and models publicly available",
    "checked": true,
    "id": "c553280c1fc1d0bc7b94683bb75910e309b0d579",
    "semantic_title": "",
    "citation_count": 40,
    "authors": [
      "Naman Goyal",
      "Jingfei Du",
      "Myle Ott",
      "Giri Anantharaman",
      "Alexis Conneau"
    ]
  },
  "https://aclanthology.org/2021.repl4nlp-1.5": {
    "title": "Learning Sparse Sentence Encoding without Supervision: An Exploration of Sparsity in Variational Autoencoders",
    "volume": "workshop",
    "abstract": "It has been long known that sparsity is an effective inductive bias for learning efficient representation of data in vectors with fixed dimensionality, and it has been explored in many areas of representation learning. Of particular interest to this work is the investigation of the sparsity within the VAE framework which has been explored a lot in the image domain, but has been lacking even a basic level of exploration in NLP. Additionally, NLP is also lagging behind in terms of learning sparse representations of large units of text e.g., sentences. We use the VAEs that induce sparse latent representations of large units of text to address the aforementioned shortcomings. First, we move in this direction by measuring the success of unsupervised state-of-the-art (SOTA) and other strong VAE-based sparsification baselines for text and propose a hierarchical sparse VAE model to address the stability issue of SOTA. Then, we look at the implications of sparsity on text classification across 3 datasets, and highlight a link between performance of sparse latent representations on downstream tasks and its ability to encode task-related information",
    "checked": true,
    "id": "4b5a6edbd3142f924bae20062923ec8356cbaa94",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Victor Prokhorov",
      "Yingzhen Li",
      "Ehsan Shareghi",
      "Nigel Collier"
    ]
  },
  "https://aclanthology.org/2021.repl4nlp-1.6": {
    "title": "Temporal-aware Language Representation Learning From Crowdsourced Labels",
    "volume": "workshop",
    "abstract": "Learning effective language representations from crowdsourced labels is crucial for many real-world machine learning tasks. A challenging aspect of this problem is that the quality of crowdsourced labels suffer high intra- and inter-observer variability. Since the high-capacity deep neural networks can easily memorize all disagreements among crowdsourced labels, directly applying existing supervised language representation learning algorithms may yield suboptimal solutions. In this paper, we propose TACMA, a temporal-aware language representation learning heuristic for crowdsourced labels with multiple annotators. The proposed approach (1) explicitly models the intra-observer variability with attention mechanism; (2) computes and aggregates per-sample confidence scores from multiple workers to address the inter-observer disagreements. The proposed heuristic is extremely easy to implement in around 5 lines of code. The proposed heuristic is evaluated on four synthetic and four real-world data sets. The results show that our approach outperforms a wide range of state-of-the-art baselines in terms of prediction accuracy and AUC. To encourage the reproducible results, we make our code publicly available at https://github.com/CrowdsourcingMining/TACMA",
    "checked": true,
    "id": "47b882a77ba3ea521e54846387599a92f7834135",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Yang Hao",
      "Xiao Zhai",
      "Wenbiao Ding",
      "Zitao Liu"
    ]
  },
  "https://aclanthology.org/2021.repl4nlp-1.7": {
    "title": "Structure-aware Sentence Encoder in Bert-Based Siamese Network",
    "volume": "workshop",
    "abstract": "Recently, impressive performance on various natural language understanding tasks has been achieved by explicitly incorporating syntax and semantic information into pre-trained models, such as BERT and RoBERTa. However, this approach depends on problem-specific fine-tuning, and as widely noted, BERT-like models exhibit weak performance, and are inefficient, when applied to unsupervised similarity comparison tasks. Sentence-BERT (SBERT) has been proposed as a general-purpose sentence embedding method, suited to both similarity comparison and downstream tasks. In this work, we show that by incorporating structural information into SBERT, the resulting model outperforms SBERT and previous general sentence encoders on unsupervised semantic textual similarity (STS) datasets and transfer classification tasks",
    "checked": true,
    "id": "c3e836d57f1ae258d5c758b6cc08047c40cba9d3",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Qiwei Peng",
      "David Weir",
      "Julie Weeds"
    ]
  },
  "https://aclanthology.org/2021.repl4nlp-1.8": {
    "title": "Preserving Cross-Linguality of Pre-trained Models via Continual Learning",
    "volume": "workshop",
    "abstract": "Recently, fine-tuning pre-trained language models (e.g., multilingual BERT) to downstream cross-lingual tasks has shown promising results. However, the fine-tuning process inevitably changes the parameters of the pre-trained model and weakens its cross-lingual ability, which leads to sub-optimal performance. To alleviate this problem, we leverage continual learning to preserve the original cross-lingual ability of the pre-trained model when we fine-tune it to downstream tasks. The experimental result shows that our fine-tuning methods can better preserve the cross-lingual ability of the pre-trained model in a sentence retrieval task. Our methods also achieve better performance than other fine-tuning baselines on the zero-shot cross-lingual part-of-speech tagging and named entity recognition tasks",
    "checked": true,
    "id": "ec501a7a907ef476781646620a0d4698f36daefa",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Zihan Liu",
      "Genta Indra Winata",
      "Andrea Madotto",
      "Pascale Fung"
    ]
  },
  "https://aclanthology.org/2021.repl4nlp-1.9": {
    "title": "Text Style Transfer: Leveraging a Style Classifier on Entangled Latent Representations",
    "volume": "workshop",
    "abstract": "Learning a good latent representation is essential for text style transfer, which generates a new sentence by changing the attributes of a given sentence while preserving its content. Most previous works adopt disentangled latent representation learning to realize style transfer. We propose a novel text style transfer algorithm with entangled latent representation, and introduce a style classifier that can regulate the latent structure and transfer style. Moreover, our algorithm for style transfer applies to both single-attribute and multi-attribute transfer. Extensive experimental results show that our method generally outperforms state-of-the-art approaches",
    "checked": true,
    "id": "0b217dd3d2ffda701c10217fa76103d0da33e844",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Xiaoyan Li",
      "Sun Sun",
      "Yunli Wang"
    ]
  },
  "https://aclanthology.org/2021.repl4nlp-1.10": {
    "title": "Inductively Representing Out-of-Knowledge-Graph Entities by Optimal Estimation Under Translational Assumptions",
    "volume": "workshop",
    "abstract": "Conventional Knowledge Graph Completion (KGC) assumes that all test entities appear during training. However, in real-world scenarios, Knowledge Graphs (KG) evolve fast with out-of-knowledge-graph (OOKG) entities added frequently, and we need to efficiently represent these entities. Most existing Knowledge Graph Embedding (KGE) methods cannot represent OOKG entities without costly retraining on the whole KG. To enhance efficiency, we propose a simple and effective method that inductively represents OOKG entities by their optimal estimation under translational assumptions. Moreover, given pretrained embeddings of the in-knowledge-graph (IKG) entities, our method even needs no additional learning. Experimental results on two KGC tasks with OOKG entities show that our method outperforms the previous methods by a large margin with higher efficiency",
    "checked": true,
    "id": "62634f2e696fca1ef11660c6807e538e3768b836",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Damai Dai",
      "Hua Zheng",
      "Fuli Luo",
      "Pengcheng Yang",
      "Tianyu Liu",
      "Zhifang Sui",
      "Baobao Chang"
    ]
  },
  "https://aclanthology.org/2021.repl4nlp-1.11": {
    "title": "Revisiting Pretraining with Adapters",
    "volume": "workshop",
    "abstract": "Pretrained language models have served as the backbone for many state-of-the-art NLP results. These models are large and expensive to train. Recent work suggests that continued pretraining on task-specific data is worth the effort as pretraining leads to improved performance on downstream tasks. We explore alternatives to full-scale task-specific pretraining of language models through the use of adapter modules, a parameter-efficient approach to transfer learning. We find that adapter-based pretraining is able to achieve comparable results to task-specific pretraining while using a fraction of the overall trainable parameters. We further explore direct use of adapters without pretraining and find that the direct fine-tuning performs mostly on par with pretrained adapter models, contradicting previously proposed benefits of continual pretraining in full pretraining fine-tuning strategies. Lastly, we perform an ablation study on task-adaptive pretraining to investigate how different hyperparameter settings can change the effectiveness of the pretraining",
    "checked": true,
    "id": "d7cec63baf11028a2ececb094cf2660f426c0e9d",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Seungwon Kim",
      "Alex Shum",
      "Nathan Susanj",
      "Jonathan Hilgart"
    ]
  },
  "https://aclanthology.org/2021.repl4nlp-1.12": {
    "title": "Knodle: Modular Weakly Supervised Learning with PyTorch",
    "volume": "workshop",
    "abstract": "Strategies for improving the training and prediction quality of weakly supervised machine learning models vary in how much they are tailored to a specific task or integrated with a specific model architecture. In this work, we introduce Knodle, a software framework that treats weak data annotations, deep learning models, and methods for improving weakly supervised training as separate, modular components. This modularization gives the training process access to fine-grained information such as data set characteristics, matches of heuristic rules, or elements of the deep learning model ultimately used for prediction. Hence, our framework can encompass a wide range of training methods for improving weak supervision, ranging from methods that only look at correlations of rules and output classes (independently of the machine learning model trained with the resulting labels), to those that harness the interplay of neural networks and weakly labeled data. We illustrate the benchmarking potential of the framework with a performance comparison of several reference implementations on a selection of datasets that are already available in Knodle",
    "checked": true,
    "id": "25412ff4315261d855f879052baa7fd48e6cade1",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Anastasiia Sedova",
      "Andreas Stephan",
      "Marina Speranskaya",
      "Benjamin Roth"
    ]
  },
  "https://aclanthology.org/2021.repl4nlp-1.13": {
    "title": "X2Parser: Cross-Lingual and Cross-Domain Framework for Task-Oriented Compositional Semantic Parsing",
    "volume": "workshop",
    "abstract": "Task-oriented compositional semantic parsing (TCSP) handles complex nested user queries and serves as an essential component of virtual assistants. Current TCSP models rely on numerous training data to achieve decent performance but fail to generalize to low-resource target languages or domains. In this paper, we present X2Parser, a transferable Cross-lingual and Cross-domain Parser for TCSP. Unlike previous models that learn to generate the hierarchical representations for nested intents and slots, we propose to predict intents and slots separately and cast both prediction tasks into sequence labeling problems. After that, we further propose a fertility-based slot predictor that first learns to detect the number of labels for each token, and then predicts the slot types. Experimental results illustrate that our model can significantly outperform existing strong baselines in cross-lingual and cross-domain settings, and our model can also achieve a good generalization ability on target languages of target domains. Furthermore, we show that our model can reduce the latency by up to 66% compared to the generation-based model",
    "checked": true,
    "id": "d1a2b23efdc7acd927403e2f573132d262340ac2",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Zihan Liu",
      "Genta Indra Winata",
      "Peng Xu",
      "Pascale Fung"
    ]
  },
  "https://aclanthology.org/2021.repl4nlp-1.14": {
    "title": "Unsupervised Representation Disentanglement of Text: An Evaluation on Synthetic Datasets",
    "volume": "workshop",
    "abstract": "To highlight the challenges of achieving representation disentanglement for text domain in an unsupervised setting, in this paper we select a representative set of successfully applied models from the image domain. We evaluate these models on 6 disentanglement metrics, as well as on downstream classification tasks and homotopy. To facilitate the evaluation, we propose two synthetic datasets with known generative factors. Our experiments highlight the existing gap in the text domain and illustrate that certain elements such as representation sparsity (as an inductive bias), or representation coupling with the decoder could impact disentanglement. To the best of our knowledge, our work is the first attempt on the intersection of unsupervised representation disentanglement and text, and provides the experimental framework and datasets for examining future developments in this direction",
    "checked": true,
    "id": "a8ea47fadab176e302de16958e6eb6aeef8ee4a1",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Lan Zhang",
      "Victor Prokhorov",
      "Ehsan Shareghi"
    ]
  },
  "https://aclanthology.org/2021.repl4nlp-1.15": {
    "title": "Learn The Big Picture: Representation Learning for Clustering",
    "volume": "workshop",
    "abstract": "Existing supervised models for text clustering find it difficult to directly optimize for clustering results. This is because clustering is a discrete process and it is difficult to estimate meaningful gradient of any discrete function that can drive gradient based optimization algorithms. So, existing supervised clustering algorithms indirectly optimize for some continuous function that approximates the clustering process. We propose a scalable training strategy that directly optimizes for a discrete clustering metric. We train a BERT-based embedding model using our method and evaluate it on two publicly available datasets. We show that our method outperforms another BERT-based embedding model employing Triplet loss and other unsupervised baselines. This suggests that optimizing directly for the clustering outcome indeed yields better representations suitable for clustering",
    "checked": true,
    "id": "48e1e9aaa2f240a4f7a9680cb24bd322d3b587fc",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Sumanta Kashyapi",
      "Laura Dietz"
    ]
  },
  "https://aclanthology.org/2021.repl4nlp-1.16": {
    "title": "Probing Cross-Modal Representations in Multi-Step Relational Reasoning",
    "volume": "workshop",
    "abstract": "We investigate the representations learned by vision and language models in tasks that require relational reasoning. Focusing on the problem of assessing the relative size of objects in abstract visual contexts, we analyse both one-step and two-step reasoning. For the latter, we construct a new dataset of three-image scenes and define a task that requires reasoning at the level of the individual images and across images in a scene. We probe the learned model representations using diagnostic classifiers. Our experiments show that pretrained multimodal transformer-based architectures can perform higher-level relational reasoning, and are able to learn representations for novel tasks and data that are very different from what was seen in pretraining",
    "checked": true,
    "id": "0aa3b13f84a19b8423cdc0fa8bef51c1d90079ca",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Iuliia Parfenova",
      "Desmond Elliott",
      "Raquel Fernández",
      "Sandro Pezzelle"
    ]
  },
  "https://aclanthology.org/2021.repl4nlp-1.17": {
    "title": "In-Batch Negatives for Knowledge Distillation with Tightly-Coupled Teachers for Dense Retrieval",
    "volume": "workshop",
    "abstract": "We present an efficient training approach to text retrieval with dense representations that applies knowledge distillation using the ColBERT late-interaction ranking model. Specifically, we propose to transfer the knowledge from a bi-encoder teacher to a student by distilling knowledge from ColBERT’s expressive MaxSim operator into a simple dot product. The advantage of the bi-encoder teacher–student setup is that we can efficiently add in-batch negatives during knowledge distillation, enabling richer interactions between teacher and student models. In addition, using ColBERT as the teacher reduces training cost compared to a full cross-encoder. Experiments on the MS MARCO passage and document ranking tasks and data from the TREC 2019 Deep Learning Track demonstrate that our approach helps models learn robust representations for dense retrieval effectively and efficiently",
    "checked": true,
    "id": "2c74e7258e7c89ca2da1784fb0cb4106a23ac382",
    "semantic_title": "",
    "citation_count": 98,
    "authors": [
      "Sheng-Chieh Lin",
      "Jheng-Hong Yang",
      "Jimmy Lin"
    ]
  },
  "https://aclanthology.org/2021.repl4nlp-1.18": {
    "title": "NPVec1: Word Embeddings for Nepali - Construction and Evaluation",
    "volume": "workshop",
    "abstract": "Word Embedding maps words to vectors of real numbers. It is derived from a large corpus and is known to capture semantic knowledge from the corpus. Word Embedding is a critical component of many state-of-the-art Deep Learning techniques. However, generating good Word Embeddings is a special challenge for low-resource languages such as Nepali due to the unavailability of large text corpus. In this paper, we present NPVec1 which consists of 25 state-of-art Word Embeddings for Nepali that we have derived from a large corpus using Glove, Word2Vec, FastText, and BERT. We further provide intrinsic and extrinsic evaluations of these Embeddings using well established metrics and methods. These models are trained using 279 million word tokens and are the largest Embeddings ever trained for Nepali language. Furthermore, we have made these Embeddings publicly available to accelerate the development of Natural Language Processing (NLP) applications in Nepali",
    "checked": true,
    "id": "528bbed23eb8e984292330098a96266228a617ae",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Pravesh Koirala",
      "Nobal B. Niraula"
    ]
  },
  "https://aclanthology.org/2021.repl4nlp-1.19": {
    "title": "Deriving Word Vectors from Contextualized Language Models using Topic-Aware Mention Selection",
    "volume": "workshop",
    "abstract": "One of the long-standing challenges in lexical semantics consists in learning representations of words which reflect their semantic properties. The remarkable success of word embeddings for this purpose suggests that high-quality representations can be obtained by summarizing the sentence contexts of word mentions. In this paper, we propose a method for learning word representations that follows this basic strategy, but differs from standard word embeddings in two important ways. First, we take advantage of contextualized language models (CLMs) rather than bags of word vectors to encode contexts. Second, rather than learning a word vector directly, we use a topic model to partition the contexts in which words appear, and then learn different topic-specific vectors for each word. Finally, we use a task-specific supervision signal to make a soft selection of the resulting vectors. We show that this simple strategy leads to high-quality word vectors, which are more predictive of semantic properties than word embeddings and existing CLM-based strategies",
    "checked": true,
    "id": "61ab5ca109134aacfe8555fbd2b3c0549f89a02d",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Yixiao Wang",
      "Zied Bouraoui",
      "Luis Espinosa Anke",
      "Steven Schockaert"
    ]
  },
  "https://aclanthology.org/2021.repl4nlp-1.20": {
    "title": "Zero-shot Sequence Labeling for Transformer-based Sentence Classifiers",
    "volume": "workshop",
    "abstract": "We investigate how sentence-level transformers can be modified into effective sequence labelers at the token level without any direct supervision. Existing approaches to zero-shot sequence labeling do not perform well when applied on transformer-based architectures. As transformers contain multiple layers of multi-head self-attention, information in the sentence gets distributed between many tokens, negatively affecting zero-shot token-level performance. We find that a soft attention module which explicitly encourages sharpness of attention weights can significantly outperform existing methods",
    "checked": true,
    "id": "38df39243ab4282a26ec3924780c160bfcbddbe4",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Kamil Bujel",
      "Helen Yannakoudakis",
      "Marek Rei"
    ]
  },
  "https://aclanthology.org/2021.repl4nlp-1.21": {
    "title": "Predicting the Success of Domain Adaptation in Text Similarity",
    "volume": "workshop",
    "abstract": "Transfer learning methods, and in particular domain adaptation, help exploit labeled data in one domain to improve the performance of a certain task in another domain. However, it is still not clear what factors affect the success of domain adaptation. This paper models adaptation success and selection of the most suitable source domains among several candidates in text similarity. We use descriptive domain information and cross-domain similarity metrics as predictive features. While mostly positive, the results also point to some domains where adaptation success was difficult to predict",
    "checked": true,
    "id": "1e2a8a79ec4a3eea2cdfeab8fe92653bc1ddac66",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Nick Pogrebnyakov",
      "Shohreh Shaghaghian"
    ]
  },
  "https://aclanthology.org/2021.repl4nlp-1.22": {
    "title": "Syntagmatic Word Embeddings for Unsupervised Learning of Selectional Preferences",
    "volume": "workshop",
    "abstract": "Selectional Preference (SP) captures the tendency of a word to semantically select other words to be in direct syntactic relation with it, and thus informs us about syntactic word configurations that are meaningful. Therefore SP is a valuable resource for Natural Language Processing (NLP) systems and for semanticists. Learning SP has generally been seen as a supervised task, because it requires a parsed corpus as a source of syntactically related word pairs. In this paper we show that simple distributional analysis can learn a good amount of SP without the need for an annotated corpus. We extend the general word embedding technique with directional word context windows giving word representations that better capture syntagmatic relations. We test on the SP-10K dataset and demonstrate that syntagmatic embeddings outperform the paradigmatic embeddings. We also evaluate supervised version of these embeddings and show that unsupervised syntagmatic embeddings can be as good as supervised embeddings. We also make available the source code of our implementation",
    "checked": true,
    "id": "8e3541513b26833724231edf2ed922b719efaecb",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renjith P. Ravindran",
      "Akshay Badola",
      "Narayana Kavi Murthy"
    ]
  },
  "https://aclanthology.org/2021.repl4nlp-1.23": {
    "title": "Bayesian Model-Agnostic Meta-Learning with Matrix-Valued Kernels for Quality Estimation",
    "volume": "workshop",
    "abstract": "Most current quality estimation (QE) models for machine translation are trained and evaluated in a fully supervised setting requiring significant quantities of labelled training data. However, obtaining labelled data can be both expensive and time-consuming. In addition, the test data that a deployed QE model would be exposed to may differ from its training data in significant ways. In particular, training samples are often labelled by one or a small set of annotators, whose perceptions of translation quality and needs may differ substantially from those of end-users, who will employ predictions in practice. Thus, it is desirable to be able to adapt QE models efficiently to new user data with limited supervision data. To address these challenges, we propose a Bayesian meta-learning approach for adapting QE models to the needs and preferences of each user with limited supervision. To enhance performance, we further propose an extension to a state-of-the-art Bayesian meta-learning approach which utilizes a matrix-valued kernel for Bayesian meta-learning of quality estimation. Experiments on data with varying number of users and language characteristics demonstrates that the proposed Bayesian meta-learning approach delivers improved predictive performance in both limited and full supervision settings",
    "checked": true,
    "id": "5b53d12bf4584e5fc9696cdc92ff806295b2c126",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Abiola Obamuyide",
      "Marina Fomicheva",
      "Lucia Specia"
    ]
  },
  "https://aclanthology.org/2021.repl4nlp-1.24": {
    "title": "Knowledge Informed Semantic Parsing for Conversational Question Answering",
    "volume": "workshop",
    "abstract": "Smart assistants are tasked to answer various questions regarding world knowledge. These questions range from retrieval of simple facts to retrieval of complex, multi-hops question followed by various operators (i.e., filter, argmax). Semantic parsing has emerged as the state-of-the-art for answering these kinds of questions by forming queries to extract information from knowledge bases (KBs). Specially, neural semantic parsers (NSPs) effectively translate natural questions to logical forms, which execute on KB and give desirable answers. Yet, NSPs suffer from non-executable logical forms for some instances in the generated logical forms might be missing due to the incompleteness of KBs. Intuitively, knowing the KB structure informs NSP with changes of the global logical forms structures with respect to changes in KB instances. In this work, we propose a novel knowledge-informed decoder variant of NSP. We consider the conversational question answering settings, where a natural language query, its context and its final answers are available at training. Experimental results show that our method outperformed strong baselines by 1.8 F1 points overall across 10 types of questions of the CSQA dataset. Especially for the “Logical Reasoning” category, our model improves by 7 F1 points. Furthermore, our results are achieved with 90.3% fewer parameters, allowing faster training for large-scale datasets",
    "checked": true,
    "id": "a89db4fca6953c3137fb0cdf8fca36d3fd6b3a11",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Raghuveer Thirukovalluru",
      "Mukund Sridhar",
      "Dung Thai",
      "Shruti Chanumolu",
      "Nicholas Monath",
      "Sankaranarayanan Ananthakrishnan",
      "Andrew McCallum"
    ]
  },
  "https://aclanthology.org/2021.repl4nlp-1.25": {
    "title": "Simultaneously Self-Attending to Text and Entities for Knowledge-Informed Text Representations",
    "volume": "workshop",
    "abstract": "Pre-trained language models have emerged as highly successful methods for learning good text representations. However, the amount of structured knowledge retained in such models, and how (if at all) it can be extracted, remains an open question. In this work, we aim at directly learning text representations which leverage structured knowledge about entities mentioned in the text. This can be particularly beneficial for downstream tasks which are knowledge-intensive. Our approach utilizes self-attention between words in the text and knowledge graph (KG) entities mentioned in the text. While existing methods require entity-linked data for pre-training, we train using a mention-span masking objective and a candidate ranking objective – which doesn’t require any entity-links and only assumes access to an alias table for retrieving candidates, enabling large-scale pre-training. We show that the proposed model learns knowledge-informed text representations that yield improvements on the downstream tasks over existing methods",
    "checked": true,
    "id": "0b4b6112da8e4ea723a998dde05d58215a47909e",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Dung Thai",
      "Raghuveer Thirukovalluru",
      "Trapit Bansal",
      "Andrew McCallum"
    ]
  },
  "https://aclanthology.org/2021.repl4nlp-1.26": {
    "title": "Deriving Contextualised Semantic Features from BERT (and Other Transformer Model) Embeddings",
    "volume": "workshop",
    "abstract": "Models based on the transformer architecture, such as BERT, have marked a crucial step forward in the field of Natural Language Processing. Importantly, they allow the creation of word embeddings that capture important semantic information about words in context. However, as single entities, these embeddings are difficult to interpret and the models used to create them have been described as opaque. Binder and colleagues proposed an intuitive embedding space where each dimension is based on one of 65 core semantic features. Unfortunately, the space only exists for a small data-set of 535 words, limiting its uses. Previous work (Utsumi, 2018, 2020; Turton et al., 2020) has shown that Binder features can be derived from static embeddings and successfully extrapolated to a large new vocabulary. Taking the next step, this paper demonstrates that Binder features can be derived from the BERT embedding space. This provides two things; (1) semantic feature values derived from contextualised word embeddings and (2) insights into how semantic features are represented across the different layers of the BERT model",
    "checked": true,
    "id": "81eb8a702308c213db803ea50dd5451f83866ae6",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Jacob Turton",
      "Robert Elliott Smith",
      "David Vinson"
    ]
  },
  "https://aclanthology.org/2021.repl4nlp-1.27": {
    "title": "Syntactic Perturbations Reveal Representational Correlates of Hierarchical Phrase Structure in Pretrained Language Models",
    "volume": "workshop",
    "abstract": "While vector-based language representations from pretrained language models have set a new standard for many NLP tasks, there is not yet a complete accounting of their inner workings. In particular, it is not entirely clear what aspects of sentence-level syntax are captured by these representations, nor how (if at all) they are built along the stacked layers of the network. In this paper, we aim to address such questions with a general class of interventional, input perturbation-based analyses of representations from pretrained language models. Importing from computational and cognitive neuroscience the notion of representational invariance, we perform a series of probes designed to test the sensitivity of these representations to several kinds of structure in sentences. Each probe involves swapping words in a sentence and comparing the representations from perturbed sentences against the original. We experiment with three different perturbations: (1) random permutations of n-grams of varying width, to test the scale at which a representation is sensitive to word position; (2) swapping of two spans which do or do not form a syntactic phrase, to test sensitivity to global phrase structure; and (3) swapping of two adjacent words which do or do not break apart a syntactic phrase, to test sensitivity to local phrase structure. Results from these probes collectively suggest that Transformers build sensitivity to larger parts of the sentence along their layers, and that hierarchical phrase structure plays a role in this process. More broadly, our results also indicate that structured input perturbations widens the scope of analyses that can be performed on often-opaque deep learning systems, and can serve as a complement to existing tools (such as supervised linear probes) for interpreting complex black-box models",
    "checked": true,
    "id": "c3fb6056ca1ec3f7cfe57103712531fdbfe69e03",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Matteo Alleman",
      "Jonathan Mamou",
      "Miguel A Del Rio",
      "Hanlin Tang",
      "Yoon Kim",
      "SueYeon Chung"
    ]
  },
  "https://aclanthology.org/2021.repl4nlp-1.28": {
    "title": "Box-To-Box Transformations for Modeling Joint Hierarchies",
    "volume": "workshop",
    "abstract": "Learning representations of entities and relations in structured knowledge bases is an active area of research, with much emphasis placed on choosing the appropriate geometry to capture the hierarchical structures exploited in, for example, isa or haspart relations. Box embeddings (Vilnis et al., 2018; Li et al., 2019; Dasgupta et al., 2020), which represent concepts as n-dimensional hyperrectangles, are capable of embedding hierarchies when training on a subset of the transitive closure. In Patel et al., (2020), the authors demonstrate that only the transitive reduction is required and further extend box embeddings to capture joint hierarchies by augmenting the graph with new nodes. While it is possible to represent joint hierarchies with this method, the parameters for each hierarchy are decoupled, making generalization between hierarchies infeasible. In this work, we introduce a learned box-to-box transformation that respects the structure of each hierarchy. We demonstrate that this not only improves the capability of modeling cross-hierarchy compositional edges but is also capable of generalizing from a subset of the transitive reduction",
    "checked": true,
    "id": "12e50a97f08a48c13b116fea55ebefa727d1d085",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Shib Sankar Dasgupta",
      "Xiang Lorraine Li",
      "Michael Boratko",
      "Dongxu Zhang",
      "Andrew McCallum"
    ]
  },
  "https://aclanthology.org/2021.repl4nlp-1.29": {
    "title": "An Overview of Uncertainty Calibration for Text Classification and the Role of Distillation",
    "volume": "workshop",
    "abstract": "Recent advances in NLP systems, notably the pretraining-and-finetuning paradigm, have achieved great success in predictive accuracy. However, these systems are usually not well calibrated for uncertainty out-of-the-box. Many recalibration methods have been proposed in the literature for quantifying predictive uncertainty and calibrating model outputs, with varying degrees of complexity. In this work, we present a systematic study of a few of these methods. Focusing on the text classification task and finetuned large pretrained language models, we first show that many of the finetuned models are not well calibrated out-of-the-box, especially when the data come from out-of-domain settings. Next, we compare the effectiveness of a few widely-used recalibration methods (such as ensembles, temperature scaling). Then, we empirically illustrate a connection between distillation and calibration. We view distillation as a regularization term encouraging the student model to output uncertainties that match those of a teacher model. With this insight, we develop simple recalibration methods based on distillation with no additional inference-time cost. We show on the GLUE benchmark that our simple methods can achieve competitive out-of-domain (OOD) calibration performance w.r.t. more expensive approaches. Finally, we include ablations to understand the usefulness of components of our proposed method and examine the transferability of calibration via distillation",
    "checked": true,
    "id": "7f16b512f9cfce8c2cc31fd59492924b3ef6d597",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Han Guo",
      "Ramakanth Pasunuru",
      "Mohit Bansal"
    ]
  },
  "https://aclanthology.org/2021.repl4nlp-1.30": {
    "title": "Entity and Evidence Guided Document-Level Relation Extraction",
    "volume": "workshop",
    "abstract": "Document-level relation extraction is a challenging task, requiring reasoning over multiple sentences to predict a set of relations in a document. In this paper, we propose a novel framework E2GRE (Entity and Evidence Guided Relation Extraction) that jointly extracts relations and the underlying evidence sentences by using large pretrained language model (LM) as input encoder. First, we propose to guide the pretrained LM’s attention mechanism to focus on relevant context by using attention probabilities as additional features for evidence prediction. Furthermore, instead of feeding the whole document into pretrained LMs to obtain entity representation, we concatenate document text with head entities to help LMs concentrate on parts of the document that are more related to the head entity. Our E2GRE jointly learns relation extraction and evidence prediction effectively, showing large gains on both these tasks, which we find are highly correlated",
    "checked": true,
    "id": "e4179bf0963012f9f13ce0ca61d3cdd12a87b336",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Kevin Huang",
      "Peng Qi",
      "Guangtao Wang",
      "Tengyu Ma",
      "Jing Huang"
    ]
  },
  "https://aclanthology.org/2021.repl4nlp-1.31": {
    "title": "Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup",
    "volume": "workshop",
    "abstract": "Contrastive learning has been applied successfully to learn vector representations of text. Previous research demonstrated that learning high-quality representations benefits from batch-wise contrastive loss with a large number of negatives. In practice, the technique of in-batch negative is used, where for each example in a batch, other batch examples’ positives will be taken as its negatives, avoiding encoding extra negatives. This, however, still conditions each example’s loss on all batch examples and requires fitting the entire large batch into GPU memory. This paper introduces a gradient caching technique that decouples backpropagation between contrastive loss and the encoder, removing encoder backward pass data dependency along the batch dimension. As a result, gradients can be computed for one subset of the batch at a time, leading to almost constant memory usage",
    "checked": true,
    "id": "8d1369a218a39214d82ea77ff964570eca057c15",
    "semantic_title": "",
    "citation_count": 35,
    "authors": [
      "Luyu Gao",
      "Yunyi Zhang",
      "Jiawei Han",
      "Jamie Callan"
    ]
  },
  "https://aclanthology.org/2021.repl4nlp-1.32": {
    "title": "Direction is what you need: Improving Word Embedding Compression in Large Language Models",
    "volume": "workshop",
    "abstract": "The adoption of Transformer-based models in natural language processing (NLP) has led to great success using a massive number of parameters. However, due to deployment constraints in edge devices, there has been a rising interest in the compression of these models to improve their inference time and memory footprint. This paper presents a novel loss objective to compress token embeddings in the Transformer-based models by leveraging an AutoEncoder architecture. More specifically, we emphasize the importance of the direction of compressed embeddings with respect to original uncompressed embeddings. The proposed method is task-agnostic and does not require further language modeling pre-training. Our method significantly outperforms the commonly used SVD-based matrix-factorization approach in terms of initial language model Perplexity. Moreover, we evaluate our proposed approach over SQuAD v1.1 dataset and several downstream tasks from the GLUE benchmark, where we also outperform the baseline in most scenarios. Our code is public",
    "checked": true,
    "id": "82d9696ad6badb2ee1cf32149907264360b1a916",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Klaudia Bałazy",
      "Mohammadreza Banaei",
      "Rémi Lebret",
      "Jacek Tabor",
      "Karl Aberer"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.1": {
    "title": "SemEval-2021 Task 1: Lexical Complexity Prediction",
    "volume": "workshop",
    "abstract": "This paper presents the results and main findings of SemEval-2021 Task 1 - Lexical Complexity Prediction. We provided participants with an augmented version of the CompLex Corpus (Shardlow et al. 2020). CompLex is an English multi-domain corpus in which words and multi-word expressions (MWEs) were annotated with respect to their complexity using a five point Likert scale. SemEval-2021 Task 1 featured two Sub-tasks: Sub-task 1 focused on single words and Sub-task 2 focused on MWEs. The competition attracted 198 teams in total, of which 54 teams submitted official runs on the test data to Sub-task 1 and 37 to Sub-task 2",
    "checked": true,
    "id": "eedbcbae6a061e44427a64c565b235e478537a20",
    "semantic_title": "",
    "citation_count": 57,
    "authors": [
      "Matthew Shardlow",
      "Richard Evans",
      "Gustavo Henrique Paetzold",
      "Marcos Zampieri"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.2": {
    "title": "OCHADAI-KYOTO at SemEval-2021 Task 1: Enhancing Model Generalization and Robustness for Lexical Complexity Prediction",
    "volume": "workshop",
    "abstract": "We propose an ensemble model for predicting the lexical complexity of words and multiword expressions (MWEs). The model receives as input a sentence with a target word or MWE and outputs its complexity score. Given that a key challenge with this task is the limited size of annotated data, our model relies on pretrained contextual representations from different state-of-the-art transformer-based language models (i.e., BERT and RoBERTa), and on a variety of training methods for further enhancing model generalization and robustness: multi-step fine-tuning and multi-task learning, and adversarial training. Additionally, we propose to enrich contextual representations by adding hand-crafted features during training. Our model achieved competitive results and ranked among the top-10 systems in both sub-tasks",
    "checked": true,
    "id": "e4cdfb01011ad4a10be1858aa7722632b42b9d23",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Yuki Taya",
      "Lis Kanashiro Pereira",
      "Fei Cheng",
      "Ichiro Kobayashi"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.3": {
    "title": "SemEval-2021 Task 2: Multilingual and Cross-lingual Word-in-Context Disambiguation (MCL-WiC)",
    "volume": "workshop",
    "abstract": "In this paper, we introduce the first SemEval task on Multilingual and Cross-Lingual Word-in-Context disambiguation (MCL-WiC). This task allows the largely under-investigated inherent ability of systems to discriminate between word senses within and across languages to be evaluated, dropping the requirement of a fixed sense inventory. Framed as a binary classification, our task is divided into two parts. In the multilingual sub-task, participating systems are required to determine whether two target words, each occurring in a different context within the same language, express the same meaning or not. Instead, in the cross-lingual part, systems are asked to perform the task in a cross-lingual scenario, in which the two target words and their corresponding contexts are provided in two different languages. We illustrate our task, as well as the construction of our manually-created dataset including five languages, namely Arabic, Chinese, English, French and Russian, and the results of the participating systems. Datasets and results are available at: https://github.com/SapienzaNLP/mcl-wic",
    "checked": true,
    "id": "eaffb817385c642cdc2a272d70e802182c71cc47",
    "semantic_title": "",
    "citation_count": 31,
    "authors": [
      "Federico Martelli",
      "Najla Kalach",
      "Gabriele Tola",
      "Roberto Navigli"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.4": {
    "title": "SemEval-2021 Task 4: Reading Comprehension of Abstract Meaning",
    "volume": "workshop",
    "abstract": "This paper introduces the SemEval-2021 shared task 4: Reading Comprehension of Abstract Meaning (ReCAM). This shared task is designed to help evaluate the ability of machines in representing and understanding abstract concepts.Given a passage and the corresponding question, a participating system is expected to choose the correct answer from five candidates of abstract concepts in cloze-style machine reading comprehension tasks. Based on two typical definitions of abstractness, i.e., the imperceptibility and nonspecificity, our task provides three subtasks to evaluate models’ ability in comprehending the two types of abstract meaning and the models’ generalizability. Specifically, Subtask 1 aims to evaluate how well a participating system models concepts that cannot be directly perceived in the physical world. Subtask 2 focuses on models’ ability in comprehending nonspecific concepts located high in a hypernym hierarchy given the context of a passage. Subtask 3 aims to provide some insights into models’ generalizability over the two types of abstractness. During the SemEval-2021 official evaluation period, we received 23 submissions to Subtask 1 and 28 to Subtask 2. The participating teams additionally made 29 submissions to Subtask 3. The leaderboard and competition website can be found at https://competitions.codalab.org/competitions/26153. The data and baseline code are available at https://github.com/boyuanzheng010/SemEval2021-Reading-Comprehension-of-Abstract-Meaning",
    "checked": true,
    "id": "8935c4d8d6b2b97e8c1a8db6b625152da78893ee",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Boyuan Zheng",
      "Xiaoyu Yang",
      "Yu-Ping Ruan",
      "Zhenhua Ling",
      "Quan Liu",
      "Si Wei",
      "Xiaodan Zhu"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.5": {
    "title": "TA-MAMC at SemEval-2021 Task 4: Task-adaptive Pretraining and Multi-head Attention for Abstract Meaning Reading Comprehension",
    "volume": "workshop",
    "abstract": "This paper describes our system used in the SemEval-2021 Task4 Reading Comprehension of Abstract Meaning, achieving 1st for subtask 1 and 2nd for subtask 2 on the leaderboard. We propose an ensemble of ELECTRA-based models with task-adaptive pretraining and a multi-head attention multiple-choice classifier on top of the pre-trained model. The main contributions of our system are 1) revealing the performance discrepancy of different transformer-based pretraining models on the downstream task, 2) presentation of an efficient method to generate large task-adaptive corpora for pretraining. We also investigated several pretraining strategies and contrastive learning objectives. Our system achieves a test accuracy of 95.11 and 94.89 on subtask 1 and subtask 2 respectively",
    "checked": true,
    "id": "fb115b0a71c91e047c635c1777ad233e98f2e78e",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Jing Zhang",
      "Yimeng Zhuang",
      "Yinpei Su"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.6": {
    "title": "SemEval-2021 Task 5: Toxic Spans Detection",
    "volume": "workshop",
    "abstract": "The Toxic Spans Detection task of SemEval-2021 required participants to predict the spans of toxic posts that were responsible for the toxic label of the posts. The task could be addressed as supervised sequence labeling, using training data with gold toxic spans provided by the organisers. It could also be treated as rationale extraction, using classifiers trained on potentially larger external datasets of posts manually annotated as toxic or not, without toxic span annotations. For the supervised sequence labeling approach and evaluation purposes, posts previously labeled as toxic were crowd-annotated for toxic spans. Participants submitted their predicted spans for a held-out test set and were scored using character-based F1. This overview summarises the work of the 36 teams that provided system descriptions",
    "checked": true,
    "id": "76ecdc5441e5e6c07e250c8f45b6d8367acec56f",
    "semantic_title": "",
    "citation_count": 79,
    "authors": [
      "John Pavlopoulos",
      "Jeffrey Sorensen",
      "Léo Laugier",
      "Ion Androutsopoulos"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.7": {
    "title": "SemEval-2021 Task 6: Detection of Persuasion Techniques in Texts and Images",
    "volume": "workshop",
    "abstract": "We describe SemEval-2021 task 6 on Detection of Persuasion Techniques in Texts and Images: the data, the annotation guidelines, the evaluation setup, the results, and the participating systems. The task focused on memes and had three subtasks: (i) detecting the techniques in the text, (ii) detecting the text spans where the techniques are used, and (iii) detecting techniques in the entire meme, i.e., both in the text and in the image. It was a popular task, attracting 71 registrations, and 22 teams that eventually made an official submission on the test set. The evaluation results for the third subtask confirmed the importance of both modalities, the text and the image. Moreover, some teams reported benefits when not just combining the two modalities, e.g., by using early or late fusion, but rather modeling the interaction between them in a joint model",
    "checked": true,
    "id": "51ec6ce7a402445055ad6d88cc2a2bf8e51b1368",
    "semantic_title": "",
    "citation_count": 40,
    "authors": [
      "Dimitar Dimitrov",
      "Bishr Bin Ali",
      "Shaden Shaar",
      "Firoj Alam",
      "Fabrizio Silvestri",
      "Hamed Firooz",
      "Preslav Nakov",
      "Giovanni Da San Martino"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.8": {
    "title": "Alpha at SemEval-2021 Task 6: Transformer Based Propaganda Classification",
    "volume": "workshop",
    "abstract": "This paper describes our system participated in Task 6 of SemEval-2021: the task focuses on multimodal propaganda technique classification and it aims to classify given image and text into 22 classes. In this paper, we propose to use transformer based architecture to fuse the clues from both image and text. We explore two branches of techniques including fine-tuning the text pretrained transformer with extended visual features, and fine-tuning the multimodal pretrained transformers. For the visual features, we have tested both grid features based on ResNet and salient region features from pretrained object detector. Among the pretrained multimodal transformers, we choose ERNIE-ViL, a two-steam cross-attended transformers pretrained on large scale image-caption aligned data. Fine-tuing ERNIE-ViL for our task produce a better performance due to general joint multimodal representation for text and image learned by ERNIE-ViL. Besides, as the distribution of the classification labels is very unbalanced, we also make a further attempt on the loss function and the experiment result shows that focal loss would perform better than cross entropy loss. Last we have won first for subtask C in the final competition",
    "checked": true,
    "id": "e302e591f5738209929a4b2af656717301bd3af3",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Zhida Feng",
      "Jiji Tang",
      "Jiaxiang Liu",
      "Weichong Yin",
      "Shikun Feng",
      "Yu Sun",
      "Li Chen"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.9": {
    "title": "SemEval 2021 Task 7: HaHackathon, Detecting and Rating Humor and Offense",
    "volume": "workshop",
    "abstract": "SemEval 2021 Task 7, HaHackathon, was the first shared task to combine the previously separate domains of humor detection and offense detection. We collected 10,000 texts from Twitter and the Kaggle Short Jokes dataset, and had each annotated for humor and offense by 20 annotators aged 18-70. Our subtasks were binary humor detection, prediction of humor and offense ratings, and a novel controversy task: to predict if the variance in the humor ratings was higher than a specific threshold. The subtasks attracted 36-58 submissions, with most of the participants choosing to use pre-trained language models. Many of the highest performing teams also implemented additional optimization techniques, including task-adaptive training and adversarial training. The results suggest that the participating systems are well suited to humor detection, but that humor controversy is a more challenging task. We discuss which models excel in this task, which auxiliary techniques boost their performance, and analyze the errors which were not captured by the best systems",
    "checked": true,
    "id": "3e9313d1bbb9e27d03541adc3dba03ec95dcb76f",
    "semantic_title": "",
    "citation_count": 57,
    "authors": [
      "J. A. Meaney",
      "Steven Wilson",
      "Luis Chiruzzo",
      "Adam Lopez",
      "Walid Magdy"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.10": {
    "title": "LangResearchLab NC at SemEval-2021 Task 1: Linguistic Feature Based Modelling for Lexical Complexity",
    "volume": "workshop",
    "abstract": "The present work aims at assigning a complexity score between 0 and 1 to a target word or phrase in a given sentence. For each Single Word Target, a Random Forest Regressor is trained on a feature set consisting of lexical, semantic, and syntactic information about the target. For each Multiword Target, a set of individual word features is taken along with single word complexities in the feature space. The system yielded the Pearson correlation of 0.7402 and 0.8244 on the test set for the Single and Multiword Targets, respectively",
    "checked": true,
    "id": "1d04e4c879f8176abcc7f28bf93cf2a86defd5eb",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Raksha Agarwal",
      "Niladri Chatterjee"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.11": {
    "title": "Complex words identification using word-level features for SemEval-2020 Task 1",
    "volume": "workshop",
    "abstract": "This article describes a system to predict the complexity of words for the Lexical Complexity Prediction (LCP) shared task hosted at SemEval 2021 (Task 1) with a new annotated English dataset with a Likert scale. Located in the Lexical Semantics track, the task consisted of predicting the complexity value of the words in context. A machine learning approach was carried out based on the frequency of the words and several characteristics added at word level. Over these features, a supervised random forest regression algorithm was trained. Several runs were performed with different values to observe the performance of the algorithm. For the evaluation, our best results reported a M.A.E score of 0.07347, M.S.E. of 0.00938, and R.M.S.E. of 0.096871. Our experiments showed that, with a greater number of characteristics, the precision of the classification increases",
    "checked": true,
    "id": "6287af969dd66455d501c2a51672cd530bf658a1",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Jenny A. Ortiz-Zambrano",
      "Arturo Montejo-Ráez"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.12": {
    "title": "TUDA-CCL at SemEval-2021 Task 1: Using Gradient-boosted Regression Tree Ensembles Trained on a Heterogeneous Feature Set for Predicting Lexical Complexity",
    "volume": "workshop",
    "abstract": "In this paper, we present our systems submitted to SemEval-2021 Task 1 on lexical complexity prediction.The aim of this shared task was to create systems able to predict the lexical complexity of word tokens and bigram multiword expressions within a given sentence context, a continuous value indicating the difficulty in understanding a respective utterance. Our approach relies on gradient boosted regression tree ensembles fitted using a heterogeneous feature set combining linguistic features, static and contextualized word embeddings, psycholinguistic norm lexica, WordNet, word- and character bigram frequencies and inclusion in wordlists to create a model able to assign a word or multiword expression a context-dependent complexity score. We can show that especially contextualised string embeddings can help with predicting lexical complexity",
    "checked": true,
    "id": "ccfcaed684d56e297d2718c8c409c40925ba2921",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Sebastian Gombert",
      "Sabine Bartsch"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.13": {
    "title": "JCT at SemEval-2021 Task 1: Context-aware Representation for Lexical Complexity Prediction",
    "volume": "workshop",
    "abstract": "In this paper, we present our contribution in SemEval-2021 Task 1: Lexical Complexity Prediction, where we integrate linguistic, statistical, and semantic properties of the target word and its context as features within a Machine Learning (ML) framework for predicting lexical complexity. In particular, we use BERT contextualized word embeddings to represent the semantic meaning of the target word and its context. We participated in the sub-task of predicting the complexity score of single words",
    "checked": true,
    "id": "ffe5059fa6854377f7bc94159bc93a1252ba0355",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Chaya Liebeskind",
      "Otniel Elkayam",
      "Shmuel Liebeskind"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.14": {
    "title": "IAPUCP at SemEval-2021 Task 1: Stacking Fine-Tuned Transformers is Almost All You Need for Lexical Complexity Prediction",
    "volume": "workshop",
    "abstract": "This paper describes our submission to SemEval-2021 Task 1: predicting the complexity score for single words. Our model leverages standard morphosyntactic and frequency-based features that proved helpful for Complex Word Identification (a related task), and combines them with predictions made by Transformer-based pre-trained models that were fine-tuned on the Shared Task data. Our submission system stacks all previous models with a LightGBM at the top. One novelty of our approach is the use of multi-task learning for fine-tuning a pre-trained model for both Lexical Complexity Prediction and Word Sense Disambiguation. Our analysis shows that all independent models achieve a good performance in the task, but that stacking them obtains a Pearson correlation of 0.7704, merely 0.018 points behind the winning submission",
    "checked": true,
    "id": "8ca8798d79ac3b654bc42f6fd6e6b5882d1dc81f",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Kervy Rivas Rojas",
      "Fernando Alva-Manchego"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.15": {
    "title": "Uppsala NLP at SemEval-2021 Task 2: Multilingual Language Models for Fine-tuning and Feature Extraction in Word-in-Context Disambiguation",
    "volume": "workshop",
    "abstract": "We describe the Uppsala NLP submission to SemEval-2021 Task 2 on multilingual and cross-lingual word-in-context disambiguation. We explore the usefulness of three pre-trained multilingual language models, XLM-RoBERTa (XLMR), Multilingual BERT (mBERT) and multilingual distilled BERT (mDistilBERT). We compare these three models in two setups, fine-tuning and as feature extractors. In the second case we also experiment with using dependency-based information. We find that fine-tuning is better than feature extraction. XLMR performs better than mBERT in the cross-lingual setting both with fine-tuning and feature extraction, whereas these two models give a similar performance in the multilingual setting. mDistilBERT performs poorly with fine-tuning but gives similar results to the other models when used as a feature extractor. We submitted our two best systems, fine-tuned with XLMR and mBERT",
    "checked": true,
    "id": "2707799fd2fd620e8524aac52c82207e6e7ccb17",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Huiling You",
      "Xingran Zhu",
      "Sara Stymne"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.16": {
    "title": "SkoltechNLP at SemEval-2021 Task 2: Generating Cross-Lingual Training Data for the Word-in-Context Task",
    "volume": "workshop",
    "abstract": "In this paper, we present a system for the solution of the cross-lingual and multilingual word-in-context disambiguation task. Task organizers provided monolingual data in several languages, but no cross-lingual training data were available. To address the lack of the officially provided cross-lingual training data, we decided to generate such data ourselves. We describe a simple yet effective approach based on machine translation and back translation of the lexical units to the original language used in the context of this shared task. In our experiments, we used a neural system based on the XLM-R, a pre-trained transformer-based masked language model, as a baseline. We show the effectiveness of the proposed approach as it allows to substantially improve the performance of this strong neural baseline model. In addition, in this study, we present multiple types of the XLM-R based classifier, experimenting with various ways of mixing information from the first and second occurrences of the target word in two samples",
    "checked": true,
    "id": "735a27e2c722a291d87b31cf1c175f9503fcb3bb",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anton Razzhigaev",
      "Nikolay Arefyev",
      "Alexander Panchenko"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.17": {
    "title": "Zhestyatsky at SemEval-2021 Task 2: ReLU over Cosine Similarity for BERT Fine-tuning",
    "volume": "workshop",
    "abstract": "This paper presents our contribution to SemEval-2021 Task 2: Multilingual and Cross-lingual Word-in-Context Disambiguation (MCL-WiC). Our experiments cover English (EN-EN) sub-track from the multilingual setting of the task. We experiment with several pre-trained language models and investigate an impact of different top-layers on fine-tuning. We find the combination of Cosine Similarity and ReLU activation leading to the most effective fine-tuning procedure. Our best model results in accuracy 92.7%, which is the fourth-best score in EN-EN sub-track",
    "checked": true,
    "id": "2d0406a202e1b43449c2955accf672b8c0f580c9",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Boris Zhestiankin",
      "Maria Ponomareva"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.18": {
    "title": "SzegedAI at SemEval-2021 Task 2: Zero-shot Approach for Multilingual and Cross-lingual Word-in-Context Disambiguation",
    "volume": "workshop",
    "abstract": "In this paper, we introduce our system that we participated with at the multilingual and cross-lingual word-in-context disambiguation SemEval 2021 shared task. In our experiments, we investigated the possibility of using an all-words fine-grained word sense disambiguation system trained purely on sense-annotated data in English and draw predictions on the semantic equivalence of words in context based on the similarity of the ranked lists of the (English) WordNet synsets returned for the target words decisions had to be made for. We overcame the multi,-and cross-lingual aspects of the shared task by applying a multilingual transformer for encoding the texts written in either Arabic, English, French, Russian and Chinese. While our results lag behind top scoring submissions, it has the benefit that it not only provides a binary flag whether two words in their context have the same meaning, but also provides a more tangible output in the form of a ranked list of (English) WordNet synsets irrespective of the language of the input texts. As our framework is designed to be as generic as possible, it can be applied as a baseline for basically any language (supported by the multilingual transformed architecture employed) even in the absence of any additional form of language specific training data",
    "checked": true,
    "id": "16672977b32a25ab5a6c36dc10210db05f971635",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gábor Berend"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.19": {
    "title": "ReCAM@IITK at SemEval-2021 Task 4: BERT and ALBERT based Ensemble for Abstract Word Prediction",
    "volume": "workshop",
    "abstract": "This paper describes our system for Task 4 of SemEval-2021: Reading Comprehension of Abstract Meaning (ReCAM). We participated in all subtasks where the main goal was to predict an abstract word missing from a statement. We fine-tuned the pre-trained masked language models namely BERT and ALBERT and used an Ensemble of these as our submitted system on Subtask 1 (ReCAM-Imperceptibility) and Subtask 2 (ReCAM-Nonspecificity). For Subtask 3 (ReCAM-Intersection), we submitted the ALBERT model as it gives the best results. We tried multiple approaches and found that Masked Language Modeling(MLM) based approach works the best",
    "checked": true,
    "id": "3bb2bf6cba1172f11f35e968a9d892f3eb9a4316",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Abhishek Mittal",
      "Ashutosh Modi"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.20": {
    "title": "ECNU_ICA_1 SemEval-2021 Task 4: Leveraging Knowledge-enhanced Graph Attention Networks for Reading Comprehension of Abstract Meaning",
    "volume": "workshop",
    "abstract": "This paper describes our system for SemEval-2021 Task 4: Reading Comprehension of Abstract Meaning. To accomplish this task, we utilize the Knowledge-Enhanced Graph Attention Network (KEGAT) architecture with a novel semantic space transformation strategy. It leverages heterogeneous knowledge to learn adequate evidences, and seeks for an effective semantic space of abstract concepts to better improve the ability of a machine in understanding the abstract meaning of natural language. Experimental results show that our system achieves strong performance on this task in terms of both imperceptibility and nonspecificity",
    "checked": true,
    "id": "ca3db7d9e82d2936d78abf6a123036f45cfd655e",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Pingsheng Liu",
      "Linlin Wang",
      "Qian Zhao",
      "Hao Chen",
      "Yuxi Feng",
      "Xin Lin",
      "Liang He"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.21": {
    "title": "LRG at SemEval-2021 Task 4: Improving Reading Comprehension with Abstract Words using Augmentation, Linguistic Features and Voting",
    "volume": "workshop",
    "abstract": "We present our approaches and methods for SemEval-2021 Task-4 Reading Comprehension of Abstract Meaning. Given a question with a fill-in-the-blank, and a corresponding context, the task is to predict the most suitable word from a list of 5 options. There are three subtasks: Imperceptibility, Non-Specificity and Intersection. We use encoders of transformers-based models pretrained on the MLM task to build our Fill-in-the-blank (FitB) models. Moreover, to model imperceptibility, we define certain linguistic features, and to model non-specificity, we leverage information from hypernyms and hyponyms provided by a lexical database. Specifically, for non-specificity, we try out augmentation techniques, and other statistical techniques. We also propose variants, namely Chunk Voting and Max Context, to take care of input length restrictions for BERT, etc. Additionally, we perform a thorough ablation study, and use Integrated Gradients to explain our predictions on a few samples. Our models achieve accuracies of 75.31% and 77.84%, on the test sets for subtask-I and subtask-II, respectively. For subtask-III, we achieve accuracies of 65.64% and 64.27%",
    "checked": true,
    "id": "364a029521dc8e56e0d05a34d72309862e9565d5",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Abheesht Sharma",
      "Harshit Pandey",
      "Gunjan Chhablani",
      "Yash Bhartia",
      "Tirtharaj Dash"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.22": {
    "title": "IIE-NLP-Eyas at SemEval-2021 Task 4: Enhancing PLM for ReCAM with Special Tokens, Re-Ranking, Siamese Encoders and Back Translation",
    "volume": "workshop",
    "abstract": "This paper introduces our systems for all three subtasks of SemEval-2021 Task 4: Reading Comprehension of Abstract Meaning. To help our model better represent and understand abstract concepts in natural language, we well-design many simple and effective approaches adapted to the backbone model (RoBERTa). Specifically, we formalize the subtasks into the multiple-choice question answering format and add special tokens to abstract concepts, then, the final prediction of QA is considered as the result of subtasks. Additionally, we employ many finetuning tricks to improve the performance. Experimental results show that our approach gains significant performance compared with the baseline systems. Our system achieves eighth rank (87.51%) and tenth rank (89.64%) on the official blind test set of subtask 1 and subtask 2 respectively",
    "checked": true,
    "id": "8a73206026404959d181fa25bdf599c6220e4bd1",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Yuqiang Xie",
      "Luxi Xing",
      "Wei Peng",
      "Yue Hu"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.23": {
    "title": "NLP-IIS@UT at SemEval-2021 Task 4: Machine Reading Comprehension using the Long Document Transformer",
    "volume": "workshop",
    "abstract": "This paper presents a technical report of our submission to the 4th task of SemEval-2021, titled: Reading Comprehension of Abstract Meaning. In this task, we want to predict the correct answer based on a question given a context. Usually, contexts are very lengthy and require a large receptive field from the model. Thus, common contextualized language models like BERT miss fine representation and performance due to the limited capacity of the input tokens. To tackle this problem, we used the longformer model to better process the sequences. Furthermore, we utilized the method proposed in the longformer benchmark on wikihop dataset which improved the accuracy on our task data from (23.01% and 22.95%) achieved by the baselines for subtask 1 and 2, respectively, to (70.30% and 64.38%)",
    "checked": true,
    "id": "b63dcedd4f0c0cd94a25baa1c404d87e76f6ae00",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Hossein Basafa",
      "Sajad Movahedi",
      "Ali Ebrahimi",
      "Azadeh Shakery",
      "Heshaam Faili"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.24": {
    "title": "IITK@Detox at SemEval-2021 Task 5: Semi-Supervised Learning and Dice Loss for Toxic Spans Detection",
    "volume": "workshop",
    "abstract": "In this work, we present our approach and findings for SemEval-2021 Task 5 - Toxic Spans Detection. The task’s main aim was to identify spans to which a given text’s toxicity could be attributed. The task is challenging mainly due to two constraints: the small training dataset and imbalanced class distribution. Our paper investigates two techniques, semi-supervised learning and learning with Self-Adjusting Dice Loss, for tackling these challenges. Our submitted system (ranked ninth on the leader board) consisted of an ensemble of various pre-trained Transformer Language Models trained using either of the above-proposed techniques",
    "checked": true,
    "id": "0f0a695bf03b8171c6645421d72fa2398751084a",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Archit Bansal",
      "Abhay Kaushik",
      "Ashutosh Modi"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.25": {
    "title": "UniParma at SemEval-2021 Task 5: Toxic Spans Detection Using CharacterBERT and Bag-of-Words Model",
    "volume": "workshop",
    "abstract": "With the ever-increasing availability of digital information, toxic content is also on the rise. Therefore, the detection of this type of language is of paramount importance. We tackle this problem utilizing a combination of a state-of-the-art pre-trained language model (CharacterBERT) and a traditional bag-of-words technique. Since the content is full of toxic words that have not been written according to their dictionary spelling, attendance to individual characters is crucial. Therefore, we use CharacterBERT to extract features based on the word characters. It consists of a CharacterCNN module that learns character embeddings from the context. These are, then, fed into the well-known BERT architecture. The bag-of-words method, on the other hand, further improves upon that by making sure that some frequently used toxic words get labeled accordingly. With a ∼4 percent difference from the first team, our system ranked 36 th in the competition. The code is available for further research and reproduction of the results",
    "checked": true,
    "id": "d33713b55f6e79270a529cdcce4843c70a051f83",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Akbar Karimi",
      "Leonardo Rossi",
      "Andrea Prati"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.26": {
    "title": "UPB at SemEval-2021 Task 5: Virtual Adversarial Training for Toxic Spans Detection",
    "volume": "workshop",
    "abstract": "The real-world impact of polarization and toxicity in the online sphere marked the end of 2020 and the beginning of this year in a negative way. Semeval-2021, Task 5 - Toxic Spans Detection is based on a novel annotation of a subset of the Jigsaw Unintended Bias dataset and is the first language toxicity detection task dedicated to identifying the toxicity-level spans. For this task, participants had to automatically detect character spans in short comments that render the message as toxic. Our model considers applying Virtual Adversarial Training in a semi-supervised setting during the fine-tuning process of several Transformer-based models (i.e., BERT and RoBERTa), in combination with Conditional Random Fields. Our approach leads to performance improvements and more robust models, enabling us to achieve an F1-score of 65.73% in the official submission and an F1-score of 66.13% after further tuning during post-evaluation",
    "checked": true,
    "id": "a7fc7116bcb32ade8792a3026d655a4eb1b1ce6f",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Andrei Paraschiv",
      "Dumitru-Clementin Cercel",
      "Mihai Dascalu"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.27": {
    "title": "NLRG at SemEval-2021 Task 5: Toxic Spans Detection Leveraging BERT-based Token Classification and Span Prediction Techniques",
    "volume": "workshop",
    "abstract": "Toxicity detection of text has been a popular NLP task in the recent years. In SemEval-2021 Task-5 Toxic Spans Detection, the focus is on detecting toxic spans within English passages. Most state-of-the-art span detection approaches employ various techniques, each of which can be broadly classified into Token Classification or Span Prediction approaches. In our paper, we explore simple versions of both of these approaches and their performance on the task. Specifically, we use BERT-based models - BERT, RoBERTa, and SpanBERT for both approaches. We also combine these approaches and modify them to bring improvements for Toxic Spans prediction. To this end, we investigate results on four hybrid approaches - Multi-Span, Span+Token, LSTM-CRF, and a combination of predicted offsets using union/intersection. Additionally, we perform a thorough ablative analysis and analyze our observed results. Our best submission - a combination of SpanBERT Span Predictor and RoBERTa Token Classifier predictions - achieves an F1 score of 0.6753 on the test set. Our best post-eval F1 score is 0.6895 on intersection of predicted offsets from top-3 RoBERTa Token Classification checkpoints. These approaches improve the performance by 3% on average than those of the shared baseline models - RNNSL and SpaCy NER",
    "checked": true,
    "id": "126f4c1a277e6fce7cfaa4351212d81bbbe0fb02",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Gunjan Chhablani",
      "Abheesht Sharma",
      "Harshit Pandey",
      "Yash Bhartia",
      "Shan Suthaharan"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.28": {
    "title": "UoB at SemEval-2021 Task 5: Extending Pre-Trained Language Models to Include Task and Domain-Specific Information for Toxic Span Prediction",
    "volume": "workshop",
    "abstract": "Toxicity is pervasive in social media and poses a major threat to the health of online communities. The recent introduction of pre-trained language models, which have achieved state-of-the-art results in many NLP tasks, has transformed the way in which we approach natural language processing. However, the inherent nature of pre-training means that they are unlikely to capture task-specific statistical information or learn domain-specific knowledge. Additionally, most implementations of these models typically do not employ conditional random fields, a method for simultaneous token classification. We show that these modifications can improve model performance on the Toxic Spans Detection task at SemEval-2021 to achieve a score within 4 percentage points of the top performing team",
    "checked": true,
    "id": "f4426194b9cbce662c6962529c07b7dff267bc10",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Erik Yan",
      "Harish Tayyar Madabushi"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.29": {
    "title": "Cisco at SemEval-2021 Task 5: What's Toxic?: Leveraging Transformers for Multiple Toxic Span Extraction from Online Comments",
    "volume": "workshop",
    "abstract": "Social network platforms are generally used to share positive, constructive, and insightful content. However, in recent times, people often get exposed to objectionable content like threat, identity attacks, hate speech, insults, obscene texts, offensive remarks or bullying. Existing work on toxic speech detection focuses on binary classification or on differentiating toxic speech among a small set of categories. This paper describes the system proposed by team Cisco for SemEval-2021 Task 5: Toxic Spans Detection, the first shared task focusing on detecting the spans in the text that attribute to its toxicity, in English language. We approach this problem primarily in two ways: a sequence tagging approach and a dependency parsing approach. In our sequence tagging approach we tag each token in a sentence under a particular tagging scheme. Our best performing architecture in this approach also proved to be our best performing architecture overall with an F1 score of 0.6922, thereby placing us 7th on the final evaluation phase leaderboard. We also explore a dependency parsing approach where we extract spans from the input sentence under the supervision of target span boundaries and rank our spans using a biaffine model. Finally, we also provide a detailed analysis of our results and model performance in our paper",
    "checked": true,
    "id": "6c1af1ba3e4bdaff5c3d3b0a585f2ca91bd188a5",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Sreyan Ghosh",
      "Sonal Kumar"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.30": {
    "title": "MedAI at SemEval-2021 Task 5: Start-to-end Tagging Framework for Toxic Spans Detection",
    "volume": "workshop",
    "abstract": "This paper describes the system submitted to SemEval 2021 Task 5: Toxic Spans Detection. The task concerns evaluating systems that detect the spans that make a text toxic when detecting such spans are possible. To address the possibly multi-span detection problem, we develop a start-to-end tagging framework on top of RoBERTa based language model. Besides, we design a custom loss function that takes distance into account. In comparison to other participating teams, our system has achieved 69.03% F1 score, which is slightly lower (-1.8 and -1.73) than the top 1(70.83%) and top 2 (70.77%), respectively",
    "checked": true,
    "id": "e8347ec669c571aa2f28e8e00f6adea1f7e2ac2d",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Zhen Wang",
      "Hongjie Fan",
      "Junfei Liu"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.31": {
    "title": "HamiltonDinggg at SemEval-2021 Task 5: Investigating Toxic Span Detection using RoBERTa Pre-training",
    "volume": "workshop",
    "abstract": "This paper presents our system submission to task 5: Toxic Spans Detection of the SemEval-2021 competition. The competition aims at detecting the spans that make a toxic span toxic. In this paper, we demonstrate our system for detecting toxic spans, which includes expanding the toxic training set with Local Interpretable Model-Agnostic Explanations (LIME), fine-tuning RoBERTa model for detection, and error analysis. We found that feeding the model with an expanded training set using Reddit comments of polarized-toxicity and labeling with LIME on top of logistic regression classification could help RoBERTa more accurately learn to recognize toxic spans. We achieved a span-level F1 score of 0.6715 on the testing phase. Our quantitative and qualitative results show that the predictions from our system could be a good supplement to the gold training set’s annotations",
    "checked": true,
    "id": "bc19c03ff225083d6e8eba25a9300a474f3515bc",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Huiyang Ding",
      "David Jurgens"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.32": {
    "title": "WVOQ at SemEval-2021 Task 6: BART for Span Detection and Classification",
    "volume": "workshop",
    "abstract": "Simultaneous span detection and classification is a task not currently addressed in standard NLP frameworks. The present paper describes why and how an EncoderDecoder model was used to combine span detection and classification to address subtask 2 of SemEval-2021 Task 6",
    "checked": true,
    "id": "f478f62b4904ffc2d45f1b192ad224a9becda53f",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Cees Roele"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.33": {
    "title": "HumorHunter at SemEval-2021 Task 7: Humor and Offense Recognition with Disentangled Attention",
    "volume": "workshop",
    "abstract": "In this paper, we describe our system submitted to SemEval 2021 Task 7: HaHackathon: Detecting and Rating Humor and Offense. The task aims at predicting whether the given text is humorous, the average humor rating given by the annotators, and whether the humor rating is controversial. In addition, the task also involves predicting how offensive the text is. Our approach adopts the DeBERTa architecture with disentangled attention mechanism, where the attention scores between words are calculated based on their content vectors and relative position vectors. We also took advantage of the pre-trained language models and fine-tuned the DeBERTa model on all the four subtasks. We experimented with several BERT-like structures and found that the large DeBERTa model generally performs better. During the evaluation phase, our system achieved an F-score of 0.9480 on subtask 1a, an RMSE of 0.5510 on subtask 1b, an F-score of 0.4764 on subtask 1c, and an RMSE of 0.4230 on subtask 2a (rank 3 on the leaderboard)",
    "checked": true,
    "id": "676dc1bfb68c969a1ce8b135fbf215421df5f0a0",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Yubo Xie",
      "Junze Li",
      "Pearl Pu"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.34": {
    "title": "Grenzlinie at SemEval-2021 Task 7: Detecting and Rating Humor and Offense",
    "volume": "workshop",
    "abstract": "This paper introduces the result of Team Grenzlinie’s experiment in SemEval-2021 task 7: HaHackathon: Detecting and Rating Humor and Offense. This task has two subtasks. Subtask1 includes the humor detection task, the humor rating prediction task, and the humor controversy detection task. Subtask2 is an offensive rating prediction task. Detection task is a binary classification task, and the rating prediction task is a regression task between 0 to 5. 0 means the task is not humorous or not offensive, 5 means the task is very humorous or very offensive. For all the tasks, this paper chooses RoBERTa as the pre-trained model. In classification tasks, Bi-LSTM and adversarial training are adopted. In the regression task, the Bi-LSTM is also adopted. And then we propose a new approach named compare method. Finally, our system achieves an F1-score of 95.05% in the humor detection task, F1-score of 61.74% in the humor controversy detection task, 0.6143 RMSE in humor rating task, 0.4761 RMSE in the offensive rating task on the test datasets",
    "checked": true,
    "id": "9ee7c7ce62071fa5d15f3e817228c8879c60c1fd",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Renyuan Liu",
      "Xiaobing Zhou"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.35": {
    "title": "abcbpc at SemEval-2021 Task 7: ERNIE-based Multi-task Model for Detecting and Rating Humor and Offense",
    "volume": "workshop",
    "abstract": "This paper describes our system participated in Task 7 of SemEval-2021: Detecting and Rating Humor and Offense. The task is designed to detect and score humor and offense which are influenced by subjective factors. In order to obtain semantic information from a large amount of unlabeled data, we applied unsupervised pre-trained language models. By conducting research and experiments, we found that the ERNIE 2.0 and DeBERTa pre-trained models achieved impressive performance in various subtasks. Therefore, we applied the above pre-trained models to fine-tune the downstream neural network. In the process of fine-tuning the model, we adopted multi-task training strategy and ensemble learning method. Based on the above strategy and method, we achieved RMSE of 0.4959 for subtask 1b, and finally won the first place",
    "checked": true,
    "id": "57aef9a3786ae50a91f51cf5176061269eac4b3c",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Chao Pang",
      "Xiaoran Fan",
      "Weiyue Su",
      "Xuyi Chen",
      "Shuohuan Wang",
      "Jiaxiang Liu",
      "Xuan Ouyang",
      "Shikun Feng",
      "Yu Sun"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.36": {
    "title": "Humor@IITK at SemEval-2021 Task 7: Large Language Models for Quantifying Humor and Offensiveness",
    "volume": "workshop",
    "abstract": "Humor and Offense are highly subjective due to multiple word senses, cultural knowledge, and pragmatic competence. Hence, accurately detecting humorous and offensive texts has several compelling use cases in Recommendation Systems and Personalized Content Moderation. However, due to the lack of an extensive labeled dataset, most prior works in this domain haven’t explored large neural models for subjective humor understanding. This paper explores whether large neural models and their ensembles can capture the intricacies associated with humor/offense detection and rating. Our experiments on the SemEval-2021 Task 7: HaHackathon show that we can develop reasonable humor and offense detection systems with such models. Our models are ranked 3rd in subtask 1b and consistently ranked around the top 33% of the leaderboard for the remaining subtasks",
    "checked": true,
    "id": "5c9319f12921a974c83a66fa4b09efb6afd0eeb0",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Aishwarya Gupta",
      "Avik Pal",
      "Bholeshwar Khurana",
      "Lakshay Tyagi",
      "Ashutosh Modi"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.37": {
    "title": "RoMa at SemEval-2021 Task 7: A Transformer-based Approach for Detecting and Rating Humor and Offense",
    "volume": "workshop",
    "abstract": "In this paper we describe the systems used by the RoMa team in the shared task on Detecting and Rating Humor and Offense (HaHackathon) at SemEval 2021. Our systems rely on data representations learned through fine-tuned neural language models. Particularly, we explore two distinct architectures. The first one is based on a Siamese Neural Network (SNN) combined with a graph-based clustering method. The SNN model is used for learning a latent space where instances of humor and non-humor can be distinguished. The clustering method is applied to build prototypes of both classes which are used for training and classifying new messages. The second one combines neural language model representations with a linear regression model which makes the final ratings. Our systems achieved the best results for humor classification using model one, whereas for offensive and humor rating the second model obtained better performance. In the case of the controversial humor prediction, the most significant improvement was achieved by a fine-tuning of the neural language model. In general, the results achieved are encouraging and give us a starting point for further improvements",
    "checked": true,
    "id": "082cb119208e4cc088b3fa11b33914531a4273cc",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Roberto Labadie",
      "Mariano Jason Rodriguez",
      "Reynier Ortega",
      "Paolo Rosso"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.38": {
    "title": "SemEval-2021 Task 8: MeasEval – Extracting Counts and Measurements and their Related Contexts",
    "volume": "workshop",
    "abstract": "We describe MeasEval, a SemEval task of extracting counts, measurements, and related context from scientific documents, which is of significant importance to the creation of Knowledge Graphs that distill information from the scientific literature. This is a new task in 2021, for which over 75 submissions from 25 participants were received. We expect the data developed for this task and the findings reported to be valuable to the scientific knowledge extraction, metrology, and automated knowledge base construction communities",
    "checked": true,
    "id": "3132c476d080e661a3f61fefe7d8ab424d678c35",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Corey Harper",
      "Jessica Cox",
      "Curt Kohler",
      "Antony Scerri",
      "Ron Daniel Jr.",
      "Paul Groth"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.39": {
    "title": "SemEval-2021 Task 9: Fact Verification and Evidence Finding for Tabular Data in Scientific Documents (SEM-TAB-FACTS)",
    "volume": "workshop",
    "abstract": "Understanding tables is an important and relevant task that involves understanding table structure as well as being able to compare and contrast information within cells. In this paper, we address this challenge by presenting a new dataset and tasks that addresses this goal in a shared task in SemEval 2020 Task 9: Fact Verification and Evidence Finding for Tabular Data in Scientific Documents (SEM-TAB-FACTS). Our dataset contains 981 manually-generated tables and an auto-generated dataset of 1980 tables providing over 180K statement and over 16M evidence annotations. SEM-TAB-FACTS featured two sub-tasks. In sub-task A, the goal was to determine if a statement is supported, refuted or unknown in relation to a table. In sub-task B, the focus was on identifying the specific cells of a table that provide evidence for the statement. 69 teams signed up to participate in the task with 19 successful submissions to subtask A and 12 successful submissions to subtask B. We present our results and main findings from the competition",
    "checked": true,
    "id": "46efbc5108e3176ebb6f4df74d38ba16f6d3ed0c",
    "semantic_title": "",
    "citation_count": 16,
    "authors": [
      "Nancy X. R. Wang",
      "Diwakar Mahajan",
      "Marina Danilevsky",
      "Sara Rosenthal"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.40": {
    "title": "BreakingBERT@IITK at SemEval-2021 Task 9: Statement Verification and Evidence Finding with Tables",
    "volume": "workshop",
    "abstract": "Recently, there has been an interest in the research on factual verification and prediction over structured data like tables and graphs. To circumvent any false news incident, it is necessary to not only model and predict over structured data efficiently but also to explain those predictions. In this paper, as the part of the SemEval-2021 Task 9, we tackle the problem of fact verification and evidence finding over tabular data. There are two subtasks, in which given a table and a statement/fact, the subtask A is to determine whether the statement is inferred from the tabular data and the subtask B is to determine which cells in the table provide evidence for the former subtask. We make a comparison of the baselines and state of the art approaches over the given SemTabFact dataset. We also propose a novel approach CellBERT to solve the task of evidence finding, as a form of Natural Language Inference task. We obtain a 3-way F1 score of 0.69 on subtask A and an F1 score of 0.65 on subtask B",
    "checked": true,
    "id": "2e1da33b4562b4da2e7a61292c0caea6dd2525b8",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Aditya Jindal",
      "Ankur Gupta",
      "Jaya Srivastava",
      "Preeti Menghwani",
      "Vijit Malik",
      "Vishesh Kaushik",
      "Ashutosh Modi"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.41": {
    "title": "SemEval-2021 Task 12: Learning with Disagreements",
    "volume": "workshop",
    "abstract": "Disagreement between coders is ubiquitous in virtually all datasets annotated with human judgements in both natural language processing and computer vision. However, most supervised machine learning methods assume that a single preferred interpretation exists for each item, which is at best an idealization. The aim of the SemEval-2021 shared task on learning with disagreements (Le-Wi-Di) was to provide a unified testing framework for methods for learning from data containing multiple and possibly contradictory annotations covering the best-known datasets containing information about disagreements for interpreting language and classifying images. In this paper we describe the shared task and its results",
    "checked": true,
    "id": "8acb6c8b3632b65114ea90be0d7fff082b68c0f0",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Alexandra Uma",
      "Tommaso Fornaciari",
      "Anca Dumitrache",
      "Tristan Miller",
      "Jon Chamberlain",
      "Barbara Plank",
      "Edwin Simpson",
      "Massimo Poesio"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.42": {
    "title": "SemEval-2021 Task 10: Source-Free Domain Adaptation for Semantic Processing",
    "volume": "workshop",
    "abstract": "This paper presents the Source-Free Domain Adaptation shared task held within SemEval-2021. The aim of the task was to explore adaptation of machine-learning models in the face of data sharing constraints. Specifically, we consider the scenario where annotations exist for a domain but cannot be shared. Instead, participants are provided with models trained on that (source) data. Participants also receive some labeled data from a new (development) domain on which to explore domain adaptation algorithms. Participants are then tested on data representing a new (target) domain. We explored this scenario with two different semantic tasks: negation detection (a text classification task) and time expression recognition (a sequence tagging task)",
    "checked": true,
    "id": "a75c1282ae25981ff0ac50b5c2623f1dd9e6f1d2",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Egoitz Laparra",
      "Xin Su",
      "Yiyun Zhao",
      "Özlem Uzuner",
      "Timothy Miller",
      "Steven Bethard"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.43": {
    "title": "BLCUFIGHT at SemEval-2021 Task 10: Novel Unsupervised Frameworks For Source-Free Domain Adaptation",
    "volume": "workshop",
    "abstract": "Domain adaptation assumes that samples from source and target domains are freely accessible during a training phase. However, such assumption is rarely plausible in the real-world and may causes data-privacy issues, especially when the label of the source domain can be a sensitive attribute as an identifier. SemEval-2021 task 10 focuses on these issues. We participate in the task and propose novel frameworks based on self-training method. In our systems, two different frameworks are designed to solve text classification and sequence labeling. These approaches are tested to be effective which ranks the third among all system in subtask A, and ranks the first among all system in subtask B",
    "checked": true,
    "id": "b6a14bd98dfb32bbe89661b9a37c0ef097e6dc3d",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weikang Wang",
      "Yi Wu",
      "Yixiang Liu",
      "Pengyuan Liu"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.44": {
    "title": "SemEval-2021 Task 11: NLPContributionGraph - Structuring Scholarly NLP Contributions for a Research Knowledge Graph",
    "volume": "workshop",
    "abstract": "There is currently a gap between the natural language expression of scholarly publications and their structured semantic content modeling to enable intelligent content search. With the volume of research growing exponentially every year, a search feature operating over semantically structured content is compelling. The SemEval-2021 Shared Task NLPContributionGraph (a.k.a. ‘the NCG task’) tasks participants to develop automated systems that structure contributions from NLP scholarly articles in the English language. Being the first-of-its-kind in the SemEval series, the task released structured data from NLP scholarly articles at three levels of information granularity, i.e. at sentence-level, phrase-level, and phrases organized as triples toward Knowledge Graph (KG) building. The sentence-level annotations comprised the few sentences about the article’s contribution. The phrase-level annotations were scientific term and predicate phrases from the contribution sentences. Finally, the triples constituted the research overview KG. For the Shared Task, participating systems were then expected to automatically classify contribution sentences, extract scientific terms and relations from the sentences, and organize them as KG triples. Overall, the task drew a strong participation demographic of seven teams and 27 participants. The best end-to-end task system classified contribution sentences at 57.27% F1, phrases at 46.41% F1, and triples at 22.28% F1. While the absolute performance to generate triples remains low, as conclusion to the article, the difficulty of producing such data and as a consequence of modeling it is highlighted",
    "checked": true,
    "id": "12d6b59b42bb44c5cd5c84d673079782c6e403f4",
    "semantic_title": "",
    "citation_count": 19,
    "authors": [
      "Jennifer D’Souza",
      "Sören Auer",
      "Ted Pedersen"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.45": {
    "title": "UIUC_BioNLP at SemEval-2021 Task 11: A Cascade of Neural Models for Structuring Scholarly NLP Contributions",
    "volume": "workshop",
    "abstract": "We propose a cascade of neural models that performs sentence classification, phrase recognition, and triple extraction to automatically structure the scholarly contributions of NLP publications. To identify the most important contribution sentences in a paper, we used a BERT-based classifier with positional features (Subtask 1). A BERT-CRF model was used to recognize and characterize relevant phrases in contribution sentences (Subtask 2). We categorized the triples into several types based on whether and how their elements were expressed in text, and addressed each type using separate BERT-based classifiers as well as rules (Subtask 3). Our system was officially ranked second in Phase 1 evaluation and first in both parts of Phase 2 evaluation. After fixing a submission error in Pharse 1, our approach yields the best results overall. In this paper, in addition to a system description, we also provide further analysis of our results, highlighting its strengths and limitations. We make our code publicly available at https://github.com/Liu-Hy/nlp-contrib-graph",
    "checked": true,
    "id": "7f0ef8999164c71b98ef18da15d7acc2c56ea5c7",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Haoyang Liu",
      "M. Janina Sarol",
      "Halil Kilicoglu"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.46": {
    "title": "KGP at SemEval-2021 Task 8: Leveraging Multi-Staged Language Models for Extracting Measurements, their Attributes and Relations",
    "volume": "workshop",
    "abstract": "SemEval-2021 Task 8: MeasEval aims at improving the machine understanding of measurements in scientific texts through a set of entity and semantic relation extraction sub-tasks on identifying quantity spans along with various attributes and relationships. This paper describes our system, consisting of a three-stage pipeline, that leverages pre-trained language models to extract the quantity spans in the text, followed by intelligent templates to identify units and modifiers. Finally, it identifies the quantity attributes and their relations using language models boosted with a feature re-using hierarchical architecture and multi-task learning. Our submission significantly outperforms the baseline, with the best model from the post-evaluation phase delivering more than 100% increase on F1 (Overall) from the baseline",
    "checked": true,
    "id": "8893567aa71140c22d61c4e5e8f387f04049c4f4",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Neel Karia",
      "Ayush Kaushal",
      "Faraaz Mallick"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.47": {
    "title": "DPR at SemEval-2021 Task 8: Dynamic Path Reasoning for Measurement Relation Extraction",
    "volume": "workshop",
    "abstract": "Scientific documents are replete with measurements mentioned in various formats and styles. As such, in a document with multiple quantities and measured entities, the task of associating each quantity to its corresponding measured entity is challenging. Thus, it is necessary to have a method to efficiently extract all measurements and attributes related to them. To this end, in this paper, we propose a novel model for the task of measurement relation extraction (MRE) whose goal is to recognize the relation between measured entities, quantities, and conditions mentioned in a document. Our model employs a deep translation-based architecture to dynamically induce the important words in the document to classify the relation between a pair of entities. Furthermore, we introduce a novel regularization technique based on Information Bottleneck (IB) to filter out the noisy information from the induced set of important words. Our experiments on the recent SemEval 2021 Task 8 datasets reveal the effectiveness of the proposed model",
    "checked": true,
    "id": "958f9294a8607c6a9e65acd82dbb5eb30d502c9c",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Amir Pouran Ben Veyseh",
      "Franck Dernoncourt",
      "Thien Huu Nguyen"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.48": {
    "title": "CLaC-np at SemEval-2021 Task 8: Dependency DGCNN",
    "volume": "workshop",
    "abstract": "MeasEval aims at identifying quantities along with the entities that are measured with additional properties within English scientific documents. The variety of styles used makes measurements, a most crucial aspect of scientific writing, challenging to extract. This paper presents ablation studies making the case for several preprocessing steps such as specialized tokenization rules. For linguistic structure, we encode dependency trees in a Deep Graph Convolution Network (DGCNN) for multi-task classification",
    "checked": true,
    "id": "f56110fb7607c30d490ec9be6d5c48e8f2fbcf9c",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Nihatha Lathiff",
      "Pavel PK Khloponin",
      "Sabine Bergler"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.49": {
    "title": "CLaC-BP at SemEval-2021 Task 8: SciBERT Plus Rules for MeasEval",
    "volume": "workshop",
    "abstract": "This paper explains the design of a heterogeneous system that ranked eighth in competition in SemEval2021 Task 8. We analyze ablation experiments and demonstrate how the system components, namely tokenizer, unit identifier, modifier classifier, and language model, affect the overall score. We compare our results to similar experiments from the literature and introduce a grouping algorithm developed in the post-evaluation phase that increased our system’s overall score, hypothetically elevating our competition rank from eight to six",
    "checked": true,
    "id": "e04424dfbfea86b63884585298c39b8887740ea3",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Benjamin Therien",
      "Parsa Bagherzadeh",
      "Sabine Bergler"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.50": {
    "title": "THiFly_Queens at SemEval-2021 Task 9: Two-stage Statement Verification with Adaptive Ensembling and Slot-based Operation",
    "volume": "workshop",
    "abstract": "This paper describes our system for verifying statements with tables at SemEval-2021 Task 9. We developed a two-stage verifying system based on the latest table-based pre-trained model GraPPa. Multiple networks are devised to verify different types of statements in the competition dataset and an adaptive model ensembling technique is applied to ensemble models in both stages. A statement-slot-based symbolic operation module is also used in our system to further improve the performance and stability of the system. Our model achieves second place in the 3-way classification and fourth place in the 2-way classification evaluation. Several ablation experiments show the effectiveness of different modules proposed in this paper",
    "checked": true,
    "id": "46e357073f8d2503db0968e8307d537fd484c52d",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Zhou",
      "Kaiyin Zhou",
      "Xien Liu",
      "Ji Wu",
      "Xiaodan Zhu"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.51": {
    "title": "TAPAS at SemEval-2021 Task 9: Reasoning over tables with intermediate pre-training",
    "volume": "workshop",
    "abstract": "We present the TAPAS contribution to the Shared Task on Statement Verification and Evidence Finding with Tables (SemEval 2021 Task 9, Wang et al. (2021)). SEM TAB FACT Task A is a classification task of recognizing if a statement is entailed, neutral or refuted by the content of a given table. We adopt the binary TAPAS model of Eisenschlos et al. (2020) to this task. We learn two binary classification models: A first model to predict if a statement is neutral or non-neutral and a second one to predict if it is entailed or refuted. As the shared task training set contains only entailed or refuted examples, we generate artificial neutral examples to train the first model. Both models are pre-trained using a MASKLM objective, intermediate counter-factual and synthetic data (Eisenschlos et al., 2020) and TABFACT (Chen et al., 2020), a large table entailment dataset. We find that the artificial neutral examples are somewhat effective at training the first model, achieving 68.03 test F1 versus the 60.47 of a majority baseline. For the second stage, we find that the pre-training on the intermediate data and TABFACT improves the results over MASKLM pre-training (68.03 vs 57.01)",
    "checked": true,
    "id": "3d0970fd82e82c1d1ef015da7697f3bcfa71c3df",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Thomas Müller",
      "Julian Eisenschlos",
      "Syrine Krichene"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.52": {
    "title": "BOUN at SemEval-2021 Task 9: Text Augmentation Techniques for Fact Verification in Tabular Data",
    "volume": "workshop",
    "abstract": "In this paper, we present our text augmentation based approach for the Table Statement Support Subtask (Phase A) of SemEval-2021 Task 9. We experiment with different text augmentation techniques such as back translation and synonym swapping using Word2Vec and WordNet. We show that text augmentation techniques lead to 2.5% improvement in F1 on the test set. Further, we investigate the impact of domain adaptation and joint learning on fact verification in tabular data by utilizing the SemTabFacts and TabFact datasets. We observe that joint learning improves the F1 scores on the SemTabFacts and TabFact test sets by 3.31% and 0.77%, respectively",
    "checked": true,
    "id": "e4ad971da4afdcbe61af12c0cd6822cec4785df4",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Abdullatif Köksal",
      "Yusuf Yüksel",
      "Bekir Yıldırım",
      "Arzucan Özgür"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.53": {
    "title": "IITK at SemEval-2021 Task 10: Source-Free Unsupervised Domain Adaptation using Class Prototypes",
    "volume": "workshop",
    "abstract": "Recent progress in deep learning has primarily been fueled by the availability of large amounts of annotated data that is obtained from highly expensive manual annotating pro-cesses. To tackle this issue of availability of annotated data, a lot of research has been done on unsupervised domain adaptation that tries to generate systems for an unlabelled target domain data, given labeled source domain data. However, the availability of annotated or labelled source domain dataset can’t always be guaranteed because of data-privacy issues. This is especially the case with medical data, as it may contain sensitive information of the patients. Source-free domain adaptation (SFDA) aims to resolve this issue by us-ing models trained on the source data instead of using the original annotated source data. In this work, we try to build SFDA systems for semantic processing by specifically focusing on the negation detection subtask of the SemEval2021 Task 10. We propose two approaches -ProtoAUGandAdapt-ProtoAUGthat use the idea of self-entropy to choose reliable and high confidence samples, which are then used for data augmentation and subsequent training of the models. Our methods report an improvement of up to 7% in F1 score over the baseline for the Negation Detection subtask",
    "checked": true,
    "id": "93fd5479b649769106326895f17448db6bdf7d0f",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harshit Kumar",
      "Jinang Shah",
      "Nidhi Hegde",
      "Priyanshu Gupta",
      "Vaibhav Jindal",
      "Ashutosh Modi"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.54": {
    "title": "PTST-UoM at SemEval-2021 Task 10: Parsimonious Transfer for Sequence Tagging",
    "volume": "workshop",
    "abstract": "This paper describes PTST, a source-free unsupervised domain adaptation technique for sequence tagging, and its application to the SemEval-2021 Task 10 on time expression recognition. PTST is an extension of the cross-lingual parsimonious parser transfer framework, which uses high-probability predictions of the source model as a supervision signal in self-training. We extend the framework to a sequence prediction setting, and demonstrate its applicability to unsupervised domain adaptation. PTST achieves F1 score of 79.6% on the official test set, with the precision of 90.1%, the highest out of 14 submissions",
    "checked": true,
    "id": "ae01e401e4d929becefc60780125dbfdb1138188",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kemal Kurniawan",
      "Lea Frermann",
      "Philip Schulz",
      "Trevor Cohn"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.55": {
    "title": "Self-Adapter at SemEval-2021 Task 10: Entropy-based Pseudo-Labeler for Source-free Domain Adaptation",
    "volume": "workshop",
    "abstract": "Source-free domain adaptation is an emerging line of work in deep learning research since it is closely related to the real-world environment. We study the domain adaption in the sequence labeling problem where the model trained on the source domain data is given. We propose two methods: Self-Adapter and Selective Classifier Training. Self-Adapter is a training method that uses sentence-level pseudo-labels filtered by the self-entropy threshold to provide supervision to the whole model. Selective Classifier Training uses token-level pseudo-labels and supervises only the classification layer of the model. The proposed methods are evaluated on data provided by SemEval-2021 task 10 and Self-Adapter achieves 2nd rank performance",
    "checked": true,
    "id": "2f57f6f95951be1b3c24151cc75e8fe00bf7344d",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Sangwon Yoon",
      "Yanghoon Kim",
      "Kyomin Jung"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.56": {
    "title": "The University of Arizona at SemEval-2021 Task 10: Applying Self-training, Active Learning and Data Augmentation to Source-free Domain Adaptation",
    "volume": "workshop",
    "abstract": "This paper describes our systems for negation detection and time expression recognition in SemEval 2021 Task 10, Source-Free Domain Adaptation for Semantic Processing. We show that self-training, active learning and data augmentation techniques can improve the generalization ability of the model on the unlabeled target domain data without accessing source domain data. We also perform detailed ablation studies and error analyses for our time expression recognition systems to identify the source of the performance improvement and give constructive feedback on the temporal normalization annotation guidelines",
    "checked": true,
    "id": "68cad63b93ca60aa7ae8f714f73cff6f14e32cdb",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Xin Su",
      "Yiyun Zhao",
      "Steven Bethard"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.57": {
    "title": "KnowGraph@IITK at SemEval-2021 Task 11: Building Knowledge Graph for NLP Research",
    "volume": "workshop",
    "abstract": "Research in Natural Language Processing is making rapid advances, resulting in the publication of a large number of research papers. Finding relevant research papers and their contribution to the domain is a challenging problem. In this paper, we address this challenge via the SemEval 2021 Task 11: NLPContributionGraph, by developing a system for a research paper contributions-focused knowledge graph over Natural Language Processing literature. The task is divided into three sub-tasks: extracting contribution sentences that show important contributions in the research article, extracting phrases from the contribution sentences, and predicting the information units in the research article together with triplet formation from the phrases. The proposed system is agnostic to the subject domain and can be applied for building a knowledge graph for any area. We found that transformer-based language models can significantly improve existing techniques and utilized the SciBERT-based model. Our first sub-task uses Bidirectional LSTM (BiLSTM) stacked on top of SciBERT model layers, while the second sub-task uses Conditional Random Field (CRF) on top of SciBERT with BiLSTM. The third sub-task uses a combined SciBERT based neural approach with heuristics for information unit prediction and triplet formation from the phrases. Our system achieved F1 score of 0.38, 0.63 and 0.76 in end-to-end pipeline testing, phrase extraction testing and triplet extraction testing respectively",
    "checked": true,
    "id": "a668ff277cc03da00e2f4941f918d55a8fffaa77",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Shashank Shailabh",
      "Sajal Chaurasia",
      "Ashutosh Modi"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.58": {
    "title": "YNU-HPCC at SemEval-2021 Task 11: Using a BERT Model to Extract Contributions from NLP Scholarly Articles",
    "volume": "workshop",
    "abstract": "This paper describes the system we built as the YNU-HPCC team in the SemEval-2021 Task 11: NLPContributionGraph. This task involves first identifying sentences in the given natural language processing (NLP) scholarly articles that reflect research contributions through binary classification; then identifying the core scientific terms and their relation phrases from these contribution sentences by sequence labeling; and finally, these scientific terms and relation phrases are categorized, identified, and organized into subject-predicate-object triples to form a knowledge graph with the help of multiclass classification and multi-label classification. We developed a system for this task using a pre-trained language representation model called BERT that stands for Bidirectional Encoder Representations from Transformers, and achieved good results. The average F1-score for Evaluation Phase 2, Part 1 was 0.4562 and ranked 7th, and the average F1-score for Evaluation Phase 2, Part 2 was 0.6541, and also ranked 7th",
    "checked": true,
    "id": "42ee952d25d8a192a7786f253eea76794a57803f",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Xinge Ma",
      "Jin Wang",
      "Xuejie Zhang"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.59": {
    "title": "ITNLP at SemEval-2021 Task 11: Boosting BERT with Sampling and Adversarial Training for Knowledge Extraction",
    "volume": "workshop",
    "abstract": "This paper describes the winning system in the End-to-end Pipeline phase for the NLPContributionGraph task. The system is composed of three BERT-based models and the three models are used to extract sentences, entities and triples respectively. Experiments show that sampling and adversarial training can greatly boost the system. In End-to-end Pipeline phase, our system got an average F1 of 0.4703, significantly higher than the second-placed system which got an average F1 of 0.3828",
    "checked": true,
    "id": "5472fe25c9fc60917cdbf548eeb6a74ceed69fbe",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Genyu Zhang",
      "Yu Su",
      "Changhong He",
      "Lei Lin",
      "Chengjie Sun",
      "Lili Shan"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.60": {
    "title": "Duluth at SemEval-2021 Task 11: Applying DeBERTa to Contributing Sentence Selection and Dependency Parsing for Entity Extraction",
    "volume": "workshop",
    "abstract": "This paper describes the Duluth system that participated in SemEval-2021 Task 11, NLP Contribution Graph. It details the extraction of contribution sentences and scientific entities and their relations from scholarly articles in the domain of Natural Language Processing. Our solution uses deBERTa for multi-class sentence classification to extract the contributing sentences and their type, and dependency parsing to outline each sentence and extract subject-predicate-object triples. Our system ranked fifth of seven for Phase 1: end-to-end pipeline, sixth of eight for Phase 2 Part 1: phrases and triples, and fifth of eight for Phase 2 Part 2: triples extraction",
    "checked": true,
    "id": "fbd77d22cdaf51b2f85a6f78599851b4f2eca729",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Anna Martin",
      "Ted Pedersen"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.61": {
    "title": "INNOVATORS at SemEval-2021 Task-11: A Dependency Parsing and BERT-based model for Extracting Contribution Knowledge from Scientific Papers",
    "volume": "workshop",
    "abstract": "In this work, we describe our system submission to the SemEval 2021 Task 11: NLP Contribution Graph Challenge. We attempt all the three sub-tasks in the challenge and report our results. Subtask 1 aims to identify the contributing sentences in a given publication. Subtask 2 follows from Subtask 1 to extract the scientific term and predicate phrases from the identified contributing sentences. The final Subtask 3 entails extracting triples (subject, predicate, object) from the phrases and categorizing them under one or more defined information units. With the NLPContributionGraph Shared Task, the organizers formalized the building of a scholarly contributions-focused graph over NLP scholarly articles as an automated task. Our approaches include a BERT-based classification model for identifying the contributing sentences in a research publication, a rule-based dependency parsing for phrase extraction, followed by a CNN-based model for information units classification, and a set of rules for triples extraction. The quantitative results show that we obtain the 5th, 5th, and 7th rank respectively in three evaluation phases. We make our codes available at https://github.com/HardikArora17/SemEval-2021-INNOVATORS",
    "checked": true,
    "id": "c2231435cbf50b26681ba666c374660c7eaf39f1",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Hardik Arora",
      "Tirthankar Ghosal",
      "Sandeep Kumar",
      "Suraj Patwal",
      "Phil Gooch"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.62": {
    "title": "MCL@IITK at SemEval-2021 Task 2: Multilingual and Cross-lingual Word-in-Context Disambiguation using Augmented Data, Signals, and Transformers",
    "volume": "workshop",
    "abstract": "In this work, we present our approach for solving the SemEval 2021 Task 2: Multilingual and Cross-lingual Word-in-Context Disambiguation (MCL-WiC). The task is a sentence pair classification problem where the goal is to detect whether a given word common to both the sentences evokes the same meaning. We submit systems for both the settings - Multilingual (the pair’s sentences belong to the same language) and Cross-Lingual (the pair’s sentences belong to different languages). The training data is provided only in English. Consequently, we employ cross-lingual transfer techniques. Our approach employs fine-tuning pre-trained transformer-based language models, like ELECTRA and ALBERT, for the English task and XLM-R for all other tasks. To improve these systems’ performance, we propose adding a signal to the word to be disambiguated and augmenting our data by sentence pair reversal. We further augment the dataset provided to us with WiC, XL-WiC and SemCor 3.0. Using ensembles, we achieve strong performance in the Multilingual task, placing first in the EN-EN and FR-FR sub-tasks. For the Cross-Lingual setting, we employed translate-test methods and a zero-shot method, using our multilingual models, with the latter performing slightly better",
    "checked": true,
    "id": "6582d9deb6fbaef9ff4961af60f54c8bcccaff33",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Rohan Gupta",
      "Jay Mundra",
      "Deepak Mahajan",
      "Ashutosh Modi"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.63": {
    "title": "HITSZ-HLT at SemEval-2021 Task 5: Ensemble Sequence Labeling and Span Boundary Detection for Toxic Span Detection",
    "volume": "workshop",
    "abstract": "This paper presents the winning system that participated in SemEval-2021 Task 5: Toxic Spans Detection. This task aims to locate those spans that attribute to the text’s toxicity within a text, which is crucial for semi-automated moderation in online discussions. We formalize this task as the Sequence Labeling (SL) problem and the Span Boundary Detection (SBD) problem separately and employ three state-of-the-art models. Next, we integrate predictions of these models to produce a more credible and complement result. Our system achieves a char-level score of 70.83%, ranking 1/91. In addition, we also explore the lexicon-based method, which is strongly interpretable and flexible in practice",
    "checked": true,
    "id": "aee47444e0d4707394742a9b1bc990674f51954a",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Qinglin Zhu",
      "Zijie Lin",
      "Yice Zhang",
      "Jingyi Sun",
      "Xiang Li",
      "Qihui Lin",
      "Yixue Dang",
      "Ruifeng Xu"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.64": {
    "title": "SarcasmDet at SemEval-2021 Task 7: Detect Humor and Offensive based on Demographic Factors using RoBERTa Pre-trained Model",
    "volume": "workshop",
    "abstract": "This paper presents one of the top winning solution systems for task 7 at SemEval2021, HaHackathon: Detecting and Rating Humor and Offense. This competition is divided into two tasks, task1 with three sub-tasks 1a,1b, and 1c, and task2. The goal for task1 is to predict if the text would be considered humorous or not, and if it is yes, then predict how humorous it is and whether the humor rating would be perceived as controversial. The goal of the task2 is to predict how the text is considered offensive for users in general. Our solution has been developed using RoBERTa pre-trained model with ensemble techniques. The paper describes the submitted solution system’s architecture with the experiments and the hyperparameter tuning that led to this robust system. Our model ranked third and fourth places out of 50 teams in tasks 1c and 1a with F1-Score of 0.6270 and 0.9675, respectively. At the same time, the model ranked one of the top 10 models in task 1b and task 2 with an RMSE scores of 0.5446 and 0.4469, respectively",
    "checked": true,
    "id": "5feeb14ede859d123a2b2a846d90abd8d8934b24",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Dalya Faraj",
      "Malak Abdullah"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.65": {
    "title": "UPB at SemEval-2021 Task 8: Extracting Semantic Information on Measurements as Multi-Turn Question Answering",
    "volume": "workshop",
    "abstract": "Extracting semantic information on measurements and counts is an important topic in terms of analyzing scientific discourses. The 8th task of SemEval-2021: Counts and Measurements (MeasEval) aimed to boost research in this direction by providing a new dataset on which participants train their models to extract meaningful information on measurements from scientific texts. The competition is composed of five subtasks that build on top of each other: (1) quantity span identification, (2) unit extraction from the identified quantities and their value modifier classification, (3) span identification for measured entities and measured properties, (4) qualifier span identification, and (5) relation extraction between the identified quantities, measured entities, measured properties, and qualifiers. We approached these challenges by first identifying the quantities, extracting their units of measurement, classifying them with corresponding modifiers, and afterwards using them to jointly solve the last three subtasks in a multi-turn question answering manner. Our best performing model obtained an overlapping F1-score of 36.91% on the test set",
    "checked": true,
    "id": "825c5ee9dfb9bf729830452132db84872b5e691c",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Andrei-Marius Avram",
      "George-Eduard Zaharia",
      "Dumitru-Clementin Cercel",
      "Mihai Dascalu"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.66": {
    "title": "IITK@LCP at SemEval-2021 Task 1: Classification for Lexical Complexity Regression Task",
    "volume": "workshop",
    "abstract": "This paper describes our contribution to SemEval 2021 Task 1 (Shardlow et al., 2021): Lexical Complexity Prediction. In our approach, we leverage the ELECTRA model and attempt to mirror the data annotation scheme. Although the task is a regression task, we show that we can treat it as an aggregation of several classification and regression models. This somewhat counter-intuitive approach achieved an MAE score of 0.0654 for Sub-Task 1 and MAE of 0.0811 on Sub-Task 2. Additionally, we used the concept of weak supervision signals from Gloss-BERT in our work, and it significantly improved the MAE score in Sub-Task 1",
    "checked": true,
    "id": "740a85d667fa694d81a9cf31b26f92401447099d",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Neil Shirude",
      "Sagnik Mukherjee",
      "Tushar Shandhilya",
      "Ananta Mukherjee",
      "Ashutosh Modi"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.67": {
    "title": "LCP-RIT at SemEval-2021 Task 1: Exploring Linguistic Features for Lexical Complexity Prediction",
    "volume": "workshop",
    "abstract": "This paper describes team LCP-RIT’s submission to the SemEval-2021 Task 1: Lexical Complexity Prediction (LCP). The task organizers provided participants with an augmented version of CompLex (Shardlow et al., 2020), an English multi-domain dataset in which words in context were annotated with respect to their complexity using a five point Likert scale. Our system uses logistic regression and a wide range of linguistic features (e.g. psycholinguistic features, n-grams, word frequency, POS tags) to predict the complexity of single words in this dataset. We analyze the impact of different linguistic features on the classification performance and we evaluate the results in terms of mean absolute error, mean squared error, Pearson correlation, and Spearman correlation",
    "checked": true,
    "id": "988ba22fbd1bf3fd12ab64e622736fa1bb0037f9",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Abhinandan Tejalkumar Desai",
      "Kai North",
      "Marcos Zampieri",
      "Christopher Homan"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.68": {
    "title": "Alejandro Mosquera at SemEval-2021 Task 1: Exploring Sentence and Word Features for Lexical Complexity Prediction",
    "volume": "workshop",
    "abstract": "This paper revisits feature engineering approaches for predicting the complexity level of English words in a particular context using regression techniques. Our best submission to the Lexical Complexity Prediction (LCP) shared task was ranked 3rd out of 48 systems for sub-task 1 and achieved Pearson correlation coefficients of 0.779 and 0.809 for single words and multi-word expressions respectively. The conclusion is that a combination of lexical, contextual and semantic features can still produce strong baselines when compared against human judgement",
    "checked": true,
    "id": "22cb5831661cc87014b8841522720ca19ca2a3dc",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Alejandro Mosquera"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.69": {
    "title": "CompNA at SemEval-2021 Task 1: Prediction of lexical complexity analyzing heterogeneous features",
    "volume": "workshop",
    "abstract": "This paper describes the CompNa model that has been submitted to the Lexical Complexity Prediction (LCP) shared task hosted at SemEval 2021 (Task 1). The solution is based on combining features of different nature through an ensambling method based on Decision Trees and trained using Gradient Boosting. We discuss the results of the model and highlight the features with more predictive capabilities",
    "checked": true,
    "id": "7fb23b41b1655c0fd9041b7cb20e98d033e02a2e",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Giuseppe Vettigli",
      "Antonio Sorgente"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.70": {
    "title": "PolyU CBS-Comp at SemEval-2021 Task 1: Lexical Complexity Prediction (LCP)",
    "volume": "workshop",
    "abstract": "In this contribution, we describe the system presented by the PolyU CBS-Comp Team at the Task 1 of SemEval 2021, where the goal was the estimation of the complexity of words in a given sentence context. Our top system, based on a combination of lexical, syntactic, word embeddings and Transformers-derived features and on a Gradient Boosting Regressor, achieves a top correlation score of 0.754 on the subtask 1 for single words and 0.659 on the subtask 2 for multiword expressions",
    "checked": true,
    "id": "207a1ada897b8fde906658afd3c14cc2d0106e23",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Rong Xiang",
      "Jinghang Gu",
      "Emmanuele Chersoni",
      "Wenjie Li",
      "Qin Lu",
      "Chu-Ren Huang"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.71": {
    "title": "LAST at SemEval-2021 Task 1: Improving Multi-Word Complexity Prediction Using Bigram Association Measures",
    "volume": "workshop",
    "abstract": "This paper describes the system developed by the Laboratoire d’analyse statistique des textes (LAST) for the Lexical Complexity Prediction shared task at SemEval-2021. The proposed system is made up of a LightGBM model fed with features obtained from many word frequency lists, published lexical norms and psychometric data. For tackling the specificity of the multi-word task, it uses bigram association measures. Despite that the only contextual feature used was sentence length, the system achieved an honorable performance in the multi-word task, but poorer in the single word task. The bigram association measures were found useful, but to a limited extent",
    "checked": true,
    "id": "6b44b4d873cc853d3e2693705442eb18b2800734",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Yves Bestgen"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.72": {
    "title": "DeepBlueAI at SemEval-2021 Task 1: Lexical Complexity Prediction with A Deep Ensemble Approach",
    "volume": "workshop",
    "abstract": "Lexical complexity plays an important role in reading comprehension. lexical complexity prediction (LCP) can not only be used as a part of Lexical Simplification systems, but also as a stand-alone application to help people better reading. This paper presents the winning system we submitted to the LCP Shared Task of SemEval 2021 that capable of dealing with both two subtasks. We first perform fine-tuning on numbers of pre-trained language models (PLMs) with various hyperparameters and different training strategies such as pseudo-labelling and data augmentation. Then an effective stacking mechanism is applied on top of the fine-tuned PLMs to obtain the final prediction. Experimental results on the Complex dataset show the validity of our method and we rank first and second for subtask 2 and 1",
    "checked": true,
    "id": "99a989d5f59dc2a4a0488a42f183085e3d76c8d5",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Chunguang Pan",
      "Bingyan Song",
      "Shengguang Wang",
      "Zhipeng Luo"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.73": {
    "title": "CS-UM6P at SemEval-2021 Task 1: A Deep Learning Model-based Pre-trained Transformer Encoder for Lexical Complexity",
    "volume": "workshop",
    "abstract": "Lexical Complexity Prediction (LCP) involves assigning a difficulty score to a particular word or expression, in a text intended for a target audience. In this paper, we introduce a new deep learning-based system for this challenging task. The proposed system consists of a deep learning model, based on pre-trained transformer encoder, for word and Multi-Word Expression (MWE) complexity prediction. First, on top of the encoder’s contextualized word embedding, our model employs an attention layer on the input context and the complex word or MWE. Then, the attention output is concatenated with the pooled output of the encoder and passed to a regression module. We investigate both single-task and joint training on both Sub-Tasks data using multiple pre-trained transformer-based encoders. The obtained results are very promising and show the effectiveness of fine-tuning pre-trained transformers for LCP task",
    "checked": true,
    "id": "47ac7250a0ba38d74c0b0b1cb05c9265c71fba86",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Nabil El Mamoun",
      "Abdelkader El Mahdaouy",
      "Abdellah El Mekki",
      "Kabil Essefar",
      "Ismail Berrada"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.74": {
    "title": "Cambridge at SemEval-2021 Task 1: An Ensemble of Feature-Based and Neural Models for Lexical Complexity Prediction",
    "volume": "workshop",
    "abstract": "This paper describes our submission to the SemEval-2021 shared task on Lexical Complexity Prediction. We approached it as a regression problem and present an ensemble combining four systems, one feature-based and three neural with fine-tuning, frequency pre-training and multi-task learning, achieving Pearson scores of 0.8264 and 0.7556 on the trial and test sets respectively (sub-task 1). We further present our analysis of the results and discuss our findings",
    "checked": true,
    "id": "613d26adf93e51535e0e7fc1e964323526ca1833",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Zheng Yuan",
      "Gladys Tyen",
      "David Strohmaier"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.75": {
    "title": "hub at SemEval-2021 Task 1: Fusion of Sentence and Word Frequency to Predict Lexical Complexity",
    "volume": "workshop",
    "abstract": "In this paper, we propose a method of fusing sentence information and word frequency information for the SemEval 2021 Task 1-Lexical Complexity Prediction (LCP) shared task. In our system, the sentence information comes from the RoBERTa model, and the word frequency information comes from the Tf-Idf algorithm. Use Inception block as a shared layer to learn sentence and word frequency information We described the implementation of our best system and discussed our methods and experiments in the task. The shared task is divided into two sub-tasks. The goal of the two sub-tasks is to predict the complexity of a predetermined word. The shared task is divided into two subtasks. The goal of the two subtasks is to predict the complexity of a predetermined word. The evaluation index of the task is the Pearson correlation coefficient. Our best performance system has Pearson correlation coefficients of 0.7434 and 0.8000 in the single-token subtask test set and the multi-token subtask test set, respectively",
    "checked": true,
    "id": "baf3e4ff94b05be755b4ea0de3c820f3f89605c1",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Bo Huang",
      "Yang Bai",
      "Xiaobing Zhou"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.76": {
    "title": "Manchester Metropolitan at SemEval-2021 Task 1: Convolutional Networks for Complex Word Identification",
    "volume": "workshop",
    "abstract": "We present two convolutional neural networks for predicting the complexity of words and phrases in context on a continuous scale. Both models utilize word and character embeddings alongside lexical features as inputs. Our system displays reasonable results with a Pearson correlation of 0.7754 on the task as a whole. We highlight the limitations of this method in properly assessing the context of the target text, and explore the effectiveness of both systems across a range of genres. Both models were submitted as part of LCP 2021, which focuses on the identification of complex words and phrases as a context dependent, regression based task",
    "checked": true,
    "id": "9394cd33bc89ba7355c6db310ccb6eb1afe48955",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Robert Flynn",
      "Matthew Shardlow"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.77": {
    "title": "UPB at SemEval-2021 Task 1: Combining Deep Learning and Hand-Crafted Features for Lexical Complexity Prediction",
    "volume": "workshop",
    "abstract": "Reading is a complex process which requires proper understanding of texts in order to create coherent mental representations. However, comprehension problems may arise due to hard-to-understand sections, which can prove troublesome for readers, while accounting for their specific language skills. As such, steps towards simplifying these sections can be performed, by accurately identifying and evaluating difficult structures. In this paper, we describe our approach for the SemEval-2021 Task 1: Lexical Complexity Prediction competition that consists of a mixture of advanced NLP techniques, namely Transformer-based language models, pre-trained word embeddings, Graph Convolutional Networks, Capsule Networks, as well as a series of hand-crafted textual complexity features. Our models are applicable on both subtasks and achieve good performance results, with a MAE below 0.07 and a Person correlation of .73 for single word identification, as well as a MAE below 0.08 and a Person correlation of .79 for multiple word targets. Our results are just 5.46% and 6.5% lower than the top scores obtained in the competition on the first and the second subtasks, respectively",
    "checked": true,
    "id": "c553394a602ef7cbfe7900819cdeacb33599616a",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "George-Eduard Zaharia",
      "Dumitru-Clementin Cercel",
      "Mihai Dascalu"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.78": {
    "title": "UTFPR at SemEval-2021 Task 1: Complexity Prediction by Combining BERT Vectors and Classic Features",
    "volume": "workshop",
    "abstract": "We describe the UTFPR systems submitted to the Lexical Complexity Prediction shared task of SemEval 2021. They perform complexity prediction by combining classic features, such as word frequency, n-gram frequency, word length, and number of senses, with BERT vectors. We test numerous feature combinations and machine learning models in our experiments and find that BERT vectors, even if not optimized for the task at hand, are a great complement to classic features. We also find that employing the principle of compositionality can potentially help in phrase complexity prediction. Our systems place 45th out of 55 for single words and 29th out of 38 for phrases",
    "checked": true,
    "id": "e4e93adca1aee11da917e519bf465e7b2070ff57",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Gustavo Henrique Paetzold"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.79": {
    "title": "RG PA at SemEval-2021 Task 1: A Contextual Attention-based Model with RoBERTa for Lexical Complexity Prediction",
    "volume": "workshop",
    "abstract": "In this paper we propose a contextual attention based model with two-stage fine-tune training using RoBERTa. First, we perform the first-stage fine-tune on corpus with RoBERTa, so that the model can learn some prior domain knowledge. Then we get the contextual embedding of context words based on the token-level embedding with the fine-tuned model. And we use Kfold cross-validation to get K models and ensemble them to get the final result. Finally, we attain the 2nd place in the final evaluation phase of sub-task 2 with pearson correlation of 0.8575",
    "checked": true,
    "id": "2829f1532fc28d437f1f854771a88340b8de66da",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Gang Rao",
      "Maochang Li",
      "Xiaolong Hou",
      "Lianxin Jiang",
      "Yang Mo",
      "Jianping Shen"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.80": {
    "title": "CSECU-DSG at SemEval-2021 Task 1: Fusion of Transformer Models for Lexical Complexity Prediction",
    "volume": "workshop",
    "abstract": "Lexical complexity prediction (LCP) conveys the anticipation of the complexity level of a token or a set of tokens in a sentence. It plays a vital role in the improvement of various NLP tasks including lexical simplification, translations, and text generation. However, multiple meaning of a word in multiple circumstances, grammatical complex structure, and the mutual dependency of words in a sentence make it difficult to estimate the lexical complexity. To address these challenges, SemEval-2021 Task 1 introduced a shared task focusing on LCP and this paper presents our participation in this task. We proposed a transformer-based approach with sentence pair regression. We employed two fine-tuned transformer models. Including BERT and RoBERTa to train our model and fuse their predicted score to the complexity estimation. Experimental results demonstrate that our proposed method achieved competitive performance compared to the participants’ systems",
    "checked": true,
    "id": "cc88223dbe0d3cbe0fca8cfd6de9bb62c9aca815",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Abdul Aziz",
      "MD. Akram Hossain",
      "Abu Nowshed Chy"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.81": {
    "title": "CLULEX at SemEval-2021 Task 1: A Simple System Goes a Long Way",
    "volume": "workshop",
    "abstract": "This paper presents the system we submitted to the first Lexical Complexity Prediction (LCP) Shared Task 2021. The Shared Task provides participants with a new English dataset that includes context of the target word. We participate in the single-word complexity prediction sub-task and focus on feature engineering. Our best system is trained on linguistic features and word embeddings (Pearson’s score of 0.7942). We demonstrate, however, that a simpler feature set achieves comparable results and submit a model trained on 36 linguistic features (Pearson’s score of 0.7925)",
    "checked": true,
    "id": "458f4f7799bc3165c4bf37fd6622fff6347b32b0",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Greta Smolenska",
      "Peter Kolb",
      "Sinan Tang",
      "Mironas Bitinis",
      "Héctor Hernández",
      "Elin Asklöv"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.82": {
    "title": "RS_GV at SemEval-2021 Task 1: Sense Relative Lexical Complexity Prediction",
    "volume": "workshop",
    "abstract": "We present the technical report of the system called RS_GV at SemEval-2021 Task 1 on lexical complexity prediction of English words. RS_GV is a neural network using hand-crafted linguistic features in combination with character and word embeddings to predict target words’ complexity. For the generation of the hand-crafted features, we set the target words in relation to their senses. RS_GV predicts the complexity well of biomedical terms but it has problems with the complexity prediction of very complex and very simple target words",
    "checked": true,
    "id": "6e65cbd7f64f6338a9eebbb97304d8b644f0413d",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Regina Stodden",
      "Gayatri Venugopal"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.83": {
    "title": "UNBNLP at SemEval-2021 Task 1: Predicting lexical complexity with masked language models and character-level encoders",
    "volume": "workshop",
    "abstract": "In this paper, we present three supervised systems for English lexical complexity prediction of single and multiword expressions for SemEval-2021 Task 1. We explore the use of statistical baseline features, masked language models, and character-level encoders to predict the complexity of a target token in context. Our best system combines information from these three sources. The results indicate that information from masked language models and character-level encoders can be combined to improve lexical complexity prediction",
    "checked": true,
    "id": "7eaf6144b6e61bea912bf28fe577850c0962519f",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Milton King",
      "Ali Hakimi Parizi",
      "Samin Fakharian",
      "Paul Cook"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.84": {
    "title": "ANDI at SemEval-2021 Task 1: Predicting complexity in context using distributional models, behavioural norms, and lexical resources",
    "volume": "workshop",
    "abstract": "In this paper we describe our participation in the Lexical Complexity Prediction (LCP) shared task of SemEval 2021, which involved predicting subjective ratings of complexity for English single words and multi-word expressions, presented in context. Our approach relies on a combination of distributional models, both context-dependent and context-independent, together with behavioural norms and lexical resources",
    "checked": true,
    "id": "af23874f0dcfa295a186a4d59defcfb10aaedbd7",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Armand Rotaru"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.85": {
    "title": "JUST-BLUE at SemEval-2021 Task 1: Predicting Lexical Complexity using BERT and RoBERTa Pre-trained Language Models",
    "volume": "workshop",
    "abstract": "Predicting the complexity level of a word or a phrase is considered a challenging task. It is even recognized as a crucial step in numerous NLP applications, such as text rearrangements and text simplification. Early research treated the task as a binary classification task, where the systems anticipated the existence of a word’s complexity (complex versus uncomplicated). Other studies had been designed to assess the level of word complexity using regression models or multi-labeling classification models. Deep learning models show a significant improvement over machine learning models with the rise of transfer learning and pre-trained language models. This paper presents our approach that won the first rank in the SemEval-task1 (sub stask1). We have calculated the degree of word complexity from 0-1 within a text. We have been ranked first place in the competition using the pre-trained language models Bert and RoBERTa, with a Pearson correlation score of 0.788",
    "checked": true,
    "id": "07df21fb90f3ab8d51da8cb967f67c87efaa4145",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Tuqa Bani Yaseen",
      "Qusai Ismail",
      "Sarah Al-Omari",
      "Eslam Al-Sobh",
      "Malak Abdullah"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.86": {
    "title": "BigGreen at SemEval-2021 Task 1: Lexical Complexity Prediction with Assembly Models",
    "volume": "workshop",
    "abstract": "This paper describes a system submitted by team BigGreen to LCP 2021 for predicting the lexical complexity of English words in a given context. We assemble a feature engineering-based model with a deep neural network model founded on BERT. While BERT itself performs competitively, our feature engineering-based model helps in extreme cases, eg. separating instances of easy and neutral difficulty. Our handcrafted features comprise a breadth of lexical, semantic, syntactic, and novel phonological measures. Visualizations of BERT attention maps offer insight into potential features that Transformers models may learn when fine-tuned for lexical complexity prediction. Our ensembled predictions score reasonably well for the single word subtask, and we demonstrate how they can be harnessed to perform well on the multi word expression subtask too",
    "checked": true,
    "id": "5fb9f8f093801aa43fa9e44015325ad6d81127ef",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Aadil Islam",
      "Weicheng Ma",
      "Soroush Vosoughi"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.87": {
    "title": "cs60075_team2 at SemEval-2021 Task 1 : Lexical Complexity Prediction using Transformer-based Language Models pre-trained on various text corpora",
    "volume": "workshop",
    "abstract": "The main contribution of this paper is to fine-tune transformer-based language models pre-trained on several text corpora, some being general (E.g., Wikipedia, BooksCorpus), some being the corpora from which the CompLex Dataset was extracted, and others being from other specific domains such as Finance, Law, etc. We perform ablation studies on selecting the transformer models and how their individual complexity scores are aggregated to get the resulting complexity scores. Our method achieves a best Pearson Correlation of 0.784 in sub-task 1 (single word) and 0.836 in sub-task 2 (multiple word expressions)",
    "checked": true,
    "id": "fb862cf70a7c2c91d9fd26459d6f7772cff0f787",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Abhilash Nandy",
      "Sayantan Adak",
      "Tanurima Halder",
      "Sai Mahesh Pokala"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.88": {
    "title": "C3SL at SemEval-2021 Task 1: Predicting Lexical Complexity of Words in Specific Contexts with Sentence Embeddings",
    "volume": "workshop",
    "abstract": "We present our approach to predicting lexical complexity of words in specific contexts, as entered LCP Shared Task 1 at SemEval 2021. The approach consists of separating sentences into smaller chunks, embedding them with Sent2Vec, and reducing the embeddings into a simpler vector used as input to a neural network, the latter for predicting the complexity of words and expressions. Results show that the pre-trained sentence embeddings are not able to capture lexical complexity from the language when applied in cross-domain applications",
    "checked": true,
    "id": "08377e35254da1c66576c137e5ad6dab7331e71e",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Raul Almeida",
      "Hegler Tissot",
      "Marcos Didonet Del Fabro"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.89": {
    "title": "Stanford MLab at SemEval-2021 Task 1: Tree-Based Modelling of Lexical Complexity using Word Embeddings",
    "volume": "workshop",
    "abstract": "This paper presents our system for the single- and multi-word lexical complexity prediction tasks of SemEval Task 1: Lexical Complexity Prediction. Text comprehension depends on the reader’s ability to understand the words present in it; evaluating the lexical complexity of such texts can enable readers to find an appropriate text and systems to tailor a text to an audience’s needs. We present our model pipeline, which applies a combination of embedding-based and manual features to predict lexical complexity on the CompLex English dataset using various tree-based and linear models. Our method is ranked 27 / 54 on single-word prediction and 14 / 37 on multi-word prediction",
    "checked": true,
    "id": "ec069d66c5d8f0d64b7a37d49a7d1173e66c189c",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Erik Rozi",
      "Niveditha Iyer",
      "Gordon Chi",
      "Enok Choe",
      "Kathy J. Lee",
      "Kevin Liu",
      "Patrick Liu",
      "Zander Lack",
      "Jillian Tang",
      "Ethan A. Chi"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.90": {
    "title": "archer at SemEval-2021 Task 1: Contextualising Lexical Complexity",
    "volume": "workshop",
    "abstract": "Evaluating the complexity of a target word in a sentential context is the aim of the Lexical Complexity Prediction task at SemEval-2021. This paper presents the system created to assess single words lexical complexity, combining linguistic and psycholinguistic variables in a set of experiments involving random forest and XGboost regressors. Beyond encoding out-of-context information about the lemma, we implemented features based on pre-trained language models to model the target word’s in-context complexity",
    "checked": true,
    "id": "06520a24df16f68c64f5c951e80d20c64c39185f",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Irene Russo"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.91": {
    "title": "katildakat at SemEval-2021 Task 1: Lexical Complexity Prediction of Single Words and Multi-Word Expressions in English",
    "volume": "workshop",
    "abstract": "This paper describes systems submitted to Se- mEval 2021 Task 1: Lexical Complexity Prediction (LCP). We compare a linear and a non-linear regression models trained to work for both tracks of the task. We show that both systems are able to generalize better when supplied with information about complexities of single word and multi-word expression (MWE) targets simultaneously. This approach proved to be the most beneficial for multi-word expression targets. We also demonstrate that some hand-crafted features differ in their importance for the target types",
    "checked": true,
    "id": "e98aa653aac5f1466a1d99d5dd05c60e04a6d597",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Katja Voskoboinik"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.92": {
    "title": "GX at SemEval-2021 Task 2: BERT with Lemma Information for MCL-WiC Task",
    "volume": "workshop",
    "abstract": "This paper presents the GX system for the Multilingual and Cross-lingual Word-in-Context Disambiguation (MCL-WiC) task. The purpose of the MCL-WiC task is to tackle the challenge of capturing the polysemous nature of words without relying on a fixed sense inventory in a multilingual and cross-lingual setting. To solve the problems, we use context-specific word embeddings from BERT to eliminate the ambiguity between words in different contexts. For languages without an available training corpus, such as Chinese, we use neuron machine translation model to translate the English data released by the organizers to obtain available pseudo-data. In this paper, we apply our system to the English and Chinese multilingual setting and the experimental results show that our method has certain advantages",
    "checked": true,
    "id": "1478f609d5f15a6e595bf9214b9329b9bd3bbc20",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Wanying Xie"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.93": {
    "title": "PALI at SemEval-2021 Task 2: Fine-Tune XLM-RoBERTa for Word in Context Disambiguation",
    "volume": "workshop",
    "abstract": "This paper presents the PALI team’s winning system for SemEval-2021 Task 2: Multilingual and Cross-lingual Word-in-Context Disambiguation. We fine-tune XLM-RoBERTa model to solve the task of word in context disambiguation, i.e., to determine whether the target word in the two contexts contains the same meaning or not. In implementation, we first specifically design an input tag to emphasize the target word in the contexts. Second, we construct a new vector on the fine-tuned embeddings from XLM-RoBERTa and feed it to a fully-connected network to output the probability of whether the target word in the context has the same meaning or not. The new vector is attained by concatenating the embedding of the [CLS] token and the embeddings of the target word in the contexts. In training, we explore several tricks, such as the Ranger optimizer, data augmentation, and adversarial training, to improve the model prediction. Consequently, we attain the first place in all four cross-lingual tasks",
    "checked": true,
    "id": "0468f9995b896885a778f45a2e87b5afc5c85b1a",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Shuyi Xie",
      "Jian Ma",
      "Haiqin Yang",
      "Lianxin Jiang",
      "Yang Mo",
      "Jianping Shen"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.94": {
    "title": "hub at SemEval-2021 Task 2: Word Meaning Similarity Prediction Model Based on RoBERTa and Word Frequency",
    "volume": "workshop",
    "abstract": "This paper introduces the system description of the hub team, which explains the related work and experimental results of our team’s participation in SemEval 2021 Task 2: Multilingual and Cross-lingual Word-in-Context Disambiguation (MCL-WiC). The data of this shared task is mainly some cross-language or multi-language sentence pair corpus. The languages covered in the corpus include English, Chinese, French, Russian, and Arabic. The task goal is to judge whether the same words in these sentence pairs have the same meaning in the sentence. This can be seen as a task of binary classification of sentence pairs. What we need to do is to use our method to determine as accurately as possible the meaning of the words in a sentence pair are the same or different. The model used by our team is mainly composed of RoBERTa and Tf-Idf algorithms. The result evaluation index of task submission is the F1 score. We only participated in the English language task. The final score of the test set prediction results submitted by our team was 84.60",
    "checked": true,
    "id": "66e253428a5641dfc5f36d83a6187671d4e050e9",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Bo Huang",
      "Yang Bai",
      "Xiaobing Zhou"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.95": {
    "title": "Lotus at SemEval-2021 Task 2: Combination of BERT and Paraphrasing for English Word Sense Disambiguation",
    "volume": "workshop",
    "abstract": "In this paper, we describe our proposed methods for the multilingual word-in-Context disambiguation task in SemEval-2021. In this task, systems should determine whether a word that occurs in two different sentences is used with the same meaning or not. We proposed several methods using a pre-trained BERT model. In two of them, we paraphrased sentences and add them as input to the BERT, and in one of them, we used WordNet to add some extra lexical information. We evaluated our proposed methods on test data in SemEval- 2021 task 2",
    "checked": true,
    "id": "dc9e222dcd5fa2d751f27ffcaf4de0400dae41f6",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Niloofar Ranjbar",
      "Hossein Zeinali"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.96": {
    "title": "Cambridge at SemEval-2021 Task 2: Neural WiC-Model with Data Augmentation and Exploration of Representation",
    "volume": "workshop",
    "abstract": "This paper describes the system of the Cambridge team submitted to the SemEval-2021 shared task on Multilingual and Cross-lingual Word-in-Context Disambiguation. Building on top of a pre-trained masked language model, our system is first pre-trained on out-of-domain data, and then fine-tuned on in-domain data. We demonstrate the effectiveness of the proposed two-step training strategy and the benefits of data augmentation from both existing examples and new resources. We further investigate different representations and show that the addition of distance-based features is helpful in the word-in-context disambiguation task. Our system yields highly competitive results in the cross-lingual track without training on any cross-lingual data; and achieves state-of-the-art results in the multilingual track, ranking first in two languages (Arabic and Russian) and second in French out of 171 submitted systems",
    "checked": true,
    "id": "149980675757565b109ec1f8f0fa8ccc43c00045",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Zheng Yuan",
      "David Strohmaier"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.97": {
    "title": "UoB_UK at SemEval 2021 Task 2: Zero-Shot and Few-Shot Learning for Multi-lingual and Cross-lingual Word Sense Disambiguation",
    "volume": "workshop",
    "abstract": "This paper describes our submission to SemEval 2021 Task 2. We compare XLM-RoBERTa Base and Large in the few-shot and zero-shot settings and additionally test the effectiveness of using a k-nearest neighbors classifier in the few-shot setting instead of the more traditional multi-layered perceptron. Our experiments on both the multi-lingual and cross-lingual data show that XLM-RoBERTa Large, unlike the Base version, seems to be able to more effectively transfer learning in a few-shot setting and that the k-nearest neighbors classifier is indeed a more powerful classifier than a multi-layered perceptron when used in few-shot learning",
    "checked": true,
    "id": "5eb454bcef6aa3e412e1abc947089850180abf22",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Li",
      "Harish Tayyar Madabushi",
      "Mark Lee"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.98": {
    "title": "PAW at SemEval-2021 Task 2: Multilingual and Cross-lingual Word-in-Context Disambiguation : Exploring Cross Lingual Transfer, Augmentations and Adversarial Training",
    "volume": "workshop",
    "abstract": "We experiment with XLM RoBERTa for Word in Context Disambiguation in the Multi Lingual and Cross Lingual setting so as to develop a single model having knowledge about both settings. We solve the problem as a binary classification problem and also experiment with data augmentation and adversarial training techniques. In addition, we also experiment with a 2-stage training technique. Our approaches prove to be beneficial for better performance and robustness",
    "checked": true,
    "id": "76ab3cd5340394b8187c1036e7d1679e8f50a1a0",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Harsh Goyal",
      "Aadarsh Singh",
      "Priyanshu Kumar"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.99": {
    "title": "LU-BZU at SemEval-2021 Task 2: Word2Vec and Lemma2Vec performance in Arabic Word-in-Context disambiguation",
    "volume": "workshop",
    "abstract": "This paper presents a set of experiments to evaluate and compare between the performance of using CBOW Word2Vec and Lemma2Vec models for Arabic Word-in-Context (WiC) disambiguation without using sense inventories or sense embeddings. As part of the SemEval-2021 Shared Task 2 on WiC disambiguation, we used the dev.ar-ar dataset (2k sentence pairs) to decide whether two words in a given sentence pair carry the same meaning. We used two Word2Vec models: Wiki-CBOW, a pre-trained model on Arabic Wikipedia, and another model we trained on large Arabic corpora of about 3 billion tokens. Two Lemma2Vec models was also constructed based on the two Word2Vec models. Each of the four models was then used in the WiC disambiguation task, and then evaluated on the SemEval-2021 test.ar-ar dataset. At the end, we reported the performance of different models and compared between using lemma-based and word-based models",
    "checked": true,
    "id": "0967dbc0739109323d1989f92a1cb6490d7e11d2",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Moustafa Al-Hajj",
      "Mustafa Jarrar"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.100": {
    "title": "GlossReader at SemEval-2021 Task 2: Reading Definitions Improves Contextualized Word Embeddings",
    "volume": "workshop",
    "abstract": "Consulting a dictionary or a glossary is a familiar way for many humans to figure out what does a word in a particular context mean. We hypothesize that a system that can select a proper definition for a particular word occurrence can also naturally solve tasks related to word senses. To verify this hypothesis we developed a solution for the Multilingual and Cross-lingual Word-in-Context (MCL-WiC) task, that does not use any of the shared task data or other WiC data for training. Instead, it is trained to embed word definitions from English WordNet and word occurrences in English texts into the same vector space following an approach previously proposed for Word Sense Disambiguation (WSD). To estimate the similarity in meaning of two word occurrences, we compared different metrics in this shared vector space and found that L1-distance between normalized contextualized word embeddings outperforms traditionally employed cosine similarity and several other metrics. To solve the task for languages other than English, we rely on zero-shot cross-lingual transfer capabilities of the multilingual XLM-R masked language model. Despite not using MCL-WiC training data, in the shared task our approach achieves an accuracy of 89.5% on the English test set, which is only 4% less than the best system. In the multilingual subtask zero-shot cross-lingual transfer shows competitive results, that are within 2% from the best systems for Russian, French, and Arabic. In the cross-lingual subtask are within 2-4% from the best systems",
    "checked": true,
    "id": "2a8b7c70e4285032d4816e8e075bb23313588a3e",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Maxim Rachinskiy",
      "Nikolay Arefyev"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.101": {
    "title": "UAlberta at SemEval-2021 Task 2: Determining Sense Synonymy via Translations",
    "volume": "workshop",
    "abstract": "We describe the University of Alberta systems for the SemEval-2021 Word-in-Context (WiC) disambiguation task. We explore the use of translation information for deciding whether two different tokens of the same word correspond to the same sense of the word. Our focus is on developing principled theoretical approaches which are grounded in linguistic phenomena, leading to more explainable models. We show that translations from multiple languages can be leveraged to improve the accuracy on the WiC task",
    "checked": true,
    "id": "07a887a7fb78e81232d9aaf8f339821e719889ee",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Bradley Hauer",
      "Hongchang Bao",
      "Arnob Mallik",
      "Grzegorz Kondrak"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.102": {
    "title": "TransWiC at SemEval-2021 Task 2: Transformer-based Multilingual and Cross-lingual Word-in-Context Disambiguation",
    "volume": "workshop",
    "abstract": "Identifying whether a word carries the same meaning or different meaning in two contexts is an important research area in natural language processing which plays a significant role in many applications such as question answering, document summarisation, information retrieval and information extraction. Most of the previous work in this area rely on language-specific resources making it difficult to generalise across languages. Considering this limitation, our approach to SemEval-2021 Task 2 is based only on pretrained transformer models and does not use any language-specific processing and resources. Despite that, our best model achieves 0.90 accuracy for English-English subtask which is very compatible compared to the best result of the subtask; 0.93 accuracy. Our approach also achieves satisfactory results in other monolingual and cross-lingual language pairs as well",
    "checked": true,
    "id": "6e91cfc05688330aa26395efa8e814e951191186",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Hansi Hettiarachchi",
      "Tharindu Ranasinghe"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.103": {
    "title": "LIORI at SemEval-2021 Task 2: Span Prediction and Binary Classification approaches to Word-in-Context Disambiguation",
    "volume": "workshop",
    "abstract": "This paper presents our approaches to SemEval-2021 Task 2: Multilingual and Cross-lingual Word-in-Context Disambiguation task. The first approach attempted to reformulate the task as a question answering problem, while the second one framed it as a binary classification problem. Our best system, which is an ensemble of XLM-R based binary classifiers trained with data augmentation, is among the 3 best-performing systems for Russian, French and Arabic in the multilingual subtask. In the post-evaluation period, we experimented with batch normalization, subword pooling and target word occurrence aggregation methods, resulting in further performance improvements",
    "checked": true,
    "id": "e4d48b6090994a0f6581402f00e683cc93213ac0",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Adis Davletov",
      "Nikolay Arefyev",
      "Denis Gordeev",
      "Alexey Rey"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.104": {
    "title": "FII_CROSS at SemEval-2021 Task 2: Multilingual and Cross-lingual Word-in-Context Disambiguation",
    "volume": "workshop",
    "abstract": "This paper presents a word-in-context disambiguation system. The task focuses on capturing the polysemous nature of words in a multilingual and cross-lingual setting, without considering a strict inventory of word meanings. The system applies Natural Language Processing algorithms on datasets from SemEval 2021 Task 2, being able to identify the meaning of words for the languages Arabic, Chinese, English, French and Russian, without making use of any additional mono- or multilingual resources",
    "checked": true,
    "id": "537c586910e1958738066387841e872629b5fdae",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ciprian Bodnar",
      "Andrada Tapuc",
      "Cosmin Pintilie",
      "Daniela Gifu",
      "Diana Trandabat"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.105": {
    "title": "XRJL-HKUST at SemEval-2021 Task 4: WordNet-Enhanced Dual Multi-head Co-Attention for Reading Comprehension of Abstract Meaning",
    "volume": "workshop",
    "abstract": "This paper presents our submitted system to SemEval 2021 Task 4: Reading Comprehension of Abstract Meaning. Our system uses a large pre-trained language model as the encoder and an additional dual multi-head co-attention layer to strengthen the relationship between passages and question-answer pairs, following the current state-of-the-art model DUMA. The main difference is that we stack the passage-question and question-passage attention modules instead of calculating parallelly to simulate re-considering process. We also add a layer normalization module to improve the performance of our model. Furthermore, to incorporate our known knowledge about abstract concepts, we retrieve the definitions of candidate answers from WordNet and feed them to the model as extra inputs. Our system, called WordNet-enhanced DUal Multi-head Co-Attention (WN-DUMA), achieves 86.67% and 89.99% accuracy on the official blind test set of subtask 1 and subtask 2 respectively",
    "checked": true,
    "id": "7420689c166a84053f62e6c947b9c5187b725b6e",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Yuxin Jiang",
      "Ziyi Shou",
      "Qijun Wang",
      "Hao Wu",
      "Fangzhen Lin"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.106": {
    "title": "UoR at SemEval-2021 Task 4: Using Pre-trained BERT Token Embeddings for Question Answering of Abstract Meaning",
    "volume": "workshop",
    "abstract": "Most question answering tasks focuses on predicting concrete answers, e.g., named entities. These tasks can be normally achieved by understanding the contexts without additional information required. In Reading Comprehension of Abstract Meaning (ReCAM) task, the abstract answers are introduced. To understand abstract meanings in the context, additional knowledge is essential. In this paper, we propose an approach that leverages the pre-trained BERT Token embeddings as a prior knowledge resource. According to the results, our approach using the pre-trained BERT outperformed the baselines. It shows that the pre-trained BERT token embeddings can be used as additional knowledge for understanding abstract meanings in question answering",
    "checked": true,
    "id": "7ec8526c6b8831192863296d6d320f4a50fb3ddf",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thanet Markchom",
      "Huizhi Liang"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.107": {
    "title": "Noobs at Semeval-2021 Task 4: Masked Language Modeling for abstract answer prediction",
    "volume": "workshop",
    "abstract": "This paper presents the system developed by our team for Semeval 2021 Task 4: Reading Comprehension of Abstract Meaning. The aim of the task was to benchmark the NLP techniques in understanding the abstract concepts present in a passage, and then predict the missing word in a human written summary of the passage. We trained a Roberta-Large model trained with a masked language modeling objective. In cases where this model failed to predict one of the available options, another Roberta-Large model trained as a binary classifier was used to predict correct and incorrect options. We used passage summary generated by Pegasus model and question as inputs. Our best solution was an ensemble of these 2 systems. We achieved an accuracy of 86.22% on subtask 1 and 87.10% on subtask 2",
    "checked": true,
    "id": "c146378fb93bbd3def01bf8e0cd3d6851cf8b89e",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shikhar Shukla",
      "Sarthak Sarthak",
      "Karm Veer Arya"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.108": {
    "title": "ZJUKLAB at SemEval-2021 Task 4: Negative Augmentation with Language Model for Reading Comprehension of Abstract Meaning",
    "volume": "workshop",
    "abstract": "This paper presents our systems for the three Subtasks of SemEval Task4: Reading Comprehension of Abstract Meaning (ReCAM). We explain the algorithms used to learn our models and the process of tuning the algorithms and selecting the best model. Inspired by the similarity of the ReCAM task and the language pre-training, we propose a simple yet effective technology, namely, negative augmentation with language model. Evaluation results demonstrate the effectiveness of our proposed approach. Our models achieve the 4th rank on both official test sets of Subtask 1 and Subtask 2 with an accuracy of 87.9% and an accuracy of 92.8%, respectively. We further conduct comprehensive model analysis and observe interesting error cases, which may promote future researches. The code and dataset used in our paper can be found at https://github.com/CheaSim/SemEval2021. The leaderboard can be found at https://competitions.codalab.org/competitions/26153",
    "checked": true,
    "id": "abff439d0b73fefc6ad6acabf18dfbf1d32ca21c",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Xin Xie",
      "Xiangnan Chen",
      "Xiang Chen",
      "Yong Wang",
      "Ningyu Zhang",
      "Shumin Deng",
      "Huajun Chen"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.109": {
    "title": "PINGAN Omini-Sinitic at SemEval-2021 Task 4:Reading Comprehension of Abstract Meaning",
    "volume": "workshop",
    "abstract": "This paper describes the winning system for subtask 2 and the second-placed system for subtask 1 in SemEval 2021 Task 4: ReadingComprehension of Abstract Meaning. We propose to use pre-trianed Electra discriminator to choose the best abstract word from five candidates. An upper attention and auto denoising mechanism is introduced to process the long sequences. The experiment results demonstrate that this contribution greatly facilitatesthe contextual language modeling in reading comprehension task. The ablation study is also conducted to show the validity of our proposed methods",
    "checked": true,
    "id": "0fa6fa53d0725442a271b8e8b28c15e006bc9308",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Ye Wang",
      "Yanmeng Wang",
      "Haijun Zhu",
      "Bo Zeng",
      "Zhenghong Hao",
      "Shaojun Wang",
      "Jing Xiao"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.110": {
    "title": "NEUer at SemEval-2021 Task 4: Complete Summary Representation by Filling Answers into Question for Matching Reading Comprehension",
    "volume": "workshop",
    "abstract": "SemEval task 4 aims to find a proper option from multiple candidates to resolve the task of machine reading comprehension. Most existing approaches propose to concat question and option together to form a context-aware model. However, we argue that straightforward concatenation can only provide a coarse-grained context for the MRC task, ignoring the specific positions of the option relative to the question. In this paper, we propose a novel MRC model by filling options into the question to produce a fine-grained context (defined as summary) which can better reveal the relationship between option and question. We conduct a series of experiments on the given dataset, and the results show that our approach outperforms other counterparts to a large extent",
    "checked": true,
    "id": "b7e33512e2e7af2f7590bdae5c2c95e1e5902d82",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhixiang Chen",
      "Yikun Lei",
      "Pai Liu",
      "Guibing Guo"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.111": {
    "title": "WLV-RIT at SemEval-2021 Task 5: A Neural Transformer Framework for Detecting Toxic Spans",
    "volume": "workshop",
    "abstract": "In recent years, the widespread use of social media has led to an increase in the generation of toxic and offensive content on online platforms. In response, social media platforms have worked on developing automatic detection methods and employing human moderators to cope with this deluge of offensive content. While various state-of-the-art statistical models have been applied to detect toxic posts, there are only a few studies that focus on detecting the words or expressions that make a post offensive. This motivates the organization of the SemEval-2021 Task 5: Toxic Spans Detection competition, which has provided participants with a dataset containing toxic spans annotation in English posts. In this paper, we present the WLV-RIT entry for the SemEval-2021 Task 5. Our best performing neural transformer model achieves an 0.68 F1-Score. Furthermore, we develop an open-source framework for multilingual detection of offensive spans, i.e., MUDES, based on neural transformers that detect toxic spans in texts",
    "checked": true,
    "id": "06a68e57265c5a7a32bf3884affa418942cae858",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Tharindu Ranasinghe",
      "Diptanu Sarkar",
      "Marcos Zampieri",
      "Alexander Ororbia"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.112": {
    "title": "YNU-HPCC at SemEval-2021 Task 5: Using a Transformer-based Model with Auxiliary Information for Toxic Span Detection",
    "volume": "workshop",
    "abstract": "Toxic span detection requires the detection of spans that make a text toxic instead of simply classifying the text. In this paper, a transformer-based model with auxiliary information is proposed for SemEval-2021 Task 5. The proposed model was implemented based on the BERT-CRF architecture. It consists of three parts: a transformer-based model that can obtain the token representation, an auxiliary information module that combines features from different layers, and an output layer used for the classification. Various BERT-based models, such as BERT, ALBERT, RoBERTa, and XLNET, were used to learn contextual representations. The predictions of these models were assembled to improve the sequence labeling tasks by using a voting strategy. Experimental results showed that the introduced auxiliary information can improve the performance of toxic spans detection. The proposed model ranked 5th of 91 in the competition. The code of this study is available at https://github.com/Chenrj233/semeval2021_task5",
    "checked": true,
    "id": "017ac35a68254d2d51614f8fabe5e622014fe8ad",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruijun Chen",
      "Jin Wang",
      "Xuejie Zhang"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.113": {
    "title": "UIT-ISE-NLP at SemEval-2021 Task 5: Toxic Spans Detection with BiLSTM-CRF and ToxicBERT Comment Classification",
    "volume": "workshop",
    "abstract": "We present our works on SemEval-2021 Task 5 about Toxic Spans Detection. This task aims to build a model for identifying toxic words in whole posts. We use the BiLSTM-CRF model combining with ToxicBERT Classification to train the detection model for identifying toxic words in posts. Our model achieves 62.23% by F1-score on the Toxic Spans Detection task",
    "checked": true,
    "id": "0ebb8f4f048aa94f00c2df9f66b9f89c365a9c84",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Son T. Luu",
      "Ngan Nguyen"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.114": {
    "title": "GHOST at SemEval-2021 Task 5: Is explanation all you need?",
    "volume": "workshop",
    "abstract": "This paper discusses different approaches to the Toxic Spans Detection task. The problem posed by the task was to determine which words contribute mostly to recognising a document as toxic. As opposed to binary classification of entire texts, word-level assessment could be of great use during comment moderation, also allowing for a more in-depth comprehension of the model’s predictions. As the main goal was to ensure transparency and understanding, this paper focuses on the current state-of-the-art approaches based on the explainable AI concepts and compares them to a supervised learning solution with word-level labels. The work consists of two xAI approaches that automatically provide the explanation for models trained for binary classification of toxic documents: an LSTM model with attention as a model-specific approach and the Shapley values for interpreting BERT predictions as a model-agnostic method. The competing approach considers this problem as supervised token classification, where models like BERT and its modifications were tested. The paper aims to explore, compare and assess the quality of predictions for different methods on the task. The advantages of each approach and further research direction are also discussed",
    "checked": true,
    "id": "71b19f0317a1f0a152a5b68fa5cc8f5b80c0ec93",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Kamil Pluciński",
      "Hanna Klimczak"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.115": {
    "title": "GoldenWind at SemEval-2021 Task 5: Orthrus - An Ensemble Approach to Identify Toxicity",
    "volume": "workshop",
    "abstract": "Many new developments to detect and mitigate toxicity are currently being evaluated. We are particularly interested in the correlation between toxicity and the emotions expressed in online posts. While toxicity may be disguised by amending the wording of posts, emotions will not. Therefore, we describe here an ensemble method to identify toxicity and classify the emotions expressed on a corpus of annotated posts published by Task 5 of SemEval 2021–our analysis shows that the majority of such posts express anger, sadness and fear. Our method to identify toxicity combines a lexicon-based approach, which on its own achieves an F1 score of 61.07%, with a supervised learning approach, which on its own achieves an F1 score of 60%. When both methods are combined, the ensemble achieves an F1 score of 66.37%",
    "checked": true,
    "id": "84e826ecd98015be07f8457df41dbfa2c4046a09",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Marco Palomino",
      "Dawid Grad",
      "James Bedwell"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.116": {
    "title": "LISAC FSDM USMBA at SemEval-2021 Task 5: Tackling Toxic Spans Detection Challenge with Supervised SpanBERT-based Model and Unsupervised LIME-based Model",
    "volume": "workshop",
    "abstract": "Toxic spans detection is an emerging challenge that aims to find toxic spans within a toxic text. In this paper, we describe our solutions to tackle toxic spans detection. The first solution, which follows a supervised approach, is based on SpanBERT model. This latter is intended to better embed and predict spans of text. The second solution, which adopts an unsupervised approach, combines linear support vector machine with the Local Interpretable Model-Agnostic Explanations (LIME). This last is used to interpret predictions of learning-based models. Our supervised model outperformed the unsupervised model and achieved the f-score of 67,84% (ranked 22/85) in Task 5 at SemEval-2021: Toxic Spans Detection",
    "checked": true,
    "id": "9c479b18c82d4e232bf03a42a9aad0665b014cd7",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Abdessamad Benlahbib",
      "Ahmed Alami",
      "Hamza Alami"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.117": {
    "title": "HITMI&T at SemEval-2021 Task 5: Integrating Transformer and CRF for Toxic Spans Detection",
    "volume": "workshop",
    "abstract": "This paper introduces our system at SemEval-2021 Task 5: Toxic Spans Detection. The task aims to accurately locate toxic spans within a text. Using BIO tagging scheme, we model the task as a token-level sequence labeling task. Our system uses a single model built on the model of multi-layer bidirectional transformer encoder. And we introduce conditional random field (CRF) to make the model learn the constraints between tags. We use ERNIE as pre-trained model, which is more suitable for the task accroding to our experiments. In addition, we use adversarial training with the fast gradient method (FGM) to improve the robustness of the system. Our system obtains 69.85% F1 score, ranking 3rd for the official evaluation",
    "checked": true,
    "id": "c06250de69d6a3df63dcb95af346c4c5f449ec26",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenyi Wang",
      "Tianshu Liu",
      "Tiejun Zhao"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.118": {
    "title": "AStarTwice at SemEval-2021 Task 5: Toxic Span Detection Using RoBERTa-CRF, Domain Specific Pre-Training and Self-Training",
    "volume": "workshop",
    "abstract": "This paper describes our contribution to SemEval-2021 Task 5: Toxic Spans Detection. Our solution is built upon RoBERTa language model and Conditional Random Fields (CRF). We pre-trained RoBERTa on Civil Comments dataset, enabling it to create better contextual representation for this task. We also employed the semi-supervised learning technique of self-training, which allowed us to extend our training dataset. In addition to these, we also identified some pre-processing steps that significantly improved our F1 score. Our proposed system achieved a rank of 41 with an F1 score of 66.16%",
    "checked": true,
    "id": "0e00bea2bcd3ab894a37be0d7c26d88d371c6551",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Thakur Ashutosh Suman",
      "Abhinav Jain"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.119": {
    "title": "NLP_UIOWA at Semeval-2021 Task 5: Transferring Toxic Sets to Tag Toxic Spans",
    "volume": "workshop",
    "abstract": "We leverage a BLSTM with attention to identify toxic spans in texts. We explore different dimensions which affect the model’s performance. The first dimension explored is the toxic set the model is trained on. Besides the provided dataset, we explore the transferability of 5 different toxic related sets, including offensive, toxic, abusive, and hate sets. We find that the solely offensive set shows the highest promise of transferability. The second dimension we explore is methodology, including leveraging attention, employing a greedy remove method, using a frequency ratio, and examining hybrid combinations of multiple methods. We conduct an error analysis to examine which types of toxic spans were missed and which were wrongly inferred as toxic along with the main reasons why they occurred. Finally, we extend our method via ensembles, which achieves our highest F1 score of 55.1",
    "checked": true,
    "id": "fc34890166fb83ad5ffeb0dcc64bbe5d81b0b677",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Jonathan Rusert"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.120": {
    "title": "S-NLP at SemEval-2021 Task 5: An Analysis of Dual Networks for Sequence Tagging",
    "volume": "workshop",
    "abstract": "The SemEval 2021 task 5: Toxic Spans Detection is a task of identifying considered-toxic spans in text, which provides a valuable, automatic tool for moderating online contents. This paper represents the second-place method for the task, an ensemble of two approaches. While one approach relies on combining different embedding methods to extract diverse semantic and syntactic representations of words in context; the other utilizes extra data with a slightly customized Self-training, a semi-supervised learning technique, for sequence tagging problems. Both of our architectures take advantage of a strong language model, which was fine-tuned on a toxic classification task. Although experimental evidence indicates higher effectiveness of the first approach than the second one, combining them leads to our best results of 70.77 F1-score on the test dataset",
    "checked": true,
    "id": "6e37a8bc7887ed206630693c72102038baf15b0d",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Viet Anh Nguyen",
      "Tam Minh Nguyen",
      "Huy Quang Dao",
      "Quang Huu Pham"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.121": {
    "title": "UAntwerp at SemEval-2021 Task 5: Spans are Spans, stacking a binary word level approach to toxic span detection",
    "volume": "workshop",
    "abstract": "This paper describes the system developed by the Antwerp Centre for Digital humanities and literary Criticism [UAntwerp] for toxic span detection. We used a stacked generalisation ensemble of five component models, with two distinct interpretations of the task. Two models attempted to predict binary word toxicity based on ngram sequences, whilst 3 categorical span based models were trained to predict toxic token labels based on complete sequence tokens. The five models’ predictions were ensembled within an LSTM model. As well as describing the system, we perform error analysis to explore model performance in relation to textual features. The system described in this paper scored 0.6755 and ranked 26th",
    "checked": true,
    "id": "e1145b35723da9491b91927d949221e98ebd5b2c",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Ben Burtenshaw",
      "Mike Kestemont"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.122": {
    "title": "hub at SemEval-2021 Task 5: Toxic Span Detection Based on Word-Level Classification",
    "volume": "workshop",
    "abstract": "This article introduces the system description of the hub team, which explains the related work and experimental results of our team’s participation in SemEval 2021 Task 5: Toxic Spans Detection. The data for this shared task comes from some posts on the Internet. The task goal is to identify the toxic content contained in these text data. We need to find the span of the toxic text in the text data as accurately as possible. In the same post, the toxic text may be one paragraph or multiple paragraphs. Our team uses a classification scheme based on word-level to accomplish this task. The system we used to submit the results is ALBERT+BILSTM+CRF. The result evaluation index of the task submission is the F1 score, and the final score of the prediction result of the test set submitted by our team is 0.6640226029",
    "checked": true,
    "id": "5e58e623381b5e2180500cc109d790d1b60ba7bc",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Huang",
      "Yang Bai",
      "Xiaobing Zhou"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.123": {
    "title": "Sefamerve ARGE at SemEval-2021 Task 5: Toxic Spans Detection Using Segmentation Based 1-D Convolutional Neural Network Model",
    "volume": "workshop",
    "abstract": "This paper describes our contribution to SemEval-2021 Task 5: Toxic Spans Detection. Our approach considers toxic spans detection as a segmentation problem. The system, Waw-unet, consists of a 1-D convolutional neural network adopted from U-Net architecture commonly applied for semantic segmentation. We customize existing architecture by adding a special network block considering for text segmentation, as an essential component of the model. We compared the model with two transformers-based systems RoBERTa and XLM-RoBERTa to see its performance against pre-trained language models. We obtained 0.6251 f1 score with Waw-unet while 0.6390 and 0.6601 with the compared models respectively",
    "checked": true,
    "id": "27bb315dc75e2b269ce12f69a72cf3d868380553",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Selman Delil",
      "Birol Kuyumcu",
      "Cüneyt Aksakallı"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.124": {
    "title": "MIPT-NSU-UTMN at SemEval-2021 Task 5: Ensembling Learning with Pre-trained Language Models for Toxic Spans Detection",
    "volume": "workshop",
    "abstract": "This paper describes our system for SemEval-2021 Task 5 on Toxic Spans Detection. We developed ensemble models using BERT-based neural architectures and post-processing to combine tokens into spans. We evaluated several pre-trained language models using various ensemble techniques for toxic span identification and achieved sizable improvements over our baseline fine-tuned BERT models. Finally, our system obtained a F1-score of 67.55% on test data",
    "checked": true,
    "id": "1a5a895e3bf204a5a99b987ab333ab12040d2f9e",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Mikhail Kotyushev",
      "Anna Glazkova",
      "Dmitry Morozov"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.125": {
    "title": "UIT-E10dot3 at SemEval-2021 Task 5: Toxic Spans Detection with Named Entity Recognition and Question-Answering Approaches",
    "volume": "workshop",
    "abstract": "The increment of toxic comments on online space is causing tremendous effects on other vulnerable users. For this reason, considerable efforts are made to deal with this, and SemEval-2021 Task 5: Toxic Spans Detection is one of those. This task asks competitors to extract spans that have toxicity from the given texts, and we have done several analyses to understand its structure before doing experiments. We solve this task by two approaches, Named Entity Recognition with spaCy’s library and Question-Answering with RoBERTa combining with ToxicBERT, and the former gains the highest F1-score of 66.99%",
    "checked": true,
    "id": "bf985b0b578c7a20f9f22cbd9deaa2798b6330fa",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Phu Gia Hoang",
      "Luan Thanh Nguyen",
      "Kiet Nguyen"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.126": {
    "title": "SkoltechNLP at SemEval-2021 Task 5: Leveraging Sentence-level Pre-training for Toxic Span Detection",
    "volume": "workshop",
    "abstract": "This work describes the participation of the Skoltech NLP group team (Sk) in the Toxic Spans Detection task at SemEval-2021. The goal of the task is to identify the most toxic fragments of a given sentence, which is a binary sequence tagging problem. We show that fine-tuning a RoBERTa model for this problem is a strong baseline. This baseline can be further improved by pre-training the RoBERTa model on a large dataset labeled for toxicity at the sentence level. While our solution scored among the top 20% participating models, it is only 2 points below the best result. This suggests the viability of our approach",
    "checked": true,
    "id": "c89a572ae5a1ecbf903bb57e300ffc6a63dde53f",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "David Dale",
      "Igor Markov",
      "Varvara Logacheva",
      "Olga Kozlova",
      "Nikita Semenov",
      "Alexander Panchenko"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.127": {
    "title": "Entity at SemEval-2021 Task 5: Weakly Supervised Token Labelling for Toxic Spans Detection",
    "volume": "workshop",
    "abstract": "Detection of toxic spans - detecting toxicity of contents in the granularity of tokens - is crucial for effective moderation of online discussions. The baseline approach for this problem using the transformer model is to add a token classification head to the language model and fine-tune the layers with the token labeled dataset. One of the limitations of such a baseline approach is the scarcity of labeled data. To improve the results, We studied leveraging existing public datasets for a related but different task of entire comment/sentence classification. We propose two approaches: the first approach fine-tunes transformer models that are pre-trained on sentence classification samples. In the second approach, we perform weak supervision with soft attention to learn token level labels from sentence labels. Our experiments show improvements in the F1 score over the baseline approach. The implementation has been released publicly",
    "checked": true,
    "id": "1eaae6f5c5783015eb9be9a722b4949b03cb9408",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vaibhav Jain",
      "Mina Naghshnejad"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.128": {
    "title": "BennettNLP at SemEval-2021 Task 5: Toxic Spans Detection using Stacked Embedding Powered Toxic Entity Recognizer",
    "volume": "workshop",
    "abstract": "With the rapid growth in technology, social media activity has seen a boom across all age groups. It is humanly impossible to check all the tweets, comments and status manually whether they follow proper community guidelines. A lot of toxicity is regularly posted on these social media platforms. This research aims to find toxic words in a sentence so that a healthy social community is built across the globe and the users receive censored content with specific warnings and facts. To solve this challenging problem, authors have combined concepts of Linked List for pre-processing and then used the idea of stacked embeddings like BERT Embeddings, Flair Embeddings and Word2Vec on the flairNLP framework to get the desired results. F1 metric was used to evaluate the model. The authors were able to produce a 0.74 F1 score on their test set",
    "checked": true,
    "id": "6583ddaf56e7d0f617f907f8c25d48b6dab58af3",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harsh Kataria",
      "Ambuje Gupta",
      "Vipul Mishra"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.129": {
    "title": "UoT-UWF-PartAI at SemEval-2021 Task 5: Self Attention Based Bi-GRU with Multi-Embedding Representation for Toxicity Highlighter",
    "volume": "workshop",
    "abstract": "Toxic Spans Detection(TSD) task is defined as highlighting spans that make a text toxic. Many works have been done to classify a given comment or document as toxic or non-toxic. However, none of those proposed models work at the token level. In this paper, we propose a self-attention-based bidirectional gated recurrent unit(BiGRU) with a multi-embedding representation of the tokens. Our proposed model enriches the representation by a combination of GPT-2, GloVe, and RoBERTa embeddings, which led to promising results. Experimental results show that our proposed approach is very effective in detecting span tokens",
    "checked": true,
    "id": "24a663f2fad1e6387db3b9a62a7cb501f7387e55",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Hamed Babaei Giglou",
      "Taher Rahgooy",
      "Mostafa Rahgouy",
      "Jafar Razmara"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.130": {
    "title": "YoungSheldon at SemEval-2021 Task 5: Fine-tuning Pre-trained Language Models for Toxic Spans Detection using Token classification Objective",
    "volume": "workshop",
    "abstract": "In this paper, we describe our system used for SemEval 2021 Task 5: Toxic Spans Detection. Our proposed system approaches the problem as a token classification task. We trained our model to find toxic words and concatenate their spans to predict the toxic spans within a sentence. We fine-tuned Pre-trained Language Models (PLMs) for identifying the toxic words. For fine-tuning, we stacked the classification layer on top of the PLM features of each word to classify if it is toxic or not. PLMs are pre-trained using different objectives and their performance may differ on downstream tasks. We, therefore, compare the performance of BERT, ELECTRA, RoBERTa, XLM-RoBERTa, T5, XLNet, and MPNet for identifying toxic spans within a sentence. Our best performing system used RoBERTa. It performed well, achieving an F1 score of 0.6841 and secured a rank of 16 on the official leaderboard",
    "checked": true,
    "id": "36ae24938bfed5a485875c64c8d73acab7180079",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Mayukh Sharma",
      "Ilanthenral Kandasamy",
      "W.b. Vasantha"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.131": {
    "title": "HLE-UPC at SemEval-2021 Task 5: Multi-Depth DistilBERT for Toxic Spans Detection",
    "volume": "workshop",
    "abstract": "This paper presents our submission to SemEval-2021 Task 5: Toxic Spans Detection. The purpose of this task is to detect the spans that make a text toxic, which is a complex labour for several reasons. Firstly, because of the intrinsic subjectivity of toxicity, and secondly, due to toxicity not always coming from single words like insults or offends, but sometimes from whole expressions formed by words that may not be toxic individually. Following this idea of focusing on both single words and multi-word expressions, we study the impact of using a multi-depth DistilBERT model, which uses embeddings from different layers to estimate the final per-token toxicity. Our quantitative results show that using information from multiple depths boosts the performance of the model. Finally, we also analyze our best model qualitatively",
    "checked": true,
    "id": "6270ee83815e05ef23872e8baaf38bc38f29d699",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Rafel Palliser-Sans",
      "Albert Rial-Farràs"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.132": {
    "title": "Lone Pine at SemEval-2021 Task 5: Fine-Grained Detection of Hate Speech Using BERToxic",
    "volume": "workshop",
    "abstract": "This paper describes our approach to the Toxic Spans Detection problem (SemEval-2021 Task 5). We propose BERToxic, a system that fine-tunes a pre-trained BERT model to locate toxic text spans in a given text and utilizes additional post-processing steps to refine the boundaries. The post-processing steps involve (1) labeling character offsets between consecutive toxic tokens as toxic and (2) assigning a toxic label to words that have at least one token labeled as toxic. Through experiments, we show that these two post-processing steps improve the performance of our model by 4.16% on the test set. We also studied the effects of data augmentation and ensemble modeling strategies on our system. Our system significantly outperformed the provided baseline and achieved an F1-score of 0.683, placing Lone Pine in the 17th place out of 91 teams in the competition. Our code is made available at https://github.com/Yakoob-Khan/Toxic-Spans-Detection",
    "checked": true,
    "id": "f65ad86b083c6a481dd23bad164ef9ab913f22e2",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Yakoob Khan",
      "Weicheng Ma",
      "Soroush Vosoughi"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.133": {
    "title": "SRPOL DIALOGUE SYSTEMS at SemEval-2021 Task 5: Automatic Generation of Training Data for Toxic Spans Detection",
    "volume": "workshop",
    "abstract": "This paper presents a system used for SemEval-2021 Task 5: Toxic Spans Detection. Our system is an ensemble of BERT-based models for binary word classification, trained on a dataset extended by toxic comments modified and generated by two language models. For the toxic word classification, the prediction threshold value was optimized separately for every comment, in order to maximize the expected F1 value",
    "checked": true,
    "id": "8652f25c7dabdfeeb33257fb37c675f66c203f29",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michał Satława",
      "Katarzyna Zamłyńska",
      "Jarosław Piersa",
      "Joanna Kolis",
      "Klaudia Firląg",
      "Katarzyna Beksa",
      "Zuzanna Bordzicka",
      "Christian Goltz",
      "Paweł Bujnowski",
      "Piotr Andruszkiewicz"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.134": {
    "title": "SINAI at SemEval-2021 Task 5: Combining Embeddings in a BiLSTM-CRF model for Toxic Spans Detection",
    "volume": "workshop",
    "abstract": "This paper describes the participation of SINAI team at Task 5: Toxic Spans Detection which consists of identifying spans that make a text toxic. Although several resources and systems have been developed so far in the context of offensive language, both annotation and tasks have mainly focused on classifying whether a text is offensive or not. However, detecting toxic spans is crucial to identify why a text is toxic and can assist human moderators to locate this type of content on social media. In order to accomplish the task, we follow a deep learning-based approach using a Bidirectional variant of a Long Short Term Memory network along with a stacked Conditional Random Field decoding layer (BiLSTM-CRF). Specifically, we test the performance of the combination of different pre-trained word embeddings for recognizing toxic entities in text. The results show that the combination of word embeddings helps in detecting offensive content. Our team ranks 29th out of 91 participants",
    "checked": true,
    "id": "75c590f18e60a727d8fb32aeba00de6c81779f16",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Flor Miriam Plaza-del-Arco",
      "Pilar López-Úbeda",
      "L. Alfonso Ureña-López",
      "M. Teresa Martín-Valdivia"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.135": {
    "title": "CSECU-DSG at SemEval-2021 Task 5: Leveraging Ensemble of Sequence Tagging Models for Toxic Spans Detection",
    "volume": "workshop",
    "abstract": "The upsurge of prolific blogging and microblogging platforms enabled the abusers to spread negativity and threats greater than ever. Detecting the toxic portions substantially aids to moderate or exclude the abusive parts for maintaining sound online platforms. This paper describes our participation in the SemEval 2021 toxic span detection task. The task requires detecting spans that convey toxic remarks from the given text. We explore an ensemble of sequence labeling models including the BiLSTM-CRF, spaCy NER model with custom toxic tags, and fine-tuned BERT model to identify the toxic spans. Finally, a majority voting ensemble method is used to determine the unified toxic spans. Experimental results depict the competitive performance of our model among the participants",
    "checked": true,
    "id": "ad7bc86f9beea2f06b5c4a82f591daec22b9a5ec",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tashin Hossain",
      "Jannatun Naim",
      "Fareen Tasneem",
      "Radiathun Tasnia",
      "Abu Nowshed Chy"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.136": {
    "title": "UTNLP at SemEval-2021 Task 5: A Comparative Analysis of Toxic Span Detection using Attention-based, Named Entity Recognition, and Ensemble Models",
    "volume": "workshop",
    "abstract": "Detecting which parts of a sentence contribute to that sentence’s toxicity—rather than providing a sentence-level verdict of hatefulness— would increase the interpretability of models and allow human moderators to better understand the outputs of the system. This paper presents our team’s, UTNLP, methodology and results in the SemEval-2021 shared task 5 on toxic spans detection. We test multiple models and contextual embeddings and report the best setting out of all. The experiments start with keyword-based models and are followed by attention-based, named entity- based, transformers-based, and ensemble models. Our best approach, an ensemble model, achieves an F1 of 0.684 in the competition’s evaluation phase",
    "checked": true,
    "id": "2be68915f64751c79b391d7c3b3d87fb39b0417e",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alireza Salemi",
      "Nazanin Sabri",
      "Emad Kebriaei",
      "Behnam Bahrak",
      "Azadeh Shakery"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.137": {
    "title": "macech at SemEval-2021 Task 5: Toxic Spans Detection",
    "volume": "workshop",
    "abstract": "Toxic language is often present in online forums, especially when politics and other polarizing topics arise, and can lead to people becoming discouraged from joining or continuing conversations. In this paper, we use data consisting of comments with the indices of toxic text labelled to train an RNN to deter-mine which parts of the comments make them toxic, which could aid online moderators. We compare results using both the original dataset and an augmented set, as well as GRU versus LSTM RNN models",
    "checked": true,
    "id": "f790abe5c2463a17aaed0325a7b6d27edcc1704d",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Maggie Cech"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.138": {
    "title": "LZ1904 at SemEval-2021 Task 5: Bi-LSTM-CRF for Toxic Span Detection using Pretrained Word Embedding",
    "volume": "workshop",
    "abstract": "Recurrent Neural Networks (RNN) have been widely used in various Natural Language Processing (NLP) tasks such as text classification, sequence tagging, and machine translation. Long Short Term Memory (LSTM), a special unit of RNN, has the benefit of memorizing past and even future information in a sentence (especially for bidirectional LSTM). In the shared task of detecting spans which make texts toxic, we first apply pretrained word embedding (GloVe) to generate the word vectors after tokenization. And then we construct Bidirectional Long Short Term Memory-Conditional Random Field (Bi-LSTM-CRF) model by Baidu research to predict whether each word in the sentence is toxic or not. We tune hyperparameters of dropout rate, number of LSTM units, embedding size with 10 epochs and choose the best epoch with validation recall. Our model achieves an F1 score of 66.99 percent in test dataset",
    "checked": true,
    "id": "de9895cb54e0e5461fd19d578d2f2ef7b3796378",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Liang Zou",
      "Wen Li"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.139": {
    "title": "LIIR at SemEval-2021 task 6: Detection of Persuasion Techniques In Texts and Images using CLIP features",
    "volume": "workshop",
    "abstract": "We describe our approach for SemEval-2021 task 6 on detection of persuasion techniques in multimodal content (memes). Our system combines pretrained multimodal models (CLIP) and chained classifiers. Also, we propose to enrich the data by a data augmentation technique. Our submission achieves a rank of 8/16 in terms of F1-micro and 9/16 with F1-macro on the test set",
    "checked": true,
    "id": "83ea24b9a2d0cd322a5a4928c2bf2e8f82a300f9",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Erfan Ghadery",
      "Damien Sileo",
      "Marie-Francine Moens"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.140": {
    "title": "AIMH at SemEval-2021 Task 6: Multimodal Classification Using an Ensemble of Transformer Models",
    "volume": "workshop",
    "abstract": "This paper describes the system used by the AIMH Team to approach the SemEval Task 6. We propose an approach that relies on an architecture based on the transformer model to process multimodal content (text and images) in memes. Our architecture, called DVTT (Double Visual Textual Transformer), approaches Subtasks 1 and 3 of Task 6 as multi-label classification problems, where the text and/or images of the meme are processed, and the probabilities of the presence of each possible persuasion technique are returned as a result. DVTT uses two complete networks of transformers that work on text and images that are mutually conditioned. One of the two modalities acts as the main one and the second one intervenes to enrich the first one, thus obtaining two distinct ways of operation. The two transformers outputs are merged by averaging the inferred probabilities for each possible label, and the overall network is trained end-to-end with a binary cross-entropy loss",
    "checked": true,
    "id": "07f05b8158e013dcf5a291f16a9fb1d80db0f198",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Nicola Messina",
      "Fabrizio Falchi",
      "Claudio Gennaro",
      "Giuseppe Amato"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.141": {
    "title": "HOMADOS at SemEval-2021 Task 6: Multi-Task Learning for Propaganda Detection",
    "volume": "workshop",
    "abstract": "Among the tasks motivated by the proliferation of misinformation, propaganda detection is particularly challenging due to the deficit of fine-grained manual annotations required to train machine learning models. Here we show how data from other related tasks, including credibility assessment, can be leveraged in multi-task learning (MTL) framework to accelerate the training process. To that end, we design a BERT-based model with multiple output layers, train it in several MTL scenarios and perform evaluation against the SemEval gold standard",
    "checked": true,
    "id": "7ea983ebc78b84ed0bc2ee1626161cb5da9aaf36",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Konrad Kaczyński",
      "Piotr Przybyła"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.142": {
    "title": "1213Li at SemEval-2021 Task 6: Detection of Propaganda with Multi-modal Attention and Pre-trained Models",
    "volume": "workshop",
    "abstract": "This paper presents the solution proposed by the 1213Li team for subtask 3 in SemEval-2021 Task 6: identifying the multiple persuasion techniques used in the multi-modal content of the meme. We explored various approaches in feature extraction and the detection of persuasion labels. Our final model employs pre-trained models including RoBERTa and ResNet-50 as a feature extractor for texts and images, respectively, and adopts a label embedding layer with multi-modal attention mechanism to measure the similarity of labels with the multi-modal information and fuse features for label prediction. Our proposed method outperforms the provided baseline method and achieves 3rd out of 16 participants with 0.54860/0.22830 for Micro/Macro F1 scores",
    "checked": true,
    "id": "1f4f387f81d845723581379db0ef907c13839600",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Peiguang Li",
      "Xuan Li",
      "Xian Sun"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.143": {
    "title": "NLyticsFKIE at SemEval-2021 Task 6: Detection of Persuasion Techniques In Texts And Images",
    "volume": "workshop",
    "abstract": "The following system description presents our approach to the detection of persuasion techniques in texts and images. The given task has been framed as a multi-label classification problem with the different techniques serving as class labels. The multi-label classification problem is one in which a list of target variables such as our class labels is associated with every input chunk and assumes that a document can simultaneously and independently be assigned to multiple labels or classes. In order to assign class labels to the given memes, we opted for RoBERTa (A Robustly Optimized BERT Pretraining Approach) as a neural network architecture for token and sequence classification. Starting off with a pre-trained model for language representation we fine-tuned this model on the given classification task with the provided annotated data in supervised training steps. To incorporate image features in the multi-modal setting, we rely on the pre-trained VGG-16 model architecture",
    "checked": true,
    "id": "d5a66d7a22f9915c39b4930b33bb0863b000883e",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Albert Pritzkau"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.144": {
    "title": "YNU-HPCC at SemEval-2021 Task 6: Combining ALBERT and Text-CNN for Persuasion Detection in Texts and Images",
    "volume": "workshop",
    "abstract": "In recent years, memes combining image and text have been widely used in social media, and memes are one of the most popular types of content used in online disinformation campaigns. In this paper, our study on the detection of persuasion techniques in texts and images in SemEval-2021 Task 6 is summarized. For propaganda technology detection in text, we propose a combination model of both ALBERT and Text CNN for text classification, as well as a BERT-based multi-task sequence labeling model for propaganda technology coverage span detection. For the meme classification task involved in text understanding and visual feature extraction, we designed a parallel channel model divided into text and image channels. Our method achieved a good performance on subtasks 1 and 3. The micro F1-scores of 0.492, 0.091, and 0.446 achieved on the test sets of the three subtasks ranked 12th, 7th, and 11th, respectively, and all are higher than the baseline model",
    "checked": true,
    "id": "1bd80c9f630bb1f86476c4696308faa199b7d72d",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Xingyu Zhu",
      "Jin Wang",
      "Xuejie Zhang"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.145": {
    "title": "LT3 at SemEval-2021 Task 6: Using Multi-Modal Compact Bilinear Pooling to Combine Visual and Textual Understanding in Memes",
    "volume": "workshop",
    "abstract": "Internet memes have become ubiquitous in social media networks today. Due to their popularity, they are also a widely used mode of expression to spread disinformation online. As memes consist of a mixture of text and image, they require a multi-modal approach for automatic analysis. In this paper, we describe our contribution to the SemEval-2021 Detection of Persuasian Techniques in Texts and Images Task. We propose a Multi-Modal learning system, which incorporates “memebeddings”, viz. joint text and vision features by combining them with compact bilinear pooling, to automatically identify rhetorical and psychological disinformation techniques. The experimental results show that the proposed system constantly outperforms the competition’s baseline, and achieves the 2nd best Macro F1-score and 14th best Micro F1-score out of all participants",
    "checked": true,
    "id": "79bf66d5ad7195a2e4a9017ca6630f2a9028de4e",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Pranaydeep Singh",
      "Els Lefever"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.146": {
    "title": "FPAI at SemEval-2021 Task 6: BERT-MRC for Propaganda Techniques Detection",
    "volume": "workshop",
    "abstract": "The objective of subtask 2 of SemEval-2021 Task 6 is to identify techniques used together with the span(s) of text covered by each technique. This paper describes the system and model we developed for the task. We first propose a pipeline system to identify spans, then to classify the technique in the input sequence. But it severely suffers from handling the overlapping in nested span. Then we propose to formulize the task as a question answering task by MRC framework which achieves a better result compared to the pipeline method. Moreover, data augmentation and loss design techniques are also explored to alleviate the problem of data sparse and imbalance. Finally, we attain the 3rd place in the final evaluation phase",
    "checked": true,
    "id": "119495b4d0016012d2f24a676f8c1a032f23413a",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Xiaolong Hou",
      "Junsong Ren",
      "Gang Rao",
      "Lianxin Lian",
      "Zhihao Ruan",
      "Yang Mo",
      "JIanping Shen"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.147": {
    "title": "NLPIITR at SemEval-2021 Task 6: RoBERTa Model with Data Augmentation for Persuasion Techniques Detection",
    "volume": "workshop",
    "abstract": "This paper describes and examines different systems to address Task 6 of SemEval-2021: Detection of Persuasion Techniques In Texts And Images, Subtask 1. The task aims to build a model for identifying rhetorical and psycho- logical techniques (such as causal oversimplification, name-calling, smear) in the textual content of a meme which is often used in a disinformation campaign to influence the users. The paper provides an extensive comparison among various machine learning systems as a solution to the task. We elaborate on the pre-processing of the text data in favor of the task and present ways to overcome the class imbalance. The results show that fine-tuning a RoBERTa model gave the best results with an F1-Micro score of 0.51 on the development set",
    "checked": true,
    "id": "fdaefe025a3aacbd6c287f1a2f539d7bb1c8818c",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Vansh Gupta",
      "Raksha Sharma"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.148": {
    "title": "LeCun at SemEval-2021 Task 6: Detecting Persuasion Techniques in Text Using Ensembled Pretrained Transformers and Data Augmentation",
    "volume": "workshop",
    "abstract": "We developed a system for task 6 sub-task 1 for detecting propaganda in memes. An external dataset and augmentation data-set were used to extend the official competition data-set. Data augmentation techniques were applied on the external data-set and competition data-set to come up with the augmented data-set. We trained 5 transformers (DeBERTa, and 4 RoBERTa) and ensembled them to make the prediction. We trained 1 RoBERTa model initially on the augmented data-set for a few epochs and then fine-tuned it on the competition data-set which improved the f1-micro up to 0.1 scores. After that, another initial RoBERTa model was trained on the external data-set merged with the augmented data-set for few epochs and fine-tuned it on the competition data-set. Furthermore, we ensembled the initial models with the models after fine-tuning. For the final model in the ensemble, we trained a DeBERTa model on the augmented data-set without fine-tuning it on the competition data-set. Finally, we averaged the output of each model in the ensemble to make the prediction",
    "checked": true,
    "id": "b957b81586fcf8f01306061eb2644c5c7d08cccb",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Dia Abujaber",
      "Ahmed Qarqaz",
      "Malak A. Abdullah"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.149": {
    "title": "Volta at SemEval-2021 Task 6: Towards Detecting Persuasive Texts and Images using Textual and Multimodal Ensemble",
    "volume": "workshop",
    "abstract": "Memes are one of the most popular types of content used to spread information online. They can influence a large number of people through rhetorical and psychological techniques. The task, Detection of Persuasion Techniques in Texts and Images, is to detect these persuasive techniques in memes. It consists of three subtasks: (A) Multi-label classification using textual content, (B) Multi-label classification and span identification using textual content, and (C) Multi-label classification using visual and textual content. In this paper, we propose a transfer learning approach to fine-tune BERT-based models in different modalities. We also explore the effectiveness of ensembles of models trained in different modalities. We achieve an F1-score of 57.0, 48.2, and 52.1 in the corresponding subtasks",
    "checked": true,
    "id": "44c0746bf2c48823a4939f39b1e9e677632a7476",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Kshitij Gupta",
      "Devansh Gautam",
      "Radhika Mamidi"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.150": {
    "title": "MinD at SemEval-2021 Task 6: Propaganda Detection using Transfer Learning and Multimodal Fusion",
    "volume": "workshop",
    "abstract": "We describe our systems of subtask1 and subtask3 for SemEval-2021 Task 6 on Detection of Persuasion Techniques in Texts and Images. The purpose of subtask1 is to identify propaganda techniques given textual content, and the goal of subtask3 is to detect them given both textual and visual content. For subtask1, we investigate transfer learning based on pre-trained language models (PLMs) such as BERT, RoBERTa to solve data sparsity problems. For subtask3, we extract heterogeneous visual representations (i.e., face features, OCR features, and multimodal representations) and explore various multimodal fusion strategies to combine the textual and visual representations. The official evaluation shows our ensemble model ranks 1st for subtask1 and 2nd for subtask3",
    "checked": true,
    "id": "6c7ed498c24104de39414dce908e8f7cc436ba28",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Junfeng Tian",
      "Min Gui",
      "Chenliang Li",
      "Ming Yan",
      "Wenming Xiao"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.151": {
    "title": "CSECU-DSG at SemEval-2021 Task 6: Orchestrating Multimodal Neural Architectures for Identifying Persuasion Techniques in Texts and Images",
    "volume": "workshop",
    "abstract": "Inscribing persuasion techniques in memes is the most impactful way to influence peoples’ mindsets. People are more inclined to memes as they are more stimulating and convincing and hence memes are often exploited by tactfully engraving propaganda in its context with the intent of attaining specific agenda. This paper describes our participation in the three subtasks featured by SemEval 2021 task 6 on the detection of persuasion techniques in texts and images. We utilize a fusion of logistic regression, decision tree, and fine-tuned DistilBERT for tackling subtask 1. As for subtask 2, we propose a system that consolidates a span identification model and a multi-label classification model based on pre-trained BERT. We address the multi-modal multi-label classification of memes defined in subtask 3 by utilizing a ResNet50 based image model, DistilBERT based text model, and a multi-modal architecture based on multikernel CNN+LSTM and MLP model. The outcomes illustrated the competitive performance of our systems",
    "checked": true,
    "id": "e5fe166fe196ab394ff71f0050a57369b9093abc",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Tashin Hossain",
      "Jannatun Naim",
      "Fareen Tasneem",
      "Radiathun Tasnia",
      "Abu Nowshed Chy"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.152": {
    "title": "UMUTeam at SemEval-2021 Task 7: Detecting and Rating Humor and Offense with Linguistic Features and Word Embeddings",
    "volume": "workshop",
    "abstract": "In writing, humor is mainly based on figurative language in which words and expressions change their conventional meaning to refer to something without saying it directly. This flip in the meaning of the words prevents Natural Language Processing from revealing the real intention of a communication and, therefore, reduces the effectiveness of tasks such as Sentiment Analysis or Emotion Detection. In this manuscript we describe the participation of the UMUTeam in HaHackathon 2021, whose objective is to detect and rate humorous and controversial content. Our proposal is based on the combination of linguistic features with contextual and non-contextual word embeddings. We participate in all the proposed subtasks achieving our best result in the controversial humor subtask",
    "checked": true,
    "id": "860ca736aa153d4e6948756929174aeef84edc20",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "José Antonio García-Díaz",
      "Rafael Valencia-García"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.153": {
    "title": "ES-JUST at SemEval-2021 Task 7: Detecting and Rating Humor and Offensive Text Using Deep Learning",
    "volume": "workshop",
    "abstract": "This research presents the work of the team’s ES-JUST at semEval-2021 task 7 for detecting and rating humor and offensive text using deep learning. The team evaluates several approaches (i.e.Bert, Roberta, XLM-Roberta, and Bert embedding + Bi-LSTM) that employ in four sub-tasks. The first sub-task deal with whether the text is humorous or not. The second sub-task is the degree of humor in the text if the first sub-task is humorous. The third sub-task represents the text is controversial or not if it is humorous. While in the last task is the degree of an offensive in the text. However, Roberta pre-trained model outperforms other approaches and score the highest in all sub-tasks. We rank on the leader board at the evaluation phase are 14, 15, 20, and 5 through 0.9564 F-score, 0.5709 RMSE, 0.4888 F-score, and 0.4467 RMSE results, respectively, for each of the first, second, third, and fourth sub-task, respectively",
    "checked": true,
    "id": "049844405266025f82349df8d9dea2e717b81770",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Emran Al Bashabsheh",
      "Sanaa Abu Alasal"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.154": {
    "title": "Tsia at SemEval-2021 Task 7: Detecting and Rating Humor and Offense",
    "volume": "workshop",
    "abstract": "This paper describes our contribution to SemEval-2021 Task 7: Detecting and Rating Humor and Of-fense.This task contains two sub-tasks, sub-task 1and sub-task 2. Among them, sub-task 1 containsthree sub-tasks, sub-task 1a ,sub-task 1b and sub-task 1c.Sub-task 1a is to predict if the text would beconsidered humorous.Sub-task 1c is described asfollows: if the text is classed as humorous, predictif the humor rating would be considered controver-sial, i.e. the variance of the rating between annota-tors is higher than the median.we combined threepre-trained model with CNN to complete these twoclassification sub-tasks.Sub-task 1b is to judge thedegree of humor.Sub-task 2 aims to predict how of-fensive a text would be with values between 0 and5.We use the idea of regression to deal with thesetwo sub-tasks.We analyze the performance of ourmethod and demonstrate the contribution of eachcomponent of our architecture.We have achievedgood results under the combination of multiple pre-training models and optimization methods",
    "checked": true,
    "id": "a8a50f551b15cd9d8318aefa095c75ce647d80ce",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Zhengyi Guan",
      "Xiaobing ZXB Zhou"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.155": {
    "title": "DLJUST at SemEval-2021 Task 7: Hahackathon: Linking Humor and Offense",
    "volume": "workshop",
    "abstract": "Humor detection and rating poses interesting linguistic challenges to NLP; it is highly subjective depending on the perceptions of a joke and the context in which it is used. This paper utilizes and compares transformers models; BERT base and Large, BERTweet, RoBERTa base and Large, and RoBERTa base irony, for detecting and rating humor and offense. The proposed models, where given a text in cased and uncased type obtained from SemEval-2021 Task7: HaHackathon: Linking Humor and Offense Across Different Age Groups. The highest scored model for the first subtask: Humor Detection, is BERTweet base cased model with 0.9540 F1-score, for the second subtask: Average Humor Rating Score, it is BERT Large cased with the minimum RMSE of 0.5555, for the fourth subtask: Average Offensiveness Rating Score, it is BERTweet base cased model with minimum RMSE of 0.4822",
    "checked": true,
    "id": "7e32568ba5a1b6e8d6467b10fe4e6c84b974620a",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Hani Al-Omari",
      "Isra’a AbedulNabi",
      "Rehab Duwairi"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.156": {
    "title": "Gulu at SemEval-2021 Task 7: Detecting and Rating Humor and Offense",
    "volume": "workshop",
    "abstract": "Humor recognition is a challenging task in natural language processing. This document presents my approaches to detect and rate humor and offense from the given text. This task includes 2 tasks: task 1 which contains 3 subtasks (1a, 1b, and 1c), and task 2. Subtask 1a and 1c can be regarded as classification problems and take ALBERT as the basic model. Subtask 1b and 2 can be viewed as regression issues and take RoBERTa as the basic model",
    "checked": true,
    "id": "49f78f17e36eabf6900a3d6f966749d5e6368775",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maoqin Yang"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.157": {
    "title": "DUTH at SemEval-2021 Task 7: Is Conventional Machine Learning for Humorous and Offensive Tasks enough in 2021?",
    "volume": "workshop",
    "abstract": "This paper describes the approach that was developed for SemEval 2021 Task 7 (Hahackathon: Incorporating Demographic Factors into Shared Humor Tasks) by the DUTH Team. We used and compared a variety of preprocessing techniques, vectorization methods, and numerous conventional machine learning algorithms, in order to construct classification and regression models for the given tasks. We used majority voting to combine the models’ outputs with small Neural Networks (NN) for classification tasks and their mean for regression for improving our system’s performance. While these methods proved weaker than modern, deep learning models, they are still relevant in research tasks because of their low requirements on computational power and faster training",
    "checked": true,
    "id": "acc4ff0aace3208c676531d61f440766a235949a",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Alexandros Karasakalidis",
      "Dimitrios Effrosynidis",
      "Avi Arampatzis"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.158": {
    "title": "DeepBlueAI at SemEval-2021 Task 7: Detecting and Rating Humor and Offense with Stacking Diverse Language Model-Based Methods",
    "volume": "workshop",
    "abstract": "This paper describes the winning system for SemEval-2021 Task 7: Detecting and Rating Humor and Offense. Our strategy is stacking diverse pre-trained language models (PLMs) such as RoBERTa and ALBERT. We first perform fine-tuning on these two PLMs with various hyperparameters and different training strategies. Then a valid stacking mechanism is applied on top of the fine-tuned PLMs to get the final prediction. Experimental results on the dataset released by the organizer of the task show the validity of our method and we win first place and third place for subtask 2 and 1a",
    "checked": true,
    "id": "82b95b3c9273e1b14d2682fc65e7af7e4c3fea4f",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Bingyan Song",
      "Chunguang Pan",
      "Shengguang Wang",
      "Zhipeng Luo"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.159": {
    "title": "CS-UM6P at SemEval-2021 Task 7: Deep Multi-Task Learning Model for Detecting and Rating Humor and Offense",
    "volume": "workshop",
    "abstract": "Humor detection has become a topic of interest for several research teams, especially those involved in socio-psychological studies, with the aim to detect the humor and the temper of a targeted population (e.g. a community, a city, a country, the employees of a given company). Most of the existing studies have formulated the humor detection problem as a binary classification task, whereas it revolves around learning the sense of humor by evaluating its different degrees. In this paper, we propose an end-to-end deep Multi-Task Learning (MTL) model to detect and rate humor and offense. It consists of a pre-trained transformer encoder and task-specific attention layers. The model is trained using MTL uncertainty loss weighting to adaptively combine all sub-tasks objective functions. Our MTL model tackles all sub-tasks of the SemEval-2021 Task-7 in one end-to-end deep learning system and shows very promising results",
    "checked": true,
    "id": "22eff5b97045d04f51636bad816ab73c1c160325",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Kabil Essefar",
      "Abdellah El Mekki",
      "Abdelkader El Mahdaouy",
      "Nabil El Mamoun",
      "Ismail Berrada"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.160": {
    "title": "hub at SemEval-2021 Task 7: Fusion of ALBERT and Word Frequency Information Detecting and Rating Humor and Offense",
    "volume": "workshop",
    "abstract": "This paper introduces the system description of the hub team, which explains the related work and experimental results of our team’s participation in SemEval 2021 Task 7: HaHackathon: Detecting and Rating Humor and Offense. We successfully submitted the test set prediction results of the two subtasks in the task. The goal of the task is to perform humor detection, grade evaluation, and offensive evaluation on each English text data in the data set. Tasks can be divided into two types of subtasks. One is a text classification task, and the other is a text regression task. What we need to do is to use our method to detect the humor and offensive information of the sentence as accurately as possible. The methods used in the results submitted by our team are mainly composed of ALBERT, CNN, and Tf-Idf algorithms. The result evaluation indicators submitted by the classification task are F1 score and Accuracy. The result evaluation index of the regression task submission is the RMSE. The final scores of the prediction results of the two subtask test sets submitted by our team are task1a 0.921 (F1), task1a 0.9364 (Accuracy), task1b 0.6288 (RMSE), task1c 0.5333 (F1), task1c 0.0.5591 (Accuracy), and task2 0.5027 (RMSE) respectively",
    "checked": true,
    "id": "61924df0d6bec262b28a9400c1065c258ddb147c",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Bo Huang",
      "Yang Bai"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.161": {
    "title": "YoungSheldon at SemEval-2021 Task 7: Fine-tuning Is All You Need",
    "volume": "workshop",
    "abstract": "In this paper, we describe our system used for SemEval 2021 Task 7: HaHackathon: Detecting and Rating Humor and Offense. We used a simple fine-tuning approach using different Pre-trained Language Models (PLMs) to evaluate their performance for humor and offense detection. For regression tasks, we averaged the scores of different models leading to better performance than the original models. We participated in all SubTasks. Our best performing system was ranked 4 in SubTask 1-b, 8 in SubTask 1-c, 12 in SubTask 2, and performed well in SubTask 1-a. We further show comprehensive results using different pre-trained language models which will help as baselines for future work",
    "checked": true,
    "id": "06ebfe29c67252d7aa7282af649dc7d7f3a0b87e",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Mayukh Sharma",
      "Ilanthenral Kandasamy",
      "W.b. Vasantha"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.162": {
    "title": "MagicPai at SemEval-2021 Task 7: Method for Detecting and Rating Humor Based on Multi-Task Adversarial Training",
    "volume": "workshop",
    "abstract": "This paper describes MagicPai’s system for SemEval 2021 Task 7, HaHackathon: Detecting and Rating Humor and Offense. This task aims to detect whether the text is humorous and how humorous it is. There are four subtasks in the competition. In this paper, we mainly present our solution, a multi-task learning model based on adversarial examples, for task 1a and 1b. More specifically, we first vectorize the cleaned dataset and add the perturbation to obtain more robust embedding representations. We then correct the loss via the confidence level. Finally, we perform interactive joint learning on multiple tasks to capture the relationship between whether the text is humorous and how humorous it is. The final result shows the effectiveness of our system",
    "checked": true,
    "id": "45a7d3cbb38168d8d605725aae26044b251614e6",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Jian Ma",
      "Shuyi Xie",
      "Haiqin Yang",
      "Lianxin Jiang",
      "Mengyuan Zhou",
      "Xiaoyi Ruan",
      "Yang Mo"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.163": {
    "title": "UPB at SemEval-2021 Task 7: Adversarial Multi-Task Learning for Detecting and Rating Humor and Offense",
    "volume": "workshop",
    "abstract": "Detecting humor is a challenging task since words might share multiple valences and, depending on the context, the same words can be even used in offensive expressions. Neural network architectures based on Transformer obtain state-of-the-art results on several Natural Language Processing tasks, especially text classification. Adversarial learning, combined with other techniques such as multi-task learning, aids neural models learn the intrinsic properties of data. In this work, we describe our adversarial multi-task network, AMTL-Humor, used to detect and rate humor and offensive texts from Task 7 at SemEval-2021. Each branch from the model is focused on solving a related task, and consists of a BiLSTM layer followed by Capsule layers, on top of BERTweet used for generating contextualized embeddings. Our best model consists of an ensemble of all tested configurations, and achieves a 95.66% F1-score and 94.70% accuracy for Task 1a, while obtaining RMSE scores of 0.6200 and 0.5318 for Tasks 1b and 2, respectively",
    "checked": true,
    "id": "2abb585de546bfc249721157b3b33ebff6f173db",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Răzvan-Alexandru Smădu",
      "Dumitru-Clementin Cercel",
      "Mihai Dascalu"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.164": {
    "title": "Team_KGP at SemEval-2021 Task 7: A Deep Neural System to Detect Humor and Offense with Their Ratings in the Text Data",
    "volume": "workshop",
    "abstract": "This paper describes the system submitted to SemEval-2021 Task-7 for all four subtasks. Two subtasks focus on detecting humor and offense from the text (binary classification). On the other hand, the other two subtasks predict humor and offense ratings of the text (linear regression). In this paper, we present two different types of fine-tuning methods by using linear layers and bi-LSTM layers on top of the pre-trained BERT model. Results show that our system is able to outperform baseline models by a significant margin. We report F1 scores of 0.90 for the first subtask and 0.53 for the third subtask, while we report an RMSE of 0.57 and 0.58 for the second and fourth subtasks, respectively",
    "checked": true,
    "id": "6497e0c763e7810c133dc3eb7df98cf4fc2c7504",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Anik Mondal",
      "Raksha Sharma"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.165": {
    "title": "ZYJ at SemEval-2021 Task 7: HaHackathon: Detecting and Rating Humor and Offense with ALBERT-Based Model",
    "volume": "workshop",
    "abstract": "This article introduces the submission of subtask 1 and subtask 2 that we participate in SemEval-2021 Task 7: HaHackathon: Detecting and Rating Humor and Offense, we use a model based on ALBERT that uses ALBERT as the module for extracting text features. We modify the upper layer structure by adding specific networks to better summarize the semantic information. Finally, our system achieves an F-Score of 0.9348 in subtask 1a, RMSE of 0.7214 in subtask 1b, F-Score of 0.4603 in subtask 1c, and RMSE of 0.5204 in subtask 2",
    "checked": true,
    "id": "c0150c8a55a271847a3fc965a5ac24474ec2ae51",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Yingjia Zhao",
      "Xin Tao"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.166": {
    "title": "UoR at SemEval-2021 Task 7: Utilizing Pre-trained DistilBERT Model and Multi-scale CNN for Humor Detection",
    "volume": "workshop",
    "abstract": "Humour detection is an interesting but difficult task in NLP. Because humorous might not be obvious in text, it can be embedded into context, hide behind the literal meaning and require prior knowledge to understand. We explored different shallow and deep methods to create a humour detection classifier for task 7-1a. Models like Logistic Regression, LSTM, MLP, CNN were used, and pre-trained models like DistilBert were introduced to generate accurate vector representation for textual data. We focused on applying multi-scale strategy on modelling, and compared different models. Our best model is the DistilBERT+MultiScale CNN, it used different sizes of CNN kernel to get multiple scales of features, which achieved 93.7% F1-score and 92.1% accuracy on the test set",
    "checked": true,
    "id": "989cc17c74a4d497abd08493d2e46f357fb16156",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zehao Liu",
      "Carl Haines",
      "Huizhi Liang"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.167": {
    "title": "TECHSSN at SemEval-2021 Task 7: Humor and Offense detection and classification using ColBERT embeddings",
    "volume": "workshop",
    "abstract": "This paper describes the system used for detecting humor in text. The system developed by the team TECHSSN uses binary classification techniques to classify the text. The data undergoes preprocessing and is given to ColBERT (Contextualized Late Interaction over BERT), a modification of Bidirectional Encoder Representations from Transformers (BERT). The model is re-trained and the weights are learned for the dataset. This system was developed for the task 7 of the competition, SemEval 2021",
    "checked": true,
    "id": "33c4fbc30ff123d0365d5d7e53a6c10bc46d78d5",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Rajalakshmi Sivanaiah",
      "Angel Deborah S",
      "S Milton Rajendram",
      "Mirnalinee Tt",
      "Abrit Pal Singh",
      "Aviansh Gupta",
      "Ayush Nanda"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.168": {
    "title": "Amherst685 at SemEval-2021 Task 7: Joint Modeling of Classification and Regression for Humor and Offense",
    "volume": "workshop",
    "abstract": "This paper describes our submission to theSemEval’21: Task 7- HaHackathon: Detecting and Rating Humor and Offense. In this challenge, we explore intermediate finetuning, backtranslation augmentation, multitask learning, and ensembling of different language models. Curiously, intermediate finetuning and backtranslation do not improve performance, while multitask learning and ensembling do improve performance. We explore why intermediate finetuning and backtranslation do not provide the same benefit as other natural language processing tasks and offer insight into the errors that our model makes. Our best performing system ranks 7th on Task 1bwith an RMSE of 0.5339",
    "checked": true,
    "id": "cb08776742290809f1dd7db9f357d381e89317d4",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Brian Zylich",
      "Akshay Gugnani",
      "Gabriel Brookman",
      "Nicholas Samoray"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.169": {
    "title": "DuluthNLP at SemEval-2021 Task 7: Fine-Tuning RoBERTa Model for Humor Detection and Offense Rating",
    "volume": "workshop",
    "abstract": "This paper presents the DuluthNLP submission to Task 7 of the SemEval 2021 competition on Detecting and Rating Humor and Offense. In it, we explain the approach used to train the model together with the process of fine-tuning our model in getting the results. We focus on humor detection, rating, and of-fense rating, representing three out of the four subtasks that were provided. We show that optimizing hyper-parameters for learning rate, batch size and number of epochs can increase the accuracy and F1 score for humor detection",
    "checked": true,
    "id": "a62537f08384f3ea0e8131f5e0dc9fc08eee4cdd",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Samuel Akrah"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.170": {
    "title": "CSECU-DSG at SemEval-2021 Task 7: Detecting and Rating Humor and Offense Employing Transformers",
    "volume": "workshop",
    "abstract": "With the emerging trends of using online platforms, peoples are increasingly interested in express their opinion through humorous texts. Identifying and rating humorous texts poses unique challenges to NLP due to subjective phenomena i.e. humor may vary to gender, profession, age, and classes of people. Besides, words with multiple senses, cultural domain, and pragmatic competence also need to be considered. A humorous text may be offensive to others. To address these challenges SemEval-2021 introduced a HaHackathon task focusing on detecting and rating humorous and offensive texts. This paper describes our participation in this task. We employed a stacked embedding and fine-tuned transformer models based classification and regression approach from the features from GPT2 medium, BERT, and RoBERTa transformer models. Besides, we utilized the fine-tuned BERT and RoBERTa models to examine the performances. Our method achieved competitive performances in this task",
    "checked": true,
    "id": "2368752cba69ebefe803f28c5e2ba93dd4968c08",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Afrin Sultana",
      "Nabila Ayman",
      "Abu Nowshed Chy"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.171": {
    "title": "RedwoodNLP at SemEval-2021 Task 7: Ensembled Pretrained and Lightweight Models for Humor Detection",
    "volume": "workshop",
    "abstract": "An understanding of humor is an essential component of human-facing NLP systems. In this paper, we investigate several methods for detecting humor in short statements as part of Semeval-2021 Shared Task 7. For Task 1a, we apply an ensemble of fine-tuned pre-trained language models; for Tasks 1b, 1c, and 2a, we investigate various tree-based and linear machine learning models. Our final system achieves an F1-score of 0.9571 (ranked 24 / 58) on Task 1a, an RMSE of 0.5580 (ranked 18 / 50) on Task 1b, an F1-score of 0.5024 (ranked 26 / 36) on Task 1c, and an RMSE of 0.7229 (ranked 45 / 48) on Task 2a",
    "checked": true,
    "id": "a5b9954df11c0ece31e7c43421e777626f05ad82",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Nathan Chi",
      "Ryan Chi"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.172": {
    "title": "EndTimes at SemEval-2021 Task 7: Detecting and Rating Humor and Offense with BERT and Ensembles",
    "volume": "workshop",
    "abstract": "This paper describes Humor-BERT, a set of BERT Large based models that we used in the SemEval-2021 Task 7: Detecting and Rating Humor and Offense. It presents pre and post processing techniques, variable threshold learning, meta learning and Ensemble approach to solve various sub-tasks that were part of the challenge. We also present a comparative analysis of various models we tried. Our method was ranked 4th in Humor Controversy Detection, 8th in Humor Detection, 19th in Average Offense Score prediction and 40th in Average Humor Score prediction globally. F1 score obtained for Humor classification was 0.9655 and for Controversy detection it was 0.6261. Our user name on the leader board is ThisIstheEnd and team name is EndTimes",
    "checked": true,
    "id": "88129b7e47ae7215a0f41b179faf83f086d380b1",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Chandan Kumar Pandey",
      "Chirag Singh",
      "Karan Mangla"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.173": {
    "title": "IIITH at SemEval-2021 Task 7: Leveraging transformer-based humourous and offensive text detection architectures using lexical and hurtlex features and task adaptive pretraining",
    "volume": "workshop",
    "abstract": "This paper describes our approach (IIITH) for SemEval-2021 Task 5: HaHackathon: Detecting and Rating Humor and Offense. Our results focus on two major objectives: (i) Effect of task adaptive pretraining on the performance of transformer based models (ii) How does lexical and hurtlex features help in quantifying humour and offense. In this paper, we provide a detailed description of our approach along with comparisions mentioned above",
    "checked": true,
    "id": "f62b6d37446f42f13548783322027d6c336d2eaf",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tathagata Raha",
      "Ishan Sanjeev Upadhyay",
      "Radhika Mamidi",
      "Vasudeva Varma"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.174": {
    "title": "FII FUNNY at SemEval-2021 Task 7: HaHackathon: Detecting and rating Humor and Offense",
    "volume": "workshop",
    "abstract": "The “HaHackathon: Detecting and Rating Humor and Offense” task at the SemEval 2021 competition focuses on detecting and rating the humor level in sentences, as well as the level of offensiveness contained in these texts with humoristic tones. In this paper, we present an approach based on recent Deep Learning techniques by both trying to train the models based on the dataset solely and by trying to fine-tune pre-trained models on the gigantic corpus",
    "checked": true,
    "id": "07a51649253055b6f91370ce4fcfe668e2986eb9",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Mihai Samson",
      "Daniela Gifu"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.175": {
    "title": "Counts@IITK at SemEval-2021 Task 8: SciBERT Based Entity And Semantic Relation Extraction For Scientific Data",
    "volume": "workshop",
    "abstract": "This paper presents the system for SemEval 2021 Task 8 (MeasEval). MeasEval is a novel span extraction, classification, and relation extraction task focused on finding quantities, attributes of these quantities, and additional information, including the related measured entities, properties, and measurement contexts. Our submitted system, which placed fifth (team rank) on the leaderboard, consisted of SciBERT with [CLS] token embedding and CRF layer on top. We were also placed first in Quantity (tied) and Unit subtasks, second in MeasuredEntity, Modifier and Qualifies subtasks, and third in Qualifier subtask",
    "checked": true,
    "id": "1cd3be6144ecf29f4434727a7a577245baafb3e8",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Akash Gangwar",
      "Sabhay Jain",
      "Shubham Sourav",
      "Ashutosh Modi"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.176": {
    "title": "CONNER: A Cascade Count and Measurement Extraction Tool for Scientific Discourse",
    "volume": "workshop",
    "abstract": "This paper presents our wining contribution to SemEval 2021 Task 8: MeasEval. The purpose of this task is identifying the counts and measurements from clinical scientific discourse, including quantities, entities, properties, qualifiers, units, modifiers, and their mutual relations. This task can be induced to a joint entity and relation extraction problem. Accordingly, we propose CONNER, a cascade count and measurement extraction tool that can identify entities and the corresponding relations in a two-step pipeline model. We provide a detailed description of the proposed model hereinafter. Furthermore, the impact of the essential modules and our in-process technical schemes are also investigated",
    "checked": true,
    "id": "f9b611df731c00f54397e3a0a76e3536ea9079c0",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Jiarun Cao",
      "Yuejia Xiang",
      "Yunyan Zhang",
      "Zhiyuan Qi",
      "Xi Chen",
      "Yefeng Zheng"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.177": {
    "title": "Stanford MLab at SemEval-2021 Task 8: 48 Hours Is All You Need",
    "volume": "workshop",
    "abstract": "This paper presents our system for the Quantity span identification, Unit of measurement identification and Value modifier classification subtasks of the MeasEval 2021 task. The purpose of the Quantity span identification task was to locate spans of text that contain a count or measurement, consisting of a value, usually followed by a unit and occasionally additional modifiers. The goal of the modifier classification task was to determine whether an associated text fragment served to indicate range, tolerance, mean value, etc. of a quantity. The developed systems used pre-trained BERT models which were fine-tuned for the task at hand. We present our system, investigate how architectural decisions affected model predictions, and conduct an error analysis. Overall, our system placed 12 / 19 in the shared task and in the 2nd place for the Unit subcategory",
    "checked": true,
    "id": "ed8ee851f48f7363122bd54c0b7c717b4981ab99",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Patrick Liu",
      "Niveditha Iyer",
      "Erik Rozi",
      "Ethan A. Chi"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.178": {
    "title": "LIORI at SemEval-2021 Task 8: Ask Transformer for measurements",
    "volume": "workshop",
    "abstract": "This work describes our approach for subtasks of SemEval-2021 Task 8: MeasEval: Counts and Measurements which took the official first place in the competition. To solve all subtasks we use multi-task learning in a question-answering-like manner. We also use learnable scalar weights to weight subtasks’ contribution to the final loss in multi-task training. We fine-tune LUKE to extract quantity spans and we fine-tune RoBERTa to extract everything related to found quantities, including quantities themselves",
    "checked": true,
    "id": "dc9db9aff2995428164ae9b99514323be5d2d7f5",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Adis Davletov",
      "Denis Gordeev",
      "Nikolay Arefyev",
      "Emil Davletov"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.179": {
    "title": "Sattiy at SemEval-2021 Task 9: An Ensemble Solution for Statement Verification and Evidence Finding with Tables",
    "volume": "workshop",
    "abstract": "Question answering from semi-structured tables can be seen as a semantic parsing task and is significant and practical for pushing the boundary of natural language understanding. Existing research mainly focuses on understanding contents from unstructured evidence, e.g., news, natural language sentences and documents. The task of verification from structured evidence, such as tables, charts, and databases, is still less-explored. This paper describes sattiy team’s system in SemEval-2021 task 9: Statement Verification and Evidence Finding with Tables (SEM-TAB-FACT)(CITATION). This competition aims to verify statements and to find evidence from tables for scientific articles and to promote proper interpretation of the surrounding article. In this paper we exploited ensemble models of pre-trained language models over tables, TaPas and TaBERT, for Task A and adjust the result based on some rules extracted for Task B. Finally, in the leadboard, we attain the F1 scores of 0.8496 and 0.7732 in Task A for the 2-way and 3-way evaluation, respectively, and the F1 score of 0.4856 in Task B",
    "checked": true,
    "id": "b9c8e25195282cf7698017da2ea2c2d86a3de19d",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyi Ruan",
      "Meizhi Jin",
      "Jian Ma",
      "Haiqin Yang",
      "Lianxin Jiang",
      "Yang Mo",
      "Mengyuan Zhou"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.180": {
    "title": "Volta at SemEval-2021 Task 9: Statement Verification and Evidence Finding with Tables using TAPAS and Transfer Learning",
    "volume": "workshop",
    "abstract": "Tables are widely used in various kinds of documents to present information concisely. Understanding tables is a challenging problem that requires an understanding of language and table structure, along with numerical and logical reasoning. In this paper, we present our systems to solve Task 9 of SemEval-2021: Statement Verification and Evidence Finding with Tables (SEM-TAB-FACTS). The task consists of two subtasks: (A) Given a table and a statement, predicting whether the table supports the statement and (B) Predicting which cells in the table provide evidence for/against the statement. We fine-tune TAPAS (a model which extends BERT’s architecture to capture tabular structure) for both the subtasks as it has shown state-of-the-art performance in various table understanding tasks. In subtask A, we evaluate how transfer learning and standardizing tables to have a single header row improves TAPAS’ performance. In subtask B, we evaluate how different fine-tuning strategies can improve TAPAS’ performance. Our systems achieve an F1 score of 67.34 in subtask A three-way classification, 72.89 in subtask A two-way classification, and 62.95 in subtask B",
    "checked": true,
    "id": "c7006f1b6e29a37e03797616e585349f2a4cbc6a",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Devansh Gautam",
      "Kshitij Gupta",
      "Manish Shrivastava"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.181": {
    "title": "KaushikAcharya at SemEval-2021 Task 9: Candidate Generation for Fact Verification over Tables",
    "volume": "workshop",
    "abstract": "This paper describes the system submitted in the SemEval-2021 Statement Verification and Evidence Finding with Tables task. The system relies on candidate generation for logical forms on the table based on keyword matching and dependency parsing on the claim statements",
    "checked": true,
    "id": "cfc13aa7f8ad56c3b2769a9e95d6e9db0ebabec7",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Kaushik Acharya"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.182": {
    "title": "AttesTable at SemEval-2021 Task 9: Extending Statement Verification with Tables for Unknown Class, and Semantic Evidence Finding",
    "volume": "workshop",
    "abstract": "This paper describes our approach for Task 9 of SemEval 2021: Statement Verification and Evidence Finding with Tables. We participated in both subtasks, namely statement verification and evidence finding. For the subtask of statement verification, we extend the TAPAS model to adapt to the ‘unknown’ class of statements by finetuning it on an augmented version of the task data. For the subtask of evidence finding, we finetune the DistilBERT model in a Siamese setting",
    "checked": true,
    "id": "736bebb9c6d4451486453e1dc4fe6726c99bb127",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Harshit Varma",
      "Aadish Jain",
      "Pratik Ratadiya",
      "Abhishek Rathi"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.183": {
    "title": "MedAI at SemEval-2021 Task 10: Negation-aware Pre-training for Source-free Negation Detection Domain Adaptation",
    "volume": "workshop",
    "abstract": "Due to the increasing concerns for data privacy, source-free unsupervised domain adaptation attracts more and more research attention, where only a trained source model is assumed to be available, while the labeled source data remain private. To get promising adaptation results, we need to find effective ways to transfer knowledge learned in source domain and leverage useful domain specific information from target domain at the same time. This paper describes our winning contribution to SemEval 2021 Task 10: Source-Free Domain Adaptation for Semantic Processing. Our key idea is to leverage the model trained on source domain data to generate pseudo labels for target domain samples. Besides, we propose Negation-aware Pre-training (NAP) to incorporate negation knowledge into model. Our method win the 1st place with F1-score of 0.822 on the official blind test set of Negation Detection Track",
    "checked": true,
    "id": "2abe6aa1e1e8022c4fdffc4fa1fd93a110044756",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinquan Sun",
      "Qi Zhang",
      "Yu Wang",
      "Lei Zhang"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.184": {
    "title": "YNU-HPCC at SemEval-2021 Task 10: Using a Transformer-based Source-Free Domain Adaptation Model for Semantic Processing",
    "volume": "workshop",
    "abstract": "Data sharing restrictions are common in NLP datasets. The purpose of this task is to develop a model trained in a source domain to make predictions for a target domain with related domain data. To address the issue, the organizers provided the models that fine-tuned a large number of source domain data on pre-trained models and the dev data for participants. But the source domain data was not distributed. This paper describes the provided model to the NER (Name entity recognition) task and the ways to develop the model. As a little data provided, pre-trained models are suitable to solve the cross-domain tasks. The models fine-tuned by large number of another domain could be effective in new domain because the task had no change",
    "checked": true,
    "id": "7838ce0dc289ac750f8bc86b89b1127e7f4d7127",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhewen Yu",
      "Jin Wang",
      "Xuejie Zhang"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.185": {
    "title": "ECNUICA at SemEval-2021 Task 11: Rule based Information Extraction Pipeline",
    "volume": "workshop",
    "abstract": "This paper presents our endeavor for solving task11, NLPContributionGraph, of SemEval-2021. The purpose of the task was to extract triples from a paper in the Nature Language Processing field for constructing an Open Research Knowledge Graph. The task includes three sub-tasks: detecting the contribution sentences in papers, identifying scientific terms and predicate phrases from the contribution sentences; and inferring triples in the form of (subject, predicate, object) as statements for Knowledge Graph building. In this paper, we apply an ensemble of various fine-tuned pre-trained language models (PLM) for tasks one and two. In addition, self-training methods are adopted for tackling the shortage of annotated data. For the third task, rather than using classic neural open information extraction (OIE) architectures, we generate potential triples via manually designed rules and develop a binary classifier to differentiate positive ones from others. The quantitative results show that we obtain the 4th, 2nd, and 2nd rank in three evaluation phases",
    "checked": true,
    "id": "0ea135bd1b568025924c0a090de2899d764f910d",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Jiaju Lin",
      "Jing Ling",
      "Zhiwei Wang",
      "Jiawei Liu",
      "Qin Chen",
      "Liang He"
    ]
  },
  "https://aclanthology.org/2021.semeval-1.186": {
    "title": "UOR at SemEval-2021 Task 12: On Crowd Annotations; Learning with Disagreements to optimise crowd truth",
    "volume": "workshop",
    "abstract": "Crowdsourcing has been ubiquitously used for annotating enormous collections of data. However, the major obstacles to using crowd-sourced labels are noise and errors from non-expert annotations. In this work, two approaches dealing with the noise and errors in crowd-sourced labels are proposed. The first approach uses Sharpness-Aware Minimization (SAM), an optimization technique robust to noisy labels. The other approach leverages a neural network layer called softmax-Crowdlayer specifically designed to learn from crowd-sourced annotations. According to the results, the proposed approaches can improve the performance of the Wide Residual Network model and Multi-layer Perception model applied on crowd-sourced datasets in the image processing domain. It also has similar and comparable results with the majority voting technique when applied to the sequential data domain whereby the Bidirectional Encoder Representations from Transformers (BERT) is used as the base model in both instances",
    "checked": true,
    "id": "b11e381e9e831c86ef9fc7204d50b6e0ff82077a",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Emmanuel Osei-Brefo",
      "Thanet Markchom",
      "Huizhi Liang"
    ]
  },
  "https://aclanthology.org/2021.sigmorphon-1.1": {
    "title": "Towards Detection and Remediation of Phonemic Confusion",
    "volume": "workshop",
    "abstract": "Reducing communication breakdown is critical to success in interactive NLP applications, such as dialogue systems. To this end, we propose a confusion-mitigation framework for the detection and remediation of communication breakdown. In this work, as a first step towards implementing this framework, we focus on detecting phonemic sources of confusion. As a proof-of-concept, we evaluate two neural architectures in predicting the probability that a listener will misunderstand phonemes in an utterance. We show that both neural models outperform a weighted n-gram baseline, showing early promise for the broader framework",
    "checked": true,
    "id": "1bcba3bdcb914e5ae9df95f1dc1b02c5634004e7",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francois Roewer-Despres",
      "Arnold Yeung",
      "Ilan Kogan"
    ]
  },
  "https://aclanthology.org/2021.sigmorphon-1.2": {
    "title": "Recursive prosody is not finite-state",
    "volume": "workshop",
    "abstract": "This paper investigates bounds on the generative capacity of prosodic processes, by focusing on the complexity of recursive prosody in coordination contexts in English (Wagner, 2010). Although all phonological processes and most prosodic processes are computationally regular string languages, we show that recursive prosody is not. The output string language is instead parallel multiple context-free (Seki et al., 1991). We evaluate the complexity of the pattern over strings, and then move on to a characterization over trees that requires the expressivity of multi bottom-up tree transducers. In doing so, we provide a foundation for future mathematically grounded investigations of the syntax-prosody interface",
    "checked": true,
    "id": "70d361e078d686bf15679c264b5100e1ab2f8cbd",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Hossep Dolatian",
      "Aniello De Santo",
      "Thomas Graf"
    ]
  },
  "https://aclanthology.org/2021.sigmorphon-1.3": {
    "title": "The Match-Extend serialization algorithm in Multiprecedence",
    "volume": "workshop",
    "abstract": "Raimy (1999; 2000a; 2000b) proposed a graphical formalism for modeling reduplication, originallymostly focused on phonological overapplication in a derivational framework. This framework is now known as Precedence-based phonology or Multiprecedence phonology. Raimy’s idea is that the segments at the input to the phonology are not totally ordered by precedence. This paper tackles a challenge that arose with Raimy’s work, the development of a deterministic serialization algorithm as part of the derivation of surface forms. The Match-Extend algorithm introduced here requires fewer assumptions and sticks tighter to the attested typology. The algorithm also contains no parameter or constraint specific to individual graphs or topologies, unlike previous proposals. Match-Extend requires nothing except knowing the last added set of links",
    "checked": true,
    "id": "8def3fe065f21f72fadf7c188bd76bf4ed5ef979",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maxime Papillon"
    ]
  },
  "https://aclanthology.org/2021.sigmorphon-1.4": {
    "title": "Incorporating tone in the calculation of phonotactic probability",
    "volume": "workshop",
    "abstract": "This paper investigates how the ordering of tone relative to the segmental string influences the calculation of phonotactic probability. Trigram and recurrent neural network models were trained on syllable lexicons of four Asian syllable-tone languages (Mandarin, Thai, Vietnamese, and Cantonese) in which tone was treated as a segment occurring in different positions in the string. For trigram models, the optimal permutation interacted with language, while neural network models were relatively unaffected by tone position in all languages. In addition to providing a baseline for future evaluation, these results suggest that phonotactic probability is robust to choices of how tone is ordered with respect to other elements in the syllable",
    "checked": true,
    "id": "86ac3f2c0f81039607f58e04897f748307391f3a",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "James Kirby"
    ]
  },
  "https://aclanthology.org/2021.sigmorphon-1.5": {
    "title": "MorphyNet: a Large Multilingual Database of Derivational and Inflectional Morphology",
    "volume": "workshop",
    "abstract": "Large-scale morphological databases provide essential input to a wide range of NLP applications. Inflectional data is of particular importance for morphologically rich (agglutinative and highly inflecting) languages, and derivations can be used, e.g. to infer the semantics of out-of-vocabulary words. Extending the scope of state-of-the-art multilingual morphological databases, we announce the release of MorphyNet, a high-quality resource with 15 languages, 519k derivational and 10.1M inflectional entries, and a rich set of morphological features. MorphyNet was extracted from Wiktionary using both hand-crafted and automated methods, and was manually evaluated to be of a precision higher than 98%. Both the resource generation logic and the resulting database are made freely available and are reusable as stand-alone tools or in combination with existing resources",
    "checked": true,
    "id": "7fbf9d5e1bbac56efba21021d4577620ba3b3ee5",
    "semantic_title": "",
    "citation_count": 12,
    "authors": [
      "Khuyagbaatar Batsuren",
      "Gábor Bella",
      "Fausto Giunchiglia"
    ]
  },
  "https://aclanthology.org/2021.sigmorphon-1.6": {
    "title": "A Study of Morphological Robustness of Neural Machine Translation",
    "volume": "workshop",
    "abstract": "In this work, we analyze the robustness of neural machine translation systems towards grammatical perturbations in the source. In particular, we focus on morphological inflection related perturbations. While this has been recently studied for English→French (MORPHEUS) (Tan et al., 2020), it is unclear how this extends to Any→English translation systems. We propose MORPHEUS-MULTILINGUAL that utilizes UniMorph dictionaries to identify morphological perturbations to source that adversely affect the translation models. Along with an analysis of state-of-the-art pretrained MT systems, we train and analyze systems for 11 language pairs using the multilingual TED corpus (Qi et al., 2018). We also compare this to actual errors of non-native speakers using Grammatical Error Correction datasets. Finally, we present a qualitative and quantitative analysis of the robustness of Any→English translation systems",
    "checked": true,
    "id": "eede77569bc0b96ff54842528e86ef7a4b25011b",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Sai Muralidhar Jayanthi",
      "Adithya Pratapa"
    ]
  },
  "https://aclanthology.org/2021.sigmorphon-1.7": {
    "title": "Sample-efficient Linguistic Generalizations through Program Synthesis: Experiments with Phonology Problems",
    "volume": "workshop",
    "abstract": "Neural models excel at extracting statistical patterns from large amounts of data, but struggle to learn patterns or reason about language from only a few examples. In this paper, we ask: Can we learn explicit rules that generalize well from only a few examples? We explore this question using program synthesis. We develop a synthesis model to learn phonology rules as programs in a domain-specific language. We test the ability of our models to generalize from few training examples using our new dataset of problems from the Linguistics Olympiad, a challenging set of tasks that require strong linguistic reasoning ability. In addition to being highly sample-efficient, our approach generates human-readable programs, and allows control over the generalizability of the learnt programs",
    "checked": true,
    "id": "29cf96907c19de528c21e177fabafbc1d80868e5",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Saujas Vaduguru",
      "Aalok Sathe",
      "Monojit Choudhury",
      "Dipti Sharma"
    ]
  },
  "https://aclanthology.org/2021.sigmorphon-1.8": {
    "title": "Findings of the SIGMORPHON 2021 Shared Task on Unsupervised Morphological Paradigm Clustering",
    "volume": "workshop",
    "abstract": "We describe the second SIGMORPHON shared task on unsupervised morphology: the goal of the SIGMORPHON 2021 Shared Task on Unsupervised Morphological Paradigm Clustering is to cluster word types from a raw text corpus into paradigms. To this end, we release corpora for 5 development and 9 test languages, as well as gold partial paradigms for evaluation. We receive 14 submissions from 4 teams that follow different strategies, and the best performing system is based on adaptor grammars. Results vary significantly across languages. However, all systems are outperformed by a supervised lemmatizer, implying that there is still room for improvement",
    "checked": true,
    "id": "893357a80188567fba7d05bc0c69ac0987b2ff96",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Adam Wiemerslage",
      "Arya D. McCarthy",
      "Alexander Erdmann",
      "Garrett Nicolai",
      "Manex Agirrezabal",
      "Miikka Silfverberg",
      "Mans Hulden",
      "Katharina Kann"
    ]
  },
  "https://aclanthology.org/2021.sigmorphon-1.9": {
    "title": "Adaptor Grammars for Unsupervised Paradigm Clustering",
    "volume": "workshop",
    "abstract": "This work describes the Edinburgh submission to the SIGMORPHON 2021 Shared Task 2 on unsupervised morphological paradigm clustering. Given raw text input, the task was to assign each token to a cluster with other tokens from the same paradigm. We use Adaptor Grammar segmentations combined with frequency-based heuristics to predict paradigm clusters. Our system achieved the highest average F1 score across 9 test languages, placing first out of 15 submissions",
    "checked": true,
    "id": "13e1bf38be47905517da3777f22d70daa4389da8",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Kate McCurdy",
      "Sharon Goldwater",
      "Adam Lopez"
    ]
  },
  "https://aclanthology.org/2021.sigmorphon-1.10": {
    "title": "Orthographic vs. Semantic Representations for Unsupervised Morphological Paradigm Clustering",
    "volume": "workshop",
    "abstract": "This paper presents two different systems for unsupervised clustering of morphological paradigms, in the context of the SIGMORPHON 2021 Shared Task 2. The goal of this task is to correctly cluster words in a given language by their inflectional paradigm, without any previous knowledge of the language and without supervision from labeled data of any sort. The words in a single morphological paradigm are different inflectional variants of an underlying lemma, meaning that the words share a common core meaning. They also - usually - show a high degree of orthographical similarity. Following these intuitions, we investigate KMeans clustering using two different types of word representations: one focusing on orthographical similarity and the other focusing on semantic similarity.Additionally, we discuss the merits of randomly initialized centroids versus pre-defined centroids for clustering. Pre-defined centroids are identified based on either a standard longest common substring algorithm or a connected graph method built off of longest common substring. For all development languages, the character-based embeddings perform similarly to the baseline, and the semantic embeddings perform well below the baseline.Analysis of the systems’ errors suggests that clustering based on orthographic representations is suitable for a wide range of morphological mechanisms, particularly as part of a larger system",
    "checked": true,
    "id": "2e645c7f9172034255ea266770fa99d43fe84e49",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "E. Margaret Perkoff",
      "Josh Daniels",
      "Alexis Palmer"
    ]
  },
  "https://aclanthology.org/2021.sigmorphon-1.11": {
    "title": "Unsupervised Paradigm Clustering Using Transformation Rules",
    "volume": "workshop",
    "abstract": "This paper describes the submission of the CU-UBC team for the SIGMORPHON 2021 Shared Task 2: Unsupervised morphological paradigm clustering. Our system generates paradigms using morphological transformation rules which are discovered from raw data. We experiment with two methods for discovering rules. Our first approach generates prefix and suffix transformations between similar strings. Secondly, we experiment with more general rules which can apply transformations inside the input strings in addition to prefix and suffix transformations. We find that the best overall performance is delivered by prefix and suffix rules but more general transformation rules perform better for languages with templatic morphology and very high morpheme-to-word ratios",
    "checked": true,
    "id": "6461033c88668c76771390fa2bc281ae42a9297c",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Changbing Yang",
      "Garrett Nicolai",
      "Miikka Silfverberg"
    ]
  },
  "https://aclanthology.org/2021.sigmorphon-1.12": {
    "title": "Paradigm Clustering with Weighted Edit Distance",
    "volume": "workshop",
    "abstract": "This paper describes our system for the SIGMORPHON 2021 Shared Task on Unsupervised Morphological Paradigm Clustering, which asks participants to group inflected forms together according their underlying lemma without the aid of annotated training data. We employ agglomerative clustering to group word forms together using a metric that combines an orthographic distance and a semantic distance from word embeddings. We experiment with two variations of an edit distance-based model for quantifying orthographic distance, but, due to time constraints, our system does not improve over the shared task’s baseline system",
    "checked": true,
    "id": "5a5ae68155568c971a1dffd7dae9f46683e1d2f8",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Andrew Gerlach",
      "Adam Wiemerslage",
      "Katharina Kann"
    ]
  },
  "https://aclanthology.org/2021.sigmorphon-1.13": {
    "title": "Results of the Second SIGMORPHON Shared Task on Multilingual Grapheme-to-Phoneme Conversion",
    "volume": "workshop",
    "abstract": "Grapheme-to-phoneme conversion is an important component in many speech technologies, but until recently there were no multilingual benchmarks for this task. The second iteration of the SIGMORPHON shared task on multilingual grapheme-to-phoneme conversion features many improvements from the previous year’s task (Gorman et al. 2020), including additional languages, a stronger baseline, three subtasks varying the amount of available resources, extensive quality assurance procedures, and automated error analyses. Four teams submitted a total of thirteen systems, at best achieving relative reductions of word error rate of 11% in the high-resource subtask and 4% in the low-resource subtask",
    "checked": true,
    "id": "f68fd8ebd2340e0cf86f1b12655ee08557c8f61e",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Lucas F.E. Ashby",
      "Travis M. Bartley",
      "Simon Clematide",
      "Luca Del Signore",
      "Cameron Gibson",
      "Kyle Gorman",
      "Yeonju Lee-Sikka",
      "Peter Makarov",
      "Aidan Malanoski",
      "Sean Miller",
      "Omar Ortiz",
      "Reuben Raff",
      "Arundhati Sengupta",
      "Bora Seo",
      "Yulia Spektor",
      "Winnie Yan"
    ]
  },
  "https://aclanthology.org/2021.sigmorphon-1.14": {
    "title": "Data augmentation for low-resource grapheme-to-phoneme mapping",
    "volume": "workshop",
    "abstract": "In this paper we explore a very simple neural approach to mapping orthography to phonetic transcription in a low-resource context. The basic idea is to start from a baseline system and focus all efforts on data augmentation. We will see that some techniques work, but others do not",
    "checked": true,
    "id": "59340c7c255b37a43ddc2b1a57d4ea95efd773df",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Michael Hammond"
    ]
  },
  "https://aclanthology.org/2021.sigmorphon-1.15": {
    "title": "Linguistic Knowledge in Multilingual Grapheme-to-Phoneme Conversion",
    "volume": "workshop",
    "abstract": "This paper documents the UBC Linguistics team’s approach to the SIGMORPHON 2021 Grapheme-to-Phoneme Shared Task, concentrating on the low-resource setting. Our systems expand the baseline model with simple modifications informed by syllable structure and error analysis. In-depth investigation of test-set predictions shows that our best model rectifies a significant number of mistakes compared to the baseline prediction, besting all other submissions. Our results validate the view that careful error analysis in conjunction with linguistic knowledge can lead to more effective computational modeling",
    "checked": true,
    "id": "88f5cb3d723dc47644583c7b4dbfa515ab60912a",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Roger Yu-Hsiang Lo",
      "Garrett Nicolai"
    ]
  },
  "https://aclanthology.org/2021.sigmorphon-1.16": {
    "title": "Avengers, Ensemble! Benefits of ensembling in grapheme-to-phoneme prediction",
    "volume": "workshop",
    "abstract": "We describe three baseline beating systems for the high-resource English-only sub-task of the SIGMORPHON 2021 Shared Task 1: a small ensemble that Dialpad’s speech recognition team uses internally, a well-known off-the-shelf model, and a larger ensemble model comprising these and others. We additionally discuss the challenges related to the provided data, along with the processing steps we took",
    "checked": true,
    "id": "edd0e3847f5e2c727ed92de6b17dd91b78a8cf22",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Vagrant Gautam",
      "Wang Yau Li",
      "Zafarullah Mahmood",
      "Frederic Mailhot",
      "Shreekantha Nadig",
      "Riqiang Wang",
      "Nathan Zhang"
    ]
  },
  "https://aclanthology.org/2021.sigmorphon-1.17": {
    "title": "CLUZH at SIGMORPHON 2021 Shared Task on Multilingual Grapheme-to-Phoneme Conversion: Variations on a Baseline",
    "volume": "workshop",
    "abstract": "This paper describes the submission by the team from the Department of Computational Linguistics, Zurich University, to the Multilingual Grapheme-to-Phoneme Conversion (G2P) Task 1 of the SIGMORPHON 2021 challenge in the low and medium settings. The submission is a variation of our 2020 G2P system, which serves as the baseline for this year’s challenge. The system is a neural transducer that operates over explicit edit actions and is trained with imitation learning. For this challenge, we experimented with the following changes: a) emitting phoneme segments instead of single character phonemes, b) input character dropout, c) a mogrifier LSTM decoder (Melis et al., 2019), d) enriching the decoder input with the currently attended input character, e) parallel BiLSTM encoders, and f) an adaptive batch size scheduler. In the low setting, our best ensemble improved over the baseline, however, in the medium setting, the baseline was stronger on average, although for certain languages improvements could be observed",
    "checked": true,
    "id": "f2815efe76e685856eafaef75291b0c43845ea3e",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Simon Clematide",
      "Peter Makarov"
    ]
  },
  "https://aclanthology.org/2021.sigmorphon-1.18": {
    "title": "What transfers in morphological inflection? Experiments with analogical models",
    "volume": "workshop",
    "abstract": "We investigate how abstract processes like suffixation can be learned from morphological inflection task data using an analogical memory-based framework. In this framework, the inflection target form is specified by providing an example inflection of another word in the language. We show that this model is capable of near-baseline performance on the SigMorphon 2020 inflection challenge. Such a model can make predictions for unseen languages, allowing us to perform one-shot inflection on natural languages and investigate morphological transfer with synthetic probes. Accuracy for one-shot transfer can be unexpectedly high for some target languages (88% in Shona) and language families (53% across Romance). Probe experiments show that the model learns partially generalizable representations of prefixation, suffixation and reduplication, aiding its ability to transfer. We argue that the degree of generality of these process representations also helps to explain transfer results from previous research",
    "checked": true,
    "id": "6c11177251e90977e7dbc49825025029739a840f",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Micha Elsner"
    ]
  },
  "https://aclanthology.org/2021.sigmorphon-1.19": {
    "title": "Simple induction of (deterministic) probabilistic finite-state automata for phonotactics by stochastic gradient descent",
    "volume": "workshop",
    "abstract": "We introduce a simple and highly general phonotactic learner which induces a probabilistic finite-state automaton from word-form data. We describe the learner and show how to parameterize it to induce unrestricted regular languages, as well as how to restrict it to certain subregular classes such as Strictly k-Local and Strictly k-Piecewise languages. We evaluate the learner on its ability to learn phonotactic constraints in toy examples and in datasets of Quechua and Navajo. We find that an unrestricted learner is the most accurate overall when modeling attested forms not seen in training; however, only the learner restricted to the Strictly Piecewise language class successfully captures certain nonlocal phonotactic constraints. Our learner serves as a baseline for more sophisticated methods",
    "checked": true,
    "id": "0609827aa9c86d26e2beee1ddf56e196ebaddc6a",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Huteng Dai",
      "Richard Futrell"
    ]
  },
  "https://aclanthology.org/2021.sigmorphon-1.20": {
    "title": "Recognizing Reduplicated Forms: Finite-State Buffered Machines",
    "volume": "workshop",
    "abstract": "Total reduplication is common in natural language phonology and morphology. However, formally as copying on reduplicants of unbounded size, unrestricted total reduplication requires computational power beyond context-free, while other phonological and morphological patterns are regular, or even sub-regular. Thus, existing language classes characterizing reduplicated strings inevitably include typologically unattested context-free patterns, such as reversals. This paper extends regular languages to incorporate reduplication by introducing a new computational device: finite state buffered machine (FSBMs). We give its mathematical definitions and discuss some closure properties of the corresponding set of languages. As a result, the class of regular languages and languages derived from them through a copying mechanism is characterized. Suggested by previous literature, this class of languages should approach the characterization of natural language word sets",
    "checked": true,
    "id": "cddc379da5fef64463763fa24f4216bb6d560951",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Yang Wang"
    ]
  },
  "https://aclanthology.org/2021.sigmorphon-1.21": {
    "title": "An FST morphological analyzer for the Gitksan language",
    "volume": "workshop",
    "abstract": "This paper presents a finite-state morphological analyzer for the Gitksan language. The analyzer draws from a 1250-token Eastern dialect wordlist. It is based on finite-state technology and additionally includes two extensions which can provide analyses for out-of-vocabulary words: rules for generating predictable dialect variants, and a neural guesser component. The pre-neural analyzer, tested against interlinear-annotated texts from multiple dialects, achieves coverage of (75-81%), and maintains high precision (95-100%). The neural extension improves coverage at the cost of lowered precision",
    "checked": true,
    "id": "637dce9bfec39c4f253688cc8b3c64b0a88fc7af",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Clarissa Forbes",
      "Garrett Nicolai",
      "Miikka Silfverberg"
    ]
  },
  "https://aclanthology.org/2021.sigmorphon-1.22": {
    "title": "Comparative Error Analysis in Neural and Finite-state Models for Unsupervised Character-level Transduction",
    "volume": "workshop",
    "abstract": "Traditionally, character-level transduction problems have been solved with finite-state models designed to encode structural and linguistic knowledge of the underlying process, whereas recent approaches rely on the power and flexibility of sequence-to-sequence models with attention. Focusing on the less explored unsupervised learning scenario, we compare the two model classes side by side and find that they tend to make different types of errors even when achieving comparable performance. We analyze the distributions of different error classes using two unsupervised tasks as testbeds: converting informally romanized text into the native script of its language (for Russian, Arabic, and Kannada) and translating between a pair of closely related languages (Serbian and Bosnian). Finally, we investigate how combining finite-state and sequence-to-sequence models at decoding time affects the output quantitatively and qualitatively",
    "checked": true,
    "id": "4e749b2e0728044af44d50a708fc99d49359ea0b",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Maria Ryskina",
      "Eduard Hovy",
      "Taylor Berg-Kirkpatrick",
      "Matthew R. Gormley"
    ]
  },
  "https://aclanthology.org/2021.sigmorphon-1.23": {
    "title": "Finite-state Model of Shupamem Reduplication",
    "volume": "workshop",
    "abstract": "Shupamem, a language of Western Cameroon, is a tonal language which also exhibits the morpho-phonological process of full reduplication. This creates two challenges for finite-state model of its morpho-syntax and morphophonology: how to manage the full reduplication and the autosegmental nature of lexical tone. Dolatian and Heinz (2020) explain how 2-way finite-state transducers can model full reduplication without an exponential increase in states, and finite-state transducers with multiple tapes have been used to model autosegmental tiers, including tone (Wiebe, 1992; Dolatian and Rawski, 2020a). Here we synthesize 2-way finite-state transducers and multitape transducers, resulting in a finite-state formalism that subsumes both, to account for the full reduplicative processes in Shupamem which also affect tone",
    "checked": true,
    "id": "9110aaecf56b3cd488efa03db48a2a8f6ec541b0",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Magdalena Markowska",
      "Jeffrey Heinz",
      "Owen Rambow"
    ]
  },
  "https://aclanthology.org/2021.sigmorphon-1.24": {
    "title": "Improved pronunciation prediction accuracy using morphology",
    "volume": "workshop",
    "abstract": "Pronunciation lexicons and prediction models are a key component in several speech synthesis and recognition systems. We know that morphologically related words typically follow a fixed pattern of pronunciation which can be described by language-specific paradigms. In this work we explore how deep recurrent neural networks can be used to automatically learn and exploit this pattern to improve the pronunciation prediction quality of words related by morphological inflection. We propose two novel approaches for supplying morphological information, using the word’s morphological class and its lemma, which are typically annotated in standard lexicons. We report improvements across a number of European languages with varying degrees of phonological and morphological complexity, and two language families, with greater improvements for languages where the pronunciation prediction task is inherently more challenging. We also observe that combining bidirectional LSTM networks with attention mechanisms is an effective neural approach for the computational problem considered, across languages. Our approach seems particularly beneficial in the low resource setting, both by itself and in conjunction with transfer learning",
    "checked": true,
    "id": "bd010399a50c95b872dd98849087b22d176fa413",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Dravyansh Sharma",
      "Saumya Sahai",
      "Neha Chaudhari",
      "Antoine Bruguier"
    ]
  },
  "https://aclanthology.org/2021.sigmorphon-1.25": {
    "title": "SIGMORPHON 2021 Shared Task on Morphological Reinflection: Generalization Across Languages",
    "volume": "workshop",
    "abstract": "This year’s iteration of the SIGMORPHON Shared Task on morphological reinflection focuses on typological diversity and cross-lingual variation of morphosyntactic features. In terms of the task, we enrich UniMorph with new data for 32 languages from 13 language families, with most of them being under-resourced: Kunwinjku, Classical Syriac, Arabic (Modern Standard, Egyptian, Gulf), Hebrew, Amharic, Aymara, Magahi, Braj, Kurdish (Central, Northern, Southern), Polish, Karelian, Livvi, Ludic, Veps, Võro, Evenki, Xibe, Tuvan, Sakha, Turkish, Indonesian, Kodi, Seneca, Asháninka, Yanesha, Chukchi, Itelmen, Eibela. We evaluate six systems on the new data and conduct an extensive error analysis of the systems’ predictions. Transformer-based models generally demonstrate superior performance on the majority of languages, achieving >90% accuracy on 65% of them. The languages on which systems yielded low accuracy are mainly under-resourced, with a limited amount of data. Most errors made by the systems are due to allomorphy, honorificity, and form variation. In addition, we observe that systems especially struggle to inflect multiword lemmas. The systems also produce misspelled forms or end up in repetitive loops (e.g., RNN-based models). Finally, we report a large drop in systems’ performance on previously unseen lemmas",
    "checked": true,
    "id": "136235d2a3dc4f1c995eaf977aec9c42114da850",
    "semantic_title": "",
    "citation_count": 21,
    "authors": [
      "Tiago Pimentel",
      "Maria Ryskina",
      "Sabrina J. Mielke",
      "Shijie Wu",
      "Eleanor Chodroff",
      "Brian Leonard",
      "Garrett Nicolai",
      "Yustinus Ghanggo Ate",
      "Salam Khalifa",
      "Nizar Habash",
      "Charbel El-Khaissi",
      "Omer Goldman",
      "Michael Gasser",
      "William Lane",
      "Matt Coler",
      "Arturo Oncevay",
      "Jaime Rafael Montoya Samame",
      "Gema Celeste Silva Villegas",
      "Adam Ek",
      "Jean-Philippe Bernardy",
      "Andrey Shcherbakov",
      "Aziyana Bayyr-ool",
      "Karina Sheifer",
      "Sofya Ganieva",
      "Matvey Plugaryov",
      "Elena Klyachko",
      "Ali Salehi",
      "Andrew Krizhanovsky",
      "Natalia Krizhanovsky",
      "Clara Vania",
      "Sardana Ivanova",
      "Aelita Salchak",
      "Christopher Straughn",
      "Zoey Liu",
      "Jonathan North Washington",
      "Duygu Ataman",
      "Witold Kieraś",
      "Marcin Woliński",
      "Totok Suhardijanto",
      "Niklas Stoehr",
      "Zahroh Nuriah",
      "Shyam Ratan",
      "Francis M. Tyers",
      "Edoardo M. Ponti",
      "Grant Aiton",
      "Richard J. Hatcher",
      "Emily Prud’hommeaux",
      "Ritesh Kumar",
      "Mans Hulden",
      "Botond Barta",
      "Dorina Lakatos",
      "Gábor Szolnok",
      "Judit Ács",
      "Mohit Raj",
      "David Yarowsky",
      "Ryan Cotterell",
      "Ben Ambridge",
      "Ekaterina Vylomova"
    ]
  },
  "https://aclanthology.org/2021.sigmorphon-1.26": {
    "title": "Training Strategies for Neural Multilingual Morphological Inflection",
    "volume": "workshop",
    "abstract": "This paper presents the submission of team GUCLASP to SIGMORPHON 2021 Shared Task on Generalization in Morphological Inflection Generation. We develop a multilingual model for Morphological Inflection and primarily focus on improving the model by using various training strategies to improve accuracy and generalization across languages",
    "checked": true,
    "id": "4fa1a3d26910458131536913e3f6db2cbeb79e4b",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Adam Ek",
      "Jean-Philippe Bernardy"
    ]
  },
  "https://aclanthology.org/2021.sigmorphon-1.27": {
    "title": "BME Submission for SIGMORPHON 2021 Shared Task 0. A Three Step Training Approach with Data Augmentation for Morphological Inflection",
    "volume": "workshop",
    "abstract": "We present the BME submission for the SIGMORPHON 2021 Task 0 Part 1, Generalization Across Typologically Diverse Languages shared task. We use an LSTM encoder-decoder model with three step training that is first trained on all languages, then fine-tuned on each language family and finally fine-tuned on individual languages. We use a different type of data augmentation technique in the first two steps. Our system outperformed the only other submission. Although it remains worse than the Transformer baseline released by the organizers, our model is simpler and our data augmentation techniques are easily applicable to new languages. We perform ablation studies and show that the augmentation techniques and the three training steps often help but sometimes have a negative effect. Our code is publicly available",
    "checked": true,
    "id": "0271547a46c1aecdd69b6825597aa8f6d99d92a4",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Gábor Szolnok",
      "Botond Barta",
      "Dorina Lakatos",
      "Judit Ács"
    ]
  },
  "https://aclanthology.org/2021.sigmorphon-1.28": {
    "title": "Not Quite There Yet: Combining Analogical Patterns and Encoder-Decoder Networks for Cognitively Plausible Inflection",
    "volume": "workshop",
    "abstract": "The paper presents four models submitted to Part 2 of the SIGMORPHON 2021 Shared Task 0, which aims at replicating human judgements on the inflection of nonce lexemes. Our goal is to explore the usefulness of combining pre-compiled analogical patterns with an encoder-decoder architecture. Two models are designed using such patterns either in the input or the output of the network. Two extra models controlled for the role of raw similarity of nonce inflected forms to existing inflected forms in the same paradigm cell, and the role of the type frequency of analogical patterns. Our strategy is entirely endogenous in the sense that the models appealing solely to the data provided by the SIGMORPHON organisers, without using external resources. Our model 2 ranks second among all submitted systems, suggesting that the inclusion of analogical patterns in the network architecture is useful in mimicking speakers’ predictions",
    "checked": true,
    "id": "9a3da922eca7979c58faf39ba693831483a41079",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Basilio Calderone",
      "Nabil Hathout",
      "Olivier Bonami"
    ]
  },
  "https://aclanthology.org/2021.sigmorphon-1.29": {
    "title": "Were We There Already? Applying Minimal Generalization to the SIGMORPHON-UniMorph Shared Task on Cognitively Plausible Morphological Inflection",
    "volume": "workshop",
    "abstract": "Morphological rules with various levels of specificity can be learned from example lexemes by recursive application of minimal generalization (Albright and Hayes, 2002, 2003). A model that learns rules solely through minimal generalization was used to predict average human wug-test ratings from German, English, and Dutch in the SIGMORPHON-UniMorph 2021 Shared Task, with competitive results. Some formal properties of the minimal generalization operation were proved. An automatic method was developed to create wug-test stimuli for future experiments that investigate whether the model’s morphological generalizations are too minimal",
    "checked": true,
    "id": "17fab13f3b81efc8d02fde6c1d32bb847905ba7f",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Colin Wilson",
      "Jane S.Y. Li"
    ]
  },
  "https://aclanthology.org/2021.splurobonlp-1.1": {
    "title": "Symbol Grounding and Task Learning from Imperfect Corrections",
    "volume": "workshop",
    "abstract": "This paper describes a method for learning from a teacher’s potentially unreliable corrective feedback in an interactive task learning setting. The graphical model uses discourse coherence to jointly learn symbol grounding, domain concepts and valid plans. Our experiments show that the agent learns its domain-level task in spite of the teacher’s mistakes",
    "checked": true,
    "id": "1e2f966f94e464a3d99b80dd95df7cc8dc047b3c",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Mattias Appelgren",
      "Alex Lascarides"
    ]
  },
  "https://aclanthology.org/2021.splurobonlp-1.2": {
    "title": "Learning to Read Maps: Understanding Natural Language Instructions from Unseen Maps",
    "volume": "workshop",
    "abstract": "Robust situated dialog requires the ability to process instructions based on spatial information, which may or may not be available. We propose a model, based on LXMERT, that can extract spatial information from text instructions and attend to landmarks on OpenStreetMap (OSM) referred to in a natural language instruction. Whilst, OSM is a valuable resource, as with any open-sourced data, there is noise and variation in the names referred to on the map, as well as, variation in natural language instructions, hence the need for data-driven methods over rule-based systems. This paper demonstrates that the gold GPS location can be accurately predicted from the natural language instruction and metadata with 72% accuracy for previously seen maps and 64% for unseen maps",
    "checked": true,
    "id": "1cc35ca7e0b5a8b4112f957cf28a18d3539f15fc",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miltiadis Marios Katsakioris",
      "Ioannis Konstas",
      "Pierre Yves Mignotte",
      "Helen Hastie"
    ]
  },
  "https://aclanthology.org/2021.splurobonlp-1.3": {
    "title": "Visually Grounded Follow-up Questions: a Dataset of Spatial Questions Which Require Dialogue History",
    "volume": "workshop",
    "abstract": "In this paper, we define and evaluate a methodology for extracting history-dependent spatial questions from visual dialogues. We say that a question is history-dependent if it requires (parts of) its dialogue history to be interpreted. We argue that some kinds of visual questions define a context upon which a follow-up spatial question relies. We call the question that restricts the context: trigger, and we call the spatial question that requires the trigger question to be answered: zoomer. We automatically extract different trigger and zoomer pairs based on the visual property that the questions rely on (e.g. color, number). We manually annotate the automatically extracted trigger and zoomer pairs to verify which zoomers require their trigger. We implement a simple baseline architecture based on a SOTA multimodal encoder. Our results reveal that there is much room for improvement for answering history-dependent questions",
    "checked": true,
    "id": "546b8d3cb1342992a48df5c0294c6b58a3763584",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Tianai Dong",
      "Alberto Testoni",
      "Luciana Benotti",
      "Raffaella Bernardi"
    ]
  },
  "https://aclanthology.org/2021.splurobonlp-1.4": {
    "title": "Modeling Semantics and Pragmatics of Spatial Prepositions via Hierarchical Common-Sense Primitives",
    "volume": "workshop",
    "abstract": "Understanding spatial expressions and using them appropriately is necessary for seamless and natural human-machine interaction. However, capturing the semantics and appropriate usage of spatial prepositions is notoriously difficult, because of their vagueness and polysemy. Although modern data-driven approaches are good at capturing statistical regularities in the usage, they usually require substantial sample sizes, often do not generalize well to unseen instances and, most importantly, their structure is essentially opaque to analysis, which makes diagnosing problems and understanding their reasoning process difficult. In this work, we discuss our attempt at modeling spatial senses of prepositions in English using a combination of rule-based and statistical learning approaches. Each preposition model is implemented as a tree where each node computes certain intuitive relations associated with the preposition, with the root computing the final value of the prepositional relation itself. The models operate on a set of artificial 3D “room world” environments, designed in Blender, taking the scene itself as an input. We also discuss our annotation framework used to collect human judgments employed in the model training. Both our factored models and black-box baseline models perform quite well, but the factored models will enable reasoned explanations of spatial relation judgements",
    "checked": true,
    "id": "8fd0ea68741ab83aed237df9822222ee368f1d5e",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Georgiy Platonov",
      "Yifei Yang",
      "Haoyu Wu",
      "Jonathan Waxman",
      "Marcus Hill",
      "Lenhart Schubert"
    ]
  },
  "https://aclanthology.org/2021.splurobonlp-1.5": {
    "title": "Towards Navigation by Reasoning over Spatial Configurations",
    "volume": "workshop",
    "abstract": "We deal with the navigation problem where the agent follows natural language instructions while observing the environment. Focusing on language understanding, we show the importance of spatial semantics in grounding navigation instructions into visual perceptions. We propose a neural agent that uses the elements of spatial configurations and investigate their influence on the navigation agent’s reasoning ability. Moreover, we model the sequential execution order and align visual objects with spatial configurations in the instruction. Our neural agent improves strong baselines on the seen environments and shows competitive performance on the unseen environments. Additionally, the experimental results demonstrate that explicit modeling of spatial semantic elements in the instructions can improve the grounding and spatial reasoning of the model",
    "checked": true,
    "id": "198159956ed954e858b213a0d531848559412470",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Yue Zhang",
      "Quan Guo",
      "Parisa Kordjamshidi"
    ]
  },
  "https://aclanthology.org/2021.splurobonlp-1.6": {
    "title": "Error-Aware Interactive Semantic Parsing of OpenStreetMap",
    "volume": "workshop",
    "abstract": "In semantic parsing of geographical queries against real-world databases such as OpenStreetMap (OSM), unique correct answers do not necessarily exist. Instead, the truth might be lying in the eye of the user, who needs to enter an interactive setup where ambiguities can be resolved and parsing mistakes can be corrected. Our work presents an approach to interactive semantic parsing where an explicit error detection is performed, and a clarification question is generated that pinpoints the suspected source of ambiguity or error and communicates it to the human user. Our experimental results show that a combination of entropy-based uncertainty detection and beam search, together with multi-source training on clarification question, initial parse, and user answer, results in improvements of 1.2% F1 score on a parser that already performs at 90.26% on the NLMaps dataset for OSM semantic parsing",
    "checked": true,
    "id": "311f8327b09260bb07d3295da2ceded3cff9af8f",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Staniek",
      "Stefan Riezler"
    ]
  },
  "https://aclanthology.org/2021.splurobonlp-1.7": {
    "title": "Plan Explanations that Exploit a Cognitive Spatial Model",
    "volume": "workshop",
    "abstract": "Ideally, people who navigate together in a complex indoor space share a mental model that facilitates explanation. This paper reports on a robot control system whose cognitive world model is based on spatial affordances that generalize over its perceptual data. Given a target, the control system formulates multiple plans, each with a model-relevant metric, and selects among them. As a result, it can provide readily understandable natural language about the robot’s intentions and confidence, and generate diverse, contrastive explanations that reference the acquired spatial model. Empirical results in large, complex environments demonstrate the robot’s ability to provide human-friendly explanations in natural language",
    "checked": true,
    "id": "bfdbe7743fc9b6c32be560465ec7397fa5065c32",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raj Korpan",
      "Susan L. Epstein"
    ]
  },
  "https://aclanthology.org/2021.splurobonlp-1.8": {
    "title": "Interactive Reinforcement Learning for Table Balancing Robot",
    "volume": "workshop",
    "abstract": "With the development of robotics, the use of robots in daily life is increasing, which has led to the need for anyone to easily train robots to improve robot use. Interactive reinforcement learning(IARL) is a method for robot training based on human–robot interaction; prior studies on IARL provide only limited types of feedback or require appropriately designed shaping rewards, which is known to be difficult and time-consuming. Therefore, in this study, we propose interactive deep reinforcement learning models based on voice feedback. In the proposed system, a robot learns the task of cooperative table balancing through deep Q-network using voice feedback provided by humans in real-time, with automatic speech recognition(ASR) and sentiment analysis to understand human voice feedback. As a result, an optimal policy convergence rate of up to 96% was realized, and performance was improved in all voice feedback-based models",
    "checked": true,
    "id": "e71a91760c524ea2f151bffad2ba430900b407c5",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Haein Jeon",
      "Yewon Kim",
      "Bo-Yeong Kang"
    ]
  },
  "https://aclanthology.org/2021.splurobonlp-1.9": {
    "title": "Multi-Level Gazetteer-Free Geocoding",
    "volume": "workshop",
    "abstract": "We present a multi-level geocoding model (MLG) that learns to associate texts to geographic coordinates. The Earth’s surface is represented using space-filling curves that decompose the sphere into a hierarchical grid. MLG balances classification granularity and accuracy by combining losses across multiple levels and jointly predicting cells at different levels simultaneously. It obtains large gains without any gazetteer metadata, demonstrating that it can effectively learn the connection between text spans and coordinates—and thus makes it a gazetteer-free geocoder. Furthermore, MLG obtains state-of-the-art results for toponym resolution on three English datasets without any dataset-specific tuning",
    "checked": true,
    "id": "9e6c0fd5fa24ea1e77709cd76c5d53e3e8b432b2",
    "semantic_title": "",
    "citation_count": 11,
    "authors": [
      "Sayali Kulkarni",
      "Shailee Jain",
      "Mohammad Javad Hosseini",
      "Jason Baldridge",
      "Eugene Ie",
      "Li Zhang"
    ]
  },
  "https://aclanthology.org/2021.spnlp-1.1": {
    "title": "RewardsOfSum: Exploring Reinforcement Learning Rewards for Summarisation",
    "volume": "workshop",
    "abstract": "To date, most abstractive summarisation models have relied on variants of the negative log-likelihood (NLL) as their training objective. In some cases, reinforcement learning has been added to train the models with an objective that is closer to their evaluation measures (e.g. ROUGE). However, the reward function to be used within the reinforcement learning approach can play a key role for performance and is still partially unexplored. For this reason, in this paper, we propose two reward functions for the task of abstractive summarisation: the first function, referred to as RwB-Hinge, dynamically selects the samples for the gradient update. The second function, nicknamed RISK, leverages a small pool of strong candidates to inform the reward. In the experiments, we probe the proposed approach by fine-tuning an NLL pre-trained model over nine summarisation datasets of diverse size and nature. The experimental results show a consistent improvement over the negative log-likelihood baselines",
    "checked": true,
    "id": "ba6e0d493ea124f7c067e6bf0f4b0ae81581997c",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Jacob Parnell",
      "Inigo Jauregi Unanue",
      "Massimo Piccardi"
    ]
  },
  "https://aclanthology.org/2021.spnlp-1.2": {
    "title": "SmBoP: Semi-autoregressive Bottom-up Semantic Parsing",
    "volume": "workshop",
    "abstract": "The de-facto standard decoding method for semantic parsing in recent years has been to autoregressively decode the abstract syntax tree of the target program using a top-down depth-first traversal. In this work, we propose an alternative approach: a Semi-autoregressive Bottom-up Parser (SmBoP) that constructs at decoding step t the top-K sub-trees of height ≤ t. Our parser enjoys several benefits compared to top-down autoregressive parsing. From an efficiency perspective, bottom-up parsing allows to decode all sub-trees of a certain height in parallel, leading to logarithmic runtime complexity rather than linear. From a modeling perspective, a bottom-up parser learns representations for meaningful semantic sub-programs at each step, rather than for semantically-vacuous partial trees. We apply SmBoP on Spider, a challenging zero-shot semantic parsing benchmark, and show that SmBoP leads to a 2.2x speed-up in decoding time and a ~5x speed-up in training time, compared to a semantic parser that uses autoregressive decoding. SmBoP obtains 71.1 denotation accuracy on Spider, establishing a new state-of-the-art, and 69.5 exact match, comparable to the 69.6 exact match of the autoregressive RAT-SQL+Grappa",
    "checked": true,
    "id": "5868a7bfe6a4590d332ca66b8097dbe5490c8a73",
    "semantic_title": "",
    "citation_count": 85,
    "authors": [
      "Ohad Rubin",
      "Jonathan Berant"
    ]
  },
  "https://aclanthology.org/2021.spnlp-1.3": {
    "title": "Learning compositional structures for semantic graph parsing",
    "volume": "workshop",
    "abstract": "AM dependency parsing is a method for neural semantic graph parsing that exploits the principle of compositionality. While AM dependency parsers have been shown to be fast and accurate across several graphbanks, they require explicit annotations of the compositional tree structures for training. In the past, these were obtained using complex graphbank-specific heuristics written by experts. Here we show how they can instead be trained directly on the graphs with a neural latent-variable model, drastically reducing the amount and complexity of manual heuristics. We demonstrate that our model picks up on several linguistic phenomena on its own and achieves comparable accuracy to supervised training, greatly facilitating the use of AM dependency parsing for new sembanks",
    "checked": true,
    "id": "d34cac6a7101068f6ba6b9e08d169340b2589595",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Jonas Groschwitz",
      "Meaghan Fowlie",
      "Alexander Koller"
    ]
  },
  "https://aclanthology.org/2021.spnlp-1.4": {
    "title": "Offline Reinforcement Learning from Human Feedback in Real-World Sequence-to-Sequence Tasks",
    "volume": "workshop",
    "abstract": "Large volumes of interaction logs can be collected from NLP systems that are deployed in the real world. How can this wealth of information be leveraged? Using such interaction logs in an offline reinforcement learning (RL) setting is a promising approach. However, due to the nature of NLP tasks and the constraints of production systems, a series of challenges arise. We present a concise overview of these challenges and discuss possible solutions",
    "checked": true,
    "id": "7c6965b1b3d4943368242914e0df0288e84dd232",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Julia Kreutzer",
      "Stefan Riezler",
      "Carolin Lawrence"
    ]
  },
  "https://aclanthology.org/2021.spnlp-1.5": {
    "title": "Mode recovery in neural autoregressive sequence modeling",
    "volume": "workshop",
    "abstract": "Despite its wide use, recent studies have revealed unexpected and undesirable properties of neural autoregressive sequence models trained with maximum likelihood, such as an unreasonably high affinity to short sequences after training and to infinitely long sequences at decoding time. We propose to study these phenomena by investigating how the modes, or local maxima, of a distribution are maintained throughout the full learning chain of the ground-truth, empirical, learned and decoding-induced distributions, via the newly proposed mode recovery cost. We design a tractable testbed where we build three types of ground-truth distributions: (1) an LSTM based structured distribution, (2) an unstructured distribution where probability of a sequence does not depend on its content, and (3) a product of these two which we call a semi-structured distribution. Our study reveals both expected and unexpected findings. First, starting with data collection, mode recovery cost strongly relies on the ground-truth distribution and is most costly with the semi-structured distribution. Second, after learning, mode recovery cost from the ground-truth distribution may increase or decrease compared to data collection, with the largest cost degradation occurring with the semi-structured ground-truth distribution. Finally, the ability of the decoding-induced distribution to recover modes from the learned distribution is highly impacted by the choices made earlier in the learning chain. We conclude that future research must consider the entire learning chain in order to fully understand the potentials and perils and to further improve neural autoregressive sequence models",
    "checked": true,
    "id": "1391d9097465eef1db8ba49960a5048033e57b84",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Ilia Kulikov",
      "Sean Welleck",
      "Kyunghyun Cho"
    ]
  },
  "https://aclanthology.org/2021.spnlp-1.6": {
    "title": "Using Hierarchical Class Structure to Improve Fine-Grained Claim Classification",
    "volume": "workshop",
    "abstract": "The analysis of public debates crucially requires the classification of political demands according to hierarchical claim ontologies (e.g. for immigration, a supercategory “Controlling Migration” might have subcategories “Asylum limit” or “Border installations”). A major challenge for automatic claim classification is the large number and low frequency of such subclasses. We address it by jointly predicting pairs of matching super- and subcategories. We operationalize this idea by (a) encoding soft constraints in the claim classifier and (b) imposing hard constraints via Integer Linear Programming. Our experiments with different claim classifiers on a German immigration newspaper corpus show consistent performance increases for joint prediction, in particular for infrequent categories and discuss the complementarity of the two approaches",
    "checked": true,
    "id": "25a73340df69440ec6be54d60270c6d8b1c087cd",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Erenay Dayanik",
      "Andre Blessing",
      "Nico Blokker",
      "Sebastian Haunss",
      "Jonas Kuhn",
      "Gabriella Lapesa",
      "Sebastian Padó"
    ]
  },
  "https://aclanthology.org/2021.spnlp-1.7": {
    "title": "A Globally Normalized Neural Model for Semantic Parsing",
    "volume": "workshop",
    "abstract": "In this paper, we propose a globally normalized model for context-free grammar (CFG)-based semantic parsing. Instead of predicting a probability, our model predicts a real-valued score at each step and does not suffer from the label bias problem. Experiments show that our approach outperforms locally normalized models on small datasets, but it does not yield improvement on a large dataset",
    "checked": true,
    "id": "48db3e5425e1582f5659d98154fc8406cea0dc54",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Chenyang Huang",
      "Wei Yang",
      "Yanshuai Cao",
      "Osmar Zaïane",
      "Lili Mou"
    ]
  },
  "https://aclanthology.org/2021.spnlp-1.8": {
    "title": "Comparing Span Extraction Methods for Semantic Role Labeling",
    "volume": "workshop",
    "abstract": "In this work, we empirically compare span extraction methods for the task of semantic role labeling (SRL). While recent progress incorporating pre-trained contextualized representations into neural encoders has greatly improved SRL F1 performance on popular benchmarks, the potential costs and benefits of structured decoding in these models have become less clear. With extensive experiments on PropBank SRL datasets, we find that more structured decoding methods outperform BIO-tagging when using static (word type) embeddings across all experimental settings. However, when used in conjunction with pre-trained contextualized word representations, the benefits are diminished. We also experiment in cross-genre and cross-lingual settings and find similar trends. We further perform speed comparisons and provide analysis on the accuracy-efficiency trade-offs among different decoding methods",
    "checked": true,
    "id": "2d9ec5a86c0525da54ce8dee7b9ac38718567e03",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Zhisong Zhang",
      "Emma Strubell",
      "Eduard Hovy"
    ]
  },
  "https://aclanthology.org/2021.starsem-1.1": {
    "title": "Did the Cat Drink the Coffee? Challenging Transformers with Generalized Event Knowledge",
    "volume": "workshop",
    "abstract": "Prior research has explored the ability of computational models to predict a word semantic fit with a given predicate. While much work has been devoted to modeling the typicality relation between verbs and arguments in isolation, in this paper we take a broader perspective by assessing whether and to what extent computational approaches have access to the information about the typicality of entire events and situations described in language (Generalized Event Knowledge). Given the recent success of Transformers Language Models (TLMs), we decided to test them on a benchmark for the dynamic estimation of thematic fit. The evaluation of these models was performed in comparison with SDM, a framework specifically designed to integrate events in sentence meaning representations, and we conducted a detailed error analysis to investigate which factors affect their behavior. Our results show that TLMs can reach performances that are comparable to those achieved by SDM. However, additional analysis consistently suggests that TLMs do not capture important aspects of event knowledge, and their predictions often depend on surface linguistic features, such as frequent words, collocations and syntactic patterns, thereby showing sub-optimal generalization abilities",
    "checked": true,
    "id": "47cc6e97d71317052672e82e42154707b4485b49",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Paolo Pedinotti",
      "Giulia Rambelli",
      "Emmanuele Chersoni",
      "Enrico Santus",
      "Alessandro Lenci",
      "Philippe Blache"
    ]
  },
  "https://aclanthology.org/2021.starsem-1.2": {
    "title": "Can Transformer Language Models Predict Psychometric Properties?",
    "volume": "workshop",
    "abstract": "Transformer-based language models (LMs) continue to advance state-of-the-art performance on NLP benchmark tasks, including tasks designed to mimic human-inspired “commonsense” competencies. To better understand the degree to which LMs can be said to have certain linguistic reasoning skills, researchers are beginning to adapt the tools and concepts of the field of psychometrics. But to what extent can the benefits flow in the other direction? I.e., can LMs be of use in predicting what the psychometric properties of test items will be when those items are given to human participants? We gather responses from numerous human participants and LMs (transformer- and non-transformer-based) on a broad diagnostic test of linguistic competencies. We then use the responses to calculate standard psychometric properties of the items in the diagnostic test, using the human responses and the LM responses separately. We then determine how well these two sets of predictions match. We find cases in which transformer-based LMs predict psychometric properties consistently well in certain categories but consistently poorly in others, thus providing new insights into fundamental similarities and differences between human and LM reasoning",
    "checked": true,
    "id": "3177b63f5d6ce3aa2e172eb5c94412fdaf3be5fd",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Antonio Laverghetta Jr.",
      "Animesh Nighojkar",
      "Jamshidbek Mirzakhalov",
      "John Licato"
    ]
  },
  "https://aclanthology.org/2021.starsem-1.3": {
    "title": "Semantic shift in social networks",
    "volume": "workshop",
    "abstract": "Just as the meaning of words is tied to the communities in which they are used, so too is semantic change. But how does lexical semantic change manifest differently across different communities? In this work, we investigate the relationship between community structure and semantic change in 45 communities from the social media website Reddit. We use distributional methods to quantify lexical semantic change and induce a social network on communities, based on interactions between members. We explore the relationship between semantic change and the clustering coefficient of a community’s social network graph, as well as community size and stability. While none of these factors are found to be significant on their own, we report a significant effect of their three-way interaction. We also report on significant word-level effects of frequency and change in frequency, which replicate previous findings",
    "checked": true,
    "id": "007331a5d4db8572fbd4c74ab7a0ffca8039a98b",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Bill Noble",
      "Asad Sayeed",
      "Raquel Fernández",
      "Staffan Larsson"
    ]
  },
  "https://aclanthology.org/2021.starsem-1.4": {
    "title": "A Study on Using Semantic Word Associations to Predict the Success of a Novel",
    "volume": "workshop",
    "abstract": "Many new books get published every year, and only a fraction of them become popular among the readers. So the prediction of a book success can be a very useful parameter for publishers to make a reliable decision. This article presents the study of semantic word associations using the word embedding of book content for a set of Roget’s thesaurus concepts for book success prediction. In this work, we discuss the method to represent a book as a spectrum of concepts based on the association score between its content embedding and a global embedding (i.e. fastText) for a set of semantically linked word clusters. We show that the semantic word associations outperform the previous methods for book success prediction. In addition, we present that semantic word associations also provide better results than using features like the frequency of word groups in Roget’s thesaurus, LIWC (a popular tool for linguistic inquiry and word count), NRC (word association emotion lexicon), and part of speech (PoS). Our study reports that concept associations based on Roget’s Thesaurus using word embedding of individual novel resulted in the state-of-the-art performance of 0.89 average weighted F1-score for book success prediction. Finally, we present a set of dominant themes that contribute towards the popularity of a book for a specific genre",
    "checked": true,
    "id": "e550bdb9fb55f42147bc0c995834d553e7955fe3",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Syeda Jannatus Saba",
      "Biddut Sarker Bijoy",
      "Henry Gorelick",
      "Sabir Ismail",
      "Md Saiful Islam",
      "Mohammad Ruhul Amin"
    ]
  },
  "https://aclanthology.org/2021.starsem-1.5": {
    "title": "Recovering Lexically and Semantically Reused Texts",
    "volume": "workshop",
    "abstract": "Writers often repurpose material from existing texts when composing new documents. Because most documents have more than one source, we cannot trace these connections using only models of document-level similarity. Instead, this paper considers methods for local text reuse detection (LTRD), detecting localized regions of lexically or semantically similar text embedded in otherwise unrelated material. In extensive experiments, we study the relative performance of four classes of neural and bag-of-words models on three LTRD tasks – detecting plagiarism, modeling journalists’ use of press releases, and identifying scientists’ citation of earlier papers. We conduct evaluations on three existing datasets and a new, publicly-available citation localization dataset. Our findings shed light on a number of previously-unexplored questions in the study of LTRD, including the importance of incorporating document-level context for predictions, the applicability of of-the-shelf neural models pretrained on “general” semantic textual similarity tasks such as paraphrase detection, and the trade-offs between more efficient bag-of-words and feature-based neural models and slower pairwise neural models",
    "checked": true,
    "id": "3b782fcca6b1217cb1bb4bdee19eb148d49b513a",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ansel MacLaughlin",
      "Shaobin Xu",
      "David A. Smith"
    ]
  },
  "https://aclanthology.org/2021.starsem-1.6": {
    "title": "Generating Hypothetical Events for Abductive Inference",
    "volume": "workshop",
    "abstract": "Abductive reasoning starts from some observations and aims at finding the most plausible explanation for these observations. To perform abduction, humans often make use of temporal and causal inferences, and knowledge about how some hypothetical situation can result in different outcomes. This work offers the first study of how such knowledge impacts the Abductive NLI task – which consists in choosing the more likely explanation for given observations. We train a specialized language model LMI that is tasked to generate what could happen next from a hypothetical scenario that evolves from a given event. We then propose a multi-task model MTL to solve the Abductive NLI task, which predicts a plausible explanation by a) considering different possible events emerging from candidate hypotheses – events generated by LMI – and b) selecting the one that is most similar to the observed outcome. We show that our MTL model improves over prior vanilla pre-trained LMs fine-tuned on Abductive NLI. Our manual evaluation and analysis suggest that learning about possible next events from different hypothetical scenarios supports abductive inference",
    "checked": true,
    "id": "c11618293ebc4215b489d1353685cbfbc9871be3",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Debjit Paul",
      "Anette Frank"
    ]
  },
  "https://aclanthology.org/2021.starsem-1.7": {
    "title": "NeuralLog: Natural Language Inference with Joint Neural and Logical Reasoning",
    "volume": "workshop",
    "abstract": "Deep learning (DL) based language models achieve high performance on various benchmarks for Natural Language Inference (NLI). And at this time, symbolic approaches to NLI are receiving less attention. Both approaches (symbolic and DL) have their advantages and weaknesses. However, currently, no method combines them in a system to solve the task of NLI. To merge symbolic and deep learning methods, we propose an inference framework called NeuralLog, which utilizes both a monotonicity-based logical inference engine and a neural network language model for phrase alignment. Our framework models the NLI task as a classic search problem and uses the beam search algorithm to search for optimal inference paths. Experiments show that our joint logic and neural inference system improves accuracy on the NLI task and can achieve state-of-art accuracy on the SICK and MED datasets",
    "checked": true,
    "id": "3196d43fb5871096b8c5dc8fc84040a119238511",
    "semantic_title": "",
    "citation_count": 19,
    "authors": [
      "Zeming Chen",
      "Qiyue Gao",
      "Lawrence S. Moss"
    ]
  },
  "https://aclanthology.org/2021.starsem-1.8": {
    "title": "Teach the Rules, Provide the Facts: Targeted Relational-knowledge Enhancement for Textual Inference",
    "volume": "workshop",
    "abstract": "We present InferBert, a method to enhance transformer-based inference models with relevant relational knowledge. Our approach facilitates learning generic inference patterns requiring relational knowledge (e.g. inferences related to hypernymy) during training, while injecting on-demand the relevant relational facts (e.g. pangolin is an animal) at test time. We apply InferBERT to the NLI task over a diverse set of inference types (hypernymy, location, color, and country of origin), for which we collected challenge datasets. In this setting, InferBert succeeds to learn general inference patterns, from a relatively small number of training instances, while not hurting performance on the original NLI data and substantially outperforming prior knowledge enhancement models on the challenge data. It further applies its inferences successfully at test time to previously unobserved entities. InferBert is computationally more efficient than most prior methods, in terms of number of parameters, memory consumption and training time",
    "checked": true,
    "id": "e88cc25f427c73c52b70c421d0d1e0e6f530e28d",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Ohad Rozen",
      "Shmuel Amar",
      "Vered Shwartz",
      "Ido Dagan"
    ]
  },
  "https://aclanthology.org/2021.starsem-1.9": {
    "title": "ParsFEVER: a Dataset for Farsi Fact Extraction and Verification",
    "volume": "workshop",
    "abstract": "Training and evaluation of automatic fact extraction and verification techniques require large amounts of annotated data which might not be available for low-resource languages. This paper presents ParsFEVER: the first publicly available Farsi dataset for fact extraction and verification. We adopt the construction procedure of the standard English dataset for the task, i.e., FEVER, and improve it for the case of low-resource languages. Specifically, claims are extracted from sentences that are carefully selected to be more informative. The dataset comprises nearly 23K manually-annotated claims. Over 65% of the claims in ParsFEVER are many-hop (require evidence from multiple sources), making the dataset a challenging benchmark (only 13% of the claims in FEVER are many-hop). Also, despite having a smaller training set (around one-ninth of that in Fever), a model trained on ParsFEVER attains similar downstream performance, indicating the quality of the dataset. We release the dataset and the annotation guidelines at https://github.com/Zarharan/ParsFEVER",
    "checked": true,
    "id": "6c76ebb7151cb69e640082b9ce97d2a0c3793f16",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Majid Zarharan",
      "Mahsa Ghaderan",
      "Amin Pourdabiri",
      "Zahra Sayedi",
      "Behrouz Minaei-Bidgoli",
      "Sauleh Eetemadi",
      "Mohammad Taher Pilehvar"
    ]
  },
  "https://aclanthology.org/2021.starsem-1.10": {
    "title": "BiQuAD: Towards QA based on deeper text understanding",
    "volume": "workshop",
    "abstract": "Recent question answering and machine reading benchmarks frequently reduce the task to one of pinpointing spans within a certain text passage that answers the given question. Typically, these systems are not required to actually understand the text on a deeper level that allows for more complex reasoning on the information contained. We introduce a new dataset called BiQuAD that requires deeper comprehension in order to answer questions in both extractive and deductive fashion. The dataset consist of 4,190 closed-domain texts and a total of 99,149 question-answer pairs. The texts are synthetically generated soccer match reports that verbalize the main events of each match. All texts are accompanied by a structured Datalog program that represents a (logical) model of its information. We show that state-of-the-art QA models do not perform well on the challenging long form contexts and reasoning requirements posed by the dataset. In particular, transformer based state-of-the-art models achieve F1-scores of only 39.0. We demonstrate how these synthetic datasets align structured knowledge with natural text and aid model introspection when approaching complex text understanding",
    "checked": true,
    "id": "5aab0e57b4a7d6d47dfbb34cafce8e728ff47102",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Frank Grimm",
      "Philipp Cimiano"
    ]
  },
  "https://aclanthology.org/2021.starsem-1.11": {
    "title": "Evaluating Universal Dependency Parser Recovery of Predicate Argument Structure via CompChain Analysis",
    "volume": "workshop",
    "abstract": "Accurate recovery of predicate-argument structure from a Universal Dependency (UD) parse is central to downstream tasks such as extraction of semantic roles or event representations. This study introduces compchains, a categorization of the hierarchy of predicate dependency relations present within a UD parse. Accuracy of compchain classification serves as a proxy for measuring accurate recovery of predicate-argument structure from sentences with embedding. We analyzed the distribution of compchains in three UD English treebanks, EWT, GUM and LinES, revealing that these treebanks are sparse with respect to sentences with predicate-argument structure that includes predicate-argument embedding. We evaluated the CoNLL 2018 Shared Task UDPipe (v1.2) baseline (dependency parsing) models as compchain classifiers for the EWT, GUMS and LinES UD treebanks. Our results indicate that these three baseline models exhibit poorer performance on sentences with predicate-argument structure with more than one level of embedding; we used compchains to characterize the errors made by these parsers and present examples of erroneous parses produced by the parser that were identified using compchains. We also analyzed the distribution of compchains in 58 non-English UD treebanks and then used compchains to evaluate the CoNLL’18 Shared Task baseline model for each of these treebanks. Our analysis shows that performance with respect to compchain classification is only weakly correlated with the official evaluation metrics (LAS, MLAS and BLEX). We identify gaps in the distribution of compchains in several of the UD treebanks, thus providing a roadmap for how these treebanks may be supplemented. We conclude by discussing how compchains provide a new perspective on the sparsity of training data for UD parsers, as well as the accuracy of the resulting UD parses",
    "checked": true,
    "id": "68ba7e0b91d7bd25efe3e7c06aca3e260bb3f891",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Sagar Indurkhya",
      "Beracah Yankama",
      "Robert C. Berwick"
    ]
  },
  "https://aclanthology.org/2021.starsem-1.12": {
    "title": "InFillmore: Frame-Guided Language Generation with Bidirectional Context",
    "volume": "workshop",
    "abstract": "We propose a structured extension to bidirectional-context conditional language generation, or “infilling,” inspired by Frame Semantic theory. Guidance is provided through one of two approaches: (1) model fine-tuning, conditioning directly on observed symbolic frames, and (2) a novel extension to disjunctive lexically constrained decoding that leverages frame semantic lexical units. Automatic and human evaluations confirm that frame-guided generation allows for explicit manipulation of intended infill semantics, with minimal loss in distinguishability from human-generated text. Our methods flexibly apply to a variety of use scenarios, and we provide an interactive web demo",
    "checked": true,
    "id": "4cba8ce640dee7df666e287d4621c997f1dee54b",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Jiefu Ou",
      "Nathaniel Weir",
      "Anton Belyy",
      "Felix Yu",
      "Benjamin Van Durme"
    ]
  },
  "https://aclanthology.org/2021.starsem-1.13": {
    "title": "Realistic Evaluation Principles for Cross-document Coreference Resolution",
    "volume": "workshop",
    "abstract": "We point out that common evaluation practices for cross-document coreference resolution have been unrealistically permissive in their assumed settings, yielding inflated results. We propose addressing this issue via two evaluation methodology principles. First, as in other tasks, models should be evaluated on predicted mentions rather than on gold mentions. Doing this raises a subtle issue regarding singleton coreference clusters, which we address by decoupling the evaluation of mention detection from that of coreference linking. Second, we argue that models should not exploit the synthetic topic structure of the standard ECB+ dataset, forcing models to confront the lexical ambiguity challenge, as intended by the dataset creators. We demonstrate empirically the drastic impact of our more realistic evaluation principles on a competitive model, yielding a score which is 33 F1 lower compared to evaluating by prior lenient practices",
    "checked": true,
    "id": "70d42b1e9ca8eb8d5afe6aa0ee08651e7979aca3",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Arie Cattan",
      "Alon Eirew",
      "Gabriel Stanovsky",
      "Mandar Joshi",
      "Ido Dagan"
    ]
  },
  "https://aclanthology.org/2021.starsem-1.14": {
    "title": "Disentangling Online Chats with DAG-structured LSTMs",
    "volume": "workshop",
    "abstract": "Many modern messaging systems allow fast and synchronous textual communication among many users. The resulting sequence of messages hides a more complicated structure in which independent sub-conversations are interwoven with one another. This poses a challenge for any task aiming to understand the content of the chat logs or gather information from them. The ability to disentangle these conversations is then tantamount to the success of many downstream tasks such as summarization and question answering. Structured information accompanying the text such as user turn, user mentions, timestamps, is used as a cue by the participants themselves who need to follow the conversation and has been shown to be important for disentanglement. DAG-LSTMs, a generalization of Tree-LSTMs that can handle directed acyclic dependencies, are a natural way to incorporate such information and its non-sequential nature. In this paper, we apply DAG-LSTMs to the conversation disentanglement task. We perform our experiments on the Ubuntu IRC dataset. We show that the novel model we propose achieves state of the art status on the task of recovering reply-to relations and it is competitive on other disentanglement metrics",
    "checked": true,
    "id": "ba3b7b92ec7046d44da7b5207d41e2f07697fd5a",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Duccio Pappadopulo",
      "Lisa Bauer",
      "Marco Farina",
      "Ozan İrsoy",
      "Mohit Bansal"
    ]
  },
  "https://aclanthology.org/2021.starsem-1.15": {
    "title": "Toward Diverse Precondition Generation",
    "volume": "workshop",
    "abstract": "A typical goal for language understanding is to logically connect the events of a discourse, but often connective events are not described due to their commonsense nature. In order to address this deficit, we focus here on generating precondition events. Precondition generation can be framed as a sequence-to-sequence problem: given a target event, generate a possible precondition. However, in most real-world scenarios, an event can have several preconditions, which is not always suitable for standard seq2seq frameworks. We propose DiP, the Diverse Precondition generation system that can generate unique and diverse preconditions. DiP consists of three stages of the generative process – an event sampler, a candidate generator, and a post-processor. The event sampler provides control codes (precondition triggers) which the candidate generator uses to focus its generation. Post-processing further improves the results through re-ranking and filtering. Unlike other conditional generation systems, DiP automatically generates control codes without training on diverse examples. Analysis reveals that DiP improves the diversity of preconditions significantly compared to a beam search baseline. Also, manual evaluation shows that DiP generates more preconditions than a strong nucleus sampling baseline",
    "checked": true,
    "id": "39b8c3a864b00e42921ec10b043f54aae5da1cd8",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Heeyoung Kwon",
      "Nathanael Chambers",
      "Niranjan Balasubramanian"
    ]
  },
  "https://aclanthology.org/2021.starsem-1.16": {
    "title": "One Semantic Parser to Parse Them All: Sequence to Sequence Multi-Task Learning on Semantic Parsing Datasets",
    "volume": "workshop",
    "abstract": "Semantic parsers map natural language utterances to meaning representations. The lack of a single standard for meaning representations led to the creation of a plethora of semantic parsing datasets. To unify different datasets and train a single model for them, we investigate the use of Multi-Task Learning (MTL) architectures. We experiment with five datasets (Geoquery, NLMaps, TOP, Overnight, AMR). We find that an MTL architecture that shares the entire network across datasets yields competitive or better parsing accuracies than the single-task baselines, while reducing the total number of parameters by 68%. We further provide evidence that MTL has also better compositional generalization than single-task models. We also present a comparison of task sampling methods and propose a competitive alternative to widespread proportional sampling strategies",
    "checked": true,
    "id": "3b8c8bbf2c1a69f2bf60b2434f2d1996aea2e740",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Marco Damonte",
      "Emilio Monti"
    ]
  },
  "https://aclanthology.org/2021.starsem-1.17": {
    "title": "Multilingual Neural Semantic Parsing for Low-Resourced Languages",
    "volume": "workshop",
    "abstract": "Multilingual semantic parsing is a cost-effective method that allows a single model to understand different languages. However, researchers face a great imbalance of availability of training data, with English being resource rich, and other languages having much less data. To tackle the data limitation problem, we propose using machine translation to bootstrap multilingual training data from the more abundant English data. To compensate for the data quality of machine translated training data, we utilize transfer learning from pretrained multilingual encoders to further improve the model. To evaluate our multilingual models on human-written sentences as opposed to machine translated ones, we introduce a new multilingual semantic parsing dataset in English, Italian and Japanese based on the Facebook Task Oriented Parsing (TOP) dataset. We show that joint multilingual training with pretrained encoders substantially outperforms our baselines on the TOP dataset and outperforms the state-of-the-art model on the public NLMaps dataset. We also establish a new baseline for zero-shot learning on the TOP dataset. We find that a semantic parser trained only on English data achieves a zero-shot performance of 44.9% exact-match accuracy on Italian sentences",
    "checked": true,
    "id": "6a336ef79ea345a444ec79bbb2c12b32b74304e6",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Menglin Xia",
      "Emilio Monti"
    ]
  },
  "https://aclanthology.org/2021.starsem-1.18": {
    "title": "Script Parsing with Hierarchical Sequence Modelling",
    "volume": "workshop",
    "abstract": "Scripts capture commonsense knowledge about everyday activities and their participants. Script knowledge proved useful in a number of NLP tasks, such as referent prediction, discourse classification, and story generation. A crucial step for the exploitation of script knowledge is script parsing, the task of tagging a text with the events and participants from a certain activity. This task is challenging: it requires information both about the ways events and participants are usually uttered in surface language as well as the order in which they occur in the world. We show how to do accurate script parsing with a hierarchical sequence model and transfer learning. Our model improves the state of the art of event parsing by over 16 points F-score and, for the first time, accurately tags script participants",
    "checked": true,
    "id": "8c16791c92a5cc82db9632f0c84a014314efcb1e",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Fangzhou Zhai",
      "Iza Škrjanec",
      "Alexander Koller"
    ]
  },
  "https://aclanthology.org/2021.starsem-1.19": {
    "title": "Incorporating EDS Graph for AMR Parsing",
    "volume": "workshop",
    "abstract": "AMR (Abstract Meaning Representation) and EDS (Elementary Dependency Structures) are two popular meaning representations in NLP/NLU. AMR is more abstract and conceptual, while EDS is more low level, closer to the lexical structures of the given sentences. It is thus not surprising that EDS parsing is easier than AMR parsing. In this work, we consider using information from EDS parsing to help improve the performance of AMR parsing. We adopt a transition-based parser and propose to add EDS graphs as additional semantic features using a graph encoder composed of LSTM layer and GCN layer. Our experimental results show that the additional information from EDS parsing indeed gives a boost to the performance of the base AMR parser used in our experiments",
    "checked": true,
    "id": "9fdcd654b7971a25637380f58ac687db89d5be97",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Ziyi Shou",
      "Fangzhen Lin"
    ]
  },
  "https://aclanthology.org/2021.starsem-1.20": {
    "title": "Dependency Patterns of Complex Sentences and Semantic Disambiguation for Abstract Meaning Representation Parsing",
    "volume": "workshop",
    "abstract": "Abstract Meaning Representation (AMR) is a sentence-level meaning representation based on predicate argument structure. One of the challenges we find in AMR parsing is to capture the structure of complex sentences which expresses the relation between predicates. Knowing the core part of the sentence structure in advance may be beneficial in such a task. In this paper, we present a list of dependency patterns for English complex sentence constructions designed for AMR parsing. With a dedicated pattern matcher, all occurrences of complex sentence constructions are retrieved from an input sentence. While some of the subordinators have semantic ambiguities, we deal with this problem through training classification models on data derived from AMR and Wikipedia corpus, establishing a new baseline for future works. The developed complex sentence patterns and the corresponding AMR descriptions will be made public",
    "checked": true,
    "id": "ebb9c425827e59a3404ebf9ca072cb255ab75566",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Yuki Yamamoto",
      "Yuji Matsumoto",
      "Taro Watanabe"
    ]
  },
  "https://aclanthology.org/2021.starsem-1.21": {
    "title": "Neural Metaphor Detection with Visibility Embeddings",
    "volume": "workshop",
    "abstract": "We present new results for the problem of sequence metaphor labeling, using the recently developed Visibility Embeddings. We show that concatenating such embeddings to the input of a BiLSTM obtains consistent and significant improvements at almost no cost, and we present further improved results when visibility embeddings are combined with BERT",
    "checked": true,
    "id": "a4273f7d0f462548cc1345931cc0e17288fdb52f",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Gitit Kehat",
      "James Pustejovsky"
    ]
  },
  "https://aclanthology.org/2021.starsem-1.22": {
    "title": "Inducing Language-Agnostic Multilingual Representations",
    "volume": "workshop",
    "abstract": "Cross-lingual representations have the potential to make NLP techniques available to the vast majority of languages in the world. However, they currently require large pretraining corpora or access to typologically similar languages. In this work, we address these obstacles by removing language identity signals from multilingual embeddings. We examine three approaches for this: (i) re-aligning the vector spaces of target languages (all together) to a pivot source language; (ii) removing language-specific means and variances, which yields better discriminativeness of embeddings as a by-product; and (iii) increasing input similarity across languages by removing morphological contractions and sentence reordering. We evaluate on XNLI and reference-free MT evaluation across 19 typologically diverse languages. Our findings expose the limitations of these approaches—unlike vector normalization, vector space re-alignment and text normalization do not achieve consistent gains across encoders and languages. Due to the approaches’ additive effects, their combination decreases the cross-lingual transfer gap by 8.9 points (m-BERT) and 18.2 points (XLM-R) on average across all tasks and languages, however",
    "checked": true,
    "id": "27ef20774bde3d529df93468823e3c09e79f8294",
    "semantic_title": "",
    "citation_count": 31,
    "authors": [
      "Wei Zhao",
      "Steffen Eger",
      "Johannes Bjerva",
      "Isabelle Augenstein"
    ]
  },
  "https://aclanthology.org/2021.starsem-1.23": {
    "title": "Modeling Sense Structure in Word Usage Graphs with the Weighted Stochastic Block Model",
    "volume": "workshop",
    "abstract": "We suggest to model human-annotated Word Usage Graphs capturing fine-grained semantic proximity distinctions between word uses with a Bayesian formulation of the Weighted Stochastic Block Model, a generative model for random graphs popular in biology, physics and social sciences. By providing a probabilistic model of graded word meaning we aim to approach the slippery and yet widely used notion of word sense in a novel way. The proposed framework enables us to rigorously compare models of word senses with respect to their fit to the data. We perform extensive experiments and select the empirically most adequate model",
    "checked": true,
    "id": "19dfd0a4d1456afb5f984f52ba2bd0590f3e2b34",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Dominik Schlechtweg",
      "Enrique Castaneda",
      "Jonas Kuhn",
      "Sabine Schulte im Walde"
    ]
  },
  "https://aclanthology.org/2021.starsem-1.24": {
    "title": "Compound or Term Features? Analyzing Salience in Predicting the Difficulty of German Noun Compounds across Domains",
    "volume": "workshop",
    "abstract": "Predicting the difficulty of domain-specific vocabulary is an important task towards a better understanding of a domain, and to enhance the communication between lay people and experts. We investigate German closed noun compounds and focus on the interaction of compound-based lexical features (such as frequency and productivity) and terminology-based features (contrasting domain-specific and general language) across word representations and classifiers. Our prediction experiments complement insights from classification using (a) manually designed features to characterise termhood and compound formation and (b) compound and constituent word embeddings. We find that for a broad binary distinction into ‘easy’ vs. ‘difficult’ general-language compound frequency is sufficient, but for a more fine-grained four-class distinction it is crucial to include contrastive termhood features and compound and constituent features",
    "checked": true,
    "id": "be8fa9b67bdb2c3c41087f0b811bf33db9949519",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Anna Hätty",
      "Julia Bettinger",
      "Michael Dorna",
      "Jonas Kuhn",
      "Sabine Schulte im Walde"
    ]
  },
  "https://aclanthology.org/2021.starsem-1.25": {
    "title": "Spurious Correlations in Cross-Topic Argument Mining",
    "volume": "workshop",
    "abstract": "Recent work in cross-topic argument mining attempts to learn models that generalise across topics rather than merely relying on within-topic spurious correlations. We examine the effectiveness of this approach by analysing the output of single-task and multi-task models for cross-topic argument mining, through a combination of linear approximations of their decision boundaries, manual feature grouping, challenge examples, and ablations across the input vocabulary. Surprisingly, we show that cross-topic models still rely mostly on spurious correlations and only generalise within closely related topics, e.g., a model trained only on closed-class words and a few common open-class words outperforms a state-of-the-art cross-topic model on distant target topics",
    "checked": true,
    "id": "e3737026564c8b64a79a6ee95f6bbb4a6eebd145",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Terne Sasha Thorn Jakobsen",
      "Maria Barrett",
      "Anders Søgaard"
    ]
  },
  "https://aclanthology.org/2021.starsem-1.26": {
    "title": "Learning Embeddings for Rare Words Leveraging Internet Search Engine and Spatial Location Relationships",
    "volume": "workshop",
    "abstract": "Word embedding techniques depend heavily on the frequencies of words in the corpus, and are negatively impacted by failures in providing reliable representations for low-frequency words or unseen words during training. To address this problem, we propose an algorithm to learn embeddings for rare words based on an Internet search engine and the spatial location relationships. Our algorithm proceeds in two steps. We firstly retrieve webpages corresponding to the rare word through the search engine and parse the returned results to extract a set of most related words. We average the vectors of the related words as the initial vector of the rare word. Then, the location of the rare word in the vector space is iteratively fine-tuned according to the order of its relevances to the related words. Compared to other approaches, our algorithm can learn more accurate representations for a wider range of vocabulary. We evaluate our learned rare-word embeddings on the word relatedness task, and the experimental results show that our algorithm achieves state-of-the-art performance",
    "checked": true,
    "id": "d7b6c071af793f901f8397ae34caccf3903c92a0",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaotao Li",
      "Shujuan You",
      "Yawen Niu",
      "Wai Chen"
    ]
  },
  "https://aclanthology.org/2021.starsem-1.27": {
    "title": "Overcoming Poor Word Embeddings with Word Definitions",
    "volume": "workshop",
    "abstract": "Modern natural language understanding models depend on pretrained subword embeddings, but applications may need to reason about words that were never or rarely seen during pretraining. We show that examples that depend critically on a rarer word are more challenging for natural language inference models. Then we explore how a model could learn to use definitions, provided in natural text, to overcome this handicap. Our model’s understanding of a definition is usually weaker than a well-modeled word embedding, but it recovers most of the performance gap from using a completely untrained word",
    "checked": true,
    "id": "e0c4a2116ec0f3dc48457627bbd8bfe1f0026479",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Christopher Malon"
    ]
  },
  "https://aclanthology.org/2021.starsem-1.28": {
    "title": "Denoising Word Embeddings by Averaging in a Shared Space",
    "volume": "workshop",
    "abstract": "We introduce a new approach for smoothing and improving the quality of word embeddings. We consider a method of fusing word embeddings that were trained on the same corpus but with different initializations. We project all the models to a shared vector space using an efficient implementation of the Generalized Procrustes Analysis (GPA) procedure, previously used in multilingual word translation. Our word representation demonstrates consistent improvements over the raw models as well as their simplistic average, on a range of tasks. As the new representations are more stable and reliable, there is a noticeable improvement in rare word evaluations",
    "checked": true,
    "id": "4e6a6c5ae0399739c1df6d4520c3d014c73f6ae8",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Avi Caciularu",
      "Ido Dagan",
      "Jacob Goldberger"
    ]
  },
  "https://aclanthology.org/2021.starsem-1.29": {
    "title": "Evaluating a Joint Training Approach for Learning Cross-lingual Embeddings with Sub-word Information without Parallel Corpora on Lower-resource Languages",
    "volume": "workshop",
    "abstract": "Cross-lingual word embeddings provide a way for information to be transferred between languages. In this paper we evaluate an extension of a joint training approach to learning cross-lingual embeddings that incorporates sub-word information during training. This method could be particularly well-suited to lower-resource and morphologically-rich languages because it can be trained on modest size monolingual corpora, and is able to represent out-of-vocabulary words (OOVs). We consider bilingual lexicon induction, including an evaluation focused on OOVs. We find that this method achieves improvements over previous approaches, particularly for OOVs",
    "checked": true,
    "id": "9082550c470561d35d3f93b129a89adac1ffee30",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Hakimi Parizi",
      "Paul Cook"
    ]
  },
  "https://aclanthology.org/2021.starsem-1.30": {
    "title": "Adversarial Training for Machine Reading Comprehension with Virtual Embeddings",
    "volume": "workshop",
    "abstract": "Adversarial training (AT) as a regularization method has proved its effectiveness on various tasks. Though there are successful applications of AT on some NLP tasks, the distinguishing characteristics of NLP tasks have not been exploited. In this paper, we aim to apply AT on machine reading comprehension (MRC) tasks. Furthermore, we adapt AT for MRC tasks by proposing a novel adversarial training method called PQAT that perturbs the embedding matrix instead of word vectors. To differentiate the roles of passages and questions, PQAT uses additional virtual P/Q-embedding matrices to gather the global perturbations of words from passages and questions separately. We test the method on a wide range of MRC tasks, including span-based extractive RC and multiple-choice RC. The results show that adversarial training is effective universally, and PQAT further improves the performance",
    "checked": true,
    "id": "9a1495afbba953a0685ee20ace659bfeefc59349",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Ziqing Yang",
      "Yiming Cui",
      "Chenglei Si",
      "Wanxiang Che",
      "Ting Liu",
      "Shijin Wang",
      "Guoping Hu"
    ]
  },
  "https://aclanthology.org/2021.unimplicit-1.1": {
    "title": "Let's be explicit about that: Distant supervision for implicit discourse relation classification via connective prediction",
    "volume": "workshop",
    "abstract": "In implicit discourse relation classification, we want to predict the relation between adjacent sentences in the absence of any overt discourse connectives. This is challenging even for humans, leading to shortage of annotated data, a fact that makes the task even more difficult for supervised machine learning approaches. In the current study, we perform implicit discourse relation classification without relying on any labeled implicit relation. We sidestep the lack of data through explicitation of implicit relations to reduce the task to two sub-problems: language modeling and explicit discourse relation classification, a much easier problem. Our experimental results show that this method can even marginally outperform the state-of-the-art, in spite of being much simpler than alternative models of comparable performance. Moreover, we show that the achieved performance is robust across domains as suggested by the zero-shot experiments on a completely different domain. This indicates that recent advances in language modeling have made language models sufficiently good at capturing inter-sentence relations without the help of explicit discourse markers",
    "checked": true,
    "id": "88eb76f8882fb44029229d0c4d319438761d6271",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Murathan Kurfalı",
      "Robert Östling"
    ]
  },
  "https://aclanthology.org/2021.unimplicit-1.2": {
    "title": "Implicit Phenomena in Short-answer Scoring Data",
    "volume": "workshop",
    "abstract": "Short-answer scoring is the task of assessing the correctness of a short text given as response to a question that can come from a variety of educational scenarios. As only content, not form, is important, the exact wording including the explicitness of an answer should not matter. However, many state-of-the-art scoring models heavily rely on lexical information, be it word embeddings in a neural network or n-grams in an SVM. Thus, the exact wording of an answer might very well make a difference. We therefore quantify to what extent implicit language phenomena occur in short answer datasets and examine the influence they have on automatic scoring performance. We find that the level of implicitness depends on the individual question, and that some phenomena are very frequent. Resolving implicit wording to explicit formulations indeed tends to improve automatic scoring performance",
    "checked": true,
    "id": "f9619cfe8bfa66d2ab8af6c77a6f5c1b87633b1a",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Marie Bexte",
      "Andrea Horbach",
      "Torsten Zesch"
    ]
  },
  "https://aclanthology.org/2021.unimplicit-1.3": {
    "title": "Evaluation Guidelines to Deal with Implicit Phenomena to Assess Factuality in Data-to-Text Generation",
    "volume": "workshop",
    "abstract": "Data-to-text generation systems are trained on large datasets, such as WebNLG, Ro-toWire, E2E or DART. Beyond traditional token-overlap evaluation metrics (BLEU or METEOR), a key concern faced by recent generators is to control the factuality of the generated text with respect to the input data specification. We report on our experience when developing an automatic factuality evaluation system for data-to-text generation that we are testing on WebNLG and E2E data. We aim to prepare gold data annotated manually to identify cases where the text communicates more information than is warranted based on the in-put data (extra) or fails to communicate data that is part of the input (missing). While analyzing reference (data, text) samples, we encountered a range of systematic uncertainties that are related to cases on implicit phenomena in text, and the nature of non-linguistic knowledge we expect to be involved when assessing factuality. We derive from our experience a set of evaluation guidelines to reach high inter-annotator agreement on such cases",
    "checked": true,
    "id": "b8fddf5c79b8ba02603804042d2ab8800819645a",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roy Eisenstadt",
      "Michael Elhadad"
    ]
  },
  "https://aclanthology.org/2021.unimplicit-1.4": {
    "title": "UnImplicit Shared Task Report: Detecting Clarification Requirements in Instructional Text",
    "volume": "workshop",
    "abstract": "This paper describes the data, task setup, and results of the shared task at the First Workshop on Understanding Implicit and Underspecified Language (UnImplicit). The task requires computational models to predict whether a sentence contains aspects of meaning that are contextually unspecified and thus require clarification. Two teams participated and the best scoring system achieved an accuracy of 68%",
    "checked": true,
    "id": "5a578e1c089c06b56f581e0a37be249aa8528336",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Michael Roth",
      "Talita Anthonio"
    ]
  },
  "https://aclanthology.org/2021.unimplicit-1.5": {
    "title": "Improvements and Extensions on Metaphor Detection",
    "volume": "workshop",
    "abstract": "Metaphors are ubiquitous in human language. The metaphor detection task (MD) aims at detecting and interpreting metaphors from written language, which is crucial in natural language understanding (NLU) research. In this paper, we introduce a pre-trained Transformer-based model into MD. Our model outperforms the previous state-of-the-art models by large margins in our evaluations, with relative improvements on the F-1 score from 5.33% to 28.39%. Second, we extend MD to a classification task about the metaphoricity of an entire piece of text to make MD applicable in more general NLU scenes. Finally, we clean up the improper or outdated annotations in one of the MD benchmark datasets and re-benchmark it with our Transformer-based model. This approach could be applied to other existing MD datasets as well, since the metaphoricity annotations in these benchmark datasets may be outdated. Future research efforts are also necessary to build an up-to-date and well-annotated dataset consisting of longer and more complex texts",
    "checked": true,
    "id": "ecd6cafe542a5c1230dec7960d33fee3ed0c596d",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weicheng Ma",
      "Ruibo Liu",
      "Lili Wang",
      "Soroush Vosoughi"
    ]
  },
  "https://aclanthology.org/2021.unimplicit-1.6": {
    "title": "Human-Model Divergence in the Handling of Vagueness",
    "volume": "workshop",
    "abstract": "While aggregate performance metrics can generate valuable insights at a large scale, their dominance means more complex and nuanced language phenomena, such as vagueness, may be overlooked. Focusing on vague terms (e.g. sunny, cloudy, young, etc.) we inspect the behavior of visually grounded and text-only models, finding systematic divergences from human judgments even when a model’s overall performance is high. To help explain this disparity, we identify two assumptions made by the datasets and models examined and, guided by the philosophy of vagueness, isolate cases where they do not hold",
    "checked": true,
    "id": "9e3051794def7d6ec7dc4fdd51b7502af67902e4",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elias Stengel-Eskin",
      "Jimena Guallar-Blasco",
      "Benjamin Van Durme"
    ]
  },
  "https://aclanthology.org/2021.unimplicit-1.7": {
    "title": "A Mention-Based System for Revision Requirements Detection",
    "volume": "workshop",
    "abstract": "Exploring aspects of sentential meaning that are implicit or underspecified in context is important for sentence understanding. In this paper, we propose a novel architecture based on mentions for revision requirements detection. The goal is to improve understandability, addressing some types of revisions, especially for the Replaced Pronoun type. We show that our mention-based system can predict replaced pronouns well on the mention-level. However, our combined sentence-level system does not improve on the sentence-level BERT baseline. We also present additional contrastive systems, and show results for each type of edit",
    "checked": true,
    "id": "77f67bdf76dae75fe29f93da5c445f54a6533f12",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Ahmed Ruby",
      "Christian Hardmeier",
      "Sara Stymne"
    ]
  },
  "https://aclanthology.org/2021.unimplicit-1.8": {
    "title": "TTCB System Description to a Shared Task on Implicit and Underspecified Language 2021",
    "volume": "workshop",
    "abstract": "In this report, we describe our transformers for text classification baseline (TTCB) submissions to a shared task on implicit and underspecified language 2021. We cast the task of predicting revision requirements in collaboratively edited instructions as text classification. We considered transformer-based models which are the current state-of-the-art methods for text classification. We explored different training schemes, loss functions, and data augmentations. Our best result of 68.45% test accuracy (68.84% validation accuracy), however, consists of an XLNet model with a linear annealing scheduler and a cross-entropy loss. We do not observe any significant gain on any validation metric based on our various design choices except the MiniLM which has a higher validation F1 score and is faster to train by a half but also a lower validation accuracy score",
    "checked": true,
    "id": "71bb7201cc2597cad880d9324a463aa1804854f4",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Peratham Wiriyathammabhum"
    ]
  },
  "https://aclanthology.org/2021.wat-1.1": {
    "title": "Overview of the 8th Workshop on Asian Translation",
    "volume": "workshop",
    "abstract": "This paper presents the results of the shared tasks from the 8th workshop on Asian translation (WAT2021). For the WAT2021, 28 teams participated in the shared tasks and 24 teams submitted their translation results for the human evaluation. We also accepted 5 research papers. About 2,100 translation results were submitted to the automatic evaluation server, and selected submissions were manually evaluated",
    "checked": true,
    "id": "d0f4394e8be52174731824592656722d4c6f6e33",
    "semantic_title": "",
    "citation_count": 41,
    "authors": [
      "Toshiaki Nakazawa",
      "Hideki Nakayama",
      "Chenchen Ding",
      "Raj Dabre",
      "Shohei Higashiyama",
      "Hideya Mino",
      "Isao Goto",
      "Win Pa Pa",
      "Anoop Kunchukuttan",
      "Shantipriya Parida",
      "Ondřej Bojar",
      "Chenhui Chu",
      "Akiko Eriguchi",
      "Kaori Abe",
      "Yusuke Oda",
      "Sadao Kurohashi"
    ]
  },
  "https://aclanthology.org/2021.wat-1.2": {
    "title": "NHK's Lexically-Constrained Neural Machine Translation at WAT 2021",
    "volume": "workshop",
    "abstract": "This paper describes the system of our team (NHK) for the WAT 2021 Japanese-English restricted machine translation task. In this task, the aim is to improve quality while maintaining consistent terminology for scientific paper translation. This task has a unique feature, where some words in a target sentence are given in addition to a source sentence. In this paper, we use a lexically-constrained neural machine translation (NMT), which concatenates the source sentence and constrained words with a special token to input them into the encoder of NMT. The key to the successful lexically-constrained NMT is the way to extract constraints from a target sentence of training data. We propose two extraction methods: proper-noun constraint and mistranslated-word constraint. These two methods consider the importance of words and fallibility of NMT, respectively. The evaluation results demonstrate the effectiveness of our lexical-constraint method",
    "checked": true,
    "id": "47e0396f7092afa063fd0917f3bceb5e987af0c8",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hideya Mino",
      "Kazutaka Kinugawa",
      "Hitoshi Ito",
      "Isao Goto",
      "Ichiro Yamada",
      "Takenobu Tokunaga"
    ]
  },
  "https://aclanthology.org/2021.wat-1.3": {
    "title": "Input Augmentation Improves Constrained Beam Search for Neural Machine Translation: NTT at WAT 2021",
    "volume": "workshop",
    "abstract": "This paper describes our systems that were submitted to the restricted translation task at WAT 2021. In this task, the systems are required to output translated sentences that contain all given word constraints. Our system combined input augmentation and constrained beam search algorithms. Through experiments, we found that this combination significantly improves translation accuracy and can save inference time while containing all the constraints in the output. For both En->Ja and Ja->En, our systems obtained the best evaluation performances in automatic and human evaluation",
    "checked": true,
    "id": "9535633a4c48a598ad56c26e83a0d529bbec4d05",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Katsuki Chousa",
      "Makoto Morishita"
    ]
  },
  "https://aclanthology.org/2021.wat-1.4": {
    "title": "NICT's Neural Machine Translation Systems for the WAT21 Restricted Translation Task",
    "volume": "workshop",
    "abstract": "This paper describes our system (Team ID: nictrb) for participating in the WAT’21 restricted machine translation task. In our submitted system, we designed a new training approach for restricted machine translation. By sampling from the translation target, we can solve the problem that ordinary training data does not have a restricted vocabulary. With the further help of constrained decoding in the inference phase, we achieved better results than the baseline, confirming the effectiveness of our solution. In addition, we also tried the vanilla and sparse Transformer as the backbone network of the model, as well as model ensembling, which further improved the final translation performance",
    "checked": true,
    "id": "464e2b4bc38244c0fde76610a15ba6337c303cb5",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Zuchao Li",
      "Masao Utiyama",
      "Eiichiro Sumita",
      "Hai Zhao"
    ]
  },
  "https://aclanthology.org/2021.wat-1.5": {
    "title": "Machine Translation with Pre-specified Target-side Words Using a Semi-autoregressive Model",
    "volume": "workshop",
    "abstract": "We introduce our TMU Japanese-to-English system, which employs a semi-autoregressive model, to tackle the WAT 2021 restricted translation task. In this task, we translate an input sentence with the constraint that some words, called restricted target vocabularies (RTVs), must be contained in the output sentence. To satisfy this constraint, we use a semi-autoregressive model, namely, RecoverSAT, due to its ability (known as “forced translation”) to insert specified words into the output sentence. When using “forced translation,” the order of inserting RTVs is a critical problem. In this work, we aligned the source sentence and the corresponding RTVs using GIZA++. In our system, we obtain word alignment between a source sentence and the corresponding RTVs and then sort the RTVs in the order of their corresponding words or phrases in the source sentence. Using the model with sorted order RTVs, we succeeded in inserting all the RTVs into output sentences in more than 96% of the test sentences. Moreover, we confirmed that sorting RTVs improved the BLEU score compared with random order RTVs",
    "checked": true,
    "id": "b7e4dc00bc6721f9ab89c2f4926867d38c24b914",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Seiichiro Kondo",
      "Aomi Koyama",
      "Tomoshige Kiyuna",
      "Tosho Hirasawa",
      "Mamoru Komachi"
    ]
  },
  "https://aclanthology.org/2021.wat-1.6": {
    "title": "NECTEC's Participation in WAT-2021",
    "volume": "workshop",
    "abstract": "In this paper, we report the experimental results of Machine Translation models conducted by a NECTEC team for the translation tasks of WAT-2021. Basically, our models are based on neural methods for both directions of English-Myanmar and Myanmar-English language pairs. Most of the existing Neural Machine Translation (NMT) models mainly focus on the conversion of sequential data and do not directly use syntactic information. However, we conduct multi-source neural machine translation (NMT) models using the multilingual corpora such as string data corpus, tree data corpus, or POS-tagged data corpus. The multi-source translation is an approach to exploit multiple inputs (e.g. in two different formats) to increase translation accuracy. The RNN-based encoder-decoder model with attention mechanism and transformer architectures have been carried out for our experiment. The experimental results showed that the proposed models of RNN-based architecture outperform the baseline model for English-to-Myanmar translation task, and the multi-source and shared-multi-source transformer models yield better translation results than the baseline",
    "checked": true,
    "id": "5821f08a938b0691bc35b9edac1e6ec9f0c438bb",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Zar Zar Hlaing",
      "Ye Kyaw Thu",
      "Thazin Myint Oo",
      "Mya Ei San",
      "Sasiporn Usanavasin",
      "Ponrudee Netisopakul",
      "Thepchai Supnithi"
    ]
  },
  "https://aclanthology.org/2021.wat-1.7": {
    "title": "Hybrid Statistical Machine Translation for English-Myanmar: UTYCC Submission to WAT-2021",
    "volume": "workshop",
    "abstract": "In this paper we describe our submissions to WAT-2021 (Nakazawa et al., 2021) for English-to-Myanmar language (Burmese) task. Our team, ID: “YCC-MT1”, focused on bringing transliteration knowledge to the decoder without changing the model. We manually extracted the transliteration word/phrase pairs from the ALT corpus and applying XML markup feature of Moses decoder (i.e. -xml-input exclusive, -xml-input inclusive). We demonstrate that hybrid translation technique can significantly improve (around 6 BLEU scores) the baseline of three well-known “Phrase-based SMT”, “Operation Sequence Model” and “Hierarchical Phrase-based SMT”. Moreover, this simple hybrid method achieved the second highest results among the submitted MT systems for English-to-Myanmar WAT2021 translation share task according to BLEU (Papineni et al., 2002) and AMFM scores (Banchs et al., 2015)",
    "checked": true,
    "id": "c2e8409544f7538d64b5769496ec58fae3cff5f9",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye Kyaw Thu",
      "Thazin Myint Oo",
      "Hlaing Myat Nwe",
      "Khaing Zar Mon",
      "Nang Aeindray Kyaw",
      "Naing Linn Phyo",
      "Nann Hwan Khun",
      "Hnin Aye Thant"
    ]
  },
  "https://aclanthology.org/2021.wat-1.8": {
    "title": "NICT-2 Translation System at WAT-2021: Applying a Pretrained Multilingual Encoder-Decoder Model to Low-resource Language Pairs",
    "volume": "workshop",
    "abstract": "In this paper, we present the NICT system (NICT-2) submitted to the NICT-SAP shared task at the 8th Workshop on Asian Translation (WAT-2021). A feature of our system is that we used a pretrained multilingual BART (Bidirectional and Auto-Regressive Transformer; mBART) model. Because publicly available models do not support some languages in the NICT-SAP task, we added these languages to the mBART model and then trained it using monolingual corpora extracted from Wikipedia. We fine-tuned the expanded mBART model using the parallel corpora specified by the NICT-SAP task. The BLEU scores greatly improved in comparison with those of systems without the pretrained model, including the additional languages",
    "checked": true,
    "id": "b59b471ff0444cd733d4f472fb7b6d39f3c7746f",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kenji Imamura",
      "Eiichiro Sumita"
    ]
  },
  "https://aclanthology.org/2021.wat-1.9": {
    "title": "Rakuten's Participation in WAT 2021: Examining the Effectiveness of Pre-trained Models for Multilingual and Multimodal Machine Translation",
    "volume": "workshop",
    "abstract": "This paper introduces our neural machine translation systems’ participation in the WAT 2021 shared translation tasks (team ID: sakura). We participated in the (i) NICT-SAP, (ii) Japanese-English multimodal translation, (iii) Multilingual Indic, and (iv) Myanmar-English translation tasks. Multilingual approaches such as mBART (Liu et al., 2020) are capable of pre-training a complete, multilingual sequence-to-sequence model through denoising objectives, making it a great starting point for building multilingual translation systems. Our main focus in this work is to investigate the effectiveness of multilingual finetuning on such a multilingual language model on various translation tasks, including low-resource, multimodal, and mixed-domain translation. We further explore a multimodal approach based on universal visual representation (Zhang et al., 2019) and compare its performance against a unimodal approach based on mBART alone",
    "checked": true,
    "id": "c66439826870908bc3b13caa7941844711aab591",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Raymond Hendy Susanto",
      "Dongzhe Wang",
      "Sunil Yadav",
      "Mausam Jain",
      "Ohnmar Htun"
    ]
  },
  "https://aclanthology.org/2021.wat-1.10": {
    "title": "BTS: Back TranScription for Speech-to-Text Post-Processor using Text-to-Speech-to-Text",
    "volume": "workshop",
    "abstract": "With the growing popularity of smart speakers, such as Amazon Alexa, speech is becoming one of the most important modes of human-computer interaction. Automatic speech recognition (ASR) is arguably the most critical component of such systems, as errors in speech recognition propagate to the downstream components and drastically degrade the user experience. A simple and effective way to improve the speech recognition accuracy is to apply automatic post-processor to the recognition result. However, training a post-processor requires parallel corpora created by human annotators, which are expensive and not scalable. To alleviate this problem, we propose Back TranScription (BTS), a denoising-based method that can create such corpora without human labor. Using a raw corpus, BTS corrupts the text using Text-to-Speech (TTS) and Speech-to-Text (STT) systems. Then, a post-processing model can be trained to reconstruct the original text given the corrupted input. Quantitative and qualitative evaluations show that a post-processor trained using our approach is highly effective in fixing non-trivial speech recognition errors such as mishandling foreign words. We present the generated parallel corpus and post-processing platform to make our results publicly available",
    "checked": true,
    "id": "f5db8ab86f002c679d6148a01ebf08b72179c664",
    "semantic_title": "",
    "citation_count": 14,
    "authors": [
      "Chanjun Park",
      "Jaehyung Seo",
      "Seolhwa Lee",
      "Chanhee Lee",
      "Hyeonseok Moon",
      "Sugyeong Eo",
      "Heuiseok Lim"
    ]
  },
  "https://aclanthology.org/2021.wat-1.11": {
    "title": "Zero-pronoun Data Augmentation for Japanese-to-English Translation",
    "volume": "workshop",
    "abstract": "For Japanese-to-English translation, zero pronouns in Japanese pose a challenge, since the model needs to infer and produce the corresponding pronoun in the target side of the English sentence. However, although fully resolving zero pronouns often needs discourse context, in some cases, the local context within a sentence gives clues to the inference of the zero pronoun. In this study, we propose a data augmentation method that provides additional training signals for the translation model to learn correlations between local context and zero pronouns. We show that the proposed method significantly improves the accuracy of zero pronoun translation with machine translation experiments in the conversational domain",
    "checked": true,
    "id": "347f0decd03b386f2626c6ca306f1e794fe06f45",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Ryokan Ri",
      "Toshiaki Nakazawa",
      "Yoshimasa Tsuruoka"
    ]
  },
  "https://aclanthology.org/2021.wat-1.12": {
    "title": "Evaluation Scheme of Focal Translation for Japanese Partially Amended Statutes",
    "volume": "workshop",
    "abstract": "For updating the translations of Japanese statutes based on their amendments, we need to consider the translation “focality;” that is, we should only modify expressions that are relevant to the amendment and retain the others to avoid misconstruing its contents. In this paper, we introduce an evaluation metric and a corpus to improve focality evaluations. Our metric is called an Inclusive Score for DIfferential Translation: (ISDIT). ISDIT consists of two factors: (1) the n-gram recall of expressions unaffected by the amendment and (2) the n-gram precision of the output compared to the reference. This metric supersedes an existing one for focality by simultaneously calculating the translation quality of the changed expressions in addition to that of the unchanged expressions. We also newly compile a corpus for Japanese partially amendment translation that secures the focality of the post-amendment translations, while an existing evaluation corpus does not. With the metric and the corpus, we examine the performance of existing translation methods for Japanese partially amendment translations",
    "checked": true,
    "id": "df35835395e4a55344d52ff7156b00c7a60994fb",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takahiro Yamakoshi",
      "Takahiro Komamizu",
      "Yasuhiro Ogawa",
      "Katsuhiko Toyama"
    ]
  },
  "https://aclanthology.org/2021.wat-1.13": {
    "title": "TMU NMT System with Japanese BART for the Patent task of WAT 2021",
    "volume": "workshop",
    "abstract": "In this paper, we introduce our TMU Neural Machine Translation (NMT) system submitted for the Patent task (Korean Japanese and English Japanese) of 8th Workshop on Asian Translation (Nakazawa et al., 2021). Recently, several studies proposed pre-trained encoder-decoder models using monolingual data. One of the pre-trained models, BART (Lewis et al., 2020), was shown to improve translation accuracy via fine-tuning with bilingual data. However, they experimented only Romanian!English translation using English BART. In this paper, we examine the effectiveness of Japanese BART using Japan Patent Office Corpus 2.0. Our experiments indicate that Japanese BART can also improve translation accuracy in both Korean Japanese and English Japanese translations",
    "checked": true,
    "id": "34db1a9b9f31eaf90d1484fd5749911878fcda34",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Hwichan Kim",
      "Mamoru Komachi"
    ]
  },
  "https://aclanthology.org/2021.wat-1.14": {
    "title": "System Description for Transperfect",
    "volume": "workshop",
    "abstract": "In this paper, we describe our participation in the 2021 Workshop on Asian Translation (team ID: tpt_wat). We submitted results for all six directions of the JPC2 patent task. As a first-time participant in the task, we attempted to identify a single configuration that provided the best overall results across all language pairs. All our submissions were created using single base transformer models, trained on only the task-specific data, using a consistent configuration of hyperparameters. In contrast to the uniformity of our methods, our results vary widely across the six language pairs",
    "checked": true,
    "id": "92ba3a067b9d491686abbaf951e7d00f07efb138",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wiktor Stribiżew",
      "Fred Bane",
      "José Conceição",
      "Anna Zaretskaya"
    ]
  },
  "https://aclanthology.org/2021.wat-1.15": {
    "title": "Bering Lab's Submissions on WAT 2021 Shared Task",
    "volume": "workshop",
    "abstract": "This paper presents the Bering Lab’s submission to the shared tasks of the 8th Workshop on Asian Translation (WAT 2021) on JPC2 and NICT-SAP. We participated in all tasks on JPC2 and IT domain tasks on NICT-SAP. Our approach for all tasks mainly focused on building NMT systems in domain-specific corpora. We crawled patent document pairs for English-Japanese, Chinese-Japanese, and Korean-Japanese. After cleaning noisy data, we built parallel corpus by aligning those sentences with the sentence-level similarity scores. Also, for SAP test data, we collected the OPUS dataset including three IT domain corpora. We then trained transformer on the collected dataset. Our submission ranked 1st in eight out of fourteen tasks, achieving up to an improvement of 2.87 for JPC2 and 8.79 for NICT-SAP in BLEU score",
    "checked": true,
    "id": "0fff3ad88f5f63be5dc3b3f6fdff4720ecba2c7d",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Heesoo Park",
      "Dongjun Lee"
    ]
  },
  "https://aclanthology.org/2021.wat-1.16": {
    "title": "NLPHut's Participation at WAT2021",
    "volume": "workshop",
    "abstract": "This paper provides the description of shared tasks to the WAT 2021 by our team “NLPHut”. We have participated in the English→Hindi Multimodal translation task, English→Malayalam Multimodal translation task, and Indic Multi-lingual translation task. We have used the state-of-the-art Transformer model with language tags in different settings for the translation task and proposed a novel “region-specific” caption generation approach using a combination of image CNN and LSTM for the Hindi and Malayalam image captioning. Our submission tops in English→Malayalam Multimodal translation task (text-only translation, and Malayalam caption), and ranks second-best in English→Hindi Multimodal translation task (text-only translation, and Hindi caption). Our submissions have also performed well in the Indic Multilingual translation tasks",
    "checked": true,
    "id": "408f9acc5409102e7f54c6df1e08dca394c0fee2",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Shantipriya Parida",
      "Subhadarshi Panda",
      "Ketan Kotwal",
      "Amulya Ratna Dash",
      "Satya Ranjan Dash",
      "Yashvardhan Sharma",
      "Petr Motlicek",
      "Ondřej Bojar"
    ]
  },
  "https://aclanthology.org/2021.wat-1.17": {
    "title": "Improved English to Hindi Multimodal Neural Machine Translation",
    "volume": "workshop",
    "abstract": "Machine translation performs automatic translation from one natural language to another. Neural machine translation attains a state-of-the-art approach in machine translation, but it requires adequate training data, which is a severe problem for low-resource language pairs translation. The concept of multimodal is introduced in neural machine translation (NMT) by merging textual features with visual features to improve low-resource pair translation. WAT2021 (Workshop on Asian Translation 2021) organizes a shared task of multimodal translation for English to Hindi. We have participated the same with team name CNLP-NITS-PP in two submissions: multimodal and text-only NMT. This work investigates phrase pairs injection via data augmentation approach and attains improvement over our previous work at WAT2020 on the same task in both text-only and multimodal NMT. We have achieved second rank on the challenge test set for English to Hindi multimodal translation where Bilingual Evaluation Understudy (BLEU) score of 39.28, Rank-based Intuitive Bilingual Evaluation Score (RIBES) 0.792097, and Adequacy-Fluency Metrics (AMFM) score 0.830230 respectively",
    "checked": true,
    "id": "7d3e146beb212f4daf5a2280dd56e7638e8e6a5b",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Sahinur Rahman Laskar",
      "Abdullah Faiz Ur Rahman Khilji",
      "Darsh Kaushik",
      "Partha Pakray",
      "Sivaji Bandyopadhyay"
    ]
  },
  "https://aclanthology.org/2021.wat-1.18": {
    "title": "IITP at WAT 2021: System description for English-Hindi Multimodal Translation Task",
    "volume": "workshop",
    "abstract": "Neural Machine Translation (NMT) is a predominant machine translation technology nowadays because of its end-to-end trainable flexibility. However, NMT still struggles to translate properly in low-resource settings specifically on distant language pairs. One way to overcome this is to use the information from other modalities if available. The idea is that despite differences in languages, both the source and target language speakers see the same thing and the visual representation of both the source and target is the same, which can positively assist the system. Multimodal information can help the NMT system to improve the translation by removing ambiguity on some phrases or words. We participate in the 8th Workshop on Asian Translation (WAT - 2021) for English-Hindi multimodal translation task and achieve 42.47 and 37.50 BLEU points for Evaluation and Challenge subset, respectively",
    "checked": true,
    "id": "a327eca38c8ee3ea0a20d9d4c31c95958516e52d",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Baban Gain",
      "Dibyanayan Bandyopadhyay",
      "Asif Ekbal"
    ]
  },
  "https://aclanthology.org/2021.wat-1.19": {
    "title": "ViTA: Visual-Linguistic Translation by Aligning Object Tags",
    "volume": "workshop",
    "abstract": "Multimodal Machine Translation (MMT) enriches the source text with visual information for translation. It has gained popularity in recent years, and several pipelines have been proposed in the same direction. Yet, the task lacks quality datasets to illustrate the contribution of visual modality in the translation systems. In this paper, we propose our system under the team name Volta for the Multimodal Translation Task of WAT 2021 from English to Hindi. We also participate in the textual-only subtask of the same language pair for which we use mBART, a pretrained multilingual sequence-to-sequence model. For multimodal translation, we propose to enhance the textual input by bringing the visual information to a textual domain by extracting object tags from the image. We also explore the robustness of our system by systematically degrading the source text. Finally, we achieve a BLEU score of 44.6 and 51.6 on the test set and challenge set of the multimodal task",
    "checked": true,
    "id": "60c7eea702813095bae01a0ac6d6163565e542a4",
    "semantic_title": "",
    "citation_count": 7,
    "authors": [
      "Kshitij Gupta",
      "Devansh Gautam",
      "Radhika Mamidi"
    ]
  },
  "https://aclanthology.org/2021.wat-1.20": {
    "title": "TMEKU System for the WAT2021 Multimodal Translation Task",
    "volume": "workshop",
    "abstract": "We introduce our TMEKU system submitted to the English-Japanese Multimodal Translation Task for WAT 2021. We participated in the Flickr30kEnt-JP task and Ambiguous MSCOCO Multimodal task under the constrained condition using only the officially provided datasets. Our proposed system employs soft alignment of word-region for multimodal neural machine translation (MNMT). The experimental results evaluated on the BLEU metric provided by the WAT 2021 evaluation site show that the TMEKU system has achieved the best performance among all the participated systems. Further analysis of the case study demonstrates that leveraging word-region alignment between the textual and visual modalities is the key to performance enhancement in our TMEKU system, which leads to better visual information use",
    "checked": true,
    "id": "f9c4ef2f65e88c924f152077c07cb6d54dbf0994",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuting Zhao",
      "Mamoru Komachi",
      "Tomoyuki Kajiwara",
      "Chenhui Chu"
    ]
  },
  "https://aclanthology.org/2021.wat-1.21": {
    "title": "Optimal Word Segmentation for Neural Machine Translation into Dravidian Languages",
    "volume": "workshop",
    "abstract": "Dravidian languages, such as Kannada and Tamil, are notoriously difficult to translate by state-of-the-art neural models. This stems from the fact that these languages are morphologically very rich as well as being low-resourced. In this paper, we focus on subword segmentation and evaluate Linguistically Motivated Vocabulary Reduction (LMVR) against the more commonly used SentencePiece (SP) for the task of translating from English into four different Dravidian languages. Additionally we investigate the optimal subword vocabulary size for each language. We find that SP is the overall best choice for segmentation, and that larger dictionary sizes lead to higher translation quality",
    "checked": true,
    "id": "6b6c650ff82849a5737ac43efa014934d5c100da",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Prajit Dhar",
      "Arianna Bisazza",
      "Gertjan van Noord"
    ]
  },
  "https://aclanthology.org/2021.wat-1.22": {
    "title": "Itihasa: A large-scale corpus for Sanskrit to English translation",
    "volume": "workshop",
    "abstract": "This work introduces Itihasa, a large-scale translation dataset containing 93,000 pairs of Sanskrit shlokas and their English translations. The shlokas are extracted from two Indian epics viz., The Ramayana and The Mahabharata. We first describe the motivation behind the curation of such a dataset and follow up with empirical analysis to bring out its nuances. We then benchmark the performance of standard translation models on this corpus and show that even state-of-the-art transformer architectures perform poorly, emphasizing the complexity of the dataset",
    "checked": true,
    "id": "3ca2fa08229d3aa1c82eb1f277578c8b886d8871",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Rahul Aralikatte",
      "Miryam de Lhoneux",
      "Anoop Kunchukuttan",
      "Anders Søgaard"
    ]
  },
  "https://aclanthology.org/2021.wat-1.23": {
    "title": "NICT-5's Submission To WAT 2021: MBART Pre-training And In-Domain Fine Tuning For Indic Languages",
    "volume": "workshop",
    "abstract": "In this paper we describe our submission to the multilingual Indic language translation wtask “MultiIndicMT” under the team name “NICT-5”. This task involves translation from 10 Indic languages into English and vice-versa. The objective of the task was to explore the utility of multilingual approaches using a variety of in-domain and out-of-domain parallel and monolingual corpora. Given the recent success of multilingual NMT pre-training we decided to explore pre-training an MBART model on a large monolingual corpus collection covering all languages in this task followed by multilingual fine-tuning on small in-domain corpora. Firstly, we observed that a small amount of pre-training followed by fine-tuning on small bilingual corpora can yield large gains over when pre-training is not used. Furthermore, multilingual fine-tuning leads to further gains in translation quality which significantly outperforms a very strong multilingual baseline that does not rely on any pre-training",
    "checked": true,
    "id": "6dbbade359d2830cf86691fd188e285c7571b41c",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Raj Dabre",
      "Abhisek Chakrabarty"
    ]
  },
  "https://aclanthology.org/2021.wat-1.24": {
    "title": "How far can we get with one GPU in 100 hours? CoAStaL at MultiIndicMT Shared Task",
    "volume": "workshop",
    "abstract": "This work shows that competitive translation results can be obtained in a constrained setting by incorporating the latest advances in memory and compute optimization. We train and evaluate large multilingual translation models using a single GPU for a maximum of 100 hours and get within 4-5 BLEU points of the top submission on the leaderboard. We also benchmark standard baselines on the PMI corpus and re-discover well-known shortcomings of translation systems and metrics",
    "checked": true,
    "id": "dc8ebb6d9908542ae474dc2b21bfb6a14216f678",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rahul Aralikatte",
      "Héctor Ricardo Murrieta Bello",
      "Miryam de Lhoneux",
      "Daniel Hershcovich",
      "Marcel Bollmann",
      "Anders Søgaard"
    ]
  },
  "https://aclanthology.org/2021.wat-1.25": {
    "title": "IIIT Hyderabad Submission To WAT 2021: Efficient Multilingual NMT systems for Indian languages",
    "volume": "workshop",
    "abstract": "This paper describes the work and the systems submitted by the IIIT-Hyderbad team in the WAT 2021 MultiIndicMT shared task. The task covers 10 major languages of the Indian subcontinent. For the scope of this task, we have built multilingual systems for 20 translation directions namely English-Indic (one-to- many) and Indic-English (many-to-one). Individually, Indian languages are resource poor which hampers translation quality but by leveraging multilingualism and abundant monolingual corpora, the translation quality can be substantially boosted. But the multilingual systems are highly complex in terms of time as well as computational resources. Therefore, we are training our systems by efficiently se- lecting data that will actually contribute to most of the learning process. Furthermore, we are also exploiting the language related- ness found in between Indian languages. All the comparisons were made using BLEU score and we found that our final multilingual sys- tem significantly outperforms the baselines by an average of 11.3 and 19.6 BLEU points for English-Indic (en-xx) and Indic-English (xx- en) directions, respectively",
    "checked": true,
    "id": "dd556ee46ccc8b6be34c37856da65ade47e5c790",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sourav Kumar",
      "Salil Aggarwal",
      "Dipti Sharma"
    ]
  },
  "https://aclanthology.org/2021.wat-1.26": {
    "title": "Language Relatedness and Lexical Closeness can help Improve Multilingual NMT: IITBombay@MultiIndicNMT WAT2021",
    "volume": "workshop",
    "abstract": "Multilingual Neural Machine Translation has achieved remarkable performance by training a single translation model for multiple languages. This paper describes our submission (Team ID: CFILT-IITB) for the MultiIndicMT: An Indic Language Multilingual Task at WAT 2021. We train multilingual NMT systems by sharing encoder and decoder parameters with language embedding associated with each token in both encoder and decoder. Furthermore, we demonstrate the use of transliteration (script conversion) for Indic languages in reducing the lexical gap for training a multilingual NMT system. Further, we show improvement in performance by training a multilingual NMT system using languages of the same family, i.e., related languages",
    "checked": true,
    "id": "1ff14caec4793d1942a39cadda2ae84914a52e1e",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Jyotsana Khatri",
      "Nikhil Saini",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/2021.wat-1.27": {
    "title": "Samsung R&D Institute Poland submission to WAT 2021 Indic Language Multilingual Task",
    "volume": "workshop",
    "abstract": "This paper describes the submission to the WAT 2021 Indic Language Multilingual Task by Samsung R&D Institute Poland. The task covered translation between 10 Indic Languages (Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil and Telugu) and English. We combined a variety of techniques: transliteration, filtering, backtranslation, domain adaptation, knowledge-distillation and finally ensembling of NMT models. We applied an effective approach to low-resource training that consist of pretraining on backtranslations and tuning on parallel corpora. We experimented with two different domain-adaptation techniques which significantly improved translation quality when applied to monolingual corpora. We researched and applied a novel approach for finding the best hyperparameters for ensembling a number of translation models. All techniques combined gave significant improvement - up to +8 BLEU over baseline results. The quality of the models has been confirmed by the human evaluation where SRPOL models scored best for all 5 manually evaluated languages",
    "checked": true,
    "id": "4ee013ece227be3f545c6f991be8fdbb2c7de403",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Adam Dobrowolski",
      "Marcin Szymański",
      "Marcin Chochowski",
      "Paweł Przybysz"
    ]
  },
  "https://aclanthology.org/2021.wat-1.28": {
    "title": "Multilingual Machine Translation Systems at WAT 2021: One-to-Many and Many-to-One Transformer based NMT",
    "volume": "workshop",
    "abstract": "In this paper, we present the details of the systems that we have submitted for the WAT 2021 MultiIndicMT: An Indic Language Multilingual Task. We have submitted two separate multilingual NMT models: one for English to 10 Indic languages and another for 10 Indic languages to English. We discuss the implementation details of two separate multilingual NMT approaches, namely one-to-many and many-to-one, that makes use of a shared decoder and a shared encoder, respectively. From our experiments, we observe that the multilingual NMT systems outperforms the bilingual baseline MT systems for each of the language pairs under consideration",
    "checked": true,
    "id": "984a5d9c77098f20bb58bd1ade6c724f8cc25ff9",
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shivam Mhaskar",
      "Aditya Jain",
      "Aakash Banerjee",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/2021.wat-1.29": {
    "title": "IITP-MT at WAT2021: Indic-English Multilingual Neural Machine Translation using Romanized Vocabulary",
    "volume": "workshop",
    "abstract": "This paper describes the systems submitted to WAT 2021 MultiIndicMT shared task by IITP-MT team. We submit two multilingual Neural Machine Translation (NMT) systems (Indic-to-English and English-to-Indic). We romanize all Indic data and create subword vocabulary which is shared between all Indic languages. We use back-translation approach to generate synthetic data which is appended to parallel corpus and used to train our models. The models are evaluated using BLEU, RIBES and AMFM scores with Indic-to-English model achieving 40.08 BLEU for Hindi-English pair and English-to-Indic model achieving 34.48 BLEU for English-Hindi pair. However, we observe that the shared romanized subword vocabulary is not helping English-to-Indic model at the time of generation, leading it to produce poor quality translations for Tamil, Telugu and Malayalam to English pairs with BLEU score of 8.51, 6.25 and 3.79 respectively",
    "checked": true,
    "id": "c38db4783ca5ad5d128a73434f10048090638a58",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Ramakrishna Appicharla",
      "Kamal Kumar Gupta",
      "Asif Ekbal",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/2021.wat-1.30": {
    "title": "ANVITA Machine Translation System for WAT 2021 MultiIndicMT Shared Task",
    "volume": "workshop",
    "abstract": "This paper describes ANVITA-1.0 MT system, architected for submission to WAT2021 MultiIndicMT shared task by mcairt team, where the team participated in 20 translation directions: English→Indic and Indic→English; Indic set comprised of 10 Indian languages. ANVITA-1.0 MT system comprised of two multi-lingual NMT models one for the English→Indic directions and other for the Indic→English directions with shared encoder-decoder, catering 10 language pairs and twenty translation directions. The base models were built based on Transformer architecture and trained over MultiIndicMT WAT 2021 corpora and further employed back translation and transliteration for selective data augmentation, and model ensemble for better generalization. Additionally, MultiIndicMT WAT 2021 corpora was distilled using a series of filtering operations before putting up for training. ANVITA-1.0 achieved highest AM-FM score for English→Bengali, 2nd for English→Tamil and 3rd for English→Hindi, Bengali→English directions on official test set. In general, performance achieved by ANVITA for the Indic→English directions are relatively better than that of English→Indic directions for all the 10 language pairs when evaluated using BLEU and RIBES, although the same trend is not observed consistently when AM-FM based evaluation was carried out. As compared to BLEU, RIBES and AM-FM based scoring placed ANVITA relatively better among all the task participants",
    "checked": true,
    "id": "27b2f89be6daeaecedf3b12fe100a07bdfb0f2d7",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Pavanpankaj Vegi",
      "Sivabhavani J",
      "Biswajit Paul",
      "Chitra Viswanathan",
      "Prasanna Kumar K R"
    ]
  },
  "https://aclanthology.org/2021.woah-1.1": {
    "title": "Exploiting Auxiliary Data for Offensive Language Detection with Bidirectional Transformers",
    "volume": "workshop",
    "abstract": "Offensive language detection (OLD) has received increasing attention due to its societal impact. Recent work shows that bidirectional transformer based methods obtain impressive performance on OLD. However, such methods usually rely on large-scale well-labeled OLD datasets for model training. To address the issue of data/label scarcity in OLD, in this paper, we propose a simple yet effective domain adaptation approach to train bidirectional transformers. Our approach introduces domain adaptation (DA) training procedures to ALBERT, such that it can effectively exploit auxiliary data from source domains to improve the OLD performance in a target domain. Experimental results on benchmark datasets show that our approach, ALBERT (DA), obtains the state-of-the-art performance in most cases. Particularly, our approach significantly benefits underrepresented and under-performing classes, with a significant improvement over ALBERT",
    "checked": true,
    "id": "314107f8ff8bc134703c49c012ea6f6d9eb17840",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Sumer Singh",
      "Sheng Li"
    ]
  },
  "https://aclanthology.org/2021.woah-1.2": {
    "title": "Modeling Profanity and Hate Speech in Social Media with Semantic Subspaces",
    "volume": "workshop",
    "abstract": "Hate speech and profanity detection suffer from data sparsity, especially for languages other than English, due to the subjective nature of the tasks and the resulting annotation incompatibility of existing corpora. In this study, we identify profane subspaces in word and sentence representations and explore their generalization capability on a variety of similar and distant target tasks in a zero-shot setting. This is done monolingually (German) and cross-lingually to closely-related (English), distantly-related (French) and non-related (Arabic) tasks. We observe that, on both similar and distant target tasks and across all languages, the subspace-based representations transfer more effectively than standard BERT representations in the zero-shot setting, with improvements between F1 +10.9 and F1 +42.9 over the baselines across all tested monolingual and cross-lingual scenarios",
    "checked": true,
    "id": "f1b3c81604806ac2d7be3a4d48ebfd41289eb104",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Vanessa Hahn",
      "Dana Ruiter",
      "Thomas Kleinbauer",
      "Dietrich Klakow"
    ]
  },
  "https://aclanthology.org/2021.woah-1.3": {
    "title": "HateBERT: Retraining BERT for Abusive Language Detection in English",
    "volume": "workshop",
    "abstract": "We introduce HateBERT, a re-trained BERT model for abusive language detection in English. The model was trained on RAL-E, a large-scale dataset of Reddit comments in English from communities banned for being offensive, abusive, or hateful that we have curated and made available to the public. We present the results of a detailed comparison between a general pre-trained language model and the retrained version on three English datasets for offensive, abusive language and hate speech detection tasks. In all datasets, HateBERT outperforms the corresponding general BERT model. We also discuss a battery of experiments comparing the portability of the fine-tuned models across the datasets, suggesting that portability is affected by compatibility of the annotated phenomena",
    "checked": true,
    "id": "9927a15ddf5313d97c98f0111fd191caf507ce72",
    "semantic_title": "",
    "citation_count": 113,
    "authors": [
      "Tommaso Caselli",
      "Valerio Basile",
      "Jelena Mitrović",
      "Michael Granitzer"
    ]
  },
  "https://aclanthology.org/2021.woah-1.4": {
    "title": "Memes in the Wild: Assessing the Generalizability of the Hateful Memes Challenge Dataset",
    "volume": "workshop",
    "abstract": "Hateful memes pose a unique challenge for current machine learning systems because their message is derived from both text- and visual-modalities. To this effect, Facebook released the Hateful Memes Challenge, a dataset of memes with pre-extracted text captions, but it is unclear whether these synthetic examples generalize to ‘memes in the wild’. In this paper, we collect hateful and non-hateful memes from Pinterest to evaluate out-of-sample performance on models pre-trained on the Facebook dataset. We find that ‘memes in the wild’ differ in two key aspects: 1) Captions must be extracted via OCR, injecting noise and diminishing performance of multimodal models, and 2) Memes are more diverse than ‘traditional memes’, including screenshots of conversations or text on a plain background. This paper thus serves as a reality-check for the current benchmark of hateful meme detection and its applicability for detecting real world hate",
    "checked": true,
    "id": "4f808f1334cee0c59ed60f20d36699e4b2faf438",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Hannah Kirk",
      "Yennie Jun",
      "Paulius Rauba",
      "Gal Wachtel",
      "Ruining Li",
      "Xingjian Bai",
      "Noah Broestl",
      "Martin Doff-Sotta",
      "Aleksandar Shtedritski",
      "Yuki M Asano"
    ]
  },
  "https://aclanthology.org/2021.woah-1.5": {
    "title": "Measuring and Improving Model-Moderator Collaboration using Uncertainty Estimation",
    "volume": "workshop",
    "abstract": "Content moderation is often performed by a collaboration between humans and machine learning models. However, it is not well understood how to design the collaborative process so as to maximize the combined moderator-model system performance. This work presents a rigorous study of this problem, focusing on an approach that incorporates model uncertainty into the collaborative process. First, we introduce principled metrics to describe the performance of the collaborative system under capacity constraints on the human moderator, quantifying how efficiently the combined system utilizes human decisions. Using these metrics, we conduct a large benchmark study evaluating the performance of state-of-the-art uncertainty models under different collaborative review strategies. We find that an uncertainty-based strategy consistently outperforms the widely used strategy based on toxicity scores, and moreover that the choice of review strategy drastically changes the overall system performance. Our results demonstrate the importance of rigorous metrics for understanding and developing effective moderator-model systems for content moderation, as well as the utility of uncertainty estimation in this domain",
    "checked": true,
    "id": "6034e7ec13f1bd38f67cc4cd0f588176447571ca",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Ian Kivlichan",
      "Zi Lin",
      "Jeremiah Liu",
      "Lucy Vasserman"
    ]
  },
  "https://aclanthology.org/2021.woah-1.6": {
    "title": "DALC: the Dutch Abusive Language Corpus",
    "volume": "workshop",
    "abstract": "As socially unacceptable language become pervasive in social media platforms, the need for automatic content moderation become more pressing. This contribution introduces the Dutch Abusive Language Corpus (DALC v1.0), a new dataset with tweets manually an- notated for abusive language. The resource ad- dress a gap in language resources for Dutch and adopts a multi-layer annotation scheme modeling the explicitness and the target of the abusive messages. Baselines experiments on all annotation layers have been conducted, achieving a macro F1 score of 0.748 for binary classification of the explicitness layer and .489 for target classification",
    "checked": true,
    "id": "ffeefc6e7cc844bc7689a7420f606e730d32376d",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Tommaso Caselli",
      "Arjan Schelhaas",
      "Marieke Weultjes",
      "Folkert Leistra",
      "Hylke van der Veen",
      "Gerben Timmerman",
      "Malvina Nissim"
    ]
  },
  "https://aclanthology.org/2021.woah-1.7": {
    "title": "Offensive Language Detection in Nepali Social Media",
    "volume": "workshop",
    "abstract": "Social media texts such as blog posts, comments, and tweets often contain offensive languages including racial hate speech comments, personal attacks, and sexual harassment. Detecting inappropriate use of language is, therefore, of utmost importance for the safety of the users as well as for suppressing hateful conduct and aggression. Existing approaches to this problem are mostly available for resource-rich languages such as English and German. In this paper, we characterize the offensive language in Nepali, a low-resource language, highlighting the challenges that need to be addressed for processing Nepali social media text. We also present experiments for detecting offensive language using supervised machine learning. Besides contributing the first baseline approaches of detecting offensive language in Nepali, we also release human annotated data sets to encourage future research on this crucial topic",
    "checked": true,
    "id": "f6205f8b476dbe21015603c50933e5b00a041f95",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Nobal B. Niraula",
      "Saurab Dulal",
      "Diwa Koirala"
    ]
  },
  "https://aclanthology.org/2021.woah-1.8": {
    "title": "MIN_PT: An European Portuguese Lexicon for Minorities Related Terms",
    "volume": "workshop",
    "abstract": "Hate speech-related lexicons have been proved to be useful for many tasks such as data collection and classification. However, existing Portuguese lexicons do not distinguish between European and Brazilian Portuguese, and do not include neutral terms that are potentially useful to detect a broader spectrum of content referring to minorities. In this work, we present MIN_PT, a new European Portuguese Lexicon for Minorities-Related Terms specifically designed to tackle the limitations of existing resources. We describe the data collection and annotation process, discuss the limitation and ethical concerns, and prove the utility of the resource by applying it to a use case for the Portuguese 2021 presidential elections",
    "checked": true,
    "id": "e3912fd498754cdc704ab677052945a7ea48f853",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Paula Fortuna",
      "Vanessa Cortez",
      "Miguel Sozinho Ramalho",
      "Laura Pérez-Mayos"
    ]
  },
  "https://aclanthology.org/2021.woah-1.9": {
    "title": "Fine-Grained Fairness Analysis of Abusive Language Detection Systems with CheckList",
    "volume": "workshop",
    "abstract": "Current abusive language detection systems have demonstrated unintended bias towards sensitive features such as nationality or gender. This is a crucial issue, which may harm minorities and underrepresented groups if such systems were integrated in real-world applications. In this paper, we create ad hoc tests through the CheckList tool (Ribeiro et al., 2020) to detect biases within abusive language classifiers for English. We compare the behaviour of two BERT-based models, one trained on a generic hate speech dataset and the other on a dataset for misogyny detection. Our evaluation shows that, although BERT-based classifiers achieve high accuracy levels on a variety of natural language processing tasks, they perform very poorly as regards fairness and bias, in particular on samples involving implicit stereotypes, expressions of hate towards minorities and protected attributes such as race or sexual orientation. We release both the notebooks implemented to extend the Fairness tests and the synthetic datasets usable to evaluate systems bias independently of CheckList",
    "checked": true,
    "id": "d0cef8486281ec74679271b23cdf30cf87072f5c",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Marta Marchiori Manerba",
      "Sara Tonelli"
    ]
  },
  "https://aclanthology.org/2021.woah-1.10": {
    "title": "Improving Counterfactual Generation for Fair Hate Speech Detection",
    "volume": "workshop",
    "abstract": "Bias mitigation approaches reduce models’ dependence on sensitive features of data, such as social group tokens (SGTs), resulting in equal predictions across the sensitive features. In hate speech detection, however, equalizing model predictions may ignore important differences among targeted social groups, as hate speech can contain stereotypical language specific to each SGT. Here, to take the specific language about each SGT into account, we rely on counterfactual fairness and equalize predictions among counterfactuals, generated by changing the SGTs. Our method evaluates the similarity in sentence likelihoods (via pre-trained language models) among counterfactuals, to treat SGTs equally only within interchangeable contexts. By applying logit pairing to equalize outcomes on the restricted set of counterfactuals for each instance, we improve fairness metrics while preserving model performance on hate speech detection",
    "checked": true,
    "id": "d3375c8459b9e53e3c9b8f8a36ab7021998cac5d",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Aida Mostafazadeh Davani",
      "Ali Omrani",
      "Brendan Kennedy",
      "Mohammad Atari",
      "Xiang Ren",
      "Morteza Dehghani"
    ]
  },
  "https://aclanthology.org/2021.woah-1.11": {
    "title": "Hell Hath No Fury? Correcting Bias in the NRC Emotion Lexicon",
    "volume": "workshop",
    "abstract": "There have been several attempts to create an accurate and thorough emotion lexicon in English, which identifies the emotional content of words. Of the several commonly used resources, the NRC emotion lexicon (Mohammad and Turney, 2013b) has received the most attention due to its availability, size, and its choice of Plutchik’s expressive 8-class emotion model. In this paper we identify a large number of troubling entries in the NRC lexicon, where words that should in most contexts be emotionally neutral, with no affect (e.g., ‘lesbian’, ‘stone’, ‘mountain’), are associated with emotional labels that are inaccurate, nonsensical, pejorative, or, at best, highly contingent and context-dependent (e.g., ‘lesbian’ labeled as Disgust and Sadness, ‘stone’ as Anger, or ‘mountain’ as Anticipation). We describe a procedure for semi-automatically correcting these problems in the NRC, which includes disambiguating POS categories and aligning NRC entries with other emotion lexicons to infer the accuracy of labels. We demonstrate via an experimental benchmark that the quality of the resources is thus improved. We release the revised resource and our code to enable other researchers to reproduce and build upon results",
    "checked": true,
    "id": "cc70ec5aa10ff158f7e9950ec38d65d6eb4a8ff3",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Samira Zad",
      "Joshuan Jimenez",
      "Mark Finlayson"
    ]
  },
  "https://aclanthology.org/2021.woah-1.12": {
    "title": "Mitigating Biases in Toxic Language Detection through Invariant Rationalization",
    "volume": "workshop",
    "abstract": "Automatic detection of toxic language plays an essential role in protecting social media users, especially minority groups, from verbal abuse. However, biases toward some attributes, including gender, race, and dialect, exist in most training datasets for toxicity detection. The biases make the learned models unfair and can even exacerbate the marginalization of people. Considering that current debiasing methods for general natural language understanding tasks cannot effectively mitigate the biases in the toxicity detectors, we propose to use invariant rationalization (InvRat), a game-theoretic framework consisting of a rationale generator and a predictor, to rule out the spurious correlation of certain syntactic patterns (e.g., identity mentions, dialect) to toxicity labels. We empirically show that our method yields lower false positive rate in both lexical and dialectal attributes than previous debiasing methods",
    "checked": true,
    "id": "15aebc062d9ec0a958f935c454bddb5e1cc3b0a6",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Yung-Sung Chuang",
      "Mingye Gao",
      "Hongyin Luo",
      "James Glass",
      "Hung-yi Lee",
      "Yun-Nung Chen",
      "Shang-Wen Li"
    ]
  },
  "https://aclanthology.org/2021.woah-1.13": {
    "title": "Fine-grained Classification of Political Bias in German News: A Data Set and Initial Experiments",
    "volume": "workshop",
    "abstract": "We present a data set consisting of German news articles labeled for political bias on a five-point scale in a semi-supervised way. While earlier work on hyperpartisan news detection uses binary classification (i.e., hyperpartisan or not) and English data, we argue for a more fine-grained classification, covering the full political spectrum (i.e., far-left, left, centre, right, far-right) and for extending research to German data. Understanding political bias helps in accurately detecting hate speech and online abuse. We experiment with different classification methods for political bias detection. Their comparatively low performance (a macro-F1 of 43 for our best setup, compared to a macro-F1 of 79 for the binary classification task) underlines the need for more (balanced) data annotated in a fine-grained way",
    "checked": true,
    "id": "95377cccf05be92f32170aa4711e7f73658f4ca3",
    "semantic_title": "",
    "citation_count": 6,
    "authors": [
      "Dmitrii Aksenov",
      "Peter Bourgonje",
      "Karolina Zaczynska",
      "Malte Ostendorff",
      "Julian Moreno-Schneider",
      "Georg Rehm"
    ]
  },
  "https://aclanthology.org/2021.woah-1.14": {
    "title": "Jibes & Delights: A Dataset of Targeted Insults and Compliments to Tackle Online Abuse",
    "volume": "workshop",
    "abstract": "Online abuse and offensive language on social media have become widespread problems in today’s digital age. In this paper, we contribute a Reddit-based dataset, consisting of 68,159 insults and 51,102 compliments targeted at individuals instead of targeting a particular community or race. Secondly, we benchmark multiple existing state-of-the-art models for both classification and unsupervised style transfer on the dataset. Finally, we analyse the experimental results and conclude that the transfer task is challenging, requiring the models to understand the high degree of creativity exhibited in the data",
    "checked": true,
    "id": "a8ad7261b533bb10b59d971b168e2d6ea8104c35",
    "semantic_title": "",
    "citation_count": 1,
    "authors": [
      "Ravsimar Sodhi",
      "Kartikey Pant",
      "Radhika Mamidi"
    ]
  },
  "https://aclanthology.org/2021.woah-1.15": {
    "title": "Context Sensitivity Estimation in Toxicity Detection",
    "volume": "workshop",
    "abstract": "User posts whose perceived toxicity depends on the conversational context are rare in current toxicity detection datasets. Hence, toxicity detectors trained on current datasets will also disregard context, making the detection of context-sensitive toxicity a lot harder when it occurs. We constructed and publicly release a dataset of 10k posts with two kinds of toxicity labels per post, obtained from annotators who considered (i) both the current post and the previous one as context, or (ii) only the current post. We introduce a new task, context-sensitivity estimation, which aims to identify posts whose perceived toxicity changes if the context (previous post) is also considered. Using the new dataset, we show that systems can be developed for this task. Such systems could be used to enhance toxicity detection datasets with more context-dependent posts or to suggest when moderators should consider the parent posts, which may not always be necessary and may introduce additional costs",
    "checked": true,
    "id": "13fbda80da1bc8ca81043a225f51b1f1ea39810a",
    "semantic_title": "",
    "citation_count": 10,
    "authors": [
      "Alexandros Xenos",
      "John Pavlopoulos",
      "Ion Androutsopoulos"
    ]
  },
  "https://aclanthology.org/2021.woah-1.16": {
    "title": "A Large-Scale English Multi-Label Twitter Dataset for Cyberbullying and Online Abuse Detection",
    "volume": "workshop",
    "abstract": "In this paper, we introduce a new English Twitter-based dataset for cyberbullying detection and online abuse. Comprising 62,587 tweets, this dataset was sourced from Twitter using specific query terms designed to retrieve tweets with high probabilities of various forms of bullying and offensive content, including insult, trolling, profanity, sarcasm, threat, porn and exclusion. We recruited a pool of 17 annotators to perform fine-grained annotation on the dataset with each tweet annotated by three annotators. All our annotators are high school educated and frequent users of social media. Inter-rater agreement for the dataset as measured by Krippendorff’s Alpha is 0.67. Analysis performed on the dataset confirmed common cyberbullying themes reported by other studies and revealed interesting relationships between the classes. The dataset was used to train a number of transformer-based deep learning models returning impressive results",
    "checked": true,
    "id": "b19b6218bb93192834a869198427a8b0ff5fe5b7",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Semiu Salawu",
      "Jo Lumsden",
      "Yulan He"
    ]
  },
  "https://aclanthology.org/2021.woah-1.17": {
    "title": "Data Integration for Toxic Comment Classification: Making More Than 40 Datasets Easily Accessible in One Unified Format",
    "volume": "workshop",
    "abstract": "With the rise of research on toxic comment classification, more and more annotated datasets have been released. The wide variety of the task (different languages, different labeling processes and schemes) has led to a large amount of heterogeneous datasets that can be used for training and testing very specific settings. Despite recent efforts to create web pages that provide an overview, most publications still use only a single dataset. They are not stored in one central database, they come in many different data formats and it is difficult to interpret their class labels and how to reuse these labels in other projects. To overcome these issues, we present a collection of more than thirty datasets in the form of a software tool that automatizes downloading and processing of the data and presents them in a unified data format that also offers a mapping of compatible class labels. Another advantage of that tool is that it gives an overview of properties of available datasets, such as different languages, platforms, and class labels to make it easier to select suitable training and test data",
    "checked": true,
    "id": "d886aacba1db4908a309f2992a7f615572d898e7",
    "semantic_title": "",
    "citation_count": 3,
    "authors": [
      "Julian Risch",
      "Philipp Schmidt",
      "Ralf Krestel"
    ]
  },
  "https://aclanthology.org/2021.woah-1.18": {
    "title": "When the Echo Chamber Shatters: Examining the Use of Community-Specific Language Post-Subreddit Ban",
    "volume": "workshop",
    "abstract": "Community-level bans are a common tool against groups that enable online harassment and harmful speech. Unfortunately, the efficacy of community bans has only been partially studied and with mixed results. Here, we provide a flexible unsupervised methodology to identify in-group language and track user activity on Reddit both before and after the ban of a community (subreddit). We use a simple word frequency divergence to identify uncommon words overrepresented in a given community, not as a proxy for harmful speech but as a linguistic signature of the community. We apply our method to 15 banned subreddits, and find that community response is heterogeneous between subreddits and between users of a subreddit. Top users were more likely to become less active overall, while random users often reduced use of in-group language without decreasing activity. Finally, we find some evidence that the effectiveness of bans aligns with the content of a community. Users of dark humor communities were largely unaffected by bans while users of communities organized around white supremacy and fascism were the most affected. Altogether, our results show that bans do not affect all groups or users equally, and pave the way to understanding the effect of bans across communities",
    "checked": true,
    "id": "c7dc71e727b4760eeeb5a972fdbbc42cff2fea43",
    "semantic_title": "",
    "citation_count": 5,
    "authors": [
      "Milo Trujillo",
      "Sam Rosenblatt",
      "Guillermo de Anda Jáuregui",
      "Emily Moog",
      "Briane Paul V. Samson",
      "Laurent Hébert-Dufresne",
      "Allison M. Roth"
    ]
  },
  "https://aclanthology.org/2021.woah-1.19": {
    "title": "Targets and Aspects in Social Media Hate Speech",
    "volume": "workshop",
    "abstract": "Mainstream research on hate speech focused so far predominantly on the task of classifying mainly social media posts with respect to predefined typologies of rather coarse-grained hate speech categories. This may be sufficient if the goal is to detect and delete abusive language posts. However, removal is not always possible due to the legislation of a country. Also, there is evidence that hate speech cannot be successfully combated by merely removing hate speech posts; they should be countered by education and counter-narratives. For this purpose, we need to identify (i) who is the target in a given hate speech post, and (ii) what aspects (or characteristics) of the target are attributed to the target in the post. As the first approximation, we propose to adapt a generic state-of-the-art concept extraction model to the hate speech domain. The outcome of the experiments is promising and can serve as inspiration for further work on the task",
    "checked": true,
    "id": "6f57d558dae5ab16b3d9c3806cf6ae283fed6fce",
    "semantic_title": "",
    "citation_count": 9,
    "authors": [
      "Alexander Shvets",
      "Paula Fortuna",
      "Juan Soler",
      "Leo Wanner"
    ]
  },
  "https://aclanthology.org/2021.woah-1.20": {
    "title": "Abusive Language on Social Media Through the Legal Looking Glass",
    "volume": "workshop",
    "abstract": "Abusive language is a growing phenomenon on social media platforms. Its effects can reach beyond the online context, contributing to mental or emotional stress on users. Automatic tools for detecting abuse can alleviate the issue. In practice, developing automated methods to detect abusive language relies on good quality data. However, there is currently a lack of standards for creating datasets in the field. These standards include definitions of what is considered abusive language, annotation guidelines and reporting on the process. This paper introduces an annotation framework inspired by legal concepts to define abusive language in the context of online harassment. The framework uses a 7-point Likert scale for labelling instead of class labels. We also present ALYT – a dataset of Abusive Language on YouTube. ALYT includes YouTube comments in English extracted from videos on different controversial topics and labelled by Law students. The comments were sampled from the actual collected data, without artificial methods for increasing the abusive content. The paper describes the annotation process thoroughly, including all its guidelines and training steps",
    "checked": true,
    "id": "0ac6edffa4af1cbc1a58e856570245e26785d3c3",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Thales Bertaglia",
      "Andreea Grigoriu",
      "Michel Dumontier",
      "Gijs van Dijck"
    ]
  },
  "https://aclanthology.org/2021.woah-1.21": {
    "title": "Findings of the WOAH 5 Shared Task on Fine Grained Hateful Memes Detection",
    "volume": "workshop",
    "abstract": "We present the results and main findings of the shared task at WOAH 5 on hateful memes detection. The task include two subtasks relating to distinct challenges in the fine-grained detection of hateful memes: (1) the protected category attacked by the meme and (2) the attack type. 3 teams submitted system description papers. This shared task builds on the hateful memes detection task created by Facebook AI Research in 2020",
    "checked": true,
    "id": "7734560d9c72d37c3d628eea493b12f65025828d",
    "semantic_title": "",
    "citation_count": 8,
    "authors": [
      "Lambert Mathias",
      "Shaoliang Nie",
      "Aida Mostafazadeh Davani",
      "Douwe Kiela",
      "Vinodkumar Prabhakaran",
      "Bertie Vidgen",
      "Zeerak Waseem"
    ]
  },
  "https://aclanthology.org/2021.woah-1.22": {
    "title": "VL-BERT+: Detecting Protected Groups in Hateful Multimodal Memes",
    "volume": "workshop",
    "abstract": "This paper describes our submission (winning solution for Task A) to the Shared Task on Hateful Meme Detection at WOAH 2021. We build our system on top of a state-of-the-art system for binary hateful meme classification that already uses image tags such as race, gender, and web entities. We add further metadata such as emotions and experiment with data augmentation techniques, as hateful instances are underrepresented in the data set",
    "checked": true,
    "id": "0a4164ee00148c36831584937bad44be4353e8ec",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Piush Aggarwal",
      "Michelle Espranita Liman",
      "Darina Gold",
      "Torsten Zesch"
    ]
  },
  "https://aclanthology.org/2021.woah-1.23": {
    "title": "Racist or Sexist Meme? Classifying Memes beyond Hateful",
    "volume": "workshop",
    "abstract": "Memes are the combinations of text and images that are often humorous in nature. But, that may not always be the case, and certain combinations of texts and images may depict hate, referred to as hateful memes. This work presents a multimodal pipeline that takes both visual and textual features from memes into account to (1) identify the protected category (e.g. race, sex etc.) that has been attacked; and (2) detect the type of attack (e.g. contempt, slurs etc.). Our pipeline uses state-of-the-art pre-trained visual and textual representations, followed by a simple logistic regression classifier. We employ our pipeline on the Hateful Memes Challenge dataset with additional newly created fine-grained labels for protected category and type of attack. Our best model achieves an AUROC of 0.96 for identifying the protected category, and 0.97 for detecting the type of attack. We release our code at https://github.com/harisbinzia/HatefulMemes",
    "checked": true,
    "id": "873cd50e44b05db3a73c4ea331e5f73c64fd420b",
    "semantic_title": "",
    "citation_count": 13,
    "authors": [
      "Haris Bin Zia",
      "Ignacio Castro",
      "Gareth Tyson"
    ]
  },
  "https://aclanthology.org/2021.woah-1.24": {
    "title": "Multimodal or Text? Retrieval or BERT? Benchmarking Classifiers for the Shared Task on Hateful Memes",
    "volume": "workshop",
    "abstract": "The Shared Task on Hateful Memes is a challenge that aims at the detection of hateful content in memes by inviting the implementation of systems that understand memes, potentially by combining image and textual information. The challenge consists of three detection tasks: hate, protected category and attack type. The first is a binary classification task, while the other two are multi-label classification tasks. Our participation included a text-based BERT baseline (TxtBERT), the same but adding information from the image (ImgBERT), and neural retrieval approaches. We also experimented with retrieval augmented classification models. We found that an ensemble of TxtBERT and ImgBERT achieves the best performance in terms of ROC AUC score in two out of the three tasks on our development set",
    "checked": true,
    "id": "706e3fb58b38c1eedb4155b52cefadb8824efe2c",
    "semantic_title": "",
    "citation_count": 2,
    "authors": [
      "Vasiliki Kougia",
      "John Pavlopoulos"
    ]
  }
}