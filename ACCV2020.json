{
  "https://openaccess.thecvf.com/content/ACCV2020/html/Zheng_Localin_Reshuffle_Net_Toward_Naturally_and_Efficiently_Facial_Image_Blending_ACCV_2020_paper.html": {
    "title": "Localin Reshuffle Net: Toward Naturally and Efficiently Facial Image Blending",
    "volume": "main",
    "abstract": "The blending of facial images is an effective way to fuse attributes such that the synthesis is robust to the finer details (e.g., periocular-region, nostrils, hairlines). Specifically, facial blending aims to transfer the style of a source image to a target such that violations in the natural appearance are minimized. Despite the many practical applications, facial image blending remains mostly unexplored with the reasons being two-fold: 1) the lack of quality paired data for supervision and 2) facial synthesizers (i.e., the models) are sensitive to small variations in lighting, texture, resolution and age. We address the reasons for the bottleneck by first building Facial Pairs to Blend (FPB) dataset, which was generated through our facial attribute optimization algorithm. Then, we propose an effective normalization scheme to capture local statistical information during blending: namely, Local Instance Normalization (LAN). Lastly, a novel local-reshuffle-layer is designed to map local patches in the feature space, which can be learned in an end-to-end fashion with dedicated loss. This new layer is essential for the proposed Localin Reshuffle Network (LRNet). Extensive experiments, and both quantitative and qualitative results, demonstrate that our approach outperforms existing methods",
    "checked": true,
    "id": "0f4b6174b74d4a8d5d6d8407aa7906d1a5e5ae88",
    "semantic_title": "localin reshuffle net: toward naturally and efficiently facial image blending",
    "citation_count": 2,
    "authors": [
      "Chengyao Zheng",
      "Siyu Xia",
      "Joseph Robinson",
      "Changsheng Lu",
      "Wayne Wu",
      "Chen Qian",
      "Ming Shao"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Xu_Fast_and_Differentiable_Message_Passing_on_Pairwise_Markov_Random_Fields_ACCV_2020_paper.html": {
    "title": "Fast and Differentiable Message Passing on Pairwise Markov Random Fields",
    "volume": "main",
    "abstract": "Despite the availability of many Markov Random Field (MRF) optimization algorithms, their widespread usage is currently limited due to imperfect MRF modelling arising from hand-crafted model parameters and the selection of inferior inference algorithm. In addition to differentiability, the two main aspects that enable learning these model parameters are the forward and backward propagation time of the MRF optimization algorithm and its inference capabilities. In this work, we introduce two fast and differentiable message passing algorithms, namely, Iterative Semi-Global Matching Revised (ISGMR) and Parallel Tree-Reweighted Message Passing (TRWP) which are greatly sped up on a GPU by exploiting massive parallelism. Specifically, ISGMR is an iterative and revised version of the standard SGM for general pairwise MRFs with improved optimization effectiveness, and TRWP is a highly parallel version of Sequential TRW (TRWS) for faster optimization. Our experiments on the standard stereo and denoising benchmarks demonstrated that ISGMR and TRWP achieve much lower energies than SGM and Mean-Field (MF), and TRWP is two orders of magnitude faster than TRWS without losing effectiveness in optimization. We further demonstrated the effectiveness of our algorithms on end-to-end learning for semantic segmentation. Notably, our CUDA implementations are at least 7 and 700 times faster than PyTorch GPU implementations for forward and backward propagation respectively, enabling efficient end-to-end learning with message passing",
    "checked": true,
    "id": "f4de6932855aa34ba3ca8ef7c4efcfe2c2695c24",
    "semantic_title": "fast and differentiable message passing on pairwise markov random fields",
    "citation_count": 3,
    "authors": [
      "Zhiwei Xu",
      "Thalaiyasingam Ajanthan",
      "Richard Hartley"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Chao_Adversarial_Refinement_Network_for_Human_Motion_Prediction_ACCV_2020_paper.html": {
    "title": "Adversarial Refinement Network for Human Motion Prediction",
    "volume": "main",
    "abstract": "Human motion prediction aims to predict future 3D skeletal sequences by giving a limited human motion as inputs. Two popular methods, recurrent neural networks and feed-forward deep networks, are able to predict rough motion trend, but motion details such as limb movement may be lost. To predict more accurate future human motion, we propose an Adversarial Refinement Network (ARNet) following a simple yet effective coarse-to-fine mechanism with novel adversarial error augmentation. Specifically, we take both the historical motion sequences and coarse prediction as input of our cascaded refinement network to predict refined human motion and strengthen the refinement network with adversarial error augmentation. During training, we deliberately introduce the error distribution by learning through the adversarial mechanism among different subjects. In testing, our cascaded refinement network alleviates the prediction error from the coarse predictor resulting in accurate prediction robustly. This adversarial error augmentation provides rich error cases as input to our refinement network, leading to better generalization performance on the testing dataset. We conduct extensive experiments on three standard benchmark datasets and show that our proposed ARNet outperforms other state-of-the-art methods, especially on challenging aperiodic actions in both short-term and long-term predictions",
    "checked": true,
    "id": "5a0be8828ada98ed3f91415ebd48638d02377a56",
    "semantic_title": "adversarial refinement network for human motion prediction",
    "citation_count": 5,
    "authors": [
      "Xianjin Chao",
      "Yanrui Bin",
      "Wenqing Chu",
      "Xuan Cao",
      "Yanhao Ge",
      "Chengjie Wang",
      "Jilin Li",
      "Feiyue Huang",
      "Howard Leung"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Cao_Over-exposure_Correction_via_Exposure_and_Scene_Information_Disentanglement_ACCV_2020_paper.html": {
    "title": "Over-exposure Correction via Exposure and Scene Information Disentanglement",
    "volume": "main",
    "abstract": "Over-exposure correction is an important problem of great consequence to social media industries. In this paper, we propose a novel model to tackle this task. Considering that reasonable enhanced results can still vary in terms of exposure, we do not strictly enforce the model to generate identical results with ground-truth images. On the contrary, we train the network to recover the lost scene information according to the existing information of the over-exposure images and generate naturalness-preserved images. Experiments compared with several state-of-the-art methods show the superior performance of the proposed network. Besides, we also verify our hypothesis with ablation studies. Our source code is available at https://github.com/0x437968/overexposure-correction-dise",
    "checked": true,
    "id": "928b8c35a84e7e51748000c69b67db83e8c3dff1",
    "semantic_title": "over-exposure correction via exposure and scene information disentanglement",
    "citation_count": 6,
    "authors": [
      "Yuhui Cao",
      "Yurui Ren",
      "Thomas H. Li",
      "Ge Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Shi_Accurate_Arbitrary-Shaped_Scene_Text_Detection_via_Iterative_Polynomial_Parameter_Regression_ACCV_2020_paper.html": {
    "title": "Accurate Arbitrary-Shaped Scene Text Detection via Iterative Polynomial Parameter Regression",
    "volume": "main",
    "abstract": "A number of scene text in natural images have irregular shapes which often cause significant difficulties for a text detector. In this paper, we propose a robust scene text detection method based on a parameterized shape modeling and regression scheme for text with arbitrary shapes. The shape model geometrically depicts a text region with a polynomial centerline and a series of width cues to capture global shape characteristics (e.g. smoothness) and local shapes of the text respectively for accurate text localization, which differs from previous text region modeling schemes based on discrete boundary points or pixels. We further propose a text detection network PolyPRNet equipped with an iterative regression module for text's shape parameters, which effectively enhances the detection accuracy of arbitrary-shaped text. Our method achieves state-of-the-art text detection results on several standard benchmarks",
    "checked": true,
    "id": "59716fa03330552ce88efcdf58786108462db068",
    "semantic_title": "accurate arbitrary-shaped scene text detection via iterative polynomial parameter regression",
    "citation_count": 5,
    "authors": [
      "Jiahao Shi",
      "Long Chen",
      "Feng Su"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Le_Cacheux_Webly_Supervised_Semantic_Embeddings_for_Large_Scale_Zero-Shot_Learning_ACCV_2020_paper.html": {
    "title": "Webly Supervised Semantic Embeddings for Large Scale Zero-Shot Learning",
    "volume": "main",
    "abstract": "Zero-shot learning (ZSL) makes object recognition in images possible in absence of visual training data for a part of the classes from a dataset. When the number of classes is large, classes are usually represented by semantic class prototypes learned automatically from unannotated text collections. This typically leads to much lower performances than with manually designed semantic prototypes such as attributes. While most ZSL works focus on the visual aspect and reuse standard semantic prototypes learned from generic text collections, we focus on the problem of semantic class prototype design for large scale ZSL. More specifically, we investigate the use of noisy textual metadata associated to photos as text collections, as we hypothesize they are likely to provide more plausible semantic embeddings for visual classes if exploited appropriately. We thus make use of a source-based filtering strategy to improve the robustness of semantic prototypes. Evaluation on the large scale ImageNet dataset shows a significant improvement in ZSL performances over two strong baselines, and over usual semantic embeddings used in previous works. We show that this improvement is obtained for several embedding methods, leading to state of the art results when one uses automatically created visual and text features",
    "checked": true,
    "id": "4c0e03c15b69d5217d5d87998ff94cc5da1b9cd0",
    "semantic_title": "webly supervised semantic embeddings for large scale zero-shot learning",
    "citation_count": 6,
    "authors": [
      "Yannick Le Cacheux",
      "Adrian Popescu",
      "Herve Le Borgne"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Ye_SpotPatch_Parameter-Efficient_Transfer_Learning_for_Mobile_Object_Detection_ACCV_2020_paper.html": {
    "title": "SpotPatch: Parameter-Efficient Transfer Learning for Mobile Object Detection",
    "volume": "main",
    "abstract": "Deep learning based object detectors are commonly deployed on mobile devices to solve a variety of tasks. For maximum accuracy, each detector is usually trained to solve one single specific task, and comes with a completely independent set of parameters. While this guarantees high performance, it is also highly inefficient, as each model has to be separately downloaded and stored. In this paper we address the question: can task-specific detectors be trained and represented as a shared set of weights, plus a very small set of additional weights for each task? The main contributions of this paper are the following: 1) we perform the first systematic study of parameter-efficient transfer learning techniques for object detection problems; 2) we propose a technique to learn a model patch with a size that is dependent on the difficulty of the task to be learned, and validate our approach on 10 different object detection tasks. Our approach achieves similar accuracy as previously proposed approaches, while being significantly more compact",
    "checked": true,
    "id": "a74c4923b5dc5789acceb2d6bdac2b0d038c862f",
    "semantic_title": "spotpatch: parameter-efficient transfer learning for mobile object detection",
    "citation_count": 6,
    "authors": [
      "Keren Ye",
      "Adriana Kovashka",
      "Mark Sandler",
      "Menglong Zhu",
      "Andrew Howard",
      "Marco Fornoni"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Wang_In_Defense_of_LSTMs_for_Addressing_Multiple_Instance_Learning_Problems_ACCV_2020_paper.html": {
    "title": "In Defense of LSTMs for Addressing Multiple Instance Learning Problems",
    "volume": "main",
    "abstract": "LSTMs have a proven track record in analyzing sequential data. But what about unordered instance bags, as found under a Multiple Instance Learning (MIL) setting? While not often used for this, we show LSTMs excell under this setting too. In addition, we show thatLSTMs are capable of indirectly capturing instance-level information us-ing only bag-level annotations. Thus, they can be used to learn instance-level models in a weakly supervised manner. Our empirical evaluation on both simplified (MNIST) and realistic (Lookbook and Histopathology) datasets shows that LSTMs are competitive with or even surpass state-of-the-art methods specially designed for handling specific MIL problems. Moreover, we show that their performance on instance-level prediction is close to that of fully-supervised methods",
    "checked": true,
    "id": "bc975eb6f143a398c6b5b5db1ea956528e6507d7",
    "semantic_title": "in defense of lstms for addressing multiple instance learning problems",
    "citation_count": 5,
    "authors": [
      "Kaili Wang",
      "Jose Oramas",
      "Tinne Tuytelaars"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Amirian_OpenTraj_Assessing_Prediction_Complexity_in_Human_Trajectories_Datasets_ACCV_2020_paper.html": {
    "title": "OpenTraj: Assessing Prediction Complexity in Human Trajectories Datasets",
    "volume": "main",
    "abstract": "Human Trajectory Prediction (HTP) having gained much momentum in the last years, this paper addresses the question of evaluating how complex is a given dataset with respect to the prediction problem. For assessing a dataset complexity, we define a series of indicators around three concepts: Trajectory predictability; Trajectory regularity; Context complexity. We compare the most common datasets used in HTP at the light of these indicators and discuss what this may imply on benchmarking of HTP algorithms",
    "checked": true,
    "id": "e64f46dc296ad5cdabcd151e2d398a264d5494e5",
    "semantic_title": "opentraj: assessing prediction complexity in human trajectories datasets",
    "citation_count": 23,
    "authors": [
      "Javad Amirian",
      "Bingqing Zhang",
      "Francisco Valente Castro",
      "Juan Jose Baldelomar",
      "Jean-Bernard Hayet",
      "Julien Pettre"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Yu_Encode_the_Unseen_Predictive_Video_Hashing_for_Scalable_Mid-Stream_Retrieval_ACCV_2020_paper.html": {
    "title": "Encode the Unseen: Predictive Video Hashing for Scalable Mid-Stream Retrieval",
    "volume": "main",
    "abstract": "This paper tackles a new problem in computer vision: mid-stream video-to-video retrieval. This task, which consists in searching a database for content similar to a video right as it is playing, e.g. from a live stream, exhibits challenging characteristics. Only the beginning part of the video is available as query and new frames are constantly added as the video plays out. To perform retrieval in this demanding situation, we propose an approach based on a binary encoder that is both predictive and incremental in order to (1) account for the missing video content at query time and (2) keep up with repeated, continuously evolving queries throughout the streaming. In particular, we present the first hashing framework that infers the unseen future content of a currently playing video. Experiments on FCVID and ActivityNet demonstrate the feasibility of this task. Our approach also yields a significant mAP@20 performance increase compared to a baseline adapted from the literature for this task, for instance 7.4% (2.6%) increase at 20% (50%) of elapsed runtime on FCVID using bitcodes of size 192 bits",
    "checked": true,
    "id": "2c386b73f74c2e7f91217bb65504d8725e17b81e",
    "semantic_title": "encode the unseen: predictive video hashing for scalable mid-stream retrieval",
    "citation_count": 3,
    "authors": [
      "Tong Yu",
      "Nicolas Padoy"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Zhang_MLIFeat_Multi-level_information_fusion_based_deep_local_features_ACCV_2020_paper.html": {
    "title": "MLIFeat: Multi-level information fusion based deep local features",
    "volume": "main",
    "abstract": "Accurate image keypoints detection and description are of central importance in a wide range of applications. Although there are various studies proposed to address these challenging tasks, they are far from optimal. In this paper, we devise a model named MLIFeat with two novel light-weight modules for multi-level information fusion based deep local features learning, to cope with both the image keypoints detection and description. On the one hand, the image keypoints are robustly detected by our Feature Shuffle Module (FSM), which can efficiently utilize the multi-level convolutional feature maps with marginal computing cost. On the other hand, the corresponding feature descriptors are generated by our well-designed Feature Blend Module (FBM), which can collect and extract the most useful information from the multi-level convolutional feature vectors. To study in-depth about our MLIFeat and other state-of-the-art methods, we have conducted thorough experiments, including image matching on HPatches and FM-Bench, and visual localization on Aachen-Day-Night, which verifies the robustness and effectiveness of our proposed model. Code at:https://github.com/yyangzh/MLIFeat",
    "checked": true,
    "id": "9ab3d403120b2ffab9f6844862d92f00929c5b8a",
    "semantic_title": "mlifeat: multi-level information fusion based deep local features",
    "citation_count": 9,
    "authors": [
      "Yuyang Zhang",
      "Jinge Wang",
      "Shibiao Xu",
      "Xiao Liu",
      "Xiaopeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Hossain_Video-Based_Crowd_Counting_Using_a_Multi-Scale_Optical_Flow_Pyramid_Network_ACCV_2020_paper.html": {
    "title": "Video-Based Crowd Counting Using a Multi-Scale Optical Flow Pyramid Network",
    "volume": "main",
    "abstract": "This paper presents a novel approach to the task of video-based crowd counting, which can be formalized as the regression problem of learning a mapping from an input image to an output crowd density map. Convolutional neural networks (CNNs) have demonstrated striking accuracy gains in a range of computer vision tasks, including crowd counting. However, the dominant focus within the crowd counting literature has been on the single-frame case or applying CNNs to videos in a frame-by-frame fashion without leveraging motion information. This paper proposes a novel architecture that exploits the spatiotemporal information captured in a video stream by combining an optical flow pyramid with an appearance-based CNN. Extensive empirical evaluation on five public datasets comparing against numerous state-of-the-art approaches demonstrates the efficacy of the proposed architecture, with our methods reporting best results on all datasets. Finally, a set of transfer learning experiments shows that, once the proposed model is trained on one dataset, it can be transferred to another using a limited number of training examples and still exhibit high accuracy",
    "checked": true,
    "id": "64c37e58d706ce429e379ed05097762ed737a87a",
    "semantic_title": "video-based crowd counting using a multi-scale optical flow pyramid network",
    "citation_count": 8,
    "authors": [
      "Mohammad Asiful Hossain",
      "Kevin Cannons",
      "Daesik Jang",
      "Fabio Cuzzolin",
      "Zhan Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Liao_Speech2Video_Synthesis_with_3D_Skeleton_Regularization_and_Expressive_Body_Poses_ACCV_2020_paper.html": {
    "title": "Speech2Video Synthesis with 3D Skeleton Regularization and Expressive Body Poses",
    "volume": "main",
    "abstract": "In this paper, we propose a novel approach to convert a given speech audio to a photo-realistic speaking video of a specific person, where the output video has synchronized, realistic and expressive rich body dynamics. We achieve this by first generating 3D skeleton movements from the audio sequence using a recurrent neural network (RNN), and then synthesizing the output video via a conditional generative adversarial network (GAN). To make the skeleton movement realistic and expressive, we embed the knowledge of an articulated 3D human skeleton and a learned dictionary of personal speech iconic gestures into the generation process in both learning and testing pipelines. The former prevents the generation of unreasonable body distortion, while the later helps our model quickly learn meaningful body movement through a few recorded videos. To produce photo-realistic and high-resolution video with motion details, we propose to insert part attention mechanisms in the conditional GAN, where each detailed part, e.g. head and hand, is automatically zoomed in to have their own discriminators. To validate our approach, we collect a dataset with 20 high-quality videos from 1 male and 1 female model reading various documents under different topics. Compared with previous SoTA pipelines handling similar tasks, our approach achieves better results by a user study",
    "checked": true,
    "id": "01ec21b137f052d80a5501fe725a72f0ad2ccc7e",
    "semantic_title": "speech2video synthesis with 3d skeleton regularization and expressive body poses",
    "citation_count": 15,
    "authors": [
      "Miao Liao",
      "Sibo Zhang",
      "Peng Wang",
      "Hao Zhu",
      "Xinxin Zuo",
      "Ruigang Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Barroso-Laguna_HDD-Net_Hybrid_Detector_Descriptor_with_Mutual_Interactive_Learning_ACCV_2020_paper.html": {
    "title": "HDD-Net: Hybrid Detector Descriptor with Mutual Interactive Learning",
    "volume": "main",
    "abstract": "Local feature extraction remains an active research area due to the advances in fields such as SLAM, 3D reconstructions, or AR applications. The success in these applications relies on the performance of the feature detector, descriptor, and its matching process. While the trend of detector-descriptor interaction of most methods is based on unifying the two into a single network, we propose an alternative approach that treats both components independently and focuses on their interaction during the learning process. We formulate the classical hard-mining triplet loss as a new detector optimisation term to improve keypoint positions based on the descriptor map. Moreover, we introduce a dense descriptor that uses a multi-scale approach within the architecture and a hybrid combination of hand-crafted and learnt features to obtain rotation and scale robustness by design. We evaluate our method extensively on several benchmarks and show improvements over the state of the art in terms of image matching and 3D reconstruction quality while keeping on par in camera localisation tasks",
    "checked": true,
    "id": "8e85ffb9d03f9b42cdb294f8fff2f44f753e3b5d",
    "semantic_title": "hdd-net: hybrid detector descriptor with mutual interactive learning",
    "citation_count": 17,
    "authors": [
      "Axel Barroso-Laguna",
      "Yannick Verdie",
      "Benjamin Busam",
      "Krystian Mikolajczyk"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Jeong_DoFNet_Depth_of_Field_Difference_Learning_for_Detecting_Image_Forgery_ACCV_2020_paper.html": {
    "title": "DoFNet: Depth of Field Difference Learning for Detecting Image Forgery",
    "volume": "main",
    "abstract": "Recently, online transactions have had exponential growth and expanded to various cases, such as opening bank accounts and filing for insurance claims. Despite the effort of many companies requiring their own mobile applications to capture images for online transactions, it is difficult to restrict users from taking a picture of other's images displayed on a screen. To detect such cases, we propose a novel approach using paired images with different depth of field (DoF) for distinguishing the real images and the display images. Also, we introduce a new dataset containing 2,752 pairs of images capturing real and display objects on various types of displays, which is the largest real dataset employing DoF with multi-focus. Furthermore, we develop a new framework to concentrate on the difference of DoF in paired images, while avoiding learning individual display artifacts. Since DoF lies on the optical fundamentals, the framework can be widely utilized with any camera, and its performance shows at least 23% improvement compared to the conventional classification models",
    "checked": true,
    "id": "dc4119969cd855311fede882020693b1a237438a",
    "semantic_title": "dofnet: depth of field difference learning for detecting image forgery",
    "citation_count": 6,
    "authors": [
      "Yonghyun Jeong",
      "Jongwon Choi",
      "Doyeon Kim",
      "Sehyeon Park",
      "Minki Hong",
      "Changhyun Park",
      "Seungjai Min",
      "Youngjune Gwon"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Minami_Knowledge_Transfer_Graph_for_Deep_Collaborative_Learning_ACCV_2020_paper.html": {
    "title": "Knowledge Transfer Graph for Deep Collaborative Learning",
    "volume": "main",
    "abstract": "Knowledge transfer among multiple networks using their outputs or intermediate activations have evolved through manual design from a simple teacher-student approach to a bidirectional cohort one. The major components of such knowledge transfer framework involve the network size, the number of networks, the transfer direction, and the design of the loss function. However, because these factors are enormous when combined and become intricately entangled, the methods of conventional knowledge transfer have explored only limited combinations. In this paper, we propose a novel graph representation called knowledge transfer graph that provides a unified view of the knowledge transfer and has the potential to represent diverse knowledge transfer patterns. We also propose four gate functions that control the gradient and can deliver diverse combinations of knowledge transfer. Searching the graph structure enables us to discover more effective knowledge transfer methods than a manually designed one. Experimental results show that the proposed method achieved performance improvements",
    "checked": true,
    "id": "3117ac0b3946a06218eeb1cdf2c4005e8ed5b866",
    "semantic_title": "knowledge transfer graph for deep collaborative learning",
    "citation_count": 8,
    "authors": [
      "Soma Minami",
      "Tsubasa Hirakawa",
      "Takayoshi Yamashita",
      "Hironobu Fujiyoshi"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Shi_Decoupled_Spatial-Temporal_Attention_Network_for_Skeleton-Based_Action-Gesture_Recognition_ACCV_2020_paper.html": {
    "title": "Decoupled Spatial-Temporal Attention Network for Skeleton-Based Action-Gesture Recognition",
    "volume": "main",
    "abstract": "Dynamic skeletal data, represented as the 2D/3D coordinates of human joints, has been widely studied for human action recognition due to its high-level semantic information and environmental robustness. However, previous methods heavily rely on designing hand-crafted traversal rules or graph topologies to draw dependencies between the joints, which are limited in performance and generalizability. In this work, we present a novel decoupled spatial-temporal attention network(DSTA-Net) for skeleton-based action recognition. It involves solely the attention blocks, allowing for modeling spatial-temporal dependencies between joints without the requirement of knowing their positions or mutual connections. Specifically, to meet the specific requirements of the skeletal data, three techniques are proposed for building attention blocks, namely, spatial-temporal attention decoupling, decoupled position encoding and spatial global regularization. Besides, from the data aspect, we introduce a skeletal data decoupling technique to emphasize the specific characteristics of space/time and different motion scales, resulting in a more comprehensive understanding of the human actions.To test the effectiveness of the proposed method, extensive experiments are conducted on four challenging datasets for skeleton-based gesture and action recognition, namely, SHREC, DHG, NTU-60 and NTU-120, where DSTA-Net achieves state-of-the-art performance on all of them",
    "checked": true,
    "id": "32e769a32b3ab458880796608f9e8f338c78d9cd",
    "semantic_title": "decoupled spatial-temporal attention network for skeleton-based action-gesture recognition",
    "citation_count": 87,
    "authors": [
      "Lei Shi",
      "Yifan Zhang",
      "Jian Cheng",
      "Hanqing Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Lei_An_Efficient_Group_Feature_Fusion_Residual_Network_for_Image_Super-Resolution_ACCV_2020_paper.html": {
    "title": "An Efficient Group Feature Fusion Residual Network for Image Super-Resolution",
    "volume": "main",
    "abstract": "Convolutional neural networks (CNNs) have made great breakthrough in the field of image super-resolution (SR). However, most current methods are usually to improve their performance by simply increasing the depth of their network. Although this strategy can get promising results, it is inefficient in many real-world scenarios because of the high computational cost. In this paper, we propose an efficient group feature fusion residual network (GFFRN) for image super-resolution. In detail, we design a novel group feature fusion residual block (GFFRB) to group and fuse the features of the intermediate layer. In this way, GFFRB can enjoy the merits of the lightweight of the group convolution and the high-efficiency of the skip connections, thus achieving better performance compared with most current residual blocks. Experiments on the benchmark test sets show that our models are more efficient than most of the state-of-the-art methods",
    "checked": true,
    "id": "190a8dd188e44d441dab879840cc00ce7cf8b1fa",
    "semantic_title": "an efficient group feature fusion residual network for image super-resolution",
    "citation_count": 1,
    "authors": [
      "Pengcheng Lei",
      "Cong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Kubade_AFN_Attentional_Feedback_Network_based_3D_Terrain_Super-Resolution_ACCV_2020_paper.html": {
    "title": "AFN: Attentional Feedback Network based 3D Terrain Super-Resolution",
    "volume": "main",
    "abstract": "Terrain, representing features of an earth surface, plays a crucial role in many applications such as simulations, route planning, analysis of surface dynamics, computer graphics-based games, entertainment, films, to name a few. With recent advancements in digital technology, these applications demand the presence of high resolution details in the terrain. In this paper, we propose a novel fully convolutional neural network based super-resolution architecture to increase the resolution of low-resolution Digital Elevation Model (LRDEM) with the help of information extracted from the corresponding aerial image as a complementary modality. We perform the super-resolution of LRDEM using an attention based feedback mechanism named 'Attentional Feedback Network' (AFN), which selectively fuses the information from LRDEM and aerial image to enhance and infuse the high-frequency features and to produce the terrain realistically . We compare the proposed architecture with existing state-of-the-art DEM super-resolution methods and show that the proposed architecture outperforms enhancing the resolution of input LRDEM accurately and in a realistic manner",
    "checked": true,
    "id": "d680dd0155731924476e5c4c1f93dde42242261d",
    "semantic_title": "afn: attentional feedback network based 3d terrain super-resolution",
    "citation_count": 9,
    "authors": [
      "Ashish Kubade",
      "Diptiben Patel",
      "Avinash Sharma",
      "K. S. Rajan"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Guo_Domain_Adaptation_Gaze_Estimation_by_Embedding_with_Prediction_Consistency_ACCV_2020_paper.html": {
    "title": "Domain Adaptation Gaze Estimation by Embedding with Prediction Consistency",
    "volume": "main",
    "abstract": "Gaze is the essential manifestation of human attention. In recent years, a series of work has achieved high accuracy in gaze estimation. However, the inter-personal difference limits the reduction of the subject-independent gaze estimation error. This paper proposes an unsupervised method for domain adaptation gaze estimation to eliminate the impact of inter-personal diversity. In domain adaption, we design an embedding representation with prediction consistency to ensure that linear relationships between gaze directions in different domains remain consistent on gaze space and embedding space. Specifically, we employ source gaze to form a locally linear representation in the gaze space for each target domain prediction. Then the same linear combinations are applied in the embedding space to generate hypothesis embedding for the target domain sample, remaining prediction consistency. The deviation between the target and source domain is reduced by approximating the predicted and hypothesis embedding for the target domain sample. Guided by the proposed strategy, we design Domain Adaptation Gaze Estimation Network(DAGEN), which learns embedding with prediction consistency and achieves state-of-the-art results on both the MPIIGaze and the EYEDIAP datasets",
    "checked": true,
    "id": "9cd8d034398a9517e0828295a4391f5bc7d96748",
    "semantic_title": "domain adaptation gaze estimation by embedding with prediction consistency",
    "citation_count": 28,
    "authors": [
      "Zidong Guo",
      "Zejian Yuan",
      "Chong Zhang",
      "Wanchao Chi",
      "Yonggen Ling",
      "Shenghao Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Liu_Sketch-to-Art_Synthesizing_Stylized_Art_Images_From_Sketches_ACCV_2020_paper.html": {
    "title": "Sketch-to-Art: Synthesizing Stylized Art Images From Sketches",
    "volume": "main",
    "abstract": "We propose a new approach for synthesizing fully detailed art-stylized images from sketches. Given a sketch, with no semantic tagging, and a reference image of a specific style, the model can synthesize meaningful details with colors and textures. Based on the GAN framework, the model consists of three novel modules designed explicitly for better artistic style capturing and generation. To enforce the content faithfulness, we introduce the dual-masked mechanism which directly shapes the feature maps according to sketch. To capture more artistic style aspects, we design feature-map transformation for a better style consistency to the reference image. Finally, an inverse process of instance-normalization disentangles the style and content information and further improves the synthesis quality. Experiments demonstrate a significant qualitative and quantitative boost over baseline models based on previous state-of-the-art techniques, modified for the proposed task (17% better Frechet Inception distance and 18% better style classification score). Moreover, the lightweight design of the proposed modules enables the high-quality synthesis at 512 * 512 resolution",
    "checked": true,
    "id": "f7207b9740a6c319e4680ca85b7429054f639067",
    "semantic_title": "sketch-to-art: synthesizing stylized art images from sketches",
    "citation_count": 23,
    "authors": [
      "Bingchen Liu",
      "Kunpeng Song",
      "Yizhe Zhu",
      "Ahmed Elgammal"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Le_Minh_Ngo_Unified_Application_of_Style_Transfer_for_Face_Swapping_and_Reenactment_ACCV_2020_paper.html": {
    "title": "Unified Application of Style Transfer for Face Swapping and Reenactment",
    "volume": "main",
    "abstract": "Face reenactment and face swap have gained a lot of attention due to their broad range of applications in computer vision. Although both tasks share similar objectives (e.g. manipulating expression and pose), existing methods do not explore the benefits of combining these two tasks.In this paper, we introduce a unified end-to-end pipeline for face swapping and reenactment. We propose a novel approach to isolated disentangled representation learning of specific visual attributes in an unsupervised manner. A combination of the proposed training losses allows us to synthesize results in a one-shot manner. The proposed method does not require subject-specific training.We compare our method against state-of-the-art methods for multiple public datasets of different complexities. The proposed method outperforms other SOTA methods in terms of realistic-looking face images",
    "checked": true,
    "id": "73bd839643b8d1647119b68fa0918770c81d060a",
    "semantic_title": "unified application of style transfer for face swapping and reenactment",
    "citation_count": 7,
    "authors": [
      "Le Minh Ngo",
      "Christian aan de Wiel",
      "Sezer Karaoglu",
      "Theo Gevers"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Cheng_Faster_Better_and_More_Detailed_3D_Face_Reconstruction_with_Graph_ACCV_2020_paper.html": {
    "title": "Faster, Better and More Detailed: 3D Face Reconstruction with Graph Convolutional Networks",
    "volume": "main",
    "abstract": "This paper addresses the problem of 3D face reconstruction from a single image. While available solutions for addressing this problem do exist, to our knowledge, we propose the very first approach which is robust, lightweight and detailed i.e. it can reconstruct fine facial details. Our system is extremely simple and consists of 3 key components: (a) a lightweight non-parametric decoder based on Graph Convolutional Networks (GCNs) trained in a supervised manner to reconstruct coarse facial geometry from image-based ResNet features. (b) An extremely lightweight (35K parameters) subnetwork -- also based on GCNs -- which is trained in an unsupervised manner to refine the output of the first network. (c) A novel feature-sampling mechanism and adaptation layer which injects fine details from the ResNet features of the first network into the second one. Overall, our system is the first one (to our knowledge) to reconstruct detailed facial geometry relying solely on GCNs. We exhaustively compare our method with 7 state-of-the-art methods on 3 datasets reporting state-of-the-art results for all of our experiments, both qualitatively and quantitatively, with our approach being, at the same time, significantly faster",
    "checked": true,
    "id": "ac8d3ece71bb68860d46d2846715121e0fb8b09c",
    "semantic_title": "faster, better and more detailed: 3d face reconstruction with graph convolutional networks",
    "citation_count": 12,
    "authors": [
      "Shiyang Cheng",
      "Georgios Tzimiropoulos",
      "Jie Shen",
      "Maja Pantic"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Song_Learning_End-to-End_Action_Interaction_by_Paired-Embedding_Data_Augmentation_ACCV_2020_paper.html": {
    "title": "Learning End-to-End Action Interaction by Paired-Embedding Data Augmentation",
    "volume": "main",
    "abstract": "In recognition-based action interaction, robots' responses to human actions are often pre-designed according to recognized categories and thus stiff.In this paper, we specify a new Interactive Action Translation (IAT) task which aims to learn end-to-end action interaction from unlabeled interactive pairs, removing explicit action recognition.To enable learning on small-scale data, we propose a Paired-Embedding (PE) method for effective and reliable data augmentation.Specifically, our method first utilizes paired relationships to cluster individual actions in an embedding space.Then two actions originally paired can be replaced with other actions in their respective neighborhood, assembling into new pairs.An Act2Act network based on conditional GAN follows to learn from augmented data.Besides, IAT-test and IAT-train scores are specifically proposed for evaluating methods on our task.Experimental results on two datasets show impressive effects and broad application prospects of our method",
    "checked": true,
    "id": "94a0271b0ccc6312765891242c4b9332132468e2",
    "semantic_title": "learning end-to-end action interaction by paired-embedding data augmentation",
    "citation_count": 1,
    "authors": [
      "Ziyang Song",
      "Zejian Yuan",
      "Chong Zhang",
      "Wanchao Chi",
      "Yonggen Ling",
      "Shenghao Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Choi_Visual_Tracking_by_TridentAlign_and_Context_Embedding_ACCV_2020_paper.html": {
    "title": "Visual Tracking by TridentAlign and Context Embedding",
    "volume": "main",
    "abstract": "Recent advances in Siamese network-based visual tracking methods have enabled high performance on numerous tracking benchmarks. However, extensive scale variations of the target object and distractor objects with similar categories have consistently posed challenges in visual tracking. To address these persisting issues, we propose novel TridentAlign and context embedding modules for Siamese network-based visual tracking methods. The TridentAlign module facilitates adaptability to extensive scale variations and large deformations of the target, where it pools the feature representation of the target object into multiple spatial dimensions to form a feature pyramid, which is then utilized in the region proposal stage. Meanwhile, context embedding module aims to discriminate the target from distractor objects by accounting for the global context information among objects. The context embedding module extracts and embeds the global context information of a given frame into a local feature representation such that the information can be utilized in the final classification stage. Experimental results obtained on multiple benchmark datasets show that the performance of the proposed tracker is comparable to that of state-of-the-art trackers, while the proposed tracker runs at real-time speed",
    "checked": true,
    "id": "15a30f481f3b604888c2ae6048b9ec9a8a217474",
    "semantic_title": "visual tracking by tridentalign and context embedding",
    "citation_count": 14,
    "authors": [
      "Janghoon Choi",
      "Junseok Kwon",
      "Kyoung Mu Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Dang_HPGCNN_Hierarchical_Parallel_Group_Convolutional_Neural_Networks_for_Point_Clouds_ACCV_2020_paper.html": {
    "title": "HPGCNN: Hierarchical Parallel Group Convolutional Neural Networks for Point Clouds Processing",
    "volume": "main",
    "abstract": "To achieve high performance but less complexity for point clouds processing, we introduce HPGCNN, an efficient and lightweight neural architecture. The key component in our approach is the Hierarchical Parallel Group Convolution(HPGConv) operation. It can capture both the discriminative independent single-point features and local geometric features of point clouds at the same time to enhance the richness of the features with less redundant information by designing two hierarchical parallel group convolutions, which is helpful to recognize elusive shapes. To significantly further reduce complexity and natively prevent overfitting, we use global average pooling and a full connected layer instead of the traditional three full connected layers for classification. Moreover, to further capture the contextual fine-grained features with higher-level semantics, we introduce a novel multi-semantic scale strategy to progressively increase the receptive field of each local area through the information communication of local areas of different scales. Extensive experiments show that our HPGCNN clearly surpasses state-of-the-art approaches for point clouds classification dataset ModelNet40 and large scale semantic segmentation datasets ShapeNet Parts, S3DIS, vKITTI and SemanticKITTI in terms of accuracy and complexity",
    "checked": true,
    "id": "93be92cb561745313cf172d0cb268213131aa951",
    "semantic_title": "hpgcnn: hierarchical parallel group convolutional neural networks for point clouds processing",
    "citation_count": 1,
    "authors": [
      "Jisheng Dang",
      "Jun Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Huang_Raw-Guided_Enhancing_Reprocess_of_Low-Light_Image_via_Deep_Exposure_Adjustment_ACCV_2020_paper.html": {
    "title": "Raw-Guided Enhancing Reprocess of Low-Light Image via Deep Exposure Adjustment",
    "volume": "main",
    "abstract": "Enhancement of images captured in low-light conditions remains to be a challenging problem even with the advanced machine learning techniques. The challenges include the ambiguity of the ground truth for a low-light image and the loss of information during the RAW image processing. To tackle the problems, in this paper, we take a novel view to regard low-light image enhancement as an exposure time adjustment problem and propose a corresponding explicit and mathematical definition. Based on that, we construct a RAW-Guiding exposure time adjustment Network (RGNET), which overcomes RGB images' nonlinearity and RAW images' inaccessibility. That is, RGNET is only trained with RGB images and corresponding RAW images, which helps project nonlinear RGB images into a linear domain, simultaneously without using RAW images in the testing phase. Furthermore, our network consists of three individual sub-modules for unprocessing, reconstruction and processing, respectively. To the best of our knowledge, the proposed sub-net for unprocessing is the first learning-based unprocessing method. After the joint training of three parts, each pre-trained seperately with the RAW image guidance, experimental results demonstrate that RGNET outperforms state-of-the-art low-light image enhancement methods",
    "checked": true,
    "id": "63cf234aaddd2844a1436eedd019ac139f61fb46",
    "semantic_title": "raw-guided enhancing reprocess of low-light image via deep exposure adjustment",
    "citation_count": 2,
    "authors": [
      "Haofeng Huang",
      "Wenhan Yang",
      "Yueyu Hu",
      "Jiaying Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Caliskan_Multi-View_Consistency_Loss_for_Improved_Single-Image_3D_Reconstruction_of_Clothed_ACCV_2020_paper.html": {
    "title": "Multi-View Consistency Loss for Improved Single-Image 3D Reconstruction of Clothed People",
    "volume": "main",
    "abstract": "We present a novel method to improve the accuracy of the 3D reconstruction of clothed human shape from a single image. Recent work has introduced volumetric, implicit and model-based shape learning frameworks for reconstruction of objects and people from one or more images. However, the accuracy and completeness for reconstruction of clothed people is limited due to the large variation in shape resulting from clothing, hair, body size, pose and camera viewpoint. This paper introduces two advances to overcome this limitation: firstly a new synthetic dataset of realistic clothed people, 3DVH;and secondly, a novel multiple-view loss function for training of monocular volumetric shape estimation, which is demonstrated to significantly improve generalisation and reconstruction accuracy. The 3DVH dataset of realistic clothed 3D human models rendered with diverse natural backgrounds is demonstrated to allows transfer to reconstruction from real images of people. Comprehensive comparative performance evaluation on both synthetic and real images of people demonstrates that the proposed method significantly outperforms the previous state-of-the-art learning-based single image 3D human shape estimation approaches achieving significant improvement of reconstruction accuracy, completeness, and quality. An ablation study shows that this is due to both the proposed multiple-view training and the new 3DVH dataset. The code and the dataset can be found at the project website: https://akincaliskan3d.github.io/MV3DH/",
    "checked": true,
    "id": "f05741ea557ebaa42855766d9754b0a20bdc8cea",
    "semantic_title": "multi-view consistency loss for improved single-image 3d reconstruction of clothed people",
    "citation_count": 12,
    "authors": [
      "Akin Caliskan",
      "Armin Mustafa",
      "Evren Imre",
      "Adrian Hilton"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Guo_Low-light_Color_Imaging_via_Dual_Camera_Acquisition_ACCV_2020_paper.html": {
    "title": "Low-light Color Imaging via Dual Camera Acquisition",
    "volume": "main",
    "abstract": "As existing low-light color imaging suffers from the unrealistic color representation or blurry texture with a single camera setup, we are motivated to devise a dual camera system using a high spatial resolution (HSR) monochrome camera and another low spatial resolution (LSR) color camera for synthesizing the high-quality color image under low-light illumination conditions. The key problem is how to efficiently learn and fuse cross-camera information for improved presentation in such heterogeneous setup with domain gaps (e.g., color vs. monochrome, HSR vs. LSR). We have divided the end-to-end pipeline into three consecutive modularized sub-tasks, including the reference-based exposure compensation (RefEC), reference-based colorization (RefColor) and reference-based super-resolution (RefSR), to alleviate domain gaps and capture inter-camera dynamics between hybrid inputs. In each step, we leverage the powerful deep neural network (DNN) to respectively transfer and enhance the illuminative, spectral and spatial granularity in a data-driven way. Each module is first trained separately, and then jointly fine-tuned for robust and reliable performance. Experimental results have shown that our work provides the leading performance in synthetic content from popular test datasets when compared to existing algorithms, and offers appealing color reconstruction using real captured scenes from an industrial monochrome and a smartphone RGB cameras, in low-light color imaging application",
    "checked": true,
    "id": "ddfbcc46369f333223a983ce39ec9eb24dac7baf",
    "semantic_title": "low-light color imaging via dual camera acquisition",
    "citation_count": 2,
    "authors": [
      "Peiyao Guo",
      "Zhan Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Hu_Multi-label_X-ray_Imagery_Classification_via_Bottom-up_Attention_and_Meta_Fusion_ACCV_2020_paper.html": {
    "title": "Multi-label X-ray Imagery Classification via Bottom-up Attention and Meta Fusion",
    "volume": "main",
    "abstract": "Automatic security inspection has received increasing interests in recent years. Due to the fixed top-down perspective of X-ray scanning of often tightly packed luggages, such images typically suffer from penetration-induced occlusions, severe object overlapping and violent changes in appearance. For this particular application, few research efforts have been made. To deal with the overlapping in X-ray images classification, we propose a novel Security X-ray Multi-label Classification Network (SXMNet). Our hypothesis is that different overlapping levels and scale variations are the primary challenges in the multi-label classification problem of prohibited items. To address these challenges, we propose to incorporate 1) spatial attention to locate prohibited items despite shape, color and texture variations; and 2) anisotropic fusion of per-stage predictions to dynamically fuse hierarchical visual information under violent variations. Motivated by these, our SXMNet is boosted by bottom-up attention and neural-guided Meta Fusion. Raw input image is exploited to generate high-quality attention masks in a bottom-up way for pyramid feature refinement. Subsequently, the per-stage predictions according to the refined features are automatically re-weighted and fused via a soft selection guided by neural knowledge. Comprehensive experiments on the Security Inspection X-ray (SIXray) and Occluded Prohibited Items X-ray (OPIXray) datasets demonstrate the superiority of the proposed method",
    "checked": true,
    "id": "0cd4692a5e88ebbeb3c347c83ac274f1a127c022",
    "semantic_title": "multi-label x-ray imagery classification via bottom-up attention and meta fusion",
    "citation_count": 16,
    "authors": [
      "Benyi Hu",
      "Chi Zhang",
      "Le Wang",
      "Qilin Zhang",
      "Yuehu Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Abdelfattah_TTPLA_An_Aerial-Image_Dataset_for_Detection_and_Segmentation_of_Transmission_ACCV_2020_paper.html": {
    "title": "TTPLA: An Aerial-Image Dataset for Detection and Segmentation of Transmission Towers and Power Lines",
    "volume": "main",
    "abstract": "Accurate detection and segmentation of transmission towers (TTs) and power lines (PLs) from aerial images plays a key role in protecting power-grid security and low-altitude UAV safety. Meanwhile,aerial images of TTs and PLs pose a number of new challenges to the computer vision researchers who work on object detection and segmentation - PLs are long and thin, and may show similar color as the back-ground; TTs can be of various shapes and most likely made up of line structures of various sparsity; The background scene, lighting, and object sizes can vary significantly from one image to another. In this paper we collect and release a new TT/PL Aerial-image (TTPLA) dataset, consisting of 1,100 images with the resolution of 3,840x2,160 pixels, as well as manually labeled 8,987 instances of TTs and PLs. We develop novel policies for collecting, annotating, and labeling the images in TTPLA. Different from other relevant datasets, TTPLA supports evaluation ofinstance segmentation, besides detection and semantic segmentation. To build a baseline for detection and segmentation tasks on TTPLA, we report the performance of several state-of-the-art deep learning models on our dataset",
    "checked": true,
    "id": "f1289d5a37813cddc4641bf092b754e8755b451b",
    "semantic_title": "ttpla: an aerial-image dataset for detection and segmentation of transmission towers and power lines",
    "citation_count": 43,
    "authors": [
      "Rabab Abdelfattah",
      "Xiaofeng Wang",
      "Song Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Xie_DEAL_Difficulty-aware_Active_Learning_for_Semantic_Segmentation_ACCV_2020_paper.html": {
    "title": "DEAL: Difficulty-aware Active Learning for Semantic Segmentation",
    "volume": "main",
    "abstract": "Active learning aims to address the paucity of labeled data by finding the most informative samples. However, when applying to semantic segmentation, existing methods ignore the segmentation difficulty of different semantic areas, which leads to poor performance on those hard semantic areas such as tiny or slender objects. To deal with this problem, we propose a semantic Difficulty-awarE Active Learning (DEAL) network composed of two branches: the common segmentation branch and the semantic difficulty branch. For the latter branch, with the supervision of segmentation error between the segmentation result and GT, a pixel-wise probability attention module is introduced to learn the semantic difficulty scores for different semantic areas. Finally, two acquisition functions are devised to select the most valuable samples with semantic difficulty. Competitive results on semantic segmentation benchmarks demonstrate that DEAL achieves state-of-the-art active learning performance and improves the performance of the hard semantic areas in particular",
    "checked": true,
    "id": "199bae678f2b21f93996ef02e08cbe30603b1411",
    "semantic_title": "deal: difficulty-aware active learning for semantic segmentation",
    "citation_count": 37,
    "authors": [
      "Shuai Xie",
      "Zunlei Feng",
      "Ying Chen",
      "Songtao Sun",
      "Chao Ma",
      "Mingli Song"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Jack_Sparse_Convolutions_on_Continuous_Domains_for_Point_Cloud_and_Event_ACCV_2020_paper.html": {
    "title": "Sparse Convolutions on Continuous Domains for Point Cloud and Event Stream Networks",
    "volume": "main",
    "abstract": "Image convolutions have been a cornerstone of a great number of deep learning advances in computer vision. The research community is yet to settle on an equivalent operator for sparse, unstructured continuous data like point clouds and event streams however. We present an elegant sparse matrix-based interpretation of the convolution operator for these cases, which is consistent with the mathematical definition of convolution and efficient during training. On benchmark point cloud classification problems we demonstrate networks built with these operations can train an order of magnitude or more faster than top existing methods, whilst maintaining comparable accuracy and requiring a tiny fraction of the memory. We also apply our operator to event stream processing, achieving state-of-the-art results on multiple tasks with streams of hundreds of thousands of events",
    "checked": true,
    "id": "0d52f59fc46f1b5f93696cf14b6df92d2133e339",
    "semantic_title": "sparse convolutions on continuous domains for point cloud and event stream networks",
    "citation_count": 3,
    "authors": [
      "Dominic Jack",
      "Frederic Maire",
      "Simon Denman",
      "Anders Eriksson"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Kim_DiscFace_Minimum_Discrepancy_Learning_for_Deep_Face_Recognition_ACCV_2020_paper.html": {
    "title": "DiscFace: Minimum Discrepancy Learning for Deep Face Recognition",
    "volume": "main",
    "abstract": "Softmax-based learning methods have shown state-of-the-art performances on large-scale face recognition tasks. In this paper, we discover an important issue of softmax-based approaches: the sample features around the corresponding class weight are similarly penalized in the training phase even though their directions are different from each other. This directional discrepancy, i.e., process discrepancy leads to performance degradation at the evaluation phase. To mitigate the issue, we propose a novel training scheme, called minimum discrepancy learning that enforces directions of intra-class sample features to be aligned toward an optimal direction by using a single learnable basis. Furthermore, the single learnable basis facilitates disentangling the so-called class-invariant vectors from sample features, such that they are effective to train under class-imbalanced datasets",
    "checked": true,
    "id": "15844202dea158ac1ea8562d3894dcbfdf53946d",
    "semantic_title": "discface: minimum discrepancy learning for deep face recognition",
    "citation_count": 12,
    "authors": [
      "Insoo Kim",
      "Seungju Han",
      "Seong-Jin Park",
      "Ji-Won Baek",
      "Jinwoo Shin",
      "Jae-Joon Han",
      "Changkyu Choi"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Wang_Jointly_Discriminating_and_Frequent_Visual_Representation_Mining_ACCV_2020_paper.html": {
    "title": "Jointly Discriminating and Frequent Visual Representation Mining",
    "volume": "main",
    "abstract": "Discovering visual representation in an image category is a challenging issue, because the visual representation should not only be discriminating but also frequently appears in these images. Previous studies have proposed many solutions, but they all separately optimized the discrimination and frequency, which makes the solutions sub-optimal. To address this issue, we propose a method to discover the jointly discriminating and frequent visual representation, named as JDFR. To ensure discrimination, JDFR employs a classification task with cross-entropy loss. To achieve frequency, JDFR uses triplet loss to optimize within-class and between-class distance, then mines frequent visual representations in feature space. Moreover, we propose an attention module to locate the representative region in the image. Extensive experiments on four benchmark datasets (i.e. CIFAR10, CIFAR100-20, VOC2012-10 and Travel) show that the discovered visual representations have better discrimination and frequency than ones mined from five state-of-the-art methods with average improvements of 7.51% on accuracy and 1.88% on frequency",
    "checked": true,
    "id": "fda4a291d3787c27768d2c0ceb768311ddbe7ff1",
    "semantic_title": "jointly discriminating and frequent visual representation mining",
    "citation_count": 1,
    "authors": [
      "Qiannan Wang",
      "Ying Zhou",
      "Zhaoyan Zhu",
      "Xuefeng Liang",
      "Yu Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Chan_Active_Learning_for_Video_Description_With_Cluster-Regularized_Ensemble_Ranking_ACCV_2020_paper.html": {
    "title": "Active Learning for Video Description With Cluster-Regularized Ensemble Ranking",
    "volume": "main",
    "abstract": "Automatic video captioning aims to train models to generate text descriptions for all segments in a video, however, the most effective approaches require large amounts of manual annotation which is slow and expensive. Active learning is a promising way to efficiently build a training set for video captioning tasks while reducing the need to manually label uninformative examples. In this work we both explore various active learning approaches for automatic video captioning and show that a cluster-regularized ensemble strategy provides the best active learning approach to efficiently gather training sets for video captioning. We evaluate our approaches on the MSR-VTT and LSMDC datasets using both transformer and LSTM based captioning models and show that our novel strategy can achieve high performance while using up to 60% fewer training data than the strong state of the art random selection baseline",
    "checked": true,
    "id": "8ac8179dbdd256e514700b076673b6d5b9083251",
    "semantic_title": "active learning for video description with cluster-regularized ensemble ranking",
    "citation_count": 5,
    "authors": [
      "David M. Chan",
      "Sudheendra Vijayanarasimhan",
      "David A. Ross",
      "John F. Canny"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Peng_RGB-T_Crowd_Counting_from_Drone_A_Benchmark_and_MMCCN_Network_ACCV_2020_paper.html": {
    "title": "RGB-T Crowd Counting from Drone: A Benchmark and MMCCN Network",
    "volume": "main",
    "abstract": "Crowd counting aims to identify the number of objects and plays an important role in intelligent transportation, city management and Security monitoring. The task of crowd counting is much challenging because of scale variations, illumination changes, occlusions and poor imaging conditions, especially in the nighttime and haze conditions. In this paper, we present a drone based RGB-Thermal crowd counting dataset (DroneRGBT) that consists of 3600 pairs of images and covers different attributes, including height, illumination, density. To exploit the complementary information in both visible and thermal infrared modalities, we propose a multi-modal crowd counting network (MMCCN) with a multi-scale feature learning module, a modal alignment module and an adaptive fusion module. Experiments on DroneRGBT demonstrate the effectiveness of the proposed approach. We also provide a new thought to the field of RGB-T translation for crowd counting",
    "checked": true,
    "id": "d29f5a34170e7a5e8e0d23910402ae0aa7e75275",
    "semantic_title": "rgb-t crowd counting from drone: a benchmark and mmccn network",
    "citation_count": 27,
    "authors": [
      "Tao Peng",
      "Qing Li",
      "Pengfei Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Shi_A_Benchmark_and_Baseline_for_Language-Driven_Image_Editing_ACCV_2020_paper.html": {
    "title": "A Benchmark and Baseline for Language-Driven Image Editing",
    "volume": "main",
    "abstract": "Language-driven image editing can significantly save the laborious image editing work and be friendly to the photography novice. However, most similar work can only deal with a specific image domain or can only do global retouching. To solve this new task, we first present a new language-driven image editing dataset that supports both local and global editing with editing operation and mask annotations. Besides, we also propose a baseline method that fully utilizes the annotation to solve this problem. Our new method treats each editing operation as a sub-module and can automatically predict operation parameters. Not only performing well on challenging user data, but such an approach is also highly interpretable. We believe our work, including both the benchmark and the baseline, will advance the image editing area towards a more general and free-form level",
    "checked": true,
    "id": "58b7c7834e794c53b78a3a78dda54d92f2faba26",
    "semantic_title": "a benchmark and baseline for language-driven image editing",
    "citation_count": 19,
    "authors": [
      "Jing Shi",
      "Ning Xu",
      "Trung Bui",
      "Franck Dernoncourt",
      "Zheng Wen",
      "Chenliang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Chin_Quantum_Robust_Fitting_ACCV_2020_paper.html": {
    "title": "Quantum Robust Fitting",
    "volume": "main",
    "abstract": "Many computer vision applications need to recover structure from imperfect measurements of the real world. The task is often solved by robustly fitting a geometric model onto noisy and outlier-contaminated data. However, recent theoretical analyses indicate that many commonly used formulations of robust fitting in computer vision are not amenable to tractable solution and approximation. In this paper, we explore the usage of quantum computers for robust fitting. To do so, we examine and establish the practical usefulness of a robust fitting formulation inspired by the analysis of monotone Boolean functions. We then investigate a quantum algorithm to solve the formulation and analyse the computational speed-up possible over the classical algorithm. Our work thus proposes one of the first quantum treatments of robust fitting for computer vision",
    "checked": true,
    "id": "2716e972ff4b71035088c15b3daface833206e37",
    "semantic_title": "quantum robust fitting",
    "citation_count": 10,
    "authors": [
      "Tat-Jun Chin",
      "David Suter",
      "Shin-Fang Ch'ng",
      "James Quach"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Zhou_RGB-D_Co-attention_Network_for_Semantic_Segmentation_ACCV_2020_paper.html": {
    "title": "RGB-D Co-attention Network for Semantic Segmentation",
    "volume": "main",
    "abstract": "Incorporating the depth (D) information for RGB images has proven the effectiveness and robustness in semantic segmentation. However, the fusion between them is still a challenge due to their meaning discrepancy, in which RGB represents the color but D depth information. In this paper, we propose a co-attention Network (CANet) to capture the fine-grained interplay between RGB' and D' features. The key part in our CANet is co-attention fusion part. It includes three modules. At first, the position and channel co-attention fusion modules adaptively fuse color and depth features in spatial and channel dimension. Finally, a final fusion module integrates the outputs of the two co-attention fusion modules for forming a more representative feature. Our extensive experiments validate the effectiveness of CANet in fusing RGB and D features, achieving the state-of-the-art performance on two challenging RGB-D semantic segmentation datasets, i.e., NYUDv2, SUN-RGBD",
    "checked": true,
    "id": "b52d13d02cd546c8ec35aa9acbe1bab866534cef",
    "semantic_title": "rgb-d co-attention network for semantic segmentation",
    "citation_count": 24,
    "authors": [
      "Hao Zhou",
      "Lu Qi",
      "Zhaoliang Wan",
      "Hai Huang",
      "Xu Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Qu_Learning_More_Accurate_Features_for_Semantic_Segmentation_in_CycleNet_ACCV_2020_paper.html": {
    "title": "Learning More Accurate Features for Semantic Segmentation in CycleNet",
    "volume": "main",
    "abstract": "Contextual information is essential for computer vision tasks, especially semantic segmentation. Previous works generally focus on how to collect contextual information by enlarging the size of receptive field, such as PSPNet, DenseASPP. In contrast to previous works, this paper proposes a new network -- CycleNet, which considers assigning a more accurate representative for every pixel. It consists of two modules, Cycle Atrous Spatial Pyramid Pooling (CycleASPP) and Alignment with Deformable Convolution (ADC). The former realizes dense connections between a series of atrous convolution layers with different dilation rates. Not only the forward connections can aggregate more contextual information, but also the backward connections can pay more attention to important information by transferring high-level features to low-level layers. Besides, ADC generates accurate information during the decoding process. It draws support from deformable convolution to select and recombine features from different blocks, thus improving the misalignment issues caused by simple interpolation. A set of experiments have been conducted on Cityscapes and ADE20K to demonstrate the effectiveness of CycleNet. In particular, our model achieved 46.14% mIoU on ADE20K validation set",
    "checked": true,
    "id": "f1dba33cf57efbc3706b64038df6476e79b92b43",
    "semantic_title": "learning more accurate features for semantic segmentation in cyclenet",
    "citation_count": 1,
    "authors": [
      "Linzi Qu",
      "Lihuo He",
      "Junjie Ke",
      "Xinbo Gao",
      "Wen Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Liu_MTNAS_Search_Multi-Task_Networks_for_Autonomous_Driving_ACCV_2020_paper.html": {
    "title": "MTNAS: Search Multi-Task Networks for Autonomous Driving",
    "volume": "main",
    "abstract": "Multi-task learning (MTL) aims to learn shared representations from multiple tasks simultaneously, which has yielded outstanding performance in widespread applications of computer vision. However, existing multi-task approaches often demand manual design on network architectures, including shared backbone and individual branches. In this work, we propose MTNAS, a practical and principled neural architecture search algorithm for multi-task learning. We focus on searching for the overall optimized network architecture with task-specific branches and task-shared backbone. Specifically, the MTNAS pipeline consists of two searching stages: branch search and backbone search. For branch search, we separately optimize each branch structure for each target task. For backbone search, we first design a pre-searching procedure t1o pre-optimize the backbone structure on ImageNet. We observe that searching on such auxiliary large-scale data can not only help learn low-/mid-level features but also offer good initialization of backbone structure. After backbone pre-searching, we further optimize the backbone structure for learning task-shared knowledge under the overall multi-task guidance. We apply MTNAS to joint learning of object detection and semantic segmentation for autonomous driving. Extensive experimental results demonstrate that our searched multi-task model achieves superior performance for each task and consumes less computation complexity compared to prior hand-crafted MTL baselines. Code and searched models will be released at https://github.com/RalphLiu/MTNAS",
    "checked": true,
    "id": "1b491d019ecfb4a9185a8389c9e0f4676d610014",
    "semantic_title": "mtnas: search multi-task networks for autonomous driving",
    "citation_count": 2,
    "authors": [
      "Hao Liu",
      "Dong Li",
      "JinZhang Peng",
      "Qingjie Zhao",
      "Lu Tian",
      "Yi Shan"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Hassan_Trainable_Structure_Tensors_for_Autonomous_Baggage_Threat_Detection_Under_Extreme_ACCV_2020_paper.html": {
    "title": "Trainable Structure Tensors for Autonomous Baggage Threat Detection Under Extreme Occlusion",
    "volume": "main",
    "abstract": "Detecting baggage threats is one of the most difficult tasks, even for expert officers. Many researchers have developed computer-aided screening systems to recognize these threats from the baggage X-ray scans. However, all of these frameworks are limited in identifying the contraband items under extreme occlusion. This paper presents a novel instance segmentation framework that utilizes trainable structure tensors to highlight the contours of the occluded and cluttered contraband items (by scanning multiple predominant orientations), while simultaneously suppressing the irrelevant baggage content. The proposed framework has been extensively tested on four publicly available X-ray datasets where it outperforms the state-of-the-art frameworks in terms of mean average precision scores. Furthermore, to the best of our knowledge, it is the only framework that has been validated on combined grayscale and colored scans obtained from four different types of X-ray scanners",
    "checked": true,
    "id": "7224679292a04b25acefb2422361f105650dff30",
    "semantic_title": "trainable structure tensors for autonomous baggage threat detection under extreme occlusion",
    "citation_count": 31,
    "authors": [
      "Taimur Hassan",
      "Naoufel Werghi"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Wu_Bi-Directional_Attention_for_Joint_Instance_and_Semantic_Segmentation_in_Point_ACCV_2020_paper.html": {
    "title": "Bi-Directional Attention for Joint Instance and Semantic Segmentation in Point Clouds",
    "volume": "main",
    "abstract": "Instance segmentation in point clouds is one of the most fine-grained ways to understand the 3D scene. Due to its close relationship to semantic segmentation, many works approach these two tasks simultaneously and leverage the benefits of multi-task learning. However, most of them only considered simple strategies such as element-wise feature fusion, which may not lead to mutual promotion. In this work, we build a Bi-Directional Attention module on backbone neural networks for 3D point cloud perception, which uses similarity matrix measured from features for one task to help aggregate non-local information for the other task, avoiding the potential feature exclusion and task conflict. From comprehensive experiments, ablation studies and efficiency studies on the S3DIS dataset and the PartNet dataset, the superiority of our method is verified. Moreover, the mechanism of how bi-directional attention module helps joint instance and semantic segmentation is also analyzed",
    "checked": true,
    "id": "ecc8744e17dcde7bf9439dc7fc56e32ebd0f658d",
    "semantic_title": "bi-directional attention for joint instance and semantic segmentation in point clouds",
    "citation_count": 6,
    "authors": [
      "Guangnan Wu",
      "Zhiyi Pan",
      "Peng Jiang",
      "Changhe Tu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Kardoost_Self-supervised_Sparse_to_Dense_Motion_Segmentation_ACCV_2020_paper.html": {
    "title": "Self-supervised Sparse to Dense Motion Segmentation",
    "volume": "main",
    "abstract": "Observable motion in videos can give rise to the definition of objects moving with respect to the scene. The task of segmenting such moving objects is referred to as motion segmentationand is usually tackled either by aggregating motion information in long, sparse point trajectories, or by directly producing per frame dense segmentations relying on large amounts of training data.In this paper, we propose a self supervised method to learn the densification of sparse motion segmentations from single video frames. While previous approaches towards motion segmentation build upon pre-training on large surrogate datasets and use dense motion information as an essential cue for the pixelwise segmentation, our model does not require pre-training and operates at test time on single frames. It can be trained in a sequence specific way to produce high quality dense segmentations from sparse and noisy input. We evaluate our method on the well-known motion segmentation datasets FBMS59 and DAVIS2016",
    "checked": true,
    "id": "72b42742c4c4f38c75a47ea134e88f18f81eb0a9",
    "semantic_title": "self-supervised sparse to dense motion segmentation",
    "citation_count": 4,
    "authors": [
      "Amirhossein Kardoost",
      "Kalun Ho",
      "Peter Ochs",
      "Margret Keuper"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Liu_Learning_3D_Face_Reconstruction_with_a_Pose_Guidance_Network_ACCV_2020_paper.html": {
    "title": "Learning 3D Face Reconstruction with a Pose Guidance Network",
    "volume": "main",
    "abstract": "We present a self-supervised learning approach to learning monocular 3D face reconstruction with a pose guidance network (PGN). First, we unveil the bottleneck of pose estimation in prior parametric 3D face learning methods, and propose to utilize 3D face landmarks for estimating pose parameters. With our specially designed PGN, our model can learn from both faces with fully labeled 3D landmarks and unlimited unlabeled in-the-wild face images. Our network is further augmented with a self-supervised learning scheme, which exploits face geometry information embedded in multiple frames of the same person, to alleviate the ill-posed nature of regressing 3D face geometry from a single image. These three insights yield a single approach that combines the complementary strengths of parametric model learning and data-driven learning techniques. We conduct a rigorous evaluation on the challenging AFLW2000-3D, Florence and FaceWarehouse datasets, and show that our method outperforms the state-of-the-art for all metrics",
    "checked": true,
    "id": "a113b9ac18560277a47fe2442a1bdfdcb58aa01d",
    "semantic_title": "learning 3d face reconstruction with a pose guidance network",
    "citation_count": 2,
    "authors": [
      "Pengpeng Liu",
      "Xintong Han",
      "Michael Lyu",
      "Irwin King",
      "Jia Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Chen_Towards_Fast_and_Robust_Adversarial_Training_for_Image_Classification_ACCV_2020_paper.html": {
    "title": "Towards Fast and Robust Adversarial Training for Image Classification",
    "volume": "main",
    "abstract": "The adversarial training, which augments the training data with adversarial examples, is one of the most effective methods to defend adversarial attacks. However, its robustness degrades for complex models, and the producing of strong adversarial examples is a time-consuming task. In this paper, we proposed methods to improve the robustness and efficiency of the adversarial training. First, we utilized a re-constructor to enforce the classifier to learn the important features under perturbations. Second, we employed the enhanced FGSM to generate adversarial examples effectively. It can detect overfitting and stop training earlier without extra cost. Experiments are conducted on MNIST and CIFAR10 to validate the effectiveness of our methods. We also compared our algorithm with the state-of-the-art defense methods. The results show that our algorithm is 4-5 times faster than the previously fastest training method. For CIFAR-10, our method can achieve above 46% robust accuracy, which is better than most of other methods",
    "checked": true,
    "id": "e190aee7634cd27e69bada033199b73435e5707e",
    "semantic_title": "towards fast and robust adversarial training for image classification",
    "citation_count": 7,
    "authors": [
      "Erh-Chung Chen",
      "Che-Rung Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Zhao_Query_by_Strings_and_Return_Ranking_Word_Regions_with_Only_ACCV_2020_paper.html": {
    "title": "Query by Strings and Return Ranking Word Regions with Only One Look",
    "volume": "main",
    "abstract": "Word spotting helps people like archaeologists, historian and internet censors to retrieve regions of interest from document images according to the queries defined by them. However, words in handwritten historical document images are generally densely distributed and have many overlapping strokes, which make it challenging to apply word spotting in such scenarios. Recently, deep learning based methods have achieved significant performance improvement, which usually adopt two-stage object detectors to produce word segmentation results and then embed cropped word regions into a word embedding space. Different from these multi-stage methods, this paper presents an effective end-to-end trainable method for segmentation-free query-by-string word spotting. To the best of our knowledge, this is the first work that uses a single network to simultaneously predict word bounding box and word embedding in only one stage by adopting feature sharing and multi-task learning strategy. Experiments on several benchmarks demonstrate that the proposed method surpasses the previous state-of-the-art segmentation-free methods",
    "checked": true,
    "id": "50768ab1c19e93a18041f784e6b130b4a15ebb6e",
    "semantic_title": "query by strings and return ranking word regions with only one look",
    "citation_count": 1,
    "authors": [
      "Peng Zhao",
      "Wenyuan Xue",
      "Qingyong Li",
      "Siqi Cai"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Lamdouar_Betrayed_by_Motion_Camouflaged_Object_Discovery_via_Motion_Segmentation_ACCV_2020_paper.html": {
    "title": "Betrayed by Motion: Camouflaged Object Discovery via Motion Segmentation",
    "volume": "main",
    "abstract": "The objective of this paper is to design a computational architecture that discovers camouflaged objects in videos, specifically by exploiting motion information to perform object segmentation. We make the following three contributions: (i) We propose a novel architecture that consists of two essential components for breaking camouflage, namely, a differentiable registration module to align consecutive frames based on the background, which effectively emphasises the object boundary in the difference image, and a motion segmentation module with memory that discovers the moving objects, while maintaining the object permanence even when motion is absent at some point. (ii) We collect the first large-scale Moving Camouflaged Animals (MoCA) video dataset, which consists of over 140 clips across a diverse range of animals (67 categories). (iii) We demonstrate the effectiveness of the proposed model on MoCA, and achieve competitive performance on the unsupervised segmentation protocol on DAVIS2016 by only relying on motion",
    "checked": true,
    "id": "1c3bcf6a760cb787f9beab00a07111cddae76596",
    "semantic_title": "betrayed by motion: camouflaged object discovery via motion segmentation",
    "citation_count": 58,
    "authors": [
      "Hala Lamdouar",
      "Charig Yang",
      "Weidi Xie",
      "Andrew Zisserman"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Xia_Unpaired_Multimodal_Facial_Expression_Recognition_ACCV_2020_paper.html": {
    "title": "Unpaired Multimodal Facial Expression Recognition",
    "volume": "main",
    "abstract": "Current works on multimodal facial expression recognition typically require paired visible and thermal facial images. Although visible cameras are readily available in our daily life, thermal cameras are expensive and less prevalent. It is costly to collect a large quantity of synchronous visible and thermal facial images. To tackle this paired training data bottleneck, we propose an unpaired multimodal facial expression recognition method, which makes full use of the massive number of unpaired visible and thermal images by utilizing thermal images to construct better image representations and classifiers for visible images during training. Specifically, two deep neural networks are trained from visible and thermal images to learn image representations and expression classifiers for two modalities. Then, an adversarial strategy is adopted to force statistical similarity between the learned visible and thermal representations, and to minimize the distribution mismatch between the predictions of the visible and thermal images. Through adversarial learning, the proposed method leverages thermal images to construct better image representations and classifiers for visible images during training, without the requirement of paired data. A decoder network is built upon the visible hidden features in order to preserve some inherent features of the visible view. We also take the variability of the different images' transferability into account via adaptive classification loss. During testing, only visible images are required and the visible network is used. Thus, the proposed method is appropriate for real-world scenarios, since thermal imaging is rare in these instances. Experiments on two benchmark multimodal expression databases and three visible facial expression databases demonstrate the superiority of the proposed method compared to state-of-the-art methods",
    "checked": true,
    "id": "8f92b7547253a277c85070b9f6adc3190a78c5e9",
    "semantic_title": "unpaired multimodal facial expression recognition",
    "citation_count": 0,
    "authors": [
      "Bin Xia",
      "Shangfei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Li_Towards_Optimal_Filter_Pruning_with_Balanced_Performance_and_Pruning_Speed_ACCV_2020_paper.html": {
    "title": "Towards Optimal Filter Pruning with Balanced Performance and Pruning Speed",
    "volume": "main",
    "abstract": "Filter pruning has drawn more attention since resource constrained platform requires more compact model for deployment. However, current pruning methods suffer either from the inferior performance of one-shot methods, or the expensive time cost of iterative training methods. In this paper, we propose a balanced filter pruning method for both performance and pruning speed. Based on the filter importance criteria, our method is able to prune a layer with approximate layer-wise optimal pruning rate at preset loss variation. The network is pruned in the layer-wise way without the time consuming prune-retrain iteration. If a pre-defined pruning rate for the entire network is given, we also introduce a method to find the corresponding loss variation threshold with fast converging speed. Moreover, we propose the layer group pruning and channel selection mechanism for channel alignment in network with short connections. The proposed pruning method is widely applicable to common architectures and does not involve any additional training except the final fine-tuning. Comprehensive experiments show that our method outperforms many state-of-the-art approaches",
    "checked": true,
    "id": "7892d87c7051d31ac4c4f3c264e7f2028311a949",
    "semantic_title": "towards optimal filter pruning with balanced performance and pruning speed",
    "citation_count": 3,
    "authors": [
      "Dong Li",
      "Sitong Chen",
      "Xudong Liu",
      "Yunda Sun",
      "Li Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/He_COG_COnsistent_data_auGmentation_for_object_perception_ACCV_2020_paper.html": {
    "title": "COG: COnsistent data auGmentation for object perception",
    "volume": "main",
    "abstract": "Recently, data augmentation techniques for training conv-nets emerge one after another, especially focusing on image classification. They're always applied to object detection without further careful design. In this paper we propose COG, a general domain migration scheme for augmentation. Specifically, based on a particular augmentation, we first analyze its inherent inconsistency, and then adopt an adaptive strategy to rectify ground-truths of the augmented input images. Next, deep detection networks are trained on the rectified data to achieve better performance. Our extensive experiments show that our method COG's performance is superior to its competitor on detection and instance segmentation tasks. In addition, the results manifest the robustness of COG when faced with hyper-parameter variations, etc",
    "checked": true,
    "id": "000a65ffcf8a2cf71e6951f23a64b8c985733496",
    "semantic_title": "cog: consistent data augmentation for object perception",
    "citation_count": 0,
    "authors": [
      "Zewen He",
      "Rui Wu",
      "Dingqian Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Shao_Domain-transferred_Face_Augmentation_Network_ACCV_2020_paper.html": {
    "title": "Domain-transferred Face Augmentation Network",
    "volume": "main",
    "abstract": "The performance of a convolutional neural network (CNN) based face recognition model largely relies on the richness of labelled training data. However, it is expensive to collect a training set with large variations of a face identity under different poses and illumination changes, so the diversity of within-class face images becomes a critical issue in practice. In this paper, we propose a 3D model-assisted domain-transferred face augmentation network (DotFAN) that can generate a series of variants of an input face based on the knowledge distilled from existing rich face datasets of other domains. Extending from StarGAN's architecture, DotFAN integrates with two additional subnetworks, i.e., face expert model (FEM) and face shape regressor (FSR), for latent facial code control. While FSR aims to extract face attributes, FEM is designed to capture a face identity. With their aid, DotFAN can separately learn facial feature codes and effectively generate face images of various facial attributes while keeping the identity of augmented faces unaltered. Experiments show that DotFAN is beneficial for augmenting small face datasets to improve their within-class diversity so that a better face recognition model can be learned from the augmented dataset",
    "checked": true,
    "id": "0e102e64064bf6cc56a5d2f0be8536f3e87b58b9",
    "semantic_title": "domain-transferred face augmentation network",
    "citation_count": 0,
    "authors": [
      "Hao-Chiang Shao",
      "Kang-Yu Liu",
      "Chia-Wen Lin",
      "Jiwen Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Kozerawski_BLT_Balancing_Long-Tailed_Datasets_with_Adversarially-Perturbed_Images_ACCV_2020_paper.html": {
    "title": "BLT: Balancing Long-Tailed Datasets with Adversarially-Perturbed Images",
    "volume": "main",
    "abstract": "Real visual-world datasets have few classes with a large number of samples (i.e., head classes) while many others have a small number of exemplars (i.e., tail classes). Unfortunately, this imbalance enables a visual recognition system to perform well on head classes but poorly on tail classes. To alleviate this imbalance, we present BLT, a novel data augmentation technique that generates extra training samples for tail classes to improve the generalization performance of a classifier given the few training samples from tail classes. Unlike prior long-tail approaches that rely on generative models (e.g., GANs or VQ-VAEs) to augment a dataset, BLT uses a gradient-ascent-based image generation algorithm, which requires significantly less training time and computational resources. In specific, BLT avoids the use of dedicated generative networks, which add significant computational overhead and require elaborate training procedures. Our experiments on naturally and synthetically long-tailed datasets and across different network architectures demonstrate that BLT consistently improves the average classification performance of tail classes by 11% w.r.t. the common approach that balances the dataset by oversampling tail-class images. BLT maintains the accuracy on head classes while improving the performance on tail-classes",
    "checked": true,
    "id": "766c78114ad33a435526dbd353ddac79bb5817db",
    "semantic_title": "blt: balancing long-tailed datasets with adversarially-perturbed images",
    "citation_count": 10,
    "authors": [
      "Jedrzej Kozerawski",
      "Victor Fragoso",
      "Nikolaos Karianakis",
      "Gaurav Mittal",
      "Matthew Turk",
      "Mei Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Garnett_Synthetic-to-real_domain_adaptation_for_lane_detection_ACCV_2020_paper.html": {
    "title": "Synthetic-to-real domain adaptation for lane detection",
    "volume": "main",
    "abstract": "Accurate lane detection, a crucial enabler for autonomous driving, currently relies on obtaining a large and diverse labeled training dataset. In this work, we explore learning from abundant, randomly generated synthetic data, together with unlabeled or partially labeled target domain data, instead. Randomly generated synthetic data has the advantage of controlled variability in the lane geometry and lighting, but it is limited in terms of photo-realism. This poses the challenge of adapting models learned on the unrealistic synthetic domain to real images. To this end we develop a novel autoencoder-based approach that uses synthetic labels unaligned with particular images for adapting to target domain data. In addition, we explore existing domain adaptation approaches, such as image translation and self-supervision, and adjust them to the lane detection task. We test all approaches in the unsupervised domain adaptation setting in which no target domain labels are available and in the semi-supervised setting in which a small portion of the target images are labeled. In extensive experiments using three different datasets, we demonstrate the possibility to save costly target domain labeling efforts. For example, using our proposed autoencoder approach on the llamas and tuSimple lane datasets, we can almost recover the fully supervised accuracy with only 10% of the labeled data. In addition, our autoencoder approach outperforms all other methods in the semi-supervised domain adaptation scenario",
    "checked": true,
    "id": "39f02da7f221d63d34ec7cf43b41882ccfcf151b",
    "semantic_title": "synthetic-to-real domain adaptation for lane detection",
    "citation_count": 11,
    "authors": [
      "Noa Garnett",
      "Roy Uziel",
      "Netalee Efrat",
      "Dan Levi"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Pranav_A_Day_on_Campus_-_An_Anomaly_Detection_Dataset_for_ACCV_2020_paper.html": {
    "title": "A Day on Campus - An Anomaly Detection Dataset for Events in a Single Camera",
    "volume": "main",
    "abstract": "Detecting anomalies in videos is a complex problem with a myriad of applications in video surveillance. However, large and complex datasets that are representative of real-world deployment of surveillance cameras are unavailable. Anomalies in surveillance videos are not well defined and the standard and existing metrics for evaluation do not quantify the performance of algorithms accurately. We provide a large scale dataset, A Day on Campus (ADOC), with 25 event types, spanning over 721 instances and occurring over a period of 24 hours. This is the largest dataset with localized bounding box annotations that is available to perform anomaly detection. We design a novel metric to evaluate the performance of methods and we perform an evaluation of the state-of-the-art methods to ascertain their readiness to transition into real-world surveillance scenarios",
    "checked": true,
    "id": "3487a04ce0a9c48387641c9fd957cc390bf267f9",
    "semantic_title": "a day on campus - an anomaly detection dataset for events in a single camera",
    "citation_count": 23,
    "authors": [
      "Mantini Pranav",
      "Li Zhenggang",
      "Shah Shishir K"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Li_Chromatic_Aberration_Correction_Using_Cross-Channel_Prior_in_Shearlet_Domain_ACCV_2020_paper.html": {
    "title": "Chromatic Aberration Correction Using Cross-Channel Prior in Shearlet Domain",
    "volume": "main",
    "abstract": "Instead of more expensive and complex optics, recent years, many researches are focused on high-quality photography using lightweight cameras, such as single-ball lens, with computational image processing. Traditional methods for image enhancement do not comprehensively address the blurring artifacts caused by strong chromatic aberrations in images produced by a simple optical system. In this paper, we propose a new method to correct both lateral and axial chromatic aberrations based on their different characteristics. To eliminate lateral chromatic aberration, cross-channel prior in shearlet domain is proposed to align texture information of red and blue channels to green channel. We also propose a new PSF estimation method to better correct axial chromatic aberration using wave propagation model, where F-number of the optical system is needed. Simulation results demonstrate our method can provide aberration-free images while there are still some artifacts in the results of the state-of-art methods. PSNRs of simulation results increase at least 2 dB and SSIM is on average 6.29% to 41.26% better than other methods. Real-captured image results prove that the proposed prior can effectively remove lateral chromatic aberration while the proposed PSF model can further correct the axial chromatic aberration",
    "checked": true,
    "id": "190659796a875bce6ead76938dcdfe67f46f1fec",
    "semantic_title": "chromatic aberration correction using cross-channel prior in shearlet domain",
    "citation_count": 1,
    "authors": [
      "Kunyi Li",
      "Xin Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Li_Online_Knowledge_Distillation_via_Multi-branch_Diversity_Enhancement_ACCV_2020_paper.html": {
    "title": "Online Knowledge Distillation via Multi-branch Diversity Enhancement",
    "volume": "main",
    "abstract": "Knowledge distillation is an effective method to transfer the knowledge from the cumbersome teacher model to the lightweight student model. Online knowledge distillation uses the ensembled prediction results of multiple student models as soft targets to train each student model. However, the homogenization problem will lead to difficulty in further improving model performance. In this work, we propose a new distillation method to enhance the diversity among multiple student models. We introduce Feature Fusion Module (FFM), which improves the performance of the attention mechanism in the network by integrating rich semantic information contained in the last block of multiple student models. Furthermore, we use the Classifier Diversification(CD) loss function to strengthen the differences between the student models and deliver a better ensemble result. Extensive experiments proved that our method significantly enhances the diversity among student models and brings better distillation performance. We evaluate our method on three image classification datasets: CIFAR-10/100 and CINIC-10. The results show that our method achieves state-of-the-art performance on these datasets",
    "checked": true,
    "id": "4dcbc4e34474f2631b879b3ce64d1a497f462050",
    "semantic_title": "online knowledge distillation via multi-branch diversity enhancement",
    "citation_count": 18,
    "authors": [
      "Zheng Li",
      "Ying Huang",
      "Defang Chen",
      "Tianren Luo",
      "Ning Cai",
      "Zhigeng Pan"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Oya_Do_We_Need_Sound_for_Sound_Source_Localization_ACCV_2020_paper.html": {
    "title": "Do We Need Sound for Sound Source Localization?",
    "volume": "main",
    "abstract": "During the performance of sound source localization which uses both visual and aural information, it presently remains unclear how much either image or sound modalities contribute to the result, i.e. do we need both image and sound for sound source localization? To address this question, we develop an unsupervised learning system that solves sound source localization by decomposing this task into two steps: (i) \"potential sound source localization\", a step that localizes possible sound sources using only visual information (ii) \"object selection\", a step that identifies which objects are actually sounding using aural information. Our overall system achieves state-of-the-art performance in sound source localization, and more importantly, we find that despite the constraint on available information, the results of (i) achieve similar performance. From this observation and further experiments, we show that visual information is dominant in \"sound\" source localization when evaluated with the currently adopted benchmark dataset. Moreover, we show that the majority of sound-producing objects within the samples in this dataset can be inherently identified using only visual information, and thus that the dataset is inadequate to evaluate a system's capability to leverage aural information. As an alternative, we present an evaluation protocol which enforces both visual and aural information to be leveraged, and verify this property through several experiments",
    "checked": true,
    "id": "476d71a4cfd1529c4705636041add568853d74b1",
    "semantic_title": "do we need sound for sound source localization?",
    "citation_count": 18,
    "authors": [
      "Takashi Oya",
      "Shohei Iwase",
      "Ryota Natsume",
      "Takahiro Itazuri",
      "Shugo Yamaguchi",
      "Shigeo Morishima"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Xiong_Gaussian_Vector_An_Efficient_Solution_for_Facial_Landmark_Detection_ACCV_2020_paper.html": {
    "title": "Gaussian Vector: An Efficient Solution for Facial Landmark Detection",
    "volume": "main",
    "abstract": "Significant progress has been made in facial landmark detection with the development of Convolutional Neural Networks.The widely-used algorithms can be classified into coordinate regression methods and heatmap based methods.However, the former loses spatial information, resulting in poor performance while the latter has drawbacks like large output size or high post-processing complexity.This paper proposes a new solution, Gaussian Vector, to preserve the spatial information as well as reduce the output size and simplify the post-processing.Our method provides novel vector supervision and introduces Band Pooling Module to convert heatmap into a pair of vectors for each landmark.This is a plug-and-play component which is simple and effective.Moreover, Beyond Box Strategy is proposed to handle the landmarks out of the face bounding box.We evaluate our method on 300W, COFW, WFLW and JD-landmark.That the results significantly surpass previous works demonstrates the effectiveness of our approach",
    "checked": true,
    "id": "1a106eb244f8ba13e9d698d69f504c3c1080ad73",
    "semantic_title": "gaussian vector: an efficient solution for facial landmark detection",
    "citation_count": 11,
    "authors": [
      "Yilin Xiong",
      "Zijian Zhou",
      "Yuhao Dou",
      "Zhizhong Su"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Tran_GAN-based_Noise_Model_for_Denoising_Real_Images_ACCV_2020_paper.html": {
    "title": "GAN-based Noise Model for Denoising Real Images",
    "volume": "main",
    "abstract": "In the present paper, we propose a new approach for realistic image noise modeling based on a generative adversarial network (GAN). The model aims to boost performance of a deep network denoiser for real-world denoising. Although deep network denoisers, such as a denoising convolutional neural network, can achieve state-of-the-art denoised results on synthetic noise, they perform poorly on real-world noisy images. To address this, we propose a two-step model. First, the images are converted to raw image data before adding noise. We then trained a GAN to estimate the noise distribution over a large collection of images (1 million). The estimated noise was used to train a deep neural network denoiser. Extensive experiments demonstrated that our new noise model achieves state-of-the-art performance on real raw images from the Smartphone Image Denoising Dataset benchmark",
    "checked": true,
    "id": "0ed2b7e3e51ce42cad9eb2ab63d6c7ccf2b43f80",
    "semantic_title": "gan-based noise model for denoising real images",
    "citation_count": 32,
    "authors": [
      "Linh Duy Tran",
      "Son Minh Nguyen",
      "Masayuki Arai"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Minar_CloTH-VTON_Clothing_Three-dimensional_reconstruction_for_Hybrid_image-based_Virtual_Try-ON_ACCV_2020_paper.html": {
    "title": "CloTH-VTON: Clothing Three-dimensional reconstruction for Hybrid image-based Virtual Try-ON",
    "volume": "main",
    "abstract": "Virtual clothing try-on, transferring a clothing image onto a target person image, is drawing industrial and research attention. Both 2D image-based and 3D model-based methods proposed recently have their benefits and limitations. Whereas 3D model-based methods provide realistic deformations of the clothing, it needs a difficult 3D model construction process and cannot handle the non-clothing areas well. Image-based deep neural network methods are good at generating disclosed human parts, retaining the unchanged area, and blending image parts, but cannot handle large deformation of clothing. In this paper, we propose CloTH-VTON that utilizes the high-quality image synthesis of 2D image-based methods and the 3D model-based deformation to the target human pose. For this 2D and 3D combination, we propose a novel 3D cloth reconstruction method from a single 2D cloth image, leveraging a 3D human body model, and transfer to the shape and pose of the target person. Our cloth reconstruction method can be easily applied to diverse cloth categories. Our method produces final try-on output with naturally deformed clothing and preserving details in high resolution",
    "checked": true,
    "id": "e4e10a64b06f7706d6104a029092607ee9dea1bb",
    "semantic_title": "cloth-vton: clothing three-dimensional reconstruction for hybrid image-based virtual try-on",
    "citation_count": 24,
    "authors": [
      "Matiur Rahman Minar",
      "Heejune Ahn"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Wu_Depth-Adapted_CNN_for_RGB-D_cameras_ACCV_2020_paper.html": {
    "title": "Depth-Adapted CNN for RGB-D cameras",
    "volume": "main",
    "abstract": "Conventional 2D Convolutional Neural Networks (CNN) extract features from an input image by applying linear filters. These filters compute the spatial coherence by weighting the photometric information on a fixed neighborhood without taking into account the geometric information. We tackle the problem of improving the classical RGB CNN methods by using the depth information provided by the RGB-D cameras. State-of-the-art approaches use depth as an additional channel or image (HHA) or pass from 2D CNN to 3D CNN. This paper proposes a novel and generic procedure to articulate both photometric and geometric information in CNN architecture. The depth data is represented as a 2D offset to adapt spatial sampling locations. The new model presented is invariant to scale and rotation around X and Y axis of the camera coordinate system. Moreover, when depth data is constant, our model is equivalent to a regular CNN. Experiments of benchmarks validate the effectiveness of our model",
    "checked": true,
    "id": "0cd08c181e5cb68edb2a7f1c2e50a1a158b55896",
    "semantic_title": "depth-adapted cnn for rgb-d cameras",
    "citation_count": 16,
    "authors": [
      "Zongwei Wu",
      "Guillaume Allibert",
      "Christophe Stolz",
      "Cedric Demonceaux"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Ding_Homography-based_Egomotion_Estimation_Using_Gravity_and_SIFT_Features_ACCV_2020_paper.html": {
    "title": "Homography-based Egomotion Estimation Using Gravity and SIFT Features",
    "volume": "main",
    "abstract": "Camera systems used, e.g., in cars, UAVs, smartphones, and tablets, are typically equipped with IMUs (inertial measurement units) that can measure the gravity vector. Using the information from an IMU, the y-axes of cameras can be aligned with the gravity, reducing their relative orientation to a single DOF (degree of freedom). In this paper, we use the gravity information to derive extremely efficient minimal solvers for homography-based egomotion estimation from orientation- and scale-covariant features. We use the fact that orientation- and scale-covariant features, such as SIFT or ORB, provide additional constraints on the homography. Based on the prior knowledge about the target plane (horizontal/vertical/general plane, w.r.t. the gravity direction) and using the SIFT/ORB constraints, we derive new minimal solvers that require fewer correspondences than traditional approaches and, thus, speed up the robust estimation procedure significantly. The proposed solvers are compared with the state-of-the-art point-based solvers on both synthetic data and real images, showing comparable accuracy and significant improvement in terms of speed. The implementation of our solvers is available at https://github.com/yaqding/relativepose-sift-gravity",
    "checked": true,
    "id": "fc12b6990c2df17fc8c49c30ada016ce6c8b1cf0",
    "semantic_title": "homography-based egomotion estimation using gravity and sift features",
    "citation_count": 1,
    "authors": [
      "Yaqing Ding",
      "Daniel Barath",
      "Zuzana Kukelova"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Meshgi_Adversarial_Semi-Supervised_Multi-Domain_Tracking_ACCV_2020_paper.html": {
    "title": "Adversarial Semi-Supervised Multi-Domain Tracking",
    "volume": "main",
    "abstract": "Neural networks for multi-domain learning empowers an effective combination of information from different domains by sharing and co-learning the parameters. In visual tracking, the emerging features in shared layers of a multi-domain tracker, trained on various sequences, are crucial for tracking the target in unseen videos. Yet, in a fully shared architecture, some of the emerging features are useful only in a specific domain, reducing the generalization of the learned feature representation. We propose a semi-supervised learning scheme to separate domain-invariant and domain-specific features using adversarial learning, to encourage mutual exclusion between them, and to leverage self-supervised learning for enhancing the shared features using the unlabeled reservoir. By employing these features and training dedicated layers for each sequence, we build a tracker that performs exceptionally on different types of videos",
    "checked": true,
    "id": "910a5ed98d7ac0a5015474d6a4c9cfd8d64ab4af",
    "semantic_title": "adversarial semi-supervised multi-domain tracking",
    "citation_count": 1,
    "authors": [
      "Kourosh Meshgi",
      "Maryam Sadat Mirzaei"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Wang_Lightweight_Single-Image_Super-Resolution_Network_with_Attentive_Auxiliary_Feature_Learning_ACCV_2020_paper.html": {
    "title": "Lightweight Single-Image Super-Resolution Network with Attentive Auxiliary Feature Learning",
    "volume": "main",
    "abstract": "Despite convolutional network-based methods have boosted the performance of single image super-resolution (SISR), the huge computation costs restrict their practical applicability. In this paper, we develop a computation efficient yet accurate network based on the proposed attentive auxiliary features (A^2F) for SISR. Firstly, to explore the features from the bottom layers, the auxiliary feature from all the previous layers are projected into a common space. Then, to better utilize these projected auxiliary features and filter the redundant information, the channel attention is employed to select the most important common feature based on current layer feature. We incorporate these two modules into a block and implement it with a lightweight network. Experimental results on large-scale dataset demonstrate the effectiveness of the proposed algorithm against the state-of-the-art (SOTA) SR methods. Notably, when parameters are less than 320k, A^2F outperforms the SOTA methods for all scales, which proves its ability to better utilize the auxiliary features",
    "checked": true,
    "id": "69154ce1cec7655805d1708b8edc202f072389ca",
    "semantic_title": "lightweight single-image super-resolution network with attentive auxiliary feature learning",
    "citation_count": 41,
    "authors": [
      "Xuehui Wang",
      "Qing Wang",
      "Yuzhi Zhao",
      "Junchi Yan",
      "Lei Fan",
      "Long Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Park_Learning_to_Adapt_to_Unseen_Abnormal_Activities_under_Weak_Supervision_ACCV_2020_paper.html": {
    "title": "Learning to Adapt to Unseen Abnormal Activities under Weak Supervision",
    "volume": "main",
    "abstract": "We present a meta-learning framework for weakly supervised anomaly detection in videos, where the detector learns to adapt to unseen types of abnormal activities effectively when only video-level annotations of binary labels are available.Our work is motivated by the fact that existing methods suffer from poor generalization to diverse unseen examples.We claim that an anomaly detector equipped with a meta-learning scheme alleviates the limitation by leading the model to an initialization point for better optimization.We evaluate the performance of our framework on two challenging datasets, UCF-Crime and ShanghaiTech. The experimental results demonstrate that our algorithm boosts the capability to localize unseen abnormal events in a weakly supervised setting.Besides the technical contributions, we perform the annotation of missing labels in the UCF-Crime dataset and make our task evaluated effectively",
    "checked": true,
    "id": "13fda5b30392d2ac12e9c7fb535ebdca6e194873",
    "semantic_title": "learning to adapt to unseen abnormal activities under weak supervision",
    "citation_count": 5,
    "authors": [
      "Jaeyoo Park",
      "Junha Kim",
      "Bohyung Han"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Irie_Cascaded_Transposed_Long-range_Convolutions_for_Monocular_Depth_Estimation_ACCV_2020_paper.html": {
    "title": "Cascaded Transposed Long-range Convolutions for Monocular Depth Estimation",
    "volume": "main",
    "abstract": "We study the shape of the convolution kernels in the upsampling block for deep monocular depth estimation. First, our empirical analysis shows that the depth estimation accuracy can be improved consistently by only changing the shape of the two consecutive convolution layers with square kernels, e.g., (5 x 5) -> (5 x 5), to two \"long-range\" kernels, one having the transposed shape of the other, e.g., (1 x 25) -> (25 x 1). Second, based on this observation, we propose a new upsampling block called Cascaded Transposed Long-range Convolutions (CTLC) that uses parallel sequences of two long-range convolutions with different kernel shapes. Experiments with NYU Depth V2 and KITTI show that our CTLC offers higher accuracy with fewer parameters and FLOPs than state-of-the-art methods",
    "checked": true,
    "id": "7b0e446e374b0211992e1630e75514c5a15b5030",
    "semantic_title": "cascaded transposed long-range convolutions for monocular depth estimation",
    "citation_count": 1,
    "authors": [
      "Go Irie",
      "Daiki Ikami",
      "Takahito Kawanishi",
      "Kunio Kashino"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/He_Image_Captioning_through_Image_Transformer_ACCV_2020_paper.html": {
    "title": "Image Captioning through Image Transformer",
    "volume": "main",
    "abstract": "Automatic captioning of images is a task that combines the challenges of image analysis and text generation. One important aspect of captioning is the notion of attention: how to decide what to describe and in which order. Inspired by the successes in text analysis and translation, previous works have proposed the transformer architecture for image captioning. However, the structure between the semantic units in images (usually the detected regions from object detection model) and sentences (each single word) is different. Limited work has been done to adapt to the transformer's internal architecture to images. In this work, we introduce the image transformer, which consists of a modified encoding transformer and an implicit decoding transformer, motivated by the relative spatial relationship between image regions. Our design widens the original transformer layer's inner architecture to adapt to the structure of images. With only regions feature as inputs, our model achieves new state-of-the-art performance on both MSCOCO offline and online testing benchmarks. The code is available at https://github.com/wtliao/ImageTransformer",
    "checked": true,
    "id": "657cce51f80e272373ab4fc0dabf5dc8b30c0070",
    "semantic_title": "image captioning through image transformer",
    "citation_count": 76,
    "authors": [
      "Sen He",
      "Wentong Liao",
      "Hamed R. Tavakoli",
      "Michael Yang",
      "Bodo Rosenhahn",
      "Nicolas Pugeault"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Lopez-Rodriguez_Project_to_Adapt_Domain_Adaptation_for_Depth_Completion_from_Noisy_ACCV_2020_paper.html": {
    "title": "Project to Adapt: Domain Adaptation for Depth Completion from Noisy and Sparse Sensor Data",
    "volume": "main",
    "abstract": "Depth completion aims to predict a dense depth map from a sparse depth input. The acquisition of dense ground truth annotations for depth completion settings can be difficult and, at the same time, a significant domain gap between real LiDAR measurements and synthetic data has prevented from successful training of models in virtual settings. We propose a domain adaptation approach for sparse-to-dense depth completion that is trained from synthetic data, without annotations in the real domain or additional sensors. Our approach simulates the real sensor noise in an RGB + LiDAR set-up, and consists of three modules: simulating the real LiDAR input in the synthetic domain via projections, filtering the real noisy LiDAR for supervision and adapting the synthetic RGB image using a CycleGAN approach. We extensively evaluate these modules against the state-of-the-art in the KITTI depth completion benchmark, showing significant improvements",
    "checked": true,
    "id": "7b7a21aa1bdad85237bd73729ddfe33194d162a6",
    "semantic_title": "project to adapt: domain adaptation for depth completion from noisy and sparse sensor data",
    "citation_count": 30,
    "authors": [
      "Adrian Lopez-Rodriguez",
      "Benjamin Busam",
      "Krystian Mikolajczyk"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Chen_Multi-scale_Attentive_Residual_Dense_Network_for_Single_Image_Rain_Removal_ACCV_2020_paper.html": {
    "title": "Multi-scale Attentive Residual Dense Network for Single Image Rain Removal",
    "volume": "main",
    "abstract": "Single image deraining is an urgent yet challenging task since rain streaks severely degrade the image quality and hamper the practical application. The investigation on rain removal has thus been attracting, while the performances of existing deraining have limitations owing to over smoothing effect, poor generalization capability and rain intensity varies both in spatial locations and color channels. To address these issues, we proposed a Multi-scale Attentive Residual Dense Network called MARD-Net in end-to-end manner, to exactly extract the negative rain streaks from rainy images while precisely preserving the image details. The architecture of modified dense network can be used to exploit the rain streaks details representation through feature reuse and propagation. Further, the Multi-scale Attentive Residual Block (MARB) is involved in the dense network to guide the rain streaks feature extraction and representation capability. Since contextual information is very critical for deraining, MARB first uses different convolution kernels along with fusion to extract multi-scale rain features and employs feature attention module to identify rain streaks regions and color channels, as well as has the skip connections to aggregate features at multiple levels and accelerate convergence. The proposed method is extensively evaluated on several frequent-use synthetic and real-world datasets. The quantitative and qualitative results show that the designed framework performs better than the recent state-of-the-art deraining approaches on promoting the rain removal performance and preserving image details under various rain streaks cases",
    "checked": true,
    "id": "c5c3f2301cbf8bd244ff9fc55eaa5c65df18531d",
    "semantic_title": "multi-scale attentive residual dense network for single image rain removal",
    "citation_count": 8,
    "authors": [
      "Xiang Chen",
      "Yufeng Huang",
      "Lei Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Buhler_DeepSEE_Deep_Disentangled_Semantic_Explorative_Extreme_Super-Resolution_ACCV_2020_paper.html": {
    "title": "DeepSEE: Deep Disentangled Semantic Explorative Extreme Super-Resolution",
    "volume": "main",
    "abstract": "Super-resolution (SR) is by definition ill-posed. There are infinitely many plausible high-resolution variants for a given low-resolution natural image. Most of the current literature aims at a single deterministic solution of either high reconstruction fidelity or photo-realistic perceptual quality. In this work, we propose an explorative facial super-resolution framework, DeepSEE, for Deep disentangled Semantic Explorative Extreme super-resolution. To the best of our knowledge, DeepSEE is the first method to leverage semantic maps for explorative super-resolution. In particular, it provides control of the semantic regions, their disentangled appearance and it allows a broad range of image manipulations. We validate DeepSEE on faces, for up to 32x magnification and exploration of the space of super-resolution. Our code and models are available at: https://mcbuehler.github.io/DeepSEE/",
    "checked": true,
    "id": "8280e4bc639a259c2e19b5b6cacd1cef0af6c0fb",
    "semantic_title": "deepsee: deep disentangled semantic explorative extreme super-resolution",
    "citation_count": 29,
    "authors": [
      "Marcel C. Buhler",
      "Andres Romero",
      "Radu Timofte"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Yang_RE-Net_A_Relation_Embedded_Deep_Model_for_AU_Occurrence_and_ACCV_2020_paper.html": {
    "title": "RE-Net: A Relation Embedded Deep Model for AU Occurrence and Intensity Estimation",
    "volume": "main",
    "abstract": "Facial action units (AUs) recognition is a multi-label clas- sification problem, where regular spatial and temporal patterns exist in AU labels due to facial anatomy and human's behavior habits. Ex- ploiting AU correlation is beneficial for obtaining robust AU detector or reducing the dependency of a large amount of AU-labeled samples. Several related works have been done to apply AU correlation to model's objective function or the extracted features. However, this may not be optimal as all the AUs still share the same backbone network, requir- ing to update the model as a whole. In this work, we present a novel AU Relation Embedded deep model (RE-Net) for AU detection that applies the AU correlation to the model's parameter space. Specifically, we format the multi-label AU detection problem as a domain adaptation task and propose a model that contains both shared and AU specific pa- rameters, where the shared parameters are used by all the AUs, and the AU specific parameters are owned by individual AU. The AU relationship based regularization is applied to the AU specific parameters. Extensive experiments on three public benchmarks demonstrate that our method outperforms the previous work and achieves state-of-the-art performance on both AU detection task and AU intensity estimation task",
    "checked": true,
    "id": "afc0db9548d06d2277f38c721a2a3e9cb6c8b6bc",
    "semantic_title": "re-net: a relation embedded deep model for au occurrence and intensity estimation",
    "citation_count": 10,
    "authors": [
      "Huiyuan Yang",
      "Lijun Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Sun_MatchGAN_A_Self-Supervised_Semi-Supervised_Conditional_Generative_Adversarial_Network_ACCV_2020_paper.html": {
    "title": "MatchGAN: A Self-Supervised Semi-Supervised Conditional Generative Adversarial Network",
    "volume": "main",
    "abstract": "We present a novel self-supervised learning approach for conditional generative adversarial networks (GANs) under a semi-supervised setting. Unlike prior self-supervised approaches which often involve geometric augmentations on the image space such as predicting rotation angles, our pretext task leverages the label space. We perform augmentation by randomly sampling sensible labels from the label space of the few labelled examples available and assigning them as target labels to the abundant unlabelled examples from the same distribution as that of the labelled ones. The images are then translated and grouped into positive and negative pairs by their target labels, acting as training examples for our pretext task which involves optimising an auxiliary match loss on the discriminator's side. We tested our method on two challenging benchmarks, CelebA and RaFD, and evaluated the results using standard metrics including Frechet Inception Distance, Inception Score, and Attribute Classification Rate. Extensive empirical evaluation demonstrates the effectiveness of our proposed method over competitive baselines and existing arts. In particular, our method surpasses the baseline with only 20% of the labelled examples used to train the baseline",
    "checked": true,
    "id": "6ae7f9743936e13fd8183ddc2174c724b63eced9",
    "semantic_title": "matchgan: a self-supervised semi-supervised conditional generative adversarial network",
    "citation_count": 9,
    "authors": [
      "Jiaze Sun",
      "Binod Bhattarai",
      "Tae-Kyun Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Elkerdawy_To_Filter_Prune_or_to_Layer_Prune_That_Is_The_ACCV_2020_paper.html": {
    "title": "To Filter Prune, or to Layer Prune, That Is The Question",
    "volume": "main",
    "abstract": "Recent advances in pruning of neural networks have made it possible to remove a large number of filters or weights without any perceptible drop in accuracy. The number of parameters and that of FLOPs are usually the reported metrics to measure the quality of the pruned models. However, the gain in speed for these pruned models is often overlooked in the literature due to the complex nature of latency measurements. In this paper, we show the limitation of filter pruning methods in terms of latency reduction and propose LayerPrune framework. LayerPrune presents a set of layer pruning methods based on different criteria that achieve higher latency reduction than filter pruning methods on similar accuracy. The advantage of layer pruning over filter pruning in terms of latency reduction is a result of the fact that the former is not constrained by the original model's depth and thus allows for a larger range of latency reduction. For each filter pruning method we examined, we use the same filter importance criterion to calculate a per-layer importance score in one-shot. We then prune the least important layers and fine-tune the shallower model which obtains comparable or better accuracy than its filter-based pruning counterpart. This one-shot process allows to remove layers from single path networks like VGG before fine-tuning, unlike in iterative filter pruning, a minimum number of filters per layer is required to allow for data flow which constraint the search space. To the best of our knowledge, we are the first to examine the effect of pruning methods on latency metric instead of FLOPs for multiple networks, datasets and hardware targets. LayerPrune also outperforms handcrafted architectures such as Shufflenet, MobileNet, MNASNet and ResNet18 by 7.3%, 4.6%, 2.8% and 0.5% respectively on similar latency budget on ImageNet dataset",
    "checked": true,
    "id": "359b84e8ad62781aa26bc15523f76c11423daa17",
    "semantic_title": "to filter prune, or to layer prune, that is the question",
    "citation_count": 18,
    "authors": [
      "Sara Elkerdawy",
      "Mostafa Elhoushi",
      "Abhineet Singh",
      "Hong Zhang",
      "Nilanjan Ray"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Huang_Addressing_Class_Imbalance_in_Scene_Graph_Parsing_by_Learning_to_ACCV_2020_paper.html": {
    "title": "Addressing Class Imbalance in Scene Graph Parsing by Learning to Contrast and Score",
    "volume": "main",
    "abstract": "Scene graph parsing aims to detect objects in an image scene and recognize their relations. Recent approaches have achieved high average scores on some popular benchmarks, but fail in detecting rare relations, as the highly long-tailed distribution of data biases the learning towards frequent labels. Motivated by the fact that detecting these rare relations can be critical in real-world applications, this paper introduces a novel integrated framework of classification and ranking to resolve the class imbalance problem in scene graph parsing. Specifically, we design a new Contrasting Cross-Entropy loss, which promotes the detection of rare relations by suppressing incorrect frequent ones. Furthermore, we propose a novel scoring module, termed as Scorer, which learns to rank the relations based on the image features and relation features to improve the recall of predictions. Our framework is simple and effective, and can be incorporated into current scene graph models. Experimental results show that the proposed approach improves the current state-of-the-art methods, with a clear advantage of detecting rare relations",
    "checked": true,
    "id": "e3b738746c4300f944fb14d556bdfc03c9d8c901",
    "semantic_title": "addressing class imbalance in scene graph parsing by learning to contrast and score",
    "citation_count": 3,
    "authors": [
      "He Huang",
      "Shunta Saito",
      "Yuta Kikuchi",
      "Eiichi Matsumoto",
      "Wei Tang",
      "Philip S. Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Price_Play_Fair_Frame_Contributions_in_Video_Models_ACCV_2020_paper.html": {
    "title": "Play Fair: Frame Contributions in Video Models",
    "volume": "main",
    "abstract": "In this paper, we introduce an attribution method for explaining action recognition models. Such models fuse information from multiple frames within a video, through score aggregation or relational reasoning. We break down a model's class score into the sum of contributions from each frame, fairly. Our method adapts an axiomatic solution to fair reward distribution in cooperative games, known as the Shapley value, for elements in a variable-length sequence, which we call the Element Shapley Value (ESV). Critically, we propose a tractable approximation of ESV that scales linearly with the number of frames in the sequence.We employ ESV to explain two action recognition models (TRN and TSN) on the fine-grained dataset Something-Something. We offer detailed analysis of supporting/distracting frames, and the relationships of ESVs to the frame's position, class prediction, and sequence length. We compare ESV to naive baselines and two commonly used attribution methods: Grad-CAM and Integrated-Gradients",
    "checked": true,
    "id": "f6378d6130b82f083882db4c0bf2065b6a3e59b8",
    "semantic_title": "play fair: frame attributions in video models",
    "citation_count": 4,
    "authors": [
      "Will Price",
      "Dima Damen"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Chang_TinyGAN_Distilling_BigGAN_for_Conditional_Image_Generation_ACCV_2020_paper.html": {
    "title": "TinyGAN: Distilling BigGAN for Conditional Image Generation",
    "volume": "main",
    "abstract": "Generative Adversarial Networks (GANs) have become a powerful approach for generative image modeling. However, GANs are notorious for their training instability, especially on large-scale, complex datasets. While the recent work of BigGAN has significantly improved the quality of image generation on ImageNet, it requires a huge model, making it hard to deploy on resource-constrained devices. To reduce the model size, we propose a black-box knowledge distillation framework for compressing GANs, which highlights a stable and efficient training process. Given BigGAN as the teacher network, we manage to train a much smaller student network to mimic its functionality, achieving competitive performance on Inception and FID scores with the generator having 16 times fewer parameters",
    "checked": true,
    "id": "66307b71a3363739d7492f417b7ac2cb76fe7168",
    "semantic_title": "tinygan: distilling biggan for conditional image generation",
    "citation_count": 23,
    "authors": [
      "Ting-Yun Chang",
      "Chi-Jen Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Vojir_Efficient_Large-Scale_Semantic_Visual_Localization_in_2D_Maps_ACCV_2020_paper.html": {
    "title": "Efficient Large-Scale Semantic Visual Localization in 2D Maps",
    "volume": "main",
    "abstract": "With the emergence of autonomous navigation systems, image-based localization is one of the essential tasks to be tackled. However, most of the current algorithms struggle to scale to city-size environments mainly because of the need to collect large (semi-)annotated datasets for CNN training and create databases for test environment of images, keypoint level features or image embeddings. This data acquisition is not only expensive and time-consuming but also may cause privacy concerns. In this work, we propose a novel framework for semantic visual localization in city-scale environments which alleviates the aforementioned problem by using freely available 2D maps such as OpenStreetMap. Our method does not require any images or image-map pairs for training or test environment database collection. Instead, a robust embedding is learned from a depth and building instance label information of a particular location in the 2D map. At test time, this embedding is extracted from a panoramic building instance label and depth images. It is then used to retrieve the closest match in the database.We evaluate our localization framework on two large-scale datasets consisting of Cambridge and San Francisco cities with a total length of drivable roads spanning over 500 km and including approximately 110k unique locations. To the best of our knowledge, this is the first large-scale semantic localization method which works on par with approaches that require the availability of images at train time or for test environment database creation",
    "checked": true,
    "id": "cb7e85db8f3566a644dd0e9e57c887b38a54d96d",
    "semantic_title": "efficient large-scale semantic visual localization in 2d maps",
    "citation_count": 6,
    "authors": [
      "Tomas Vojir",
      "Ignas Budvytis",
      "Roberto Cipolla"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Tseng_Regularizing_Meta-Learning_via_Gradient_Dropout_ACCV_2020_paper.html": {
    "title": "Regularizing Meta-Learning via Gradient Dropout",
    "volume": "main",
    "abstract": "With the growing attention on learning-to-learn new tasks using only a few examples, meta-learning has been widely used in numerous problems such as few-shot classification, reinforcement learning, and domain generalization. However, meta-learning models are prone to overfitting when there are no sufficient training tasks for the meta-learners to generalize. Although existing approaches such as Dropout are widely used to address the overfitting problem, these methods are typically designed for regularizing models of a single task in supervised training.In this paper, we introduce a simple yet effective method to alleviate the risk of overfitting for gradient-based meta-learning. Specifically, during the gradient-based adaptation stage, we randomly drop the gradient in the inner-loop optimization of each parameter in deep neural networks, such that the augmented gradients improve generalization to new tasks. We present a general form of the proposed gradient dropout regularization and show that this term can be sampled from either the Bernoulli or Gaussian distribution. To validate the proposed method, we conduct extensive experiments and analysis on numerous computer vision tasks, demonstrating that the gradient dropout regularization mitigates the overfitting problem and improves the performance upon various gradient-based meta-learning frameworks",
    "checked": true,
    "id": "7b201e42e32430d951458916810a7dbf1e946a6d",
    "semantic_title": "regularizing meta-learning via gradient dropout",
    "citation_count": 28,
    "authors": [
      "Hung-Yu Tseng",
      "Yi-Wen Chen",
      "Yi-Hsuan Tsai",
      "Sifei Liu",
      "Yen-Yu Lin",
      "Ming-Hsuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Behera_Rotation_Axis_Focused_Attention_Network_RAFA-Net_for_Estimating_Head_Pose_ACCV_2020_paper.html": {
    "title": "Rotation Axis Focused Attention Network (RAFA-Net) for Estimating Head Pose",
    "volume": "main",
    "abstract": "Head pose is a vital indicator of human attention and behavior. Therefore, automatic estimation of head pose from images is key to many real-world applications. In this paper, we propose a novel approach for head pose estimation from a single RGB image. Many existing approaches often predict head poses by localizing facial landmarks and then solve 2D to 3D correspondence problem with a mean head model. Such approaches completely rely on the landmark detection accuracy, an ad-hoc alignment step, and the extraneous head model. To address this drawback, we present an end-to-end deep network, which explores rotation axis (yaw, pitch, and roll) focused innovative attention mechanism to capture the subtle changes in images. The mechanism uses attentional spatial pooling from a self-attention layer and learns the importance over fine-grained to coarse spatial structures and combine them to capture rich semantic information concerning a given rotation axis. The experimental evaluation of our approach using three benchmark datasets is very competitive to state-of-the-art methods, including with and without landmark-based approaches",
    "checked": true,
    "id": "d4b060b16329c6e44cc78b0e293f8c7ba3ca5a01",
    "semantic_title": "rotation axis focused attention network (rafa-net) for estimating head pose",
    "citation_count": 5,
    "authors": [
      "Ardhendu Behera",
      "Zachary Wharton",
      "Pradeep Hewage",
      "Swagat Kumar"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Shi_Discrete_Spatial_Importance-Based_Deep_Weighted_Hashing_ACCV_2020_paper.html": {
    "title": "Discrete Spatial Importance-Based Deep Weighted Hashing",
    "volume": "main",
    "abstract": "Hashing is a widely used technique for large-scale approximate nearest neighbor searching in multimedia retrieval. Recent works have proved that using deep neural networks is a promising solution for learning both feature representation and hash codes. However, most existing deep hashing methods directly learn hash codes from a convolutional neural network, ignoring the spatial importance distribution of images. The loss of spatial importance negatively affects the performance of hash learning and thus reduces its accuracy. To address this issue, we propose a new deep hashing method with weighted spatial information, which generates hash codes by using discrete spatial importance distribution. In particular, to extract the discrete spatial importance information of images effectively, we propose a method to learn the spatial attention map and hash code simultaneously, which makes the spatial attention map more conductive to hash-based retrieval. The experimental results of three widely used datasets show that the proposed deep weighted hashing method is superior to the state-of-the-art hashing method",
    "checked": true,
    "id": "d6d057da35e8c51de9caa5bcf3d630d01bb1fc6c",
    "semantic_title": "discrete spatial importance-based deep weighted hashing",
    "citation_count": 2,
    "authors": [
      "Yang Shi",
      "Xiushan Nie",
      "Quan Zhou",
      "Xiaoming Xi",
      "Yilong Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Sun_Local_Facial_Makeup_Transfer_via_Disentangled_Representation_ACCV_2020_paper.html": {
    "title": "Local Facial Makeup Transfer via Disentangled Representation",
    "volume": "main",
    "abstract": "Facial makeup transfer aims to render a non-makeup face image in an arbitrary given makeup one while preserving face identity. The most advanced method separates makeup style information from face images to realize makeup transfer. However, makeup style includes several semantic clear local styles which are still entangled together. In this paper, we propose a novel unified adversarial disentangling network to further decompose face images into four independent components, i.e., personal identity, lips makeup style, eyes makeup style and face makeup style. Owing to the disentangled makeup representation, our method can not only flexible control the degree of local makeup styles, but also can transfer local makeup styles from different images into the final result, which any other approaches fail to handle. For makeup removal, different from other methods which regard makeup removal as the reverse process of makeup transfer, we integrate the makeup transfer with the makeup removal into one uniform framework and obtain multiple makeup removal results. Extensive experiments have demonstrated that our approach can produce visually pleasant and accurate makeup transfer results compared to the state-of-the-art methods",
    "checked": true,
    "id": "2012af145e850d18fdcdf9632135452ad729c5b7",
    "semantic_title": "local facial makeup transfer via disentangled representation",
    "citation_count": 4,
    "authors": [
      "Zhaoyang Sun",
      "Feng Liu",
      "Wen Liu",
      "Shengwu Xiong",
      "Wenxuan Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Duan_Channel_Pruning_for_Accelerating_Convolutional_Neural_Networks_via_Wasserstein_Metric_ACCV_2020_paper.html": {
    "title": "Channel Pruning for Accelerating Convolutional Neural Networks via Wasserstein Metric",
    "volume": "main",
    "abstract": "Channel pruning is an effective way to accelerate deep convolutional neural networks. However, it is still a challenge to reduce the computational complexity while preserving the performance of deep models. In this paper, we propose a novel channel pruning method via the Wasserstein metric. First, the output features of a channel are aggregated through the Wasserstein barycenter, which is called the basic response of the channel. Then the channel discrepancy based on the Wasserstein distance is introduced to measure channel importance, by considering both the channel's feature representation ability and the substitutability of the basic responses. Finally, channels with the least discrepancies are removed directly, and the loss in accuracy of the pruned model is regained by fine-tuning. Extensive experiments on popular benchmarks and various network architectures demonstrate that the proposed approach outperforms the existing methods",
    "checked": true,
    "id": "486bc6df94a550ca2677751e6fca30fb2102a92d",
    "semantic_title": "channel pruning for accelerating convolutional neural networks via wasserstein metric",
    "citation_count": 1,
    "authors": [
      "Haoran Duan",
      "Hui Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Qian_Long-Term_Cloth-Changing_Person_Re-identification_ACCV_2020_paper.html": {
    "title": "Long-Term Cloth-Changing Person Re-identification",
    "volume": "main",
    "abstract": "Person re-identification (Re-ID) aims to match a target person across camera views at different locations and times. Existing Re-ID studies focus on the short-term cloth-consistent setting, under which a person re-appears in different camera views with the same outfit. A discriminative feature representation learned by existing deep Re-ID models is thus dominated by the visual appearance of clothing. In this work, we focus on a much more difficult yet practical setting where person matching is conducted over long-duration, e.g., over days and months and therefore inevitably under the new challenge of changing clothes. This problem, termed Long-Term Cloth-Changing (LTCC) Re-ID is much understudied due to the lack of large scale datasets. The first contribution of this work is a new LTCC dataset containing people captured over a long period of time with frequent clothing changes. As a second contribution, we propose a novel Re-ID method specifically designed to address the cloth-changing challenge. Specifically, we consider that under cloth-changes, soft-biometrics such as body shape would be more reliable. We, therefore, introduce a shape embedding module as well as a cloth-elimination shape-distillation module aiming to eliminate the now unreliable clothing appearance features and focus on the body shape information. Extensive experiments show that superior performance is achieved by the proposed model on the new LTCC dataset. The dataset is available on the project website: https: //naiq.github.io/LTCC_Perosn_ReID.html",
    "checked": true,
    "id": "cf430a05fffa6f2b2f07985504bdccba78ed09b9",
    "semantic_title": "long-term cloth-changing person re-identification",
    "citation_count": 103,
    "authors": [
      "Xuelin Qian",
      "Wenxuan Wang",
      "Li Zhang",
      "Fangrui Zhu",
      "Yanwei Fu",
      "Tao Xiang",
      "Yu-Gang Jiang",
      "Xiangyang Xue"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Fang_Channel_Recurrent_Attention_Networks_for_Video_Pedestrian_Retrieval_ACCV_2020_paper.html": {
    "title": "Channel Recurrent Attention Networks for Video Pedestrian Retrieval",
    "volume": "main",
    "abstract": "Full attention, which generates an attention value per element of the input feature maps, has been successfully demonstrated to be beneficial in visual tasks. In this work, we propose a fully attentional network, termed channel recurrent attention network, for the task of video pedestrian retrieval. The main attention unit, channel recurrent attention, identifies attention maps at the frame level by jointly leveraging spatial and channel patterns via a recurrent neural network. This channel recurrent attention is designed to build a global receptive field by recurrently receiving and learning the spatial vectors. Then, a set aggregation cell is employed to generate a compact video representation. Empirical experimental results demonstrate the superior performance of the proposed deep network, outperforming current state-of-the-art results across standard video person retrieval benchmarks, and a thorough ablation study shows the effectiveness of the proposed units",
    "checked": true,
    "id": "4782214c4adbc6df05d850e4c17810bfb9f1bc5d",
    "semantic_title": "channel recurrent attention networks for video pedestrian retrieval",
    "citation_count": 1,
    "authors": [
      "Pengfei Fang",
      "Pan Ji",
      "Jieming Zhou",
      "Lars Petersson",
      "Mehrtash Harandi"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Raab_Bridging_Adversarial_and_Statistical_Domain_Transfer_via_Spectral_Adaptation_Networks_ACCV_2020_paper.html": {
    "title": "Bridging Adversarial and Statistical Domain Transfer via Spectral Adaptation Networks",
    "volume": "main",
    "abstract": "Statistical and adversarial adaptation are currently two extensive categories of neural network architectures in unsupervised deep domain adaptation. The latter has become the new standard due to its good theoretical foundation and empirical performance. However, there are two shortcomings. First, recent studies show that these approaches focus too much on easily transferable features and thus neglect important discriminative information. Second, adversarial networks are challenging to train. We addressed the first issue by the alignment of transferable spectral properties within an adversarial model to balance the focus between the easily transferable features and the necessary discriminatory features, while at the same time limiting the learning of domain-specific semantics by relevance considerations.Second, we stabilized the discriminator networks training procedure by Spectral Normalization employing the Lipschitz continuous gradients.We provide a theoretical and empirical evaluation of our improved approach and show its effectiveness in a performance study on standard benchmark data sets against various other state of the art methods",
    "checked": true,
    "id": "22f33c017506c8b14ea7273335ecb4848a1c4334",
    "semantic_title": "bridging adversarial and statistical domain transfer via spectral adaptation networks",
    "citation_count": 8,
    "authors": [
      "Christoph Raab",
      "Philipp Vath",
      "Peter Meier",
      "Frank-Michael Schleif"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Jenni_Self-Supervised_Multi-View_Synchronization_Learning_for_3D_Pose_Estimation_ACCV_2020_paper.html": {
    "title": "Self-Supervised Multi-View Synchronization Learning for 3D Pose Estimation",
    "volume": "main",
    "abstract": "Current state of the art methods cast monocular 3D human pose estimation as a learning problem by training neural networks on costly large data sets of images and corresponding skeleton poses. In contrast, we propose an approach that can exploit small annotated data sets by fine-tuning networks pre-trained via self-supervised learning on (large) unlabeled data sets. To drive such models in the pre-training step towards supporting 3D pose estimation, we introduce a novel self-supervised feature learning task designed to focus on the 3D structure in an image. We exploit images extracted from videos captured with a multi-view camera system. The task is to classify whether two images depict two views of the same scene up to a rigid transformation. In a multi-view data set, where objects deform in a non-rigid manner, a rigid transformation occurs only between two views taken at the exact same time, i.e., when they are synchronized.We demonstrate the effectiveness of the synchronization task on the Human3.6M data set and achieve state-of-the-art results in 3D human pose estimation",
    "checked": true,
    "id": "537e0373a8efdb939a7501f847d28e143f271784",
    "semantic_title": "self-supervised multi-view synchronization learning for 3d pose estimation",
    "citation_count": 8,
    "authors": [
      "Simon Jenni",
      "Paolo Favaro"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Xu_A_Global_to_Local_Double_Embedding_Method_for_Multi-person_Pose_ACCV_2020_paper.html": {
    "title": "A Global to Local Double Embedding Method for Multi-person Pose Estimation",
    "volume": "main",
    "abstract": "Multi-person pose estimation is a fundamental and challeng-ing problem to many computer vision tasks. Most existing methods canbe broadly categorized into two classes: top-down and bottom-up meth-ods. Both of the two types of methods involve two stages, namely, persondetection and joints detection. Conventionally, the two stages are imple-mented separately without considering their interactions between them,and this may inevitably cause some issue intrinsically. In this paper, wepresent a novel method to simplify the pipeline by implementing per-son detection and joints detection simultaneously. We propose a DoubleEmbedding (DE) method to complete the multi-person pose estimationtask in a global-to-local way. DE consists of Global Embedding (GE)and Local Embedding (LE). GE encodes different person instances andprocesses information covering the whole image and LE encodes the lo-cal limbs information. GE functions for the person detection in top-downstrategy while LE connects the rest joints sequentially which functionsfor joint grouping and information processing in A bottom-up strategy.Based on LE, we design the Mutual Refine Machine (MRM) to reducethe prediction difficulty in complex scenarios. MRM can effectively re-alize the information communicating between keypoints and further im-prove the accuracy. We achieve state-of-the-art results on benchmarksMSCOCO, MPII and CrowdPose, demonstrating the effectiveness andgeneralization ability of our method",
    "checked": true,
    "id": "b26e4a82c7aeac120448ec7308afec0c4de9a22c",
    "semantic_title": "a global to local double embedding method for multi-person pose estimation",
    "citation_count": 0,
    "authors": [
      "Yiming Xu",
      "Jiaxin Li",
      "Yan Ding",
      "Hua-Liang Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Fujimura_Dehazing_Cost_Volume_for_Deep_Multi-view_Stereo_in_Scattering_Media_ACCV_2020_paper.html": {
    "title": "Dehazing Cost Volume for Deep Multi-view Stereo in Scattering Media",
    "volume": "main",
    "abstract": "We propose a learning-based multi-view stereo (MVS) method in scattering media such as fog or smoke with a novel cost volume, called the dehazing cost volume. An image captured in scattering media degrades due to light scattering and attenuation caused by suspended particles. This degradation depends on scene depth; thus it is difficult for MVS to evaluate photometric consistency because the depth is unknown before three-dimensional reconstruction. Our dehazing cost volume can solve this chicken-and-egg problem of depth and scattering estimation by computing the scattering effect using swept planes in the cost volume. Experimental results on synthesized hazy images indicate the effectiveness of our dehazing cost volume against the ordinary cost volume regarding scattering media. We also demonstrated the applicability of our dehazing cost volume to real foggy scenes",
    "checked": true,
    "id": "9097f463fe7373fe7da7aea275e7616cb5e83805",
    "semantic_title": "dehazing cost volume for deep multi-view stereo in scattering media with airlight and scattering coefficient estimation",
    "citation_count": 2,
    "authors": [
      "Yuki Fujimura",
      "Motoharu Sonogashira",
      "Masaaki Iiyama"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Liu_Learning_Global_Pose_Features_in_Graph_Convolutional_Networks_for_3D_ACCV_2020_paper.html": {
    "title": "Learning Global Pose Features in Graph Convolutional Networks for 3D Human Pose Estimation",
    "volume": "main",
    "abstract": "As the human body skeleton can be represented as a sparse graph, it is natural to exploit graph convolutional networks (GCNs) to model the articulated body structure for 3D human pose estimation (HPE). However, a vanilla graph convolutional layer, the building block of a GCN, only models the local relationships between each body joint and their neighbors on the skeleton graph. Some global attributes, e.g., the action of the person, can be critical to 3D HPE, especially in the case of occlusion or depth ambiguity. To address this issue, this paper introduces a new 3D HPE framework by learning global pose features in GCNs. Specifically, we add a global node to the graph and connect it to all the body joint nodes. On one hand, global features are updated by aggregating all body joint features to model the global attributes. On the other hand, the feature update of each body joint depends on not only their neighbors but also the global node. Furthermore, we propose a heterogeneous multi-task learning framework to learn the local and global features. While each local node regresses the 3D coordinate of the corresponding body joint, we force the global node to classify an action category or learn a low-dimensional pose embedding. Experimental results demonstrate the effectiveness of the proposed approach",
    "checked": true,
    "id": "6c5f6531d4c983d7cb64473a67a80f9d0e9fac48",
    "semantic_title": "learning global pose features in graph convolutional networks for 3d human pose estimation",
    "citation_count": 19,
    "authors": [
      "Kenkun Liu",
      "Zhiming Zou",
      "Wei Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Yamamichi_MCGKT-Net_Multi-level_Context_Gating_Knowledge_Transfer_Network_for_Single_Image_ACCV_2020_paper.html": {
    "title": "MCGKT-Net: Multi-level Context Gating Knowledge Transfer Network for Single Image Deraining",
    "volume": "main",
    "abstract": "Rain streak removal in a single image is very challenging task due to its ill-pose nature in essence. Recently, the end-to-end learning techniques with deep convolutional neural networks(DCNN) have made great progress in this task. However, the conventional DCNN-based deraining methods have struggled to exploit deeper and more complex network architectures for pursuing better performance. This study proposes a novel MCGKT-Net for boosting deraining performance, which is naturally multi-scale learning framework being capable of exploring multi-scale attributes of rain streaks and different semantic structures of the clear images. In order to obtain high representative features inside MCGKT-Net, we explore internal knowledge transfer module using ConvLSTM unit for conducting interaction learning between different layers, and investigate external knowledge transfer module for leveraging the knowledge already learned in other task domains. Furthermore, to dynamically select useful features in learning procedure, we propose a multi-scale context gating module in the MCGKT-Net using squeeze-and-excitation block. Experiments on three benchmark datasets: Rain100H, Rain100L and Rain800, manifest impressive performance compared with state-of-the-art methods",
    "checked": true,
    "id": "b11f5c1501c9e84e47b74a1dbda1e3be5ce399b7",
    "semantic_title": "mcgkt-net: multi-level context gating knowledge transfer network for single image deraining",
    "citation_count": 3,
    "authors": [
      "Kohei Yamamichi",
      "Xian-Hua Han"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Ranade_Mapping_of_Sparse_3D_Data_using_Alternating_Projection_ACCV_2020_paper.html": {
    "title": "Mapping of Sparse 3D Data using Alternating Projection",
    "volume": "main",
    "abstract": "We propose a novel technique to register sparse 3D scans in the absence of texture. While existing methods such as KinectFusion or Iterative Closest Points (ICP) heavily rely on dense point clouds, this task is particularly challenging under sparse conditions without RGB data. Sparse texture-less data does not come with high-quality boundary signal, and this prohibits the use of correspondences from corners, junctions, or boundary lines. Moreover, in the case of sparse data, it is incorrect to assume that the same point will be captured in two consecutive scans. We take a different approach and first re-parameterize the point-cloud using a large number of line segments. In this re-parameterized data, there exists a large number of line intersection (and not correspondence) constraints that allow us to solve the registration task. We propose the use of a two-step alternating projection algorithm by formulating the registration as the simultaneous satisfaction of intersection and rigidity constraints. Despite the simplicity, the proposed approach outperforms other top-scoring algorithms on both Kinect and LiDAR datasets. In Kinect, we can use 100X downsampled sparse data and still outperform competing methods operating on full-resolution data",
    "checked": true,
    "id": "cc56a36287770ce72b778a95c955b79babfd2177",
    "semantic_title": "mapping of sparse 3d data using alternating projection",
    "citation_count": 2,
    "authors": [
      "Siddhant Ranade",
      "Xin Yu",
      "Shantnu Kakkar",
      "Pedro Miraldo",
      "Srikumar Ramalingam"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Wang_Fully_Supervised_and_Guided_Distillation_for_One-Stage_Detectors_ACCV_2020_paper.html": {
    "title": "Fully Supervised and Guided Distillation for One-Stage Detectors",
    "volume": "main",
    "abstract": "Model distillation has been extended from image classification to object detection. However, existing approaches are difficult to focus on both object regions and false detection regions of student networks to effectively distill the feature representation from teacher networks. To address it, we propose a fully supervised and guided distillation algorithm for one-stage detectors, where an excitation and suppression loss is designed to make a student network mimic the feature representation of a teacher network in the object regions and its own high-response regions in the background, so as to excite the feature expression of object regions and adaptively suppress the feature expression of high-response regions that may cause false detections. Besides, a process-guided learning strategy is proposed to train the teacher along with the student and transfer knowledge throughout the training process. Extensive experiments on Pascal VOC and COCO benchmarks demonstrate the following advantages of our algorithm, including the effectiveness for improving recall and reducing false detections, the robustness on common one-stage detector heads and the superiority compared with state-of-the-art methods",
    "checked": true,
    "id": "4d7580be72a7d996dd436f65740694990a20542a",
    "semantic_title": "fully supervised and guided distillation for one-stage detectors",
    "citation_count": 3,
    "authors": [
      "Deyu Wang",
      "Dongchao Wen",
      "Junjie Liu",
      "Wei Tao",
      "Tse-Wei Chen",
      "Kinya Osa",
      "Masami Kato"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Lee_VAN_Versatile_Affinity_Network_for_End-to-end_Online_Multi-Object_Tracking_ACCV_2020_paper.html": {
    "title": "VAN: Versatile Affinity Network for End-to-end Online Multi-Object Tracking",
    "volume": "main",
    "abstract": "In recent years, tracking-by-detection has become the most popular multi-object tracking (MOT) method, and deep convolutional neural networks (CNNs)-based appearance features have been successfully applied to enhance the performance of candidate association. Several MOT methods adopt single-object tracking (SOT) and handcrafted rules to deal with incomplete detection, resulting in numerous false positives (FPs) and false negatives (FNs). However, a separately trained SOT network is not directly adaptable because domains can differ, and handcrafted rules contain a considerable number of hyperparameters, thus making it difficult to optimize the MOT method. To address this issue, we propose a versatile affinity network (VAN) that can perform the entire MOT process in a single network including target specific SOT to handle incomplete detection issues, affinity computation between target and candidates, and decision of tracking termination. We train the VAN in an end-to-end manner by using event-aware learning that is designed to reduce the potential error caused by FNs, FPs, and identity switching. The proposed VAN significantly reduces the number of hyperparameters and handcrafted rules required for the MOT framework and successfully improves the MOT performance. We implement the VAN using two baselines with different candidate refinement methods to demonstrate the effects of the proposed VAN. We also conduct extensive experiments including ablation studies on three public benchmark datasets: 2D MOT2015, MOT2016, and MOT2017. The results indicate that the proposed method successfully improves the object tracking performance compared with that of baseline methods, and outperforms recent state-of-the-art MOT methods in terms of several tracking metrics including MOT accuracy (MOTA), identity F1 score (IDF1), percentage of mostly tracked targets (MT), and FP",
    "checked": true,
    "id": "58c4208ec0868e6fcaade4f5450bb40e0afa4604",
    "semantic_title": "van: versatile affinity network for end-to-end online multi-object tracking",
    "citation_count": 5,
    "authors": [
      "Hyemin Lee",
      "Inhan Kim",
      "Daijin Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Shiraki_Spatial_Temporal_Attention_Graph_Convolutional_Networks_with_Mechanics-Stream_for_Skeleton-based_ACCV_2020_paper.html": {
    "title": "Spatial Temporal Attention Graph Convolutional Networks with Mechanics-Stream for Skeleton-based Action Recognition",
    "volume": "main",
    "abstract": "The static relationship between joints and the dynamic importance of joints leads to high accuracy in skeletal action recognition. Nevertheless, existing methods define the graph structure beforehand by skeletal patterns, so they cannot capture features considering the relationship between joints specific to actions. Moreover, the importance of joints is expected to be different for each action. We propose spatial-temporal attention graph convolutional networks (STA-GCN). It acquires an attention edge that represents a static relationship between joints for each action and an attention node that represents the dynamic importance of joints for each time. STA-GCN is the first method to consider joint importance and relationship at the same time. The proposed method consists of multiple networks, that reflect the difference of spatial (coordinates) and temporal (velocity and acceleration) characteristics as mechanics-stream. We aggregate these network predictions as final result. We show the potential that the attention edge and node can be easily applied to existing methods and improve the performance. Experimental results with NTU-RGB+D and NTU-RGB+D120 demonstrate that it is possible to obtain a attention edge and node specific to the action that can explain behavior and achieves state-of-the-art performances",
    "checked": true,
    "id": "4280ab3111c6d4922a832f11f7a073afe062736d",
    "semantic_title": "spatial temporal attention graph convolutional networks with mechanics-stream for skeleton-based action recognition",
    "citation_count": 5,
    "authors": [
      "Katsutoshi Shiraki",
      "Tsubasa Hirakawa",
      "Takayoshi Yamashita",
      "Hironobu Fujiyoshi"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Benz_Double_Targeted_Universal_Adversarial_Perturbations_ACCV_2020_paper.html": {
    "title": "Double Targeted Universal Adversarial Perturbations",
    "volume": "main",
    "abstract": "Despite their impressive performance, deep neural networks (DNNs) are widely known to be vulnerable to adversarial attacks, which makes it challenging for them to be deployed in security-sensitive applications, such as autonomous driving. Image-dependent perturbations can fool a network for one specific image, while universal adversarial perturbations are capable of fooling a network for samples from all classes without selection. We introduce a double targeted universal adversarial perturbations (DT-UAPs) to bridge the gap between the instance-discriminative image-dependent perturbations and the generic universal perturbations. This universal perturbation attacks one targeted source class to sink class, while having a limited adversarial effect on other non-targeted source classes, for avoiding raising suspicions. Targeting the source and sink class simultaneously, we term it double targeted attack (DTA). This provides an attacker with the freedom to perform precise attacks on a DNN model while raising little suspicion. We show the effectiveness of the proposed DTA algorithm on a wide range of datasets and also demonstrate its potential as a physical attack",
    "checked": true,
    "id": "42cf02f9e21aef51152631f2da48cb40b4855afd",
    "semantic_title": "double targeted universal adversarial perturbations",
    "citation_count": 42,
    "authors": [
      "Philipp Benz",
      "Chaoning Zhang",
      "Tooba Imtiaz",
      "In So Kweon"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Han_MMD_based_Discriminative_Learning_for_Face_Forgery_Detection_ACCV_2020_paper.html": {
    "title": "MMD based Discriminative Learning for Face Forgery Detection",
    "volume": "main",
    "abstract": "Face forensic detection is to distinguish manipulated from pristine face images. The main drawback of existing face forensics detection methods is their limited generalization ability due to differences in domains. Furthermore, artifacts such as imaging variations or face attributes do not persistently exist among all generated results for a single generation method.Therefore, in this paper, we propose a novel framework to address the domain gap induced by multiple deep fake datasets. To this end, the maximum mean discrepancy (MMD) loss is incorporated to align the different feature distributions. The center and triplet losses are added to enhance generalization. This addition ensures that the learned features are shared by multiple domains and provides better generalization abilities to unseen deep fake samples. Evaluations on various deep fake benchmarks (DFTIMIT, UADFV, Celeb-DF, and FaceForensics++) show that the proposed method achieves the best overall performance. An ablation study is performed to investigate the effect of the different components and style transfer losses",
    "checked": true,
    "id": "3048e73038f7bdc5fc2566c36c711cab5342c53f",
    "semantic_title": "mmd based discriminative learning for face forgery detection",
    "citation_count": 4,
    "authors": [
      "Jian Han",
      "Theo Gevers"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Ho_A_Two-Stage_Minimum_Cost_Multicut_Approach_to_Self-Supervised_Multiple_Person_ACCV_2020_paper.html": {
    "title": "A Two-Stage Minimum Cost Multicut Approach to Self-Supervised Multiple Person Tracking",
    "volume": "main",
    "abstract": "Multiple Object Tracking (MOT) is a long-standing task in computer vision. Current approaches based on the tracking by detection paradigm either require some sort of domain knowledge or supervision to associate data correctly into tracks. In this work, we present a self-supervised multiple object tracking approach based on visual features and minimum cost lifted multicuts. Our method is based on straight-forward spatio-temporal cues that can be extracted from neighboring frames in an image sequences without supervision. Clustering based on these cues enables us to learn the required appearance invariances for the tracking task at hand and train an AutoEncoder to generate suitable latent representations. Thus, the resulting latent representations can serve as robust appearance cues for tracking even over large temporal distances where no reliable spatio-temporal features can be extracted. We show that, despite being trained without using the provided annotations, our model provides competitive results on the challenging MOT Benchmark for pedestrian tracking",
    "checked": true,
    "id": "d866e13a7276910e430597a492c261cd826cec02",
    "semantic_title": "a two-stage minimum cost multicut approach to self-supervised multiple person tracking",
    "citation_count": 6,
    "authors": [
      "Kalun Ho",
      "Amirhossein Kardoost",
      "Franz-Josef Pfreundt",
      "Janis Keuper",
      "Margret Keuper"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Hao_Horizontal_Flipping_Assisted_Disentangled_Feature_Learning_for_Semi-Supervised_Person_Re-Identification_ACCV_2020_paper.html": {
    "title": "Horizontal Flipping Assisted Disentangled Feature Learning for Semi-Supervised Person Re-Identification",
    "volume": "main",
    "abstract": "In this paper, we propose to learn a powerful Re-ID model by using less labeled data together with lots of unlabeled data,i.e.semi-supervised Re-ID. Such kind of learning enables Re-ID model to be more generalizable and scalable to real-world scenes. Specifically, we design a two-stream encoder-decoder-based structure with shared modules and parameters. For the encoder module, we take the original person image with its horizontal mirror image as a pair of inputs and encode deep features with identity and structural information properly disentangled. Then different combinations of disentangling features are used to reconstruct images in the decoder module. In addition to the commonly used constraints from identity consistency and image reconstruction consistency for loss function definition, we design a novel loss function of en-forcing consistent transformation constraints on disentangled features. It is free of labels, but can be applied to both supervised and unsupervised learning branches in our model. Extensive results on four Re-ID datasets demonstrate that by reducing 5/6 labeled data, Our method achieves the best performance on Market-1501 and CUHK03, and comparable accuracy on DukeMTMC-reID and MSMT17",
    "checked": true,
    "id": "9ed98816b43cfbc5d37b8beb5f5c361617fbbddd",
    "semantic_title": "horizontal flipping assisted disentangled feature learning for semi-supervised person re-identification",
    "citation_count": 2,
    "authors": [
      "Gehan Hao",
      "Yang Yang",
      "Xue Zhou",
      "Guanan Wang",
      "Zhen Lei"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Nguyen-Ha_Sequential_View_Synthesis_with_Transformer_ACCV_2020_paper.html": {
    "title": "Sequential View Synthesis with Transformer",
    "volume": "main",
    "abstract": "This paper addresses the problem of novel view synthesis by means of neural rendering, where we are interested in predicting the novel view at an arbitrary camera pose based on a given set of input imagesfrom other viewpoints. Using the known query pose and input poses, we create an ordered set of observations that leads to the target view. Thus, the problem of single novel view synthesis is reformulated as a sequential view prediction task. In this paper, the proposed Transformer-based Generative Query Network (T-GQN) extends the neural-rendering methods by adding two new concepts. First, we use multi-view attention learning between context images to obtain multiple implicit scene representations. Second, we introduce a sequential rendering decoder to predict an image sequence, including the target view, based on the learned representations. Finally, we evaluate our model on various challenging datasets and demonstrate that our model not only gives consistent predictions but also doesn't require any retraining for fine-tuning",
    "checked": true,
    "id": "36f655eae9016024ad6a58e2f49888986f4a13de",
    "semantic_title": "sequential view synthesis with transformer",
    "citation_count": 3,
    "authors": [
      "Phong Nguyen-Ha",
      "Lam Huynh",
      "Esa Rahtu",
      "Janne Heikkila"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Felix_Augmentation_Network_for_Generalised_Zero-Shot_Learning_ACCV_2020_paper.html": {
    "title": "Augmentation Network for Generalised Zero-Shot Learning",
    "volume": "main",
    "abstract": "Generalised zero-shot learning (GZSL) is defined by a training process containing a set of visual samples from seen classes and a set of semantic samples from seen and unseen classes, while the testing process consists of the classification of visual samples from the seen and the unseen classes. Current approaches are based on inference processes that rely on the result of a single modality classifier (visual, semantic, or latent joint space) that balances the classification between the seen and unseen classes using gating mechanisms. There are a couple of problems with such approaches: 1) multi-modal classifiers are known to generally be more accurate than single modality classifiers, and 2) gating mechanisms rely on a complex one-class training of an external domain classifier that modulates the seen and unseen classifiers. In this paper, we mitigate these issues by proposing a novel GZSL method -- augmentation network that tackles multi-modal and multi-domain inference for generalised zero-shot learning (AN-GZSL). The multi-modal inference combines visual and semantic classification and automatically balances the seen and unseen classification using temperature calibration, without requiring any gating mechanisms or external domain classifiers. Experiments show that our method produces the new state-of-the-art GZSL results for fine-grained benchmark data sets CUB and FLO and for the large-scale data set ImageNet. We also obtain competitive results for coarse-grained data sets SUN and AWA. We show an ablation study that justifies each stage of the proposed AN-GZSL",
    "checked": true,
    "id": "1e72030048ab7c0266a6e09f31644983cf4df242",
    "semantic_title": "augmentation network for generalised zero-shot learning",
    "citation_count": 1,
    "authors": [
      "Rafael Felix",
      "Michele Sasdelli",
      "Ian Reid",
      "Gustavo Carneiro"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Bain_Condensed_Movies_Story_Based_Retrieval_with_Contextual_Embeddings_ACCV_2020_paper.html": {
    "title": "Condensed Movies: Story Based Retrieval with Contextual Embeddings",
    "volume": "main",
    "abstract": "Our objective in this work is the long range understandingof the narrative structure of movies. Instead of considering the entire movie, we propose to learn from the `key scenes' of the movie, providing a condensed look at the full storyline. To this end, we make the following three contributions: (i) We create the Condensed Movie Dataset (CMD) consisting of the key scenes from over 3K movies: each key scene is accompanied by a high level semantic description of the scene, character face tracks, and metadata about the movie. Our dataset is scalable, obtained automatically from YouTube, and is freely available for anybody to download and use. It is also an order of magnitude larger than existing movie datasets in the number of movies; (ii) We provide a deep network baseline for text-to-video retrieval on our dataset, combining character, speech and visual cues into a single video embedding; and finally (iii) We demonstrate how the addition of context from other video clips improves retrieval performance",
    "checked": true,
    "id": "239c34ab213229725c6428e63b1315f2e8cdcbc8",
    "semantic_title": "condensed movies: story based retrieval with contextual embeddings",
    "citation_count": 71,
    "authors": [
      "Max Bain",
      "Arsha Nagrani",
      "Andrew Brown",
      "Andrew Zisserman"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Sharma_Single-Image_Camera_Response_Function_Using_Prediction_Consistency_and_Gradual_Refinement_ACCV_2020_paper.html": {
    "title": "Single-Image Camera Response Function Using Prediction Consistency and Gradual Refinement",
    "volume": "main",
    "abstract": "A few methods have been proposed to estimate the CRF from a single image, however most of them tend to fail in handling general real images. For instance, EdgeCRF based on patches extracted from colour edges works effectively only when the presence of noise is insignificant, which is not the case for many real images; and, CRFNet, a recent method based on fully supervised deep learning works only for the CRFs that are in the training data, and hence fail to deal with other possible CRFs beyond the training data. To address these problems, we introduce a non-deep-learning method using prediction consistency and gradual refinement. First, we rely more on the patches of the input image that provide more consistent predictions. If the predictions from a patch are more consistent, it means that the patch is likely to be less affected by noise or any inferior colour combinations, and hence, it can be more reliable for CRF estimation. Second, we employ a gradual refinement scheme in which we start from a simple CRF model to generate a result which is more robust to noise but less accurate, and then we gradually increase the model's complexity to improve the estimation. This is because a simple model, while being less accurate, overfits less to noise than a complex model does. Our experiments confirm that our method outperforms the existing single-image methods for both daytime and nighttime real images",
    "checked": true,
    "id": "5447a815045f145785c71ce6e285b37a460e3cb9",
    "semantic_title": "single-image camera response function using prediction consistency and gradual refinement",
    "citation_count": 5,
    "authors": [
      "Aashish Sharma",
      "Robby T. Tan",
      "Loong-Fah Cheong"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Dunnhofer_Tracking-by-Trackers_with_a_Distilled_and_Reinforced_Model_ACCV_2020_paper.html": {
    "title": "Tracking-by-Trackers with a Distilled and Reinforced Model",
    "volume": "main",
    "abstract": "Visual object tracking was generally tackled by reasoning independently on fast processing algorithms, accurate online adaptation methods, and fusion of trackers. In this paper, we unify such goals by proposing a novel tracking methodology that takes advantage of other visual trackers, offline and online. A compact student model is trained via the marriage of knowledge distillation and reinforcement learning. The first allows to transfer and compress tracking knowledge of other trackers. The second enables the learning of evaluation measures which are then exploited online. After learning, the student can be ultimately used to build (i) a very fast single-shot tracker, (ii) a tracker with a simple and effective online adaptation mechanism, (iii) a tracker that performs fusion of other trackers. Extensive validation shows that the proposed algorithms compete with real-time state-of-the-art trackers",
    "checked": true,
    "id": "e86bf5f5ee98db3cc3e40d1448b5835470d172bc",
    "semantic_title": "tracking-by-trackers with a distilled and reinforced model",
    "citation_count": 21,
    "authors": [
      "Matteo Dunnhofer",
      "Niki Martinel",
      "Christian Micheloni"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Chang_Attention-Aware_Feature_Aggregation_for_Real-time_Stereo_Matching_on_Edge_Devices_ACCV_2020_paper.html": {
    "title": "Attention-Aware Feature Aggregation for Real-time Stereo Matching on Edge Devices",
    "volume": "main",
    "abstract": "Recent works have demonstrated superior results for depth estimation from a stereo pair of images using convolutional neural networks. However, these methods require large amounts of computational resources and are not suited to real-time applications on edge devices. In this work, we propose a novel method for real-time stereo matching on edge devices, which consists of an efficient backbone for feature extraction, an attention-aware feature aggregation, and a cascaded 3D CNN architecture for multi-scale disparity estimation. The efficient backbone is designed to generate multi-scale feature maps with constrained computational power. The multi-scale feature maps are further adaptively aggregated via the proposed attention-aware feature aggregation module to improve representational capacity of features. Multi-scale cost volumes are constructed using aggregated feature maps and regularized using a cascaded 3D CNN architecture to estimate disparity maps in anytime settings. The network infers a disparity map at low resolution and then progressively refines the disparity maps at higher resolutions by calculating the disparity residuals. Because of the efficient extraction and aggregation of informative features, the proposed method can achieve accurate depth estimation in real-time inference. Experimental results demonstrated that the proposed method processed stereo image pairs with resolution 1242x375 at 12-33 fps on an NVIDIA Jetson TX2 module and achieved competitive accuracy in depth estimation. The code is available at https://github.com/JiaRenChang/RealtimeStereo",
    "checked": true,
    "id": "b34886f677afe1689f751130849c715f18665f4c",
    "semantic_title": "attention-aware feature aggregation for real-time stereo matching on edge devices",
    "citation_count": 19,
    "authors": [
      "Jia-Ren Chang",
      "Pei-Chun Chang",
      "Yong-Sheng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Marcos_Contextual_Semantic_Interpretability_ACCV_2020_paper.html": {
    "title": "Contextual Semantic Interpretability",
    "volume": "main",
    "abstract": "Convolutional neural networks (CNN) are known to learn an image representation that captures concepts relevant to the task, but do so in an implicit way that hampers model interpretability.However, one could argue that such a representation is hidden in the neurons and can be made explicit by teaching the model to recognize semantically interpretable attributes that are present in the scene. We call such an intermediate layer a semantic bottleneck.Once the attributes are learned, they can be re-combined to reach the final decision and provide both an accurate prediction and an explicit reasoning behind the CNN decision.In this paper, we look into semantic bottlenecks that capture context: we want attributes to be in groups of a few meaningful elements and participate jointly to the final decision.We use a two-layer semantic bottleneck that gathers attributes into interpretable, sparse groups, allowing them contribute differently to the final output depending on the context.We test our contextual semantic interpretable bottleneck (CSIB) on the task of landscape scenicness estimation and train the semantic interpretable bottleneck using an auxiliary database (SUN Attributes).Our model yields in predictions as accurate as a non-interpretable baseline when applied to a real-world test set of Flickr images, all while providing clear and interpretable explanations for each prediction",
    "checked": true,
    "id": "0eb157a5474ef4640ebba037b60ad88fdc317ff7",
    "semantic_title": "contextual semantic interpretability",
    "citation_count": 24,
    "authors": [
      "Diego Marcos",
      "Ruth Fong",
      "Sylvain Lobry",
      "Remi Flamary",
      "Nicolas Courty",
      "Devis Tuia"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Kim_Low-level_Sensor_Fusion_Network_for_3D_Vehicle_Detection_using_Radar_ACCV_2020_paper.html": {
    "title": "Low-level Sensor Fusion Network for 3D Vehicle Detection using Radar Range-Azimuth Heatmap and Monocular Image",
    "volume": "main",
    "abstract": "Robust and accurate object detection on roads with various objects is essential for automated driving. The radar has been employed in commercial advanced driver assistance systems (ADAS) for a decade due to its low-cost and high-reliability advantages. However, the radar has been used only in limited driving conditions such as highways to detect a few forwarding vehicles because of the limited performance of radar due to low resolution or poor classification. We propose a learning-based detection network using radar range-azimuth heatmap and monocular image in order to fully exploit the radar in complex road environments. We show that radar-image fusion can overcome the inherent weakness of the radar by leveraging camera information. Our proposed network has a two-stage architecture that combines radar and image feature representations rather than fusing each sensor's prediction results to improve detection performance over a single sensor. To demonstrate the effectiveness of the proposed method, we collected radar, camera, and LiDAR data in various driving environments in terms of vehicle speed, lighting conditions, and traffic volume. Experimental results show that the proposed fusion method outperforms the radar-only and the image-only method",
    "checked": true,
    "id": "39c87f3c3e43759906ed5816d0f977434a41ddca",
    "semantic_title": "low-level sensor fusion network for 3d vehicle detection using radar range-azimuth heatmap and monocular image",
    "citation_count": 16,
    "authors": [
      "Jinhyeong Kim",
      "Youngseok Kim",
      "Dongsuk Kum"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Suda_Deep_Snapshot_HDR_Imaging_Using_Multi-Exposure_Color_Filter_Array_ACCV_2020_paper.html": {
    "title": "Deep Snapshot HDR Imaging Using Multi-Exposure Color Filter Array",
    "volume": "main",
    "abstract": "In this paper, we propose a deep snapshot high dynamic range (HDR) imaging framework that can effectively reconstruct an HDR image from the RAW data captured using a multi-exposure color filter array (ME-CFA), which consists of a mosaic pattern of RGB filters with different exposure levels. To effectively learn the HDR image reconstruction network, we introduce the idea of luminance normalization that simultaneously enables effective loss computation and input data normalization by considering relative local contrasts in the \"normalized-by-luminance\" HDR domain. This idea makes it possible to equally handle the errors in both bright and dark areas regardless of absolute luminance levels, which significantly improves the visual image quality in a tone-mapped domain. Experimental results using two public HDR image datasets demonstrate that our framework outperforms other snapshot methods and produces high-quality HDR images with fewer visual artifacts",
    "checked": true,
    "id": "8409791c400fab3cdde8613b0657c82867e01b7f",
    "semantic_title": "deep snapshot hdr imaging using multi-exposure color filter array",
    "citation_count": 8,
    "authors": [
      "Takeru Suda",
      "Masayuki Tanaka",
      "Yusuke Monno",
      "Masatoshi Okutomi"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Marcu_Semantics_through_Time_Semi-supervised_Segmentation_of_Aerial_Videos_with_Iterative_ACCV_2020_paper.html": {
    "title": "Semantics through Time: Semi-supervised Segmentation of Aerial Videos with Iterative Label Propagation",
    "volume": "main",
    "abstract": "Semantic segmentation is a crucial task for robot navigation and safety. However, current supervised methods require a large amount of pixelwise annotations to yield accurate results. Labeling is a tedious and time consuming process that has hampered progress in low altitude UAV applications. This paper makes an important step towards automatic annotation by introducing SegProp, a novel iterative flow-based method, with a direct connection to spectral clustering in space and time, to propagate the semantic labels to frames that lack human annotations. The labels are further used in semi-supervised learning scenarios. Motivated by the lack of a large video aerial dataset, we also introduce Ruralscapes, a new dataset with high resolution (4K) images and manually annotated dense labels every 50 frames - the largest of its kind, to the best of our knowledge. Our novel SegProp automatically annotates the remaining unlabeled 98% of frames with an accuracy exceeding 90% (F-measure), significantly outperforming other state-of-the-art label propagation methods. Moreover, when integrating other methods as modules inside SegProp's iterative label propagation loop, we achieve a significant boost over the baseline labels. Finally, we test SegProp in a full semi-supervised setting: we train several state-of-the-art deep neural networks on the SegProp-automatically-labeled training frames and test them on completely novel videos. We convincingly demonstrate, every time, a significant improvement over the supervised scenario",
    "checked": true,
    "id": "58391df9948d3d6ea603e1de0601b3457a04af31",
    "semantic_title": "semantics through time: semi-supervised segmentation of aerial videos with iterative label propagation",
    "citation_count": 11,
    "authors": [
      "Alina Marcu",
      "Vlad Licaret",
      "Dragos Costea",
      "Marius Leordeanu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Zhu_Imbalance_Robust_Softmax_for_Deep_Embedding_Learning_ACCV_2020_paper.html": {
    "title": "Imbalance Robust Softmax for Deep Embedding Learning",
    "volume": "main",
    "abstract": "Deep embedding learning is expected to learn a metric space in which features have smaller maximal intra-class distance than minimal inter-class distance.In recent years, one research focus is to solve the open-set problem by discriminative deep embedding learning in the field of face recognition (FR) and person re-identification (re-ID). Apart from open-set problem, we find that imbalanced training data is another main factor causing the performance degradation of FR and re-ID, and data imbalance widely exists in the real applications. However, very little research explores why and how data imbalance influences the performance of FR and re-ID. In this work, we deeply investigate data imbalance in the perspective of neural network optimisation and feature distribution. We find one main reason of performance degradation caused by data imbalance is that the weights (from the penultimate fully-connected layer) are far from their class centers in feature space. Based on this investigation, we propose a unified framework, Imbalance-Robust Softmax (IR-Softmax), which can simultaneously solve the open-set problem and reduce the influence of data imbalance. IR-Softmax can generalise to any softmax and its variants (which are discriminative for open-set problem) by directly setting the weights as their class centers, naturally solving the data imbalance problem. In this work, we explicitly re-formulate two discriminative softmax (A-Softmax [??] and AM-Softmax [??]) under the framework of IR-Softmax. We conduct extensive experiments on FR databases (LFW, MegaFace) and re-ID database (Market-1501, Duke), and IR-Softmax outperforms many state-of-the-art methods",
    "checked": true,
    "id": "03b93b3ec98d53352ca38137f196af9c926eb12b",
    "semantic_title": "imbalance robust softmax for deep embeeding learning",
    "citation_count": 1,
    "authors": [
      "Hao Zhu",
      "Yang Yuan",
      "Guosheng Hu",
      "Xiang Wu",
      "Neil Robertson"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Lumentut_Human_Motion_Deblurring_using_Localized_Body_Prior_ACCV_2020_paper.html": {
    "title": "Human Motion Deblurring using Localized Body Prior",
    "volume": "main",
    "abstract": "In recent decades, the skinned multi-person linear model (SMPL) is widely exploited in the image-based 3D body reconstruction. This model, however, depends fully on the quality of the input image. Degraded image case, such as the motion-blurred issue, downgrades the quality of the reconstructed 3D body. This issue becomes severe as recent motion deblurring methods mainly focused on solving the camera motion case while ignoring the blur caused by human-articulated motion. In this work, we construct a localized adversarial framework that solves both human-articulated and camera motion blurs. To achieve this, we utilize the result of the restored image in a 3D body reconstruction module and produces a localized map. The map is employed to guide the adversarial modules on learning both the human body and scene regions. Nevertheless, training these modules straight-away is impractical since the recent blurry dataset is not supported by the 3D body predictor module. To settle this issue, we generate a novel dataset that simulates realistic blurry human motion while maintaining the presence of camera motion. By engaging this dataset and the proposed framework, we show that our deblurring results are superior among the state-of-the-art algorithms in both quantitative and qualitative performances",
    "checked": true,
    "id": "bbd587de3b0aef2fccdeb1f5d37630e4f25d6545",
    "semantic_title": "human motion deblurring using localized body prior",
    "citation_count": 5,
    "authors": [
      "Jonathan Samuel Lumentut",
      "Joshua Santoso",
      "In Kyu Park"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Yin_FAN_Feature_Adaptation_Network_for_Surveillance_Face_Recognition_and_Normalization_ACCV_2020_paper.html": {
    "title": "FAN: Feature Adaptation Network for Surveillance Face Recognition and Normalization",
    "volume": "main",
    "abstract": "This paper studies face recognition (FR) and normalization in surveillance imagery. Surveillance FR is a challenging problem that has great values in law enforcement. Despite recent progress in conventional FR, less effort has been devoted to surveillance FR. To bridge this gap, we propose a Feature Adaptation Network (FAN) to jointly perform surveillance FR and normalization. Our face normalization mainly acts on the aspect of image resolution, closely related to face super-resolution. However, previous face super-resolution methods require paired training data with pixel-to-pixel correspondence, which is typically unavailable between real-world low-resolution and high-resolution faces. FAN can leverage both paired and unpaired data as we disentangle the features into identity and non-identity components and adapt the distribution of the identity features, which breaks the limit of current face super-resolution methods. We further propose a random scale augmentation scheme to learn resolution robust identity features, with advantages over previous fixed scale augmentation. Extensive experiments on LFW, WIDER FACE, QUML-SurvFace and SCface datasets have shown the effectiveness of our method on surveillance FR and normalization",
    "checked": true,
    "id": "28da38c1ac869a40a107f4be3a4492191fb7a2af",
    "semantic_title": "fan: feature adaptation network for surveillance face recognition and normalization",
    "citation_count": 37,
    "authors": [
      "Xi Yin",
      "Ying Tai",
      "Yuge Huang",
      "Xiaoming Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Jeon_Compensating_for_the_Lack_of_Extra_Training_Data_by_Learning_ACCV_2020_paper.html": {
    "title": "Compensating for the Lack of Extra Training Data by Learning Extra Representation",
    "volume": "main",
    "abstract": "Outperforming the previous state of the art, numerous deep learning models have been proposed for image classification using the ImageNet database. In most cases, significant improvement has been made through novel data augmentation techniques and learning or hyperparameter tuning strategies, leading to the advent of approaches such as FixNet, NoisyStudent, and Big Transfer. However, the latter examples, while achieving the state-of-the-art performance on ImageNet, required a significant amount of extra training data, namely the JFT-300M dataset. This dataset contains 300 million images, which is 250 times larger in sample numbers than ImageNet, but is publicly unavailable; the model pre-trained on it is available instead. In this paper, we introduce a novel framework, Extra Representation (ExRep), to surmount the problem of not having access to the JFT-300M data by instead using ImageNet and the publicly available model that has been pre-trained on JFT-300M. We take a knowledge distillation approach, treating the model pre-trained on JFT-300M as well as ImageNet as the teacher network and that pre-trained only on ImageNet as the student network. Our proposed method is capable of learning additional representation effects of the teacher model, bolstering the performance of the student model to a similar level to that of the teacher model, achieving high classification performance even without extra training data",
    "checked": true,
    "id": "45c727bd36858144f1fc0da15acfc80f1ae9dbc1",
    "semantic_title": "compensating for the lack of extra training data by learning extra representation",
    "citation_count": 1,
    "authors": [
      "Hyeonseong Jeon",
      "Siho Han",
      "Sangwon Lee",
      "Simon S. Woo"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Seibold_Self-Guided_Multiple_Instance_Learning_for_Weakly_Supervised_Thoracic_DiseaseClassification_and_ACCV_2020_paper.html": {
    "title": "Self-Guided Multiple Instance Learning for Weakly Supervised Thoracic DiseaseClassification and Localizationin Chest Radiographs",
    "volume": "main",
    "abstract": "Due to the high complexity of medical images and the scarcity of trained personnel, most large-scale radiological datasets are lacking fine-grained annotations and are often only described on image-level. These shortcomings hinder the deployment of automated diagnosis systems, which require human-interpretable justification for their decision process. In this paper, we address the problem of weakly supervised identification and localization of abnormalities in chest radiographs in a multiple-instance learning setting. To that end, we introduce a novel loss function for training convolutional neural networks increasing the localization confidence and assisting the overall disease identification. The loss leverages both image- and patch-level predictions to generate auxiliary supervision and enables specific training at patch-level. Rather than forming strictly binary from the predictions as done in previous loss formulations, we create targets in a more customized manner.This way, the loss accounts for possible misclassification of less certain instances. We show that the supervision provided within the proposed learning scheme leads to better performance and more precise predictions on prevalent datasets for multiple-instance learning as well as on the NIH ChestX-Ray14 benchmark for disease recognition than previously used losses",
    "checked": true,
    "id": "0f34efe7aa4eb2bf733860b715c10aace172e306",
    "semantic_title": "self-guided multiple instance learning for weakly supervised thoracic diseaseclassification and localizationin chest radiographs",
    "citation_count": 5,
    "authors": [
      "Constantin Seibold",
      "Jens Kleesiek",
      "Heinz-Peter Schlemmer",
      "Rainer Stiefelhagen"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Ki_In-sample_Contrastive_Learning_and_Consistent_Attention_for_Weakly_Supervised_Object_ACCV_2020_paper.html": {
    "title": "In-sample Contrastive Learning and Consistent Attention for Weakly Supervised Object Localization",
    "volume": "main",
    "abstract": "Weakly supervised object localization (WSOL) aims to localize the target object using only the image-level supervision. Recent methods encourage the model to activate feature maps over the entire object by dropping the most discriminative parts. However, they are likely to induce excessive extension to the backgrounds which leads to over-estimated localization. In this paper, we consider the background as an important cue that guides the feature activation to cover the sophisticated object region and propose contrastive attention loss. The loss promotes similarity between foreground and its dropped version, and, dissimilarity between the dropped version and background. Furthermore, we propose foreground consistency loss that penalizes earlier layers producing noisy attention regarding the later layer as a reference to provide them with a sense of backgroundness. It guides the early layers to activate on objects rather than locally distinctive backgrounds so that their attentions to be similar to the later layer. For better optimizing the above losses, we use the non-local attention blocks to replace channel-pooled attention leading to enhanced attention maps considering the spatial similarity. Last but not least, we propose to drop background regions in addition to the most discriminative region. Our method achieves state-of-the-art performance on CUB-200-2011 and ImageNet benchmark datasets regarding top-1 localization accuracy and MaxBoxAccV2, and we provide detailed analysis on our individual components. The code will be publicly available online for reproducibility",
    "checked": true,
    "id": "f31a4497e730560fabfb5bc17f97035e99ac274b",
    "semantic_title": "in-sample contrastive learning and consistent attention for weakly supervised object localization",
    "citation_count": 27,
    "authors": [
      "Minsong Ki",
      "Youngjung Uh",
      "Wonyoung Lee",
      "Hyeran Byun"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Courdier_Real-Time_Segmentation_Networks_should_be_Latency_Aware_ACCV_2020_paper.html": {
    "title": "Real-Time Segmentation Networks should be Latency Aware",
    "volume": "main",
    "abstract": "As scene segmentation systems reach visually accurate results, many recent papers focus on making these network architectures faster, smaller and more efficient. In particular, studies often aim at designing `real-time' systems. Achieving this goal is particularly relevant in the context of real-time video understanding for autonomous vehicles, and robots.In this paper, we argue that the commonly used performance metric of mean Intersection over Union (mIoU) does not fully capture the information required to estimate the true performance of these networks when they operate in `real-time'.We propose a change of objective in the segmentation task, and its associated metric that encapsulates this missing information in the following way: We propose to predict the future output segmentation map that will match the future input frame at the time when the network finishes the processing.We introduce the associated latency-aware metric, from which we can determine a ranking.We perform latency timing experiments of some recent networks on different hardware and assess the performances of these networks on our proposed task. We propose improvements to scene segmentation networks to better perform on our task by using multi-frames input and increasing capacity in the initial convolutional layers",
    "checked": true,
    "id": "84121ad4eb01c4fc45231350200340e92962cf1a",
    "semantic_title": "real-time segmentation networks should be latency aware",
    "citation_count": 5,
    "authors": [
      "Evann Courdier",
      "Francois Fleuret"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Sarhan_Utilizing_Transfer_Learning_and_a_Customized_Loss_Function_for_Optic_ACCV_2020_paper.html": {
    "title": "Utilizing Transfer Learning and a Customized Loss Function for Optic Disc Segmentation from Retinal Images",
    "volume": "main",
    "abstract": "Accurate segmentation of the optic disc from a retinal image is vital to extracting retinal features that may be highly correlated with retinal conditions such as glaucoma. In this paper, we propose a deep-learning based approach capable of segmenting the optic disc given a high-precision retinal fundus image. Our approach utilizes a UNET-based model with a VGG16 encoder trained on the ImageNet dataset. This study can be distinguished from other studies in the customization made for the VGG16 model, the diversity of the datasets adopted, the duration of disc segmentation, the loss function utilized, and the number of parameters required to train our model. Our approach was tested on seven publicly available datasets augmented by a dataset from a private clinic that was annotated by two Doctors of Optometry through a web portal built for this purpose. We achieved an accuracy of 99.78% and a Dice coefficient of 94.73% for a disc segmentation from a retinal image in 0.03 seconds. The results obtained from comprehensive experiments demonstrate the robustness of our approach to disc segmentation of retinal images obtained from different sources",
    "checked": true,
    "id": "5012a4529d0909fc9221ad7180f7f56412e8d259",
    "semantic_title": "utilizing transfer learning and a customized loss function for optic disc segmentation from retinal images",
    "citation_count": 5,
    "authors": [
      "Abdullah Sarhan",
      "Ali Al-Khaz'Aly",
      "Adam Gorner",
      "Andrew Swift",
      "Jon Rokne",
      "Reda Alhajj",
      "Andrew Crichton"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Sun_3D_Guided_Weakly_Supervised_Semantic_Segmentation_ACCV_2020_paper.html": {
    "title": "3D Guided Weakly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "Pixel-wise clean annotation is necessary for fully-supervised semantic segmentation, which is laborious and expensive to obtain. In this paper, we propose a weakly supervised 2D semantic segmentation model by incorporating sparse bounding box labels with available 3D information, which is much easier to obtain with advanced sensors. We manually labeled a subset of the 2D-3D Semantics(2D-3D-S) dataset with bounding boxes, and introduce our 2D-3D inference module to generate accurate pixel-wise segment proposal masks. Guided by 3D information, we first generate a point cloud of objects and calculate objectness probability score for each point. Then we project the point cloud with objectness probabilities back to 2D images followed by a refinement step to obtain segment proposals, which are treated as pseudo labels to train a semantic segmentation network. Our method works in a recursive manner to gradually refine the above-mentioned segment proposals. Extensive experimental results on the 2D-3D-S dataset show that the proposed method can generate accurate segment proposals when bounding box labels are available on only a small subset of training images. Performance comparison with recent state-of-the-art methods further illustrates the effectiveness of our method",
    "checked": true,
    "id": "8e665753d31745e7b839c9a7ab1abad20ff02f2c",
    "semantic_title": "3d guided weakly supervised semantic segmentation",
    "citation_count": 13,
    "authors": [
      "Weixuan Sun",
      "Jing Zhang",
      "Nick Barnes"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Sun_Second_Order_enhanced_Multi-glimpse_Attention_in_Visual_Question_Answering_ACCV_2020_paper.html": {
    "title": "Second Order enhanced Multi-glimpse Attention in Visual Question Answering",
    "volume": "main",
    "abstract": "Visual Question Answering (VQA) is formulated as predicting the answer given an image and question pair. A successful VQA model relies on the information from both visual and textual modalities. Previous endeavours of VQA are made on the good attention mechanism, and multi-modal fusion strategies. For example, most models, till date, are proposed to fuse the multi-modal features based on implicit neural network through cross-modal interactions. To better explore and exploit the information of different modalities, the idea of second order interactions of different modalities, which is prevalent in recommendation system, is re-purposed to VQA in efficiently and explicitly modeling the second order interaction on both the visual and textual features, learned in a shared embedding space. To implement this idea, we propose a novel Second Order enhanced Multi-glimpse Attention model (SOMA) where each glimpse denotes an attention map. SOMA adopts multi-glimpse attention to focus on different contents in the image. With projected the multi-glimpse outputs and question feature into a shared embedding space, an explicit second order feature is constructed to model the interaction on both the intra-modality and cross-modality of features. Furthermore, we advocate a semantic deformation method as data augmentation to generate more training examples in Visual Question Answering. Experimental results on VQA v2.0 and VQA-CP v2.0 have demonstrated the effectiveness of our method. Extensive ablation studies are studied to evaluate the components of the proposed model",
    "checked": true,
    "id": "5c8a627ac93b822e2e625c4a5bf02c4abc490d75",
    "semantic_title": "second order enhanced multi-glimpse attention in visual question answering",
    "citation_count": 1,
    "authors": [
      "Qiang Sun",
      "Binghui Xie",
      "Yanwei Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Ko_Deep_Priors_inside_an_Unrolled_and_Adaptive_Deconvolution_Model_ACCV_2020_paper.html": {
    "title": "Deep Priors inside an Unrolled and Adaptive Deconvolution Model",
    "volume": "main",
    "abstract": "Image deconvolution is an essential but ill-posed problem even if the degradation kernel is known. Recently, learning based methods have demonstrated superior image restoration quality in comparison to traditional methods which are typically based on empirical statistics and parameter adjustment. Though coming up with outstanding performance, most of the plug-and-play priors are trained in a specific degradation model, leading to inferior performance on restoring high-frequency components. To address this problem, a deblurring architecture that adopts (1) adaptive deconvolution modules and (2) learning based image prior solvers is proposed. The adaptive deconvolution module adjusts the regularization weight locally to well process both smooth and non-smooth regions. Moreover, a cascade made of image priors is learned from the mapping between intermediates thus robust to arbitrary noise, aliasing, and artifact. According to our analysis, the proposed architecture can achieve a significant improvement on the convergence rate and result in an even better restoration performance",
    "checked": true,
    "id": "3120c6ab48de3cd933df95a1e1b40078bc6b47ef",
    "semantic_title": "deep priors inside an unrolled and adaptive deconvolution model",
    "citation_count": 3,
    "authors": [
      "Hung-Chih Ko",
      "Je-Yuan Chang",
      "Jian-Jiun Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Lakhal_Novel-View_Human_Action_Synthesis_ACCV_2020_paper.html": {
    "title": "Novel-View Human Action Synthesis",
    "volume": "main",
    "abstract": "Novel-View Human Action Synthesis aims to synthesize the movement of a body from a virtual viewpoint, given a video from a real viewpoint.We present a novel 3D reasoning to synthesize the target viewpoint. We first estimate the 3D mesh of the target body and transfer the rough textures from the 2D images to the mesh.As this transfer may generate sparse textures on the mesh due to frame resolution or occlusions.We produce a semi-dense textured mesh by propagating the transferred textures both locally, within local geodesic neighborhoods, and globally, across symmetric semantic parts.Next, we introduce a context-based generator to learn how to correct and complete the residual appearance information.This allows the network to independently focus on learning the foreground and background synthesis tasks.We validate the proposed solution on the public NTU RGB+D dataset. The code and resources are available at https://bit.ly/36u3h4K",
    "checked": true,
    "id": "89e1229e47b02e013b37e08ccf50af9115aa4182",
    "semantic_title": "novel-view human action synthesis",
    "citation_count": 3,
    "authors": [
      "Mohamed Ilyes Lakhal",
      "Davide Boscaini",
      "Fabio Poiesi",
      "Oswald Lanz",
      "Andrea Cavallaro"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Yan_RAF-AU_Database_In-the-Wild_Facial_Expressions_with_Subjective_Emotion_Judgement_and_ACCV_2020_paper.html": {
    "title": "RAF-AU Database: In-the-Wild Facial Expressions with Subjective Emotion Judgement and Objective AU Annotations",
    "volume": "main",
    "abstract": "Much of the work on automatic facial expression recognition relies on databases containing a certain number of emotion classes and their exaggerated facial configurations (generally six prototypical facial expressions), based on Ekman's Basic Emotion Theory. However, recent studies have revealed that facial expressions in our human life can be blended with multiple basic emotions. And the emotion labels for these in-the-wild facial expressions cannot easily be annotated solely on pre-defined AU patterns. How to analyze the action units for such complex expressions is still an open question. To address this issue, we develop a RAF-AU database that employs a sign-based (i.e., AUs) and judgement-based (i.e., perceived emotion) approach to annotating blended facial expressions in the wild. We first reviewed the annotation methods in existing databases and identified crowdsourcing as a promising strategy for labeling in-the-wild facial expressions. Then, RAF-AU was finely annotated by experienced coders, on which we also conducted a preliminary investigation of which key AUs contribute most to a perceived emotion, and the relationship between AUs and facial expressions. Finally, we provided a baseline for AU recognition in RAF-AU using popular features and multi-label learning methods",
    "checked": true,
    "id": "fc8b824f611e080e4cf88b5bc3abdacd01db1885",
    "semantic_title": "raf-au database: in-the-wild facial expressions with subjective emotion judgement and objective au annotations",
    "citation_count": 17,
    "authors": [
      "Wen-Jing Yan",
      "Shan Li",
      "Chengtao Que",
      "Jiquan Pei",
      "Weihong Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Patra_Learn_more_forget_less_Cues_from_human_brain_ACCV_2020_paper.html": {
    "title": "Learn more, forget less: Cues from human brain",
    "volume": "main",
    "abstract": "Humans learn new information incrementally while consolidating old information at every stage in a lifelong learning process. While this appears perfectly natural for humans, the same task has proven to be challenging for learning machines. Deep neural networks are still prone to catastrophic forgetting of previously learnt information when presented with information from a sufficiently new distribution. To address this problem, we present NeoNet, a simple yet effective method that is motivated by recent findings in computational neuroscience on the process of long-term memory consolidation in humans. The network relies on a pseudorehearsal strategy to model the working of relevant sections of the brain that are associated with long-term memory consolidation processes. Experiments on benchmark classification tasks achieve state-of-the-art results that demonstrate the potential of the proposed method, with improvements in additions of novel information attained without requiring to store exemplars of past classes",
    "checked": true,
    "id": "dc4b88ad3113a731201818e2309d5425ea1e83ee",
    "semantic_title": "learn more, forget less: cues from human brain",
    "citation_count": 0,
    "authors": [
      "Arijit Patra",
      "Tapabrata Chakraborti"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Yang_Unsupervised_Domain_Adaptive_Object_Detection_using_Forward-Backward_Cyclic_Adaptation_ACCV_2020_paper.html": {
    "title": "Unsupervised Domain Adaptive Object Detection using Forward-Backward Cyclic Adaptation",
    "volume": "main",
    "abstract": "We present a novel approach to perform the unsupervised domain adaptation for object detection through forward-backward cyclic (FBC) training. Recent adversarial training based domain adaptation methods have shown their effectiveness on minimizing domain discrepancy via marginal feature distributions alignment. However, aligning the marginal feature distributions does not guarantee the alignment of class conditional distributions. This limitation is more evident when adapting object detectors as the domain discrepancy is larger compared to the image classification task, e.g. various number of objects exist in one image and the majority of content in an image is background. This motivates us to learn domain-invariance for category-level semantics via gradient alignment for instance-level adaptation. Intuitively, if the gradients of two domains point in similar directions, then the learning of one domain can improve that of another domain. To achieve gradient alignment, we propose Forward- Backward Cyclic Adaptation, which iteratively computes adaptation from source to target via backward hopping and from target to source via forward passing. In addition, we align low-level features for adapting image-level color/texture via adversarial training. However, the detector performs well on both domains is not ideal for the target domain. As such, in each cycle, domain diversity is enforced by maximum entropy regularization on the source domain to penalize confident source-specific learning and minimum entropy regularization on target domain to intrigue target-specific learning. Theoretical analysis of the training process is provided, and extensive experiments on challenging cross-domain object detection datasets have shown the superiority of our approach over the state-of-the-art",
    "checked": true,
    "id": "5cd4fea42f4b6e65b9dd9944c074c154eb297198",
    "semantic_title": "unsupervised domain adaptive object detection using forward-backward cyclic adaptation",
    "citation_count": 17,
    "authors": [
      "Siqi Yang",
      "Lin Wu",
      "Arnold Wiliem",
      "Brian C. Lovell"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Hwang_Exploiting_Transferable_Knowledge_for_Fairness-aware_Image_Classification_ACCV_2020_paper.html": {
    "title": "Exploiting Transferable Knowledge for Fairness-aware Image Classification",
    "volume": "main",
    "abstract": "Recent studies have revealed the importance of fairness in machine learning and computer vision systems, in accordance with the concerns about the unintended social discrimination produced by the systems. In this work, we aim to tackle the fairness-aware image classification problem, whose goal is to classify a target attribute (e.g., attractiveness) in a fair manner regarding protected attributes (e.g., gender, age, race). To this end, existing methods mainly rely on protected attribute labels for training, which are costly and sometimes unavailable for real-world scenarios. To alleviate the restriction and enlarge the scalability of fair models, we introduce a new framework where a fair classification model can be trained on datasets without protected attribute labels (i.e., target datasets) by exploiting knowledge from pre-built benchmarks (i.e., source datasets). Specifically, when training a target attribute encoder, we encourage its representations to be independent of the features from the pre-trained encoder on a source dataset. Moreover, we design a Group-wise Fair loss to minimize the gap in error rates between different protected attribute groups. To the best of our knowledge, this work is the first attempt to train the fairness-aware image classification model on a target dataset without protected attribute annotations. To verify the effectiveness of our approach, we conduct experiments on CelebA and UTK datasets with two settings: the conventional and the transfer settings. In the both settings, our model shows the fairest results when compared to the existing methods",
    "checked": true,
    "id": "4cf2c02f21d9269cbd5925f9a98514ee5f0e2fb0",
    "semantic_title": "exploiting transferable knowledge for fairness-aware image classification",
    "citation_count": 10,
    "authors": [
      "Sunhee Hwang",
      "Sungho Park",
      "Pilhyeon Lee",
      "Seogkyu Jeon",
      "Dohyung Kim",
      "Hyeran Byun"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Kim_Restoring_Spatially-Heterogeneous_Distortions_using_Mixture_of_Experts_Network_ACCV_2020_paper.html": {
    "title": "Restoring Spatially-Heterogeneous Distortions using Mixture of Experts Network",
    "volume": "main",
    "abstract": "In recent years, deep learning-based methods have been successfully applied to the image distortion restoration tasks.However, scenarios that assume a single distortion only may not be suitable for many real-world applications.To deal with such cases, some studies have proposed sequentially combined distortions datasets.Viewing in a different point of combining, we introduce a spatially-heterogeneous distortion dataset in which multiple corruptions are applied to the different locations of each image.In addition, we also propose a mixture of experts network to effectively restore a multi-distortion image.Motivated by the multi-task learning, we design our network to have multiple paths that learn both common and distortion-specific representations.Our model is effective for restoring real-world distortions and we experimentally verify that our method outperforms other models designed to manage both single distortion and multiple distortions",
    "checked": true,
    "id": "3e1a423224681100606a2d7e34ffd58c64c5d64a",
    "semantic_title": "restoring spatially-heterogeneous distortions using mixture of experts network",
    "citation_count": 4,
    "authors": [
      "Sijin Kim",
      "Namhyuk Ahn",
      "Kyung-Ah Sohn"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Ito_Point_Proposal_based_Instance_Segmentation_with_Rectangular_Masks_for_Robot_ACCV_2020_paper.html": {
    "title": "Point Proposal based Instance Segmentation with Rectangular Masks for Robot Picking Task",
    "volume": "main",
    "abstract": "In this paper, we focus on instance segmentation of a top-view image for robot picking task. One difficulty in this setting is that objects are located in various orientations and highly overlapped, where a traditional box proposal approach such as Mask R-CNN does not work well because more than one objects often have very similar bounding-boxes. To address this issue, we adopt a recently developed point proposal approach. This approach firstly generates point proposals instead of box proposals, then an instance mask is predicted over an image for each proposal point. This procedure enables us to obtain pixel-precise masks even for objects sharing the same bounding-box. However, mask prediction over an image may produce a few false positive pixels apart from objects and these false positives are problematic for robot picking task. To suppress them, we introduce rectangular masks. A rectangular mask for each proposal point restricts the existence area of the corresponding object within the rectangle. The experimental result on WISDOM dataset shows that our method achieves superior performance to Mask R-CNN with the same backbone model and introduction of rectangular masks gives small improvement of mask AP and large improvement of box AP",
    "checked": true,
    "id": "0403bb13752c03536ca3c2f7aa43262cd7b98198",
    "semantic_title": "point proposal based instance segmentation with rectangular masks for robot picking task",
    "citation_count": 3,
    "authors": [
      "Satoshi Ito",
      "Susumu Kubota"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Hayat_Synthesizing_the_Unseen_for_Zero-shot_Object_Detection_ACCV_2020_paper.html": {
    "title": "Synthesizing the Unseen for Zero-shot Object Detection",
    "volume": "main",
    "abstract": "The existing zero-shot detection approaches project visual features to the semantic domain for seen objects, hoping to map unseen objects to their corresponding semantics during inference. However, since the unseen objects are never visualized during training, the detection model is skewed towards seen content, thereby labeling unseen as background or a seen class. In this work, we propose to synthesize visual features for unseen classes, so that the model learns both seen and unseen objects in the visual domain. Consequently, the major challenge becomes, how to accurately synthesize unseen objects merely using their class semantics? Towards this ambitious goal, we propose a novel generative model that uses class-semantics to not only generate the features but also to discriminatively separate them. Further, using a unified model, we ensure the synthesized features have high diversity that represents the intra-class differences and variable localization precision in the detected bounding boxes. We test our approach on three object detection benchmarks, PASCAL VOC, MSCOCO, and ILSVRC detection, under both conventional and generalized settings, showing impressive gains over the state-of-the-art methods",
    "checked": true,
    "id": "9ff328cac394cf89fe138479d971e24b9013b7ab",
    "semantic_title": "synthesizing the unseen for zero-shot object detection",
    "citation_count": 44,
    "authors": [
      "Nasir Hayat",
      "Munawar Hayat",
      "Shafin Rahman",
      "Salman Khan",
      "Syed Waqas Zamir",
      "Fahad Shahbaz Khan"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/He_DeepVoxels_Enhancing_the_Fidelity_of_Novel_View_Synthesis_from_3D_ACCV_2020_paper.html": {
    "title": "DeepVoxels++: Enhancing the Fidelity of Novel View Synthesis from 3D Voxel Embeddings",
    "volume": "main",
    "abstract": "We present a novel view synthesis method based upon latent voxel embeddings of an object, which encode both shape and appearance information and are learned without explicit 3D occupancy supervision. Our method uses an encoder-decoder architecture to learn such deep volumetric representations from a set of images taken at multiple viewpoints. Compared with DeepVoxels, our DeepVoxels++ applies a series of enhancements: a) a patch-based image feature extraction and neural rendering scheme that learns local shape and texture patterns, and enables neural rendering at high resolution; b) learned view-dependent feature transformation kernels to explicitly model perspective transformations induced by viewpoint changes; c) a recurrent-concurrent aggregation technique to alleviate single-view update bias of the voxel embeddings recurrent learning process. Combined with d) a simple yet effective implementation trick of frustum representation sufficient sampling, we achieve improved visual quality over the prior deep voxel-based methods (33% SSIM error reduction and 22% PSNR improvement) on 360-degree novel-view synthesis benchmarks",
    "checked": true,
    "id": "e90215aab23e85f2ecd7fd2bcf4138762b312b0c",
    "semantic_title": "deepvoxels++: enhancing the fidelity of novel view synthesis from 3d voxel embeddings",
    "citation_count": 9,
    "authors": [
      "Tong He",
      "John Collomosse",
      "Hailin Jin",
      "Stefano Soatto"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Liu_Transforming_Multi-Concept_Attention_into_Video_Summarization_ACCV_2020_paper.html": {
    "title": "Transforming Multi-Concept Attention into Video Summarization",
    "volume": "main",
    "abstract": "Video summarization is among challenging tasks in computer vision, which aims at identifying highlight frames or shots over a lengthy video input. In this paper, we propose an novel attention-based framework for video summarization with complex video data. Unlike previous works which only apply attention mechanism on the correspondence between frames, our multi-concept video self-attention (MC-VSA) model is presented to identify informative regions across temporal and concept video features, which jointly exploit context diversity over time and space for summarization purposes. Together with consistency between video and summary enforced in our framework, our model can be applied to both labeled and unlabeled data, making our method preferable to real-world applications. Extensive and complete experiments on two benchmarks demonstrate the effectiveness of our model both quantitatively and qualitatively, and confirms its superiority over the state-of-the-arts",
    "checked": true,
    "id": "5960012b66f7a0f745c99ffa66d6e9d49a98f4dc",
    "semantic_title": "transforming multi-concept attention into video summarization",
    "citation_count": 16,
    "authors": [
      "Yen-Ting Liu",
      "Yu-Jhe Li",
      "Yu-Chiang Frank Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Sagawa_Dense_Pixel-wise_Micro-motion_Estimation_of_Object_Surface_by_using_Low_ACCV_2020_paper.html": {
    "title": "Dense Pixel-wise Micro-motion Estimation of Object Surface by using Low Dimensional Embedding of Laser Speckle Pattern",
    "volume": "main",
    "abstract": "This paper proposes a method of estimating micro-motion of an object at eachpixel that is too small to detect under a common setup of camera andillumination. The method introduces an active-lighting approach to make themotion visually detectable. The approach is based on speckle pattern, which isproduced by the mutual interference of laser light on object's surface andcontinuously changes its appearance according to the out-of-plane motion of thesurface. In addition, speckle pattern becomes uncorrelated with large motion. Tocompensate such micro- and large motion, the method estimates the motionparameters up to scale at each pixel by nonlinear embedding of the specklepattern into low-dimensional space. The out-of-plane motion is calculated bymaking the motion parameters spatially consistent across the image. In theexperiments, the proposed method is compared with other measuring devices toprove the effectiveness of the method",
    "checked": true,
    "id": "a09cae6b0e623c3af06bf01a8d8fe2bd85c194d2",
    "semantic_title": "dense pixel-wise micro-motion estimation of object surface by using low dimensional embedding of laser speckle pattern",
    "citation_count": 0,
    "authors": [
      "Ryusuke Sagawa",
      "Yusuke Higuchi",
      "Hiroshi Kawasaki",
      "Ryo Furukawa",
      "Takahiro Ito"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Wang_Synergistic_Saliency_and_Depth_Prediction_for_RGB-D_Saliency_Detection_ACCV_2020_paper.html": {
    "title": "Synergistic Saliency and Depth Prediction for RGB-D Saliency Detection",
    "volume": "main",
    "abstract": "Depth information available from an RGB-D camera can be useful in segmenting salient objects when figure/ground cues from RGB channels are weak. This has motivated the development of several RGB-D saliency datasets and algorithms that use all four channels of the RGB-D data for both training and inference. Unfortunately, existing RGB-D saliency datasets are small, which may lead to overfitting and limited generalization for diverse scenarios. Here we propose a semi-supervised system for RGB-D saliency detection that can be trained on smaller RGB-D saliency datasets without saliency ground truth, while also make effective joint use of a large RGB saliency dataset with saliency ground truth together. To generalize our method on RGB-D saliency datasets, a novel prediction-guided cross-refinement module which jointly estimates both saliency and depth by mutual refinement between two respective tasks, and an adversarial learning approach are employed. Critically, our system does not require saliency ground-truth for the RGB-D datasets, which saves the massive human labor for hand labeling, and does not require the depth data for inference, allowing the method to be used for the much broader range of applications where only RGB data are available. Evaluation on seven RGB-D datasets demonstrates that even without saliency ground truth for RGB-D datasets and using only the RGB data of RGB-D datasets at inference, our semi-supervised system performs favorable against state-of-the-art fully-supervised RGB-D saliency detection methods that use saliency ground truth for RGB-D datasets at training and depth data at inference on two largest testing datasets. Our approach also achieves comparable results on other popular RGB-D saliency benchmarks",
    "checked": true,
    "id": "c1db077b0bdd237048c6300622ad039813db50ae",
    "semantic_title": "synergistic saliency and depth prediction for rgb-d saliency detection",
    "citation_count": 8,
    "authors": [
      "Yue Wang",
      "Yuke Li",
      "James H. Elder",
      "Runmin Wu",
      "Huchuan Lu",
      "Lu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Lebailly_Motion_Prediction_Using_Temporal_Inception_Module_ACCV_2020_paper.html": {
    "title": "Motion Prediction Using Temporal Inception Module",
    "volume": "main",
    "abstract": "Human motion prediction is a necessary component for many applications in robotics and autonomous driving. Recent methods propose using sequence-to-sequence deep learning models to tackle this problem. However, they do not focus on exploiting different temporal scales for different length inputs. We argue that the diverse temporal scales are important as they allow us to look at the past frames with different receptive fields, which can lead to better predictions. In this paper, we propose a Temporal Inception Module (TIM) to encode human motion. Making use of TIM, our framework produces input embeddings using convolutional layers, by using different kernel sizes for different input lengths. The experimental results on standard motion prediction benchmark datasets Human3.6M and CMU motion capture dataset show that our approach consistently outperforms the state of the art methods",
    "checked": true,
    "id": "568394fe0efe3f16f03be4e96dc7fb0798d9e4bd",
    "semantic_title": "motion prediction using temporal inception module",
    "citation_count": 31,
    "authors": [
      "Tim Lebailly",
      "Sena Kiciroglu",
      "Mathieu Salzmann",
      "Pascal Fua",
      "Wei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Srinivasan_Hierarchical_X-Ray_Report_Generation_via_Pathology_tags_and_Multi_Head_ACCV_2020_paper.html": {
    "title": "Hierarchical X-Ray Report Generation via Pathology tags and Multi Head Attention",
    "volume": "main",
    "abstract": "Examining radiology images, such as X-Ray images as accurately as possible, forms a crucial step in providing the best healthcare facilities. However, this requires high expertise and clinical experience. Even for experienced radiologists, this is a time-consuming task. Hence, the automated generation of accurate radiology reports from chest X-Ray images is gaining popularity. Compared to other image captioning tasks where coherence is the key criterion, medical image captioning requires high accuracy in detecting anomalies and extracting information along with coherence. That is, the report must be easy to read and convey medical facts accurately. We propose a deep neural network to achieve this. Given a set of Chest X-Ray images of the patient, the proposed network predicts the medical tags and generates a readable radiology report. For generating the report and tags, the proposed network learns to extract salient features of the image from a deep CNN and generates tag embeddings for each patient's X-Ray images. We use transformers for learning self and cross attention. We encode the image and tag features with self-attention to get a finer representation. Use both the above features in cross attention with the input sequence to generate the report's Findings. Then, cross attention is applied between the generated Findings and the input sequence to generate the report's Impressions. We use a publicly available dataset to evaluate the proposed network. The performance indicates that we can generate a readable radiology report, with a relatively higher BLEU score over SOTA. The code and trained models are available at https://medicalcaption.github.io",
    "checked": true,
    "id": "eed57902470501bf75720a19b56dfff8778cdf5e",
    "semantic_title": "hierarchical x-ray report generation via pathology tags and multi head attention",
    "citation_count": 15,
    "authors": [
      "Preethi Srinivasan",
      "Daksh Thapar",
      "Arnav Bhavsar",
      "Aditya Nigam"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Zhang_Rotation_Equivariant_Orientation_Estimation_for_Omnidirectional_Localization_ACCV_2020_paper.html": {
    "title": "Rotation Equivariant Orientation Estimation for Omnidirectional Localization",
    "volume": "main",
    "abstract": "Deep learning for 6-degree-of-freedom (6-DoF) camera pose estimation is highly efficient at test time and can achieve accurate results in challenging, weakly textured environments. Typically, however, success in deep learning relies on large amounts of training images spanning many orientations and positions of the environment making it impractical for medium size or large environments. In this work we present a direct 6-DoF camera pose estimation method which alleviates the need for orientation augmentation at train time while still supporting any SO(3) rotation at test time. This property is achieved by the following three step procedure. Firstly, omni-directional training images are rotated to a common orientation. Secondly, a fully rotation equivariant CNN encoder is applied and its output is used to obtain: (i) a rotation invariant prediction of the camera position and (ii) a rotation equivariant prediction of the probability distribution over camera orientations. Finally, for a test image of a novel orientation, the camera position is predicted robustly due to in-built rotation invariance, while the camera orientation is recovered from the relative shift of the peak in the probability distribution of camera orientations. We demonstrate our approach on synthetic and real-image datasets, where we significantly outperform classical CNN-based pose regression, (i) in terms of accuracy when a single training orientation is used, and (ii) in training efficiency when orientation augmentation is employed. To the best of our knowledge, our proposed rotation equivariant CNN for localization is the first direct pose estimation method able to predict orientation without explicit rotation augmentation at train time",
    "checked": true,
    "id": "a2e388abbb834cbee41592c26fdb68f1f5bd0f62",
    "semantic_title": "rotation equivariant orientation estimation for omnidirectional localization",
    "citation_count": 7,
    "authors": [
      "Chao Zhang",
      "Ignas Budvytis",
      "Stephan Liwicki",
      "Roberto Cipolla"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Yang_RealSmileNet_A_Deep_End-To-End_Network_for_Spontaneous_and_Posed_Smile_ACCV_2020_paper.html": {
    "title": "RealSmileNet: A Deep End-To-End Network for Spontaneous and Posed Smile Recognition",
    "volume": "main",
    "abstract": "Smiles play a vital role in the understanding of social interactions within different communities, and reveals the physical state of mind of people in both real and deceptive ways. Several methods have been proposed to recognize spontaneous and posed smiles. All follow a feature-engineering based pipeline requiring costly pre-processing steps such as manual annotation of face landmarks, tracking, segmentation of smile phases, and hand-crafted features. The resulting computation is expensive, and strongly dependent on pre-processing steps. We investigate an end-to-end deep learning model to address these problems, the first end-to-end model for spontaneous and posed smile recognition. Our fully automated model is fast and learns the feature extraction processes by training a series of convolution and ConvLSTM layer from scratch. Our experiments on four datasets demonstrate the robustness and generalization of the proposed model by achieving state-of-the-art performances",
    "checked": true,
    "id": "0ce0c8474ed8b5a168e4b32ea2b0bacea353514e",
    "semantic_title": "realsmilenet: a deep end-to-end network for spontaneous and posed smile recognition",
    "citation_count": 6,
    "authors": [
      "Yan Yang",
      "Md Zakir Hossain",
      "Tom Gedeon",
      "Shafin Rahman"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Choi_Adversarially_Robust_Deep_Image_Super-Resolution_using_Entropy_Regularization_ACCV_2020_paper.html": {
    "title": "Adversarially Robust Deep Image Super-Resolution using Entropy Regularization",
    "volume": "main",
    "abstract": "Image super-resolution has been widely employed in various applications with boosted performance thanks to the deep learning techniques. However, many deep learning-based models are highly vulnerable to adversarial attacks, which is also applied to super-resolution models in recent studies. In this paper, we propose a defense method that is formulated as an entropy regularization loss for model training, which can be augmented to the original training loss of super-resolution models. We show that various state-of-the-art super-resolution models trained with our defense method are more robust against adversarial attacks than their original versions. To the best of our knowledge, this is the first attempt of adversarial defense for deep super-resolution models",
    "checked": true,
    "id": "db47ebfcb60538df93e8c78cc9c2fc935d970535",
    "semantic_title": "adversarially robust deep image super-resolution using entropy regularization",
    "citation_count": 5,
    "authors": [
      "Jun-Ho Choi",
      "Huan Zhang",
      "Jun-Hyuk Kim",
      "Cho-Jui Hsieh",
      "Jong-Seok Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Manttari_Interpreting_Video_Features_A_Comparison_of_3D_Convolutional_Networks_and_ACCV_2020_paper.html": {
    "title": "Interpreting Video Features: A Comparison of 3D Convolutional Networks and Convolutional LSTM Networks",
    "volume": "main",
    "abstract": "A number of techniques for interpretability have been presented for deep learning in computer vision, typically with the goal of understanding what the networks have based their classification on. However, interpretability for deep video architectures is still in its infancy and we do not yet have a clear concept of how to decode spatiotemporal features. In this paper, we present a study comparing how 3D convolutional networks and convolutional LSTM networks learn features across temporally dependent frames. This is the first comparison of two video models that both convolve to learn spatial features but have principally different methods of modeling time. Additionally, we extend the concept of meaningful perturbation introduced by Fong & Vedaldi to the temporal dimension, to identify the temporal part of a sequence most meaningful to the network for a classification decision. Our findings indicate that the 3D convolutional model concentrates on shorter events in the input sequence, and places its spatial focus on fewer, contiguous areas",
    "checked": true,
    "id": "ab78636736e978b814af4ecbe42d116bbfbbac1f",
    "semantic_title": "interpreting video features: a comparison of 3d convolutional networks and convolutional lstm networks",
    "citation_count": 25,
    "authors": [
      "Joonatan Manttari",
      "Sofia Broome",
      "John Folkesson",
      "Hedvig Kjellstrom"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Wang_Multiple_Exemplars-based_Hallucination_for_Face_Super-resolution_and_Editing_ACCV_2020_paper.html": {
    "title": "Multiple Exemplars-based Hallucination for Face Super-resolution and Editing",
    "volume": "main",
    "abstract": "Given a really low-resolution input image of a face (say 16x16 or 8x8 pixels), the goal of this paper is to reconstruct a high-resolution version thereof. This, by itself, is an ill-posed problem, as the high-frequency information is missing in the low-resolution input and needs to be hallucinated, based on prior knowledge about the image content. Rather than relying on a generic face prior, in this paper, we explore the use of a set of exemplars, i.e. other high-resolution images of the same person. These guide the neural network as we condition the output on them. Multiple exemplars work better than a single one. To combine the information from multiple exemplars effectively, we intro-duce a pixel-wise weight generation module. Besides standard face super-resolution, our method allows to perform subtle face editing simply by replacing the exemplars with another set with different facial features. A user study is conducted and shows the super-resolved images can hardly be distinguished from real images on the CelebA dataset. A qualitative comparison indicates our model outperforms methods proposed in the literature on the CelebA and WebFace data",
    "checked": true,
    "id": "212c73e6750308b4f19f0fd58b0c8c0dc1d8423f",
    "semantic_title": "",
    "citation_count": 4,
    "authors": [
      "Kaili Wang",
      "Jose Oramas",
      "Tinne Tuytelaars"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Le_Progressive_Batching_for_Efficient_Non-linear_Least_Squares_ACCV_2020_paper.html": {
    "title": "Progressive Batching for Efficient Non-linear Least Squares",
    "volume": "main",
    "abstract": "Non-linear least squares solvers are used across a broad range of offline and real-time model fitting problems. Most improvements of the basic Gauss-Newton algorithm tackle convergence guarantees or leverage the sparsity of the underlying problem structure for computational speedup. With the success of deep learning methods leveraging large datasets, stochastic optimization methods received recently a lot of attention. Our work borrows ideas from both stochastic machine learning and statistics, and we present an approach for non-linear least-squares that guarantees convergence while at the same time significantly reduces the required amount of computation. Empirical results show that our proposed method achieves competitive convergence rates compared to traditional second-order approaches on common computer vision problems such as essential/fundamental matrix estimation with very large numbers of correspondences",
    "checked": true,
    "id": "4a2656182e72e5699ba8f7acbaf70ced22a343bd",
    "semantic_title": "progressive batching for efficient non-linear least squares",
    "citation_count": 3,
    "authors": [
      "Huu Le",
      "Christopher Zach",
      "Edward Rosten",
      "Oliver J. Woodford"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Varamesh_MIXEM_Unsupervised_Image_Classification_using_a_Mixture_of_Embeddings_ACCV_2020_paper.html": {
    "title": "MIX'EM: Unsupervised Image Classification using a Mixture of Embeddings",
    "volume": "main",
    "abstract": "We present MIX'EM, a novel solution for unsupervised image classification. MIX'EM generates representations that by themselves are sufficient to drive a general-purpose clustering algorithm to deliver high-quality classification. This is accomplished by building a mixture of embeddings module into a contrastive visual representation learning framework in order to disentangle representations at the category level. It first generates a set of embedding and mixing coefficients from a given visual representation, and then combines them into a single embedding. We introduce three techniques to successfully train MIX'EM and avoid degenerate solutions; (i) diversify mixture components by maximizing entropy, (ii) minimize instance conditioned component entropy to enforce a clustered embedding space, and (iii) use an associative embedding loss to enforce semantic separability. By applying (i) and (ii), semantic categories emerge through the mixture coefficients, making it possible to apply (iii). Subsequently, we run K-means on the representations to acquire semantic classification. We conduct extensive experiments and analyses on STL10, CIFAR10, and CIFAR100-20 datasets, achieving state-of-the-art classification accuracy of 78%, 82%, and 44%, respectively. To achieve robust and high accuracy, it is essential to use the mixture components to initialize K-means. Finally, we report competitive baselines (70% on STL10) obtained by applying K-means to the \"normalized\" representations learned using the contrastive loss",
    "checked": true,
    "id": "64bb6866289e2fb8e2be47fd8ec46b347cce73fb",
    "semantic_title": "mix'em: unsupervised image classification using a mixture of embeddings",
    "citation_count": 4,
    "authors": [
      "Ali Varamesh",
      "Tinne Tuytelaars"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Zheng_Modular_Graph_Attention_Network_for_Complex_Visual_Relational_Reasoning_ACCV_2020_paper.html": {
    "title": "Modular Graph Attention Network for Complex Visual Relational Reasoning",
    "volume": "main",
    "abstract": "Visual Relational Reasoning is crucial for many vision-and-language based tasks, such as Visual Question Answering and Vision Language Navigation. In this paper, we consider reasoning on complex referring expression comprehension (c-REF) task that seeks to localise the target objects in an image guided by complex queries. Such queries often contain complex logic and thus impose two key challenges for reasoning: (i) It can be very difficult to comprehend the query since it often refers to multiple objects and describes complex relationships among them. (ii) It is non-trivial to reason among multiple objects guided by the query and localise the target correctly. To address these challenges, we propose a novel Modular Graph Attention Network (MGA-Net). Specifically, to comprehend the long queries, we devise a language attention network to decompose them into four types: basic attributes, absolute location, visual relationship and relative locations, which mimics the human language understanding mechanism. Moreover, to capture the complex logic in a query, we construct a relational graph to represent the visual objects and their relationships, and propose a multi-step reasoning method to progressively understand the complex logic. Extensive experiments on CLEVR-Ref+, GQA and CLEVR-CoGenT datasets demonstrate the superior reasoning performance of our MGA-Net",
    "checked": true,
    "id": "198e13b2ed0fd29c692ec82d4bc80d022defb430",
    "semantic_title": "modular graph attention network for complex visual relational reasoning",
    "citation_count": 5,
    "authors": [
      "Yihan Zheng",
      "Zhiquan Wen",
      "Mingkui Tan",
      "Runhao Zeng",
      "Qi Chen",
      "Yaowei Wang",
      "Qi Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Kok_FootNet_An_efficient_convolutional_network_for_multiview_3D_foot_reconstruction_ACCV_2020_paper.html": {
    "title": "FootNet: An efficient convolutional network for multiview 3D foot reconstruction",
    "volume": "main",
    "abstract": "Automatic biometric analysis of the human body is normally reserved for expensive customisation of clothing items e.g. for sports or medical purposes. These systems are usually built upon photogrammetric techniques currently requiring a rig and well calibrated cameras. Here we propose building on advancements in deep learning as well as utilising technology present in mobile phones for cheaply and accurately determining biometric data of the foot. The system is designed to run efficiently in a mobile phone app where it can be used in uncalibrated environments and without rigs. By scanning the foot with the phone camera, our system recovers both the 3D shape as well as the scale of the foot, opening the door way for automatic shoe size suggestion. Our contributions are (1) an efficient multiview feed forward neural network capable of inferring foot shape and scale, (2) a system for training from completely synthetic data and (3) a dataset of multiview feet images for evaluation. We fully ablate our system and show our design choices to improve performance at every stage. Our final design has a vertex error of only 1mm (for 25cm long synthetic feet) and 4mm error in foot length on real feet",
    "checked": true,
    "id": "afe30f58fead8e2f80251559ab7611048982d1a1",
    "semantic_title": "footnet: an efficient convolutional network for multiview 3d foot reconstruction",
    "citation_count": 3,
    "authors": [
      "Felix Kok",
      "James Charles",
      "Roberto Cipolla"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Huang_Generic_Image_Segmentation_in_Fully_Convolutional_Networks_by_Superpixel_Merging_ACCV_2020_paper.html": {
    "title": "Generic Image Segmentation in Fully Convolutional Networks by Superpixel Merging Map",
    "volume": "main",
    "abstract": "Recently, the Fully Convolutional Network (FCN) has been adopted in image segmentation. However, existing FCN-based segmentation algorithms were designed for semantic segmentation. Before learning-based algorithms were developed, many advanced generic segmentation algorithms are superpixel-based. However, due to the irregular shape and size of superpixels, it is hard to apply deep learning to superpixel-based image segmentation directly. In this paper, we combined the merits of the FCN and superpixels and proposed a highly accurate and extremely fast generic image segmentation algorithm. We treated image segmentation as multiple superpixel merging decision problems and determined whether the boundary between two adjacent superpixels should be kept. In other words, if the boundary of two adjacent superpixels should be deleted, then the two superpixels will be merged. The network applies the colors, the edge map, and the superpixel information to make decision about merging suprepixels. By solving all the superpixel-merging subproblems with just one forward pass, the FCN facilitates the speed of the whole segmentation process by a wide margin meanwhile gaining higher accuracy. Simulations show that the proposed algorithm has favorable runtime, meanwhile achieving highly accurate segmentation results. It outperforms state-of-the-art image segmentation methods, including feature-based and learning-based methods, in all metrics",
    "checked": true,
    "id": "80a52ed8704df08745da17de4a6c2638cc6ed270",
    "semantic_title": "generic image segmentation in fully convolutional networks by superpixel merging map",
    "citation_count": 4,
    "authors": [
      "Jin-Yu Huang",
      "Jian-Jiun Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Gittings_Vax-a-Net_Training-time_Defence_Against_Adversarial_Patch_Attacks_ACCV_2020_paper.html": {
    "title": "Vax-a-Net: Training-time Defence Against Adversarial Patch Attacks",
    "volume": "main",
    "abstract": "We present Vax-a-Net; a technique for immunizing convolutional neural networks (CNNs) against adversarial patch attacks (APAs). APAs insert visually overt, local regions (patches) into an image to induce misclassification. We introduce a conditional Generative Adversarial Network (GAN) architecture that simultaneously learns to synthesise patches for use in APAs, whilst exploiting those attacks to adapt a pre-trained target CNN to reduce its susceptibility to them. This approach enables resilience against APAs to be conferred to pre-trained models, which would be impractical with conventional adversarial training due to the slow convergence of APA methods. We demonstrate transferability of this protection to defend against existing APAs, and show its efficacy across several contemporary CNN architectures",
    "checked": true,
    "id": "24f5e5b5ee1287b9bd281d790f410dfeea567947",
    "semantic_title": "vax-a-net: training-time defence against adversarial patch attacks",
    "citation_count": 9,
    "authors": [
      "Thomas Gittings",
      "Steve Schneider",
      "John Collomosse"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Xiang_Second-order_Camera-aware_Color_Transformation_for_Cross-domain_Person_Re-identification_ACCV_2020_paper.html": {
    "title": "Second-order Camera-aware Color Transformation for Cross-domain Person Re-identification",
    "volume": "main",
    "abstract": "In recent years, supervised person re-identification (person ReID) has achieved great performance on public datasets, however, cross-domain person ReID remains a challenging task. The performance of ReID model trained on the labeled dataset (source) is often inferior on the new unlabeled dataset (target), due to large variation in color, resolution, scenes of different datasets. Therefore, unsupervised person ReID has gained a lot of attention due to its potential to solve the domain adaptation problem. Many methods focus on minimizing the distribution discrepancy in feature domain but neglecting the differences among input distributions. This motivates us to handle the variation between input distributions of source and target datasets directly. We propose a Second-order Camera-aware Color Transformation (SCCT) that can operate on image level and align the second-order statistics of all the views of both source and target domain data with original ImageNet data statistics. This new input normalization method, as shown in our experiments, is much more efficient than simply using ImageNet statistics. We test our method under different settings on Market1501, DukeMTMC and MSMT17 and achieve leading performance in unsupervised person ReID. To show that our methods can generalize well on other tasks we also conduct experiments on Vehicle ReID and achieves consistent improvements over baseline methods",
    "checked": true,
    "id": "bb93805a53f40bcbcb395441053f4314bdf86356",
    "semantic_title": "second-order camera-aware color transformation for cross-domain person re-identification",
    "citation_count": 6,
    "authors": [
      "Wangmeng Xiang",
      "Hongwei Yong",
      "Jianqiang Huang",
      "Xian-Sheng Hua",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Schroeter_Learning_Multi-Instance_Sub-pixel_Point_Localization_ACCV_2020_paper.html": {
    "title": "Learning Multi-Instance Sub-pixel Point Localization",
    "volume": "main",
    "abstract": "In this work, we propose a novel approach that allows for the end-to-end learning of multi-instance point detection with inherent sub-pixel precision capabilities. To infer unambiguous localization estimates, our model relies on three components: the continuous prediction capabilities of offset-regression-based models, the finer-grained spatial learning ability of a novel continuous heatmap matching loss function introduced to that effect, and the prediction sparsity ability of count-based regularization. We demonstrate strong sub-pixel localization accuracy on single molecule localization microscopy and checkerboard corner detection, and improved sub-frame event detection performance in sport videos",
    "checked": true,
    "id": "724a024d6e9bdbaf633d6a819258763aeabbef08",
    "semantic_title": "learning multi-instance sub-pixel point localization",
    "citation_count": 7,
    "authors": [
      "Julien Schroeter",
      "Tinne Tuytelaars",
      "Kirill Sidorov",
      "David Marshall"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Golinski_Feedback_Recurrent_Autoencoder_for_Video_Compression_ACCV_2020_paper.html": {
    "title": "Feedback Recurrent Autoencoder for Video Compression",
    "volume": "main",
    "abstract": "Recent advances in deep generative modeling have enabled efficient modeling of high dimensional data distributions and opened up a new horizon for solving data compression problems. Specifically, autoencoder based learned image or video compression solutions are emerging as strong competitors to traditional approaches. In this work, We propose a new network architecture, based on common and well studied components, for learned video compression operating in low latency mode. Our method yields competitive MS-SSIM/rate performance on the high-resolution UVG dataset, among both learned video compression approaches and classical video compression methods (H.265 and H.264) in the rate range of interest for streaming applications. Additionally, we provide an analysis of existing approaches through the lens of their underlying probabilistic graphical models.Finally, we point out issues with temporal consistency and color shift observed in empirical evaluation, and suggest directions forward to alleviate those",
    "checked": true,
    "id": "7abcd10189a3a395ce76481d76a036948335ba37",
    "semantic_title": "feedback recurrent autoencoder for video compression",
    "citation_count": 40,
    "authors": [
      "Adam Golinski",
      "Reza Pourreza",
      "Yang Yang",
      "Guillaume Sautiere",
      "Taco S. Cohen"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Chang_EPSNet_Efficient_Panoptic_Segmentation_Network_with_Cross-layer_Attention_Fusion_ACCV_2020_paper.html": {
    "title": "EPSNet: Efficient Panoptic Segmentation Network with Cross-layer Attention Fusion",
    "volume": "main",
    "abstract": "Panoptic segmentation is a scene parsing task which unifies semantic segmentation and instance segmentation into one single task. However, the current state-of-the-art studies did not take too much concern on inference time. In this work, we propose an Efficient Panoptic Segmentation Network (EPSNet) to tackle the panoptic segmentation tasks with fast inference speed. Basically, EPSNet generates masks based on simple linear combination of prototype masks and mask coefficients. The light-weight network branches for instance segmentation and semantic segmentation only need to predict mask coefficients and produce masks with the shared prototypes predicted by prototype network branch. Furthermore, to enhance the quality of shared prototypes, we adopt a module called \"cross-layer attention fusion module\", which aggregates the multi-scale features with attention mechanism helping them capture the long-range dependencies between each other. To validate the proposed work, we have conducted various experiments on the challenging COCO panoptic dataset, which achieve highly promising performance with significantly faster inference speed (51ms on GPU)",
    "checked": true,
    "id": "1faf3477c1dac16623720c47339822497679e0bc",
    "semantic_title": "epsnet: efficient panoptic segmentation network with cross-layer attention fusion",
    "citation_count": 10,
    "authors": [
      "Chia-Yuan Chang",
      "Shuo-En Chang",
      "Pei-Yung Hsiao",
      "Li-Chen Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Laskar_Data-Efficient_Ranking_Distillation_for_Image_Retrieval_ACCV_2020_paper.html": {
    "title": "Data-Efficient Ranking Distillation for Image Retrieval",
    "volume": "main",
    "abstract": "Recent advances in deep learning has lead to rapid developments in the field of image retrieval. However, the best performing architectures incur significant computational cost. The paper addresses this issue using knowledge distillation for metric learning problems. Unlike previous approaches, our proposed method jointly addresses the following constraints: i) limited queries to teacher model, ii) black box teacher model with access to the final output representation, and iii) small fraction of original training data without any ground-truth labels. In addition, the distillation method does not require the student and teacher to have same dimensionality. The key idea is to augment the original training set with additional samples by performing linear interpolation in the final output representation space. In low training sample settings, our approach outperforms the fully supervised baseline approach on ROxford5k and RParis6k with the least possible teacher supervision",
    "checked": true,
    "id": "3443e7a472a0bbcecc799b0296e518e912f71e24",
    "semantic_title": "data-efficient ranking distillation for image retrieval",
    "citation_count": 3,
    "authors": [
      "Zakaria Laskar",
      "Juho Kannala"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Zhou_IAFA_Instance-Aware_Feature_Aggregation_for_3D_Object_Detection_from_a_ACCV_2020_paper.html": {
    "title": "IAFA: Instance-Aware Feature Aggregation for 3D Object Detection from a Single Image",
    "volume": "main",
    "abstract": "3D object detection from a single image is an important task in Autonomous Driving (AD), where various approaches have been proposed. However, the task is intrinsically ambiguous and challenging as single image depth estimation is already an ill-posed problem. In this paper, we propose an instance-aware approach to aggregate useful information for improving the accuracy of 3D object detection with the following contributions. First, an instance-aware feature aggregation (IAFA) module is proposed to collect local and global features for 3D bounding boxes regression. Second, we empirically find that the spatial attention module can be well learned by taking coarse-level instance annotations as a supervision signal. The proposed module has significantly boosted the performance of the baseline method on both 3D detection and 2D bird-eye's view of vehicle detection among all three categories. Third, our proposed method outperforms all single image-based approaches (even these methods trained with depth as auxiliary inputs) and achieves state-of-the-art 3D detection performance on the KITTI benchmark",
    "checked": true,
    "id": "ff040d62bc218fffbc5c9ba931281f8d27a8b014",
    "semantic_title": "iafa: instance-aware feature aggregation for 3d object detection from a single image",
    "citation_count": 13,
    "authors": [
      "Dingfu Zhou",
      "Xibin Song",
      "Yuchao Dai",
      "Junbo Yin",
      "Feixiang Lu",
      "Miao Liao",
      "Jin Fang",
      "Liangjun Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Wang_Faster_Self-adaptive_Deep_Stereo_ACCV_2020_paper.html": {
    "title": "Faster Self-adaptive Deep Stereo",
    "volume": "main",
    "abstract": "Fueled by the power of deep learning, stereo vision has made unprecedented advances in recent years. Existing deep stereo models, however, can be hardly deployed to real-world scenarios where the data comes on-the-fly without any ground-truth information, and the data distribution continuously changes over time. Recently, Tonioni et al. [??] proposed the first real-time self-adaptive deep stereo system (MADNet) to address this problem, which, however, still runs at a relatively low speed with not so satisfactory performance. In this paper, we significantly upgrade their work in both speed and accuracy by incorporating two key components. First, instead of adopting only the image reconstruction loss as the proxy supervision, a second more powerful supervision is proposed, termed Knowledge Reverse Distillation (KRD), to guide the learning of deep stereo models. Second, we introduce a straightforward yet surprisingly effective Adapt-or-Hold (AoH) mechanism to automatically determine whether or not to fine-tune the stereo model in the online environment. Both components are lightweight and can be integrated into MADNet with only a few lines of code. Experiments demonstrate that the two proposed components improve the system by a large margin in both speed and accuracy. Our final system is twice as fast as MADNet, meanwhile attains considerable superior performance on the popular benchmark datasets KITTI",
    "checked": true,
    "id": "2caf53ad2202f3264c38ed8f05ec66a955a77fdc",
    "semantic_title": "faster self-adaptive deep stereo",
    "citation_count": 3,
    "authors": [
      "Haiyang Wang",
      "Xinchao Wang",
      "Jie Song",
      "Jie Lei",
      "Mingli Song"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Navasardyan_Image_Inpainting_with_Onion_Convolutions_ACCV_2020_paper.html": {
    "title": "Image Inpainting with Onion Convolutions",
    "volume": "main",
    "abstract": "Recently deep learning methods have achieved a great success in image inpainting problem. However, reconstructing continuities of complex structures with non-stationary textures remains a challenging task for computer vision. In this paper, a novel approach to image inpainting problem is presented, which adapts exemplar-based methods for deep convolutional neural networks. The concept of onion convolution is introduced with the purpose of preserving feature continuities and semantic coherence. Similar to recent approaches, our onion convolution is able to capture long-range spatial correlations. In general, the implementation of modules with such ability in low-level features leads to impractically high latency and complexity. To address this limitations, the onion convolution suggests an efficient implementation. As qualitative and quantitative comparisons show, our method with onion convolutions outperforms state-of-the-art methods by producing more realistic, visually plausible and semantically coherent results",
    "checked": true,
    "id": "b1c21216e8abe4f20544a108471368729d01fefc",
    "semantic_title": "image inpainting with onion convolutions",
    "citation_count": 6,
    "authors": [
      "Shant Navasardyan",
      "Marianna Ohanyan"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Liu_Color_Enhancement_using_Global_Parameters_and_Local_Features_Learning_ACCV_2020_paper.html": {
    "title": "Color Enhancement using Global Parameters and Local Features Learning",
    "volume": "main",
    "abstract": "This paper proposes a neural network to learn global parameters and extract local features for color enhancement. Firstly, the global parameters extractor subnetwork with dilated convolution is used to estimate a global color transformation matrix. The introduction of the dilated convolution enhances the ability to aggregate spatial information. Secondly, the local features extractor subnetwork with a light dense block structure is designed to learn the matrix of local details. Finally, an enhancement map is obtained by multiplying these two matrices. A novel loss function is formulated to make the color of the generated image more consistent with that of the target. The enhanced image is formed by adding the original image with an enhancement map. Thus, we make it possible to adjust the enhancement intensity by multiplying the enhancement map with a weighting coefficient. We conduct experiments on the MIT-Adobe FiveK benchmark, and our algorithm generates superior performance compared with the state-of-the-art methods on images and videos, both qualitatively and quantitatively",
    "checked": true,
    "id": "b4aab2adb3f4f2a838096c5a60b82d334ccad513",
    "semantic_title": "color enhancement using global parameters and local features learning",
    "citation_count": 10,
    "authors": [
      "Enyu Liu",
      "Songnan Li",
      "Shan Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Lin_Audiovisual_Transformer_with_Instance_Attention_for_Audio-Visual_Event_Localization_ACCV_2020_paper.html": {
    "title": "Audiovisual Transformer with Instance Attention for Audio-Visual Event Localization",
    "volume": "main",
    "abstract": "Audio-visual event localization requires one to identify the event label across video frames by jointly observing visual and audio information. To address this task, we propose a deep learning framework of cross-modality co-attention for video event localization. Our proposed audiovisual transformer (AV-transformer) is able to exploit intra and inter-frame visual information, with audio features jointly observed to perform co-attention over the above three modalities. With visual, temporal, and audio information observed across consecutive video frames, our model achieves promising capability in extracting informative spatial/temporal features for improved event localization. Moreover, our model is able to produce instance-level attention, which would identify image regions at the instance level which are associated with the sound/event of interest. Experiments on a benchmark dataset confirm the effectiveness of our proposed framework, with ablation studies performed to verify the design of our propose network model",
    "checked": true,
    "id": "6a43b68c63b059ca884e08c04ac7e6d167655b12",
    "semantic_title": "audiovisual transformer with instance attention for audio-visual event localization",
    "citation_count": 62,
    "authors": [
      "Yan-Bo Lin",
      "Yu-Chiang Frank Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Tan_Local_Context_Attention_for_Salient_Object_Segmentation_ACCV_2020_paper.html": {
    "title": "Local Context Attention for Salient Object Segmentation",
    "volume": "main",
    "abstract": "Salient object segmentation aims at distinguishing various salient objects from backgrounds. Despite the lack of semantic consistency, salient objects often have obvious texture and location characteristics in local area. Based on this priori, we propose a novel Local Context Attention Network (LCANet) to generate locally reinforcement feature maps in a uniform representational architecture. The proposed network introduces an Attentional Correlation Filter (ACF) module to generate explicit local attention by calculating the correlation feature map between coarse prediction and global context. Then it is expanded to a Local Context Block(LCB). Furthermore, a one-stage coarse-to-fine structure is implemented based on LCB to adaptively enhance the local context description ability. Comprehensive experiments are conducted on several salient object segmentation datasets, demonstrating the superior performance of the proposed LCANet against the state-of-the-art methods, especially with 0.883 max F-score and 0.034 MAE on DUTS-TE dataset",
    "checked": true,
    "id": "bf5f32d6f532d933e8730e0d07ab9fe3835cb60f",
    "semantic_title": "local context attention for salient object segmentation",
    "citation_count": 7,
    "authors": [
      "Jing Tan",
      "Pengfei Xiong",
      "Zhengyi Lv",
      "Kuntao Xiao",
      "Yuwen He"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Zhang_CPTNet_Cascade_Pose_Transform_Network_for_Single_Image_Talking_Head_ACCV_2020_paper.html": {
    "title": "CPTNet: Cascade Pose Transform Network for Single Image Talking Head Animation",
    "volume": "main",
    "abstract": "We study the problem of talking head animation from a sin-gle image. Most of the existing methods focus on generating talking headsfor human. However, little attention has been paid to the creation of talk-ing head anime. In this paper, our goal is to synthesize vivid talking headsfrom a single anime image. To this end, we propose cascade pose trans-form network, termed CPTNet, that consists of a face pose transformnetwork and a head pose transform network. Specifically, we introducea mask generator to animate facial expression (e.g., close eyes and openmouth) and a grid generator for head movement animation, followed by afusion module to generate talking heads. In order to handle large motionand obtain more accurate results, we design a pose vector decomposi-tion and cascaded refinement strategy. In addition, we create an animetalking head dataset, that includes various anime characters and poses,to train our model. Extensive experiments on our dataset demonstratethat our model outperforms other methods, generating more accurateand vivid talking heads from a single anime image",
    "checked": true,
    "id": "abaee4d6616ccd330d58e2f5fb35f39c332820e0",
    "semantic_title": "cptnet: cascade pose transform network for single image talking head animation",
    "citation_count": 2,
    "authors": [
      "Jiale Zhang",
      "Ke Xian",
      "Chengxin Liu",
      "Yinpeng Chen",
      "Zhiguo Cao",
      "Weicai Zhong"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Gu_Introspective_Learning_by_Distilling_Knowledge_from_Online_Self-explanation_ACCV_2020_paper.html": {
    "title": "Introspective Learning by Distilling Knowledge from Online Self-explanation",
    "volume": "main",
    "abstract": "In recent years, many methods have been proposed to explain individual classification predictions of deep neural networks. However, how to leverage the created explanations to improve the learning process has been less explored. The explanations extracted from a model can be used to guide the learning process of the model itself. Another type of information used to guide the training of a model is the knowledge provided by a powerful teacher model. The goal of this work is to leverage the self-explanation to improve the learning process by borrowing ideas from knowledge distillation. We start by investigating the effective components of the knowledge transferred from the teacher network to the student network. Our investigation reveals that both the responses in non-ground-truth classes and the class-similarity information in teacher's outputs contribute to the success of the knowledge distillation. Motivated by the conclusion, we propose an implementation of introspective learning by distilling knowledge from online self-explanations. The models trained with the introspective learning procedure outperform the ones trained with the standard learning procedure, as well as the ones trained with different regularization methods. When compared to the models learned from peer networks or teacher networks, our models also show competitive performance and requires neither peers nor teachers",
    "checked": true,
    "id": "0bf6b733f85a25938f9760436151c873587af741",
    "semantic_title": "introspective learning by distilling knowledge from online self-explanation",
    "citation_count": 2,
    "authors": [
      "Jindong Gu",
      "Zhiliang Wu",
      "Volker Tresp"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Ohgushi_Road_Obstacle_Detection_Method_Based_on_an_Autoencoder_with_Semantic_ACCV_2020_paper.html": {
    "title": "Road Obstacle Detection Method Based on an Autoencoder with Semantic Segmentation",
    "volume": "main",
    "abstract": "Accurate detection of road obstacles is vital for ensuring safe autonomous driving, particularly on highways.However, existing methods tend to perform poorly when analyzing road scenes with complex backgrounds, because supervised approaches cannot detect unknown objects that are not included in the training dataset.Hence, in this study, we propose a road obstacle detection method using an autoencoder with semantic segmentation that was trained with only data from normal road scenes.The proposed method requires only a color image captured by a common in-vehicle camera as input. It then creates a resynthesized image using an autoencoder consisting of a semantic image generator as the encoder and a photographic image generator as the decoder.Extensive experiments demonstrate that the performance of the proposed method is comparable to that of existing methods, even without postprocessing. The proposed method with postprocessing outperformed state-of-the-art methods on the Lost and Found dataset.Further, in evaluations using our Highway Anomaly Dataset, which includes actual and synthetic road obstacles, the proposed method significantly outperformed a supervised method that explicitly learns road obstacles.Thus, the proposed machine-learning-based road obstacle detection method is a practical solution that will advance the development of autonomous driving systems",
    "checked": true,
    "id": "5fbb5c62250fe6134f7d32bc84cfe369b6446298",
    "semantic_title": "road obstacle detection method based on an autoencoder with semantic segmentation",
    "citation_count": 35,
    "authors": [
      "Toshiaki Ohgushi",
      "Kenji Horiguchi",
      "Masao Yamanaka"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Ouyang_Dynamic_Depth_Fusion_and_Transformation_for_Monocular_3D_Object_Detection_ACCV_2020_paper.html": {
    "title": "Dynamic Depth Fusion and Transformation for Monocular 3D Object Detection",
    "volume": "main",
    "abstract": "Removing particular objects in a video and filling up the corresponding blank regions with a plausible background is a challenging and often ill-posed task. In this paper, we propose a framework to solve this difficult problem in complex, dynamic scenes by leveraging multi-view geometry and convolutional neural networks based approaches. Given an input video with undesired object masks, we first extract the depth map and relative camera pose for each of the input frames. We then fuse the estimated depth and pose to create a global 3D scene reconstruction. By projecting the point clouds from the reconstructed grid volume, we can fill in the most of the regions masked in the original input. We then use learning-based approaches to inpaint the remaining pixels in the input video which could not be resolved by 3D reconstruction. Compared with previous video inpainting approaches, our system generates superior qualitative results on the DAVIS 2016 and KITTI datasets, particularly in scenes where multiple, large objects are removed",
    "checked": true,
    "id": "a5bd3e3ce1fb54a7c391c135475aa67935cd928a",
    "semantic_title": "dynamic depth fusion and transformation for monocular 3d object detection",
    "citation_count": 6,
    "authors": [
      "Erli Ouyang",
      "Li Zhang",
      "Mohan Chen",
      "Anurag Arnab",
      "Yanwei Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Priisalu_Semantic_Synthesis_of_Pedestrian_Locomotion_ACCV_2020_paper.html": {
    "title": "Semantic Synthesis of Pedestrian Locomotion",
    "volume": "main",
    "abstract": "We present a model for generating 3d articulated pedestrian locomotion in urban scenarios, with synthesis capabilities informed by the 3d scene semantics and geometry. We reformulate pedestrian trajectory forecasting as a structured reinforcement learning (RL) problem. This allows us to naturally combine prior knowledge on collision avoidance, 3d human motion capture and the motion of pedestrians as observed e.g. in Cityscapes, Waymo or simulation environments like Carla. Our proposed RL-based model allows pedestrians to accelerate and slow down to avoid imminent danger (e.g. cars), while obeying human dynamics learnt from in-lab motion capture datasets. Specifically, we propose a hierarchical model consisting of a semantic trajectory policy network that provides a distribution over possible movements, and a human locomotion network that generates 3d human poses in each step. The RL-formulation allows the model to learn even from states that are seldom exhibited in the dataset, utilizing all of the available prior and scene information. Extensive evaluations using both real and simulated data illustrate that the proposed model is on par with recent models such as S-GAN, ST-GAT and S-STGCNN in pedestrian forecasting, while outperforming these in collision avoidance. We also show that our model can be used to plan goal reaching trajectories in urban scenes with dynamic actors",
    "checked": true,
    "id": "71c78d42796f7a79ee3e017681c4595a525cdbbc",
    "semantic_title": "semantic synthesis of pedestrian locomotion",
    "citation_count": 7,
    "authors": [
      "Maria Priisalu",
      "Ciprian Paduraru",
      "Aleksis Pirinen",
      "Cristian Sminchisescu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Luo_3D_Human_Motion_Estimation_via_Motion_Compression_and_Refinement_ACCV_2020_paper.html": {
    "title": "3D Human Motion Estimation via Motion Compression and Refinement",
    "volume": "main",
    "abstract": "We develop a technique for generating smooth and accurate 3D human pose and motion estimates from RGB video sequences. Our technique, which we call Motion Estimation via Variational Autoencoder (MEVA), decomposes a temporal sequence of human motion into a smooth motion representation using auto-encoder-based motion compression and a residual representation learned through motion refinement. This two-step encoding process of human motion can represent a wide variety of general human motions while also retaining person-specific motion details. Experiments show that our method produces both smooth and accurate 3D human pose and motion estimates",
    "checked": true,
    "id": "09ec88cd49ce2104cb95b48bcb60721aac04580e",
    "semantic_title": "3d human motion estimation via motion compression and refinement",
    "citation_count": 120,
    "authors": [
      "Zhengyi Luo",
      "S. Alireza Golestaneh",
      "Kris M. Kitani"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Dendorfer_Goal-GAN_Multimodal_Trajectory_Prediction_Based_on_Goal_Position_Estimation_ACCV_2020_paper.html": {
    "title": "Goal-GAN: Multimodal Trajectory Prediction Based on Goal Position Estimation",
    "volume": "main",
    "abstract": "In this paper, we present Goal-GAN, an interpretable and end-to-end trainable model for human trajectory prediction. Inspired by human navigation, we model the task of trajectory prediction as an intuitive two-stage process: (i) goal estimation, which predicts the most likely target positions of the agent, followed by a (ii) routing module which estimates a set of plausible trajectories that route towards the estimated goal. We leverage information about the past trajectory and visual context of the scene to estimate a multi-modal probability distribution over the possible goal positions, which is used to sample a potential goal during the inference. The routing is governed by a recurrent neural network that reacts to physical constraints in the nearby surroundings and generates feasible paths that route towards the sampled goal. Our extensive experimental evaluation shows that our method establishes a new state-of-the-art on several benchmarks while being able to generate a realistic and diverse set of trajectories that conform to physical constraints",
    "checked": true,
    "id": "331489d303c0a0f1d8fffe73eea3f222d264430a",
    "semantic_title": "goal-gan: multimodal trajectory prediction based on goal position estimation",
    "citation_count": 76,
    "authors": [
      "Patrick Dendorfer",
      "Aljosa Osep",
      "Laura Leal-Taixe"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Pitteri_3D_Object_Detection_and_Pose_Estimation_of_Unseen_Objects_in_ACCV_2020_paper.html": {
    "title": "3D Object Detection and Pose Estimation of Unseen Objects in Color Images with Local Surface Embeddings",
    "volume": "main",
    "abstract": "We present an approach for detecting and estimating the 3D poses of objects in images that requires only an untextured CAD model and no training phase for new objects. Our approach combines Deep Learning and 3D geometry: It relies on an embedding of local 3D geometry to match the CAD models to the input images. For points at the surface of objects, this embedding can be computed directly from the CAD model; for image locations, we learn to predict it from the image itself. This establishes correspondences between 3D points on the CAD model and 2D locations of the input images. However, many of these correspondences are ambiguous as many points may have similar local geometries. We show that we can use Mask-RCNN in a class-agnostic way to detect the new objects without retraining and thus drastically limit the number of possible correspondences. We can then robustly estimate a 3D pose from these discriminative correspondences using a RANSAC-like algorithm. We demonstrate the performance of this approach on the T-LESS dataset, by using a small number of objects to learn the embedding and testing it on the other objects. Our experiments show that our method is on par or better than previous methods",
    "checked": true,
    "id": "02ec037ce6e6e6009ccfaa7087e54835b3ddbfac",
    "semantic_title": "3d object detection and pose estimation of unseen objects in color images with local surface embeddings",
    "citation_count": 26,
    "authors": [
      "Giorgia Pitteri",
      "Aurelie Bugeau",
      "Slobodan Ilic",
      "Vincent Lepetit"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Liu_TSI_Temporal_Scale_Invariant_Network_for_Action_Proposal_Generation_ACCV_2020_paper.html": {
    "title": "TSI: Temporal Scale Invariant Network for Action Proposal Generation",
    "volume": "main",
    "abstract": "Despite the great progress in temporal action proposal generation, most state-of-the-art methods ignore the impact of action scales and the performance of short actions is still far from satisfaction. In this paper, we first analyze the sample imbalance issue in action proposal generation, and correspondingly devise a novel scale-invariant loss function to alleviate the insufficient learning of short actions. To further achieve proposal generation task, we adopt the pipeline of boundary evaluation and proposal completeness regression, and propose the Temporal Scale Invariant network. To better leverage the temporal context, boundary evaluation module generates action boundaries with high-precision-assured global branch and high-recall-assured local branch. Simultaneously, the proposal evaluation module is supervised with introduced scale-invariant loss, predicting accurate proposal completeness for different scales of actions. Comprehensive experiments are conducted on ActivityNet-1.3 and THUMOS14 benchmarks, where TSI achieves state-of-the-art performance. Especially, AUC performance of short actions is boosted from 36.53% to 39.63% compared with baseline",
    "checked": true,
    "id": "dad4f81bd2d2c3e43ef17ae573ce9e58769acc62",
    "semantic_title": "tsi: temporal scale invariant network for action proposal generation",
    "citation_count": 20,
    "authors": [
      "Shuming Liu",
      "Xu Zhao",
      "Haisheng Su",
      "Zhilan Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Tang_Branch_Interaction_Network_for_Person_Re-identification_ACCV_2020_paper.html": {
    "title": "Branch Interaction Network for Person Re-identification",
    "volume": "main",
    "abstract": "Most existing Person Re-identification (Re-ID) models aim to learn global and multi-granularity local features by designing a multi-branch structure and performing a uniform partition with the various number of divisions in different branches. However, the uniform partition is likely to separate meaningful regions in a single branch, and interaction between various branches disappeared after the split. In this paper, we propose the Branch Interaction Network (BIN), a multi-branch network architecture with three branches for learning coarse-to-fine features. Instead of traditional uniform partition, a horizontal overlapped division is employed to make sure essential local areas between adjacent parts are covered. Additionally, a novel attention module called Inter-Branch Attention Module (IBAM) is introduced to model contextual dependencies in the spatial domain across branches and learn better shared and specific representations for each branch. Extensive experiments are conducted on three mainstream datasets, i.e., DukeMTMC-reID, Market-1501 and CUHK03, showing the effectiveness of our approach, which outperforms the state-of-the-art methods. For instance, we achieve a top result of 90.50% mAP and 92.06% rank-1 accuracy on DukeMTMC-reID with re-ranking",
    "checked": true,
    "id": "85628733f8a42495d89175ce4a2b8004087b9762",
    "semantic_title": "branch interaction network for person re-identification",
    "citation_count": 1,
    "authors": [
      "Zengming Tang",
      "Jun Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Sinha_Class-Wise_Difficulty-Balanced_Loss_for_Solving_Class-Imbalance_ACCV_2020_paper.html": {
    "title": "Class-Wise Difficulty-Balanced Loss for Solving Class-Imbalance",
    "volume": "main",
    "abstract": "Class-imbalance is one of the major challenges in real world datasets where a few classes (called majority classes) constitute much more data samples than the rest (called minority classes). Learning deep neural networks using such datasets leads to performance which is typically biased towards the majority classes. Most of the prior works try to solve class-imbalance by assigning more weights to the minority classes in various manners (e.g., data re-sampling, cost-sensitive learning). However, we argue that the number of available training data may not be always a good clue to determine the weighting strategy because some of the minority classes might be sufficiently represented even by a small number of training data. Overweighting samples of such classes can lead to drop in the model's overall performance. We claim that the 'difficulty' of a class as perceived by the model is more important to determine the weighting. In this light, we propose a novel loss function named Class-wise Difficulty-Balanced loss, or CDB loss, which dynamically distributes weights to each sample according to the difficulty of the class that the sample belongs to. Note that the assigned weights dynamically change as the 'difficulty' for the model may change with the learning progress. Extensive experiments are conducted on both image (artificially induced class-imbalanced MNIST, long-tailed CIFAR and ImageNet-LT) and video (EGTEA) datasets. The results show that CDB loss consistently outperforms the recently proposed loss functions on class-imbalanced datasets irrespective of the data type (i.e., video or image)",
    "checked": true,
    "id": "1516fe9eeca6e8bd3d26723f725d222107cb2551",
    "semantic_title": "class-wise difficulty-balanced loss for solving class-imbalance",
    "citation_count": 26,
    "authors": [
      "Saptarshi Sinha",
      "Hiroki Ohashi",
      "Katsuyuki Nakamura"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Park_Emotional_Landscape_Image_Generation_Using_Generative_Adversarial_Networks_ACCV_2020_paper.html": {
    "title": "Emotional Landscape Image Generation Using Generative Adversarial Networks",
    "volume": "main",
    "abstract": "We design a deep learning framework that generates landscape images that match an given emotion. We are working on a more challenging approach to generate landscape scenes that do not have main objects making it easier to recognize the emotion. To solve this problem, deep networks based on generative adversarial networks are proposed. A new residual unit called emotional residual unit (ERU) is proposed to better reflect the emotion on training. An affective feature matching loss (AFM-loss) optimized for the emotional image generation is also proposed. This approach produced better images according to the given emotions. To demonstrate performance of the proposed model, a set of experiments including user studies was conducted. The results reveal a higher preference in the new model than the previous ones, demonstrating the production of images suitable for the given emotions. Ablation studies demonstrate that the ERU and AFM-loss enhanced the performance of the model",
    "checked": true,
    "id": "c67014216db6c2ecbfb5c43da5ba48538b62df69",
    "semantic_title": "emotional landscape image generation using generative adversarial networks",
    "citation_count": 3,
    "authors": [
      "Chanjong Park",
      "In-Kwon Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Wang_L2R_GAN_LiDAR-to-Radar_Translation_ACCV_2020_paper.html": {
    "title": "L2R GAN: LiDAR-to-Radar Translation",
    "volume": "main",
    "abstract": "The lack of annotated public radar datasets causes difficulties for research in environmental perception from radar observations. In this paper, we propose a novel neural network based framework which we call L2R GAN to generate the radar spectrum of natural scenes from a given LiDAR point cloud.We adapt ideas from existing image-to-image translation GAN frameworks, which we investigate as a baseline for translating radar spectra image from a given LiDAR bird's eye view (BEV). However, for our application, we identify several shortcomings of existing approaches. As a remedy, we learn radar data generation with an occupancy-grid-mask as a guidance, and further design a set of local region generators and discriminator networks. This allows our L2R GAN to combine the advantages of global image features and local region detail, and not only learn the cross-modal relations between LiDAR and radar in large scale, but also refine details in small scale. Qualitative and quantitative comparison show that L2R GAN outperforms previous GAN architectures with respect to details by a large margin. A L2R-GAN-based GUI also allows users to define and generate radar data of special emergency scenarios to test corresponding ADAS applications such as Pedestrian Collision Warning (PCW)",
    "checked": true,
    "id": "2f4095117b37850046c246653138e9d7915cb3d5",
    "semantic_title": "l2r gan: lidar-to-radar translation",
    "citation_count": 12,
    "authors": [
      "Leichen Wang",
      "Bastian Goldluecke",
      "Carsten Anklam"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Wang_Attention-Based_Fine-Grained_Classification_of_Bone_Marrow_Cells_ACCV_2020_paper.html": {
    "title": "Attention-Based Fine-Grained Classification of Bone Marrow Cells",
    "volume": "main",
    "abstract": "Computer aided fine-grained classification of bone marrow cells is a significant task because manual morphological examination is time-consuming and highly dependent on the expert knowledge. Limited methods are proposed for the fine-grained classification of bone marrow cells. This can be partially attributed to challenges of insufficient data, high intra-class and low inter-class variances.In this work, we design a novel framework Attention-based Suppression and Attention-based Enhancement Net (ASAE-Net) to better distinguish different classes. Concretely, inspired by recent advances of weakly supervised learning, we develop an Attention-based Suppression and Attention-based Enhancement (ASAE) layer to capture subtle differences between cells. In ASAE layer, two parallel modules with no training parameters improve the discrimination in two different ways. Furthermore, we propose a Gradient-boosting Maximum-Minimum Cross Entropy (GMMCE) loss to reduce the confusion between subclasses. In order to decrease the intra-class variance, we adjust the hue in a simple way. In addition, we adopt a balanced sampler aiming to alleviate the issue of the data imbalance.Extensive experiments prove the effectiveness of our method. Our approach achieves favorable performance against other methods on our dataset",
    "checked": true,
    "id": "670e4209e785922f86f2992a1be50a70cabd06bb",
    "semantic_title": "attention-based fine-grained classification of bone marrow cells",
    "citation_count": 3,
    "authors": [
      "Weining Wang",
      "Peirong Guo",
      "Lemin Li",
      "Yan Tan",
      "Hongxia Shi",
      "Yan Wei",
      "Xiangmin Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Dai_Contrastively_Smoothed_Class_Alignment_for_Unsupervised_Domain_Adaptation_ACCV_2020_paper.html": {
    "title": "Contrastively Smoothed Class Alignment for Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "Recent unsupervised approaches to domain adaptation primarily focus on minimizing the gap between the source and the target domains through refining the feature generator, in order to learn a better alignment between the two domains. This minimization can be achieved via a domain classifier to detect target-domain features that are divergent from source-domain features. However, by optimizing via such domain classification discrepancy, ambiguous target samples that are not smoothly distributed on the low-dimensional data manifold are often missed. To solve this issue, we propose a novel Contrastively Smoothed Class Alignment (CoSCA) model, that explicitly incorporates both intra- and inter-class domain discrepancy to better align ambiguous target samples with the source domain. CoSCA estimates the underlying label hypothesis of target samples, and simultaneously adapts their feature representations by optimizing a proposed contrastive loss. In addition, Maximum Mean Discrepancy (MMD) is utilized to directly match features between source and target samples for better global alignment. Experiments on several benchmark datasets demonstrate that CoSCA can outperform state-of-the-art approaches for unsupervised domain adaptation by producing more discriminative features",
    "checked": true,
    "id": "003d5161cebc2746368f1a853738a06357160e48",
    "semantic_title": "contrastively smoothed class alignment for unsupervised domain adaptation",
    "citation_count": 22,
    "authors": [
      "Shuyang Dai",
      "Yu Cheng",
      "Yizhe Zhang",
      "Zhe Gan",
      "Jingjing Liu",
      "Lawrence Carin"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Cao_Anatomy_and_Geometry_Constrained_One-Stage_Framework_for_3D_Human_Pose_ACCV_2020_paper.html": {
    "title": "Anatomy and Geometry Constrained One-Stage Framework for 3D Human Pose Estimation",
    "volume": "main",
    "abstract": "Although significant progress has been achieved in monocular3D human pose estimation, the correlation between body parts andcross-view geometry consistency have not been well studied. In this work,to fully explore the priors on body structure and view-relationship for3D human pose estimation, we propose an anatomy and geometry constrainedone-stage framework. First of all, we define a kinematic structuremodel in deep learning framework which represents the joint positionsin a tree-structure model. Then we propose bone-length and bone-symmetrylosses based on the anatomy prior, to encode the body structureinformation. To further explore the cross-view geometry information,we introduce a novel training mechanism for multi-view consistencyconstraints, which effectively reduces unnatural and implausible estimationresults. The proposed approach achieves state-of-the-art results onboth Human3.6M and MPI-INF-3DHP data sets",
    "checked": true,
    "id": "db55c70068b00b07784b5db7d22aaae796b30b32",
    "semantic_title": "anatomy and geometry constrained one-stage framework for 3d human pose estimation",
    "citation_count": 3,
    "authors": [
      "Xin Cao",
      "Xu Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Zhong_Modeling_Cross-Modal_interaction_in_a_Multi-detector_Multi-modal_Tracking_Framework_ACCV_2020_paper.html": {
    "title": "Modeling Cross-Modal interaction in a Multi-detector, Multi-modal Tracking Framework",
    "volume": "main",
    "abstract": "Different modalities have their own advantages and disadvantages. In a tracking-by-detection framework, fusing data from multiple modalities would ideally improve tracking performance than using a single modality, but this is a challenge. This study builds upon previous research in this area. We propose a deep-learning based tracking-by-detection pipeline that uses multiple detectors and multiple sensors. For the input, we associate object proposals from 2D and 3D detectors. Through a cross-modal attention module, we optimize interaction between the 2D RGB and 3D point clouds features of each proposal. This helps to generate 2D features with suppressed irrelevant information for boosting performance. Through experiments on a published benchmark, we prove the value and ability of our design in introducing a multi-modal tracking solution to the current research on Multi-Object Tracking (MOT)",
    "checked": true,
    "id": "89188599d76f050649e87cd02bf0532835213cc2",
    "semantic_title": "modeling cross-modal interaction in a multi-detector, multi-modal tracking framework",
    "citation_count": 2,
    "authors": [
      "Yiqi Zhong",
      "Suya You",
      "Ulrich Neumann"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Li_End-to-end_Model-based_Gait_Recognition_ACCV_2020_paper.html": {
    "title": "End-to-end Model-based Gait Recognition",
    "volume": "main",
    "abstract": "Most existing gait recognition approaches adopt a two-step procedure: a preprocessing step to extract silhouettes or skeletons followed by recognition. In this paper, we propose an end-to-end model-based gait recognition method. Specifically, we employ a skinned multi-person linear (SMPL) model for human modeling, and estimate its parameters using a pre-trained human mesh recovery (HMR) network. As the pre-trained HMR is not recognition-oriented, we fine-tune it in an end-to-end gait recognition framework. To cope with differences between gait datasets and those used for pre-training the HMR, we introduce a reconstruction loss between the silhouette masks in the gait datasets and the rendered silhouettes from the estimated SMPL model produced by a differentiable renderer. This enables us to adapt the HMR to the gait dataset without supervision using the ground-truth joint locations. Experimental results with the OU-MVLP and CASIA-B datasets demonstrate the state-of-the-art performance of the proposed method for both gait identification and verification scenarios, a direct consequence of the explicitly disentangled pose and shape features produced by the proposed end-to-end model-based framework",
    "checked": true,
    "id": "69f68f943c6905b116dd4902e59961c18a5e2e3c",
    "semantic_title": "end-to-end model-based gait recognition",
    "citation_count": 87,
    "authors": [
      "Xiang Li",
      "Yasushi Makihara",
      "Chi Xu",
      "Yasushi Yagi",
      "Shiqi Yu",
      "Mingwu Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Kataoka_Pre-training_without_Natural_Images_ACCV_2020_paper.html": {
    "title": "Pre-training without Natural Images",
    "volume": "main",
    "abstract": "Is it possible to use convolutional neural networks pre-trained without any natural images to assist natural image understanding? The paper proposes a novel concept, Formula-driven Supervised Learning. We automatically generate image patterns and their category labels by assigning fractals, which are based on a natural law existing in the background knowledge of the real world. Theoretically, the use of automatically generated images instead of natural images in the pre-training phase allows us to generate an infinite scale dataset of labeled images. Although the models pre-trained with the proposed Fractal DataBase (FractalDB), a database without natural images, does not necessarily outperform models pre-trained with human annotated datasets at all settings, we are able to partially surpass the accuracy of ImageNet/Places pre-trained models. The image representation with the proposed FractalDB captures a unique feature in the visualization of convolutional layers and attentions",
    "checked": true,
    "id": "090232509508427eadf1dc1fc04871ed317aa8d7",
    "semantic_title": "pre-training without natural images",
    "citation_count": 85,
    "authors": [
      "Hirokatsu Kataoka",
      "Kazushige Okayasu",
      "Asato Matsumoto",
      "Eisuke Yamagata",
      "Ryosuke Yamada",
      "Nakamasa Inoue",
      "Akio Nakamura",
      "Yutaka Satoh"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Van_Luan_Tran_MBNet_A_Multi-Task_Deep_Neural_Network_for_Semantic_Segmentation_and_ACCV_2020_paper.html": {
    "title": "MBNet: A Multi-Task Deep Neural Network for Semantic Segmentation and Lumbar Vertebra Inspection on X-ray Images",
    "volume": "main",
    "abstract": "Deep learning methods, especially multi-task learning with CNNs, have achieved good results in many fields of computer vision. Semantic segmentation and shape detection of lumbar vertebrae, sacrum, and femoral heads from clinical X-ray images are important and challenging tasks. In this paper, we propose a multi-task deep neural network, MBNet. It is developed based on our new multi-path convolutional neural network, BiLuNet, for semantic segmentation on X-ray images. Our MBNet has two branches, one is for semantic segmentation of lumbar vertebrae, sacrum, and femoral heads. It shares the main features with the second branch to learn and classify by supervised learning. The output of the second branch is to predict the inspected values for lumbar vertebra inspection. These networks are capable of performing the two tasks with very limited training data. We collect our dataset and annotated it by doctors for model training and performance evaluation. Compared to the state-of-the-art methods, our BiLuNet model provides better mIoUs with the same training data. The experimental results have demonstrated the feasibility of our MBNet for semantic segmentation of lumbar vertebrae, as well as the parameter prediction for the doctors to perform clinical diagnosis of low back pains.Code is available at https://github.com/LuanTran07/BiLUnet-Lumbar-Spine",
    "checked": true,
    "id": "5d0ac44fb109b953b27580e643a5d100b682ddc4",
    "semantic_title": "mbnet: a multi-task deep neural network for semantic segmentation and lumbar vertebra inspection on x-ray images",
    "citation_count": 12,
    "authors": [
      "Van Luan Tran",
      "Huei-Yung Lin",
      "Hsiao-Wei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Sun_Weakly-supervised_Reconstruction_of_3D_Objects_with_Large_Shape_Variation_from_ACCV_2020_paper.html": {
    "title": "Weakly-supervised Reconstruction of 3D Objects with Large Shape Variation from Single In-the-Wild Images",
    "volume": "main",
    "abstract": "Existing unsupervised 3D object reconstruction methods can not work well if the shape of objects varies substantially across images or if the images have distracting background. This paper proposes a novel learning framework for reconstructing 3D objects with large shape variation from single in-the-wild images. Considering that shape variation leads to appearance change of objects at various scales, we propose a fusion module to form combined multi-scale image features for 3D reconstruction. To deal with the ambiguity caused by shape variation, we propose side-output mask constraint to supervise the feature extraction, and adaptive edge constraint and initial shape constraint to supervise the shape reconstruction. Moreover, we propose background manipulation to augment the training images such that the obtained model is robust to background distraction. Extensive experiments have been done for both non-rigid objects (birds) and rigid objects (planes and vehicles), and the results prove the superiority of the proposed method",
    "checked": true,
    "id": "3ff44b17f712423e3aea8db3797c5c3bcb00c136",
    "semantic_title": "weakly-supervised reconstruction of 3d objects with large shape variation from single in-the-wild images",
    "citation_count": 3,
    "authors": [
      "Shichen Sun",
      "Zhengbang Zhu",
      "Xiaowei Dai",
      "Qijun Zhao",
      "Jing Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Townsend_ERIC_Extracting_Relations_Inferred_from_Convolutions_ACCV_2020_paper.html": {
    "title": "ERIC: Extracting Relations Inferred from Convolutions",
    "volume": "main",
    "abstract": "Our main contribution is to show that the behaviour of kernels across multiple layers of a convolutional neural network can be approximated using a logic program. The extracted logic programs yield accuracies that correlate with those of the original model, though with some information loss in particular as approximations of multiple layers are chained together or as lower layers are quantised. We also show that an extracted program can be used as a framework for further understanding the behaviour of CNNs. Specifically, it can be used to identify key kernels worthy of deeper inspection and also identify relationships with other kernels in the form of the logical rules. Finally, we make a preliminary, qualitative assessment of rules we extract from the last convolutional layer and show that kernels identified are symbolic in that they react strongly to sets of similar images that effectively divide output classes into sub-classes with distinct characteristics",
    "checked": true,
    "id": "6e860027d6e8f6b4c326df42d3aae96c028ef9a7",
    "semantic_title": "eric: extracting relations inferred from convolutions",
    "citation_count": 15,
    "authors": [
      "Joe Townsend",
      "Theodoros Kasioumis",
      "Hiroya Inakoshi"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Zhou_Reconstructing_Human_Body_Mesh_from_Point_Clouds_by_Adversarial_GP_ACCV_2020_paper.html": {
    "title": "Reconstructing Human Body Mesh from Point Clouds by Adversarial GP Network",
    "volume": "main",
    "abstract": "We study the problem of reconstructing the template-aligned mesh for human body estimation from unstructured point cloud data. Recent studies of the shape matching problem using DNN methodologies have shown state-of-the-art results with generic point-wise architectures, but in so doing exploit much weaker human shape and surface priors in the inference than previous methods with explicit shape surface models. Since they are bound to improve the performance even more, we investigate the impact of adding back such constraints by proposing a new dedicated human template matching process with a point-based deep-autoencoder architecture, where surface consistency of surface points is enforced and parameterized with a specialized Gaussian Process layer, and whose global consistency and generalization abilities are enforced with adversarial training. The choice of these elements is grounded in a detailed review of failure cases in standard datasets SURREAL and FAUST. We validate and evaluate the impact of these components on this data with measured improvement over state of the art DNN methods, which also show through a leap in the visual quality of the results",
    "checked": true,
    "id": "c08bd4541327207d6f5dae0de65e65008675892a",
    "semantic_title": "reconstructing human body mesh from point clouds by adversarial gp network",
    "citation_count": 7,
    "authors": [
      "Boyao Zhou",
      "Jean-Sebastien Franco",
      "Federica Bogo",
      "Bugra Tekin",
      "Edmond Boyer"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Ditria_OpenGAN_Open_Set_Generative_Adversarial_Networks_ACCV_2020_paper.html": {
    "title": "OpenGAN: Open Set Generative Adversarial Networks",
    "volume": "main",
    "abstract": "Many existing conditional Generative Adversarial Networks (cGANs) are limited to conditioning on pre-defined and fixed class-level semantic labels or attributes. We propose an open set GAN architecture (OpenGAN) that is conditioned per-input sample with a feature embedding drawn from a metric space. Using a state-of-the-art metric learning model that encodes both class-level and fine-grained semantic information, we are able to generate samples that are semantically similar to a given source image. The semantic information extracted by the metric learning model transfers to out-of-distribution novel classes, allowing the generative model to produce samples that are outside of the training distribution. We show that our proposed method is able to generate 256x256 resolution images from novel classes that are of similar visual quality to those from the training classes. In lieu of a source image, we demonstrate that random sampling of the metric space also results in high-quality samples. We show that interpolation in the feature space and latent space results in semantically and visually plausible transformations in the image space. Finally, the usefulness of the generated samples to the downstream task of data augmentation is demonstrated. We show that classifier performance can be significantly improved by augmenting the training data with OpenGAN samples on classes that are outside of the GAN training distribution",
    "checked": true,
    "id": "663df917647bf527e6664dfc6cb9ea6ea3d77da0",
    "semantic_title": "opengan: open set generative adversarial networks",
    "citation_count": 18,
    "authors": [
      "Luke Ditria",
      "Benjamin J. Meyer",
      "Tom Drummond"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Meshgi_Leveraging_Tacit_Information_Embedded_in_CNN_Layers_for_Visual_Tracking_ACCV_2020_paper.html": {
    "title": "Leveraging Tacit Information Embedded in CNN Layers for Visual Tracking",
    "volume": "main",
    "abstract": "Different layers in CNNs provide not only different levels of abstraction for describing the objects in the input but also encode various implicit information about them. The activation patterns of different features contain valuable information about the stream of incoming images: spatial relations, temporal patterns, and co-occurrence of spatial and spatiotemporal (ST) features. The studies in visual tracking literature, so far, utilized only one of the CNN layers, a pre-fixed combination of them, or an ensemble of trackers built upon individual layers. In this study, we employ an adaptive combination of several CNN layers in a single DCF tracker to address variations of the target appearances and propose the use of style statistics on both spatial and temporal properties of the target, directly extracted from CNN layers for visual tracking.Experiments demonstrate that using the additional implicit data of CNNs significantly improves the performance of the tracker. Results demonstrate the effectiveness of using style similarity and activation consistency regularization in improving its localization and scale accuracy",
    "checked": true,
    "id": "45084e6ee329bde87f1ac882994fe017431a1834",
    "semantic_title": "leveraging tacit information embedded in cnn layers for visual tracking",
    "citation_count": 0,
    "authors": [
      "Kourosh Meshgi",
      "Maryam Sadat Mirzaei",
      "Shigeyuki Oba"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Li_A_cost-effective_method_for_improving_and_re-purposing_large_pre-trained_GANs_ACCV_2020_paper.html": {
    "title": "A cost-effective method for improving and re-purposing large, pre-trained GANs by fine-tuning their class-embeddings",
    "volume": "main",
    "abstract": "Large, pre-trained generative models have been increasingly popular and useful to both the research and wider communities. Specifically, BigGANs a class-conditional Generative Adversarial Networks trained on ImageNet---achieved excellent, state-of-the-art capability in generating realistic photos. However, fine-tuning or training BigGANs from scratch is practically impossible for most researchers and engineers because (1) GAN training is often unstable and suffering from mode-collapse; and (2) the training requires a significant amount of computation, 256 Google TPUs for 2 days or 8xV100 GPUs for 15 days. Importantly, many pre-trained generative models both in NLP and image domains were found to contain biases that are harmful to society. Thus, we need computationally-feasible methods for modifying and re-purposing these huge, pre-trained models for downstream tasks. In this paper, we propose a cost-effective optimization method for improving and re-purposing BigGANs by fine-tuning only the class-embedding layer. We show the effectiveness of our model-editing approach in three tasks: (1) significantly improving the realism and diversity of samples of complete mode-collapse classes; (2) re-purposing ImageNet BigGANs for generating images for Places365; and (3) de-biasing or improving the sample diversity for selected ImageNet classes",
    "checked": true,
    "id": "07f6a8855dd99c57fae489bcc8e96d748f53e3fa",
    "semantic_title": "a cost-effective method for improving and re-purposing large, pre-trained gans by fine-tuning their class-embeddings",
    "citation_count": 6,
    "authors": [
      "Qi Li",
      "Long Mai",
      "Michael A. Alcorn",
      "Anh Nguyen"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Bhalodia_dpVAEs_Fixing_Sample_Generation_for_Regularized_VAEs_ACCV_2020_paper.html": {
    "title": "dpVAEs: Fixing Sample Generation for Regularized VAEs",
    "volume": "main",
    "abstract": "Unsupervised representation learning via generative modeling is a staple to many computer vision applications in the absence of labeled data. Variational Autoencoders (VAEs) are powerful generative models that learn representations useful for data generation. However, due to inherent challenges in the training objective, VAEs fail to learn useful representations amenable for downstream tasks. Regularization-based methods that attempt to improve the representation learning aspect of VAEs come at a price: poor sample generation. In this paper, we explore this representation-generation trade-off for regularized VAEs and introduce a new family of priors, namely decoupled priors, or dpVAEs, that decouple the representation space from the generation space. This decoupling enables the use of VAE regularizers on the representation space without impacting the distribution used for sample generation, and thereby reaping the representation learning benefits of the regularizations without sacrificing the sample generation. dpVAE leverages invertible networks to learn a bijective mapping from an arbitrarily complex representation distribution to a simple, tractable, generative distribution. Decoupled priors can be adapted to the state-of-the-art VAE regularizers without additional hyperparameter tuning. We showcase the use of dpVAEs with different regularizers. Experiments on MNIST, SVHN, and CelebA demonstrate, quantitatively and qualitatively, that dpVAE fixes sample generation for regularized VAEs",
    "checked": true,
    "id": "a4faa55208ef4d2c2f7a66b0bca8b45b7f06c0b5",
    "semantic_title": "dpvaes: fixing sample generation for regularized vaes",
    "citation_count": 5,
    "authors": [
      "Riddhish Bhalodia",
      "Iain Lee",
      "Shireen Elhabian"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Yi_Patch_SVDD_Patch-level_SVDD_for_Anomaly_Detection_and_Segmentation_ACCV_2020_paper.html": {
    "title": "Patch SVDD: Patch-level SVDD for Anomaly Detection and Segmentation",
    "volume": "main",
    "abstract": "In this paper, we address the problem of image anomaly detection and segmentation. Anomaly detection involves making a binary decision as to whether an input image contains an anomaly, and anomaly segmentation aims to locate the anomaly on the pixel level. Support vector data description (SVDD) is a long-standing algorithm used for an anomaly detection, and we extend its deep learning variant to the patch-based method using self-supervised learning. This extension enables anomaly segmentation and improves detection performance. As a result, anomaly detection and segmentation performances measured in AUROC on MVTec AD dataset increased by 9.8% and 7.0%, respectively, compared to the previous state-of-the-art methods. Our results indicate the efficacy of the proposed method and its potential for industrial application. Detailed analysis of the proposed method offers insights regarding its behavior, and the code is available online",
    "checked": true,
    "id": "62b77e5cb85fc61b84edd532f6d65714be152596",
    "semantic_title": "patch svdd: patch-level svdd for anomaly detection and segmentation",
    "citation_count": 287,
    "authors": [
      "Jihun Yi",
      "Sungroh Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Pu_Robust_High_Dynamic_Range_HDR_Imaging_with_Complex_Motion_and_ACCV_2020_paper.html": {
    "title": "Robust High Dynamic Range (HDR) Imaging with Complex Motion and Parallax",
    "volume": "main",
    "abstract": "High dynamic range (HDR) imaging is widely used in consumer photography, computer game rendering, autonomous driving, and surveillance systems. Reconstructing ghosting-free HDR images of dynamic scenes from a set of multi-exposure images is a challenging task, especially with large object motion, disparity, and occlusions, leading to visible artifacts using existing methods. In this paper, we propose a Pyramidal Alignment and Masked merging network (PAMnet) that learns to synthesize HDR images from input low dynamic range (LDR) images in an end-to-end manner. Instead of aligning under/overexposed images to the reference view directly in pixel-domain, we apply deformable convolutions across multiscale features for pyramidal alignment. Aligned features offer more flexibility to refine the inevitable misalignment for subsequent merging network without reconstructing the aligned image explicitly. To make full use of aligned features, we use dilated dense residual blocks with squeeze-and-excitation (SE) attention. Such attention mechanism effectively helps to remove redundant information and suppress misaligned features. Additional mask-based weighting is further employed to refine the HDR reconstruction, which offers better image quality and sharp local details. Experiments demonstrate that PAMnet can produce ghosting-free HDR results in the presence of large disparity and motion. We present extensive comparative studies using several popular datasets to demonstrate superior quality compared to the state-of-the-art algorithms",
    "checked": true,
    "id": "0f397b0c22f83c972117fdd79a10f2d736f49e5a",
    "semantic_title": "robust high dynamic range (hdr) imaging with complex motion and parallax",
    "citation_count": 27,
    "authors": [
      "Zhiyuan Pu",
      "Peiyao Guo",
      "M. Salman Asif",
      "Zhan Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Wimmer_FreezeNet_Full_Performance_by_Reduced_Storage_Costs_ACCV_2020_paper.html": {
    "title": "FreezeNet: Full Performance by Reduced Storage Costs",
    "volume": "main",
    "abstract": "Pruning generates sparse networks by setting parameters tozero. In this work we improve one-shot pruning methods, applied beforetraining, without adding any additional storage costs while preservingthe sparse gradient computations. The main difference to pruning is thatwe do not sparsify the network's weights but learn just a few key parame-ters and keep the other ones fixed at their random initialized value. Thismechanism is called freezing the parameters. Those frozen weights canbe stored efficiently with a single 32bit random seed number. The pa-rameters to be frozen are determined one-shot by a single for- and back-ward pass applied before training starts. We call the introduced methodFreezeNet. In our experiments we show that FreezeNets achieve good re-sults, especially for extreme freezing rates. Freezing weights preserves thegradient flow throughout the network and consequently, FreezeNets trainbetter and have an increased capacity compared to their pruned counter-parts. On the classification tasks MNIST and CIFAR-10/100 we outper-form SNIP, in this setting the best reported one-shot pruning method,applied before training. On MNIST, FreezeNet achieves 99.2% perfor-mance of the baseline LeNet-5-Caffe architecture, while compressing thenumber of trained and stored parameters by a factor of x157",
    "checked": true,
    "id": "908ef4da472415ded4537b61d9f409d0f341156d",
    "semantic_title": "freezenet: full performance by reduced storage costs",
    "citation_count": 14,
    "authors": [
      "Paul Wimmer",
      "Jens Mehnert",
      "Alexandru Condurache"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Nakka_Towards_Robust_Fine-grained_Recognition_by_Maximal_Separation_of_Discriminative_Features_ACCV_2020_paper.html": {
    "title": "Towards Robust Fine-grained Recognition by Maximal Separation of Discriminative Features",
    "volume": "main",
    "abstract": "Adversarial attacks have been widely studied for general classification tasks, but remain unexplored in the context of fine-grained recognition, where the inter-class similarities facilitate the attacker's task.In this paper, we identify the proximity of the latent representations of local regions of different classes in fine-grained recognition networks as a key factor to the success of adversarial attacks. We, therefore, introduce an attention-based regularization mechanism that maximally separates the latent features of discriminative regions of different classes while minimizing the contribution of the non-discriminative regions to the final class prediction. As evidenced by our experiments, this allows us to significantly improve robustness to adversarial attacks, to the point of matching or even surpassing that of adversarial training, but without requiring access to adversarial samples. Further, our formulation also improves the detection AUROC score of adversarial samples over baselines on adversarially trained models",
    "checked": true,
    "id": "e42b6dbf2e050b78890022d6b834c26567eb2fd5",
    "semantic_title": "towards robust fine-grained recognition by maximal separation of discriminative features",
    "citation_count": 6,
    "authors": [
      "Krishna Kanth Nakka",
      "Mathieu Salzmann"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Rodriguez_Understanding_Motion_in_Sign_Language_A_New_Structured_Translation_Dataset_ACCV_2020_paper.html": {
    "title": "Understanding Motion in Sign Language: A New Structured Translation Dataset",
    "volume": "main",
    "abstract": "Sign languages are the main mechanism of communication and interaction in the Deaf community. These languages are highly variable in communication with divergences between gloss representation, sign configuration, and multiple variants, among others, due to cultural and regional aspects. Current methods for automatic and continuous sign translation include robust and deep learning models that encode the visual signs representation. Despite the significant progress, the convergence of such models requires huge amounts of data to exploit sign representation, resulting in very complex models. This fact is associated to the highest variability but also to the shortage exploration of many language components that support communication. For instance, gesture motion and grammatical structure are fundamental components in communication, which can deal with visual and geometrical sign misinterpretations during video analysis. This work introduces a new Colombian sign language translation dataset (CoL-SLTD), that focuses on motion and structural information, and could be a significant resource to determine the contribution of several language components. Additionally, an encoder-decoder deep strategy is herein introduced to support automatic translation, including attention modules that capture short, long, and structural kinematic dependencies and their respective relationships with sign recognition. The evaluation in CoL-SLTD proves the relevance of the motion representation, allowing compact deep architectures to represent the translation. Also, the proposed strategy shows promising results in translation, achieving Bleu-4 scores of 35.81 and 4.65 in signer independent and unseen sentences tasks",
    "checked": true,
    "id": "cdf1e054087c308d3ffe1a0c90252856939aa12b",
    "semantic_title": "understanding motion in sign language: a new structured translation dataset",
    "citation_count": 5,
    "authors": [
      "Jefferson Rodriguez",
      "Juan Chacon",
      "Edgar Rangel",
      "Luis Guayacan",
      "Claudia Hernandez",
      "Luisa Hernandez",
      "Fabio Martinez"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Nie_Bidirectional_Pyramid_Networks_for_Semantic_Segmentation_ACCV_2020_paper.html": {
    "title": "Bidirectional Pyramid Networks for Semantic Segmentation",
    "volume": "main",
    "abstract": "Semantic segmentation is a fundamental problem in com-puter vision that has attracted a lot of attention. Recent eorts havebeen devoted to network architecture innovations for ecient semanticsegmentation that can run in real-time for autonomous driving and otherapplications. Information ow between scales is crucial because accuratesegmentation needs both large context and ne detail. However, most ex-isting approaches still rely on pretrained backbone models (e.g. ResNeton ImageNet). In this work, we propose to open up the backbone and de-sign a simple yet eective multiscale network architecture, BidirectionalPyramid Network (BPNet). BPNet takes the shape of a pyramid: infor-mation ows from bottom (high-resolution, small receptive eld) to top(low-resolution, large receptive eld), and from top to bottom, in a sys-tematic manner, at every step of the processing. More importantly, fusionneeds to be ecient; this is done through an add-and-multiply modulewith learned weights. We also apply a unary-pairwise attention mecha-nism to balance position sensitivity and context aggregation. Auxiliaryloss is applied at multiple steps of the pyramid bottom. The resultingnetwork achieves high accuracy with eciency, without the need of pre-training. On the standard Cityscapes dataset, we achieve test mIoU 76:3with 5:1M parameters and 36 fps (on Nvidia 2080 Ti), competitive withthe state of the time real-time models. Meanwhile, our design is generaland can be used to build heavier networks: a ResNet-101 equivalent ver-sion of BPNet achieves mIoU 81.9 on Cityscapes, competitive with thebest published results. We further demonstrate the exibility of BPNeton a prostate MRI segmentation task, achieving the state of the art with a45x speed-up",
    "checked": true,
    "id": "6d2748c9164f02d70275c3598a66682ba958b2c5",
    "semantic_title": "bidirectional pyramid networks for semantic segmentation",
    "citation_count": 18,
    "authors": [
      "Dong Nie",
      "Jia Xue",
      "Xiaofeng Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Marvasti-Zadeh_COMET_Context-Aware_IoU-Guided_Network_for_Small_Object_Tracking_ACCV_2020_paper.html": {
    "title": "COMET: Context-Aware IoU-Guided Network for Small Object Tracking",
    "volume": "main",
    "abstract": "We consider the problem of tracking an unknown small target from aerial videos of medium to high altitudes. This is a challenging problem, which is even more pronounced in unavoidable scenarios of drastic camera motion and high density. To address this problem, we introduce a context-aware IoU-guided tracker (COMET) that exploits a multitask two-stream network and an offline reference proposal generation strategy. The proposed network fully exploits target-related information by multi-scale feature learning and attention modules. The proposed strategy introduces an efficient sampling strategy to generalize the network on the target and its parts without imposing extra computational complexity during online tracking. These strategies contribute considerably in handling significant occlusions and viewpoint changes. Empirically, COMET outperforms the state-of-the-arts in a range of aerial view datasets that focusing on tracking small objects. Specifically, COMET outperforms the celebrated ATOM tracker by an average margin of 6.2% (and 7%) in precision (and success) score on challenging benchmarks of UAVDT, VisDrone-2019, and Small-90",
    "checked": true,
    "id": "842451bbece5958301283c9398139130643dcb73",
    "semantic_title": "comet: context-aware iou-guided network for small object tracking",
    "citation_count": 21,
    "authors": [
      "Seyed Mojtaba Marvasti-Zadeh",
      "Javad Khaghani",
      "Hossein Ghanei-Yakhdan",
      "Shohreh Kasaei",
      "Li Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Yang_Dense_Dual-Path_Network_for_Real-time_Semantic_Segmentation_ACCV_2020_paper.html": {
    "title": "Dense Dual-Path Network for Real-time Semantic Segmentation",
    "volume": "main",
    "abstract": "Semantic segmentation has achieved remarkable results with high computational cost and a large number of parameters. However, real-world applications require efficient inference speed on embedded devices. Most previous works address the challenge by reducing depth, width and layer capacity of network, which leads to poor performance. In this paper, we introduce a novel Dense Dual-Path Network (DDPNet) for real-time semantic segmentation under resource constraints. We design a light-weight and powerful backbone with dense connectivity to facilitate feature reuse throughout the whole network and the proposed Dual-Path module (DPM) to sufficiently aggregate multi-scale contexts. Meanwhile, a simple and effective framework is built with a skip architecture utilizing the high-resolution feature maps to refine the segmentation output and an upsampling module leveraging context information from the feature maps to refine the heatmaps. The proposed DDPNet shows an obvious advantage in balancing accuracy and speed. Specifically, on Cityscapes test dataset, DDPNet achieves 75.3% mIoU with 52.6 FPS for an input of 1024 X 2048 resolution on a single GTX 1080Ti card. Compared with other state-of-the-art methods, DDPNet achieves a significant better accuracy with a comparable speed and fewer parameters",
    "checked": true,
    "id": "551a0cd44acdc1abdd533c485f4e93eb9122c171",
    "semantic_title": "dense dual-path network for real-time semantic segmentation",
    "citation_count": 10,
    "authors": [
      "Xinneng Yang",
      "Yan Wu",
      "Junqiao Zhao",
      "Feilin Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Jang_Pose_Correction_Algorithm_for_Relative_Frames_between_Keyframes_in_SLAM_ACCV_2020_paper.html": {
    "title": "Pose Correction Algorithm for Relative Frames between Keyframes in SLAM",
    "volume": "main",
    "abstract": "With the dominance of keyframe-based SLAM in the field of robotics, the relative frame poses between keyframes have typically been sacrificed for a faster algorithm to achieve online applications. However, those approaches can become insufficient for applications that may require refined poses of all frames, not just keyframes which are relatively sparse compared to all input frames. This paper proposes a novel algorithm to correct the relative frames between keyframes after the keyframes have been updated by a back-end optimization process. The correction model is derived using conservation of the measurement constraint between landmarks and the robot pose. The proposed algorithm is designed to be easily integrable to existing keyframe-based SLAM systems while exhibiting robust and accurate performance superior to existing interpolation methods. The algorithm also requires low computational resources and hence has a minimal burden on the whole SLAM pipeline. We provide the evaluation of the proposed pose correction algorithm in comparison to existing interpolation methods in various vector spaces, and our method has demonstrated excellent accuracy in both KITTI and EuRoC datasets",
    "checked": true,
    "id": "2f8cf3cf57aa53ee1372630dfb9d3d18c2ad2ac9",
    "semantic_title": "pose correction algorithm for relative frames between keyframes in slam",
    "citation_count": 1,
    "authors": [
      "Youngseok Jang",
      "Hojoon Shin",
      "H. Jin Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Han_Self-supervised_Learning_of_Orc-Bert_Augmentator_for_Recognizing_Few-Shot_Oracle_Characters_ACCV_2020_paper.html": {
    "title": "Self-supervised Learning of Orc-Bert Augmentator for Recognizing Few-Shot Oracle Characters",
    "volume": "main",
    "abstract": "This paper studies the recognition of oracle character, the earliest known hieroglyphs in China. Essentially, oracle character recognition suffers from the problem of data limitation and imbalance. Recognizing the oracle characters of extremely limited samples, naturally, should be taken as the few-shot learning task. Different from the standard few-shot learning setting, our model has only access to large-scale unlabeled source Chinese characters and few labeled oracle characters. In such a setting, meta-based or metric-based few-shot methods are failed to be efficiently trained on source unlabeled data; and thus the only possible methodologies are self-supervised learning and data augmentation. Unfortunately, the conventional geometric augmentation always performs the same global transformations to all samples in pixel format, without considering the diversity of each part within a sample. Moreover, to the best of our knowledge, there is no effective self-supervised learning method for few-shot learning. To this end, this paper integrates the idea of self-supervised learning in data augmentation. And we propose a novel data augmentation approach, named Orc-Bert Augmentor pre-trained by self-supervised learning, for few-shot oracle character recognition. Specifically, Orc-Bert Augmentor leverages a self-supervised BERT model pre-trained on large unlabeled Chinese characters datasets to generate sample-wise augmented samples. Given a masked input in vector format, Orc-Bert Augmentor can recover it and then output a pixel format image as augmented data. Different mask proportion brings diverse reconstructed output. Concatenated with Gaussian noise, the model further performs point-wise displacement to improve diversity. Experimentally, we collect two large-scale datasets of oracle characters and other Chinese ancient characters for few-shot oracle character recognition and Orc-Bert Augmentor pre-training. Extensive experiments on few-shot learning demonstrate the effectiveness of our Orc-Bert Augmentor on improving the performance of various networks in the few-shot oracle character recognition",
    "checked": true,
    "id": "2071c27a72cd45c440443ea200f43378d4dbb6c3",
    "semantic_title": "self-supervised learning of orc-bert augmentor for recognizing few-shot oracle characters",
    "citation_count": 3,
    "authors": [
      "Wenhui Han",
      "Xinlin Ren",
      "Hangyu Lin",
      "Yanwei Fu",
      "Xiangyang Xue"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Ma_Accurate_and_Efficient_Single_Image_Super-Resolution_with_Matrix_Channel_Attention_ACCV_2020_paper.html": {
    "title": "Accurate and Efficient Single Image Super-Resolution with Matrix Channel Attention Network",
    "volume": "main",
    "abstract": "In recent years, deep learning methods have achieved impressive results with higher peak signal-to-noise ratio in single image super-resolution (SISR) tasks. However, these methods are usually computationally expensive, which constrains their application in mobile scenarios. In addition, most of the existing methods rarely take full advantage of the intermediate features which are helpful for restoration. To address these issues, we propose a moderate-size SISR network named matrix channel attention network (MCAN) by constructing a matrix ensemble of multi-connected channel attention blocks (MCAB). Several models of different sizes are released to meet various practical requirements. Extensive benchmark experiments show that the proposed models achieve better performance with much fewer multiply-adds and parameters",
    "checked": true,
    "id": "5aee999ffdc15312bc94810d1279afbcdc9d1e62",
    "semantic_title": "accurate and efficient single image super-resolution with matrix channel attention network",
    "citation_count": 6,
    "authors": [
      "Hailong Ma",
      "Xiangxiang Chu",
      "Bo Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Wang_Dense-Scale_Feature_Learning_in_Person_Re-Identification_ACCV_2020_paper.html": {
    "title": "Dense-Scale Feature Learning in Person Re-Identification",
    "volume": "main",
    "abstract": "For mass pedestrians re-identification (Re-ID), models must be capable of representing extremely complex and diverse multi-scale features. However, existing models only learn limited multi-scale features in a multi-branches manner, and directly expanding the number of scale branches for more scales will confuse the discrimination and affect performance. Because for a specific input image, there are a few scale features that are critical. In order to fulfill vast scale representation for person Re-ID and solve the contradiction of excessive scale declining performance, we proposed a novel Dense-Scale Feature Learning Network (DSLNet) which consist of two core components: Dense Connection Group (DCG) for providing abundant scale features, and Channel-Wise Scale Selection (CSS) module for dynamic select the most discriminative scale features to each input image. DCG is composed of a densely connected convolutional stream. The receptive field gradually increases as the feature flows along the convolution stream. Dense shortcut connections provide much more fused multi-scale features than existing methods. CSS is a novel attention module different from any existing model which calculates attention along the branch direction. By enhancing or suppressing specific scale branches, truly channel-wised multi-scale selection is realized. To the best of our knowledge, DSLNet is most lightweight and achieves state-of-the-art performance among lightweight models on four commonly used Re-ID datasets, surpassing most large-scale models",
    "checked": true,
    "id": "0608dffc15d4b4bb5fdeddd729e77fab2e72ede9",
    "semantic_title": "dense-scale feature learning in person re-identification",
    "citation_count": 8,
    "authors": [
      "Li Wang",
      "Baoyu Fan",
      "Zhenhua Guo",
      "Yaqian Zhao",
      "Runze Zhang",
      "Rengang Li",
      "Weifeng Gong"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Wang_Compact_and_Fast_Underwater_Segmentation_Network_for_Autonomous_Underwater_Vehicles_ACCV_2020_paper.html": {
    "title": "Compact and Fast Underwater Segmentation Network for Autonomous Underwater Vehicles",
    "volume": "main",
    "abstract": "Reliable and real-time semantic segmentation is crucial for vision-based navigation tasks undertaken by AUVs (Autonomous Underwater Vehicles). However state-of-art deep learning segmentation networks could not be deployed on embedded devices with limited onboard resources, due to the required high computation capacity and the lack of capability to deal with poor underwater image quality. In this work, we present a new deep underwater segmentation network, featured by a compact encoder and a lightweight decoder. We use only one step upsampling block to recover features maps from the encoder to significantly speed up the inference time. Furthermore, we adopt three strategies to improve network accuracy. Firstly, in parallel with the main decoder path, we introduce a branch path to extract additional low-level features. Secondly, we use position attention module to enhance the high-level semantic information and use channel attention module to introduce extra global context as well as refine the inter-dependencies of each feature. Thirdly, we proposed to use two additional auxiliary loss and smooth loss functions to better train the network, such that it will be more robust in segmenting images at varying resolutions and generating smooth boundaries.We validate our network accuracy on two different underwater segmentation datasets, a generalistic and a specialist one, and our model achieves the same level of accuracy of state-of-art networks. We also tested the network speed on different embedded platforms, and we showed it reaches real-time inference speed on both Nvidia Jetson GPU platforms TX2 and Nano, with respectively around 24 and 18 FPS (Frame Per Second). The proposed network inference is up to 27 times faster than other considered networks. Its high accuracy and speed will so pave the way for its deployment and application on AUVs systems",
    "checked": true,
    "id": "45e064fbfdf724f32741276283df0581f7a441c0",
    "semantic_title": "compact and fast underwater segmentation network for autonomous underwater vehicles",
    "citation_count": 3,
    "authors": [
      "Jiangtao Wang",
      "Baihua Li",
      "Yang Zhou",
      "Emanuele Rocco",
      "Qinggang Meng"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Sanchez_Semi-supervised_Facial_Action_Unit_Intensity_Estimation_with_Contrastive_Learning_ACCV_2020_paper.html": {
    "title": "Semi-supervised Facial Action Unit Intensity Estimation with Contrastive Learning",
    "volume": "main",
    "abstract": "This paper tackles the challenging problem of estimating the intensity of Facial Action Units with few labeled images. Contrary to previous works, our method does not require to manually select key frames, and produces state-of-the-art results with as little as 2% of annotated frames, which are randomly chosen. To this end, we propose a semi-supervised learning approach where a spatio-temporal model combining a feature extractor and a temporal module are learned in two stages. The first stage uses datasets of unlabeled videos to learn a strong spatio-temporal representation of facial behavior dynamics based on contrastive learning. To our knowledge we are the first to build upon this framework for modeling facial behavior in an unsupervised manner. The second stage uses another dataset of randomly chosen labeled frames to train a regressor on top of our spatio-temporal model for estimating the AU intensity. We show that although backpropagation through time is applied only with respect to the output of the network for extremely sparse and randomly chosen labeled frames, our model can be effectively trained to estimate AU intensity accurately, thanks to the unsupervised pre-training of the first stage. We experimentally validate that our method outperforms existing methods when working with as little as 2% of randomly chosen data for both DISFA and BP4D datasets, without a careful choice of labeled frames, a time-consuming task still required in previous approaches",
    "checked": true,
    "id": "0496633efa0259fa2d8dd9faf706eecae963d56f",
    "semantic_title": "semi-supervised facial action unit intensity estimation with contrastive learning",
    "citation_count": 3,
    "authors": [
      "Enrique Sanchez",
      "Adrian Bulat",
      "Anestis Zaganidis",
      "Georgios Tzimiropoulos"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Biswas_Discovering_Multi-Label_Actor-Action_Association_in_a_Weakly_Supervised_Setting_ACCV_2020_paper.html": {
    "title": "Discovering Multi-Label Actor-Action Association in a Weakly Supervised Setting",
    "volume": "main",
    "abstract": "Since collecting and annotating data for spatio-temporal action detection is very expensive, there is a need to learn approaches with less supervision. Weakly supervised approaches do not require any bounding box annotations and can be trained only from labels that indicate whether an action occurs in a video clip. Current approaches, however, cannot handle the case when there are multiple persons in a video that perform multiple actions at the same time. In this work, we address this very challenging task for the first time. We propose a baseline based on multi-instance and multi-label learning. Furthermore, we propose a novel approach that uses sets of actions as representation instead of modeling individual action classes. Since computing the probabilities for the full power set becomes intractable as the number of action classes increases, we assign an action set to each detected person under the constraint that the assignment is consistent with the annotation of the video clip. We evaluate the proposed approach on the challenging AVA dataset where the proposed approach outperforms the MIML baseline and is competitive to fully supervised approaches",
    "checked": true,
    "id": "1f2bd6effe8666d2843f66bcaae9724f7c453290",
    "semantic_title": "discovering multi-label actor-action association in a weakly supervised setting",
    "citation_count": 1,
    "authors": [
      "Sovan Biswas",
      "Juergen Gall"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Li_Mask-Ranking_Network_for_Semi-Supervised_Video_Object_Segmentation_ACCV_2020_paper.html": {
    "title": "Mask-Ranking Network for Semi-Supervised Video Object Segmentation",
    "volume": "main",
    "abstract": "Video object segmentation is the fundamental problem of video analysis and many methods based on mask propagation and matching have been proposed in recent years. However, the two strategies are highly dependent on the last mask or the fixed mask given in the first frame and hence cannot adapt well to high deformation and rapid motion of objects. In this paper, we proposed a novel architecture named Mask-Ranking Network(MRNet), which takes advantage of both the propagation-based method and the matching-based method, to address the above problem. Specifically, in order to make better use of the long-term previous masks, we propose a novel propagation mechanism to make the network comprehensively consider the previous information. Under a unified encoder-decoder framework, we track the pixel-wise similarity of the object activation area in a long-term manner and explore the correlation between frames. In contrast to propagation-based only or matching-based only techniques, our method reduces the accumulation of errors in the propagation process and effectively uses the long-term previous frame information. In the video object segmentation task, MRNet can better handle the deformation of the objects, and make the segmentation result more accurate. We validate the effectiveness of the proposed method on the DAVIS 2016 and DAVIS 2017 dataset. Experiment results show that our method achieve state-of-the-art performance without using online fine-tuning and is robust to long-term propagation",
    "checked": true,
    "id": "68af313b8936c811eb5a8b0b375d5aa905941d8a",
    "semantic_title": "mask-ranking network for semi-supervised video object segmentation",
    "citation_count": 1,
    "authors": [
      "Wenjing Li",
      "Xiang Zhang",
      "Yujie Hu",
      "Yingqi Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Zhang_Lossless_Image_Compression_Using_a_Multi-Scale_Progressive_Statistical_Model_ACCV_2020_paper.html": {
    "title": "Lossless Image Compression Using a Multi-Scale Progressive Statistical Model",
    "volume": "main",
    "abstract": "Lossless image compression is an important technique for im-age storage and transmission when information loss is not allowed. Withthe fast development of deep learning techniques, deep neural networkshave been used in this field to achieve a higher compression rate. Meth-ods based on pixel-wise autoregressive statistical models have showngood performance. However, the sequential processing way prevents thesemethods to be used in practice. Recently, multi-scale autoregressive mod-els have been proposed to address this limitation. Multi-scale approachescan use parallel computing systems efficiently and build practical sys-tems. Nevertheless, these approaches sacrifice compression performancein exchange for speed. In this paper, we propose a multi-scale progressivestatistical model that takes advantage of the pixel-wise approach and themulti-scale approach. We developed a flexible mechanism where the pro-cessing order of the pixels can be adjusted easily. Our proposed methodoutperforms the state-of-the-art lossless image compression methods ontwo large benchmark datasets by a significant margin without degradingthe inference speed dramatically",
    "checked": true,
    "id": "e8b0ad21efe02265b2b6b3aed897f2463ad5a9e7",
    "semantic_title": "lossless image compression using a multi-scale progressive statistical model",
    "citation_count": 14,
    "authors": [
      "Honglei Zhang",
      "Francesco Cricri",
      "Hamed R. Tavakoli",
      "Nannan Zou",
      "Emre Aksu",
      "Miska M. Hannuksela"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Cheng_3D_Object_Detection_from_Consecutive_Monocular_Images_ACCV_2020_paper.html": {
    "title": "3D Object Detection from Consecutive Monocular Images",
    "volume": "main",
    "abstract": "Detecting objects in 3D space plays an important role in scene understanding, such as urban autonomous driving and mobile robot navigation. Many image-based methods are recently proposed due to the high cost of LiDAR. However, monocular images are lack of depth information and difficult to detect objects with occlusion. In this paper, we propose to integrate 2D/3D object detection and 3D motion estimation for consecutive monocular images to overcome these problems. Additionally, we estimate the relative motion of the object between frames to reconstruct the scene in the previous timestamp. Then, we can recover depth cues from multi-view geometric constraints. To learn motion estimation from unlabeled data, we propose an unsupervised motion loss which learns 3D motion estimation from consecutive images. Our experiments on KITTI dataset show that the proposed method outperforms the state-of-the-art methods for 3D Pedestrian and Cyclist detection and achieves competitive results for 3D Car detection",
    "checked": true,
    "id": "1f510a3879bf734e3ca96349a73c9428ff94b193",
    "semantic_title": "3d object detection from consecutive monocular images",
    "citation_count": 1,
    "authors": [
      "Chia-Chun Cheng",
      "Shang-Hong Lai"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Wu_Synthetic-to-Real_Unsupervised_Domain_Adaptation_for_Scene_Text_Detection_in_the_ACCV_2020_paper.html": {
    "title": "Synthetic-to-Real Unsupervised Domain Adaptation for Scene Text Detection in the Wild",
    "volume": "main",
    "abstract": "Deep learning-based scene text detection can achieve preferable performance, powered with sufficient labeled training data. However, manual labeling is time consuming and laborious. At the extreme, the corresponding annotated data are unavailable. Exploiting synthetic data is a very promising solution except for domain distribution mismatches between synthetic datasets and real datasets. To address the severe domain distribution mismatch, we propose a synthetic-to-real domain adaptation method for scene text detection, which transfers knowledge from synthetic data (source domain) to real data (target domain). In this paper, a text self-training (TST) method and adversarial text instance alignment (ATA) for domain adaptive scene text detection are introduced. ATA helps the network learn domain-invariant features by training a domain classifier in an adversarial manner. TST diminishes the adverse effects of false positives(FPs) and false negatives(FNs) from inaccurate pseudo-labels. Two components have positive effects on improving the performance of scene text detectors when adapting from synthetic-to-real scenes. We evaluate the proposed method by transferring from SynthText, VISD to ICDAR2015, ICDAR2013. The results demonstrate the effectiveness of the proposed method with up to 10% improvement, which has important exploration significance for domain adaptive scene text detection",
    "checked": true,
    "id": "1455704c691e459959ea91e4dc61b5ca12ef4c75",
    "semantic_title": "synthetic-to-real unsupervised domain adaptation for scene text detection in the wild",
    "citation_count": 18,
    "authors": [
      "Weijia Wu",
      "Ning Lu",
      "Enze Xie",
      "Yuxing Wang",
      "Wenwen Yu",
      "Cheng Yang",
      "Hong Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Mo_Frequency_Attention_Network_Blind_Noise_Removal_for_Real_Images_ACCV_2020_paper.html": {
    "title": "Frequency Attention Network: Blind Noise Removal for Real Images",
    "volume": "main",
    "abstract": "With outstanding feature extraction capabilities, deep convolutional neural networks(CNNs) have achieved extraordinary improvements in image denoising tasks. However, because of the difference of statistical characteristics of signal-dependent noise and signal-independent noise, it is hard to model real noise for training and blind real image denoising is still an important challenge problem. In this work we propose a method for blind image denoising that combines frequency domain analysis and attention mechanism, named frequency attention network (FAN). We adopt wavelet transform to convert images from spatial domain to frequency domain with more sparse features to utilize spectrum information and structure information. For the denoising task, the objective of the neural network is to estimate the optimal solution of the wavelet coefficients of the clean image by nonlinear characteristics, which makes FAN possess good interpretability. Meanwhile, spatial and channel mechanisms are employed to enhance feature maps at different scales for capturing contextual information. Extensive experiments on the synthetic noise dataset and two real-world noise benchmarks indicate the superiority of our method over other competing methods at different noise type cases in blind image denoising",
    "checked": true,
    "id": "8bcf35b53ce3b33df805e5d0b770bf8faf3dbe13",
    "semantic_title": "frequency attention network: blind noise removal for real images",
    "citation_count": 3,
    "authors": [
      "Hongcheng Mo",
      "Jianfei Jiang",
      "Qin Wang",
      "Dong Yin",
      "Pengyu Dong",
      "Jingjun Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Techapanurak_Hyperparameter-Free_Out-of-Distribution_Detection_Using_Cosine_Similarity_ACCV_2020_paper.html": {
    "title": "Hyperparameter-Free Out-of-Distribution Detection Using Cosine Similarity",
    "volume": "main",
    "abstract": "The ability to detect out-of-distribution (OOD) samples is vital to secure the reliability of deep neural networks in real-world applications. Considering the nature of OOD samples, detection methods should not have hyperparameters that need to be tuned depending on incoming OOD samples. However, most of the recently proposed methods do not meet this requirement, leading to compromised performance in real-world applications. In this paper, we propose a simple and computationally efficient, hyperparameter-free method that uses cosine similarity. Although recent studies show its effectiveness for metric learning, it remains uncertain if cosine similarity works well also for OOD detection and, if so, why. We provide an intuitive explanation of why cosine similarity works better than the standard methods that use the maximum of softmax outputs or logits. Besides, there are several differences in the design of output layers, which are essential to achieve the best performance. We show through experiments that our method outperforms the existing methods on the evaluation test recently proposed by Shafaei et al., which takes the above issue of hyperparameter dependency into account; it achieves at least comparable performance to the state-of-the-art on the conventional test, where other methods but ours are allowed to use explicit OOD samples for determining hyperparameters",
    "checked": true,
    "id": "71ea4fce29a64843f1c3de747e1b4cb31bb2bedc",
    "semantic_title": "hyperparameter-free out-of-distribution detection using cosine similarity",
    "citation_count": 51,
    "authors": [
      "Engkarat Techapanurak",
      "Masanori Suganuma",
      "Takayuki Okatani"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Wei_MagGAN_High-Resolution_Face_Attribute_Editing_with_Mask-Guided_Generative_Adversarial_Network_ACCV_2020_paper.html": {
    "title": "MagGAN: High-Resolution Face Attribute Editing with Mask-Guided Generative Adversarial Network",
    "volume": "main",
    "abstract": "We present Mask-guided Generative Adversarial Network (MagGAN) for high-resolution face attribute editing, in which semantic facial masks from a pre-trained face parser are used to guide the finegrained image editing process. With the introduction of a mask-guided reconstruction loss, MagGAN learns to only edit the facial parts that are relevant to the desired attribute changes, while preserving the attributeirrelevant regions (e.g., hat, scarf for modification 'To Bald'). Further, a novel mask-guided conditioning strategy is introduced to incorporate the influence region of each attribute change into the generator. In addition, a multi-level patch-wise discriminator structure is proposed to scale our model for high-resolution (1024 x 1024) face editing. Experiments on the CelebA benchmark show that the proposed method significantly outperforms prior state-of-the-art approaches in terms of both image quality and editing performance",
    "checked": true,
    "id": "deaf2eacbb8441850fbd03f99f375ce3d283d2ab",
    "semantic_title": "maggan: high-resolution face attribute editing with mask-guided generative adversarial network",
    "citation_count": 14,
    "authors": [
      "Yi Wei",
      "Zhe Gan",
      "Wenbo Li",
      "Siwei Lyu",
      "Ming-Ching Chang",
      "Lei Zhang",
      "Jianfeng Gao",
      "Pengchuan Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Zheng_Background_Learnable_Cascade_for_Zero-Shot_Object_Detection_ACCV_2020_paper.html": {
    "title": "Background Learnable Cascade for Zero-Shot Object Detection",
    "volume": "main",
    "abstract": "Zero-shot detection (ZSD) is crucial to large-scale object detection with the aim of simultaneously localizing and recognizing unseen objects. There remain several challenges for ZSD, including reducing the ambiguity between background and unseen objects as well as improving the alignment between visual and semantic concept. In this work, we propose a novel framework named Background Learnable Cascade (BLC) to improve ZSD performance. The major contributions for BLC are as follows: (i) we propose a multi-stage cascade structure named Cascade Semantic R-CNN to progressively refine the alignment between visual and semantic of ZSD; (ii) we develop the semantic information flow structure and directly add it between each stage in Cascade Semantic RCNN to further improve the semantic feature learning; (iii) we propose the background learnable region proposal network (BLRPN) to learn an appropriate word vector for background class and use this learned vector in Cascade Semantic R-CNN, this design makes \"Background Learnable\" and reduces the confusion between background and unseen classes. Our extensive experiments show BLC obtains significantly performance improvements for MS-COCO over state-of-the-art methods",
    "checked": true,
    "id": "4799a2c6e3d9dd383f19065618ec7967ffe44c9c",
    "semantic_title": "background learnable cascade for zero-shot object detection",
    "citation_count": 38,
    "authors": [
      "Ye Zheng",
      "Ruoran Huang",
      "Chuanqi Han",
      "Xi Huang",
      "Li Cui"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Roziere_EvolGAN_Evolutionary_Generative_Adversarial_Networks_ACCV_2020_paper.html": {
    "title": "EvolGAN: Evolutionary Generative Adversarial Networks",
    "volume": "main",
    "abstract": "We propose to use a quality estimator and evolutionarymethods to search the latent space of generative adversarial networkstrained on small, difficult datasets, or both. The new method leads tothe generation of significantly higher quality images while preserving theoriginal generator's diversity. Human raters preferred an image from thenew version with frequency 83.7% for Cats, 74% for FashionGen, 70.4%for Horses, and 69.2% for Artworks - minor improvements for the alreadyexcellent GANs for faces. This approach applies to any quality scorer andGAN generator",
    "checked": true,
    "id": "47234a32a2a57ffa5f78443c85479a183a13ad41",
    "semantic_title": "evolgan: evolutionary generative adversarial networks",
    "citation_count": 13,
    "authors": [
      "Baptiste Roziere",
      "Fabien Teytaud",
      "Vlad Hosu",
      "Hanhe Lin",
      "Jeremy Rapin",
      "Mariia Zameshina",
      "Olivier Teytaud"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Zhang_SDP-Net_Scene_Flow_Based_Real-time_Object_Detection_and_Prediction_from_ACCV_2020_paper.html": {
    "title": "SDP-Net: Scene Flow Based Real-time Object Detection and Prediction from Sequential 3D Point Clouds",
    "volume": "main",
    "abstract": "Robust object detection in 3D point clouds faces the challenges caused by sparse range data. Accumulating multi-frame data could densify the 3D point clouds and greatly benefit detection task. However, accurately aligning the point clouds before the detecting process is a difficult task since there may exist moving objects in the scene. In this paper a novel scene flow based multi-frame network named SDP-Net is proposed. It is able to perform multiple tasks such as self-alignment, 3D object detection, prediction and tracking simultaneously. Thanks to the design of scene flow and the scheme of multi-task, our network is capable of working effectively with a simple network backbone. We further improve the annotations on KITTI RAW dataset by supplementing the ground truth. Experimental results show that our approach greatly outperforms the state-of-the-art and can perform multiple tasks in real-time",
    "checked": true,
    "id": "ac2ef5a07644336b135274b700aab818353d92d0",
    "semantic_title": "sdp-net: scene flow based real-time object detection and prediction from sequential 3d point clouds",
    "citation_count": 7,
    "authors": [
      "Yi Zhang",
      "Yuwen Ye",
      "Zhiyu Xiang",
      "Jiaqi Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Nazarczuk_V2A_-_Vision_to_Action_Learning_robotic_arm_actions_based_ACCV_2020_paper.html": {
    "title": "V2A - Vision to Action: Learning robotic arm actions based on vision and language",
    "volume": "main",
    "abstract": "In this work, we present a new AI task - Vision to Action (V2A) - where an agent (robotic arm) is asked to perform a high-level task with objects (e.g. stacking) present in a scene. The agent has to suggest a plan consisting of primitive actions (e.g. simple movement, grasping) in order to successfully complete the given task. Instructions are formulated in a way that forces the agent to perform visual reasoning over the presented scene before inferring the actions. We extend the recently introduced dataset SHOP-VRB with task instructions for each scene as well as an engine capable of assessing whether the sequence of primitives leads to a successful task completion. We also propose a novel approach based on multimodal attention for this task and demonstrate its performance on the new dataset",
    "checked": true,
    "id": "2e9d7d59be88d59e1cad97ea3ed198edfbdc3db0",
    "semantic_title": "v2a - vision to action: learning robotic arm actions based on vision and language",
    "citation_count": 5,
    "authors": [
      "Michal Nazarczuk",
      "Krystian Mikolajczyk"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Lei_Class-incremental_Learning_with_Rectified_Feature-Graph_Preservation_ACCV_2020_paper.html": {
    "title": "Class-incremental Learning with Rectified Feature-Graph Preservation",
    "volume": "main",
    "abstract": "In this paper, we address the problem of distillation-based class-incremental learning with a single head. A central theme of this task is to learn new classes that arrive in sequential phases over time while keeping the model's capability of recognizing seen classes with only limited memory for preserving seen data samples. Many regularization strategies have been proposed to mitigate the phenomenon of catastrophic forgetting. To understand better the essence of these regularizations, we introduce a feature-graph preservation perspective. Insights into their merits and faults motivate our weighted-Euclidean regularization for old knowledge preservation. We further propose rectified cosine normalization and show how it can work with binary cross-entropy to increase class separation for effective learning of new classes. Experimental results on both CIFAR-100 and ImageNet datasets demonstrate that our method outperforms the state-of-the-art approaches in reducing classification error, easing catastrophic forgetting, and encouraging evenly balanced accuracy over different classes. Our project page is at : https://github.com/yhchen12101/FGP-ICL",
    "checked": true,
    "id": "4322c5e9b03ff62129bf1f46f7941919d3ab0bfc",
    "semantic_title": "class-incremental learning with rectified feature-graph preservation",
    "citation_count": 4,
    "authors": [
      "Cheng-Hsun Lei",
      "Yi-Hsin Chen",
      "Wen-Hsiao Peng",
      "Wei-Chen Chiu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Nguyen_Attended-Auxiliary_Supervision_Representation_for_Face_Anti-spoofing_ACCV_2020_paper.html": {
    "title": "Attended-Auxiliary Supervision Representation for Face Anti-spoofing",
    "volume": "main",
    "abstract": "Recent face anti-spoofing methods have achieved impressive performance in recognizing the subtle discrepancies between live and spoof faces. However, due to directly holistic extraction and the resulting ineffective clues used for the models' perception, the previous methods are still subject to setbacks of not being generalizable to the diversity of presentation attacks. In this paper, we present an attended-auxiliary supervision approach for radical exploitation, which automatically concentrates on the most important regions of the input, that is, those that make significant contributions towards distinguishing the spoof cases from live faces. Through a multi-task learning approach, the proposed network is able to locate the most relevant/attended/highly selective regions more accurately than previous methods, leading to notable improvements in performance. We also suggest that introducing spatial attention mechanisms can greatly enhance our model's perception of the important information, partly intensifying the resilience of our model against diverse types of face anti-spoofing attacks. We carried out extensive experiments on publicly available face anti-spoofing datasets, showing that our approach and hypothesis converge to some extent and demonstrating state-of-the-art performance",
    "checked": true,
    "id": "99fa15c5d222e9e304acf4a8ad207d107aa1b6c4",
    "semantic_title": "attended-auxiliary supervision representation for face anti-spoofing",
    "citation_count": 3,
    "authors": [
      "Son Minh Nguyen",
      "Linh Duy Tran",
      "Masayuki Arai"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Drory_Best_Buddies_Registration_for_Point_Clouds_ACCV_2020_paper.html": {
    "title": "Best Buddies Registration for Point Clouds",
    "volume": "main",
    "abstract": "We propose new, and robust, loss functions for the point cloud registration problem. Our loss functions are inspired by the Best Buddies Similarity (BBS) measure that counts the number of mutual nearest neighbors between two point sets. This measure has been shown to be robust to outliers and missing data in the case of template matching for images. We present several algorithms, collectively named Best Buddy Registration (BBR), where each algorithm consists of optimizing one of these loss functions with Adam gradient descent. The loss functions differ in several ways, including the distance function used (point-to-point vs. point-to-plane), and how the BBS measure is combined with the actual distances between pairs of points. Experiments on various data sets, both synthetic and real, demonstrate the effectiveness of the BBR algorithms, showing that they are quite robust to noise, outliers, and distractors, and cope well with extremely sparse point clouds. One variant, BBR-F, achieves state-of-the-art accuracy in the registration of automotive lidar scans taken up to several seconds apart, from the KITTI and Apollo-Southbay datasets",
    "checked": true,
    "id": "7d187ba8b8e0abc49af6bb22be174a24d6c1ded0",
    "semantic_title": "best buddies registration for point clouds",
    "citation_count": 5,
    "authors": [
      "Amnon Drory",
      "Tal Shomer",
      "Shai Avidan",
      "Raja Giryes"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Zheng_Overwater_Image_Dehazing_via_Cycle-Consistent_Generative_Adversarial_Network_ACCV_2020_paper.html": {
    "title": "Overwater Image Dehazing via Cycle-Consistent Generative Adversarial Network",
    "volume": "main",
    "abstract": "In contrast to images taken on land scenes, images taken over water are more prone to degradation due to the influence of the haze. However, existing image dehazing methods are mainly developed for land scenes and perform poorly when applied to overwater images. To address this problem, we collect the first overwater image dehazing dataset and propose an OverWater Image Dehazing GAN (OWI-DehazeGAN). Due to the difficulties of collecting paired hazy and clean images, the dataset is composed of unpaired hazy and clean images taken over water. The proposed OWI-DehazeGAN learns the underlying style mapping between hazy and clean images in an encoder-decoder framework, which is supervised by a forward-backward translation consistency loss for self-supervision and a perceptual loss for content preservation. In addition to qualitative evaluation, we design an image quality assessment network to rank the dehazed images. Experimental results on both real and synthetic test data demonstrate that the proposed method performs superiorly against several state-of-the-art land dehazing methods",
    "checked": true,
    "id": "48a4e3c25b92264ca0e91947384f012180e8d3e1",
    "semantic_title": "overwater image dehazing via cycle-consistent generative adversarial network",
    "citation_count": 7,
    "authors": [
      "Shunyuan Zheng",
      "Jiamin Sun",
      "Qinglin Liu",
      "Yuankai Qi",
      "Shengping Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Hatakeyama_Visualizing_Color-wise_Saliency_of_Black-Box_Image_Classification_Models_ACCV_2020_paper.html": {
    "title": "Visualizing Color-wise Saliency of Black-Box Image Classification Models",
    "volume": "main",
    "abstract": "Image classification based on machine learning is being commonly used.However, a classification result given by an advanced method, including deep learning, is often hard to interpret.This problem of interpretability is one of the major obstacles in deploying a trained model in safety-critical systems.Several techniques have been proposed to address this problem;one of which is RISE, which explains a classification result by a heatmap, called a saliency map, that explains the significance of each pixel.We propose MC-RISE (Multi-Color RISE), which is an enhancement of RISE to take color information into account in an explanation.Our method not only shows the saliency of each pixel in a given image as the original RISE does, but the significance of color components of each pixel;a saliency map with color information is useful especially in the domain where the color information matters (e.g., traffic-sign recognition).We implemented MC-RISE and evaluate them using two datasets (GTSRB and ImageNet) to demonstrate the effectiveness of our methods in comparison with existing techniques for interpreting image classification results",
    "checked": true,
    "id": "fbf38feb7218a5664cec0b96db57b4b28bd2df93",
    "semantic_title": "visualizing color-wise saliency of black-box image classification models",
    "citation_count": 2,
    "authors": [
      "Yuhki Hatakeyama",
      "Hiroki Sakuma",
      "Yoshinori Konishi",
      "Kohei Suenaga"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Huang_CS-MCNetA_Video_Compressive_Sensing_Reconstruction_Network_with_Interpretable_Motion_Compensation_ACCV_2020_paper.html": {
    "title": "CS-MCNet:A Video Compressive Sensing Reconstruction Network with Interpretable Motion Compensation",
    "volume": "main",
    "abstract": "In this paper, a deep neural network with interpretable motion compensation called CS-MCNet is proposed to realize high-quality and real-time decoding of video compressive sensing. Firstly, explicit multi-hypothesis motion compensation is applied in our network to extract correlation information of adjacent frames, which improves the recover performance. And then, a residual module further narrows down the gap between reconstruction result and original signal. The overall architecture is interpretable by using algorithm unrolling, Which brings the benefits of being able to transfer prior knowledge about the conventional algorithms. As a result, a PSNR of 22dB can be achieved at 64x compression rate, which is about 4% to 9% better than state-of-the-art methods. In addition, due to the feed-forward architecture, the reconstruction can be processed by our network in real time and up to three orders of magnitude faster than traditional iterative methods",
    "checked": true,
    "id": "781e45acdf9ea07a913705bf4fc0129fb8c8508e",
    "semantic_title": "cs-mcnet: a video compressive sensing reconstruction network with interpretable motion compensation",
    "citation_count": 2,
    "authors": [
      "Bowen Huang",
      "Jinjia Zhou",
      "Xiao Yan",
      "Ming'e Jing",
      "Rentao Wan",
      "Yibo Fan"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Zhang_Few-Shot_Object_Detection_by_Second-order_Pooling_ACCV_2020_paper.html": {
    "title": "Few-Shot Object Detection by Second-order Pooling",
    "volume": "main",
    "abstract": "In this paper, we tackle a challenging problem of Few-shot Object Detection rather than recognition. We propose Power Normalizing Second-order Detector consisting of the Encoding Network (EN), the Multi-scale Feature Fusion (MFF), Second-order Pooling (SOP) with Power Normalization (PN), the Hyper Attention Region Proposal Network (HARPN) and Similarity Network (SN). EN takes support image crops and a query image per episode to produce covolutional feature maps across several layers while MFF combines them into multi-scale feature maps. SOP aggregates them per support image while PN detects the presence of visual feature instead of counting its frequency of occurrence. HARPN cross-correlates the PN pooled support features against the query feature map to match regions and produce query region proposals that are then aggregated with SOP/PN. Finally, support and query second-order descriptors are passed to SN. Our approach performs well because: (i) HARPN leverages SOP/PN for cross-correlation of detected rather than counted support features with query features which improves region proposals, (ii) SOP/PN capture second-order statistics per region proposal and factor out spatial locations, and (iii) PN limits the complexity of the space of functions over which HARPN and SN learn. These properties lead to the state of the art on the PASCAL VOC 2007/12, MS COCO and the FSOD datasets",
    "checked": true,
    "id": "cb454523e11f530de1e7b4909d13f9ff5e8663bd",
    "semantic_title": "few-shot object detection by second-order pooling",
    "citation_count": 34,
    "authors": [
      "Shan Zhang",
      "Dawei Luo",
      "Lei Wang",
      "Piotr Koniusz"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Uhlig_A_Calibration_Method_for_the_Generalized_Imaging_Model_with_Uncertain_ACCV_2020_paper.html": {
    "title": "A Calibration Method for the Generalized Imaging Model with Uncertain Calibration Target Coordinates",
    "volume": "main",
    "abstract": "The developments in optical metrology and computer vision require more and more advanced camera models. Their geometric calibration is of essential importance. Usually, low-dimensional models are used, which however often have insufficient accuracy for the respective applications. A more sophisticated approach uses the generalized camera model. Here, each pixel is described individually by its geometric ray properties. Our efforts in this article strive to improve this model. Hence, we propose a new approach for calibration. Moreover, we show how the immense number of parameters can be efficiently calculated and how the measurement uncertainties of reference features can be effectively utilized. We demonstrate the benefits of our method through an extensive evaluation of different cameras, namely a standard webcam and a microlens-based light field camera",
    "checked": true,
    "id": "f68e3018a2a183964006259da2bc14f17f2a6b1c",
    "semantic_title": "a calibration method for the generalized imaging model with uncertain calibration target coordinates",
    "citation_count": 8,
    "authors": [
      "David Uhlig",
      "Michael Heizmann"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Tian_D2D_Keypoint_Extraction_with_Describe_to_Detect_Approach_ACCV_2020_paper.html": {
    "title": "D2D: Keypoint Extraction with Describe to Detect Approach",
    "volume": "main",
    "abstract": "In this paper, we present a novel approach that exploits the information within the descriptor space to propose keypoint locations. Detect then describe, or detect and describe jointly are two typical strategies for extracting local descriptors. In contrast, we propose an approach that inverts this process by first describing and then detecting the keypoint locations. Describe-to-Detect (D2D) leverages successful descriptor models without the need for any additional training. Our method selects keypoints as salient locations with high information content which is defined by the descriptors rather than some independent operators. We perform experiments on multiple benchmarks including image matching, camera localisation, and 3D reconstruction. The results indicate that our method improves the matching performance of various descriptors and that it generalises across methods and tasks",
    "checked": true,
    "id": "d23826a25de6ec30b86fe507261cc11b9c18a5fb",
    "semantic_title": "d2d: keypoint extraction with describe to detect approach",
    "citation_count": 52,
    "authors": [
      "Yurun Tian",
      "Vassileios Balntas",
      "Tony Ng",
      "Axel Barroso-Laguna",
      "Yiannis Demiris",
      "Krystian Mikolajczyk"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Son_SAUM_Symmetry-Aware_Upsampling_Module_for_Consistent_Point_Cloud_Completion_ACCV_2020_paper.html": {
    "title": "SAUM: Symmetry-Aware Upsampling Module for Consistent Point Cloud Completion",
    "volume": "main",
    "abstract": "Point cloud completion estimates the complete shape given incomplete point cloud, which is a crucial task as the raw point cloud measurements suffer from missing data. Most of previous methods for point cloud completion share the encoder-decoder structure, where the encoder projects the raw point cloud into low-dimensional latent space and the decoder decodes the condensed latent information back into the list of points. While the low-dimensional projection extracts semantic features to guide the global completion of the missing data, the unique local geometric details observed from partial data are often lost. In this paper, we propose a shape completion framework that maintains both of the global context and the local characteristics. Our network is composed of two complementary prediction branches. One of the branches fills the unseen parts with the global context learned from the database model, which can be replaced by any of the conventional shape completion network. The other branch, which we refer as a Symmetry-Aware Upsampling Module (SAUM), conservatively maintains the geometric details given the observed partial data, clearly utilizing the symmetry for the shape completion. Experimental results show that the combination of the two prediction branches enables more plausible shape completion for point clouds than the state-of-the-art approaches",
    "checked": true,
    "id": "973b28ed2ea6b1f007d144c7ac42cd25b1e70db2",
    "semantic_title": "saum: symmetry-aware upsampling module for consistent point cloud completion",
    "citation_count": 11,
    "authors": [
      "Hyeontae Son",
      "Young Min Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Xiao_Degradation_Model_Learning_for_Real-World_Single_Image_Super-resolution_ACCV_2020_paper.html": {
    "title": "Degradation Model Learning for Real-World Single Image Super-resolution",
    "volume": "main",
    "abstract": "It is well-known that the single image super-resolution (SISR) models trained on those synthetic datasets, where a low-resolution (LR) image is generated by applying a simple degradation operator (e.g., bicubic downsampling) to its high-resolution (HR) counterpart, have limited generalization capability on real-world LR images, whose degradation process is much more complex. Several real-world SISR datasets have been constructed to reduce this gap; however, their scale is relatively small due to laborious and costly data collection process. To remedy this issue, we propose to learn a realistic degradation model from the existing real-world datasets, and use the learned degradation model to synthesize realistic HR-LR image pairs. Specifically, we learn a group of basis degradation kernels, and simultaneously learn a weight prediction network to predict the pixel-wise spatially variant degradation kernel as the weighted combination of the basis kernels. With the learned degradation model, a large number of realistic HR-LR pairs can be easily generated to train a more robust SISR model. Extensive experiments are performed to quantitatively and qualitatively validate the proposed degradation learning method and its effectiveness in improving the generalization performance of SISR models in practical scenarios",
    "checked": true,
    "id": "e8f6e2568efe75b30fa33caa534758dfdf06b720",
    "semantic_title": "degradation model learning for real-world single image super-resolution",
    "citation_count": 10,
    "authors": [
      "Jin Xiao",
      "Hongwei Yong",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Doi_Descriptor-Free_Multi-View_Region_Matching_for_Instance-Wise_3D_Reconstruction_ACCV_2020_paper.html": {
    "title": "Descriptor-Free Multi-View Region Matching for Instance-Wise 3D Reconstruction",
    "volume": "main",
    "abstract": "This paper proposes a multi-view extension of instance segmentation without relying on texture or shape descriptor matching. Multi-view instance segmentation becomes challenging for scenes with repetitive textures and shapes, e.g., plant leaves, due to the difficulty of multi-view matching using texture or shape descriptors. To this end, we propose a multi-view region matching method based on epipolar geometry, which does not rely on any feature descriptors. We further show that the epipolar region matching can be easily integrated into instance segmentation and effective for instance-wise 3D reconstruction. Experiments demonstrate the improved accuracy of multi-view instance matching and the 3D reconstruction compared to the baseline methods",
    "checked": true,
    "id": "b47edd6f41eb5bcf3e62f220add08acdac7f05b7",
    "semantic_title": "descriptor-free multi-view region matching for instance-wise 3d reconstruction",
    "citation_count": 1,
    "authors": [
      "Takuma Doi",
      "Fumio Okura",
      "Toshiki Nagahara",
      "Yasuyuki Matsushita",
      "Yasushi Yagi"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Chen_SGNet_Semantics_Guided_Deep_Stereo_Matching_ACCV_2020_paper.html": {
    "title": "SGNet: Semantics Guided Deep Stereo Matching",
    "volume": "main",
    "abstract": "Stereovision has been an intensive research area of computer vision. Based on deep learning, stereo matching networks are becoming popular in recent years. Despite of great progress, it's still challenging to achieve high accurate disparity map due to low texture and illumination changes in the scene. High-level semantic information can be helpful to handle these problems. In this paper a deep semantics guided stereo matching network (SGNet) is proposed. Apart from necessary semantic branch, three semantic guided modules are proposed to embed semantic constraints on matching. The joint confidence module produces confidence of cost volume based on the consistency of disparity and semantic features between left and right images. The residual module is responsible for optimizing the initial disparity results according to its semantic categories. Finally, in the loss module, the smooth of disparity is well supervised based on semantic boundary and region. The proposed network has been evaluated on various public datasets like KITTI 2015, KITTI 2012 and Virtual KITTI, and achieves the state-of-the-art performance",
    "checked": true,
    "id": "bae0f086d513d6366ac4f7cbf3c8bbd9467a67d5",
    "semantic_title": "sgnet: semantics guided deep stereo matching",
    "citation_count": 11,
    "authors": [
      "Shuya Chen",
      "Zhiyu Xiang",
      "Chengyu Qiao",
      "Yiman Chen",
      "Tingming Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Zhang_Localize_to_Classify_and_Classify_to_Localize_Mutual_Guidance_in_ACCV_2020_paper.html": {
    "title": "Localize to Classify and Classify to Localize: Mutual Guidance in Object Detection",
    "volume": "main",
    "abstract": "Most deep learning object detectors are based on the anchor mechanism and resort to the Intersection over Union (IoU) between predefined anchor boxes and ground truth boxes to evaluate the matching quality between anchors and objects. In this paper, we question this use of IoU and propose a new anchor matching criterion guided, during the training phase, by the optimization of both the localization and the classification tasks: the predictions related to one task are used to dynamically assign sample anchors and improve the model on the other task, and vice versa. Despite the simplicity of the proposed method, our experiments with different state-of-the-art deep learning architectures on PASCAL VOC and MS COCO datasets demonstrate the effectiveness and generality of our Mutual Guidance strategy",
    "checked": true,
    "id": "51353003eff2f93e820bcbe9fab33c5ad596fa31",
    "semantic_title": "localize to classify and classify to localize: mutual guidance in object detection",
    "citation_count": 13,
    "authors": [
      "Heng Zhang",
      "Elisa Fromont",
      "Sebastien Lefevre",
      "Bruno Avignon"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Kim_Multi-task_Learning_with_Future_States_for_Vision-based_Autonomous_Driving_ACCV_2020_paper.html": {
    "title": "Multi-task Learning with Future States for Vision-based Autonomous Driving",
    "volume": "main",
    "abstract": "Human drivers consider past and future driving environments to maintain stable control of a vehicle. To adopt a human driver's behavior, we propose a vision-based autonomous driving model, called Future Actions and States Network (FASNet), which uses predicted future actions and generated future states in multi-task learning manner. Future states are generated using an enhanced deep predictive-coding network and motion equations dened by the kinematic vehicle model. The nal control values are determined by the weighted average of thepredicted actions for a stable decision. With these methods, the proposed FASNet has a high generalization ability in unseen environments. To validate the proposed FASNet, we conducted several experiments, including ablation studies in realistic three-dimensional simulations. FASNet achieves a higher Success Rate (SR) on the recent CARLA benchmarks under several conditions as compared to state-of-the-art models",
    "checked": true,
    "id": "dacdb23af71010301bf7ff06a86cc6ebb1a04952",
    "semantic_title": "multi-task learning with future states for vision-based autonomous driving",
    "citation_count": 9,
    "authors": [
      "Inhan Kim",
      "Hyemin Lee",
      "Joonyeong Lee",
      "Eunseop Lee",
      "Daijin Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Wu_Graph-based_Heuristic_Search_for_Module_Selection_Procedure_in_Neural_Module_ACCV_2020_paper.html": {
    "title": "Graph-based Heuristic Search for Module Selection Procedure in Neural Module Network",
    "volume": "main",
    "abstract": "Neural Module Network (NMN) is a machine learning model for solving the visual question answering tasks. NMN uses programs to encode modules' structures, and its modularized architecture enables it to solve logical problems more reasonably. However, because of the non-differentiable procedure of module selection, NMN is hard to be trained end-to-end. To overcome this problem, existing work either included ground-truth program into training data or applied reinforcement learning to explore the program. However, both of these methods still have weaknesses. In consideration of this, we proposed a new learning framework for NMN. Graph-based Heuristic Search is the algorithm we proposed to discover the optimal program through a heuristic search on the data structure named Program Graph. Our experiments on FigureQA and CLEVR dataset show that our methods can realize the training of NMN without ground-truth programs and achieve superior program exploring efficiency compared to existing reinforcement learning methods",
    "checked": true,
    "id": "9d4c70649151c2fdc77f31e6aa898b1e8b8bb942",
    "semantic_title": "graph-based heuristic search for module selection procedure in neural module network",
    "citation_count": 1,
    "authors": [
      "Yuxuan Wu",
      "Hideki Nakayama"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Zhu_Visually_Guided_Sound_Source_Separation_using_Cascaded_Opponent_Filter_Network_ACCV_2020_paper.html": {
    "title": "Visually Guided Sound Source Separation using Cascaded Opponent Filter Network",
    "volume": "main",
    "abstract": "The objective of this paper is to recover the original component signals from a mixture audio with the aid of visual cues of the sound sources. Such task is usually referred as visually guided sound source separation. The proposed Cascaded Opponent Filter (COF) framework consists of multiple stages, which recursively refine the source separation. A key element in COF is a novel opponent filter module that identifies and relocates residual components between sources. The system is guided by the appearance and motion of the source, and, for this purpose, we study different representations based on video frames, optical flows, dynamic images, and their combinations. Finally, we propose a Sound Source Location Masking (SSLM) technique, which, together with COF, produces a pixel level mask of the source location. The entire system is trained in an end-to-end manner using a large set of unlabelled videos. We compare COF with recent baselines and obtain the state-of-the-art performance in three challenging datasets (MUSIC, A-MUSIC, and A-NATURAL)",
    "checked": true,
    "id": "40b1de39e75c053ece5a90d59e5ebd3b5917f23f",
    "semantic_title": "visually guided sound source separation using cascaded opponent filter network",
    "citation_count": 21,
    "authors": [
      "Lingyu Zhu",
      "Esa Rahtu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Tang_CLASS_Cross-Level_Attention_and_Supervision_for_Salient_Objects_Detection_ACCV_2020_paper.html": {
    "title": "CLASS: Cross-Level Attention and Supervision for Salient Objects Detection",
    "volume": "main",
    "abstract": "Salient object detection (SOD) is a fundamental computer vision task. Recently, with the revival of deep neural networks, SOD has made great progresses. However, there still exist two thorny issues that cannot be well addressed by existing methods, indistinguishable regions and complex structures. To address these two issues, in this paper we propose a novel deep network for accurate SOD, named CLASS. First, in order to leverage the different advantages of low-level and high-level features, we propose a novel non-local cross-level attention (CLA), which can capture the long-range feature dependencies to enhance the distinction of complete salient object. Second, a novel cross-level supervision (CLS) is designed to learn complementary context for complex structures through pixel-level, region-level and object-level. Then the fine structures and boundaries of salient objects can be well restored. In experiments, with the proposed CLA and CLS, our CLASS net consistently outperforms 13 state-of-the-art methods on five datasets",
    "checked": true,
    "id": "dc2b263c4d65972095aadc58589d5209ae97e59a",
    "semantic_title": "class: cross-level attention and supervision for salient objects detection",
    "citation_count": 17,
    "authors": [
      "Lv Tang",
      "Bo Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Bi_Scale-Aware_Polar_Representation_for_Arbitrarily-Shaped_Text_Detection_ACCV_2020_paper.html": {
    "title": "Scale-Aware Polar Representation for Arbitrarily-Shaped Text Detection",
    "volume": "main",
    "abstract": "Arbitrarily-shaped text detection faces two major challenges: 1) various scales and 2) irregular angles. Previous works regress the text boundary in Cartesian coordinates as ordinary object detection. However, such grid space interleaves the unique scale and angle attributes of text, which seriously affects detection performance. The implicit disregard of text scale also impairs multi-scale detection ability. To better learn the arbitrary text boundary and handle the text scale variation, we propose a novel Scale-Aware Polar Representation (SAPR) framework. The text boundary is represented in Polar coordinates, where scale and angle of text could be both clearly expressed for targeted learning. This simple but effective transformation brings significant performance improvement. The explicit learning on separated text scale also promotes the multi-scale detection ability. Based on the Polar representation, we design line IoU loss and symmetry sine loss to better optimize the scale and angle of text with a multi-path decoder architecture. Furthermore, an accurate center line calculation is proposed to guide text boundary restoration under various scales. Overall, the proposed SAPR framework is able to effectively detect arbitrarily-shaped texts and tackle the scale variation simultaneously. The state-of-the-art results on multiple benchmarks solidly demonstrate the effectiveness and superiority of SAPR",
    "checked": true,
    "id": "170b172c0dbfe6a04724da11e20a1f485913119a",
    "semantic_title": "scale-aware polar representation for arbitrarily-shaped text detection",
    "citation_count": 1,
    "authors": [
      "Yanguang Bi",
      "Zhiqiang Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Zhang_Unified_Density-Aware_Image_Dehazing_and_Object_Detection_in_Real-World_Hazy_ACCV_2020_paper.html": {
    "title": "Unified Density-Aware Image Dehazing and Object Detection in Real-World Hazy Scenes",
    "volume": "main",
    "abstract": "It is an important yet challenging task to detect objects on hazy images in real-world applications. The major challenge comes from low visual quality and large haze density variations. In this work, we aim to jointly solve the image dehazing and the object detection tasks in real hazy scenarios by using haze density as prior knowledge. Our proposed Unified Dehazing and Detection (UDnD) framework consists of three parts: a residual-aware haze density classifier, a density-aware dehazing network, and a density-aware object detector. First, the classifier exploits the residuals of hazy images to accurately predict density levels, which provide rich domain knowledge for the subsequent two tasks. Then, we design respectively a High-Resolution Dehazing Network (HRDN) and a Faster R-CNN-based multi-domain object detector to leverage the extracted density information and tackle hazy object detection. Experiments demonstrate that UDnD performs favorably against other methods for object detection in real-world hazy scenes. Also, HRDN achieves better results than state-of-the-art dehazing methods in terms of PSNR and SSIM. Hence, HRDN can conduct haze removal effectively, based on which UDnD is able to provide high-quality detection results",
    "checked": true,
    "id": "c04056fea81fc9e688c40abc50c310359d07152a",
    "semantic_title": "unified density-aware image dehazing and object detection in real-world hazy scenes",
    "citation_count": 15,
    "authors": [
      "Zhengxi Zhang",
      "Liang Zhao",
      "Yunan Liu",
      "Shanshan Zhang",
      "Jian Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Tattersall_Reconstructing_Creative_Lego_Models_ACCV_2020_paper.html": {
    "title": "Reconstructing Creative Lego Models",
    "volume": "main",
    "abstract": "Lego is one of the most successful toys in the world. Being able to scan, analyse and reconstruct Lego models has many applications, for example studying creativity. In this paper, from a set of 2D input images, we create a monolithic mesh, representing a physical 3D Lego model as input, and split it in to its known components such that the output of the program can be used to completely reconstruct the input model, brick for brick. We present a novel, fully automatic pipeline to reconstruct Lego models in 3D from 2D images; A-DBSCAN, an angular variant of DBSCAN, useful for grouping both parallel and anti-parallel vectors; and a method for reducing the problem of non-Manhattan reconstruction to that of Manhattan reconstruction. We evaluate the presented approach both qualitatively and quantitatively on a set of Lego duck models from a public data set, and show that the algorithm is able to identify and reconstruct the Lego models successfully",
    "checked": true,
    "id": "7639f52736377559dff010ff6bf074ab7671b9f4",
    "semantic_title": "reconstructing creative lego models",
    "citation_count": 1,
    "authors": [
      "George Tattersall",
      "Dizhong Zhu",
      "William A. P. Smith",
      "Sebastian Deterding",
      "Patrik Huber"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Guan_Large-Scale_Cross-Domain_Few-Shot_Learning_ACCV_2020_paper.html": {
    "title": "Large-Scale Cross-Domain Few-Shot Learning",
    "volume": "main",
    "abstract": "Learning classifiers for novel classes with a few training examples (shots) in a new domain is a practical problem setting. However, the two problems involved in this setting, few-shot learning (FSL) and domain adaption (DA), have only been studied separately so far. In this paper, for the first time, the problem of large-scale cross-domain few-shot learning is tackled. To overcome the dual challenges of few-shot and domain gap, we propose a novel Triplet Autoencoder (TriAE) model. The model aims to learn a latent subspace where not only transfer learning from the source classes to the novel classes occurs, but also domain alignment takes place. An efficient model optimization algorithm is formulated, followed by rigorous theoretical analysis. Extensive experiments on two large-scale cross-domain datasets show that our TriAE model outperforms the state-of-the-art FSL and domain adaptation models, as well as their naive combinations. Interestingly, under the conventional large-scale FSL setting, our TriAE model also outperforms existing FSL methods by significantly margins, indicating that domain gaps are universally present",
    "checked": true,
    "id": "59089cc90a82c116d3315c2db9c64a51fc9bba3b",
    "semantic_title": "large-scale cross-domain few-shot learning",
    "citation_count": 11,
    "authors": [
      "Jiechao Guan",
      "Manli Zhang",
      "Zhiwu Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Momeni_Watch_read_and_lookup_learning_to_spot_signs_from_multiple_ACCV_2020_paper.html": {
    "title": "Watch, read and lookup: learning to spot signs from multiple supervisors",
    "volume": "main",
    "abstract": "The focus of this work is sign spotting--given a video of an isolated sign, our task is to identify whether and where it has been signed in a continuous, co-articulated sign language video. To achieve this sign spotting task, we train a model using multiple types of available supervision by: (1) watching existing sparsely labelled footage; (2) reading associated subtitles (readily available translations of the signed content) which provide additional weak-supervision; (3) looking up words (for which no co-articulated labelled examples are available) in visual sign language dictionaries to enable novel sign spotting. These three tasks are integrated into a unified learning framework using the principles of Noise Contrastive Estimation and Multiple Instance Learning. We validate the effectiveness of our approach on low-shot sign spotting benchmarks. In addition, we contribute a machine-readable British Sign Language (BSL) dictionary dataset of isolated signs, BSLDict, to facilitate study of this task. The dataset, models and code are available at our project page",
    "checked": true,
    "id": "631bbcce16387e76e4780d7c84b07b2a37d6bfc4",
    "semantic_title": "watch, read and lookup: learning to spot signs from multiple supervisors",
    "citation_count": 28,
    "authors": [
      "Liliane Momeni",
      "Gul Varol",
      "Samuel Albanie",
      "Triantafyllos Afouras",
      "Andrew Zisserman"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Piao_Reweighted_Non-convex_Non-smooth_Rank_Minimization_based_Spectral_Clustering_on_Grassmann_ACCV_2020_paper.html": {
    "title": "Reweighted Non-convex Non-smooth Rank Minimization based Spectral Clustering on Grassmann Manifold",
    "volume": "main",
    "abstract": "Low Rank Representation (LRR) based unsupervised clustering methods have achieved great success since these methods could explore low-dimensional subspace structure embedded in original data effectively. The conventional LRR methods generally treat the data as the points in Euclidean space. However, it is no longer suitable for high-dimension data (such as video or imageset). That is because high-dimension data are always considered as non-linear manifold data such as Grassmann manifold. Besides, the typical LRR methods always adopt the traditional single nuclear norm based low rank constraint which can not fully reveal the low rank property of the data representation and often leads to suboptimal solution. In this paper, a new LRR based clustering model is constructed on Grassmann manifold for high-dimension data. In the proposed method, each high-dimension data is formed as a sample on Grassmann manifold with non-linear metric. Meanwhile, a non-convex low rank representation is adopt to reveal the intrinsic property of these high-dimension data and reweighted rank minimization constraint is introduced. The experimental results on several public datasets show that the proposed method outperforms the state-of-the-art clustering methods",
    "checked": true,
    "id": "9516f768adb4e65c24f419c907fa8d571ad1f979",
    "semantic_title": "reweighted non-convex non-smooth rank minimization based spectral clustering on grassmann manifold",
    "citation_count": 1,
    "authors": [
      "Xinglin Piao",
      "Yongli Hu",
      "Junbin Gao",
      "Yanfeng Sun",
      "Xin Yang",
      "Baocai Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Kwon_Recursive_Bayesian_Filtering_for_Multiple_Human_Pose_Tracking_from_Multiple_ACCV_2020_paper.html": {
    "title": "Recursive Bayesian Filtering for Multiple Human Pose Tracking from Multiple Cameras",
    "volume": "main",
    "abstract": "Markerless motion capture allows the extraction of multiple 3D human poses from natural scenes, without the need for a controlled but artificial studio environment or expensive hardware. In this work we present a novel tracking algorithm which utilizes recent advancements in 2D human pose estimation as well as 3D human motion anticipation. During the prediction step we utilize an RNN to forecast a set of plausible future poses while we utilize a 2D multiple human pose estimation model during the update step to incorporate observations. Casting the problem of estimating multiple persons from multiple cameras as a tracking problem rather than an association problem results in a linear relationship between runtime and the number of tracked persons. Furthermore, tracking enables our method to overcome temporary occlusions by relying on the prediction model. Our approach achieves state-of-the-art results on popular benchmarks for 3D human pose estimation and tracking",
    "checked": true,
    "id": "7143e1a62345f486b9649df814d7387d6189aaae",
    "semantic_title": "recursive bayesian filtering for multiple human pose tracking from multiple cameras",
    "citation_count": 7,
    "authors": [
      "Oh-Hun Kwon",
      "Julian Tanke",
      "Juergen Gall"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Fei_Few-Shot_Zero-Shot_Learning_Knowledge_Transfer_with_Less_Supervision_ACCV_2020_paper.html": {
    "title": "Few-Shot Zero-Shot Learning: Knowledge Transfer with Less Supervision",
    "volume": "main",
    "abstract": "Existing zero-shot learning (ZSL) methods assume that there exist sufficient training samples from seen classes, each annotated with semantic descriptors such as attributes, for knowledge transfer to unseen classes without any training samples. However, this assumption is often invalid because collecting sufficient seen class samples can be difficult and attribute annotation is expensive; it thus severely limits the scalability of ZSL. In this paper, we define a new setting termed Few-Shot Zero-Shot Learning (FSZSL), where only a few annotated images are collected from each seen class (i.e., few-shot). This is clearly more challenging yet more realistic than the conventional ZSL setting. To overcome the resultant image-level attribute sparsity, we propose a novel inductive ZSL model termed sparse attribute propagation (SAP) by propagating attribute annotations to more unannotated images using sparse coding. This is followed by learning bidirectional projections between features and attributes for ZSL. An efficient solver is provided for such knowledge transfer with less supervision, together with rigorous theoretic analysis. With our SAP, we show that a ZSL training dataset can also be augmented by the abundant web images returned by image search engine, to further improve the model performance. Extensive experiments show that the proposed model achieves state-of-the-art results",
    "checked": true,
    "id": "723ef11269253c3be978fbb4f4adaf46ef60099a",
    "semantic_title": "few-shot zero-shot learning: knowledge transfer with less supervision",
    "citation_count": 3,
    "authors": [
      "Nanyi Fei",
      "Jiechao Guan",
      "Zhiwu Lu",
      "Yizhao Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Ranjan_Uncertainty_Estimation_and_Sample_Selection_for_Crowd_Counting_ACCV_2020_paper.html": {
    "title": "Uncertainty Estimation and Sample Selection for Crowd Counting",
    "volume": "main",
    "abstract": "We present a method for image-based crowd counting, one that can predict a crowd density map together with the uncertainty values pertaining to the predicted density map. To obtain prediction uncertainty, we model the crowd density values using Gaussian distributions and develop a convolutional neural network architecture to predict these distributions. A key advantage of our method over existing crowd counting methods is its ability to quantify the uncertainty of its predictions. We illustrate the benefits of knowing the prediction uncertainty by developing a method to reduce the human annotation effort needed to adapt counting networks to a new domain. We present sample selection strategies which make use of the density and uncertainty of predictions from the networks trained on one domain to select the informative images from a target domain of interest to acquire human annotation. We show that our sample selection strategy drastically reduces the amount of labeled data from the target domain needed to adapt a counting network trained on a source domain to the target domain. Empirically, the networks trained on the UCF-QNRF dataset can be adapted to surpass the performance of the previous state-of-the-art results on NWPU dataset and Shanghaitech dataset using only 17% of the labeled training samples from the target domain. Github Page: https://github.com/cvlab-stonybrook/UncertaintyCrowdCounting",
    "checked": true,
    "id": "eba746c064f95e843a28fbd43717412ea341a506",
    "semantic_title": "uncertainty estimation and sample selection for crowd counting",
    "citation_count": 22,
    "authors": [
      "Viresh Ranjan",
      "Boyu Wang",
      "Mubarak Shah",
      "Minh Hoai"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Cicek_Spatial_Class_Distribution_Shift_in_Unsupervised_Domain_Adaptation_Local_Alignment_ACCV_2020_paper.html": {
    "title": "Spatial Class Distribution Shift in Unsupervised Domain Adaptation: Local Alignment Comes to Rescue",
    "volume": "main",
    "abstract": "We propose a method for semantic segmentation in unsupervised domain adaptation (UDA) setting. We particularly examine the domain gap between spatial-class distributions and propose to align the local distributions of the segmentation predictions. Despite its simplicity, the proposed method achieves state-of-the-art results in UDA segmentation benchmarks",
    "checked": true,
    "id": "810d98e061694c113ca44d490364dd04854db564",
    "semantic_title": "spatial class distribution shift in unsupervised domain adaptation: local alignment comes to rescue",
    "citation_count": 1,
    "authors": [
      "Safa Cicek",
      "Ning Xu",
      "Zhaowen Wang",
      "Hailin Jin",
      "Stefano Soatto"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Yan_SDCNet_Size_Divide_and_Conquer_Network_for_Salient_Object_Detection_ACCV_2020_paper.html": {
    "title": "SDCNet: Size Divide and Conquer Network for Salient Object Detection",
    "volume": "main",
    "abstract": "The fully convolutional neural network (FCN) based methods achieve great performances in salient object detection (SOD). However, most existing methods have difficulty in detecting small or large objects. To solve this problem, we propose Size Divide and Conquer Network (SDCNet) which learning the features of salient objects of different sizes separately for better detection. Specifically, SDCNet contains two main aspects: (1)We calculate the proportion of objects in the image (with the ground truth of pixel-level) and train a size inference module to predict the size of salient objects. (2) We novelly propose a Multi-channel Size Divide Module (MSDM) to learning the features of salient objects with different sizes, respectively. In detail, we employ MSDM following each block of the backbone network and use different channels to extract features of salient objects within different size range at various resolutions. Unlike coupling additional features, we encode the network based on the idea of divide and conquer for different data distributions, and learn the features of salient objects of different sizes specifically. The experimental results show that SDCNet outperforms 14 state-of-the-art methods on five benchmark datasets without using other auxiliary techniques",
    "checked": true,
    "id": "50f6757a0e93af16e6129b83a9635a00a4723234",
    "semantic_title": "sdcnet: size divide and conquer network for salient object detection",
    "citation_count": 2,
    "authors": [
      "Senbo Yan",
      "Xiaowen Song",
      "Chuer Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Rahman_Any-Shot_Object_Detection_ACCV_2020_paper.html": {
    "title": "Any-Shot Object Detection",
    "volume": "main",
    "abstract": "Previous work on novel object detection considers zero or few-shot settings where none or few examples of each category are available for training. In real world scenarios, it is less practical to expect that 'all' the novel classes are either unseen or have few-examples. Here, we propose a more realistic setting termed 'Any-shot detection', where totally unseen and few-shot categories can simultaneously co-occur during inference. Any-shot detection offers unique challenges compared to conventional novel object detection such as, a high imbalance between unseen, few-shot and seen object classes, susceptibility to forget base-training while learning novel classes and distinguishing novel classes from the background. To address these challenges, we propose a unified any-shot detection model, that can concurrently learn to detect both zero-shot and few-shot object classes. Our core idea is to use class semantics as prototypes for object detection, a formulation that naturally minimizes knowledge forgetting and mitigates the class-imbalance in the label space. Besides, we propose a rebalanced loss function that emphasizes difficult few-shot cases but avoids overfitting on the novel classes to allow detection of totally unseen classes. Without bells and whistles, our framework can also be used solely for Zero-shot object detection and Few-shot object detection tasks. We report extensive experiments on Pascal VOC and MS-COCO datasets where our approach is shown to provide significant improvements",
    "checked": true,
    "id": "e63018f73788852fc92812660be9f88c2aa17dc6",
    "semantic_title": "any-shot object detection",
    "citation_count": 14,
    "authors": [
      "Shafin Rahman",
      "Salman Khan",
      "Nick Barnes",
      "Fahad Shahbaz Khan"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Boulch_FKAConv_Feature-Kernel_Alignment_for_Point_Cloud_Convolution_ACCV_2020_paper.html": {
    "title": "FKAConv: Feature-Kernel Alignment for Point Cloud Convolution",
    "volume": "main",
    "abstract": "Recent state-of-the-art methods for point cloud semantic segmentation are based on convolution defined for point clouds The interest goes beyond semantic segmentation. We propose a formulation of the convolution for point cloud directly inspired by the discrete convolution in image processing. The resulting formulation underlines the separation between the discrete kernel space and the geometric space where the points lies. Several existing methods fall under this formulation.The two spaces are linked with a space change matrix A, estimated with a neural network. A softly assigns the input features on the convolution kernel. Finally, we show competitive results on several semantic segmentation benchmarks while being efficient both in computation time and memory",
    "checked": true,
    "id": "4355fe5189cc24fe20e8ae608c94e1d36b7f6b5f",
    "semantic_title": "fkaconv: feature-kernel alignment for point cloud convolution",
    "citation_count": 52,
    "authors": [
      "Alexandre Boulch",
      "Gilles Puy",
      "Renaud Marlet"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Koksal_RF-GAN_A_Light_and_Reconfigurable_Network_for_Unpaired_Image-to-Image_Translation_ACCV_2020_paper.html": {
    "title": "RF-GAN: A Light and Reconfigurable Network for Unpaired Image-to-Image Translation",
    "volume": "main",
    "abstract": "Generative adversarial networks (GANs) have been widely studied for unpaired image-to-image translation in recent years. On the other hand, state-of-the-art translation GANs are often constrained by large model sizes and inflexibility in translating across various domains. Inspired by the observation that the mappings between two domains are often approximately invertible, we design an innovative reconfigurable GAN (RF-GAN) that has a small size but is versatile in high-fidelity image translation either across two domains or among multiple domains. One unique feature of RF-GAN lies with its single generator which is reconfigurable and can perform bidirectional image translations by swapping its parameters. In addition, a multi-domain discriminator is designed which allows joint discrimination of original and translated samples in multiple domains. Experiments over eight unpaired image translation datasets (on various tasks such as object transfiguration, season transfer, and painters' style transfer, etc.) show that RF-GAN reduces the model size by up to 75% as compared with state-of-the-art translation GANs but produces superior image translation performance with lower Frechet Inception Distance consistently",
    "checked": true,
    "id": "3b1f1a6dc14038d580974f9d96b1fbf7eb683094",
    "semantic_title": "rf-gan: a light and reconfigurable network for unpaired image-to-image translation",
    "citation_count": 5,
    "authors": [
      "Ali Koksal",
      "Shijian Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Stoiber_A_Sparse_Gaussian_Approach_to_Region-Based_6DoF_Object_Tracking_ACCV_2020_paper.html": {
    "title": "A Sparse Gaussian Approach to Region-Based 6DoF Object Tracking",
    "volume": "main",
    "abstract": "We propose a novel, highly efficient sparse approach to region-based 6DoF object tracking that requires only a monocular RGB camera and the 3D object model. The key contribution of our work is a probabilistic model that considers image information sparsely along correspondence lines. For the implementation, we provide a highly efficient discrete scale-space formulation. In addition, we derive a novel mathematical proof that shows that our proposed likelihood function follows a Gaussian distribution. Based on this information, we develop robust approximations for the derivatives of the log-likelihood that are used in a regularized Newton optimization. In multiple experiments, we show that our approach outperforms state-of-the-art region-based methods in terms of tracking success while being about one order of magnitude faster. The source code of our tracker is publicly available",
    "checked": true,
    "id": "129708061de2946a030569eb9c0d50b7026e41ed",
    "semantic_title": "a sparse gaussian approach to region-based 6dof object tracking",
    "citation_count": 19,
    "authors": [
      "Manuel Stoiber",
      "Martin Pfanne",
      "Klaus H. Strobl",
      "Rudolph Triebel",
      "Alin Albu-Schaeffer"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Huang_Show_Conceive_and_Tell_Image_Captioning_with_Prospective_Linguistic_Information_ACCV_2020_paper.html": {
    "title": "Show, Conceive and Tell: Image Captioning with Prospective Linguistic Information",
    "volume": "main",
    "abstract": "Attention based encoder-decoder models have achieved competitive performances in image captioning. However, these models usually follow the auto-regressive way during inference, meaning that only the previously generated words, namely the explored linguistic information, can be utilized for caption generation.Intuitively, enabling the model to conceive the prospective linguistic information contained in the words to be generated can be beneficial for further improving the captioning results. Consequently, we devise a novel Prospective information guided LSTM (Pro-LSTM) model, to exploit both prospective and explored information to boost captioning. For each image, we first draft a coarse caption which roughly describes the whole image contents. At each time step, we mine the prospective and explored information from the coarse caption. These two kinds of information are further utilized by a Prospective information guided Attention (ProA) module to guide our model to comprehensively utilize the visual feature from a semantically global perspective. We also propose an Attentive Attribute Detector (AAD) which refines the object features to predict the image attributes more precisely. This further improves the semantic quality of the generated caption. Thanks to the prospective information and more accurate attributes, the Pro-LSTM model achieves state-of-the-art performances on the MSCOCO dataset with a 129.5 CIDEr-D",
    "checked": true,
    "id": "482c81f15f126a036c8e01cffaf9f1bee2d595fc",
    "semantic_title": "show, conceive and tell: image captioning with prospective linguistic information",
    "citation_count": 0,
    "authors": [
      "Yiqing Huang",
      "Jiansheng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/He_Feature_Variance_Ratio-Guided_Channel_Pruning_for_Deep_Convolutional_Network_Acceleration_ACCV_2020_paper.html": {
    "title": "Feature Variance Ratio-Guided Channel Pruning for Deep Convolutional Network Acceleration",
    "volume": "main",
    "abstract": "Most existing channel pruning approaches utilize the magnitude of network parameters to guide the pruning process. However, these methods suffer from some limitations in modern networks, where the magnitude of parameters can vary independently of the importance of corresponding channels. To recognize redundancies more accurately and therefore, accelerate networks better, we propose a novel channel pruning criterion based on the Pearson correlation coefficient. The criterion preserves the features that are essentially informative to the given task and avoids the influence of useless parameter scales. Based on this criterion, we further establish our channel pruning framework named Feature Variance Ratio-guided Channel Pruning (FVRCP). FVRCP prunes channels globally with little human intervention. Moreover, it can automatically find important layers in the network. Extensive numerical experiments on CIFAR-10 and ImageNet with widely varying architectures present state-of-the-art performance of our method",
    "checked": true,
    "id": "9d6259b996a91b3d1d1fe929d31adc272601fbc9",
    "semantic_title": "feature variance ratio-guided channel pruning for deep convolutional network acceleration",
    "citation_count": 0,
    "authors": [
      "Junjie He",
      "Bohua Chen",
      "Yinzhang Ding",
      "Dongxiao Li"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Mykheievskyi_Learning_Local_Feature_Descriptors_for_Multiple_Object_Tracking_ACCV_2020_paper.html": {
    "title": "Learning Local Feature Descriptors for Multiple Object Tracking",
    "volume": "main",
    "abstract": "The present study aims at learning class-agnostic embedding, which is suitable for Multiple Object Tracking (MOT). We demonstrate that the learning of local feature descriptors could provide a sufficient level of generalization. Proposed embedding function exhibits on-par performance with its dedicated person re-identification counterparts in their target domain and outperforms them in others. Through its utilization, our solutions achieve state-of-the-art performance in a number of MOT benchmarks, which includes CVPR'19 Tracking Challenge",
    "checked": true,
    "id": "045189660c237eeeb24975cc8643e1f65babed2d",
    "semantic_title": "learning local feature descriptors for multiple object tracking",
    "citation_count": 16,
    "authors": [
      "Dmytro Mykheievskyi",
      "Dmytro Borysenko",
      "Viktor Porokhonskyy"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Tsou_Multi-Task_Learning_for_Simultaneous_Video_Generation_and_Remote_Photoplethysmography_Estimation_ACCV_2020_paper.html": {
    "title": "Multi-Task Learning for Simultaneous Video Generation and Remote Photoplethysmography Estimation",
    "volume": "main",
    "abstract": "Remote photoplethysmography (rPPG) is a contactless method for estimating physiological signals from facial videos. Without large supervised datasets, learning a robust rPPG estimation model is extremely challenging. Instead of merely focusing on model learning, we believe data augmentation may be of greater importance for this task. In this paper, we propose a novel multi-task learning framework to simultaneously augment training data while learning the rPPG estimation model. We design three joint-learning networks: rPPG estimation network, Image-to-Video network, and Video-to-Video network, to estimate rPPG signals from face videos, to generate synthetic videos from a source image and a specified rPPG signal, and to generate synthetic videos from a source video and a specified rPPG signal, respectively. Experimental results on three benchmark datasets, COHFACE, UBFC, and PURE, show that our method successfully generates photo-realistic videos and significantly outperforms existing methods with a large margin",
    "checked": true,
    "id": "f25a230f47027f2114c22c3a4e721501929b8a9f",
    "semantic_title": "multi-task learning for simultaneous video generation and remote photoplethysmography estimation",
    "citation_count": 11,
    "authors": [
      "Yun-Yun Tsou",
      "Yi-An Lee",
      "Chiou-Ting Hsu"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Agarwal_Explaining_image_classifiers_by_removing_input_features_using_generative_models_ACCV_2020_paper.html": {
    "title": "Explaining image classifiers by removing input features using generative models",
    "volume": "main",
    "abstract": "Perturbation-based explanation methods often measure the contribution of an input feature to an image classifier's outputs by heuristically removing it via e.g. blurring, adding noise, or graying out, which often produce unrealistic, out-of-samples. Instead, we propose to integrate a generative inpainter into three representative attribution methods to remove an input feature. Our proposed change improved all three methods in (1) generating more plausible counterfactual samples under the true data distribution; (2) being more accurate according to three metrics: object localization, deletion, and saliency metrics; and (3) being more robust to hyperparameter changes. Our findings were consistent across both ImageNet and Places365 datasets and two different pairs of classifiers and inpainters",
    "checked": true,
    "id": "fe708d9340ff0d108ba8009d2522ce0a312bca53",
    "semantic_title": "explaining image classifiers by removing input features using generative models",
    "citation_count": 33,
    "authors": [
      "Chirag Agarwal",
      "Anh Nguyen"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Xiang_Part-aware_Attention_Network_for_Person_Re-Identification_ACCV_2020_paper.html": {
    "title": "Part-aware Attention Network for Person Re-Identification",
    "volume": "main",
    "abstract": "Multi-level feature aggregation and part feature extraction are widely used to boost the performance of person re-identification (Re-ID). Most multi-level feature aggregation methods treat feature maps on different levels equally and use simple local operations for feature fusion, which neglects the long-distance connection among feature maps. On the other hand, the popular horizon pooling part based feature extraction methods may lead to feature misalignment. In this paper, we propose a novel Part-aware Attention Network (PAN) to connect part feature maps and middle-level features. Given a part feature map and a source feature map, PAN uses part features as queries to perform second-order information propagation from the source feature map. The attention is computed based on the compatibility of the source feature map with the part feature map. Specifically, PAN uses high-level part features of different human body parts to aggregate information from mid-level feature maps. As a part-aware feature aggregation method, PAN operates on all spatial positions of feature maps so that it can discover long-distance relations. Extensive experiments show that PAN achieves leading performance on Re-ID benchmarks Market1501, DukeMTMC, and CUHK03",
    "checked": true,
    "id": "4c56567a6678826ca678d78fa2e21282af3e65c3",
    "semantic_title": "part-aware attention network for person re-identification",
    "citation_count": 5,
    "authors": [
      "Wangmeng Xiang",
      "Jianqiang Huang",
      "Xian-Sheng Hua",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Zhan_Adversarial_Image_Composition_with_Auxiliary_Illumination_ACCV_2020_paper.html": {
    "title": "Adversarial Image Composition with Auxiliary Illumination",
    "volume": "main",
    "abstract": "Dealing with the inconsistency between a foreground object and a background image is a challenging task in high-fidelity image composition. State-of-the-art methods strive to harmonize the composed image by adapting the style of foreground objects to be compatible with the background image, whereas the potential shadow of foreground objects within the composed image which is critical to the composition realism is largely neglected. In this paper, we propose an Adversarial Image Composition Net (AIC-Net) that achieves realistic image composition by considering potential shadows that the foreground object projects in the composed image. A novel branched generation mechanism is proposed, which disentangles the generation of shadows and the transfer of foreground styles for optimal accomplishment of the two tasks simultaneously. A differentiable spatial transformation module is designed which bridges the local harmonization and the global harmonization to achieve their joint optimization effectively. Extensive experiments on pedestrian and car composition tasks show that the proposed AIC-Net achieves superior composition performance qualitatively and quantitatively",
    "checked": true,
    "id": "01df4e78091e3eddab1ba65e96d63bfbf6096ecf",
    "semantic_title": "adversarial image composition with auxiliary illumination",
    "citation_count": 31,
    "authors": [
      "Fangneng Zhan",
      "Shijian Lu",
      "Changgong Zhang",
      "Feiying Ma",
      "Xuansong Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Xu_Adaptive_Spatio-Temporal_Regularized_Correlation_Filters_for_UAV-based_Tracking_ACCV_2020_paper.html": {
    "title": "Adaptive Spatio-Temporal Regularized Correlation Filters for UAV-based Tracking",
    "volume": "main",
    "abstract": "The advance of visual tracking has provided unmanned aerial vehicle (UAV) with the intriguing capability for various practical applications. With promising performance and efficiency, discriminative correlation filter (DCF)-based trackers have drawn great attention and undergone remarkable progress. However, the boundary effect and filter degradation remain two challenging problems. In this work, we propose a novel Adaptive Spatio-Temporal Regularized Correlation Filter (ASTR-CF) model to address these two problems. The ASTR-CF can optimize the spatial regularization weight and the temporal regularization weight simultaneously. Meanwhile, the proposed model can be effectively optimized based on the alternating direction method of multipliers (ADMM), where each subproblem has a closed-form solution. Experimental results on DTB70 and UAV123@10fps benchmarks have proven the superiority of our method compared to the state-of-the-art trackers in terms of both accuracy and computational speed",
    "checked": true,
    "id": "6ce66e24431e02b6f2b91584aebeed382faf6c1b",
    "semantic_title": "adaptive spatio-temporal regularized correlation filters for uav-based tracking",
    "citation_count": 0,
    "authors": [
      "Libin Xu",
      "Qilei Li",
      "Jun Jiang",
      "Guofeng Zou",
      "Zheng Liu",
      "Mingliang Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Krishna_Adaptive_Spotting_Deep_Reinforcement_Object_Search_in_3D_Point_Clouds_ACCV_2020_paper.html": {
    "title": "Adaptive Spotting: Deep Reinforcement Object Search in 3D Point Clouds",
    "volume": "main",
    "abstract": "In this paper, we study the task of searching for a query object of unknown position and pose in a scene, both given in the form of 3D point cloud data. A straightforward approach that exhaustively scans the scene is often prohibitive due to computational inefficiencies. High-quality feature representation also needs to be learned to achieve accurate recognition and localization. Aiming to address these two fundamental problems in a unified framework, we propose Adaptive Spotting, a deep reinforcement learning approach that jointly learns both the features and the efficient search path. Our network is designed to directly take raw point cloud data of the query object and the search bounding box and to sequentially determine the next pose to be searched. This network is successfully trained in an end-to-end manner by integrating a contrastive loss and a reinforcement localization reward. Evaluations on ModelNet40 and Stanford 2D-3D-S datasets demonstrate the superiority of the proposed approach over several state-of-the-art baselines",
    "checked": true,
    "id": "3109b16e38810a8d4f055e360d8c78a4fcc6f4ed",
    "semantic_title": "adaptive spotting: deep reinforcement object search in 3d point clouds",
    "citation_count": 4,
    "authors": [
      "Onkar Krishna",
      "Go Irie",
      "Xiaomeng Wu",
      "Takahito Kawanishi",
      "Kunio Kashino"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Ji_Backbone_Based_Feature_Enhancement_for_Object_Detection_ACCV_2020_paper.html": {
    "title": "Backbone Based Feature Enhancement for Object Detection",
    "volume": "main",
    "abstract": "FPN (Feature Pyramid Networks) and many of its variants have been widely used in state of the art object detectors and made remarkable progress in detection performance. However, almost all the architectures of feature pyramid are manually designed, which requires ad hoc design and prior knowledge. Meanwhile, existing methods focus on exploring more appropriate connections to generate features with strong semantics features from inherent pyramidal hierarchy of deep ConvNets (Convolutional Networks). In this paper, we propose a simple but effective approach, named BBFE (Backbone Based Feature Enhancement), to directly enhance the semantics of shallow features from backbone ConvNets. The proposed BBFE consists of two components: reusing backbone weight and personalized feature enhancement. We also proposed a fast version of BBFE, named Fast-BBFE, to achieve better trade-off between efficiency and accuracy. Without bells and whistles, our BBFE improves different baseline methods (both anchor-based and anchor-free) by a large margin ( 2.0 points higher AP) on COCO, surpassing common feature pyramid networks including FPN and PANet",
    "checked": true,
    "id": "8132355e7f09f4bff25ccea511078d071e9be24b",
    "semantic_title": "backbone based feature enhancement for object detection",
    "citation_count": 2,
    "authors": [
      "Haoqin Ji",
      "Weizeng Lu",
      "Linlin Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/ACCV2020/html/Perrett_Meta-Learning_with_Context-Agnostic_Initialisations_ACCV_2020_paper.html": {
    "title": "Meta-Learning with Context-Agnostic Initialisations",
    "volume": "main",
    "abstract": "Meta-learning approaches have addressed few-shot problems by finding initialisations suited for fine-tuning to target tasks. Often there are additional properties within training data (which we refer to as context), not relevant to the target task, which act as a distractor to meta-learning, particularly when the target task contains examples from a novel context not seen during training. We address this oversight by incorporating a context-adversarial component into the meta-learning process. This produces an initialisation which is both context-agnostic and task-generalised. We evaluate our approach on three commonly used meta-learning algorithms and four case studies. We demonstrate our context-agnostic meta-learning improves results in each case. First, we report few-shot character classification on the Omniglot dataset, using alphabets as context. An average improvement of 4.3% is observed across methods and tasks when classifying characters from an unseen alphabet. Second, we perform few-shot classification on Mini-ImageNet, obtaining context from the label hierarchy, with an average improvement of 2.8%. Third, we perform few-shot classification on CUB, with annotation metadata as context, and demonstrate an average improvement of 1.9%. Fourth, we evaluate on a dataset for personalised energy expenditure predictions from video, using participant knowledge as context. We demonstrate that context-agnostic meta-learning decreases the average mean square error by 30%",
    "checked": true,
    "id": "2eb62be675751ed2253434ffd01fca02af35e32e",
    "semantic_title": "meta-learning with context-agnostic initialisations",
    "citation_count": 2,
    "authors": [
      "Toby Perrett",
      "Alessandro Masullo",
      "Tilo Burghardt",
      "Majid Mirmehdi",
      "Dima Damen"
    ]
  }
}